<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Trajectory Convolution for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<email>dhlin@ie.cuhk.edu.hk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Trajectory Convolution for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">56085920EB7448DA2023F2DA5240EFFF</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to leverage the temporal dimension is one major question in video analysis. Recent works <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b35">36]</ref> suggest an efficient approach to video feature learning, i.e., factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption -the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption can be overly strong in practical applications, especially in action recognition where the motion serves as a crucial cue. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution. This operation explicitly takes into account the changes in contents caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths, trajectories. On two large-scale action recognition datasets, Something-Something V1 and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The past decade has witnessed significant progress in action recognition <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b0">1]</ref>, especially due to the advances in deep learning. Deep learning based methods for action recognition mostly fall into two categories, two-stream architectures <ref type="bibr" target="#b28">[29]</ref> with 2D convolutional networks and 3D convolutional networks <ref type="bibr" target="#b33">[34]</ref>. Particularly, the latter has demonstrated great potential on large-scale video datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b24">25]</ref>, with the use of new training strategies like transferring weights from pretrained 2D CNNs <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>However, for 3D convolution, several key questions remain to be answered: (1) 3D convolution involves substantially increased computing cost. Is it really necessary? (2) 3D convolution treats the spatial and temporal dimensions uniformly. Is it the most effective way for video modeling? We are not the first to raise such questions. In recent works, there have been attempts to move beyond 3D convolution and further improve the efficiency and effectiveness of joint spatio-temporal analysis. For instance, both Separable-3D (S3D) <ref type="bibr" target="#b46">[47]</ref> and R(2+1)D <ref type="bibr" target="#b35">[36]</ref> obtain superior performance by factorizing the 3D convolutional filter into separate spatial and temporal operations. However, both methods are based on an implicit assumption that the feature maps across frames are well aligned so that the features at the same locations (across consecutive frames) can be aggregated via temporal convolution. This assumption ignores the motion of people or objects, a key aspect in video analysis.  A natural idea to address this issue is to track the objects of interest and extract the features along their motion paths, i.e., trajectories. This idea has been explored in previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>. The most recent work along this direction is the Trajectory-pooled Deep-convolutional Descriptor (TDD) <ref type="bibr" target="#b40">[41]</ref>, which aggregates off-the-shelf deep features along trajectories. However, in this method, the visual features are derived separately from an existing deep network, just as a replacement of hand-crafted features. Hence, a question emerges: can we learn better video features in conjunction with feature tracking?</p><p>In pursuit of this question, we develop a new CNN architecture for learning video features, called TrajectoryNet. Inspired by the Separable-3D network <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b46">47]</ref>, our design involves a cascade of convolutional operations respectively along the spatial and temporal dimensions. A distinguishing feature of this architecture is that it introduces a new operation, namely the trajectory convolution, to take the place of the standard temporal convolution. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, the trajectory convolution operates along the trajectories that trace the pixels corresponding to the same physical points, rather than at fixed pixel locations. The trajectories can be derived from either a precomputed optical flow field or a dense flow prediction network trained jointly with the features. The standard temporal convolution can be seen as a special case of the trajectory convolution where all pixels are considered to be stationary over time.</p><p>Experimental results on Something-Something V1 and Kinetics datasets show that by explicitly taking into account the motion dynamics in the temporal operation, the proposed network obtains considerable improvements over the Separable-3D, a competitive baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Trajectory-based Methods for Action Recognition Action recognition in videos has been greatly advanced thanks to the up-springing of powerful features. It was firstly tackled by extracting spatial-temporal local descriptors <ref type="bibr" target="#b38">[39]</ref> from space-time interest points <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b45">46]</ref>. These successful local features include: Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b2">[3]</ref>, Histogram of Optical Flow (HOF) <ref type="bibr" target="#b20">[21]</ref>, and Motion Boundary Histogram (MBH) <ref type="bibr" target="#b3">[4]</ref>.</p><p>Over the years, it was recognized that the 2D space domain and 1D time domain have different characteristics and should be handled in a different manner intuitively. As of the motion modeling in the temporal domain, trajectories have been a powerful intermediary to convey such motion information. Messing et al <ref type="bibr" target="#b23">[24]</ref> used a KLT tracker <ref type="bibr" target="#b21">[22]</ref> to extract feature trajectories and applied log-polar uniform quantization. Sun et al <ref type="bibr" target="#b32">[33]</ref> extracted trajectories by matching SIFT feature between frames. These trajectories are based on sparse interest points, which have been later proved to be inferior to dense sampling. In <ref type="bibr" target="#b36">[37]</ref>, Wang et al used dense trajectories to extract low-level features within aligned 3D volumes. An improved version <ref type="bibr" target="#b37">[38]</ref> increased recognition accuracy by estimating and compensating the effect of camera motion. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> also revealed that trajectory itself can serve as a component of descriptors in the form of concatenated displacement vectors, which was consolidated by deep learning methods <ref type="bibr" target="#b28">[29]</ref>.</p><p>Wang et al first proposed TDD in <ref type="bibr" target="#b40">[41]</ref> to introduce deep features to trajectory analysis. It conducts trajectory-constrained pooling to aggregate deep features into video descriptors. However, the backbone two-stream CNN <ref type="bibr" target="#b28">[29]</ref>, from which the deep feature is extracted, is learned from very short frame snippets and is unaware of the information of temporal evolution. In addition, all of these trajectory-aligned methods rely on encoding methods such as Fisher vectors (FV) <ref type="bibr" target="#b44">[45]</ref> and vectors of locally aggregated descriptors (VLAD) <ref type="bibr" target="#b13">[14]</ref> and an extra SVM is needed for classification, which prohibits end-to-end training. To sum up the discussion above, we provide a comparison of our approach with previous works on action recognition in Table <ref type="table">1</ref>.</p><p>Table <ref type="table">1</ref>: A comparison of our approach with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Use deep feature? Feature tracking? End-to-end? STIP <ref type="bibr" target="#b19">[20]</ref> DT <ref type="bibr" target="#b36">[37]</ref>, iDT <ref type="bibr" target="#b37">[38]</ref> TSN <ref type="bibr" target="#b41">[42]</ref>, I3D <ref type="bibr" target="#b0">[1]</ref> TDD <ref type="bibr" target="#b40">[41]</ref> TrajectoryNet (Ours) Action Recognition in the Context of Deep Learning Deep convolutional neural networks based models have been widely applied to action recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>, which can be mostly categorized into two families, i.e. two-stream networks <ref type="bibr" target="#b28">[29]</ref> and 3D convolution networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, 3D convolutonal network has drawn attention since Carreira et al introduced Inflated-3D models <ref type="bibr" target="#b0">[1]</ref> by inflating an existing 2D convolutional network to its 3D variant and training on a very large action recognition dataset <ref type="bibr" target="#b18">[19]</ref>. Tran et al argued in <ref type="bibr" target="#b35">[36]</ref> that factorizing 3D convolutions into separable spatial and temporal convolutions obtains higher accuracy. Similar phenomenon is also observed in Separable-3D models by Xie et al <ref type="bibr" target="#b46">[47]</ref>. Wang et al incorporated multiplicative interaction into 3D convolution for modeling relation in <ref type="bibr" target="#b39">[40]</ref>. All of these modifications are focused on the single modality, i.e. the appearance branch.</p><p>Apart from network architectural designs, another direction is to exploit the interaction of appearance and motion information of action. Feichtenhofer et al explored the strategies of spatio-temporal fusion of two-stream networks at earlier stages in <ref type="bibr" target="#b6">[7]</ref>. Such attempts are mostly simple manipulation of feature such as stacking, addition <ref type="bibr" target="#b6">[7]</ref>, and multiplicative gating <ref type="bibr" target="#b5">[6]</ref>. Motion Representation using Convolutional Networks Optical flow has been used as a generic representation of motion as well as trajectory in particular for decades. As a competitive counterpart to the classical variational approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b30">31]</ref>, many parametric models based on CNN have been recently proposed and achieved promising results in estimating optical flow. These include, but are not limited to, the FlowNet family <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11]</ref>, SpyNet <ref type="bibr" target="#b25">[26]</ref>, and PWC-Net <ref type="bibr" target="#b31">[32]</ref>. The aforementioned models are learned in a supervised manner on large-scale simulated flow datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>, possibly leaving a large gap between simulated animations and real-world videos. Also, these datasets are designed for accurate flow prediction, which is possibly not appropriate for motion estimation of human action due to inhomogeneity of displacement across optical flow dataset and human action dataset, as revealed in <ref type="bibr" target="#b10">[11]</ref>. As of the network architecture, most models require parameters in the magnitude of 10 7 ∼ 10 8 , which both prohibits being plugged into action recognition networks as a submodule and causes too much computational cost. Zhu et al proposed MotionNet <ref type="bibr" target="#b50">[51]</ref> to learn dense flow fields in an unsupervised manner and plugged it into a two-stream network <ref type="bibr" target="#b28">[29]</ref> to be finetuned for action recognition task. The MotionNet is relatively light-weighted and can accept a sequence of multiple images. However, this is only used to substitute the pre-calculated optical flow while maintaining the conventional two-stream architecture. Zhao et al proposed an alternative representation based on cost volume for efficiency at the cost of degraded quality of motion field <ref type="bibr" target="#b48">[49]</ref>. Transformation-Sensitive Convolutional Networks Conventional CNN operates on fixed locations in a regular grid, which limits its ability to modeling unknown geometric transformations. Spatial Transform Networks (STN) <ref type="bibr" target="#b12">[13]</ref> is the first to introduce spatial transformation learning into deep models. It estimates a global parametric transformation on which the ordinary feature map is warped. Such warping is computationally expensive and the transformation is considered to be universal across the whole image, which is usually not the case for action recognition, since different body parts have their own movement. In Dynamic Filter Networks <ref type="bibr" target="#b15">[16]</ref>, Xu et al introduce dynamic filters which are conditioned on the input and can change over samples. This enables learning local spatial transformations. Deformable Convolutional Network (DCN) <ref type="bibr" target="#b1">[2]</ref> achieves similar local transformation in a different way. While maintaining filter weights invariant to the input, the proposed deformable convolution first learns a dense offset map from the input, and then applies it to the regular feature map for re-sampling. The proposed trajectory convolution is inspired by the deformable sampling in DCN and utilizes it for feature tracking in the spatiotemporal convolution operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>The TrajectoryNet model is built with the trajectory convolution operation. In this section, we first introduce the concept of trajectory convolution. Then we illustrate the architecture of TrajectoryNet. Finally we describe the approach to learning the trajectory together with the trajectory convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Trajectory Convolution</head><p>In the context of separable spatial temporal 3D convolution, the 1D temporal convolution is conducted pixel-wise on the 2D spatial feature map along the temporal dimension. Given input feature maps x t (p) at the t-th time step, the output feature y t (p), at position p = (h, w) ∈ [0, H) × [0, W ), is calculated by the inner product of input feature sequences at same spatial position across neighboring frames and the 1D convolution kernels.</p><p>By revisiting the idea of trajectory modeling in the action recognition literature, we introduce the concept of trajectory convolution. In trajectory convolution, the convolutional operation is done across irregular grids such that the sampled positions at different times correspond to the same physical point of a moving object. Formally, parameterized by the filter weight {w τ : τ ∈ [-∆t, ∆t]} with kernel size (2∆t + 1), the output feature y t (p) is calculated as</p><formula xml:id="formula_0">y t (p) = ∆t τ =-∆t w τ x t+τ ( p t+τ ).<label>(1)</label></formula><p>Following the formulation of trajectory in <ref type="bibr" target="#b36">[37]</ref>, the point p t at frame t can be tracked to position p t+1 at next frame (t + 1) in the presence of a forward dense optical flow field -→ ω = (u t , v t ) = F(I t , I t+1 ) using the following equation</p><formula xml:id="formula_1">p t+1 = (h t+1 , w t+1 ) = p t + - → ω (p t ) = (h t , w t ) + - → ω | (ht,wt) .<label>(2)</label></formula><p>For τ &gt; 1, the sample position p t+τ can be calculated by applying Eq. ( <ref type="formula" target="#formula_1">2</ref>) iteratively. To track to the previous frame (t -1), a backward dense optical flow field ←ω = (u t , v t ) = F(I t , I t-1 ) is used likewise.</p><p>Since the optical flow field is typically real-valued, the sampling position p t+τ becomes fractional. Therefore, the corresponding feature x( p t+τ ) is derived via interpolation with a specific sampling kernel G, written as</p><formula xml:id="formula_2">x( p t+τ ) = p G(p , p t+τ ) • x(p ).<label>(3)</label></formula><p>In this paper, we will not go deeper into the usage of different choices of sampling kernels G and use the bilinear interpolation as default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Relation with Deformable Convolution</head><p>The original deformable convolution is introduced for 2D convolution. But it is natural to extend it to the 3D scenarios. A spatio-temporal grid R ∈ R 3 can be defined by an ordinary 3D convolution specified by a certain receptive field size and dilation. For each location q 0 ∈ (t, h, w) on the output feature map y, the response is calculated by sampling on irregular locations offset by ∆q n .</p><formula xml:id="formula_3">y(q 0 ) = qn∈R w(q n ) • x(q 0 + q n + ∆q n )<label>(4)</label></formula><p>The trajectory convolution can then be viewed as a special case of 3D deformable convolution where the offset map is from the trajectories. Here, the grid R = {(-1, 0, 0), (0, 0, 0), (1, 0, 0)} is defined by a 3 × 1 × 1 kernel with dilation 1. The temporal component of the offset is always 0, i.e. ∆q n = (0, ∆p n ). The discussion above reveals the relationship with deformable convolution. Therefore, the trajectory convolution can be efficiently implemented similar to the way discussed in <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combining Motion and Appearance Features</head><p>The trajectory convolution helps the network to aggregate appearance features along motion path, alleviating the motion artifact by trajectory alignment. However, the motion information itself is important for action recognition. Inspired by the trajectory descriptor proposed in <ref type="bibr" target="#b36">[37]</ref>, we describe local motion patterns at each position p using the sequence of trajectory information in the form of coordinates of sampling offsets {∆p τ : τ ∈ [-∆t, ∆t]}. This is equivalent to stacking the offset map for trajectory convolution and the original appearance feature map. The offset map is normalized through Batch-Normalization <ref type="bibr" target="#b11">[12]</ref> before concatenation. As a result, we achieve the combination of appearance feature and motion information in terms of trajectory with minimal increase of network parameters. Compared with the canonical two-stream approaches, which are based on late fusion of two networks, our approach leads to a unified network architecture and is much more parameter and computation efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The TrajectoryNet Architecture</head><p>Based on the concept of trajectory convolution, we design a unified architecture that can align appearance and motion features along the motion trajectories. We call it TrajectoryNet by integrating trajectory convolution into the Separable-3D ResNet18 architecture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36]</ref>. The 1D temporal convolution component of a (2+1)D-convolutional block is replaced by a trajectory convolution with down-sampled motion field, such as a pre-computed optical flow, in the middle level of the network. The appearance feature map for trajectory convolution is optionally concatenated with the down-sampled motion field to introduce extra motion information. Adding trajectory convolution at higher levels is likely to provide less motion information since spatial resolution is reduced and down-sampled optical flow may be inaccurate. Adding trajectory convolution at lower levels increases the precision of motion estimation, but the receptive field for sampling position is limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Learning Trajectory</head><p>As discussed in the previous subsection, the trajectory convolution can be viewed as deformable convolution with a special deformation map, that is the motion trajectory in the video. It is capable of accumulating gradients from higher layers via back-propagation. Therefore, if the trajectory can be estimated by a parametric model, we can learn the model parameters using back-propagation as well.</p><p>The most straight forward approach for this cause is applying a small 3D CNN to estimate trajectories as an mimic of the 2D CNN used in the deformable convolution networks <ref type="bibr" target="#b1">[2]</ref>. Preliminary experiments show that this is not very effective. It can be observed that the offsets obtained simply by applying a 3D convolutional layer over the same input feature map are highly correlated to the appearance. On the contrary, the motion representation, which we use trajectory as a medium, has long been considered to be invariant to appearance intuitively and empirically <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28]</ref>. Therefore, we cannot naïvely adopt the way of learning offsets in <ref type="bibr" target="#b1">[2]</ref>. This also reveals the difference between the original deformable convolution for object detection and our trajectory convolution for action recognition:</p><p>The original deformable convolution attempts to learn deformation of spatial configuration within an single image while our trajectory convolution tries to model the variation of appearance deformation across neighboring images, despite sharing the similar mathematical formulation.</p><p>To tackle such issue, we train another network to predict the trajectory individually as an alternative.</p><p>In particular, we use MotionNet <ref type="bibr" target="#b50">[51]</ref> as the basis due to its lightweightness. It accepts a stack of (M + 1) images as a 3(M + 1)-channel input and predicts a series of M motion field maps as a 2Mchannel output. Following a "downsample-upsample" design like FlowNet-SD <ref type="bibr" target="#b10">[11]</ref>, motion fields with multiple spatial resolutions are predicted. The network is trained without external supervision such as ground-truth optical flow. An unsupervised loss L unsup <ref type="bibr" target="#b50">[51]</ref> is designed to enforce pair-wise reconstruction and similarity, with motion smoothness as a regularization.</p><p>Once pre-trained, the MotionNet can be plugged into the TrajectoryNet architecture to substitute the input of pre-computed optical flow. We modify the original model in <ref type="bibr" target="#b50">[51]</ref> to produce optical flow map of the same resolution of feature maps where the trajectory convolution operates on. The MotionNet can also be fine-tuned with the classification network. In this case, the loss for network training is a weighted sum of the unsupervised loss L unsup and the cross-entropy loss for classification L cls , written as L = γL unsup + L cls .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>To evaluate the effectiveness of our TrajectoryNet, we conduct experiments on two benchmark datasets for action recognition: Something-Something V1 <ref type="bibr" target="#b7">[8]</ref> and Kinetics <ref type="bibr" target="#b18">[19]</ref>. Visualization of intermediate features for both appearance and trajectory is also provided. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setups</head><p>Network configuration We use the Separable-3D ResNet-18 <ref type="bibr" target="#b8">[9]</ref> as the base model, if not specified.</p><p>Starting from the base ResNet-18 model, A 1-D temporal convolution module with temporal kernel size of 3, followed by Batch Normalization <ref type="bibr" target="#b11">[12]</ref> and ReLU non-linearity is inserted after every 2-D spatial convolution module. A dropout of 0.2 is used between the global pooling and the last C-dimensional (C equals the total number of classes) fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generating trajectories</head><p>As stated above, we study two methods to generate trajectories: one is based on variational methods and the other is based on CNNs. For the former, we adopt the TV-L1 algorithm <ref type="bibr" target="#b47">[48]</ref> which is implemented in OpenCV with CUDA. To match the size of input feature, two types of pooling are used to down-sample the optical flow field: average pooling and max pooling.</p><p>For the latter, the MotionNet is trained by randomly sampling images pairs from UCF-101 <ref type="bibr" target="#b29">[30]</ref>. The training policy follows the practices in <ref type="bibr" target="#b50">[51]</ref>.</p><p>Training The network is trained with stochastic gradient descent with momentum set to 0.9. The weights for 2D spatial convolution are initialized with the 2D ResNet pre-trained on ImageNet <ref type="bibr" target="#b26">[27]</ref>.</p><p>The length of each input clip is 16 and the sampling step varies from 1 to 2. For Something-Something V1, the batch size is set to 64 while for Kinetics, the batch size is 128. On Kinetics, the network is trained from an initial learning rate of 0.01 and is reduced by 1 10 every 40 epochs. The whole training procedure takes 100 epochs. For Something-Something V1, the epoch number is halved because the duration of its videos is shorter.</p><p>Testing At test time, we follow the common practice by sampling a fixed number of N snippets (N = 7 for Something-Something V1 and N = 25 for Kinetics) with an equal temporal interval. By cropping and flipping four corners and the center of each frame within a snippet, 10 inputs are obtain for each snippet. The final class scores are calculated by averaging the scores across all 10N inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Trajectory convolution We first evaluate the effect of using trajectory convolution in the Separable-3D ResNet architecture in Table <ref type="table" target="#tab_0">2</ref>. Consistent improvement of accuracy can be observed if trajectory convolution is used. Then, we study the effect of incorporating trajectory in different locations. Adding trajectory convolution increases the top-5 accuracy but the top-1 accuracy saturates. In the remaining experiments, we use only 1 trajectory convolution at the res3b1.conv1 block, if not specified.</p><p>Since we did not see remarkable gain, we conjecture that this is because the used trajectory is derived from the optical flow down-sampled via average pooling. The optical flow is already smoothed with TV-L1 and the extra average pooling degrades the quality more. To verify this, we preform an additional experiment by replacing average pooling with max pooling. This alternative downsampling strategy preserves more details without degrading the trajectory. Furthermore, as will be shown in Table <ref type="table" target="#tab_2">4</ref>, using trajectory learned from MotionNet leads to higher accuracy. This indicates that the performance of TrajectoryNet highly depends on the quality of trajectory. Combining motion and appearance features We compare the results of incorporating motion information into the trajectory convolution in Table <ref type="table" target="#tab_1">3</ref>. We can clearly see the improvement of more than 1% after encoding a 4-dimensional feature map of trajectory coordinates. We compare with several other methods, such as the early spatial fusion by concatenation with motion feature map <ref type="bibr" target="#b6">[7]</ref> and the late fusion used in the two-stream network <ref type="bibr" target="#b28">[29]</ref>. Though there is still an apparent gap between ours and the late-fusion strategy, our fusion strategy achieves notable increase with negligible increase of parameters. And it also completely removes the computation for running a motion-stream recognition network.  <ref type="formula">17</ref>) outperform those derived from TV-L1 <ref type="bibr" target="#b47">[48]</ref>. It is interesting to observe that jointly training MotionNet and TrajectoryNet will yield lower accuracies than freezing MotionNet unless the unsupervised loss is introduced. We conjecture that the existence of L unsup can help to maintain the quality of trajectories by enforcing the pair-wise consistency. The necessity of multi-task fine-tuning may also explain the difficulty of using shallow convolutional modules with random initialization to estimate the trajectory, which we have discussed in Sec 3.5. Trajectories with step greater than one Here we evaluate the model which accepts an input of 16 frames but at a sampling step of 2. To be more specific, we collect a consecutive of 32 frames and randomly sample one frame for every two neighboring frames. This enlarges the effective coverage of the architecture, i.e. from 16 to 32, while keeping the computation the same. With the strategy of learning trajectory mentioned above, the TrajectoryNet can still improve over the baseline. This also reflects the flexibility of learnable trajectory, since pre-computed optical flow has to be re-run for the whole training set under such circumstances. We compare the performance of our TrajectoryNet with other state-of-the-art methods. The results on Something-Something V1 <ref type="bibr" target="#b7">[8]</ref> and Kinetics <ref type="bibr" target="#b18">[19]</ref> are shown in Table <ref type="table" target="#tab_6">7</ref> and Table <ref type="table" target="#tab_7">8</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Visualization</head><p>We present a qualitative study by visualizing the intermediate feature of our TrajectoryNet in Figure <ref type="figure" target="#fig_3">2</ref>. Given a pair of two consecutive images on top of the first column, we first compare the feature map at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone network Pre-train Val. Avg. Acc. TSN (RGB) <ref type="bibr" target="#b41">[42]</ref> BN-Inception-v2 ImageNet 77.8 I3D (RGB) <ref type="bibr" target="#b0">[1]</ref> BN-Inception-v1 ImageNet 81.2 Nonlocal-I3D (RGB) <ref type="bibr" target="#b42">[43]</ref> ResNet-101 ImageNet 85.5 R(2+1)D (RGB) <ref type="bibr" target="#b35">[36]</ref> ResNet-34 Sports-1M 82.6 C3D <ref type="bibr" target="#b34">[35]</ref> ResNet-18 -75.7 ARTNet w/. TSN <ref type="bibr" target="#b39">[40]</ref> ResNet the layer of res3b1.conv1, i.e. on which the trajectory convolution is applied, at the bottom of the first column. We can observe a visible spatial shift between the two images' high response regions, which conforms to our assumption that feature map is not well aligned due to object movement. We also demonstrate different types of trajectories that we use in the experiments, namely the TV-L1 <ref type="bibr" target="#b47">[48]</ref> optical flow and prediction from MotionNet before and after finetune on the second, third and fourth column. We can see that the motion estimation by the original MotionNet is less smooth than TV-L1 especially in the background regions. For foreground objects, however, MotionNet does well and can sometimes produce motion with more rigid shape, e.g. the hand on the left example of Figure <ref type="figure" target="#fig_3">2</ref>. Also, the joint training further improves the quality of trajectories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a unified end-to-end architecture called TrajectoryNet for action recognition. The approach is to incorporate the repeatedly proven idea of trajectory modeling into the Separable-3D network by introducing a new operation named trajectory convolution. The Trajec-toryNet further combines appearance and motion information in a unified model architecture. The proposed architecture achieves notable improvements over the Separable-3D baseline, providing a new perspective of explicitly considering motion dynamics in the deep networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>32nd</head><label></label><figDesc>Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada. Motion field ! = ℱ $ % , $ %'( ! = ℱ $ % , $ %)( Convolute over x p , -, . ∈ 1 -1, 1, 1 + 1 Interpolate feature x p , %)( Determine p , %)( from motion field direction for regular convolution direction along</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of our trajectory convolution. Given a sequence of video frames (left) and its corresponding input feature map of size C × T × H × W (bottom-middle; the dimension of channels C is simplified as one for clarity), in order to calculate the response of a specific point at time step t, we leverage the motion fields ←ω and -→ ω (top-middle; the arrows in blue denote the motion velocity) to determine the sampling location at neighboring time step t -1 and t + 1 in the sense of tracking along the motion path. The response is denoted on the output feature map (bottom-right). The operation of trajectory convolution (denoted in a red box) is illustrated on the top-right. This figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 1</head><label>1</label><figDesc>Dataset descriptionsSomething-Something V1<ref type="bibr" target="#b7">[8]</ref> is a large-scale crowd-sourced video dataset on human-object interaction. It contains 108,499 video clips in 174 classes. The dataset is split into train, validation and test subset in the ratio of around 8:1:1. The top-1 and top-5 accuracy is reported.Kinetics<ref type="bibr" target="#b18">[19]</ref> is a large-scale video dataset on human-centric activities sourced from YouTube. We use the version released in 2017, covering 400 human action classes. Due to the inaccessibility of some videos on YouTube, our version contains 240436, 19796 and 38685 clips in the training, validation and test subset, respectively. The recognition performance is measured by the average of top-1 and top-5 accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the intermediate of the TrajectoryNet. These two image pairs depict the action of "moving something (a pen) down" and "trying but failing to attach something (a ball) to something (a cat) because it doesn't stick." For each block, the first column show a pair of input images and their corresponding feature map at the layer of res3b1.conv1; the second, third and fourth column show the optical flow field generated by TV-L1 algorithm and learned by MotionNet before and after finetuning (The motion field encoded in HSV color map as well as the components of x-axis and y-axis are shown from top to bottom). The figure is best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Results of using trajectory convolution in different convolutional layers in the Separable-3D ResNet-18 network. The accuracy is reported on the validation subset of Something-Something V1. Usage of Traj. Conv. Down-sample Method Top-1 Acc. Top-5 Acc.</figDesc><table><row><cell>None</cell><cell>None</cell><cell>34.30</cell><cell>65.66</cell></row><row><cell>res2b1.conv1</cell><cell>Avg. Pool</cell><cell>34.49</cell><cell>66.23</cell></row><row><cell>res3a.conv1</cell><cell>Avg. Pool</cell><cell>34.79</cell><cell>66.21</cell></row><row><cell>res3b1.conv1</cell><cell>Avg. Pool</cell><cell>34.96</cell><cell>66.24</cell></row><row><cell>res3b1.conv1,2</cell><cell>Avg. Pool</cell><cell>34.72</cell><cell>66.89</cell></row><row><cell>res3b1.conv1</cell><cell>Max Pool</cell><cell>36.04</cell><cell>67.72</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Results of incorporating different sources of input into the trajectory convolution in the Separable-3D ResNet-18 network. The ft. denotes the feature map. The accuracy is reported on the validation subset of Something-Something V1. ) fine-tuning the MotionNet with both the unsupervised loss and classification loss. The loss weight γ is set to 0.01. The results are listed in Table4. It turns out that the trajectories learned by both MotionNet-(2) and MotionNet-(</figDesc><table><row><cell>Source</cell><cell cols="4">Usage of Traj. Conv. # param. Top-1 Acc. Top-5 Acc.</cell></row><row><cell>appearance</cell><cell>res3b1.conv1</cell><cell>15.2M</cell><cell>34.96</cell><cell>66.24</cell></row><row><cell>appearance + motion (ft.)</cell><cell>res3b1.conv1</cell><cell>15.9M</cell><cell>35.24</cell><cell>67.22</cell></row><row><cell>appearance + trajectory (# dim=4)</cell><cell>res3b1.conv1</cell><cell>15.2M</cell><cell>36.08</cell><cell>67.72</cell></row><row><cell>two-stream S3D (late fusion)</cell><cell>None</cell><cell>30.4M</cell><cell>40.67</cell><cell>72.79</cell></row><row><cell cols="5">Learning trajectory Here we compare the learned trajectory against pre-computed optical flow</cell></row><row><cell cols="5">from TV-L1 [48]. We choose two architectures of MotionNet: one accepts one image pair and outputs</cell></row><row><cell cols="5">one motion field (denoted by MotionNet-(2)), and the other accepts 17 consecutive images and</cell></row><row><cell cols="5">produces 16 motion fields (denoted by MotionNet-(17)). We study three training policies: (1)</cell></row><row><cell cols="5">fixing the MotionNet once it is pre-trained; (2) fine-tuning the MotionNet with the classification cross-</cell></row><row><cell>entropy loss; and (3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Results of learning trajectory. The settings are elaborated in the body part.</figDesc><table><row><cell cols="5">source of trajectory fine-tune weight unsup. loss Top-1 Acc. Top-5 Acc.</cell></row><row><cell>TV-L1</cell><cell>-</cell><cell>-</cell><cell>34.96</cell><cell>66.24</cell></row><row><cell>MotionNet-(2)</cell><cell></cell><cell></cell><cell>36.37</cell><cell>67.74</cell></row><row><cell>MotionNet-(2)</cell><cell></cell><cell></cell><cell>34.72</cell><cell>65.59</cell></row><row><cell>MotionNet-(2)</cell><cell></cell><cell></cell><cell>36.91</cell><cell>68.47</cell></row><row><cell>MotionNet-(17)</cell><cell></cell><cell></cell><cell>35.69</cell><cell>66.82</cell></row><row><cell>MotionNet-(17)</cell><cell></cell><cell></cell><cell>35.25</cell><cell>66.65</cell></row><row><cell>MotionNet-(17)</cell><cell></cell><cell></cell><cell>36.69</cell><cell>68.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results of using trajectories with step greater than one.Runtime CostIn Table6, we report the runtime of the proposed TrajectoryNet with two settings:(1) the one whose trajectories are from pre-computed TV-L1 (time not included) and (2) the one whose trajectories are inferred from MotionNet-(17) (time included). Compared with its plain counterpart, the TrajectortNet with pre-computed TV-L1 incurs less than 10% additional computation for the operation of trajectory convolution. It takes TrajectoryNet with MotionNet-(17) an extra 0.137 second for network forward compared to TrajectoryNet with TV-L1, which can be ascribed to the forward time of the MotionNet plugged in.</figDesc><table><row><cell cols="2"># of frame × step Effective coverage</cell><cell>Usage of Trajectories</cell><cell cols="2">Top-1 Acc. Top-5 Acc.</cell></row><row><cell>16 × 2</cell><cell>32</cell><cell>None</cell><cell>42.47</cell><cell>74.57</cell></row><row><cell>16 × 2</cell><cell>32</cell><cell>MotionNet-(17)-ft.-unsup.</cell><cell>43.32</cell><cell>74.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Runtime comparison of TrajectoryNet and the counterpart. The network is tested on a workstation with Intel(R) Xeon(R) CPU (E5-2640 v3 @2.60GHz) and Nvidia Titan X GPU.</figDesc><table><row><cell>Method</cell><cell cols="2">Net. forward (sec) ∆t (sec)</cell></row><row><cell>S3D</cell><cell>0.390</cell><cell>-</cell></row><row><cell>TrajectoryNet (TV-L1)</cell><cell>0.426</cell><cell>+0.036</cell></row><row><cell>TrajectoryNet (MotionNet-(17))</cell><cell>0.563</cell><cell>+0.137</cell></row><row><cell>4.4 Comparison with State-of-the-Arts</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>respectively. For Something-Something V1, we use 16 frames with a step of 2 as input and apply MotionNet-<ref type="bibr" target="#b16">(17)</ref> to produce trajectory. Motion information encoded by trajectory is used optionally. On Table7, we can see that our TrajectoryNet achieves competitive results with state-of-the-art models including those with deeper models or those pre-trained on larger models. After pre-training on Kinetics, the accuracy is boosted to a new level. For Kinetics, a MotionNet-(2) is used. On Table8, the TrajectoryNet improves the Separable-3D baseline. With 16 input frames at a step of 2, it performs on par with models with similar model complexity.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison with state-of-the-art methods on the validation and test set of Something-Something V1. The performance is measured by the Top-1 accuracy.</figDesc><table><row><cell>Method</cell><cell>Backbone network</cell><cell cols="2">Pre-train Val Top-1</cell></row><row><cell>3D-CNN [8]</cell><cell>C3D</cell><cell cols="2">Sports-1M 11.5</cell></row><row><cell>MultiScale TRN [50]</cell><cell>BN-Inception</cell><cell>ImageNet</cell><cell>34.4</cell></row><row><cell>ECO lite [52]</cell><cell cols="2">BN-Inception + 3D-ResNet18 Kinetics</cell><cell>46.4</cell></row><row><cell>Non-local I3D + GCN [44]</cell><cell>ResNet-50</cell><cell>Kinetics</cell><cell>46.1</cell></row><row><cell>TrajectoryNet-MotionNet-(17) w/o. motion</cell><cell>ResNet-18</cell><cell>ImageNet</cell><cell>43.3</cell></row><row><cell>TrajectoryNet-MotionNet-(17) w/. motion</cell><cell>ResNet-18</cell><cell>ImageNet</cell><cell>44.0</cell></row><row><cell>TrajectoryNet-MotionNet-(17) w/o. motion</cell><cell>ResNet-18</cell><cell>Kinetics</cell><cell>47.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Comparison with state-of-the-art methods on the validation subset of Kinetics. The performance is measured by the average of Top-1 and Top-5 accuracy.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment This work is partially supported by the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No. TS1610626), and the Early Career Scheme (ECS) of Hong Kong (No. 24204215).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7445" to="7454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional two-stream fusion for video action recognition</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Berthold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">G</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning(ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1704" to="1716" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bert</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="667" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual perception of biological motion and a model for its analysis</title>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Johansson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; psychophysics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="201" to="211" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial intelligence (IJCAI)</title>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Activity recognition using the velocity histories of tracked keypoints</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Messing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="104" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Mathew</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Gutfruend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03150</idno>
		<title level="m">Moments in time dataset: one million videos for event understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08416</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hierarchical spatiotemporal context modeling for action recognition</title>
		<author>
			<persName><forename type="first">Ju</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loong-Fah</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="2004" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><forename type="middle">Muneeb</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="124" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Appearance-and-relation networks for video classification</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A comparative study of encoding, pooling and normalization methods for action recognition</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="572" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An efficient dense and scale-invariant spatio-temporal interest point detector</title>
		<author>
			<persName><forename type="first">Geert</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="650" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: : Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognize actions by disentangling components of dynamics</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6566" to="6575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Hidden two-stream convolutional networks for action recognition</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
