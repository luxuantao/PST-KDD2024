<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Term Global Motion Estimation and Its Application for Sprite Coding, Content Description, and Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aljoscha</forename><surname>SmoliÄ‡</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Thomas</forename><surname>Sikora</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jens-Rainer</forename><surname>Ohm</surname></persName>
						</author>
						<title level="a" type="main">Long-Term Global Motion Estimation and Its Application for Sprite Coding, Content Description, and Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">04DAF8B6F7FCB15BCCB466A2E4C058B0</idno>
					<note type="submission">received November 16, 1998; revised August 13, 1999.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Global motion estimation</term>
					<term>MPEG-7 content description</term>
					<term>segmentation</term>
					<term>sprite coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a new technique for longterm global motion estimation of image objects. The estimated motion parameters describe the continuous and time-consistent motion over the whole sequence relatively to a fixed reference coordinate system. The proposed method is suitable for the estimation of affine motion parameters as well as for higher order motion models like the parabolic model-combining the advantages of feature matching and optical flow techniques. A hierarchical strategy is applied for the estimation, first translation, affine motion, and finally higher order motion parameters, which is robust and computationally efficient. A closed-loop prediction scheme is applied to avoid the problem of error accumulation in long-term motion estimation. The presented results indicate that the proposed technique is a very accurate and robust approach for long-term global motion estimation, which can be used for applications such as MPEG-4 sprite coding or MPEG-7 motion description. We also show that the efficiency of global motion estimation can be significantly increased if a higher order motion model is applied, and we present a new sprite coding scheme for on-line applications. We further demonstrate that the proposed estimator serves as a powerful tool for segmentation of video sequences.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b0">[1]</ref><p>. While static sprites will already be part of the first version of the standard <ref type="bibr" target="#b6">[7]</ref>, a simplified form of dynamic sprites will be adopted in version 2, which is called global motion compensation.</p><p>For static sprites, long-term parameters are used that describe the continuous motion over the sequence relative to the sprite <ref type="bibr" target="#b6">[7]</ref>. The sprite is supposed to be generated off-line prior to encoding and transmitted in the beginning. Such a technique would not be suitable for real-time applications. However, it is also possible to transmit pieces of the sprite, which makes the concept suitable for low-delay applications.</p><p>The dynamic sprite coding scheme currently used in the MPEG-4 verification model applies short-term global motion parameters that describe the motion between consecutive frames <ref type="bibr" target="#b0">[1]</ref>. The accumulation is achieved by warping the sprite at every time step, controlled by the estimated shortterm parameters. The image generated from the sprite is used as reference for predictive coding, i.e., the prediction error is always coded and transmitted. Then, the sprite is updated by blending from the reconstructed image.</p><p>This method has two main drawbacks. The open-loop motion estimation of frame-to-frame parameters leads to accumulation of estimation and model errors. While the first errors are caused by inaccurate estimation, the latter are due to improper modeling of the motion of the object. These model errors may be minimized using an affine model from frame to frame, but over a longer period, the model errors will still accumulate if no prediction loop is applied. Because of these inherent problems, the prediction error must always be coded. The second drawback is the warping of the sprite at every time step, necessary to achieve a long-term description of the motion. It has to be also performed at the decoder.</p><p>This paper focuses on the presentation of a new approach for the estimation of long-term global motion parameters that combines feature matching and optical flow techniques, resulting in very accurate, stable, time-consistent, and robust estimates. Besides an affine motion model, we use a nonlinear parabolic model with 12 parameters that provides a better representation of the object motion over a long period. The estimation is performed hierarchically (translation-affineparabolic), which is robust and computationally efficient.</p><p>The estimator can readily be used for MPEG-4 sprite coding, but in addition we introduce a new very efficient sprite coding scheme for on-line applications-in parts similar to MPEG-4 static and dynamic sprites. Long-term parameters are used to avoid error accumulation encountered using MPEG-4 static sprite coding techniques. A fixed sprite is employed, and only new image content is updated at every timestep, thus sprite warping is not required as using MPEG-4 dynamic sprite coding techniques. The sprite updates are not coded in the sprite domain as for MPEG-4 static sprites but in the actual image domain as for MPEG-4 dynamic sprites. We have developed a very efficient compression scheme in this context.</p><p>The developed concept can also be applied in the framework of MPEG-7, which aims to define the description of multimedia content <ref type="bibr" target="#b8">[9]</ref>. The targeted descriptors range from high-level semantics that often have to be generated manually to lowlevel features that can be extracted automatically. Motion is one of the key features of video, and therefore motion descriptors and associated automatic extraction algorithms will play an important role for the MPEG-7 low-level video descriptors. The motion parameters and models presented in this paper are very powerful and flexible descriptors for global motion and the motion of image objects themselves. A parametric motion descriptor applying such models has been proposed to MPEG <ref type="bibr" target="#b17">[18]</ref> and is currently under investigation in the experimentation model of MPEG-7 <ref type="bibr" target="#b18">[19]</ref>.</p><p>For a meaningful application in the context of MPEG-7, it is necessary to provide robust automatic extraction algorithms that are stable over a long period. Therefore, the presented long-term algorithm is highly attractive for MPEG-7 applications. For compression of video in the context of MPEG-4, the goal is a visually optimal reconstruction of the image content. This implies that the motion estimation has to be very accurate. In a classification application (MPEG-7), the goal is a good description of the image content. The requirement on accuracy is not necessarily important if a good representation of the motion characteristics is provided.</p><p>A further possible field of application is segmentation of moving objects in front of moving background. Preliminary results demonstrate that for selected sequences our estimator can be used as a tool for fully automatic segmentation without using any a priori information about the image content for the processing.</p><p>This paper is organized as follows. In Section II, we describe the motion models used. The hierarchical longterm global motion estimation algorithm is presented in Section III. Section IV details the new sprite coding technique. In Section V, we provide experimental results, and Section VI summarizes and concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTION MODELS</head><p>The motion of two-dimensional (2-D) objects can be described by several models of increasing complexity. The simplest case is to allow only translational motion, which can be modeled by two parameters. In the next levels, parameters for planar rotation (around -axis) and zoom can be added. All these transformations are linear, which means that they can be described by a linear set of equations. The complete linear transformation is represented by the affine transformation (1) where and denote the image coordinates of a point before and after the displacement, respectively. The motion is determined by the affine motion parameters for the -coordinates and influencing the -coordinates, which are combined to the parameter vector <ref type="bibr" target="#b1">(2)</ref> The parameters and describe the translation, and describe the scaling, and and describe the shear of the object. Fig. <ref type="figure" target="#fig_0">1</ref>(a) illustrates the possible deformations of an affine transformation. The more complex the motion model is, the more freedom it allows to the deformation of the observed 2-D object, which means it is suitable to describe the motion of a greater variety of image objects. The extension to higher order polynomial models leads to the parabolic transformation <ref type="bibr" target="#b2">[3]</ref>, which is described with 12 parameters by (3) A parabolic motion model is suitable to represent parabolic curvature for the deformation of the observed object. Fig. <ref type="figure" target="#fig_0">1(b</ref>) illustrates the possible deformations with a parabolic transformation. Between consecutive frames, or over a short period, the object motion may be described sufficiently by an affine model. But over a longer period, a more sophisticated modeling of the motion may be required, especially if the observed object has a complex depth structure. Since we focus on long-term motion estimation, we have developed techniques for the estimation of higher order polynomial models. The parameter vector that has to be determined is given by (4) However, due to the increased complexity of the motion model, the computational complexity and the sensitivity against errors (numerical errors, model failures, and noise) increases The parabolic model is described by a nonlinear set of equations, and therefore estimation of the parameters requires sophisticated mathematical methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HIERARCHICAL LONG-TERM GLOBAL MOTION ESTIMATION</head><p>The developed long-term global estimation algorithm employs a recursive closed-loop prediction scheme inspired by earlier work on Kalman filtering techniques <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and the long-term estimation technique for motion parameters of a face model in <ref type="bibr" target="#b5">[6]</ref>. Update motion parameters are calculated between the actual image and a predicted image, using shortterm estimation techniques. The predicted image is always generated from a special long-term memory called sprite that contains all the image information that was visible throughout the sequence, by image warping controlled by the previously estimated parameters. Next the update parameters are accumulated to the previously calculated parameters. This yields an accurate, robust, and time-consistent long-term estimation avoiding error accumulation.</p><p>The proposed algorithm is an extension of earlier work on short-term global motion estimation <ref type="bibr" target="#b9">[10]</ref>, that employs a hierarchical (translation-affine-parabolic) structure and yields excellent stability and accuracy. As outlined in Fig. <ref type="figure" target="#fig_1">2</ref>, the approach taken is based on an initial estimate of translational motion between the actual and the predicted image using a feature matching algorithm in a first stage of the hierarchy. This initial estimate is used to compute affine motion parameters based on an optical flow algorithm in a next stage. In a third stage, more complex estimates of parabolic motion models are supported, using the results of the motion estimation of the previous stages. At every stage, the estimated shortterm parameters are accumulated to long-term parameters in a closed-loop prediction scheme, which overcomes the error accumulation problem. Finally, the sprite is updated with newly appearing image content, controlled by the resulting long-term parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Predicted Image and Sprite Generation</head><p>The algorithm employs a sprite (also referenced as background mosaic) to store all image information visible throughout the sequence. Fig. <ref type="figure" target="#fig_2">3</ref> illustrates the generation of a sprite over time for the background object of sequence Stefan. In our approach, the reference coordinate system is that of the sprite. This means that are sprite coordinates and are image coordinates. After the final motion parameters are estimated, the new image content is added to the long-term memory at a certain timestep. For all points that are not yet registered in the sprite, (1) or ( <ref type="formula">3</ref>) is employed depending on the motion model that is applied, using the calculated motion parameters. If falls into the object region inside the actual image, the sprite pixel is updated. Since does not correspond to integer pixel values, a bilinear interpolation has to be carried out, which introduces a low-pass effect.</p><p>The inverse procedure has to be carried out for the generation of a predicted image. For all image coordinates , (1) or (3) has to be inverted to find the corresponding controlled by the given set of motion parameters. This operation can be performed straightforward if an affine motion model is applied, since (1) is linear. Since (3) is nonlinear, it cannot be inverted and an iterative algorithm (Newton-Raphson <ref type="bibr" target="#b4">[5]</ref>) is applied to solve for Again, the calculated positions do not correspond to pixel values, and a bilinear interpolation is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stage 1: Feature Matching Algorithm</head><p>The first operation at every time instant in our estimation scheme is the generation of a predicted reference image from the sprite using the estimated parameters at the last time instant. Next, a first update only for the translational part of the motion is estimated between the predicted and the actual image using a fast and robust feature matching algorithm.</p><p>In general, matching techniques can be described by the following equation <ref type="bibr" target="#b0">[1]</ref>: <ref type="bibr" target="#b4">(5)</ref> is the image region to be matched and is the location of one pixel inside the region. The vector consists of the motion parameters that have to be estimated. The image intensities of the reference frame and the actual frame are denoted by and respectively. The transformation of a pixel with respect to a set of motion parameters is given by Equation ( <ref type="formula">5</ref>) describes the minimization of the sum of distance measures with respect to the motion parameters that have to be estimated.</p><p>In existing video coding standards, the region represents a square of <ref type="bibr" target="#b15">16</ref> 16 or 8 8 pixels, and the motion vector consists only of two translational motion parameters. The parameter space is discrete with half-pel accuracy. Nevertheless, an exhaustive search is very time consuming, which makes the motion estimation the most complex part of these coders leading to the development of a variety of fast low-complexity block matching algorithms.</p><p>In the case of global motion, the region can represent the whole image. The parameter vector consists of six or more parameters that are not discrete. This makes an exhaustive search impossible. Sophisticated numerical methods have to be applied <ref type="bibr" target="#b2">[3]</ref> for the iterative minimization of (5). Disadvantages of these methods are the very high complexity, the risk of being trapped in local minima, and the sensitivity against measurement errors and noise. Especially the nontranslational parameters are very sensitive against disturbances.</p><p>Optical-flow-based estimation algorithms described in the next section do not work well in the presence of large translational motion <ref type="bibr" target="#b12">[13]</ref>. The basic assumptions of linearity and invariance of the image intensity of an optical flow approach-that are used to derive (7)-are violated in such a case. This can lead to inaccurate estimates as shown in <ref type="bibr" target="#b9">[10]</ref>.</p><p>One advantage of matching techniques is the robustness for the estimation even of large translational motion. Therefore, we apply a simple low-complexity feature matching algorithm for the initial update of the translational motion parameters. The first step is the selection of a set of (typically ) feature points. We choose points where the image intensity has the largest Hessian <ref type="bibr" target="#b3">[4]</ref> (6)</p><p>These are peaks and pits in the image intensity, which can be localized without aperture problems <ref type="bibr" target="#b3">[4]</ref>. The feature displacements are calculated using block matching in anneighborhood (typically ) around the feature point. Besides the use of characteristic features, false matches cannot be avoided. Therefore, we estimate the global translation from the feature displacements with a robust M-estimator algorithm <ref type="bibr" target="#b4">[5]</ref>. An M-estimator is much less sensitive against outliers than, e.g., a least squares algorithm, and therefore perfectly suitable for our application. Having computed the initial update of the translational motion, we accumulate it to the previously calculated motion parameters, which is a simple addition.</p><p>As mentioned before, the purpose of the feature matching stage is to provide the following optical-flow-based stages with a good starting point, by selection of characteristic feature points and the calculation of the global translation with a robust M-estimator. This does not have to be the exact translational part of the global motion, since it will be refined in the next stages of our hierarchical approach. However, if no segmentation mask is available, it may occur that too many feature points are chosen from a differently moving foreground object-leading to a wrong initial estimate of the global translation. If this cannot be corrected in the following stages, inaccurate and unstable estimates will result. A solution for this problem is described in Section V-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Stage 2: Optical Flow Algorithm for Affine Motion</head><p>After the first stage, we again compute a predicted image now using the updated motion parameters. This new predicted image, the actual image and the updated motion parameters are input to the second stage.</p><p>The estimation of the affine motion is based on the optical flow constraint equation, which assumes that the intensity of a moving point remains constant <ref type="bibr" target="#b1">[2]</ref> (7)</p><p>Here, and denote the spatial derivatives, denotes the temporal derivative of the image intensity, and is the optical flow. From (1), we can express the optical flow based on an affine motion model (in analogy to the formulation for three-dimensional motion parameters of a face model in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>) as <ref type="bibr" target="#b7">(8)</ref> Inserting <ref type="bibr" target="#b7">(8)</ref> in the optical flow constraint equation ( <ref type="formula">7</ref>) yields <ref type="bibr" target="#b8">(9)</ref> Taking into consideration all points inside the object, (9) extends into a linear matrix equation <ref type="bibr" target="#b9">(10)</ref> Each row of the matrix consists of the vector in brackets in <ref type="bibr" target="#b8">(9)</ref> for a certain point and each component of the vector consists of the right part of ( <ref type="formula">9</ref>) at this point. In practice, we evaluate only points where the spatial intensity derivatives have nonzero values, avoiding aperture problems in unstructured areas. Moreover, we exclude some points along the image border and along the object border, if a segmentation mask is available. This makes the estimation more robust and, as a side effect, it reduces the computational complexity because the matrix dimensions are decreased. In general, it is possible to reduce complexity by excluding pixels, for instance, using a regular subsampling raster.</p><p>The least mean square error solution of ( <ref type="formula">10</ref>) for the parameter vector is given by <ref type="bibr" target="#b10">(11)</ref> The result is accumulated to the output motion parameters of the first stage, which is a linear operation, since the motion equation is linear. As emphasized above, this also includes another refinement of the translation parameters. Due to linearization errors, model failures, and noise, the first estimate from ( <ref type="formula">11</ref>) can be quite inaccurate in some cases. Therefore, the second stage is repeated iteratively. This includes the generation of a new predicted image, now using the new updated parameters, estimation, and accumulation of estimated parameters. The quality of an estimated set of motion parameters is measured in terms of the displaced frame difference between the actual and the predicted image, which is generated controlled by these parameters. This procedure stops when the displaced frame difference after an iteration is larger than after the previous iteration. Since the displaced frame difference is a bounded function, it is guaranteed that the algorithm will converge-however, the risk of being trapped in local minima exists. Mathematically, the goal of the first stage is to get close to the global minimum, which also reduces the computational complexity.</p><p>The speed of convergence highly depends on the content itself. Convergence can be reached after a single iteration if the global motion only consists of translation. For more complex motions including zoom and rotation, between two and eight iterations are required. In our implementation, we have restricted the maximum number of iterations to ten-alternatively processing is aborted, if the change of the displaced frame difference between two iterations is smaller than a threshold, which is well justified based on experimental data.</p><p>If the overall system employs an affine model, the hierarchical procedure is completed at this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stage 3: Optical Flow Algorithm for Parabolic Motion</head><p>If the overall system applies a parabolic model, the higher order part of the long-term motion parameters remains to be updated. In this case, only the linear part of the 12-parameter long-term motion vector in ( <ref type="formula">4</ref>) is known approximately at this stage-the higher order part of the motion model remained unchanged. Notice that the full motion vector has been employed in the generation of predicted images. Similar to the transition between the first and the second stage, only a subset of the complete mathematical problem has been optimized, with the goal to get close to the global minimum. The last stage is finally the overall optimization of the complete vector, which also includes the refinement of the affine part of the motion, but now also considering changing higher order parameters.</p><p>Similar to the transition between stage 1 and 2, the output of the second stage may be an ill-defined starting point for stage 3. This could result in inaccurate and unstable estimates if the final stage cannot correct this estimate. However, if the linear part of the motion cannot be estimated accurately, it is very unlikely that the direct estimation of the even more complex and sensitive complete nonlinear parameter set would succeed. The linear part of the motion model is dominant, and the higher order part is mainly a refinement providing a better modeling in detail.</p><p>The third stage of the algorithm is analogous to the second stage. The inputs are the actual image, the predicted image resulting from the second stage, and the output parameters of the second stage. We formulate expressions for the optical flow in analogy to <ref type="bibr" target="#b7">(8)</ref> and insert them into the optical flow constraint equation. The solution is calculated in analogy to the procedure described for the affine parameters, also including refinement iterations until convergence of the displaced frame difference.</p><p>However, the estimation of parabolic motion parameters is more complex compared to affine parameters. The set of equations is of a higher order, making the matrix calculation in <ref type="bibr" target="#b8">(9)</ref> and the solution in ( <ref type="formula">11</ref>) much more complex. The accumulation of the update parameters cannot be calculated in a straightforward way. Therefore, a numerical fitting algorithm has to be applied. Our hierarchical structure of the estimation process minimizes the computational complexity and yields robust and accurate estimates of higher order polynomial motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A NEW APPROACH TO SPRITE CODING</head><p>As outlined in Section II-A, the sprite has to be updated at every timestep with the new image content that appears at the image borders or in uncovered regions. For a coding application, an update has to be transmitted. For MPEG-4 static sprites <ref type="bibr" target="#b6">[7]</ref>, this can be done by first reconstructing the sprite and then transmitting the new image content. The coding of texture takes place in the sprite domain. For MPEG-4 dynamic sprites, a predicted image is generated and used for predictive coding. Next, the sprite is updated using the reconstructed image-here, the coding of texture is performed in the image domain.</p><p>In the following, we present an on-line coding scheme, where the texture coding is applied in the image domain. This is illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. First we compute a reliability mask from the segmentation mask by excluding some pixels along the image border and the object. The excluded areas belong to the "nonreliable" image region (NR), the other to the "reliable" image region (R). This ensures that only reliable image information will be used for the sprite update. In correspondence, "reliable," "nonreliable," and "undefined" As outlined in Fig. <ref type="figure" target="#fig_3">4</ref>, the actual image is reconstructed from the previous sprite controlled by the actual estimated motion parameters. This information is available at the decoder too, if the motion parameters are transmitted. Parts of the actual image that can be reconstructed from the R sprite region do not have to be coded at all. Also, NR image parts that can be reconstructed from NR sprite parts are not coded again. If R image parts can be reconstructed from the NR sprite region, these values are used for prediction. The difference will be small if the NR sprite provides already a good representation of the pixel. The remaining parts of the actual image that are not yet defined in the sprite have to be coded in INTRA mode. After decoding the actual image, the sprite is updated using the new coded information.</p><p>The result is a coding mask for the actual image containing three possible values-no coding, predictive coding, and INTRA coding-that can be fully generated from the previous sprite and the actual motion parameters. Also, the prediction values can be generated from this information. This means that the important side information can also be generated at the decoder if the motion parameters and the segmentation mask are transmitted. The same procedure is performed for the luminance and the chrominance components. A typical luminance coding mask is shown in Fig. <ref type="figure">5(b)</ref>. Typically, the coding masks contain some lines of pixels along image borders and uncovered areas. Coding using an MPEG-4 I-VOP approach would be very inefficient because of the complex shape compared to the content. Also, the region update technique using MPEG-4 static sprite coding would be inefficient when applied this way at every timestep, as required for on-line applications (note that in this case, the coding mask would be in the sprite domain and not in the image domain as shown here).</p><p>Since the coding mask can also be generated at the decoder, the pixel values to be coded can be simply collected in blocks and macroblocks by scanning the coding masks and selecting each value to be coded. In an efficient way, we first select all predictive coded luminance pixels, then the predictive chrominance pixels, the chrominance INTRA pixels, and finally the luminance INTRA pixels. The macroblocks are stored in an image, which is then efficiently coded using MPEG-4 I-VOP syntax. A typical VOP generated by this approach is shown in Fig. <ref type="figure">5(c</ref>), and Fig. <ref type="figure">5</ref>(a) depicts the frame to be coded. This technique is not efficient for the first frame, where all pixels have to be coded in INTRA mode. Therefore, we code the first frame using MPEG-4 I-VOP syntax.    The shape or the object in every frame is coded as an MPEG-4 shape-only object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motion Estimation with Segmentation Masks</head><p>The first results demonstrate the performance of long-term estimation compared to short-term estimation with accumulation and compare the suitability of affine and parabolic motion models. Fig. <ref type="figure">6</ref>(a) shows a sprite generated over 250 frames of the Stefan sequence (352 240, 30 Hz), which was generated by short-term motion estimation using an affine model as described in <ref type="bibr" target="#b9">[10]</ref> and the concatenation of frame-to-frame parameters to long-term parameters. This results in unnatural edges due to accumulation of local errors in the affine model. Fig. <ref type="figure">6</ref>(b) depicts the sprite generated by direct estimation of long-term parameters, also using an affine model. Here we observe fewer edge artifacts. The motion is captured much more time consistently and accurately over the long period. Fig. <ref type="figure">6(c</ref>) depicts the sprite that was generated by direct estimation of long-term parameters using a parabolic motion model. This sprite provides the best representation of the image content. It contains parabolic curvature and is much larger (same scaling) than the affine sprites. This is due to the nonlinear distortions of the image content. Fig. <ref type="figure" target="#fig_5">7</ref> illustrates the quality of reconstructed background objects that were generated from the sprites by warping, controlled by the estimated parameters. In Fig. <ref type="figure" target="#fig_5">7</ref>, the reconstruction area covers one of the edges of the sprite constructed from short-term parameters. Very unnatural artifacts are observed. Fig. <ref type="figure" target="#fig_5">7</ref>(b) does not contain such artifacts because of the recursive estimation of long-term parameters. The best reconstruction is achieved with parabolic long-term parameters [Fig. <ref type="figure" target="#fig_5">7(c)]</ref>.</p><p>Fig. <ref type="figure">8</ref> compares the prediction accuracy in terms of peak signal-to-noise ratio (PSNR) over 250 frames of sequence Stefan. The presented direct long-term estimation technique significantly outperforms short-term estimation with accumulation. A higher order polynomial motion model again provides clear benefit over the affine model. Similar results were observed with other test sequences.</p><p>Fig. <ref type="figure">9</ref> shows the estimated translation in -direction and the estimated scaling in both directions, using long-term estimation and an affine motion model. The estimation is smooth and time consistent. The translation corresponds to the camera pan and the scaling to the zoom. Notice that affine parameters do not exactly describe the camera motion, but they are strongly related if a stationary background object is observed. The estimated parameters provide a meaningful and accurate description of the object motion over a long period, which can be used also for applications like content classification and query. For example, the end of the sequence between frames 210 and 300 can be classified as rapid translation to the left side. Between frames 5 and 25, there is a zoom out at a certain speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Motion Estimation without Segmentation</head><p>The following results illustrate the robustness of the proposed long-term global motion estimation algorithm and possible applications for content segmentation. We modified the algorithm described in Section III slightly by excluding all points for the estimation, where the absolute difference between the actual and the predicted frame was bigger than 50. This can be regarded as a very simple implementation of a robust M-estimator <ref type="bibr" target="#b4">[5]</ref> that minimizes the influence of outliers in the estimation process. Pixels that do not participate in the global motion will probably result in a large frame difference and will therefore probably be excluded for the estimation. The threshold of 50 was found experimentally by a few trials.</p><p>Fig. <ref type="figure" target="#fig_0">10</ref> depicts a sprite that was generated for sequence Stefan over 300 frames using long-term estimation and an affine motion model but without using any segmentation mask at all. The sprite contains the foreground object from the first frame and artifacts that result from some stationary black pixels along the upper and right image border [see also Fig. <ref type="figure" target="#fig_2">3(a)</ref>]. Since there is no information about the location of the background object, everything is stored in the sprite. The original frame 191 is depicted in Fig. <ref type="figure" target="#fig_0">11(b</ref>). The reconstructed frame 190 is shown in Fig. <ref type="figure" target="#fig_0">11(a)</ref>. Notice that this image is not an output image of a coder. It is only used inside the motion estimation as reference. The frame difference between the two is shown in Fig. <ref type="figure" target="#fig_0">11(c</ref>). The global motion estimation is performed between the frames in Fig. <ref type="figure" target="#fig_0">11(a</ref>) and (b). Besides the observed artifacts, the global motion is estimated accurately, stably, and time consistently. In Fig. <ref type="figure" target="#fig_1">12</ref>, the estimated parameters for translation and scaling in -direction are compared for the experiments with and without segmentation masks. The accuracy without segmentation is still good enough for a classification of the image content.</p><p>Figs. 13 and 14 depict results for test sequence Bus (352 240, 60 Hz, 300 frames)-again without using an a priori known segmentation mask. The generated sprite after frame 300 is shown in Fig. <ref type="figure" target="#fig_2">13</ref>. Fig. <ref type="figure" target="#fig_3">14</ref> depicts output images compared to original images. The motion is estimated very accurately and time consistently over the whole sequence, even though no a priori information about the location of the object was available. Notice that the output images do not contain the foreground object anymore. This verifies that a fully automatic segmentation of a moving object in front of moving background can be achieved without any a priori information.</p><p>Fig. <ref type="figure" target="#fig_9">15</ref> depicts some of the estimated parameters for sequence Bus over time. The estimation is again time consistent and accurate. In the context of MPEG-7, this would be a possible motion description, and it might be classified for example as "left translation over the whole sequence, first fast, then slower, fast again, in the end stopping" and "zoom out over the whole sequence, stopping in the middle." Fig. <ref type="figure" target="#fig_0">16</ref> shows a sprite generated for the test sequence Horse (352 288, 25 Hz, 184 frames), which contains complex motion for both camera and foreground objects. Fig. <ref type="figure" target="#fig_5">17</ref> compares some predicted and original images. Again, the global motion is estimated accurately and time consistently over the whole sequence. The output images do not contain the foreground object. Although the visual quality of the predicted images is good, there is a big frame difference to the original images. The applied affine model can only be an approximation of the complex motion in this sequence.</p><p>The algorithm cannot a priori distinguish between foreground and background, and therefore both are stored in the sprite (see also Figs. <ref type="bibr">10, 13, and 16)</ref>. If image content complies with the model above-as many real-world sequences do-processing is performed automatically, thus providing a powerful tool to user-guided segmentation systems, such as described in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sprite Coding</head><p>It remains to be shown that the proposed long-term global motion estimation also provides sufficient accuracy for coding of image sequences. Our experiments demonstrate the performance of the proposed on-line sprite coding scheme compared to MPEG-4 arbitrary shape coding. To this purpose, we have applied sprite coding to the background objects of the test sequences Stefan (352 240, 30 Hz, 250 frames), Foreman (352 288, 25 Hz, 250 frames), and Mobile &amp; Calendar (360 240, 30 Hz, 300 frames). Global motion estimation was performed as described in Section III using a parabolic motion model. New image content was coded as outlined in Section IV. A quantizer value of was used, providing nearly lossless coding of the updates. The total bit rate was calculated by adding the bit rate for MPEG-4 I-VOP coding of the first frame object, the sum of bits for all texture updates, the sum of bits for the shape-only objects, and the bits needed for coding the motion parameters. We did not implement the coding of the motion parameters, but we assumed that a nearly lossless coding is possible with 20 bits/parameter, resulting in approximately 250 bits/frame. After calculating the total bit rate of the proposed coder, we coded the background objects using the MPEG-4 arbitrary shape coding approach. The bit rate was adjusted by selecting an appropriate fixed quantizer value.   a) The assigned motion model is a good approximation of the real motion. b) The foreground object is not too large. c) The object does not straddle the image borders. The drawbacks of the proposed method are that it will fail if the motion model does not comply to the real motion, the motion estimation fails, or there is significant change in the already coded sprite over time. However, we believe that it is possible to develop suitable techniques for detection and correction of these failures. The difficulty is to distinguish between failures that are visually not important and those that are important for the human observer, for instance, a real change of already coded sprite content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. SUMMARY AND CONCLUSION</head><p>We have presented a new technique for long-term global motion estimation that provides a continuous and timeconsistent description of motion over a long period. The motion is represented by one set of parameters according to a motion model for every time instant relatively to a fixed reference coordinate system. The proposed method is suitable for the estimation of linear affine motion parameters as well as for higher order motion models like the parabolic model. Sophisticated mathematical methods have been developed for the estimation of the parameters of nonlinear motion models.</p><p>The application of a closed-loop prediction-and-correction scheme inspired by Kalman filtering techniques overcomes the problems of error accumulation in long-term motion estimation. The combination of the advantages of feature matching and optical flow techniques, and the hierarchical strategy for the estimation-first translation, then affine motion, and finally higher order motion parameters-makes the algorithm robust and computationally efficient.</p><p>The developed parameter estimation technique was discussed in the context of three application domains. In the context of MPEG-7, the motion models and estimated longterm parameters represent a meaningful and flexible descriptor for object motion, readily applied for classification and query. The possibility of a robust automatic extraction with the algorithm proposed makes such a descriptor very attractive for MPEG-7 video.</p><p>Further, the results prove that the proposed estimator can be used as a powerful tool for segmenting moving foreground objects in front of a moving background. So far, we have only implemented a very simplistic algorithm for robust estimation. More sophisticated implementations and further investigation of the potential for segmentation will be subject to future research.</p><p>The motion estimation method can be readily applied for MPEG-4 sprite coding techniques, since it provides accurate and robust estimation of affine motion parameters. We have also shown that the efficiency of global motion estimation can be significantly increased if a higher order motion models is applied.</p><p>Last, we presented a new sprite coding scheme for on-line applications that significantly improves the subjective image quality compared to MPEG-4 arbitrary shape coding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of deformations possible with (a) affine and (b) parabolic motion models.</figDesc><graphic coords="2,354.66,149.94,153.84,72.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of the hierarchical long-term global motion estimation algorithm. The first three blocks include the generation of predicted images from the previous sprite controlled by the estimated parameters.</figDesc><graphic coords="3,121.44,55.02,357.36,209.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Content of long-term memory (sprite) for background of sequence Stefan at various time steps. New content is continuously updated from images.</figDesc><graphic coords="4,120.72,54.97,358.69,371.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the new on-line sprite coding approach. Reliability masks for previous sprite and actual image. White: outside object or not defined in sprite (U). Light gray: reliable regions (R). Dark gray: nonreliable regions (NR). Arrows indicate reconstruction by image warping, controlled by estimated parameters.</figDesc><graphic coords="6,324.06,54.97,215.02,249.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Fig. 5. Second frame of the Stefan sequence: (a) original image and (b) coding mask. White: outside the object; black: not coded; light gray: predictive coding; dark gray: INTRA coding. (c) Resulting coding VOP.</figDesc><graphic coords="7,144.12,211.60,311.89,94.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Reconstructed background objects of frame 195: (a) accumulated affine short-term parameters, (b) affine long-term parameters, and (c) parabolic long-term parameters.</figDesc><graphic coords="8,73.62,55.00,452.95,102.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. PSNR between original and reconstructed background objects.</figDesc><graphic coords="8,127.50,218.64,345.12,148.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .Fig. 12 .</head><label>101112</label><figDesc>Fig. 10. Sprite for sequence Stefan generated by direct estimation of long-term parameters using an affine motion model without segmentation masks.</figDesc><graphic coords="9,69.66,54.98,460.80,127.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .Fig. 14 .</head><label>1314</label><figDesc>Fig. 13. Sprite for sequence Bus generated by direct estimation of long-term parameters using an affine motion model without segmentation masks.</figDesc><graphic coords="10,81.96,55.00,436.25,141.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Estimated motion parameters for sequence Bus using long-term estimation and an affine motion model: (a) translation in x-direction a 1 and (b) scaling in x-and y-direction a 2 and b 3 :</figDesc><graphic coords="11,143.16,212.46,313.85,133.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 16 .Fig. 17 .</head><label>1617</label><figDesc>Fig. 16. Sprite for sequence Horse generated by direct estimation of long-term parameters using an affine motion model without segmentation masks.</figDesc><graphic coords="12,78.54,55.01,443.13,139.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 18 .</head><label>18</label><figDesc>Fig. 18. Coding results for background object sequence Stefan (352 2 240, 30 Hz, 250 frames), frame 100, 305 kbit/s. (a) Original. (b) MPEG-4 arbitrary shape. (c) Proposed sprite coder.</figDesc><graphic coords="13,303.78,447.76,255.60,178.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 19 .</head><label>19</label><figDesc>Fig. 19. Coding results for background object sequence Foreman (352 2 288, 25 Hz, 250 frames), frame 100, 121 kbit/s. (a) Original. (b) MPEG-4 arbitrary shape. (c) Proposed sprite coder.</figDesc><graphic coords="14,57.96,468.71,221.24,182.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Coding results for background object sequence Mobile &amp; Calendar (360 2 240, 30 Hz, 300 frames), frame 100, 428 kbit/s. (a) Original, (b) MPEG-4 arbitrary shape. (c) Proposed sprite coder.</figDesc><graphic coords="14,306.42,438.58,250.36,167.24" type="bitmap" /></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the German Ministry of Science and Technology, Federal Republic of Germany, under Grant BN 702. This paper was recommended by Guest Editor S.-F. Chang. The authors are with the Heinrich-Hertz-Institut fÃ¼r Nachrichtentechnik Berlin GmbH, Berlin 10587 Germany. Publisher Item Identifier S 1051-8215(99)09589-0.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>From 1985 to 1990, he was a Research and Teaching Assistant at the Institute for Telecommunications, TUB. From 1990 to 1995, he performed work within government-funded research projects on image coding at the same location. Since 1996, he has been with the Heinrich-Hertz-Institut in Berlin as a Project Manager, where he has been responsible for research projects on motion-compensated, stereoscopic, and 3-D image processing, and is presently mostly concerned with image/video coding issues in the context of development of the MPEG-4 and MPEG-7 standards. He teaches courses on digital image processing, coding, and transmission at TUB. He has chaired ad hoc groups on multifunctional coding and core experiments within the MPEG-4 video standardization group. He is the author of numerous papers and a German-language textbook on image coding.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ISO/IEC JTC1/SC29/WG11</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dufaux</surname></persName>
		</author>
		<idno>MPEG96/M0653</idno>
		<imprint>
			<date type="published" when="1996-01">Jan. 1996</date>
			<pubPlace>Munich, Germany</pubPlace>
		</imprint>
	</monogr>
	<note>Background mosaicking</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Functional coding of video using a shape-adaptive DCT algorithm and an object-based motion prediction toolbox</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kauff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Makai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rauthenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Goelz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L P</forename><surname>Delameillieure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="181" to="196" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visually controlled graphics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azarbayejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="602" to="605" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Teukolsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Vetterling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Flannary</surname></persName>
		</author>
		<title level="m">Numerical Recipes in C</title>
		<meeting><address><addrLine>Cambridge, U.K.</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge Univ. Press</publisher>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3-D motion estimation in model-based facial image coding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roivainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="545" to="555" />
			<date type="published" when="1993-06">June 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Committee Draft</title>
		<author>
			<persName><forename type="first">Group</forename><surname>Video</surname></persName>
			<affiliation>
				<orgName type="collaboration">N2202</orgName>
			</affiliation>
		</author>
		<idno>ISO/IEC JTC1/SC29/WG11</idno>
		<imprint>
			<date type="published" when="1998-03">Mar. 1998</date>
			<pubPlace>Tokyo, Japan</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The MPEG-4 video standard verification model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1997-02">Feb. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MPEG-7 Context and Objectives</title>
		<imprint>
			<date type="published" when="1998-07">July 1998</date>
			<pubPlace>Dublin, Ireland</pubPlace>
		</imprint>
	</monogr>
	<note>ISO/IEC JTC1/SC29/WG11, N2326</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object-based global motion estimation using a combined feature matching and optical flow approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>SmoliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLBV&apos;98</title>
		<meeting>VLBV&apos;98<address><addrLine>Urbana, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998-10">Oct. 1998</date>
			<biblScope unit="page" from="165" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adaptive Kalman filtering for prediction and global motion parameter tracking of segments in video</title>
		<author>
			<persName><forename type="first">A</forename><surname>SmoliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lorei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PCS&apos;96, Australia</title>
		<meeting>PCS&apos;96, Australia</meeting>
		<imprint>
			<date type="published" when="1996-03">Mar. 1996</date>
			<biblScope unit="page" from="363" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Real-time estimation of longterm 3-D motion parameters for SNHC face animation and model-based coding applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>SmoliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Makai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="255" to="263" />
			<date type="published" when="1999-03">Mar. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Coding of image sequences using a layered 2-D/3-D model-based coding approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>SmoliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PCS&apos;97</title>
		<meeting>PCS&apos;97<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-09">Sept. 1997</date>
			<biblScope unit="page" from="541" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Analysis and synthesis of facial image sequences in model-based image coding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="257" to="275" />
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-view facial movement estimation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Forchheimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="276" to="287" />
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3-D motion estimation and wireframe adaptation including photometric effects for model-based coding of facial image sequences</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bozdagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Onural</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="246" to="256" />
			<date type="published" when="1994-06">June 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image sequence analysis for emerging interactive multimedia services-The European COST 211 framework</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Onural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuncel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="802" to="813" />
			<date type="published" when="1998-11">Nov. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Request for CE on parametric 2-D motion descriptors in MPEG-7</title>
		<author>
			<persName><forename type="first">A</forename><surname>SmoliÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Tekalp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Preteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heuer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kaup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISO/IEC JTC1/SC</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="1999-07">July 1999</date>
			<pubPlace>Vancouver, B.C., Canada</pubPlace>
		</imprint>
	</monogr>
	<note>WG</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">MPEG-7 Visual part of eXperimentation Model Version 2.0</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jeannin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Group</forename><surname>Video</surname></persName>
		</author>
		<ptr target="ISO/IECJTC1/SC29/WG11,N2822" />
		<imprint>
			<date type="published" when="1999-07">July 1999</date>
			<pubPlace>Vancouver, B.C., Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">He currently is pursuing the Dr.-Ing. degree at Heinrich-Hertz-Institut (HHI) Berlin. In 1994, he joined the Image Processing Department of HHI, where he is currently a Member of Research Staff. His research interests are mainly in the field of object-based motion estimation, segmentation, object-based coding, and content description of video. He is involved in several European research projects in the context of MPEG-4 and MPEG-7</title>
	</analytic>
	<monogr>
		<title level="m">Aljoscha SmoliÄ‡ was born in Salzburg, Austria</title>
		<imprint>
			<date type="published" when="1969">1969. 1996</date>
		</imprint>
		<respStmt>
			<orgName>Technical University of Berlin, Germany</orgName>
		</respStmt>
	</monogr>
	<note>He received the Dipl. where he contributes to the development of algorithms for video processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
