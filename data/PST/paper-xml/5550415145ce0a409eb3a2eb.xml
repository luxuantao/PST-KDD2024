<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<email>mfritz@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">84C8C0EDAA27E607DE23CF7F620E011C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multiworld approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As vision techniques like segmentation and object recognition begin to mature, there has been an increasing interest in broadening the scope of research to full scene understanding. But what is meant by "understanding" of a scene and how do we measure the degree of "understanding"? Most often "understanding" refers to a correct labeling of pixels, regions or bounding boxes in terms of semantic annotations. All predictions made by such methods inevitably come with uncertainties attached due to limitations in features or data or even inherent ambiguity of the visual input.</p><p>Equally strong progress has been made on the language side, where methods have been proposed that can learn to answer questions solely from question-answer pairs <ref type="bibr" target="#b0">[1]</ref>. These methods operate on a set of facts given to the system, which is refered to as a world. Based on that knowledge the answer is inferred by marginalizing over multiple interpretations of the question. However, the correctness of the facts is a core assumption.</p><p>We like to unite those two research directions by addressing a question answering task based on realworld images. To combine the probabilistic output of state-of-the-art scene segmentation algorithms, we propose a Bayesian formulation that marginalizes over multiple possible worlds that correspond to different interpretations of the scene.</p><p>To date, we are lacking a substantial dataset that serves as a benchmark for question answering on real-world images. Such a test has high demands on "understanding" the visual input and tests a whole chain of perception, language understanding and deduction. This very much relates to the "AI-dream" of building a turing test for vision. While we are still not ready to test our vision system on completely unconstrained settings that were envisioned in early days of AI, we argue that a question-answering task on complex indoor scenes is a timely step in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions:</head><p>In this paper we combine automatic, semantic segmentations of real-world scenes with symbolic reasoning about questions in a Bayesian framework by proposing a multi-world approach for automatic question answering. We introduce a novel dataset of more than 12,000 question-answer pairs on RGBD images produced by humans, as a modern approach to a visual turing test. We benchmark our approach on this new challenge and show the advantages of our multi-world approach. Furthermore, we provide additional insights regarding the challenges that lie ahead of us by factoring out sources of error from different components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Semantic parsers: Our work is mainly inspired by <ref type="bibr" target="#b0">[1]</ref> that learns the semantic representation for the question answering task solely based on questions and answers in natural language. Although the architecture learns the mapping from weak supervision, it achieves comparable results to the semantic parsers that rely on manual annotations of logical forms ( <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>). In contrast to our work, <ref type="bibr" target="#b0">[1]</ref> has never used the semantic parser to connect the natural language to the perceived world. Language and perception: Previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> has proposed models for the language grounding problem with the goal of connecting the meaning of the natural language sentences to a perceived world. Both methods use images as the representation of the physical world, but concentrate rather on constrained domain with images consisting of very few objects. For instance <ref type="bibr" target="#b4">[5]</ref> considers only two mugs, monitor and table in their dataset, whereas <ref type="bibr" target="#b3">[4]</ref> examines objects such as blocks, plastic food, and building bricks. In contrast, our work focuses on a diverse collection of real-world indoor RGBD images <ref type="bibr" target="#b5">[6]</ref> -with many more objects in the scene and more complex spatial relationship between them. Moreover, our paper considers complex questions -beyond the scope of <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b4">[5]</ref> -and reasoning across different images using only textual question-answer pairs for training. This imposes additional challenges for the question-answering engines such as scalability of the semantic parser, good scene representation, dealing with uncertainty in the language and perception, efficient inference and spatial reasoning. Although others <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> propose interesting alternatives for learning the language binding, it is unclear if such approaches can be used to provide answers on questions. Integrated systems that execute commands: Others <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> focus on the task of learning the representation of natural language in the restricted setting of executing commands. In such scenario, the integrated systems execute commands given natural language input with the goal of using them in navigation. In our work, we aim for less restrictive scenario with the question-answering system in the mind. For instance, the user may ask our architecture about counting and colors ('How many green tables are in the image?'), negations ('Which images do not have tables?') and superlatives ('What is the largest object in the image?'). Probabilistic databases: Similarly to <ref type="bibr" target="#b13">[14]</ref> that reduces Named Entity Recognition problem into the inference problem from probabilistic database, we sample multiple-worlds based on the uncertainty introduced by the semantic segmentation algorithm that we apply to the visual input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our method answers on questions based on images by combining natural language input with output from visual scene analysis in a probabilistic framework as illustrated in Figure <ref type="figure" target="#fig_1">1</ref>. In the single world approach, we generate a single perceived world W based on segmentations -a unique interpretation of a visual scene. In contrast, our multi-world approach integrates over many latent worlds W, and hence taking different interpretations of the scene and question into account.</p><p>Single-world approach for question answering problem We build on recent progress on end-toend question answering systems that are solely trained on question-answer pairs (Q, A) <ref type="bibr" target="#b0">[1]</ref>. Top part of Figure <ref type="figure" target="#fig_1">1</ref> outlines how we build on <ref type="bibr" target="#b0">[1]</ref> by modeling the logical forms associated with a question as latent variable T given a single world W. More formally the task of predicting an answer A given a question Q and a world W is performed by computing the following posterior which marginalizes over the latent logical forms (semantic trees in <ref type="bibr" target="#b0">[1]</ref>) T :</p><formula xml:id="formula_0">P (A|Q, W) := T P (A|T , W)P (T |Q).<label>(1)</label></formula><p>P (A|T , W) corresponds to denotation of a logical form T on the world W. In this setting, the answer is unique given the logical form and the world:</p><formula xml:id="formula_1">P (A|T , W) = 1[A ∈ σ W (T )]</formula><p>with the evaluation function σ W , which evaluates a logical form on the world W. Following <ref type="bibr" target="#b0">[1]</ref> we use DCS Trees that yield the following recursive evaluation function σ W : σ W (T ) :=   </p><formula xml:id="formula_2">j {v : v ∈ σ W (p), t ∈ σ W (T j ), R j (v, t)} where T := p, (T 1 , R 1 ), (T 2 , R 2 ), ..., (T d , R d )</formula><p>is the semantic tree with a predicate p associated with the current node, its subtrees T 1 , T 2 , ..., T d , and relations R j that define the relationship between the current node and a subtree T j .</p><p>In the predictions, we use a log-linear distribution P (T |Q) ∝ exp(θ T φ(Q, T )) over the logical forms with a feature vector φ measuring compatibility between Q and T and parameters θ learnt from training data. Every component φ j is the number of times that a specific feature template occurs in (Q, T ). We use the same templates as <ref type="bibr" target="#b0">[1]</ref>: string triggers a predicate, string is under a relation, string is under a trace predicate, two predicates are linked via relation and a predicate has a child. The model learns by alternating between searching over a restricted space of valid trees and gradient descent updates of the model parameters θ. We use the Datalog inference engine to produce the answers from the latent logical forms. The linguistic phenomena such as superlatives and negations are handled by the logical forms and the inference engine. For a detailed exposition, we refer the reader to <ref type="bibr" target="#b0">[1]</ref>.</p><p>Question answering on real-world images based on a perceived world Similar to <ref type="bibr" target="#b4">[5]</ref>, we extend the work of <ref type="bibr" target="#b0">[1]</ref> to operate now on what we call perceived world W. This still corresponds to the single world approach in our overview Figure <ref type="figure" target="#fig_1">1</ref>. However our world is now populated with "facts" derived from automatic, semantic image segmentations S. For this purpose, we build the world by running a state-of-the-art semantic segmentation algorithm <ref type="bibr" target="#b14">[15]</ref> over the images and collect the recognized information about objects such as object class, 3D position, and   color <ref type="bibr" target="#b15">[16]</ref> (Figure <ref type="figure" target="#fig_1">1</ref> -middle part). Every object hypothesis is therefore represented as an n-tuple: predicate(instance id, image id, color, spatial loc) where predicate ∈ {bag, bed, books, ...}, instance id is the object's id, image id is id of the image containing the object, color is estimated color of the object <ref type="bibr" target="#b15">[16]</ref>, and spatial loc is the object's position in the image. Latter is represented as (X min , X max , X mean , Y min , Y max , Y mean , Z min , Z max , Z mean ) and defines minimal, maximal, and mean location of the object along X, Y, Z axes. To obtain the coordinates we fit axis parallel cuboids to the cropped 3d objects based on the semantic segmentation. Note that the X, Y, Z coordinate system is aligned with direction of gravity <ref type="bibr" target="#b14">[15]</ref>. As shown in Figure <ref type="figure" target="#fig_4">2b</ref>, this is a more meaningful representation of the object's coordinates over simple image coordinates. The complete schema will be documented together with the code release.</p><p>We realize that the skilled use of spatial relations is a complex task and grounding spatial relations is a research thread on its own (e.g. <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b18">[19]</ref>). For our purposes, we focus on predefined relations shown in Table <ref type="table" target="#tab_0">1</ref>, while the association of them as well as the object classes are still dealt within the question answering architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-worlds approach for combining uncertain visual perception and symbolic reasoning</head><p>Up to now we have considered the output of the semantic segmentation as "hard facts", and hence ignored uncertainty in the class labeling. Every such labeling of the segments corresponds to different interpretation of the scene -different perceived world. Drawing on ideas from probabilistic databases <ref type="bibr" target="#b13">[14]</ref>, we propose a multi-world approach (Figure <ref type="figure" target="#fig_1">1</ref> -lower part) that marginalizes over multiple possible worlds W -multiple interpretations of a visual scene -derived from the segmentation S. Therefore the posterior over the answer A given question Q and semantic segmentation S of the image marginalizes over the latent worlds W and logical forms T :</p><formula xml:id="formula_3">P (A | Q, S) = W T P (A | W, T )P (W | S) P (T | Q)<label>(2)</label></formula><p>The semantic segmentation of the image is a set of segments s i with the associated probabilities p ij over the C object categories c j . More precisely S = {(s 1 , L 1 ), (s 2 , L 2 ), ..., (s k , L k )} where</p><formula xml:id="formula_4">L i = {(c j , p ij )} C j=1 , P (s i = c j ) = p ij , and k is the number of segments of given image. Let Ŝf = (s 1 , c f (1) ), (s 2 , c f (2) ), ..., (s k , c f (k) )</formula><p>) be an assignment of the categories into segments of the image according to the binding function f ∈ F = {1, ..., C} {1,...,k} . With such notation, for a fixed binding function f , a world W is a set of tuples consistent with Ŝf , and define P (W |S) = i p (i,f (i)) . Hence we have as many possible worlds as binding functions, that is C k . Eq. 2 becomes quickly intractable for k and C seen in practice, wherefore we use a sampling strategy that draws a finite sample W = (W 1 , W 2 , ..., W N ) from P (•|S) under an assumption that for each segment s i every object's category c j is drawn independently according to p ij . A few sampled perceived worlds are shown in Figure <ref type="figure" target="#fig_4">2a</ref>.</p><p>Regarding the computational efficiency, computing T P (A | W i , T )P (T | Q) can be done independently for every W i , and therefore in parallel without any need for synchronization. Since for small N the computational costs of summing up computed probabilities is marginal, the overall cost is about the same as single inference modulo parallelism. The presented multi-world approach to question answering on real-world scenes is still an end-to-end architecture that is trained solely on the question-answer pairs.   Implementation and Scalability For worlds containing many facts and spatial relations the induction step becomes computationally demanding as it considers all pairs of the facts (we have about 4 million predicates in the worst case). Therefore we use a batch-based approximation in such situations. Every image induces a set of facts that we call a batch of facts. For every test image, we find k nearest neighbors in the space of training batches with a boolean variant of TF.IDF to measure similarity <ref type="bibr" target="#b19">[20]</ref>. This is equivalent to building a training world from k images with most similar content to the perceived world of the test image. We use k = 3 and 25 worlds in our experiments.</p><p>Dataset and the source code can be found in our website<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>4.1 DAtaset for QUestion Answering on Real-world images (DAQUAR) Images and Semantic Segmentation Our new dataset for question answering is built on top of the NYU-Depth V2 dataset <ref type="bibr" target="#b5">[6]</ref>. NYU-Depth V2 contains 1449 RGBD images together with annotated semantic segmentations (Figure <ref type="figure" target="#fig_5">3</ref>) where every pixel is labeled into some object class with a confidence score. Originally 894 classes are considered. According to <ref type="bibr" target="#b14">[15]</ref>, we preprocess the data to obtain canonical views of the scenes and use X, Y , Z coordinates from the depth sensor to define spatial placement of the objects in 3D. To investigate the impact of uncertainty in the visual analysis of the scenes, we also employ computer vision techniques for automatic semantic segmentation. We use a state-of-the-art scene analysis method <ref type="bibr" target="#b14">[15]</ref> which maps every pixel into 40 classes: 37 informative object classes as well as 'other structure', 'other furniture' and 'other prop'. We ignore the latter three. We use the same data split as <ref type="bibr" target="#b14">[15]</ref>: 795 training and 654 test images. To use our spatial representation on the image content, we fit 3d cuboids to the segmentations. New dataset of questions and answers In the spirit of a visual turing test, we collect question answer pairs from human annotators for the NYU dataset. In our work, we consider two types of the annotations: synthetic and human. The synthetic question-answer pairs are automatically generated question-answer pairs, which are based on the templates shown in Table <ref type="table" target="#tab_2">2</ref>. These templates are then instantiated with facts from the database. To collect 12468 human question-answer pairs we ask 5 in-house participants to provide questions and answers. They were instructed to give valid answers that are either basic colors <ref type="bibr" target="#b15">[16]</ref>, numbers or objects (894 categories) or sets of those. Besides the answers, we don't impose any constraints on the questions. We also don't correct the questions as we believe that the semantic parsers should be robust under the human errors. Finally, we use 6794 training and 5674 test question-answer pairs -about 9 pairs per image on average (8.63, 8.75) <ref type="foot" target="#foot_1">2</ref> .</p><p>The database exhibit some biases showing humans tend to focus on a few prominent objects. For instance we have more than 400 occurrences of table and chair in the answers. In average the object's category occurs <ref type="bibr">(14.25, 4)</ref> times in training set and (22.48, 5.75) times in total. Figure <ref type="figure" target="#fig_6">4</ref> shows example question-answer pairs together with the corresponding image that illustrate some of the challenges captured in this dataset.</p><p>Performance Measure While the quality of an answer that the system produces can be measured in terms of accuracy w.r.t. the ground truth (correct/wrong), we propose, inspired from the work on Fuzzy Sets <ref type="bibr" target="#b21">[22]</ref>, a soft measure based on the WUP score <ref type="bibr" target="#b22">[23]</ref>, which we call WUPS (WUP Set) score. As the number of classes grows, the semantic boundaries between them are becoming more fuzzy. For example, both concepts 'carton' and 'box' have similar meaning, or 'cup' and 'cup of coffee' are almost indifferent. Therefore we seek a metric that measures the quality of an answer and penalizes naive solutions where the architecture outputs too many or too few answers. Standard Accuracy is defined as:</p><formula xml:id="formula_5">1 N N i=1 1{A i = T i }</formula><p>• 100 where A i , T i are i-th answer and ground-truth respectively. Since both the answers may include more than one object, it is beneficial to represent them as sets of the objects T = {t 1 , t 2 , ...}. From this point of view we have for every i ∈ {1, 2, ..., N }:</p><formula xml:id="formula_6">1{A i = T i } = 1{A i ⊆ T i ∩ T i ⊆ A i } = min{1{A i ⊆ T i }, 1{T i ⊆ A i }} (3) = min{ a∈A i 1{a ∈ T i }, t∈T i 1{t ∈ A i }} ≈ min{ a∈A i µ(a ∈ T i ), t∈T i µ(t ∈ A i )}<label>(4)</label></formula><p>We use a soft equivalent of the intersection operator in Eq. 3, and a set membership measure µ,</p><formula xml:id="formula_7">with properties µ(x ∈ X) = 1 if x ∈ X, µ(x ∈ X) = max y∈X µ(x = y) and µ(x = y) ∈ [0, 1],</formula><p>in Eq. 4 with equality whenever µ = 1. For µ we use a variant of Wu-Palmer similarity <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. WUP(a, b) calculates similarity based on the depth of two words a and b in the taxonomy <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>, and define the WUPS score:</p><formula xml:id="formula_8">WUPS(A, T ) = 1 N N i=1 min{ a∈A i max t∈T i WUP(a, t), t∈T i max a∈A i WUP(a, t)} • 100<label>(5)</label></formula><p>Empirically, we have found that in our task a WUP score of around 0.9 is required for precise answers. Therefore we have implemented down-weighting WUP(a, b) by one order of magnitude (0.1 • WUP) whenever WUP(a, b) &lt; t for a threshold t. We plot a curve over thresholds t ranging from 0 to 1 5). Since "WUPS at 0" refers to the most 'forgivable' measure without any downweighting and "WUPS at 1.0" corresponds to plain accuracy. Figure <ref type="figure">5</ref> benchmarks architectures by requiring answers with precision ranging from low to high. Here we show some examples of the pure WUP score to give intuitions about the range: WUP(curtain, blinds) = 0.94, WUP(carton, box) = 0.94, WUP(stove, fire extinguisher) = 0.82.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative results</head><p>We perform a series of experiments to highlight particular challenges like uncertain segmentations, unknown true logical forms, some linguistic phenomena as well as show the advantages of our proposed multi-world approach. In particular, we distinguish between experiments on synthetic question-answer pairs (SynthQA) based on templates and those collected by annotators (Hu-manQA), automatic scene segmentation (AutoSeg) with a computer vision algorithm <ref type="bibr" target="#b14">[15]</ref> and human segmentations (HumanSeg) based on the ground-truth annotations in the NYU dataset as well as single world (single) and multi-world (multi) approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Synthetic question-answer pairs (SynthQA)</head><p>Based on human segmentations (HumanSeg, 37 classes) (1st and 2nd rows in Table <ref type="table" target="#tab_4">3</ref>) uses automatically generated questions (we use templates shown in Table <ref type="table" target="#tab_2">2</ref>) and human segmentations.</p><p>We have generated 20 training and 40 test question-answer pairs per template category, in total 140 training and 280 test pairs (as an exception negations type 1 and 2 have 10 training and 20 test examples each). This experiment shows how the architecture generalizes across similar type of questions provided that we have human annotation of the image segments. We have further removed negations of type 3 in the experiments as they have turned out to be particularly computationally demanding. Performance increases hereby from 56% to 59.9% with about 80% training Accuracy. Since some incorrect derivations give correct answers, the semantic parser learns wrong associations. Other difficulties stem from the limited training data and unseen object categories during training.</p><p>Based on automatic segmentations (AutoSeg, 37 classes, single) (3rd row in Table <ref type="table" target="#tab_4">3</ref>) tests the architecture based on uncertain facts obtained from automatic semantic segmentation <ref type="bibr" target="#b14">[15]</ref> where the most likely object labels are used to create a single world. Here, we are experiencing a severe drop in performance from 59.9% to 11.25% by switching from human to automatic segmentation. Note that there are only 37 classes available to us. This result suggests that the vision part is a serious bottleneck of the whole architecture.</p><p>Based on automatic segmentations using multi-world approach (AutoSeg, 37 classes, multi) (4th row in Table <ref type="table" target="#tab_4">3</ref>) shows the benefits of using our multiple worlds approach to predict the answer. Here we recover part of the lost performance by an explicit treatment of the uncertainty in the segmentations. Performance increases from 11.25% to 13.75%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.3</head><p>Human question-answer pairs (HumanQA) Based on human segmentations 894 classes (HumanSeg, 894 classes) (1st row in Table <ref type="table" target="#tab_5">4</ref>) switching to human generated question-answer pairs. The increase in complexity is twofold. First, the human annotations exhibit more variations the synthetic approach based on templates. Second, the questions are typically longer and include more spatially related objects. Figure <ref type="figure" target="#fig_6">4</ref> shows a few samples from our dataset that highlights challenges including complex and nested spatial reference and use of reference frames. We yield an accuracy of 7.86% in this scenario. As argued above, we also evaluate the experiments on the human data under the softer WUPS scores given different thresholds (Table <ref type="table" target="#tab_5">4</ref> and Figure <ref type="figure">5</ref>). In order to put these numbers in perspective, we also show performance numbers for two simple methods: predicting the most popular answer yields 4.4% Accuracy, and our untrained architecture gives 0.18% and 1.3% Accuracy and WUPS (at 0.9).</p><p>Based on human segmentations 37 classes (HumanSeg, 37 classes) (2nd row in Table <ref type="table" target="#tab_5">4</ref>) uses human segmentation and question-answer pairs. Since only 37 classes are supported by our automatic segmentation algorithm, we run on a subset of the whole dataset. We choose the 25 test images yielding a total of 286 question answer pairs for the following experiments. This yields 12.47% and 15.89% Accuracy and WUPS at 0.9 respectively. Based on automatic segmentations (AutoSeg, 37 classes) (3rd row in Table <ref type="table" target="#tab_5">4</ref>) Switching from the human segmentations to the automatic yields again a drop from 12.47% to 9.69% in Accuracy and we observe a similar trend for the whole spectrum of the WUPS scores.</p><p>Based on automatic segmentations using multi-world approach (AutoSeg, 37 classes, multi) (4th row in Table <ref type="table" target="#tab_5">4</ref>) Similar to the synthetic experiments our proposed multi-world approach yields an improvement across all the measure that we investigate. Human baseline (5th and 6th rows in Table <ref type="table" target="#tab_5">4</ref> for 894 and 37 classes) shows human predictions on our dataset. We ask independent annotators to provide answers on the questions we have collected.</p><p>They are instructed to answer with a number, basic colors <ref type="bibr" target="#b15">[16]</ref>, or objects (from 37 or 894 categories) or set of those. This performance gives a practical upper bound for the question-answering algorithms with an accuracy of 60.27% for the 37 class case and 50.20% for the 894 class case.</p><p>We also ask to compare the answers of the AutoSeg single world approach with HumanSeg single world and AutoSeg multi-worlds methods. We use a two-sided binomial test to check if difference in preferences is statistically significant. As a result AutoSeg single world is the least preferred method with the p-value below 0.01 in both cases. Hence the human preferences are aligned with our accuracy measures in Table <ref type="table" target="#tab_5">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results</head><p>We choose examples in Fig. <ref type="figure" target="#fig_8">6</ref> to illustrate different failure cases -including last example where all methods fail. Since our multi-world approach generates different sets of facts about the perceived worlds, we observe a trend towards a better representation of high level concepts like 'counting' (leftmost the figure) as well as language associations. A substantial part of incorrect answers is attributed to missing segments, e.g. no pillow detection in third example in Fig. <ref type="figure" target="#fig_8">6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary</head><p>We propose a system and a dataset for question answering about real-world scenes that is reminiscent of a visual turing test. Despite the complexity in uncertain visual perception, language understanding and program induction, our results indicate promising progress in this direction. We bring ideas together from automatic scene analysis, semantic parsing with symbolic reasoning, and combine them under a multi-world approach. As we have mature techniques in machine learning, computer vision, natural language processing and deduction at our disposal, it seems timely to bring these disciplines together on this open challenge.     </p><formula xml:id="formula_9">Threshold WUPS • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • • •</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>brown, image 1, X,Y,Z) chair (1,brown, image 4, X,Y,Z) chair (2,brown, image 4, X,Y,Z) table (1,brown, image 1,X,Y,Z) wall (1,white, image 1, X,Y,Z) bed (1, white, image 2 X,Y,Z) chair (1,brown, image 5, X,Y,Z)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our approach to question answering with multiple latent worlds in contrast to single world approach.</figDesc><graphic coords="3,324.91,216.35,79.83,79.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Fig.2ashows a few sampled worlds where only segments of the class 'person' are shown. In the clock-wise order: original picture, most confident world, and three possible worlds (gray-scale values denote the class confidence). Although, at first glance the most confident world seems to be a reasonable approach, our experiments show opposite -we can benefit from imperfect but multiple worlds. Fig.2bshows object's coordinates (original and Z, Y , X images in the clock-wise order), which better represent the spatial location of the objects than the image coordinates.</figDesc><graphic coords="3,244.76,552.44,69.75,55.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: NYU-Depth V2 dataset: image, Z axis, ground truth and predicted semantic segmentations.</figDesc><graphic coords="5,235.07,136.04,70.53,54.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>QA: (whatFigure 4 :</head><label>4</label><figDesc>Figure 4: Examples of human generated question-answer pairs illustrating the associated challenges. In the descriptions we use following notation: 'A' -answer, 'Q' -question, 'QA' -question-answer pair. Last two examples (bottom-right column) are from the extended dataset not used in our experiments.</figDesc><graphic coords="8,117.83,213.60,74.24,56.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Q:</head><label></label><figDesc>What is on the right side of the table?! H: chair M: window, floor, wall! C: floor Q: How many red chairs are there?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Questions and predicted answers. Notation: 'Q' -question, 'H' -architecture based on human segmentation, 'M' -architecture with multiple worlds, 'C' -most confident architecture, '()' -no answer. Red color denotes correct answer.</figDesc><graphic coords="8,305.85,561.15,98.33,74.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Predicates defining spatial relations between A and B. Auxiliary relations define actual spatial relations. The Y axis points downwards, functions Xmax, Xmin, ... take appropriate values from the tuple predicate, and is a 'small' amount. Symmetrical relations such as rightOf , below, behind, etc. can readily be defined in terms of other relations (i.e. below(A, B) = above(B, A)).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>How many {color} {object} are in {image id}? How many gray cabinets are in image1? room type Which type of the room is depicted in {image id}? Which type of the room is depicted in image1? Individual superlatives What is the largest {object} in {image id}? What is the largest object in image1? counting and colors How many {color} {object}? How many black bags?</figDesc><table><row><cell>Description</cell><cell>Template</cell><cell>Example</cell></row><row><cell>counting counting and colors</cell><cell>How many {object} are in {image id}?</cell><cell>How many cabinets are in image1?</cell></row></table><note><p>negations type 1 Which images do not have {object}? Which images do not have sofa? set negations type 2 Which images are not {room type}? Which images are not bedroom? negations type 3 Which images have {object} but do not have a {object}? Which images have desk but do not have a lamp?</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Synthetic question-answer pairs. The questions can be about individual images or the sets of images.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Accuracy results for the experiments with synthetic question-answer pairs.Human question-answer pairs (HumanQA) Segmentation World(s) #classes Accuracy WUPS at 0.9 WUPS at 0</figDesc><table><row><cell>HumanSeg</cell><cell>Single</cell><cell>894</cell><cell>7.86%</cell><cell>11.86%</cell><cell>38.79%</cell></row><row><cell>HumanSeg</cell><cell>Single</cell><cell>37</cell><cell>12.47%</cell><cell>16.49%</cell><cell>50.28%</cell></row><row><cell>AutoSeg</cell><cell>Single</cell><cell>37</cell><cell>9.69%</cell><cell>14.73%</cell><cell>48.57%</cell></row><row><cell>AutoSeg</cell><cell>Multi</cell><cell>37</cell><cell>12.73%</cell><cell>18.10%</cell><cell>51.47%</cell></row><row><cell cols="2">Human Baseline</cell><cell>894</cell><cell>50.20%</cell><cell>50.82%</cell><cell>67.27%</cell></row><row><cell cols="2">Human Baseline</cell><cell>37</cell><cell>60.27%</cell><cell>61.04%</cell><cell>78.96%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Accuracy and WUPS scores for the experiments with human question-answer pairs. We show WUPS scores at two opposite sides of the WUPS spectrum.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.d2.mpi-inf.mpg.de/visual-turing-challenge</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Our notation (x, y) denotes mean x and trimean y. We use Tukey's trimean 1 4 (Q1 + 2Q2 + Q3), where Qj denotes the j-th quartile<ref type="bibr" target="#b20">[21]</ref>. This measure combines the benefits of both median (robustness to the extremes) and empirical mean (attention to the hinge values).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Inducing probabilistic ccg grammars from logical form with higher-order unification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<editor>EMNLP.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online learning of relaxed ccg grammars for parsing to logical form</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-CoNLL-2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A joint model of language and perception for grounded attribute learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Jointly learning to parse and perceive: Connecting natural language to the physical world</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What are you talking about? text-toimage coreference</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep fragment embeddings for bidirectional image sentence mapping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to parse natural language commands to a robot control system</title>
		<author>
			<persName><forename type="first">C</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Robotics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretation of spatial language in a map navigation task</title>
		<author>
			<persName><forename type="first">M</forename><surname>Levit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Situated dialogue and spatial organization: What, where</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J M</forename><surname>Kruijff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jensfelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJARS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scalable probabilistic databases with factor graphs and mcmc</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
		<editor>VLDB.</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning color names from real-world images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grounding spatial language in perception: an empirical and computational investigation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Regier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Carlson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: General</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image retrieval with structured object queries using latent ranking svm</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Grounding spatial relations for human-robot interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Riano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gouhring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<editor>IROS.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Cambridge university press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Tukey</surname></persName>
		</author>
		<title level="m">Exploratory data analysis</title>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fuzzy sets</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<editor>ACL.</editor>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Krishnamoorthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Malkarnenkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<editor>ICCV.</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>CACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>WordNet. Wiley Online Library</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
