<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Debiasing Neighbor Aggregation for Graph Neural Network in Recommender Systems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minseok</forename><surname>Kim</surname></persName>
							<email>kminseok@amazon.com</email>
						</author>
						<author>
							<persName><forename type="first">Jinoh</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jaeyoung</forename><surname>Do</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sungjin</forename><surname>Lee</surname></persName>
							<email>sungjinl@amazon.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Amazon Alexa AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Amazon Alexa AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Amazon Alexa AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Amazon Alexa AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">CIKM &apos;22</orgName>
								<address>
									<addrLine>October 17-22</addrLine>
									<postCode>2022</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Debiasing Neighbor Aggregation for Graph Neural Network in Recommender Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3511808.3557576</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural networks (GNNs) have achieved remarkable success in recommender systems by representing users and items based on their historical interactions. However, little attention was paid to GNN's vulnerability to exposure bias: users are exposed to a limited number of items so that a system only learns a biased view of user preference to result in suboptimal recommendation quality. Although inverse propensity weighting is known to recognize and alleviate exposure bias, it usually works on the final objective with the model outputs, whereas GNN can also be biased during neighbor aggregation. In this paper, we propose a simple but effective approach, neighbor aggregation via inverse propensity (Navip) for GNNs. Specifically, given a user-item bipartite graph, we first derive propensity score of each user-item interaction in the graph. Then, inverse of the propensity score with Laplacian normalization is applied to debias neighbor aggregation from exposure bias. We validate the effectiveness of our approach through our extensive experiments on two public and Amazon Alexa datasets where the performance enhances up to 14.2%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>However, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(a), real-world user-item graphs are often biased because edges are user responses to the system behavior decided by a policy. Specifically, a system can expose only some part of all items to users so that how user-item interactions occur becomes biased by the system's policy. Since GNNs represent a node by aggregating neighbors connected via edges, GNNs induce a biased view of user preference, which may not be aligned with true user interest. As such, recommender systems based on GNNs leave room for improvement through debiasing the neighbor aggregation process with respect to the exposure bias.</p><p>A common way to mitigate exposure bias includes inverse propensity scoring (IPS) that emphasizes less-exposed items during training <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>. Specifically, IPS multiplies the inverse propensity of user-item interactions in the objective function, which accordingly upweights the learning on less-exposed items. However, since IPS deals with the bias problem after GNNs encode users and items into embeddings, the system is still susceptible to the bias caused by the unadjusted process of neighbor aggregation.</p><p>In this paper, we propose a simple yet effective approach, neighbor aggregation via inverse propensity (Navip) for GNNs. Specifically, given a user-item bipartite graph, we first derive the propensity score of each user-item interaction in the graph. We then use the inverse propensity score with Laplacian normalization as edge weight for the neighbor aggregation process. As such, less popular neighbors are relatively emphasized in an embedding to balance out the biased local structure of each target node as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b). We validate the effectiveness of Navip with a comprehensive experiment on three real-world datasets including a large-scale voice assistant system, Amazon Alexa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY AND RELATED WORK</head><p>In online services, user-item interactions data {(?, ?, ? ?,? )} ? D ? 0 is collected according to policy ? 0 , which is a prior on user behaviors. Recommender systems perform personalized recommendation by finding similar users and items based on the collected data D ? 0 , which is known as collaborative filtering <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16]</ref>. Specifically, given a recommender model ? (?) and its prediction for user ?'s relevance on item ? as ? (? ? , ? ? ) based on their embeddings, ? ? and ? ? , we minimize empirical risk ?(? 0 ) on implicit feedback ? ?,? ,</p><formula xml:id="formula_0">?(? 0 ) = ?? (?,?,? ?,? ) ? D ? 0 L ? (? ? , ? ? ), ? ?,? ,<label>(1)</label></formula><p>where L is a loss function such as BPR <ref type="bibr" target="#b14">[15]</ref>, and ? ?,? is 1 if the interaction between user ? and item ? is observed or 0 if not. A prevalent approach for collaborative filtering follows two steps: (i) transforms each user and item into a latent representation, ? ? and ? ? , then (ii) exploits the representations to estimate the relevance ? (? ? , ? ? ) based on dot product <ref type="bibr" target="#b14">[15]</ref> or non-linear neural networks (DNN) <ref type="bibr" target="#b5">[6]</ref> based on ? ? and ? ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">GNN for Recommender System</head><p>GNN is the state-of-the-art approach for learning representation over graphs <ref type="bibr" target="#b4">[5]</ref>. The biggest difference between GNN and conventional CF methods is that GNN explicitly leverages neighbors of a target node during inference of the target node's representation. Specifically, given a bipartite graph of users and items as nodes, the embedding ? of a user ? at layer ? is computed as follows,</p><formula xml:id="formula_1">? ? ? = TRANSFORM ? ?-1 ? , ? ? ? (<label>2</label></formula><formula xml:id="formula_2">)</formula><formula xml:id="formula_3">? ? ? = AGGREGATE ? ?-1 ? , ? ? N (?) ,<label>(3)</label></formula><p>where ? ? ? denotes the aggregated embedding of ?'s neighbors N (?) at layer ?. Note that we can augment the embedding of item ? in the same way. The design of AGGREGATE(?) and TRANSFORM(?), or leveraging high-order connectivity are used to distinguish GNN methods. Initial GNN methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref> used only one-hop neighbors using one GNN layer, while following studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref> exploit multiple layers with non-linear DNN to benefit from high-order connectivity. Recently, LightGCN <ref type="bibr" target="#b4">[5]</ref> claimed that such non-linear DNN is not essential for GNN, and showed a state-of-the-art performance by employing symmetric Laplacian norm in AGGREGATE(?) and identity function for TRANSFORM(?),</p><formula xml:id="formula_4">? ? ? = ? ? ? = ?? ? ?N (?) ? ?-1 ? ?? |N (?)||N (?)| .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inverse Propensity Scoring</head><p>In a service, users are exposed to only a part of items selected by a system policy ? 0 . Thus, collected log data D ? 0 becomes dependent toward the policy ? 0 , which is called exposure bias. Naturally, a recommender ? trained on D ? 0 becomes biased and even intensifies the bias <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. To this end, IPS is popularly exploited to acquire unbiased estimator ?(?) of the performance on the uniform item exposure policy ? by reweighting D ? 0 with propensity <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>,</p><formula xml:id="formula_5">R(?; D ? 0 ) = 1 |D ? 0 | ?? (?,?,? ?,? ) ? D ? 0 L ? (? ? , ? ? ), ? ?,? ? (? ?,? = 1|? 0 ) ,<label>(5)</label></formula><p>where ? (? ?,? = 1|? 0 ) is propensity defined in Definition 2.1.</p><p>Definition 2.1. Propensity ? (? ?,? = 1|? 0 ) is the observation probability of user-item interaction ? ?,? under policy ? 0 .</p><p>However, IPS has a few drawbacks, which are complemented by several studies. For example, SNIPS <ref type="bibr" target="#b19">[20]</ref> was suggested to address high variance of IPS at the expense of a small estimation bias <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, UEBPR <ref type="bibr" target="#b2">[3]</ref> improves IPS-based recommender learning by reflecting recommendation explainability during training. However, we note that the above-described IPS studies work in the objective function after GNN neighbor aggregation, therefore the system is sill biased even with IPS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>Previous methods have mainly focused on applying IPS on loss as shown in Eq. ( <ref type="formula" target="#formula_5">5</ref>), but gave less attention to how the neighbors are aggregated even though it is one of the key factors that contribute to the success of GNN. Because collected log data D ? 0 has exposure bias (i.e., D ? 0 ? D ? ), GNNs also employ neighbor aggregation to biased interactions {? = (?, ?, ? ?,? ) | ? ? D ? 0 ? ? ? D ? }, which can bias user embedding ? ? ? . Even worse, the bias can propagate to users without biased interactions due to the repeated GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Navip for Debiasing GNN</head><p>Inspired by the motivation, we suggest Navip that applies IPS weighting scheme to neighbor aggregation to reduce the impact of biased interactions and debias GNN neighbor aggregation. That is, Navip can be considered as upweighting interactions with low propensity and downweighting interactions with high propensity during aggregation. The rationale behind Navip is that neighbors with lower propensity interaction reveal more about the true interest of a given target user because such interactions are not likely to occur by a biased system policy. On the other hand, because interactions with high propensity often occur as a response to a biased system policy, they may not be the true user interest.</p><p>For simplicity, we tentatively assume a simple neighbor aggregation, ? ? ? = E[? ?-1 ? ], which is the expected embedding of neighbors at layer ? -1. For clarity, we further define ? ? ? (?) to be the aggregated neighbor embedding of user ? under the system policy ?, and ? ? ? (? |D ? ) to be the Monte-Carlo approximated aggregated neighbor embedding of ? over D ? . Formally speaking,</p><formula xml:id="formula_6">? ? ? (?) ? 1 |N ? (?)| ?? ? ?N ? (?) ? ?-1 ? (?) = ? ? ? (? |D ? )<label>(6)</label></formula><p>where ? ?-1 ? (?) is a node embedding at ? -1 layer under policy ?. In practice, ? ? ? (?) needs to be approximated based on D ? 0 because D ? is not available while developing ?. Navip estimates ? ? ? (?) with respect to D ? by combining Eq. ( <ref type="formula" target="#formula_5">5</ref>) and Eq. ( <ref type="formula" target="#formula_6">6</ref>),</p><formula xml:id="formula_7">? ? ? (?) ? 1 |N ? 0 (?)| ?? ? ?N ? 0 (?) ? ?-1 ? (?) ? (? ?,? = 1|? 0 ) = ? ? ? (? |D ? 0 ).<label>(7)</label></formula><p>However, we note that using inverse propensity results in numerical instability issue due to unnormalized magnitude of neighbor weights. To avoid the issue, Navip further normalizes neighbor aggregation by replacing 1</p><formula xml:id="formula_8">|N ? 0 (?) | , 1 ? ? (?) ?? ? ?N ? 0 (?) ? ?-1 ? (?) ? (? ?,? = 1|? 0 ) = ? ? ? (? |D ? 0 ),<label>(8)</label></formula><p>where ? ? 0 (?) is a normalizing term defined by ? ?N ? 0 (?)</p><p>1 ? (? ?,? =1|? 0 ) . We replace Eq. (3) with Eq. ( <ref type="formula" target="#formula_8">8</ref>) to debias neighbor aggregation. </p><formula xml:id="formula_9">? ?,? = 1 ? (? ?,? =1 |? 0 ) if ? ?,? = 1 0 else, ? = 0 ? ? ? 0. (9)</formula><p>Then, the weighted random walk Laplacian can be shown as</p><formula xml:id="formula_10">? ? (? 0 |D ? ) = ? -1 ?? ? (? 0 ),<label>(10)</label></formula><p>where ? ? and ? ? are the matrix of aggregated neighbor embedding and node embedding at layer ?, ? is the adjacency matrix, and ? is a diagonal degree matrix whose ?-th diagonal elements is the sum of ?-th row of ?, ? ?? = ? ? ?? .</p><p>It is worth noting that ? -1 ? can be interpreted as random walk Laplacian, ? ? ? , on a weighted graph, whose weights are inverse propensity weights. This simple difference from default aggregation encourages neighbors with low propensity edge become dominant in ? ? ? , even with more GNN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Training</head><p>The simplicity of Navip brings two benefits during training: (i) no computational overhead due to Navip during training because Navip debiases GNN neighbor aggregation without any additional training parameter, and (ii) no objective adjustment because Navip modifies neighbor aggregation before final model output ? (? ? , ? ? ) so as to be independent of learning objectives as in Eq.( <ref type="formula" target="#formula_0">1</ref>) or Eq.( <ref type="formula" target="#formula_5">5</ref>).</p><p>In this regard, Navip can follow given recommender's training scheme while debiasing the recommender. In addition, it is worth noting that Navip can achieve the debiasing effect for the pretrained model without additional training (see Section 4.2 for more details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>We conducted experiments to answer the following questions:</p><p>? Does Navip improve overall recommender performance?</p><p>? How does Navip impact recommender's exposure bias?</p><p>? Does Navip have overlapping effect with IPS?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>4.1.1 Datasets. We conducted experiments on three real-world datasets: Alexa , Yahoo 1 , and Movielens 2 .</p><p>? Alexa : it contains a truncated user interaction logs from a voice assistant service, Amazon Alexa. The interactions are collected for a week in August 2021. Redundant user-item interactions such as users' daily weather checking were filtered out to align setting with other two datasets. ? Yahoo: it contains users' five-star song ratings from Yahoo! Music.</p><p>Different from other two datasets, Yahoo contains unbiased test data which is obtained by asking 5, 400 users to rate 10 randomly selected songs. To make the ratings as implicit feedback, we transformed all the ratings into ? ?,? = 1 and unobserved entries as ? ?,? = 0 to indicate whether a user ? interacted with an item ?. ? Movielens: it contains users' five-star ratings for movies. Like Yahoo, all the ratings were binarized. We filtered out users and items that contain less than 10 interactions following usual practice <ref type="bibr" target="#b5">[6]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the profile of the three datasets after preprocessing.</p><p>We created pseudo-unbiased testing and validation data for Alexa and Movielens dataset. Because real-world datasets are biased, test data created by random splitting is not effective to evaluate the debiasing effect. Having a dedicated unbiased test dataset generated from pure random exposure is ideal, but this is practically infeasible in many cases. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, we generated pseudo-unbiased test data by sampling 5% of total data based on the inverse of relative item popularity. In case of Yahoo, we used the original test data, but for validation set, we followed the same process to generate a unbiased validation data.</p><p>4.1.2 Implementation detail. We used LightGCN <ref type="bibr" target="#b4">[5]</ref> <ref type="foot" target="#foot_0">3</ref> , which is the state-of-the-art GNN-based recommender system, as a backbone GNN throughout the experiment. We investigated three neighbor aggregations to validate the effect of neighbor aggregation.</p><p>? Mean: a basic neighbor aggregation strategy that averages embeddings of neighbors. ? Propensity: a bias-oriented neighbor aggregation strategy with propensity ? (? ?,? = 1|? 0 ) as edge weight. ? Navip: a debiasing neighbor aggregation strategy that uses IPS as edge weight for each neighbor as in Eq.( <ref type="formula" target="#formula_8">8</ref>). In addition, to validate whether the effect of Navip overlaps with that of IPS methods, we conducted experiment with the state-ofthe-art IPS method for ranking loss, UEBPR <ref type="bibr" target="#b2">[3]</ref>.</p><p>Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>, we estimate the propensity of an item to a user by relative item popularity,</p><formula xml:id="formula_11">? (? ?,? = 1|? 0 ) = ?? ? ?? ? ?,? max ? ?? ? ?? ? ?,? ,<label>(11)</label></formula><p>where ? and ? are the set of users and items, respectively. All the experiments were trained for 100 epochs with batch size 256. The size of input feature and embedding are 64. We used Adam <ref type="bibr" target="#b10">[11]</ref> with a learning rate ? = 0.003. Our implementation was written using Pytorch and tested on Nvidia Tesla V100. We conducted experiment 10 times and reported their average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Evaluation Metrics.</head><p>We used two widely-recognized metrics, hit rate (HR) and normalized discounted cumulative gain (NDCG). Given a model's recommendation list, HR measures the proportion of true user-item interactions in a recommended list, and NDCG additionally reflects the ranking of interactions. Two metrics were calculated with top-? recommendation: a model chooses ? most probable items that each user is expected to like and suggest them to the user. We varied the number of recommended items ? ? {5, 10, 20}. Because it is time-consuming to rank all items, a recommender evaluated the rank of the user's 1 interacted item among randomly sampled 99 non-interacted items following literature <ref type="bibr" target="#b6">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>Table <ref type="table" target="#tab_1">2</ref> shows the performance of pretrained LightGCN model with varying neighbor aggregation during inference to evaluate the effect of neighbor aggregation. The performance difference among neighbor aggregation methods verifies that neighbor aggregation significantly affects the quality of embedding so as to impact on the success of recommendation. Overall, Navip shows the best performance among three neighbor aggregations in most cases. Specifically, Navip outperforms Mean and Propensity by up to 14.2% and 126% in terms of ? ???@5 in Alexa dataset, respectively. Such performance improvement is attributed to the debiasing effect of Navip on neighbor aggregation, which successfully downweights neighbors with biased interaction so as to receive relatively more information from relevant neighbors. This claim can be further supported by the performance of Propensity which always recorded the worst performance among three neighbor aggregations. That is, Propensity highlights neighbors with biased interactions, which may not represent the true interest of users.</p><p>We also investigate the impact of neighbor aggregation when they are used in both training and inference (Table <ref type="table" target="#tab_2">3</ref>). The performance gap among neighbor aggregation strategies decreased so that all three aggregations showed performance similar to Mean. We conjecture that training learnable LightGCN input features under the same learning objective can weaken the effect of different neighbor aggregation. Nevertheless, the order of recommendation performance among neighbor aggregation strategies maintains; Navip scored the best performance in general, while Propensity consistently performs the worst.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Method Head Items Tail Items HR@5 HR@10 HR@20 HR@5 HR@10 HR@20  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact on Recommender's Exposure Bias</head><p>Table <ref type="table" target="#tab_3">4</ref> shows the performance of LightGCN on head items with top 10% popularity and tail items with bottom 10% popularity. The result shows that the performance on head items is higher than on tail items for all datasets, because recommenders are known to expose popular items more to users <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Navip alleviates such issues by emphasizing user interest in tail items, which leads to more exposure to tail items. That is, Navip encourages LightGCN to aggregate more information from neighbors with a low propensity, which correspond to tail items, so that LightGCN can reveal user interest in tail items. Although Navip may lead to a performance drop on head items as in Yahoo and Movielens dataset, it is negligible because Navip improves overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relationship with IPS</head><p>Table <ref type="table" target="#tab_4">5</ref> shows the performance of LightGCN trained with IPS method UEBPR during training. While Navip with IPS showed the best performance in Alexa and Yahoo datasets for both types of Laplacian norm, Navip without IPS scored the best in Movielens dataset. We point out that such inconsistent results in Movielens arose from the high variance of IPS that may lead training to be far from the ideal training objective ?(?) <ref type="bibr" target="#b19">[20]</ref>. Nevertheless, we can conclude that Navip and IPS can be synergetic in debiasing GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we proposed Navip to debias in exposure bias during neighbor aggregation of GNN. Navip acquires inverse propensity score for each user-item interaction and exploits its Laplacian norm as neighbor weight. Experiments conducted on three real-world datasets confirmed that Navip can successfully debias GNN neighbor aggregation and enhance performance up to 14.2%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison of (a) biased default GNN and (b) debiased GNN via Navip given biased user-item graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 . 1</head><label>11</label><figDesc>Connection to Laplacian. The neighbor aggregation function of Navip can be formulated in a matrix form. Specifically,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of three real-world datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Users Items Interactions</cell></row><row><cell>Alexa</cell><cell>40k</cell><cell>2.5k</cell><cell>0.7m</cell></row><row><cell>Yahoo</cell><cell>15k</cell><cell>1.0k</cell><cell>0.3m</cell></row><row><cell>Movielens</cell><cell>943</cell><cell>1.6k</cell><cell>0.1m</cell></row></table><note><p>1 http://webscope.sandbox.yahoo.com/ 2 http://grouplens.org/datasets/movielens/</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Recommendation performance of pretrained Light-GCN varying neighbor aggregation method during testing.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>@5</cell><cell>HR @10</cell><cell>@20</cell><cell>@5</cell><cell>NDCG @10</cell><cell>@20</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.266 0.364 0.498 0.190 0.222 0.256</cell></row><row><cell>Alexa</cell><cell cols="7">Propensity 0.153 0.255 0.410 0.096 0.129 0.167</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.287 0.379 0.509 0.217 0.247 0.279</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.145 0.242 0.397 0.093 0.124 0.163</cell></row><row><cell>Yahoo</cell><cell cols="7">Propensity 0.134 0.224 0.376 0.086 0.115 0.153</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.148 0.249 0.408 0.095 0.127 0.167</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.350 0.506 0.681 0.233 0.283 0.327</cell></row><row><cell>Movielens</cell><cell cols="7">Propensity 0.343 0.493 0.668 0.230 0.278 0.322</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.349 0.511 0.691 0.230 0.283 0.328</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>@5</cell><cell>HR @10</cell><cell>@20</cell><cell>@5</cell><cell>NDCG @10</cell><cell>@20</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.266 0.364 0.498 0.190 0.222 0.256</cell></row><row><cell>Alexa</cell><cell cols="7">Propensity 0.260 0.357 0.489 0.187 0.218 0.251</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.268 0.368 0.504 0.192 0.224 0.258</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.145 0.242 0.397 0.093 0.124 0.163</cell></row><row><cell>Yahoo</cell><cell cols="7">Propensity 0.145 0.241 0.392 0.093 0.124 0.162</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.147 0.246 0.397 0.095 0.126 0.164</cell></row><row><cell></cell><cell>Mean</cell><cell cols="6">0.350 0.506 0.681 0.233 0.283 0.327</cell></row><row><cell>Movielens</cell><cell cols="7">Propensity 0.347 0.503 0.678 0.233 0.283 0.327</cell></row><row><cell></cell><cell>Navip</cell><cell cols="6">0.352 0.512 0.687 0.234 0.286 0.330</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Recommendation performance of each neighbor aggregation method during training and testing.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on head (top 10% popularity) and tail (bottom 10% popularity) items with respect to two neighbor aggregations, Mean and Navip.</figDesc><table><row><cell>Alexa</cell><cell>Mean Navip</cell><cell>0.268 0.271</cell><cell>0.369 0.373</cell><cell>0.505 0.510</cell><cell>0.264 0.265</cell><cell>0.361 0.363</cell><cell>0.494 0.499</cell></row><row><cell>Yahoo</cell><cell>Mean Navip</cell><cell>0.497 0.483</cell><cell>0.638 0.631</cell><cell>0.795 0.781</cell><cell>0.033 0.038</cell><cell>0.071 0.076</cell><cell>0.141 0.159</cell></row><row><cell>Movielens</cell><cell>Mean Navip</cell><cell>0.649 0.647</cell><cell>0.812 0.811</cell><cell>0.933 0.931</cell><cell>0.077 0.080</cell><cell>0.142 0.149</cell><cell>0.236 0.250</cell></row><row><cell>Dataset</cell><cell>Method</cell><cell>@5</cell><cell>HR @10</cell><cell>@20</cell><cell>@5</cell><cell>NDCG @10</cell><cell>@20</cell></row><row><cell>Alexa</cell><cell cols="7">Mean Navip 0.301 0.403 0.538 0.220 0.253 0.287 0.299 0.400 0.533 0.218 0.251 0.284</cell></row><row><cell>Yahoo</cell><cell cols="7">Mean Navip 0.153 0.252 0.411 0.098 0.130 0.170 0.152 0.250 0.408 0.098 0.130 0.169</cell></row><row><cell>Movielens</cell><cell>Mean Navip</cell><cell cols="6">0.350 0.512 0.699 0.230 0.282 0.330 0.349 0.512 0.693 0.229 0.282 0.328</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Recommendation performance with UEBPR on two neighbor aggregation methods, Mean and Navip. The results that exceeds the performance in Table3are marked in bold.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://github.com/gusye1234/LightGCN-PyTorch</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Mansoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13286</idno>
		<title level="m">The unfairness of popularity bias in recommendation</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph convolutional matrix completion</title>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD Workshop on Deep Learning Day</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Debiased Explainable Pairwise Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Khalil Damak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olfa</forename><surname>Khenissi</surname></persName>
		</author>
		<author>
			<persName><surname>Nasraoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="321" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Offline evaluation to make decisions about playlist recommendation algorithms</title>
		<author>
			<persName><forename type="first">Alois</forename><surname>Gruson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Charbuillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Mcinerney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samantha</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Tardieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<idno>ICWSM. 420-428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TheWebConf</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast matrix factorization for online recommendation with implicit feedback</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weighted average importance sampling and defensive mixture distributions</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Hesterberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="185" to="194" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbiased learning-to-rank with biased feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fism: factored item similarity models for top-n recommender systems</title>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Kabbur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="659" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Grouplens: Applying collaborative filtering to usenet news</title>
		<author>
			<persName><forename type="first">Bradley</forename><forename type="middle">N</forename><surname>Joseph A Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">L</forename><surname>Maltz</surname></persName>
		</author>
		<author>
			<persName><surname>Herlocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lee R Gordon</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Factorization meets the neighborhood: a multifaceted collaborative filtering model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feedback loop and bias amplification in recommender systems</title>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Mansoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himan</forename><surname>Abdollahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bamshad</forename><surname>Mobasher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Burke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2145" to="2148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">UAI</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grouplens: An open architecture for collaborative filtering of netnews</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Resnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neophytos</forename><surname>Iacovou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><surname>Suchak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bergstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSCW</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unbiased Pairwise Learning from Implicit Feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2019 Workshop on Causal Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unbiased recommender learning from missing-not-at-random implicit feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suguru</forename><surname>Yaginuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayato</forename><surname>Sakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuhide</forename><surname>Nakata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In WSDM. 501-509</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The self-normalized estimator for counterfactual learning</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to rank with selection bias in personal search</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Unbiased offline recommender evaluation for missing-not-atrandom implicit feedback</title>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="279" to="287" />
		</imprint>
	</monogr>
	<note>Serge Belongie, and Deborah Estrin</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Causal Intervention for Leveraging Popularity Bias in Recommendation. SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
