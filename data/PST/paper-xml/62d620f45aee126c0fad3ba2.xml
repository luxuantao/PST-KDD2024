<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Greykite: Deploying Flexible Forecasting at Scale at LinkedIn</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-07-15">15 Jul 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Reza</forename><surname>Hosseini</surname></persName>
							<email>rhosseini@linkedin.com</email>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Chen</surname></persName>
							<email>abchen@linkedin.com</email>
						</author>
						<author>
							<persName><forename type="first">Kaixu</forename><surname>Yang</surname></persName>
							<email>kayang@linkedin.com</email>
						</author>
						<author>
							<persName><forename type="first">Sayan</forename><surname>Patra</surname></persName>
							<email>sapatra@linkedin.com</email>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saad</forename><forename type="middle">Eddin</forename><surname>Al Orjany</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sishi</forename><surname>Tang</surname></persName>
							<email>sistang@linkedin.com</email>
						</author>
						<author>
							<persName><forename type="first">Parvez</forename><surname>Ahammad</surname></persName>
							<email>pahammad@linkedin.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">LinkedIn Corporation Sunnyvale</orgName>
								<address>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<postCode>14-18, 2022</postCode>
									<settlement>August, Washington</settlement>
									<region>DC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Greykite: Deploying Flexible Forecasting at Scale at LinkedIn</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-07-15">15 Jul 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3534678.3539165</idno>
					<idno type="arXiv">arXiv:2207.07788v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>forecasting</term>
					<term>time series</term>
					<term>scalability</term>
					<term>interpretable machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Forecasts help businesses allocate resources and achieve objectives. At LinkedIn, product owners use forecasts to set business targets, track outlook, and monitor health. Engineers use forecasts to efficiently provision hardware. Developing a forecasting solution to meet these needs requires accurate and interpretable forecasts on diverse time series with sub-hourly to quarterly frequencies. We present Greykite, an open-source Python library for forecasting that has been deployed on over twenty use cases at LinkedIn. Its flagship algorithm, Silverkite, provides interpretable, fast, and highly flexible univariate forecasts that capture effects such as time-varying growth and seasonality, autocorrelation, holidays, and regressors. The library enables self-serve accuracy and trust by facilitating data exploration, model configuration, execution, and interpretation. Our benchmark results show excellent out-ofthe-box speed and accuracy on datasets from a variety of domains. Over the past two years, Greykite forecasts have been trusted by Finance, Engineering, and Product teams for resource planning and allocation, target setting and progress tracking, anomaly detection and root cause analysis. We expect Greykite to be useful to forecast practitioners with similar applications who need accurate, interpretable forecasts that capture complex dynamics common to time series related to human activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Software and its engineering ? Software libraries and repositories; ? Applied computing ? Forecasting; ? Mathematics of computing ? Time series analysis; ? Computing methodologies ? Machine learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Time series forecasts aim to provide accurate future expectations for metrics and other quantities that are measurable over time. At LinkedIn, forecasts are used for performance management and resource management. Performance management is the process of setting business metric targets and tracking progress to ensure we achieve them. This is the heartbeat of how LinkedIn manages its business <ref type="bibr" target="#b29">[30]</ref>. Resource management involves optimizing allocation of budget, hardware, and other resources, often based on forecasts of business and infrastructure metrics.</p><p>Before this work, the processes for resource and performance management at LinkedIn were highly manual, relying on rulebased spreadsheets, simple linear regressions, or expert opinions. Domain knowledge and expert judgment, while often accurate, are subjective, hard to validate over time, and very hard to scale. Furthermore, they have trouble adapting to ecosystem shocks (such as COVID-19) that change underlying dynamics and invalidate existing approaches to estimate metric growth. We have developed a forecasting library and framework that bring accuracy, scale, and consistency to the process. Furthermore, algorithmic forecasts can be automated and therefore integrated into systems that derive additional insights for decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MOTIVATION AND RELATED WORK</head><p>LinkedIn has diverse forecast requirements, with data frequencies from sub-hourly to quarterly (fiscal quarter) and forecast horizons from 15 minutes to over a year. Many of our time series exhibit strong growth and seasonality patterns that change over time due to product launches, ecosystem dynamics, or long-term drifts. Seasonality here refers to periodic/cyclic patterns over multiple horizons, e.g. daily seasonality refers to a cyclic pattern over the course of a day. For example, a new recommendation system may change the delivery schedule of mobile app notifications, affecting the seasonality pattern of user engagement; external shocks like COVID-19 can have an effect on hiring, job-seeking activity, and advertiser demand; long-term changes may shift engagement from desktop to mobile and affect traffic to our services. New features are constantly impacting the metrics, as LinkedIn runs hundreds of concurrent A/B tests each day.</p><p>A suitable forecast algorithm must account for time series characteristics such as: (a) strong seasonality, with periods from daily to yearly, (b) growth and seasonality changes, (c) high variability around holidays, month/quarter boundaries, (d) local anomalies from external events and engineering bugs, (e) floating holidays (those with irregular intervals), (f) dependency on external factors such as macroeconomic indicators. It must also provide reliable prediction intervals to quantify uncertainty for decision making and anomaly detection. We expect such requirements to be common across many industries, and certainly within the technology sector, where time series patterns depend on human activity (of users) and product launches, and where forecasts are needed for both long-term planning and short-term monitoring.</p><p>Our goal is to deliver a self-serve forecasting solution to data scientists and engineers across the company, many of whom have no specialized forecasting expertise. Developers need a way to develop accurate forecasts with little effort. Stakeholders need to trust the forecasts, which requires answering questions such as: Does the forecast include impact from the latest product launch? How quickly does the forecast adapt to new patterns? What is the expected impact of the upcoming holiday? Why has the forecast changed? Can the forecast account for macroeconomic effects? What would happen if economic recovery is fast/slow? To answer these questions, the model must be interpretable and easily tuned according to expert input when available.</p><p>The area of forecasting time series has a long history, with many models and techniques developed in the past decades. Some important examples include: classical time series models such as ARIMA (e.g. <ref type="bibr" target="#b16">[17]</ref>) and GARCH <ref type="bibr" target="#b28">[29]</ref>; exponential smoothing methods <ref type="bibr" target="#b31">[32]</ref>; state-space models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>; generalized linear model extensions to time series <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>; deep learning models such as RNN <ref type="bibr" target="#b21">[22]</ref>. There are several popular open-source forecasting packages. Notable examples include fable <ref type="bibr" target="#b20">[21]</ref>, statsmodels <ref type="bibr" target="#b22">[23]</ref>, Prophet <ref type="bibr" target="#b25">[26]</ref>, and GluonTS <ref type="bibr" target="#b0">[1]</ref>. The list of packages has grown quickly in recent years due to high demand for reliable forecasting in many domains. Each package has a different focus within the forecasting landscape.</p><p>fable and statsmodels implement statistical models such as ARIMA and ETS, which can be trained and scored quickly. The ARIMA model captures temporal dependence and non-stationarity, and has extensions to include seasonality and regressors. However, it is not flexible enough to capture effects such as seasonality that changes in magnitude and shape over time, or volatility that increases around certain periods such as holidays or weekends. Nor is it readily interpretable for business stakeholders. ETS, while more interpretable than ARIMA, has similar problems capturing interaction effects and does not support regressors.</p><p>Prophet is a popular forecasting library that includes the Prophet model, designed to capture trend, seasonality, trend changepoints, and holidays. The package allows visualization of these effects for interpretability. However, model training and inference are slower due to its Bayesian formulation. And while the interface is user-friendly, with only a few tuning parameters, the model is not flexible enough to achieve high accuracy on complex time series. For example, it does not perform as well for short-term forecasts due to lack of native support for autoregression or other mechanisms to capture short-term dynamics. Prophet supports custom regressors, but these must be provided by the user for both training and inference; this is inconvenient for standard time features and cumbersome for complex interaction terms.</p><p>GluonTS is a time series modeling library that includes the DeepAR algorithm <ref type="bibr" target="#b21">[22]</ref>. DeepAR is a deep learning model that trains a single global model on multiple time series to forecast. While this can be powerful for automated forecasting, the crosstime series dependencies between input and output and lack of intuitive parameters make it hard to interpret the model or apply expert knowledge to improve the forecast.</p><p>To support LinkedIn's forecasting needs, we developed Greykite, a Python library for self-serve forecasting. <ref type="foot" target="#foot_0">1</ref> Its flagship algorithm, Silverkite, provides univariate forecasts that capture diverse time series characteristics with speed and interpretabilty. Our contributions include: (1) flexible design to capture complex time series dynamics for any frequency and forecast horizon, <ref type="bibr" target="#b1">(2)</ref> forecast components that can be explicitly tuned, (3) interpretable output to explain forecast drivers, (4) fast training and inference, (5) decoupled volatility model that allows time-varying prediction intervals, <ref type="bibr" target="#b5">(6)</ref> flexible objective function to predict peak as well as mean, <ref type="bibr" target="#b6">(7)</ref> self-serve forecasting library that facilitates data exploration, model configuration, tuning, and diagnostics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we explain the design of Silverkite, the core forecasting algorithm in the Greykite library. The Silverkite algorithm architecture is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The user provides the input time series and any known anomalies, events, regressors, or changepoint dates. The model returns forecasts, prediction intervals, and diagnostics. The computation steps of the algorithm are decomposed into two phases:</p><p>? Phase (1): the conditional mean model.</p><p>? Phase (2): the volatility/uncertainty model.</p><p>In <ref type="bibr" target="#b0">(1)</ref>, a model predicts the metric of interest, and in (2), a volatility model is fit to the residuals. This design helps us with flexibility and speed, because integrated models are often more susceptible to poor tractability (convergence issues for parameter estimates) or divergence in the predicted values. Phase (1) can be broken down into these steps:</p><p>(1.a) Extract raw features from timestamps, events data, and history of the series (e.g. hour, day of week). (1.b) Transform the features to appropriate basis functions (e.g. Fourier series terms for various time scales). (1.c) Apply a changepoint detection algorithm to the data to discover changes in the trend and seasonality over time. (1.d) Apply an appropriate machine learning algorithm to fit the covariates from (1.b) and (1.c) (depending on the objective).</p><p>Step (1.b) transforms the features into a space which can be used in additive models for better interpretability. For Step (1.d), we recommend regularization-based models such as ridge regression for mean prediction and (regularized) quantile regression for quantile prediction (e.g. for peak demand). In Phase (2), a conditional variance model is fit to the residuals, which allows the volatility to be a function of specified categories, e.g. day of week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Model Formulation</head><p>In this subsection, we give a brief overview of the mathematical formulation of the Silverkite model. We will demonstrate how this works on a real example in Section 4. Suppose {? (?)}, ? = 0, 1, . . . is a real-valued time series where ? denotes time. Following <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b12">[13]</ref>, we denote the available information up to time ? by F (?). For example F (?) can include covariates such as ? (? -1), ? (? -2), ? (?), ? (? -1) where ? (? -?) denotes lags of ? ; ? (?) is the value of a regressor known at time ? and ? (? -1) is the value of the same regressor at time ? -1. The latter is often referred to as lagged regressor.</p><p>The conditional mean model. The conditional mean model is</p><formula xml:id="formula_0">E[? (?)|F (?)] ? ? (?) + ? (?) + ? (?) + ?(?) + ?(?) + ? (?),<label>(1)</label></formula><p>where ?, ?, ?, ?, ?, ? are functions of covariates in F (?). They are linear combinations of these covariates or their interactions. ? (?) is the general growth term that may include trend changepoints ? 1 , . . . , ? ? , as</p><formula xml:id="formula_1">? (?) = ? 0 ? (?) + ? ?? ?=1 ? ? 1 {? &gt;? ? } (? (?) -? (? ? )),</formula><p>where ? (?) is any growth function, e.g. ? (?) = ? or ? (?) = ? ? and ? ? 's are parameters to be estimated. Note that ? (?) is a continuous and piecewise smooth function of ?. ? (?) = ? ? P ? ? (?) includes all Fourier series bases for different seasonality components (yearly seasonality, weekly seasonality, etc.), where P is the set of all seasonal periods. A single seasonality component ? ? (?) can be written as</p><formula xml:id="formula_2">? ? (?) = ? ?? ?=1 [? ? sin(2??? (?)) + ? ? cos(2??? (?))],</formula><p>where ? ? , ? ? are Fourier series coefficients to be estimated by the model and ? is the series order. ? (?) ? [0, 1] is the corresponding time ? within a seasonal period. For example, daily seasonality has ? (?) equal to the time of day (in hours divided by 24) at time ?. Moreover, for a list of time points ? 1 , . . . , ? ? where the seasonality is expected to change in either shape or magnitude or both, Silverkite models these changes as</p><formula xml:id="formula_3">? ?? (?) = ? ? (?; {? ? , ? ? }) + ? ?? ?=1 1 {? &gt;? ? } ? ? (?; {? ?? , ? ?? }),</formula><p>where ? ?? (?) is a single seasonality component and ? ? is the seasonality term with coefficients {? ?? , ? ?? }. Similar to trend changepoints, this formulation allows the Fourier series' coefficients to change and adapt to the most recent seasonal patterns. ? ? (?) can also be modeled with categorical variables such as hour of day.</p><p>? (?) includes indicators on holidays/events and their neighboring days. For example, 1 {? ?Christmas day} models Christmas effect. Holidays can have extended impact over several days in their proximity. Silverkite allows the user to customize the number of days before and after the event where the impact is non-negligible, and models them with separate indicators/effects. Indicators on month/quarter/year boundaries also belong to this category.</p><p>?(?) includes any time series information known up to time ? to model the remaining time dependence. For example, it can be lagged observations ? (? -1), . . . , ? (? -? ) for some order ? , or the aggregation of lagged observations such as ?? ? (? (?); 1, 2, 3) = ? 3 ?=1 ? (? -?)/3. Aggregation allows for parsimonious models that capture long-range time dependencies.</p><p>?(?) includes other time series that have the same frequency as ? (?) that can meaningfully be used to predict ? (?). These time series are regressors, denoted ? (?) = ? 1 (?), . . . , ? ? (?) , ? = 0, 1, . . . in the case of ? regressors. If available forecasts X (?) of ? (?) are available, let ?(?) = X (?). Otherwise, lagged regressors or aggregations of lagged regressors can be used, such as ?(?) = ? (??) where ? is the forecast horizon. If the minimum lag order is at least the forecast horizon, the inputs needed for forecasting have already been observed. Otherwise, in autoregression, having lag orders smaller than forecast horizon means that forecasted values must be used. To handle this, we incrementally simulate the future values needed for calculating the later forecasted values.</p><p>? (?) includes any interaction of the above terms to model complex patterns. For example, we can model different daily seasonality patterns during weekend and weekdays by including 1 {? ?weekend} ? daily seasonality Fourier series terms, where ? denotes interaction between two components.</p><p>To mitigate the risk of the model becoming degenerate or overfitting, regularization can be used in the machine learning fitting algorithm for the conditional model. In fact, regularization also helps in minimizing the risk of divergence of the simulated future series for the model <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>Automatic changepoint detection. The trend and seasonality changepoints help Silverkite stay flexible and adapt to the most recent growth and seasonal patterns. Silverkite offers fast automatic changepoint detection algorithms to reduce manual modeling efforts. Automatic changepoint detection is described below.</p><p>For trend changepoints, we first aggregate the time series into a coarser time series to eliminate short-term fluctuations (which are captured by other features, such as holidays). For example, daily data can be aggregated into weekly data. Next, a large number of potential trend changepoints are placed evenly across time, except for a time window at the end of the time series to prevent extrapolation from limited data. We model the aggregated time series as a function of trend, while controlling for yearly seasonality:</p><formula xml:id="formula_4">? ? (?) ? ? (?) + ? ? (?),</formula><p>where ? ? (?) is the aggregated time series, ? (?) is the growth with all potential trend changepoints and ? ? (?) is yearly seasonality. The adaptive lasso penalty is used to identify significant trend changepoints <ref type="bibr" target="#b32">[33]</ref> (lasso would over-shrink significant changepoints' coefficients to reach the desired sparsity level <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>). Finally, identified changepoints that are too close to each other are merged to improved model stability, and the detected changepoints are used to construct the piecewise growth basis function. See <ref type="bibr" target="#b14">[15]</ref> for implementation details.</p><p>For seasonality changepoints, we use a similar formulation:</p><formula xml:id="formula_5">? ? (?) ? ?? ? ? P ? ?? (?),</formula><p>where ? ? (?) is the de-trended time series and ? ?? (?) is defined in Section 3.1 , with ? 1 , . . . , ? ? being potential seasonality changepoints. Automatic selection is also done with adaptive lasso. The formulation allows seasonality estimates to change in both pattern and magnitude. This approach can capture similar effects as multiplicative seasonality, but is far more flexible in terms of the pattern changes, and avoids the problems of multiplicative seasonality magnitude growing too large with longer forecast horizons.</p><p>The volatility model. The volatility model is fit separately from the conditional mean model, with the following benefits compared to an integrated model that estimates mean and volatility jointly:</p><p>(a) Stable parameter estimates and forecasts (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28]</ref>).</p><p>(b) Speed gain by avoiding computationally heavy Maximum Likelihood Estimation (e.g. <ref type="bibr" target="#b13">[14]</ref>) or Monte Carlo methods (e.g. <ref type="bibr" target="#b30">[31]</ref>). These factors are important in a production environment that requires speed, accuracy, reliability, and code maintainability. It is typical to see larger volatility around certain periods such as holidays and month/quarter boundaries. Silverkite's default volatility model uses conditional prediction intervals to adapt to such effects, allowing volatility to depend on categorical features. The model is described below.</p><p>Let ? (?) be the target series and ? (?) be the forecasted series. Define the residual series as ? (?) = ? (?) -? (?). Assume the volatility depends on given categorical features ? 1 , . . . , ? ? which are also known into the future, for example, day of week. Consider the empirical distribution (?|? 1 , . . . , ? ? ) and fit a parametric or nonparametric distribution to the combination as long as the sample size for that combination, denoted by ?(? 1 , . . . , ? ? ) is sufficiently large e.g. ?(? 1 , . . . , ? ? ) &gt; ? , ? = 20. Note that one can find an appropriate ? using data (for example, during cross-validation by checking the distribution of the residuals). Then from this distribution, we estimate the quantiles ? (? 1 , . . . , ? ? ) to form the prediction interval with level 1 -?:</p><formula xml:id="formula_6">( ? (?) + ? (? 1 , . . . , ? ? )(?/2), ? (?) + ? (? 1 , . . . , ? ? )(1 -?/2)).</formula><p>One choice for a parametric distribution is the Gaussian distribution N (0, ? 2 (? 1 , . . . , ? ? )). While the residuals of a naive model can be far from normal, it is possible that after conditioning on the appropriate features, the residuals of a sufficiently complex mean model are approximately normal as observed in <ref type="bibr" target="#b13">[14]</ref>. This assumption can be checked by inspecting the qq-plots of the conditional errors.</p><p>Silverkite offers an option to use empirical quantiles to construct the prediction intervals when this assumption is violated. Silverkite's flexibility allows other volatility models to be added. For example, a regression-based volatility model could be used to condition on many features, including continuous ones. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">How Silverkite Meets the Requirements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODELING FRAMEWORK</head><p>We implemented these algorithms in Greykite, a Python library for data scientists and engineers across the company. Figure <ref type="figure" target="#fig_2">2</ref> shows how the library facilitates each step of the modeling process. The first step is data exploration. Greykite provides interactive, exploratory plots to assess seasonality, trend, and holiday effects. These plots help users to identify effects that are hard to see when plotting the entire time series. We illustrate this with the D.C. bikesharing time series <ref type="bibr" target="#b1">[2]</ref> in Figure <ref type="figure" target="#fig_3">3</ref>. The top plot shows the daily seasonality pattern across the week. The shape changes between weekends and weekdays, revealing an interaction between daily seasonality and is_weekend. The bottom plot shows the (mean centered) yearly seasonality and an overlay for each year. By comparing how the lines change over time, we can see that seasonality magnitude increased until 2017, then decreased. The automatic changepoint detection module can be used on its own to explore trend and seasonality. Figure <ref type="figure" target="#fig_4">4</ref> shows a changepoint detection result, revealing both trend and seasonality changepoints in the time series.</p><p>The second step is configuration. The user provides time series data and a forecast config to the Forecaster class, which produces the forecast, diagnostics, and the trained model object. The config allows the user to select an algorithm, forecast horizon, coverage, model tuning parameters, and evaluation parameters. The config is optional to make quick start easy, but also very customizable.</p><p>Greykite exposes different algorithms, including Silverkite, Prophet, and SARIMA, as scikit-learn estimators that can be configured from the forecast config. Silverkite provides intuitive tuning parameters such as: autoregressive lags, changepoint regularization strength, and the list of holiday countries. Many Silverkite tuning parameters, including changepoints, seasonality, autoregression, and interaction terms have an intelligent "auto" setting. Others have reasonable defaults, such as ridge regression for the machine learning model.</p><p>For high-level tuning, we introduce the concept of model templates. Model templates define forecast parameters for various data characteristics and forecast requirements (e.g. hourly short-term forecast, daily long-term forecast). Model templates drastically reduce the search space to find a satisfactory forecast. They allow decent forecasts out-of-the-box even without data exploration. When model template is "AUTO", Greykite automatically selects the best model template for the input data.</p><p>Fine-tuning is important to get the best possible accuracy for key business metrics with high visibility and strict accuracy requirements. Therefore, our library provides full flexibility to customize the settings of a model template. For example, the user can add custom changepoints to enable faster adaptation to known changes and label known anomalies in the training data. The user can easily experiment with derived features by specifying model formula terms such as 'is_weekend:y_lag_1' (weekend/AR1 interaction). Because Silverkite generates many features internally, the user can leverage these to fine-tune the model without writing any code.</p><p>The third step is running the model. Internally, the Forecaster class runs an end-to-end forecasting pipeline with pre-processing, hyperparameter grid search, evaluation, backtest, and forecast. Grid search enables forecasting many metrics in a more automated way by selecting the optimal model from multiple candidates. We offer a default parameter grid for efficient search of the space. Automated machine learning techniques may also be used. Silverkite's fast training and inference facilitates such hyperparameter tuning. Greykite offers a benchmarking class to compare algorithms.</p><p>The last step is to diagnose the model and interpret results, both to improve the model and to establish trust with stakeholders. Again, we illustrate this on the D.C. bike-sharing dataset. Figure <ref type="figure" target="#fig_5">5</ref> plots forecast components such as trend, seasonality, autoregression, and holidays, representing the forecast as a sum of the contribution from each group. This view helps stakeholders understand the algorithm's assumptions (how it makes predictions) and the patterns present in the dataset. In Figure <ref type="figure" target="#fig_5">5</ref>, the fitted trend first increases then slightly decreases after a few detected changepoints. The yearly seasonality reflects a higher number of rides during warmer months and a lower number of rides during colder months, with increasing magnitude over time. In the presence of multicollinearity, one should treat this plot as descriptive of the model rather than showing the true effect of each component. Effect interpretation is improved through (1) groups of covariates that capture mostly orthogonal effects, (2) regularization, (3) fewer covariates when data are limited, and (4) enough training data to distinguish effects. While component plot shows the effect of groups of covariates, model summary shows the effect of individual covariates, as shown in Figure <ref type="figure" target="#fig_6">6</ref>. It includes model overview, coefficients, p-values, and confidence intervals. Greykite supports model summary for OLS and regularized models from scikit-learn and statsmodels, calculating the intervals using bootstrap for ridge and multi-sample splitting for lasso <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8]</ref>. This diagnostic can be used to inspect coefficients and assess whether any terms might be removed.</p><p>Component plot and model summary provide explanations of why forecasts change after the model is trained on new data. When strategic business decisions depend on the forecasted value, it is particularly important to assess whether the drivers are reasonable before deciding to take action.</p><p>Thus, Greykite's modeling framework addresses three key requirements of self-serve forecasting adoption: (1) accuracy, (2) ease of use, (3) trust. It achieves accuracy by making the Silverkite's flexible algorithms easy to configure and tune, ease of use by aiding each step of the modeling process, and trust through interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">BENCHMARK</head><p>In this section, we compare Silverkite against Prophet and SARIMA (seasonal ARIMA) on a variety of datasets. Prophet is a popular univariate forecasting algorithm designed for interpretability and self-serve in the business context <ref type="bibr" target="#b25">[26]</ref>. SARIMA is a well-known forecasting model that has stood the test of time and is widely available in many languages. While the model is not readily interpretable, it captures seasonality, temporal-dependence, and non-stationarity and has been a strong baseline in recent forecasting competitions <ref type="bibr" target="#b18">[19]</ref>. For the benchmark, we use the python packages greykite v0.4.0, prophet v1.0.1, and pmdarima v1.8.0 <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Benchmark Setup</head><p>Due to temporal dependency in time series data, the standard ?-fold cross-validation is not appropriate. Rolling origin cross-validation is a common technique in the time series literature <ref type="bibr" target="#b24">[25]</ref>. Unlike "fixed origin, " which fits the data on the training set and forecasts on the following test period only once, the rolling origin method evaluates a model with a sequence of forecasting origins that keeps moving forward. This makes model evaluation more robust by averaging across time periods. We use ?-fold rolling origin evaluation, described in Figure <ref type="figure" target="#fig_9">10</ref> in Appendix A.1.</p><p>To evaluate multiple time series at different scales, scaleindependent metrics are frequently used, such as MAPE, sMAPE <ref type="bibr" target="#b8">[9]</ref>, and Mean Absolute Scaled Error (MASE) <ref type="bibr" target="#b15">[16]</ref>. The first two metrics have the disadvantage of being infinite or undefined when there are zero values in the data. In addition, they put a heavier penalty on errors for low actual values, hence they are biased. Thus, we compare MASE with seasonal period according to the data frequency. For calculation details, see Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Dataset and Setup.</head><p>Comparing time series forecasting models has gained interest in recent years, such as in the M Competitions <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The curation of time series datasets has also grown rapidly, including UCI Machine Learning Repository (with 100+ datasets) <ref type="bibr" target="#b5">[6]</ref>, and the recently released Monash time series archive (30 datasets from different domains) <ref type="bibr" target="#b10">[11]</ref>. Prophet and Greykite packages also have built-in datasets.</p><p>Many dataset collections contain either multivariate time series for global modeling or hundreds of univariate time series. They are intended for fixed origin evaluation, and the result is averaged across many datasets. In our benchmark framework, we intend to evaluate the algorithms over a comprehensive period of time using a large number of splits. Thus, we choose nine datasets from the above sources suitable for rolling origin evaluation. The chosen datasets span a broad range of categories: energy, web, economy, finance, transportation, and nature. Their frequencies range from hourly to monthly and we benchmark them across multiple forecast horizons. The datasets are summarized in Table <ref type="table">1</ref>, with their detailed descriptions in Appendix A. <ref type="bibr" target="#b2">3</ref>. Any missing values are imputed by linear interpolation for model training. The imputed values are not used when calculating model accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(Frequency) Dataset</head><p>Length Test Length ? Splits (H) Solar power <ref type="bibr" target="#b10">[11]</ref> 1 yr. 30 days 30*6 (H) Wind power <ref type="bibr" target="#b10">[11]</ref> 1 yr. 30 days 30*6 (H) Electricity <ref type="bibr" target="#b5">[6]</ref> 3 yr. 365 days 365*6 (H) SF Bay Area traffic <ref type="bibr" target="#b10">[11]</ref> 2 yr. 60 days 60*6 (D) Peyton Manning <ref type="bibr" target="#b25">[26]</ref> 9 yr. 365 days 365 (D) Bike sharing <ref type="bibr" target="#b1">[2]</ref> 8 yr. 365 days 365 (D) SF Bay Area traffic <ref type="bibr" target="#b10">[11]</ref> 2 yr. 365 days 365 (D) Bitcoin transactions <ref type="bibr" target="#b10">[11]</ref> 11 yr.</p><p>365 days 365 (M) Sunspot <ref type="bibr" target="#b10">[11]</ref> 203 yr. 24 months 24 (M) House supply <ref type="bibr" target="#b2">[3]</ref> 59 yr. 24 months 24 Table <ref type="table">1</ref>: Datasets and their benchmark configuration. We use forecast horizons 1 and 24 for hourly data; 1, 7, and 30 for daily data; 1 and 12 for monthly data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Model Comparison.</head><p>We benchmark the performance of Silverkite, Prophet, and SARIMA. For Silverkite, we use the "AUTO" model template. For SARIMA, we use the out-of-the-box settings of pmdarima. It uses statistical tests and AIC to identify the optimal ?, ?, ?, ?, ?, ? parameters. <ref type="foot" target="#foot_1">2</ref> For Prophet, we use the out-of-the-box settings with holidays added for hourly and daily frequencies to match the configuration of Silverkite. Because Prophet also natively supports holidays, we use the same default holiday countries for a fair comparison: US, GB, IN, FR, CN. Holidays are not needed for monthly data due to the level of aggregation, so they are not included for monthly frequency. For each frequency and forecast horizon combination, Table <ref type="table" target="#tab_1">2</ref> shows the MASE (lower is better) averaged across benchmark datasets. Silverkite significantly outperforms the other two algorithms in all but one setting. For that setting (monthly frequency with horizon 1), Silverkite is optimal on sunspot data and a close second on house supply data. The full results for each dataset are in Table <ref type="table" target="#tab_2">3</ref> in Appendix A.4. Of the 24 dataset/horizon combinations, Silverkite is optimal on 20. Because Silverkite has the advantage of being the most flexible of the three models, it is possible to fine-tune Silverkite to achieve better performance on the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MASE</head><p>For a given frequency, Silverkite and SARIMA show noticable improvement on shorter horizons, but Prophet does not. This suggests that Silverkite and SARIMA make better use of recent information through autoregression, whereas Prophet focuses on overall trends. Silverkite's interpretable approach, with groups of covariates that capture underlying time series dynamics, could be one reason it outperforms SARIMA.</p><p>It is noteworthy that Prophet and SARIMA usually have MASE &gt; 1 (i.e. forecast error is higher than the naive method's in-sample error). Silverkite, on the other hand, usually has MASE &lt; 1. Thus, Silverkite offers good out-of-the-box performance on a wide range of datasets, frequencies, and horizons. Moreover, Silverkite is often faster than the other algorithms. The runtime comparison for training and inference can be found in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref> in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DEPLOYMENT</head><p>We deployed model-driven forecasting solutions at LinkedIn for more than twenty use cases to streamline product performance management and optimize resource allocation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Performance Management</head><p>The performance management process at Linkedin involves setting targets for core business metrics, making investments, tracking progress, detecting anomalies, and finding root causes. Given its centrality and ubiquity across the business, it is critical to move away from manual processes and adapt to growing data volume and complexity. To achieve this, we facilitate adoption of Greykite.</p><p>First, we partnered with Financial Planning &amp; Analysis (FP&amp;A) to build automated forecasting-based performance management for LinkedIn Marketing Solutions (LMS). Success for this customer was key to extending our outcomes and learnings to more use cases for other business verticals.</p><p>LMS is a fast-growing business, with a complex ads ecosystem for advertisers and potential customers. Ads marketplace metrics exhibit complex growth and seasonality patterns and need to be modeled at different frequencies and across many dimensions. Due to this complexity, this use case became an essential learning experience for us to deploy new solutions and scale them to other business verticals. It helped us define requirements, for example:</p><p>? Autoregression to improve next-day forecast accuracy.</p><p>? Indicators to capture sharp changes around month/quarter start and end. ? Trend and seasonality changepoints to capture the effect of large feature launches. ? User-provided anomalies to ignore temporary issues.</p><p>To enable efficient detection of revenue issues, our forecasts needed to meet a high level of accuracy (e.g. &lt; 2% MAPE for daily nextday forecast) and have reliable prediction intervals. An internal benchmark showed that Silverkite met the requirements and has 75% lower error on revenue metrics compared to Prophet.</p><p>We deployed models for over thirty LMS metrics such as ads revenue and ads impressions and their key dimensions. The forecasts are at daily, weekly, and monthly frequencies with horizons from 1 day to 6 months. These are integrated into production dashboards and are sent in daily emails to show business outlook and flag concerns about business health. The emails compare forecasted metrics against their targets and alert anomalies in observed data compared to the forecasted baseline interval. To aid investigation when revenue falls outside expectations, the dashboard includes forecasts for supply-side and demand-side marketplace metrics and their dimensions. This helps isolate the problem to a particular part of the supply or demand funnel or segment of the business. The automated performance management solution has been in production for 18 months and is the primary source for FP&amp;A to quickly assess business health and begin investigations if needed.</p><p>To scale our solution to other business verticals, we partnered with LinkedIn Premium and Talent Solutions to provide short-term dimensional forecasts for key metrics such as sign-ups, bookings, sessions, and weekly active users. We observed significant improvements in forecasting accuracy (MAPE) and faster anomaly detection with higher precision and recall. Figure <ref type="figure" target="#fig_7">7</ref> shows how Greykite detected anomalies for LinkedIn Premium and helped them assess impact severity; these anomalies were missed by the existing weekover-week threshold detection. Greykite achieves remarkable performance on long-term forecasts as well. In partnership with FP&amp;A, we developed monthly forecasts for the next half-year of revenue that significantly outperformed manual forecasts, providing better guidance for strategic decisions. Figure <ref type="figure" target="#fig_8">8</ref> shows how the forecast adapted to revenue growth momentum earlier than the manual forecast through autoregression and automatic changepoint detection. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Resource Management</head><p>On the infrastructure side, forecasts help LinkedIn maintain site and service availability in a cost-effective manner as site traffic continues to increase. Better projections about future traffic, combined with accurate site capacity measurements, enable confident decision-making and sustainable growth through right-sized applications whose provisioned instances match required workloads.</p><p>Prior to deploying Greykite forecasts, capacity planning was highly manual, reactive, and had a tendency to overprovision resources. We provide hourly peak-minute queries per second (QPS) forecasts for hundreds of services, which are used to automate right-sizing. With the Automatic Rightsizing system conducting hundreds of rightsize actions a day, we eliminate most of the toil required of application owners to manually adjust compute resources to support organic business growth, save millions of dollars by removing excess capacity, and optimize fleet resource utilization by re-purposing such excess capacity to applications lacking capacity. The forecasts are shown in production dashboards alongside allocated serving capacity, as shown in Figure <ref type="figure">9</ref>. The system has been in production for over two years. Since this collaboration, we have seen significant reduction of capacity-related incidents as applications are automatically uplifted to match expected workload.</p><p>Figure <ref type="figure">9</ref>: The production dashboard for application headroom shows projected peak load QPS (red) and excess capacity (shaded blue) to inform decision-making on resource allocation. This plot reveals excess capacity that could be removed while safely supporting peak load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Lessons Learned</head><p>We learned some lessons on algorithms, framework, and deployment through solving company-wide forecasting problems at scale.</p><p>Diverse requirements can be met with a highly flexible family of models. The flexible design that captures trend, seasonality, changepoints, holidays, autoregression, interactions, and regressors enables accurate forecasts of business and infrastructure metrics across frequencies and horizons.</p><p>It is possible to achieve high accuracy without sacrificing interpretability. Interpretation is essential to building trust with stakeholders who want to understand how forecasts are made, modify assumptions, and understand why forecasted values change after training on more data. Silverkite transforms features into a space that can be used in a regularized linear model, allowing additive component plots and model summary.</p><p>Enabling self-serve allowed scaling forecast solutions across the business. This required high out-of-the-box accuracy with little effort, which we achieved with an end-to-end forecasting library and intuitive tuning interfaces, and a fast algorithm for interactive tuning and hyperparameter search.</p><p>Partnership with customers is essential to deploying new research ideas. During alpha and beta, we worked with a few champion customers to clarify requirements and deliver wins. Then, we generalized the solution and scaled it through a self-serve model. Champion customers should be high impact, representative of other customers, aligned with company priorities, and willing to form a close partnership. As we proved success, the champion use cases became advocates and examples to drive adoption forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>Prior to this work, LinkedIn's forecasts were mostly manual, ad-hoc, and intuition-driven. Now, customers from all lines of business and all functions (engineering, product, finance) are adopting algorithmic forecasts. Customers understand the benefits of accuracy, scale, and consistency. Our forecasts save time and drive clarity on the business/infrastructure that empowers LinkedIn to plan and adapt dynamically to new information. This culture shift was achieved through a fast, flexible, and interpretable family of models and a modeling framework that enables self-serve forecasting with ease and accuracy.</p><p>The Silverkite algorithm performs well on both internal datasets and out-of-the-box on external datasets with frequencies from hourly to monthly from a variety of domains. Its flexible design allows fine-tuning of covariates, objective function, and volatility model. Thus, we expect the open-source Greykite library to be useful to forecast practitioners, especially those whose time series include time-varying growth and seasonality, holiday effects, anomalies, and/or dependency on external factors, characteristics that are common in time series related to human activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 Rolling Origin Evaluation</head><p>Greykite includes the BenchmarkForecastConfig class to perform rolling origin benchmarking. Rolling origin evaluation works as follows. We replicate the time series data to ? train-test splits. Each split contains a test period (red) immediately after its training period (deep and light blue). The ? forecasting origins (test start dates) are picked in a rolling manner, working backward from the end of the time series.</p><p>Each candidate model is fit on the training period of each of the rolling splits, and the trained model is used to predict on the corresponding test period. The average test error across all ? splits mimics the backtest error if the model were put in production, and this provides a reliable estimate of the future forecast error. A few parameters control how the rolling origin splits are generated. First, since one of the goals of rolling origin evaluation is to make sure the estimated forecast error is robust and representative, ? needs to be sufficiently large. We recommend that the test periods cover at least one year of data for daily and less granular frequencies (weekly, monthly, etc.). For sub-daily data with a short forecast horizon, we recommend the test periods cover at least one month.</p><p>? should be chosen together with the period between the splits, which controls the step size between every two successive splits. In general, we recommend step size of 1 to obtain the most comprehensive evaluation. In practice, for small granularity data (such as hourly or minutely), it can make sense to increase the period between splits and reduce ? to decrease computational cost. As a result, the test periods in these rolling splits may or may not overlap.</p><p>Another two parameters are the lengths of the training and test periods for each split. In order to best estimate the model's performance in real applications, the length of each test period should match the desired forecast horizon. Ideally, the minimum training period is at least two years for daily and less granular frequencies in order to accurately estimate the yearly seasonality effect. For sub-daily data, a shorter training period could be used for short-term forecasts. The model that minimizes the average error across all the rolling splits is deemed as the best model.</p><p>Finally, the split training periods could either use expanding windows (fixed train start date) or moving windows (fixed length), as shown in Figure <ref type="figure" target="#fig_9">10</ref>. We recommend choosing the one that best mimics the setting for deployment. Moving windows is preferred to increase speed when not all history is needed for an accurate forecast. Expanding windows is preferred if speed is not an issue and using more history improves accuracy.</p><p>Such an extensive rolling evaluation not only provides a robust estimate of the model performance, but also enables users to run comprehensive diagnostics, such as error breakdowns on different seasonal cycles (e.g. day of week) and holiday effects. Then, these systematic errors could be addressed by incorporating these signals as features into the model. We developed an internal product Greykite-on-Spark for benchmarking and hyperparameter tuning on Spark. With parallel execution on Spark, all experiments could be finished within a day.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Evaluation Metrics</head><p>MASE <ref type="bibr" target="#b15">[16]</ref> is calculated by dividing the out-of-sample MAE by the in-sample MAE from a naive forecast. For daily time series with weekly seasonality, the naive forecast uses the value at ? -7 to predict the value at time ? , where 7 is called the "seasonal period". For non-seasonal time series, the seasonal period is 1. Similar to <ref type="bibr" target="#b10">[11]</ref>, we choose the seasonal period based on the frequency of the data. In our experiments, we use 24 for hourly data, 7 for daily data, and 12 for monthly data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Benchmark Datasets</head><p>Hourly. 4 datasets, horizons 1 and 24. The hourly datasets are solar power, wind power, electricity, and traffic. These datasets are from Monash <ref type="bibr" target="#b10">[11]</ref>.</p><p>The solar (wind) power dataset contains the solar (wind) power production of an Australian wind farm from August 2019 to July 2020, with original frequency 4-second. We aggregate it to an hourly series and remove any incomplete hours. Since the time series is shorter than one year, the seasonal effects may not be well estimated. Also, the solar power dataset is an intermittent time series where a lot of zeros are present, hence using MASE is better than MAPE or sMAPE. We use a moving 300-day window for training, and test on every 4 hours in the last 30 days (30*6 splits).</p><p>The electricity dataset contains the hourly consumption (in kW) of 321 clients from 2012 to 2014 published by Monash <ref type="bibr" target="#b10">[11]</ref>, originally from <ref type="bibr" target="#b5">[6]</ref>. We aggregate them by taking the average across the 321 clients. The averaged series has a history of 3 years. We use a moving two-year window for training, and test on every 4 hours in the last year (365*6 splits).</p><p>The SF Bay Area traffic dataset records the road occupancy rates (between 0 and 1) measured by different sensors on San Francisco Bay area freeways from 2015 to 2016. We take the average of these occupancy rates. We use a moving 600-day window for training, and test on every 4 hours in the last 60 days (60*6 splits).</p><p>These datasets present both intraday and interday seasonalities. Daily. 4 datasets, horizons 1, 7, and 30. The daily datasets are page views, bike rental counts, traffic, and Bitcoin transactions. We test on the last year (365 splits, period between splits 1), with an expanding window for training.</p><p>The Peyton Manning dataset contains the logarithm of daily page views for the Wikipedia page for football player Peyton Manning. This dataset is from Prophet <ref type="bibr" target="#b25">[26]</ref> and it has 8 years of history. Missing values are imputed by us with linear interpolation when fitting the models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MASE</head><p>The bike-sharing dataset contains aggregated daily counts of the number of rented bikes in Washington, D.C. from 2010 to 2019 (incomplete days are removed). The raw dataset is from Capital Bikeshare <ref type="bibr" target="#b1">[2]</ref> and it is also available in Greykite.</p><p>The SF Bay Area traffic dataset is the same as for hourly benchmarking. We aggregate it to a daily time series from 2015 to 2016.</p><p>The Bitcoin dataset has the number of Bitcoin transactions from 2009 to 2021. The dataset was curated (with missing values filled) by Monash <ref type="bibr" target="#b10">[11]</ref>.</p><p>The Peyton Manning and bike-sharing datasets present weekly and yearly seasonalities and possible changepoints. The SF Bay Area traffic dataset also shows relatively strong weekly and yearly seasonalities but has a shorter history, which makes it harder to estimate long-term effects. The Bitcoin dataset is much more volatile than the others.</p><p>Monthly. 2 datasets, horizons 1 and 12. The monthly datasets are sunspot activity and house supply. We test on the last 2 years of data (24 splits, 1 period between splits), with an expanding window for training.</p><p>The first dataset is daily sunspot activity from 1818 to 2020 published by Monash <ref type="bibr" target="#b10">[11]</ref>, originally from <ref type="bibr" target="#b3">[4]</ref>. We aggregate it to a monthly time series more than 200 years long. The second dataset is monthly house supply from 1963 to 2021 obtained from FRED <ref type="bibr" target="#b2">[3]</ref>. Both have mild yearly seasonality and some multi-year patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Full Benchmark Results</head><p>The complete benchmark results (MASE) are shown in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Runtime Comparison</head><p>We compare the training and inference runtime on a MacBook Pro with 2.4 GHz 8-Core Intel Core i9 processor, and 32 GB 2667 MHz DDR4 memory. Results are recorded in Tables <ref type="table" target="#tab_3">4</ref> and<ref type="table" target="#tab_4">5</ref>. Hourly models are evaluated with 2 years of training data using the electricity dataset. Daily models are evaluated with 8 years of training data using the Peyton Manning dataset. Monthly models are evaluated with 202 years of training data using the sunspot dataset. All models use forecast horizon 1. All models use the same configurations as before to produce a forecast and 95% prediction interval. All measurements are the best of 7 runs.</p><p>Silverkite's training speed is significantly faster than Prophet and SARIMA. The only exception is the monthly Prophet model, which is extremely fast because its features for monthly data are quite limited. However, Prophet's MASE for sunspot monthly data with horizon 1 is 17x higher than Silverkite's (1.497 vs 0.088).</p><p>Silverkite's inference speed is significantly faster than Prophet's for all frequencies because Prophet requires MCMC to sample from a distribution. The inference time of SARIMA is near zero because its predictions are easy to calculate from the analytical solution, whereas Silverkite prepares the future features and performs a larger matrix multiplication.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture diagram for Greykite's main forecasting algorithm: Silverkite.</figDesc><graphic url="image-1.png" coords="3,53.80,76.49,504.37,216.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) Flexible to pair any conditional mean model with any volatility model for better accuracy. (d) Modular engineering framework for simple development and testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Library aids each step of the forecast workflow.</figDesc><graphic url="image-2.png" coords="5,53.80,80.09,240.24,121.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Greykite provides exploratory plots to identify seasonality effects and interactions.</figDesc><graphic url="image-4.png" coords="5,53.80,473.80,240.24,105.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Automatic trend and seasonality changepoint detection in bike-sharing data.</figDesc><graphic url="image-5.png" coords="5,317.96,83.69,240.23,151.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The component plot shows how groups of covariates contribute to the forecasted value. It can be used for interpretability and debugging.</figDesc><graphic url="image-6.png" coords="6,53.80,76.49,240.25,109.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model summary shows the effect of individual covariates for interpretability, model tuning, and debugging.</figDesc><graphic url="image-7.png" coords="6,53.80,406.22,240.23,159.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Greykite detected unexpected anomalies and helped LinkedIn Premium assess impact severity.</figDesc><graphic url="image-8.png" coords="8,53.80,290.10,240.25,115.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Greykite outperforms manual forecast by picking up revenue growth momentum at the start of the half-year.</figDesc><graphic url="image-9.png" coords="8,53.80,529.39,240.25,144.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: ?-fold rolling origin evaluation. The example shows four splits with forecast horizon 4 and period between splits 2. The light blue points could either be included or not included in the training set, depending on user's choice of whether to use expanding windows (light blue + deep blue) or moving windows (deep blue only).</figDesc><graphic url="image-11.png" coords="10,53.80,247.28,240.19,87.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model comparison. Average MASE across datasets.The best model for each frequency and horizon is in bold.</figDesc><table><row><cell></cell><cell cols="4">Horizon Silverkite Prophet SARIMA</cell></row><row><cell>Hourly</cell><cell>1</cell><cell>0.741</cell><cell>2.047</cell><cell>1.178</cell></row><row><cell></cell><cell>24</cell><cell>0.945</cell><cell>2.034</cell><cell>3.429</cell></row><row><cell>Daily</cell><cell>1</cell><cell>0.940</cell><cell>1.527</cell><cell>1.358</cell></row><row><cell></cell><cell>7</cell><cell>1.097</cell><cell>1.539</cell><cell>1.515</cell></row><row><cell></cell><cell>30</cell><cell>1.251</cell><cell>1.575</cell><cell>1.705</cell></row><row><cell cols="2">Monthly 1</cell><cell>0.333</cell><cell>1.289</cell><cell>0.303</cell></row><row><cell></cell><cell>12</cell><cell>0.579</cell><cell>1.292</cell><cell>0.724</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Detailed model comparison. Showing MASE for each dataset and horizon. The best model for each row is in bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">Horizon Silverkite Prophet SARIMA</cell></row><row><cell cols="2">(H) Solar power</cell><cell>1</cell><cell>0.732</cell><cell>1.760</cell><cell>1.397</cell></row><row><cell></cell><cell></cell><cell>24</cell><cell>0.854</cell><cell>1.721</cell><cell>3.794</cell></row><row><cell cols="2">(H) Wind power</cell><cell>1</cell><cell>0.220</cell><cell>0.569</cell><cell>0.211</cell></row><row><cell></cell><cell></cell><cell>24</cell><cell>0.742</cell><cell>0.706</cell><cell>0.626</cell></row><row><cell cols="2">(H) Electricity</cell><cell>1</cell><cell>0.820</cell><cell>3.168</cell><cell>1.323</cell></row><row><cell></cell><cell></cell><cell>24</cell><cell>1.030</cell><cell>3.403</cell><cell>6.738</cell></row><row><cell cols="2">(H) SF Bay Area</cell><cell>1</cell><cell>1.191</cell><cell>2.689</cell><cell>1.779</cell></row><row><cell>traffic</cell><cell></cell><cell>24</cell><cell>1.153</cell><cell>2.306</cell><cell>2.558</cell></row><row><cell>(D) Peyton</cell><cell></cell><cell>1</cell><cell>0.646</cell><cell>0.826</cell><cell>0.709</cell></row><row><cell>Manning</cell><cell></cell><cell>7</cell><cell>0.810</cell><cell>0.827</cell><cell>0.884</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>0.792</cell><cell>0.842</cell><cell>1.116</cell></row><row><cell cols="2">(D) Bike sharing</cell><cell>1</cell><cell>0.843</cell><cell>1.002</cell><cell>0.927</cell></row><row><cell></cell><cell></cell><cell>7</cell><cell>0.964</cell><cell>1.010</cell><cell>1.031</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>1.006</cell><cell>1.029</cell><cell>1.166</cell></row><row><cell cols="2">(D) SF Bay Area</cell><cell>1</cell><cell>0.712</cell><cell>0.840</cell><cell>1.921</cell></row><row><cell>traffic</cell><cell></cell><cell>7</cell><cell>0.798</cell><cell>0.847</cell><cell>2.244</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>0.814</cell><cell>0.874</cell><cell>2.354</cell></row><row><cell>(D) Bitcoin</cell><cell></cell><cell>1</cell><cell>1.557</cell><cell>3.438</cell><cell>1.876</cell></row><row><cell>transactions</cell><cell></cell><cell>7</cell><cell>1.816</cell><cell>3.470</cell><cell>1.902</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell>2.392</cell><cell>3.554</cell><cell>2.184</cell></row><row><cell>(M) Sunspot</cell><cell></cell><cell>1</cell><cell>0.088</cell><cell>1.497</cell><cell>0.114</cell></row><row><cell></cell><cell></cell><cell>12</cell><cell>0.219</cell><cell>1.572</cell><cell>0.397</cell></row><row><cell cols="3">(M) House supply 1</cell><cell>0.577</cell><cell>1.080</cell><cell>0.492</cell></row><row><cell></cell><cell></cell><cell>12</cell><cell>0.939</cell><cell>1.012</cell><cell>1.050</cell></row><row><cell cols="6">Runtime Train Length Silverkite Prophet SARIMA</cell></row><row><cell>Hourly</cell><cell>17520</cell><cell></cell><cell>30.57</cell><cell>142.39</cell><cell>173.86</cell></row><row><cell>Daily</cell><cell>2963</cell><cell></cell><cell>4.75</cell><cell>16.93</cell><cell>7.98</cell></row><row><cell cols="2">Monthly 2428</cell><cell></cell><cell>3.41</cell><cell>0.39</cell><cell>5.46</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Training time comparison (in seconds).</figDesc><table><row><cell cols="5">Runtime Horizon Silverkite Prophet SARIMA</cell></row><row><cell>Hourly</cell><cell>1</cell><cell>0.88</cell><cell>1.39</cell><cell>0.00</cell></row><row><cell>Daily</cell><cell>1</cell><cell>0.51</cell><cell>2.08</cell><cell>0.00</cell></row><row><cell cols="2">Monthly 1</cell><cell>0.12</cell><cell>1.05</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Inference time comparison (in seconds).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/linkedin/greykite</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima. AutoARIMA.html#pmdarima.arima.AutoARIMA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is done by the Data Science Applied Research team at LinkedIn. Special thanks to our TPM <rs type="person">Shruti Sharma</rs> and our collaborators in Data Science, Engineering, SRE, FP&amp;A, BizOps, and Product for adopting the library. In particular, thanks to <rs type="person">Ashok Sridhar</rs>, <rs type="person">Mingyuan Zhong</rs>, <rs type="person">Peter Huang</rs>, <rs type="person">Hamin Oh</rs>, <rs type="person">Neha Gupta</rs>, <rs type="person">Neelima Rathi</rs>, <rs type="person">Deepti Rai</rs>, <rs type="person">Christian Rhally</rs>, <rs type="person">Camilo Rivera</rs>, <rs type="person">Priscilla Tam</rs>, <rs type="person">Meenakshi Adaikalavan</rs>, <rs type="person">Zheng Shao</rs>, <rs type="person">Mike Snow</rs>, <rs type="person">Stephen Bisordi</rs>, <rs type="person">Dong Wang</rs>, <rs type="person">Ankit Upadhyaya</rs>, and <rs type="person">Rachit Kumar</rs> for allowing us to share their use cases. We also thank our leadership team <rs type="person">Ya Xu</rs> and <rs type="person">Sofus Macsk?ssy</rs> for their support.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">GluonTS: Probabilistic and Neural Time Series Modeling in Python</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Alexandrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Benidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bohlke-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><forename type="middle">C</forename><surname>Maddix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Syama</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Caner T?rkmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/19-820.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Capital</forename><surname>Bikeshare</surname></persName>
		</author>
		<ptr target="https://www.capitalbikeshare.com/system-data" />
		<title level="m">System Data. Retrieved February 9</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Monthly Supply of Houses in the United States [MSACSR]. Retrieved February 9</title>
		<ptr target="https://fred.stlouisfed.org/series/MSACSR" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>U.S. Census Bureau, U.S. Department of Housing, and Urban Development</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting the sunspot number</title>
		<author>
			<persName><forename type="first">Fr?d?ric</forename><surname>Clette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leif</forename><surname>Svalgaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jos?</forename><forename type="middle">M</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Cliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Space Science Reviews</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="35" to="103" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-dimensional inference: confidence intervals, p-values and R-software hdi</title>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Dezeure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="page" from="533" to="558" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">James</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siem</forename></persName>
		</author>
		<title level="m">Time Series Analysis by State Space Methods</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012-01">Jan Koopman. 2012</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An introduction to the bootstrap</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Pragmatic View of Accuracy Measurement in Forecasting</title>
		<author>
			<persName><forename type="first">Benito</forename><forename type="middle">E</forename><surname>Flores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Omega</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="93" to="98" />
			<date type="published" when="1986">1986. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Regression models for time series analysis</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Fokianos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Wiley-Interscience</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Rakshitha</forename><surname>Godahewa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bergmeir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Montero-Manso</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06643</idno>
		<title level="m">Monash time series forecasting archive</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Model selection for count timeseries with applications in forecasting number of trips in bike-sharing systems and its volatility</title>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hosseini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08389</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A characterization of categorical Markov chains</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhu</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Zidek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Theory and Practice</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="261" to="284" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Non-linear timevarying stochastic models for agroclimate risk assessment</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akimichi</forename><surname>Takemura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alireza</forename><surname>Hosseini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmental and Ecological Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="227" to="246" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Reza</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Patra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01098</idno>
		<title level="m">A flexible forecasting model for production systems</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Another look at forecast-accuracy metrics for intermittent demand</title>
		<author>
			<persName><forename type="first">Rob</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foresight: The International Journal of Applied Forecasting</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="43" to="46" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Forecasting: principles and practice</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><surname>Athanasopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OTexts</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A New Approach to Linear Filtering and Prediction Problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kalman</surname></persName>
		</author>
		<idno type="DOI">10.1115/1.3662552</idno>
		<ptr target="https://doi.org/10.1115/1.3662552" />
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="45" />
			<date type="published" when="1960-03">1960. 03 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The M4 Competition: Results, findings, conclusion and way forward</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evangelos Spiliotis, and Vassilios Assimakopoulos</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="802" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. M5 accuracy competition: Results, findings, and conclusions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">O'</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Hara-Wild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Earo</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=fableRpackageversion0.3.1" />
		<title level="m">Forecasting Models for Tidy Time Series</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">DeepAR: Probabilistic forecasting with autoregressive recurrent networks</title>
		<author>
			<persName><forename type="first">David</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Flunkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Gasthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Januschowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1181" to="1191" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Statsmodels: Econometric and statistical modeling with python</title>
		<author>
			<persName><forename type="first">Skipper</forename><surname>Seabold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Perktold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Python in Science Conference</title>
		<meeting>the 9th Python in Science Conference<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">61</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Taylor</forename><forename type="middle">G</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="http://www.alkaline-ml.com/pmdarima" />
		<title level="m">ARIMA estimators for Python. Retrieved February 9</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Out-of-sample tests of forecasting accuracy: an analysis and review</title>
		<author>
			<persName><surname>Leonard J Tashman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of forecasting</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="437" to="450" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Forecasting at scale</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><surname>Letham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="37" to="45" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the Lasso</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Non-linear time series: a dynamical system approach</title>
		<author>
			<persName><forename type="first">Howell</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ruey</surname></persName>
		</author>
		<author>
			<persName><surname>Tsay</surname></persName>
		</author>
		<title level="m">Analysis of Financial Time Series</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>rd ed.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<ptr target="https://www.nextplayventures.com/forecasting" />
		<title level="m">Next Play Ventures. 2022. Forecasting. Retrieved February 9</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Bayesian forecasting and dynamic models</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Harrison</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Forecasting sales by exponentially weighted moving averages</title>
		<author>
			<persName><surname>Peter R Winters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="324" to="342" />
			<date type="published" when="1960-04">1960. apr 1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The adaptive Lasso and its oracle properties</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
