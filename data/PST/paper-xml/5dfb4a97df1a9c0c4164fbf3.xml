<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Michael</forename><surname>Heinzinger</surname></persName>
							<email>mheinzinger@rostlab.org</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics, Bioinformatics &amp; Computational Biology -i12</orgName>
								<orgName type="institution">TUM (Technical University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstr. 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">TUM Graduate School</orgName>
								<orgName type="department" key="dep2">Center of Doctoral Studies in Informatics and its Applications (CeDoSIA)</orgName>
								<address>
									<addrLine>Boltzmannstr. 11</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
							<idno type="ORCID">0000-0002-9601-3580</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics, Bioinformatics &amp; Computational Biology -i12</orgName>
								<orgName type="institution">TUM (Technical University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstr. 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">TUM Graduate School</orgName>
								<orgName type="department" key="dep2">Center of Doctoral Studies in Informatics and its Applications (CeDoSIA)</orgName>
								<address>
									<addrLine>Boltzmannstr. 11</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Dallago</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics, Bioinformatics &amp; Computational Biology -i12</orgName>
								<orgName type="institution">TUM (Technical University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstr. 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">TUM Graduate School</orgName>
								<orgName type="department" key="dep2">Center of Doctoral Studies in Informatics and its Applications (CeDoSIA)</orgName>
								<address>
									<addrLine>Boltzmannstr. 11</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dmitrii</forename><surname>Nechaev</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics, Bioinformatics &amp; Computational Biology -i12</orgName>
								<orgName type="institution">TUM (Technical University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstr. 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">TUM Graduate School</orgName>
								<orgName type="department" key="dep2">Center of Doctoral Studies in Informatics and its Applications (CeDoSIA)</orgName>
								<address>
									<addrLine>Boltzmannstr. 11</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Matthes</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics, Bioinformatics &amp; Computational Biology -i12</orgName>
								<orgName type="institution">TUM (Technical University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstr. 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching, Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Modeling aspects of the language of life through transfer-learning protein sequences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1186/s12859-019-3220-8</idno>
					<note type="submission">Received: 3 May 2019 Accepted: 13 November 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Machine Learning</term>
					<term>Language Modeling</term>
					<term>Sequence Embedding</term>
					<term>Secondary structure prediction</term>
					<term>Localization prediction</term>
					<term>Transfer Learning</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background: Predicting protein function and structure from sequence is one important challenge for computational biology. For 26 years, most state-of-the-art approaches combined machine learning and evolutionary information. However, for some applications retrieving related proteins is becoming too time-consuming. Additionally, evolutionary information is less powerful for small families, e.g. for proteins from the Dark Proteome. Both these problems are addressed by the new methodology introduced here. Results: We introduced a novel way to represent protein sequences as continuous vectors (embeddings) by using the language model ELMo taken from natural language processing. By modeling protein sequences, ELMo effectively captured the biophysical properties of the language of life from unlabeled big data (UniRef50). We refer to these new embeddings as SeqVec (Sequence-to-Vector) and demonstrate their effectiveness by training simple neural networks for two different tasks. At the per-residue level, secondary structure (Q3 = 79% ± 1, Q8 = 68% ± 1) and regions with intrinsic disorder (MCC = 0.59 ± 0.03) were predicted significantly better than through one-hot encoding or through Word2veclike approaches. At the per-protein level, subcellular localization was predicted in ten classes (Q10 = 68% ± 1) and membrane-bound were distinguished from water-soluble proteins (Q2 = 87% ± 1). Although SeqVec embeddings generated the best predictions from single sequences, no solution improved over the best existing method using evolutionary information. Nevertheless, our approach improved over some popular methods using evolutionary information and for some proteins even did beat the best. Thus, they prove to condense the underlying principles of protein sequences. Overall, the important novelty is speed: where the lightning-fast HHblits needed on average about two minutes to generate the evolutionary information for a target protein, SeqVec created embeddings on average in 0.03 s. As this speed-up is independent of the size of growing sequence databases, SeqVec provides a highly scalable approach for the analysis of big data in proteomics, i.e. microbiome or metaproteome analysis. Conclusion: Transfer-learning succeeded to extract information from unlabeled sequence databases relevant for various protein prediction tasks. SeqVec modeled the language of life, namely the principles underlying protein sequences better than any features suggested by textbooks and prediction methods. The exception is evolutionary information, however, that information is not available on the level of a single sequence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>The combination of evolutionary information (from Multiple Sequence Alignments -MSA) and Machine Learning/Artificial Intelligence (standard feed-forward artificial neural networks -ANN) completely changed protein secondary structure prediction <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. The concept was quickly taken up <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref> and predictions improved even more with larger families increasing evolutionary information through diversity <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. The idea was applied to other tasks, including the prediction of transmembrane regions <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, solvent accessibility <ref type="bibr" target="#b13">[14]</ref>, residue flexibility (B-values) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, inter-residue contacts <ref type="bibr" target="#b16">[17]</ref> and protein disorder <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Later, automatic methods predicting aspects of protein function improved by combining evolutionary information and machine learning, including predictions of subcellular localization (aka cellular compartment or CC in GO <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>), protein interaction sites <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>, and the effects of sequence variation upon function <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Arguably, the most important breakthrough for protein structure prediction over the last decade was a more efficient way of using evolutionary couplings <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>.</p><p>Although evolutionary information has increasingly improved prediction methods, it is also becoming increasingly costly. As sequencing becomes cheaper, the number of bio-sequence databases grow faster than computing power. For instance, the number of UniProt entries is now more than doubling every two years <ref type="bibr" target="#b31">[32]</ref>. An all-against-all comparison executed to build up profiles of evolutionary information squares this number: every two years the job increases 4-fold while computer power grows less than 2-fold. Consequently, methods as fast as PSI-BLAST <ref type="bibr" target="#b32">[33]</ref> have to be replaced by faster solutions such as HHblits <ref type="bibr" target="#b33">[34]</ref>. Even its latest version HHblits3 <ref type="bibr" target="#b34">[35]</ref> still needs several minutes to search Uni-Ref50 (subset of UniProt) for a single query protein. The next step up in speed such as MMSeqs2 <ref type="bibr" target="#b35">[36]</ref> appear to cope with the challenge at the expense of increasing hardware requirements while databases keep growing. However, even these solutions might eventually lose the battle against the speedup of sequencing. Analyzing data sets involving millions of proteins, i.e. samples of the human gut microbiota or metagenomic samples, have already become a major challenge <ref type="bibr" target="#b34">[35]</ref>. Secondly, evolutionary information is still missing for some proteins, e.g. for proteins with substantial intrinsically disordered regions <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>, or the entire Dark Proteome <ref type="bibr" target="#b38">[39]</ref> full of proteins that are less-well studied but important for function <ref type="bibr" target="#b39">[40]</ref>.</p><p>Here, we propose a novel embedding of protein sequences that replaces the explicit search for evolutionary related proteins by an implicit transfer of biophysical information derived from large, unlabeled sequence data (here UniRef50). We adopted a method that has been revolutionizing Natural Language Processing (NLP), namely the bi-directional language model ELMo (Embeddings from Language Models) <ref type="bibr" target="#b40">[41]</ref>. In NLP, ELMo is trained on unlabeled text-corpora such as Wikipedia to predict the most probable next word in a sentence, given all previous words in this sentence. By learning a probability distribution for sentences, these models autonomously develop a notion for syntax and semantics of language. The trained vector representations (embeddings) are contextualized, i.e. the embeddings of a given word depend on its context. This has the advantage that two identical words can have different embeddings, depending on the words surrounding them. In contrast to previous non-contextualized approaches such as word2vec <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, this allows to take the ambiguous meaning of words into account.</p><p>We hypothesized that the ELMo concept could be applied to model protein sequences. Three main challenges arose. <ref type="bibr" target="#b0">(1)</ref> Proteins range from about 30 to 33,000 residues, a much larger range than for the average English sentence extending over 15-30 words <ref type="bibr" target="#b43">[44]</ref>, and even more extreme than notable literary exceptions such as James Joyce's Ulysses <ref type="bibr">(1922)</ref> with almost 4000 words in a sentence. Longer proteins require more GPU memory and the underlying models (so-called LSTMs: Long Short-Term Memory networks <ref type="bibr" target="#b44">[45]</ref>) have only a limited capability to remember long-range dependencies. <ref type="bibr" target="#b1">(2)</ref> Proteins mostly use 20 standard amino acids, 100,000 times less tokens than in the English language. Smaller vocabularies might be problematic if protein sequences encode a similar complexity as sentences. <ref type="bibr" target="#b2">(3)</ref> We found UniRef50 to contain almost ten times more tokens (9.5 billion amino acids) than the largest existing NLP corpus (1 billion words). Simply put: Wikipedia is roughly ten times larger than Webster's Third New International Dictionary and the entire UniProt is over ten times larger than Wikipedia. As a result, larger models might be required to absorb the information in biological databases.</p><p>We trained ELMo on UniRef50 and assessed the predictive power of the embeddings by application to tasks on two levels: per-residue (word-level) and per-protein (sentence-level). For the per-residue prediction task, we predicted secondary structure and long intrinsic disorder. For the per-protein prediction task, we predicted subcellular localization and trained a classifier distinguishing between membrane-bound and water-soluble proteins. We used publicly available data sets from two recent methods that achieved break-through performance through Deep Learning, namely NetSurfP-2.0 for secondary structure <ref type="bibr" target="#b45">[46]</ref> and DeepLoc for localization <ref type="bibr" target="#b46">[47]</ref>. We compared the performance of the SeqVec embeddings to state-of-the-art methods using evolutionary information, and also to a popular embedding tool for protein sequences originating from the Word2vec approach, namely ProtVec <ref type="bibr" target="#b41">[42]</ref>. Notably, while ProtVec captures local information, it loses information on sequence ordering, and the resulting residue embeddings are insensitive to their context (non-contextualized), i.e. the same word results in the same embedding regardless of the specific context.</p><p>Understanding a language typically implies to understand most typical constructs convened in that language. Modeling a language in a computer can have many meanings, spanning from the automatic understanding of the semantic of languages, to parsing some underlying rules of a language (e.g. syntax). Arguably, proteins are the most important machinery of life. Protein sequence largely determines protein structure, which somehow determines protein function <ref type="bibr" target="#b47">[48]</ref>. Thus, the expression of the language of life are essentially protein sequences. Understanding those sequences implies to predict protein structure from sequence. Despite recent successes <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref>, this is still not possible for all proteins. However, the novel approach introduced here succeeds to model protein sequences in the sense that it implicitly extracts grammar-like principles (as embeddings) which are much more successful in predicting aspects of protein structure and function than any of the biophysical features previously used to condensate expert knowledge of protein folding, or any other previously tried simple encoding of protein sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling protein sequences through SeqVec embeddings</head><p>SeqVec, our ELMo-based implementation, was trained for three weeks on 5 Nvidia Titan GPUs with 12 GB memory each. The model was trained until its perplexity (uncertainty when predicting the next token) converged at around 10.5 (Additional file 1: Figure <ref type="figure" target="#fig_0">S1</ref>). Training and testing were not split due to technical limitations (incl. CPU/GPU). ELMo was designed to reduce the risk of overfitting by sharing weights between forward and backward LSTMs and by using dropout. The model had about 93 M (mega/million) free parameters compared to the 9.6G (giga/billion) tokens to predict leading to a ratio of samples/free parameter below 1/100, the best our group has ever experienced in a prediction task. Similar approaches have shown that even todays largest models (750 M free parameters) are not able to overfit on a large corpus (250 M protein sequences) <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SeqVec embeddings appeared robust</head><p>When training ELMo on SWISS-PROT (0.5 M sequences), we obtained less useful models, i.e. the subsequent prediction methods based on those embeddings were less accurate. Training on UniRef50 (33 M sequences) gave significantly better results in subsequent supervised prediction tasks, and we observed similar results when using different hyperparameters. For instance, increasing the number of LSTM layers in ELMo (from two to four) gave a small, non-significant improvement. As the expansion of 2 to 4 layers roughly doubled time for training and retrieving embeddings, we decided to trade speed for insignificant improvement and continued with the faster two-layer ELMo architecture. Computational limitations hindered us from fully completeing the modelling of UniRef90 (100 million sequences). Nevertheless, after four weeks of training, the models neither appeared to be better nor significantly worse than those for UniRef50. Users of the embeddings need to be aware that every time a new ELMo model is trained, the downstream supervised prediction method needs to be retrained in the following sense. Assume we transfer-learn UniRef50 through SeqVec1, then use SeqVec1 to machine learn DeepSeqVec1 for a supervised task (e.g. localization prediction). In a later iteration, we redo the transfer learning with different hyperparameters to obtain SeqVec2. For any given sequence, the embeddings of SeqVec2 will differ from those of SeqVec1, as a result, passing embeddings derived from SeqVec2 to DeepSeqVec1 will not provide meaningful predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-residue performance high, not highest</head><p>NetSurfP-2.0 feeds HHblits or MMseqs2 profiles into advanced combinations of Deep Learning architectures <ref type="bibr" target="#b45">[46]</ref> to predict secondary structure, reaching a threestate per-residue accuracy Q3 of 82-85% (lower value: small, partially non-redundant CASP12 set, upper value: larger, more redundant TS115 and CB513 sets; Table <ref type="table" target="#tab_0">1</ref>, Fig. <ref type="figure" target="#fig_0">1</ref>; several contenders such as Spider3 and RaptorX reach within three standard errors). All six methods developed by us fell short of reaching this mark, both methods not using evolutionary information/profiles (DeepSeqVec, DeepProtVec, DeepOneHot, DeepBLO-SUM65), but also those that did use profiles (DeepProf, DeepProf+SeqVec, Fig. <ref type="figure" target="#fig_0">1a</ref>, Table <ref type="table" target="#tab_0">1</ref>). The logic in our acronyms was as follows (Methods): "Prof" implied using profiles (evolutionary information), SeqVec (Sequence-to-Vector) described using pre-trained ELMo embeddings, "Deep" before the method name suggested applying a simple deep learning method trained on particular prediction tasks using SeqVec embeddings only (DeepSeqVec), profiles without (DeepProf) or with embeddings (Deep-Prof+SeqVec), or other simple encoding schema (ProtVec, OneHot or sparse encoding, or BLOSUM65). When comparing methods that use only single protein sequences as input (DeepSeqVec, DeepProtVec, DeepOneHot, Deep-BLOSUM65; all white in Table <ref type="table" target="#tab_0">1</ref>), the new method introduced here, SeqVec outperformed others not using profiles by three standard errors (P-value&lt; 0.01; Q3: 5-10 percentage points, Q8: 5-13 percentage points, MCC: 0.07-0.12, Table <ref type="table" target="#tab_0">1</ref>). Using a context-independent language model derived from the Word2vec approach, namely DeepProtVec was worse by 10 percentage points (almost six standard errors). On the other hand, our implementation of evolutionary information (DeepProf using HHblits profiles) remained about 4-6 percentage points below NetSurfP-2.0 (Q3 = 76-81%, Fig. <ref type="figure" target="#fig_0">1</ref>, Table <ref type="table" target="#tab_0">1</ref>). Depending on the test set, using SeqVec embeddings instead of evolutionary information (DeepSeqVec: Fig. <ref type="figure" target="#fig_0">1a</ref>, Table <ref type="table" target="#tab_0">1</ref>) remained 2-3 percentage points below that mark (Q3 = 73-79%, Fig. <ref type="figure" target="#fig_0">1a</ref>, Table <ref type="table" target="#tab_0">1</ref>). Using both evolutionary information and SeqVec embeddings (DeepProf+SeqVec) improved over both, but still did not reach the top (Q3 = 77-82%). In fact, the ELMo embeddings alone (DeepSeq-Vec) did not surpass any of the best methods using evolutionary information tested on the same data set (Fig. <ref type="figure" target="#fig_0">1a</ref>). For the prediction of intrinsic disorder, we observed the same: NetSurfP-2.0 performed best; our implementation of evolutionary information (DeepProf) performed worse (Fig. <ref type="figure" target="#fig_0">1b</ref>, Table <ref type="table" target="#tab_0">1</ref>). However, for this task the embeddings alone (DeepSeqVec) performed relatively well, exceeding our in-house implementation of a model using evolutionary information (DeepSeqVec MCC = 0.575-0.591 vs. DeepProf MCC = 0.506-0.516, Table <ref type="table" target="#tab_0">1</ref>). The combination of evolutionary information and embeddings (DeepProf+SeqVec) improved over using evolutionary information alone but did not improve over the SeqVec embeddings for disorder. Compared to other methods, the embeddings alone reached similar values (Fig. <ref type="figure" target="#fig_0">1b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Per-protein performance close to best</head><p>For predicting subcellular localization (cellular compartments) in ten classes, DeepLoc <ref type="bibr" target="#b46">[47]</ref> is top with Q10 = 78% (Fig. <ref type="figure" target="#fig_0">1c</ref>, Table <ref type="table" target="#tab_1">2</ref>). For simplicity, we only tested methods not using evolutionary information/profiles for this task. Our sequence-only embeddings model DeepSeqVec-Loc reached second best performance together with iLoc-Euk <ref type="bibr" target="#b51">[52]</ref> at Q10 = 68% (Fig. <ref type="figure" target="#fig_0">1c</ref>, Table <ref type="table" target="#tab_1">2</ref>). Unlike the per-residue predictions, for this application the SeqVec embeddings outperformed several popular prediction methods that use evolutionary information by up to 13 percentage points in Q10 (Table <ref type="table" target="#tab_1">2</ref>: DeepSeqVec-Loc vs. methods shown in grayed rows). The gain of the context-dependent SeqVec model introduced here over context-independent versions such as ProtVec (from Word2vec) was even more pronounced than for the per-residue prediction task (Q10 68 ± 1% vs. 42 ± 1%).</p><p>Performance for the classification into membranebound and water-soluble proteins followed a similar trend (Fig. <ref type="figure" target="#fig_0">1d</ref>, Table <ref type="table" target="#tab_1">2</ref>): while DeepLoc still performed best (Q2 = 92.3, MCC = 0.844), DeepSeqVec-Loc reached just a few percentage points lower (Q2 = 86.8 ± 1.0,  <ref type="bibr" target="#b46">[47]</ref>) and embeddings based on single sequences (Word2vec-like ProtVec <ref type="bibr" target="#b41">[42]</ref> and our ELMo-based SeqVec). Panel D: the same data set was used to assess the predictive power of SeqVec for the classification of a protein into membrane-bound and water-soluble MCC = 0.725 ± 0.021; full confusion matrix Additional file 1: Figure <ref type="figure">S2</ref>). In contrast to this, ProtVec, another method using only single sequences, performed substantially worse (Q2 = 77.6 ± 1.3, MCC = 0.531 ± 0.026).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing results</head><p>Lack of insight often triggers the misunderstanding that machine learning methods are black box solutions barring understanding. In order to interpret the SeqVec embeddings, we have projected the protein-embeddings of the per-protein prediction data upon two dimensions using t-SNE <ref type="bibr" target="#b52">[53]</ref>. We performed this analysis once for the raw embeddings (SeqVec, Fig. <ref type="figure">2</ref> upper row) and once for the hidden layer representation of the perprotein network (DeepSeqVec-Loc) after training (Fig. <ref type="figure">2</ref> lower row). All t-SNE representations in Fig. <ref type="figure">2</ref> were created using 3000 iterations and the cosine distance as metric. The two analyses differed only in that the perplexity was set to 20 for one (SeqVec) and 15 for the other (DeepSeqVec-Loc). The t-SNE representations were colored either according to their localization within the cell (left column of Fig. <ref type="figure">2</ref>) or according to whether they are membrane-bound or water-soluble (right column).</p><p>Despite never provided during training, the raw embeddings appeared to capture some signal for classifying proteins by localization (Fig. <ref type="figure">2</ref>, upper row, left column). The most consistent signal was visible for extra-cellular proteins. Proteins attached to the cell membrane or located in the endoplasmic reticulum also formed welldefined clusters. In contrast, the raw embeddings neither captured a consistent signal for nuclear nor for mitochondrial proteins. Through training, the network improved the signal to reliably classify mitochondrial and plastid proteins. However, proteins in the nucleus and cell membrane continued to be poorly distinguished via t-SNE.</p><p>Coloring the t-SNE representations for membranebound or water-soluble proteins (Fig. <ref type="figure">2</ref>, right column), revealed that the raw embeddings already provided welldefined clusters although never trained on membrane prediction (Fig. <ref type="figure">2</ref>, upper row). After training, the classification was even better (Fig. <ref type="figure">2</ref>, lower row).</p><p>Analogously, we used t-SNE projections to analyze Seq-Vec embeddings on different levels of complexity inherent to proteins (Fig. <ref type="figure" target="#fig_1">3</ref>), ranging from the building blocks (amino acids, Fig. <ref type="figure" target="#fig_1">3a</ref>), to secondary structure defined protein classes (Fig. <ref type="figure" target="#fig_1">3b</ref>), over functional features (Fig. <ref type="figure" target="#fig_1">3c</ref>), and onto the macroscopic level of the kingdoms of life and viruses (Fig. <ref type="figure" target="#fig_1">3d</ref>; classifications in panels 3b-3d based on SCOPe <ref type="bibr" target="#b53">[54]</ref>). Similar to the results described in <ref type="bibr" target="#b50">[51]</ref>, our projection of the embedding space confirmed that the model successfully captured bio-chemical and biophysical properties on the most fine-grained level, i.e. the 20 standard amino acids (Fig. <ref type="figure" target="#fig_1">3a</ref>). For example, aromatic amino acids (W, F, Y) are well separated from aliphatic amino acids (A, I, L, M, V) and small amino acids (A, C, G, P, S, T) are well separated from large ones (F, H, R, W, Y). The projection of the letter indicating an unknown amino acid (X), clustered closest to the amino acids alanine (A) and glycine (G) (data not shown). Possible explanations for this could be that the two amino acids with the smallest side chains might be least biased towards other biochemical features like charge and that they are the 2nd (A) and 4th (G) most frequent amino acids in our training set (Additional file 1: Table <ref type="table" target="#tab_0">S1</ref>). Rare (O, U) and ambiguous amino acids (Z, B) were removed from the projection as their clustering showed that the model could not learn reasonable embeddings from the very small number of samples.</p><p>High-level structural classes as defined in SCOPe (Fig. <ref type="figure" target="#fig_1">3b</ref>) were also captured by SeqVec embeddings. Although the embeddings were only trained to predict the next amino acid in a protein sequence, well separated clusters emerged from those embeddings in structure space. Especially, membrane proteins and small proteins formed distinct clusters (note: protein length is not explicitly encoded in SeqVec). Also, these results indicated that the embeddings captured complex relationships between proteins which are not directly observable from sequence similarity alone as SCOPe was redundancy reduced at 40% sequence identity. Therefore, the new embeddings could complement sequence-based structural classification as it was shown that the sequence similarity does not necessarily lead to structural similarity <ref type="bibr" target="#b54">[55]</ref>. To further investigate the clusters emerging from the SCOPe data set, we colored the same data set based on protein functions (Fig. <ref type="figure" target="#fig_1">3c</ref>) and kingdoms (Fig. <ref type="figure" target="#fig_1">3d</ref>). This analysis revealed that many of the small, distinct clusters emerged based on protein functions. For instance, transferases and hydrolases formed many small clusters. When increasing the level of abstraction by coloring the proteins according to their kingdoms, we observed certain clusters to be dominated by e.g. eukaryotes. Comparing the different views captured in panels 3B-3D revealed connections, e.g. that all-beta or small proteins dominate in eukaryotes (compare blue and orange islands in Fig. <ref type="figure" target="#fig_1">3b</ref> with the same islands in Fig. <ref type="figure" target="#fig_1">3d</ref> colored blue to mark eukaryotes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CPU/GPU time used</head><p>Due to the sequential nature of LSTMs, the time required to embed a protein grows linearly with protein length. Depending on the available main memory or GPU memory, this process could be massively parallelized. To optimally use available memory, batches are typically based on tokens rather than on sentences. In order to retrieve embeddings, we sorted proteins according to their length and created batches of ≤15 K tokens that could still be handled by a single Nvidia GeForce GTX1080 with 8GB VRAM. The processing of a single protein took on average 0.027 s when applying this batch-strategy to the NetSurfP-2.0 data set (average protein length: 256 residues, i.e. shorter than proteins for which 3D structure is not known). The batch with the shortest proteins (on average 38 residues, corresponding to 15% of the average protein length in the whole data set) required about one tenth (0.003 s per protein, i.e. 11% of that for whole set). The batch containing the longest protein sequences in this data set (1578 residues on average, corresponding to 610% of average protein length in the whole data set), took about six times more (1.5 s per protein, i.e. 556% of that for whole set). When creating SeqVec for the DeepLoc set (average length: 558 residues; as this set does not require Fig. <ref type="figure">2</ref> t-SNE representations of SeqVec. Shown are t-SNE projections from embedded space onto a 2D representation; upper row: unsupervised 1024-dimensional "raw" ELMo-based SeqVec embeddings, averaged over all residues in a protein; lower row: supervised 32-dimensional ELMobased SeqVec embeddings, reduced via per-protein machine learning predictions (data: redundancy reduced set from DeepLoc). Proteins were colored according to their localization (left column) or whether they are membrane-bound or water-soluble (right column). Left and right panel would be identical except for the color, however, on the right we had to leave out some points due to lacking membrane/non-membrane annotations. The upper row suggests that SeqVec embeddings capture aspects of proteins without ever seeing labels of localization or membrane, i.e. without supervised training. After supervised training (lower row), this information is transferred to, and further distilled by networks with simple architectures. After training, the power of SeqVeq embeddings to distinguish aspects of function and structure become even more pronounced, sometimes drastically so, as suggested by the almost fully separable clusters in the lower right panel a 3D structure, it provides a more realistic view on the distribution of protein lengths), the average processing time for a single protein was 0.08 with a minimum of 0.006 for the batch containing the shortest sequences (67 residues on average) and a maximum of 14.5 s (9860 residues on average). On a single Intel i7-6700 CPU with 64GB RAM, processing time increased by roughly 50% to 0.41 s per protein, with a minimum and a maximum computation time of 0.06 and 15.3 s, respectively. Compared to an average processing time of one hour for 1000 proteins when using evolutionary information directly <ref type="bibr" target="#b45">[46]</ref>, this implied an average speed up of 120-fold on a single GeForce GTX1080 and 9-fold on a single i7-6700 when predicting structural features; the inference time of DeepSeqVec for a single protein is on average 0.0028 s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer-learning alone not top</head><p>The context-dependent transfer-learning model ELMo <ref type="bibr" target="#b40">[41]</ref> applied to proteins sequences (here dubbed SeqVec) clearly succeeded to model the language of protein sequences much better than simple schema (e.g. one-hot encoding), more advanced context-independent language models such as ProtVec (based on Word2vec <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>), more advanced distillations of text-book knowledge (biophysical features used as input for prediction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>), and also some family-independent information about evolution as represented by the expertise condensed in the BLOSSUM62 matrix. In this sense, our approach worked. However, none of our SeqVec implementations reached today's best methods: NetSurfP-2.0 for secondary structure and protein disorder and DeepLoc for localization and membrane protein classification (Fig. <ref type="figure" target="#fig_0">1</ref>, Table <ref type="table" target="#tab_0">1</ref>, Table <ref type="table" target="#tab_1">2</ref>). Clearly, "just" using SeqVec embeddings to train subsequent prediction methods did not suffice to crack the challenges. Due to computational limitations, testing models trained on larger sequence database, which may over-come this limitation, could not be tested. What about more advanced transferlearning models, e.g. TransformerXL <ref type="bibr" target="#b55">[56]</ref>, or different pre-training objectives which model bidirectional contexts, e.g. Bert <ref type="bibr" target="#b56">[57]</ref> or XLNet <ref type="bibr" target="#b57">[58]</ref>? We have some evidence that transformer-based models might reach further (Elnaggar et al. in preparation), with competing groups already showing promising results <ref type="bibr" target="#b50">[51]</ref>. Nevertheless, there is one major reality to remember: we model single protein sequences. Such models might learn the rules for "writing protein sequences" and still miss the constraints imposed by the "survival of the fittest", i.e. by evolutionary selection.</p><p>On the other hand, some of our solutions appeared surprisingly competitive given the simplicity of the architectures. In particular, for the per-protein predictions, for which SeqVec clearly outperformed the previously popular ProtVec <ref type="bibr" target="#b41">[42]</ref> approach and even commonly used expert solutions (Fig. <ref type="figure" target="#fig_0">1</ref>, Table <ref type="table" target="#tab_1">2</ref>: no method tested other than the top-of-the-line DeepLoc reached higher numerical values). For that comparison, we used the same data sets but could not rigorously compare standard errors (SE) that were unavailable for other methods. Estimating standard errors for our methods suggested the differences to be statistically significant: &gt; 7 SE throughout (exception: DeepLoc (Q10 = 78) and iLoc-Euk(Q10 = 68)). The results for localization prediction implied that frequently used methods using evolutionary information (all marked with shaded boxes in Table <ref type="table" target="#tab_1">2</ref>) did not clearly outperform our simple ELMo-based tool (DeepSeqVec-Loc in Table <ref type="table" target="#tab_1">2</ref>). This was very different for the perresidue prediction tasks: here almost all top methods using evolutionary information numerically outperformed the simple model built on the ELMo embeddings (DeepSeqVec in Fig. <ref type="figure" target="#fig_0">1</ref> and Table <ref type="table" target="#tab_0">1</ref>). However, all models introduced in this work were deliberately designed to be relatively simple to demonstrate the predictive power of SeqVec. More sophisticated architectures building up on SeqVec embeddings will likely outperform the approaches introduced here.</p><p>Combining SeqVec with evolutionary information for per-residue predictions still did not reach the top (set TS115: Q3(NetSurfP-2.0) = 85.3% vs. Q3(DeepProf + SeqVec) = 82.4%, Table <ref type="table" target="#tab_0">1</ref>). This might suggest some limit for the usefulness of the ELMo-based SeqVec embeddings. However, it might also point to the more advanced solutions realized by NetSurfP-2.0 which applies two LSTMs of similar complexity as our entire system (including ELMo) on top of their last step leading to 35 M (35 million) free parameters compared to about 244 K for DeepProf + SeqVec. Twenty times more free parameters might explain some fraction of the success. Due to limited GPU resources, we could not test how much.</p><p>Why did the ELMo-based approach improve more (relative to competition) for per-protein than for perresidue predictions? We can only speculate because none of the possible explanations have held consistently for all methods to which we have been applying ELMo embeddings over the recent six months (data not shown). For instance, the per-protein data sets were over two orders of magnitude smaller than those for perresidue predictions; simply because every protein constitutes one sample in the first and protein length samples for the second. SeqVec might have helped more for the smaller data sets because the unlabeled data is pre-processed so meaningful that less information needs to be learned by the ANN during per-protein prediction. This view was strongly supported by the t-SNE <ref type="bibr" target="#b52">[53]</ref> results (Fig. <ref type="figure">2</ref>, Fig. <ref type="figure" target="#fig_1">3</ref>): ELMo apparently had learned the "grammar" of the language of life well enough to realize a very rough clustering of structural classes, protein function, localization and membrane/not. Another, yet complementary, explanation for this trend could be that the training of ELMo inherently provides a natural way of summarizing information of proteins of varying length. Other approaches usually learn this summarization step together with the actual prediction tasks which gets increasingly difficult the smaller the data set.</p><p>We picked four tasks as proof-of-principle for our ELMo/SeqVec approach. These tasks were picked because recent breakthroughs had been reported (e.g. NetSurfP-2.0 <ref type="bibr" target="#b45">[46]</ref> and DeepLoc <ref type="bibr" target="#b46">[47]</ref>) and those had made data for training and testing publicly available. We cannot imagine why our findings should not hold true for other tasks of protein prediction and invite the community to apply the SeqVec embeddings for their tasks. We assume the SeqVec embeddings to be more beneficial for small than for large data sets. For instance, we expect little or no gain in predicting inter-residue contacts, and more in predicting protein binding sites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Good and fast predictions without using evolutionary information</head><p>Although our SeqVec embeddings were over five percentage points worse than the best method NetSurfP-2.0 (Table <ref type="table" target="#tab_0">1</ref>: TS115 Q3: 85.3 vs. 79.1), for some proteins (12% in CB513) DeepSeqVec performed better (Additional file 1: Figure <ref type="figure">S4</ref>). We expect those to be proteins with small or incorrect alignments, however, due to the fact that we did not have the alignments available used by NetSurfP-2.0, we could not quite establish the validity of this assumption (analyzing pre-computed alignments from ProteinNet <ref type="bibr" target="#b58">[59]</ref> revealed no clear relation of the type: more evolutionary information leads to better prediction). However, the real strength of our solutions is its speed: SeqVec predicted secondary structure and protein disorder over 100-times faster (on a single 8GB GPU) than NetSurfP-2.0 when counting the time it needs to retrieve the evolutionary information summarized in alignment profiles although using the fastest available alignment method, namely MMseqs2 <ref type="bibr" target="#b35">[36]</ref> which already can reach speed-up values of 100-times over PSI-BLAST <ref type="bibr" target="#b32">[33]</ref>. For those who do not have enough resources for running MMSeqs2 and therefore have to rely on PSI-BLAST, the speed-up of our prediction becomes 10,000-fold. Even the 100-fold speed-up is so substantial that for some applications, the speedup might outweigh the reduction in performance. Embedding based approaches such as SeqVec suggest a promising solution toward solving one of the biggest challenges for computational biology: how to efficiently handle the exponentially increasing number of sequences in protein databases? Here, we showed that relevant information from large unannotated biological databases can be compressed into embeddings that condense and abstract the underlying biophysical principles. These embeddings, essentially the weights of a neural network, help as input to many problems for which smaller sets of annotated data are available (secondary structure, disorder, localization). Although the compression step needed to build the SeqVec model is very GPU-intensive, it can be performed in a centralized way using large clusters. After training, the model can be shipped and used on any consumer hardware. Such solutions are ideal to support researches without access to expensive cluster infrastructure.</p><p>Modeling the language of life? SeqVec, our pre-trained ELMo adaption, learned to model a probability distribution over a protein sequence. The sum over this probability distribution constituted a very informative input vector for any machine learning task trying to predict protein features. It also picked up context-dependent protein motifs without explicitly explaining what these motifs are relevant for. In contrast, context-independent tools such as ProtVec <ref type="bibr" target="#b41">[42]</ref> will always create the same vectors regardless of the residues surrounding this k-mer in a protein sequence.</p><p>Our hypothesis had been that the ELMo-based SeqVec embeddings trained on large databases of un-annotated protein sequences could extract a probabilistic model of the language of life in the sense that the resulting system will extract aspects relevant both for per-residue and per-protein prediction tasks. All results presented here have added independent evidence in full support of this hypothesis. For instance, the three state per-residue accuracy for secondary structure prediction improved by over eight percentage points through ELMo (Table <ref type="table" target="#tab_0">1</ref>, e.g. Q3: 79.1 vs. 70.3%), the per-residue MCC for protein disorder prediction also increased substantially (Table <ref type="table" target="#tab_0">1</ref>, e.g. MCC: 0.591 vs. 0.488). On the per-protein level, the improvement over the previously popular tool extracting "meaning" from proteins, ProtVec, was even more substantial (Table <ref type="table" target="#tab_0">1</ref>: e.g. Q10: 68% vs. 42%). We could demonstrate this reality even more directly using the t-SNE <ref type="bibr" target="#b52">[53]</ref> results (Fig. <ref type="figure">2</ref> and Fig. <ref type="figure" target="#fig_1">3</ref>): different levels of complexity ranging from single amino acids, over some localizations, structural features, functions and the classification of membrane/non-membrane had been implicitly learned by SeqVec without training. Clearly, our ELMo-driven implementation of transfer-learning fully succeeded to model some aspects of the language of life as proxied by protein sequences. How much more will be possible? Time will tell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have shown that it is possible to capture and transfer knowledge, e.g. biochemical or biophysical properties, from a large unlabeled data set of protein sequences to smaller, labelled data sets. In this first proof-of-principle, our comparably simple models have already reached promising performance for a variety of per-residue and per-protein prediction tasks obtainable from only single protein sequences as input, that is: without any direct evolutionary information, i.e. without profiles from multiple sequence alignments of protein families. This reduces the dependence on the time-consuming and computationally intensive calculation of protein profiles, allowing the prediction of per-residue and per-protein features of a whole proteome within less than an hour. For instance, on a single GeForce GTX 1080, the creation of embeddings and predictions of secondary structure and subcellular localization for the whole human proteome took about 32 min. Building more sophisticated architectures on top of SeqVec might increase sequence-based performance further.</p><p>Our new SeqVec embeddings may constitute an ideal starting point for many different applications in particular when labelled data are limited. The embeddings combined with evolutionary information might even improve over the best available methods, i.e. enable high-quality predictions. Alternatively, they might ease high-throughput predictions of whole proteomes when used as the only input feature. Alignment-free predictions bring speed and improvements for proteins for which alignments are not readily available or limited, such as for intrinsically disordered proteins, for the Dark Proteome, or for particular unique inventions of evolution. The trick was to tap into the potential of Deep Learning through transfer learning from large repositories of unlabeled data by modeling the language of life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>UniRef50 training of SeqVec: We trained ELMo on UniRef50 <ref type="bibr" target="#b31">[32]</ref>, a sequence redundancy-reduced subset of the UniProt database clustered at 50% pairwise sequence identity (PIDE). It contained 25 different letters (20 standard and 2 rare amino acids (U and O) plus 3 special cases describing either ambiguous (B, Z) or unknown amino acids (X); Additional file 1: Table <ref type="table" target="#tab_0">S1</ref>) from 33 M proteins with 9,577,889,953 residues. In order to train ELMo, each protein was treated as a sentence and each amino acid was interpreted as a single word.</p><p>Visualization of embedding space: The current release of the "Structural Classification Of Proteins" (SCOPe, <ref type="bibr" target="#b53">[54]</ref>) database (2.07) contains 14,323 proteins at a redundancy level of 40%. Functions encoded by the Enzyme Commission number (E.C., <ref type="bibr" target="#b59">[60]</ref>) were retrieved via the "Structure Integration with Function, Taxonomy and Sequence" (SIFTS) mapping <ref type="bibr" target="#b60">[61]</ref>. SIFTS allows, among other things, a residue-level mapping between UniProt and PDB entries and a mapping from PDB identifiers to E.C.s. If no function annotation was available for a protein or if the same PDB identifier was assigned to multiple E.C.s, it was removed from Fig. <ref type="figure" target="#fig_1">3c</ref>. Taxonomic identifiers from UniProt were used to map proteins to one of the 3 kingdoms of life or to viruses. Again, proteins were removed if no such information was available. The number of iterations for the t-SNE projections was set again to 3000 and the perplexity was adjusted (perplexity = 5 for Fig. <ref type="figure" target="#fig_1">3a</ref> and perplexity = 30 for Fig. <ref type="figure" target="#fig_1">3b-d</ref>).</p><p>Per-residue level: secondary structure &amp; intrinsic disorder (NetSurfP-2.0). To simplify comparability, we used the data set published with a recent method seemingly achieving the top performance of the day in secondary structure prediction, namely NetSurfP-2.0 <ref type="bibr" target="#b45">[46]</ref>. Performance values for the same data set exist also for other recent methods such as Spider3 <ref type="bibr" target="#b61">[62]</ref>, RaptorX <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref> and JPred4 <ref type="bibr" target="#b64">[65]</ref>. The set contains 10,837 sequence-unique (at 25% PIDE) proteins of experimentally known 3D structures from the PDB <ref type="bibr" target="#b65">[66]</ref> with a resolution of 2.5 Å (0.25 nm) or better, collected by the PISCES server <ref type="bibr" target="#b66">[67]</ref>. DSSP <ref type="bibr" target="#b67">[68]</ref> assigned secondary structure and intrinsically disordered residues are flagged (residues without atomic coordinates, i.e. REMARK-465 in the PDB file). The original seven DSSP states (+ 1 for unknown) were mapped upon three states using the common convention: [G,H, I] → H (helix), [B,E] → E (strand), all others to O (other; often misleadingly referred to as coil or loop). As the authors of NetSurfP-2.0 did not include the raw protein sequences in their public data set, we used the SIFTS file to obtain the original sequence. Only proteins with identical length in SIFTS and NetSurfP-2.0 were used. This filtering step removed 56 sequences from the training set and three from the test sets (see below: two from CB513, one from CASP12 and none from TS115). We randomly selected 536 (~5%) proteins for early stopping (cross-training), leaving 10,256 proteins for training. All published values referred to the following three test sets (also referred to as validation set): TS115 <ref type="bibr" target="#b68">[69]</ref>: 115 proteins from high-quality structures (&lt; 3 Å) released after 2015 (and at most 30% PIDE to any protein of known structure in the PDB at the time); CB513 <ref type="bibr" target="#b69">[70]</ref>: 513 nonredundant sequences compiled 20 years ago (511 after SIFTS mapping); CASP12 <ref type="bibr" target="#b70">[71]</ref>: 21 proteins taken from the CASP12 free-modelling targets (20 after SIFTS mapping; all 21 fulfilled a stricter criterion toward nonredundancy than the two other sets; non-redundant with respect to all 3D structures known until May 2018 and all their relatives). Each of these sets covers different aspects of the secondary structure prediction problem: CB513 and TS115 only use structures determined by Xray crystallography and apply similar cutoffs with respect to redundancy (30%) and resolution (2.5-3.0 Å). While these serve as a good proxy for a baseline performance, CASP12 might better reflect the true generalization capability for unseen proteins as it includes structures determined via NMR and Cryo-EM. Also, the strict redundancy reduction based on publication date reduces the bias towards well studied families. Nevertheless, toward our objective of establishing a proof-of-principle, these sets sufficed. All test sets had fewer than 25% PIDE to any protein used for training and cross-training (ascertained by the NetSurfP-2.0 authors). To compare methods using evolutionary information and those using our new word embeddings, we took the HHblits profiles published along with the NetSurfP-2.0 data set.</p><p>Per-protein level: subcellular localization &amp; membrane proteins (DeepLoc). Subcellular localization prediction was trained and evaluated using the DeepLoc data set <ref type="bibr" target="#b46">[47]</ref> for which performance was measured for several methods, namely: LocTree2 <ref type="bibr" target="#b71">[72]</ref>, MultiLoc2 <ref type="bibr" target="#b72">[73]</ref>, Sher-Loc2 <ref type="bibr" target="#b73">[74]</ref>, CELLO <ref type="bibr" target="#b74">[75]</ref>, iLoc-Euk <ref type="bibr" target="#b51">[52]</ref>, WoLF PSORT <ref type="bibr" target="#b75">[76]</ref> and YLoc <ref type="bibr" target="#b76">[77]</ref>. The data set contained proteins from UniProtKB/Swiss-Prot <ref type="bibr" target="#b77">[78]</ref> (release: 2016_04) with experimental annotation (code: ECO:0000269). The Dee-pLoc authors mapped these annotations to ten classes, removing all proteins with multiple annotations. All these proteins were also classified into water-soluble or membrane-bound (or as unknown if the annotation was ambiguous). The resulting 13,858 proteins were clustered through PSI-CD-HIT <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref> (version 4.0; at 30% PIDE or Eval&lt; 10 − 6 ). Adding the requirement that the alignment had to cover 80% of the shorter protein, yielded 8464 clusters. This set was split into training and testing by using the same proteins for testing as the authors of DeepLoc. The training set was randomly subdivided into 90% for training and 10% for determining early stopping (cross-training set).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding terminology and related work</head><p>One-hot encoding (also known as sparse encoding) assigns each word (referred to as token in NLP) in the vocabulary an integer N used as the Nth component of a vector with the dimension of the vocabulary size (number of different words). Each component is binary, i.e. either 0 if the word is not present in a sentence/text or 1 if it is. This encoding drove the first application of machine learning that clearly improved over all other methods in protein prediction <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>. TF-IDF represents tokens as the product of "frequency of token in data set" times "inverse frequency of token in document". Thereby, rare tokens become more relevant than common words such as "the" (so called stop words). This concept resembles that of using k-mers for database searches <ref type="bibr" target="#b32">[33]</ref>, clustering <ref type="bibr" target="#b80">[81]</ref>, motifs <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref>, and prediction methods <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b83">[84]</ref><ref type="bibr" target="#b84">[85]</ref><ref type="bibr" target="#b85">[86]</ref><ref type="bibr" target="#b86">[87]</ref><ref type="bibr" target="#b87">[88]</ref>. Context-insensitive word embeddings replaced expert features, such as TF-IDF, by algorithms that extracted such knowledge automatically from unlabeled corpus such as Wikipedia, by either predicting the neighboring words, given the center word (skip-gram) or vice versa (CBOW). This became known in Word2Vec <ref type="bibr" target="#b42">[43]</ref> and showcased for computational biology through ProtVec <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b88">89]</ref>. ProtVec assumes that every token or word consists of three consecutive residues (amino acid 3-mers). During training, each protein sequence in SwissProt <ref type="bibr" target="#b77">[78]</ref> is split into overlapping 3mers and the skip-gram version of word2vec is used to predict adjacent 3-mers, given the 3-mer at the center. After training, protein sequences can be split into overlapping 3-mers which are mapped onto a 100dimensional latent space. More specialized implementations are mut2vec <ref type="bibr" target="#b89">[90]</ref> learning mutations in cancer, and phoscontext2vec <ref type="bibr" target="#b90">[91]</ref> identifying phosphorylation sites. Even though the performance of context-insensitive approaches was pushed to its limits by adding sub-word information (FastText <ref type="bibr" target="#b91">[92]</ref>) or global statistics on word co-occurance (GloVe <ref type="bibr" target="#b92">[93]</ref>), their expressiveness remained limited because the models inherently assigned the same vector to the same word, regardless of its context. Context-sensitive word embeddings started a new wave of word embedding techniques for NLP in 2018: the embedding renders the meaning of words and phrases such as "paper tiger" dependent upon the context, allowing to account for the ambiguous meanings of words. Popular examples like ELMo <ref type="bibr" target="#b40">[41]</ref> and Bert <ref type="bibr" target="#b56">[57]</ref> have achieved state-of-the-art results in several NLP tasks. Both require substantial GPU computing power and time to be trained from scratch. One of the main differences between ELMo and Bert is their pre-training objective: while auto-regressive models like ELMo predict the next word in a sentence given all previous words, autoencoder-based models like Bert predict masked-out words given all words which were not masked out. However, in this work we focused on ELMo as it allows processing of sequences of variable length. The original ELMo model consists of a single, contextinsensitive CharCNN <ref type="bibr" target="#b93">[94]</ref> over the characters in a word and two layers of bidirectional LSTMs that introduce the context information of surrounding words (Fig. <ref type="figure">4</ref>). The CharCNN transforms all characters within a single word via an embedding layer into vector space and runs multiple CNNs of varying window size (here: ranging from 1 to 7) and number of filters (here: 32, 64, …, 1024). In order to obtain a fixed-dimensional vector for each word, regardless of its length, the output of the CNNs is max-pooled and concatenated. This feature is crucial for NLP in order to be able to process words of variable length. As our words consist only of single amino acids, this layer learns an uncontextualized mapping of single amino acids onto a latent space. The first bi-directional LSTM operates directly on the output of the CharCNN, while the second LSTM layer takes the output of the first LSTM as input. Due to their sequential nature, the LSTM layers render the embeddings dependent on their context as their internal state always depends on the previous hidden state. However, the bidirectionality of the LSTMs would lead to information leakage, rendering the training objective trivial, i.e. the backward pass had already seen the word which needs to be predicted in the forward pass. This problem is solved by training the forward and the backward pass of the LSTMs independently, i.e. the forward pass is conditioned only on words to its left and vice versa. During inference the internal states of both directions are concatenated allowing the final embeddings to carry information from both sides of the context. As described in the original ELMo publication, the weights of the forward and the backward model are shared in order to reduce the memory overhead of the model and to combat overfitting. Even though, the risk of overfitting is small due to the high imbalance between number of trainable parameters (93 M) versus number of tokens (9.3B), dropout at a rate of 10% was used to reduce the risk of overfitting. This model is trained to predict the next amino acid given all previous amino acids in a protein sequence. To the best of our knowledge, the contextsensitive ELMo has not been adapted to protein sequences, yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELMo adaptation</head><p>In order to adapt ELMo <ref type="bibr" target="#b40">[41]</ref> to protein sequences, we used the standard ELMo configuration with the following changes: (i) reduction to 28 tokens (20 standard and 2 rare (U,O) amino acids + 3 special tokens describing ambiguous (B,Z) or unknown (X) amino acids + 3 special tokens for ELMo indicating padded elements ('&lt; MASK&gt;') or the beginning ('&lt;S&gt;') or the end of a sequence ('&lt;/S&gt;')), (ii) increase number of unroll steps to 100 to account for the increased length of protein sequences compared to sentences in natural languages, (iii) decrease number of negative samples to 20, (iv) increase token number to 9,577,889,953. After pre-training the ELMo architecture (1 CharCNN, 2 LSTM-Layers, see "Embedding terminology and related work" section and Fig. <ref type="figure">4</ref> for more details) with our parameters on Uni-Ref50, the embedding model takes a protein sequence of arbitrary length and returns 3076 features for each residue in the sequence. These 3076 features were derived by concatenating the outputs of the three layers of ELMo, each describing a token with a vector of length 1024. The LSTM layers were composed of the embedding of the forward pass (first 512 dimensions) and the backward pass (last 512 dimensions). In order to demonstrate the general applicability of ELMo or SeqVec and to allow for easy integration into existing models, we neither fine-tuned the pre-trained model on a specific prediction task, nor optimized the combination of the three internal layers. Thus, researchers could just replace (or concatenate) their current machine learning inputs with our embeddings to boost their task-specific performance. Furthermore, it will simplify the development of custom models that fit other use-cases. For simplicity, we summed the components of the three 1024-dimensional vectors to form a single 1024dimensional feature vector describing each residue in a protein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Using SeqVec for predicting protein features</head><p>On the per-residue level, the predictive power of the new SeqVec embeddings was demonstrated by training a small two-layer Convolutional Neural Network (CNN) in PyTorch using a specific implementation <ref type="bibr" target="#b94">[95]</ref> of the ADAM optimizer <ref type="bibr" target="#b95">[96]</ref>, cross-entropy loss, a learning rate of 0.001 and a batch size of 128 proteins. The first layer (in analogy to the sequence-to-structure network of earlier solutions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>) consisted of 32-filters each with a sliding window-size of w = 7. The second layer (structure-to-structure <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>) created the final predictions by applying again a CNN (w = 7) over the output of the first layer. These two layers were connected through a rectified linear unit (ReLU) and a dropout layer <ref type="bibr" target="#b96">[97]</ref> with a dropout-rate of 25% (Fig. <ref type="figure" target="#fig_2">5</ref>, left panel). This simple architecture was trained independently on six different types of input, resulting in different number of free parameters. (i) DeepProf (14,000 = 14 k free parameters): Each residue was described by a vector of size 50 which included a one-hot encoding (20 features), the profiles of evolutionary information (20 features) from HHblits as published previously <ref type="bibr" target="#b45">[46]</ref>, the state transition probabilities of the Hidden-Markov-Model (7 features) and 3 features describing the local alignment diversity. (ii) DeepSeqVec (232 k free parameters): Each protein sequence was represented by the output of SeqVec. The Fig. <ref type="figure">4</ref> ELMo-based architecture adopted for SeqVec. First, an input sequence, e.g. "S E Q W E N C E" (shown at bottom row), is padded with special tokens indicating the start ("&lt;start&gt;") and the end ("&lt;end&gt;") of the sentence (here: protein sequences). On the 2nd level (2nd row from bottom), character convolutions (CharCNN, <ref type="bibr" target="#b93">[94]</ref>) map each word (here: amino acid) onto a fixed-length latent space (here: 1024-dimensional) without considering information from neighboring words. On the third level (3rd row from bottom), the output of the CharCNN-layer is used as input by a bidirectional Long Short Term Memory (LSTM, <ref type="bibr" target="#b44">[45]</ref>) which introduces context-specific information by processing the sentence (protein sequence) sequentially. For simplicity, only the forward pass of the bi-directional LSTM-layer is shown (here: 512-dimensional). On the fourth level (4th row from bottom), the second LSTM-layer operates directly on the output of the first LSTM-layer and tries to predict the next word given all previous words in a sentence. The forward and backward pass are optimized independently during training in order to avoid information leakage between the two directions. During inference, the hidden states of the forward and backward pass of each LSTM-layer are concatenated to a 1024-dimensional embedding vector summarizing information from the left and the right context resulting embedding described each residue as a 1024dimensional vector. (iii) DeepProf+SeqVec (244 k free parameters): This model simply concatenated the input vectors used in (i) and (ii). (iv) DeepProtVec (25 k free parameters): Each sequence was split into overlapping 3mers each represented by a 100-dimensional ProtVec <ref type="bibr" target="#b41">[42]</ref>. (v) DeepOneHot (7 k free parameters): The 20 amino acids were encoded as one-hot vectors as described above. Rare amino acids were mapped to vectors with all components set to 0. Consequently, each protein residue was encoded as a 20-dimensional one-hot vector. (vi) DeepBLOSUM65 (8 k free parameters): Each protein residue was encoded by its BLOSUM65 substitution matrix <ref type="bibr" target="#b97">[98]</ref>. In addition to the 20 standard amino acids, BLOSUM65 also contains substitution scores for the special cases B, Z (ambiguous) and X (unknown), resulting in a feature vector of length 23 for each residue.</p><p>On the per-protein level, a simple feed-forward neural network was used to demonstrate the power of the new embeddings. In order to ensure equal-sized input vectors for all proteins, we averaged over the 1024-dimensional embeddings of all residues in a given protein resulting in a 1024-dimensional vector representing any protein in the data set. ProtVec representations were derived the same way, resulting in a 100-dimensional vector. These vectors (either 100-or 1024 dimensional) were first compressed to 32 features, then dropout with a dropout rate of 25%, batch normalization <ref type="bibr" target="#b98">[99]</ref> and a rectified linear Unit (ReLU) were applied before the final prediction (Fig. <ref type="figure" target="#fig_2">5</ref>, right panel). In the following, we refer to the models trained on the two different input types as (i) DeepSeqVec-Loc (33 k free parameters): average over SeqVec embedding of a protein as described above and (ii) DeepProtVec-Loc (320 free parameters): average over ProtVec embedding of a protein. We used the following hyper-parameters: learning rate: 0.001, Adam optimizer with cross-entropy loss, batch size: 64. The losses of the individual tasks were summed before backpropagation. Due to the relatively small number of free parameters in our models, the training of all networks completed on a single Nvidia GeForce GTX1080 within a few minutes (11 s for DeepProtVec-Loc, 15 min for DeepSeqVec).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation measures</head><p>To simplify comparisons, we ported the evaluation measures from the publications we derived our data sets from, i.e. those used to develop NetSurfP-2.0 <ref type="bibr" target="#b45">[46]</ref> and DeepLoc <ref type="bibr" target="#b46">[47]</ref>. All numbers reported constituted averages over all proteins in the final test sets. This work aimed at a proof-of-principle that the SeqVec embedding contain predictive information. In the absence of any claim for state-of-the-art performance, we did not calculate any significance values for the reported values.</p><p>Per-residue performance: Toward this end, we used the standard three-state per-residue accuracy (Q3 = percentage correctly predicted in either helix, strand, other <ref type="bibr" target="#b1">[2]</ref>) along with its eight-state analog (Q8). Predictions of intrinsic disorder were evaluated through the Matthew's correlation coefficient (MCC <ref type="bibr" target="#b99">[100]</ref>) and the False-Positive Rate (FPR) as those are more informative for tasks with high class imbalance. For completeness, we also provided the entire confusion matrices for both secondary structure prediction problems (Additional file 1: Figure <ref type="figure">S2</ref>). Standard errors were calculated over the distribution of each performance measure for all proteins. Per-protein performance: The predictions whether a protein was membrane-bound or water-soluble were evaluated by calculating the two-state per set accuracy (Q2: percentage of proteins correctly predicted), and the MCC. A generalized MCC using the Gorodkin measure <ref type="bibr" target="#b100">[101]</ref> for K (=10) categories as well as accuracy (Q10), was used to evaluate localization predictions. Standard errors were calculated using 1000 bootstrap samples, each chosen randomly by selecting a sub-set of the predicted test set that had the same size (draw with replacement).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Performance comparisons. The predictive power of the ELMo-based SeqVec embeddings was assessed for per-residue (upper row) and per-protein (lower row) prediction tasks. Methods using evolutionary information are highlighted by hashes above the bars. Approaches using only the proposed SeqVec embeddings are highlighted by stars after the method name. Panel A used three different data sets (CASP12, TS115, CB513) to compare three-state secondary structure prediction (y-axis: Q3; all DeepX developed here to test simple deep networks on top of the encodings tested; DeepProf used evolutionary information). Panel B compared predictions of intrinsically disordered residues on two data sets (CASP12, TS115; y-axis: MCC). Panel C compared per-protein predictions for subcellular localization between top methods (numbers for Q10 taken from DeepLoc [47]) and embeddings based on single sequences (Word2vec-like ProtVec [42] and our ELMo-based SeqVec). Panel D: the same data set was used to assess the predictive power of SeqVec for the classification of a protein into membrane-bound and water-soluble</figDesc><graphic url="image-4.png" coords="5,63.84,94.85,467.66,304.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3 Modeling aspects of the language of life. 2D t-SNE projections of unsupervised SeqVec embeddings highlight different realities of proteins and their constituent parts, amino acids. Panels B to D are based on the same data set (Structural Classification of Proteinsextended (SCOPe) 2.07, redundancy reduced at 40%). For these plots, only subsets of SCOPe containing proteins with the annotation of interest (enzymatic activity C and kingdom D) may be displayed. Panel A: the embedding space confirms: the 20 standard amino acids are clustered according to their biochemical and biophysical properties, i.e. hydrophobicity, charge or size. The unique role of Cysteine (C, mostly hydrophobic and polar) is conserved. Panel B: SeqVec embeddings capture structural information as annotated in the main classes in SCOPe without ever having been explicitly trained on structural features. Panel C: many small, local clusters share function as given by the main classes in the Enzyme Commission Number (E.C.). Panel D: similarly, small, local clusters represent different kingdoms of life</figDesc><graphic url="image-6.png" coords="8,127.28,94.85,340.76,333.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>Fig.5Prediction tasks' architectures. On the left the architecture of the model used for the per-residue level predictions (secondary structure and disorder) is sketched, on the right that used for per-protein level predictions (localization and membrane/not membrane). The 'X', on the left, indicates that different input features corresponded to a difference in the number of input channels, e.g. 1024 for SeqVec or 50 for profile-based input. The letter 'W' refers to the window size of the corresponding convolutional layer (W = 7 implies a convolution of size 7 × 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-5.png" coords="7,152.41,113.54,308.20,205.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Per-residue predictions: secondary structure and disorder</figDesc><table><row><cell>Data</cell><cell>Prediction task</cell><cell>Secondary structure</cell><cell></cell><cell>Disorder</cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell>Q3 (%)</cell><cell>Q8 (%)</cell><cell>MCC</cell><cell>FPR</cell></row><row><cell>CASP12</cell><cell>NetSurfP-2.0 (hhblits) a,b</cell><cell>82.4</cell><cell>71.1</cell><cell>0.604</cell><cell>0.011</cell></row><row><cell></cell><cell>NetSurfP-1.0 a,b</cell><cell>70.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Spider3 a,b</cell><cell>79.1</cell><cell>-</cell><cell>0.582</cell><cell>0.026</cell></row><row><cell></cell><cell>RaptorX a,b</cell><cell>78.6</cell><cell>66.1</cell><cell>0.621</cell><cell>0.045</cell></row><row><cell></cell><cell>Jpred4 a,b</cell><cell>76.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepSeqVec</cell><cell>73.1 ± 1.3</cell><cell>61.2 ± 1.6</cell><cell>0.575 ± 0.075</cell><cell>0.026 ± 0.008</cell></row><row><cell></cell><cell>DeepProf b</cell><cell>76.4 ± 2.0</cell><cell>62.7 ± 2.2</cell><cell>0.506 ± 0.057</cell><cell>0.022 ± 0.009</cell></row><row><cell></cell><cell>DeepProf + SeqVec b</cell><cell>76.5 ± 1.5</cell><cell>64.1 ± 1.5</cell><cell>0.556 ± 0.080</cell><cell>0.022 ± 0.008</cell></row><row><cell></cell><cell>DeepProtVec</cell><cell>62.8 ± 1.7</cell><cell>50.5 ± 2.4</cell><cell>0.505 ± 0.064</cell><cell>0.016 ± 0.006</cell></row><row><cell></cell><cell>DeepOneHot</cell><cell>67.1 ± 1.6</cell><cell>54.2 ± 2.1</cell><cell>0.461 ± 0.064</cell><cell>0.012 ± 0.005</cell></row><row><cell></cell><cell>DeepBLOSUM65</cell><cell>67.0 ± 1.6</cell><cell>54.5 ± 2.0</cell><cell>0.465 ± 0.065</cell><cell>0.012 ± 0.005</cell></row><row><cell>TS115</cell><cell>NetSurfP-2.0 (hhblits) a,b</cell><cell>85.3</cell><cell>74.4</cell><cell>0.663</cell><cell>0.006</cell></row><row><cell></cell><cell>NetSurfP-1.0 a,b</cell><cell>77.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Spider3 a,b</cell><cell>83.9</cell><cell>-</cell><cell>0.575</cell><cell>0.008</cell></row><row><cell></cell><cell>RaptorX a,b</cell><cell>82.2</cell><cell>71.6</cell><cell>0.567</cell><cell>0.027</cell></row><row><cell></cell><cell>Jpred4 a,b</cell><cell>76.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepSeqVec</cell><cell>79.1 ± 0.8</cell><cell>67.6 ± 1.0</cell><cell>0.591 ± 0.028</cell><cell>0.012 ± 0.001</cell></row><row><cell></cell><cell>DeepProf b</cell><cell>81.1 ± 0.6</cell><cell>68.3 ± 0.9</cell><cell>0.516 ± 0.028</cell><cell>0.012 ± 0.002</cell></row><row><cell></cell><cell>DeepProf + SeqVec b</cell><cell>82.4 ± 0.7</cell><cell>70.3 ± 1.0</cell><cell>0.585 ± 0.029</cell><cell>0.013 ± 0.003</cell></row><row><cell></cell><cell>DeepProtVec</cell><cell>66.0 ± 1.0</cell><cell>54.4 ± 1.3</cell><cell>0.470 ± 0.028</cell><cell>0.011 ± 0.002</cell></row><row><cell></cell><cell>DeepOneHot</cell><cell>70.1 ± 0.8</cell><cell>58.5 ± 1.1</cell><cell>0.476 ± 0.028</cell><cell>0.008 ± 0.001</cell></row><row><cell></cell><cell>Deep BLOSUM65</cell><cell>70.3 ± 0.8</cell><cell>58.1 ± 1.1</cell><cell>0.488 ± 0.029</cell><cell>0.007 ± 0.001</cell></row><row><cell>CB513</cell><cell>NetSurfP-2.0 (hhblits) a,b</cell><cell>85.3</cell><cell>72.0</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>NetSurfP-1.0 a,b</cell><cell>78.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Spider3 a,b</cell><cell>84.5</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>RaptorX a,b</cell><cell>82.7</cell><cell>70.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Jpred4 a,b</cell><cell>77.9</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepSeqVec</cell><cell>76.9 ± 0.5</cell><cell>62.5 ± 0.6</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepProf b</cell><cell>80.2 ± 0.4</cell><cell>64.9 ± 0.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepProf + SeqVec b</cell><cell>80.7 ± 0.5</cell><cell>66.0 ± 0.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepProtVec</cell><cell>63.5 ± 0.4</cell><cell>48.9 ± 0.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepOneHot</cell><cell>67.5 ± 0.4</cell><cell>52.9 ± 0.5</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>DeepBLOSUM65</cell><cell>67.4 ± 0.4</cell><cell>53.0 ± 0.5</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Per-protein predictions: localization and membrane/ globular</figDesc><table><row><cell>Method</cell><cell cols="2">Localization</cell><cell cols="2">Membrane/globular</cell></row><row><cell></cell><cell cols="3">Q10 (%) Gorodkin (MCC) Q2</cell><cell>MCC</cell></row><row><cell>LocTree2 a,b</cell><cell>61</cell><cell>0.53</cell><cell></cell><cell></cell></row><row><cell>MultiLoc2 a,b</cell><cell>56</cell><cell>0.49</cell><cell></cell><cell></cell></row><row><cell>CELLO a</cell><cell>55</cell><cell>0.45</cell><cell></cell><cell></cell></row><row><cell>WoLF PSORT a</cell><cell>57</cell><cell>0.48</cell><cell></cell><cell></cell></row><row><cell>YLoc a</cell><cell>61</cell><cell>0.53</cell><cell></cell><cell></cell></row><row><cell>SherLoc2 a,b</cell><cell>58</cell><cell>0.51</cell><cell></cell><cell></cell></row><row><cell>iLoc-Euk a,b</cell><cell>68</cell><cell>0.64</cell><cell></cell><cell></cell></row><row><cell>DeepLoc a,b</cell><cell>78</cell><cell>0.73</cell><cell>92.3</cell><cell>0.844</cell></row><row><cell cols="2">DeepSeqVec-Loc 68 ± 1</cell><cell>0.61 ± 0.01</cell><cell cols="2">86.8 ± 1.0 0.725 ± 0.021</cell></row><row><cell cols="2">DeepProtVec-Loc 42 ± 1</cell><cell>0.19 ± 0.01</cell><cell cols="2">77.6 ± 1.3 0.531 ± 0.026</cell></row></table><note>Performance for per-protein prediction of subcellular localization and classifying proteins into membrane-bound and water-soluble. Results marked by a taken from DeepLoc<ref type="bibr" target="#b46">[47]</ref>; the authors provided no standard errors. The results reported for SeqVec and ProtVec were based on single protein sequences, i.e. methods NOT using evolutionary information (neither during training nor testing). All methods using evolutionary information are marked by b ; best in each set marked by bold numbers</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0">Performance comparison for secondary structure (3-vs. 8-classes) and disorder prediction (binary) for the CASP12, TS115 and CB513 data sets. Accuracy (Q3, Q10) is given in percentage. Results marked by a are taken from NetSurfP-2.0<ref type="bibr" target="#b45">[46]</ref>; the authors did not provide standard errors. Highest numerical values in each column in bold letters. Methods DeepSeqVec, DeepProtVec, DeepOneHot and DeepBLOSUM65 use only information from single protein sequences. Methods using evolutionary information (MSA profiles) are marked by b ; these performed best throughout</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank primarily Tim Karl for invaluable help with hardware and software and Inga Weise for support with many other aspects of this work. Last, not least, thanks to all those who deposit their experimental data in public databases, and to those who maintain these databases. 1 Department of Informatics, Bioinformatics &amp; Computational Biology -i12, TUM (Technical University of Munich), Boltzmannstr. 3, 85748 Garching/ Munich, Germany. 2 TUM Graduate School, Center of Doctoral Studies in Informatics and its Applications (CeDoSIA), Boltzmannstr. 11, 85748 Garching, Germany. 3 Leibniz Supercomputing Centre, Boltzmannstr. 1, 85748 Garching/ Munich, Germany. 4 TUM Department of Informatics, Software Engineering and Business Information Systems, Boltzmannstr. 1, 85748 Garching/Munich, Germany. 5 Institute for Advanced Study (TUM-IAS), Lichtenbergstr. 2a, 85748 Garching/Munich, Germany. 6 TUM School of Life Sciences Weihenstephan (WZW), Alte Akademie 8, Freising, Germany. 7 Department of Biochemistry and Molecular Biophysics &amp; New York Consortium on Membrane Protein Structure (NYCOMPS), Columbia University, 701 West, 168th Street, New York, NY 10032, USA.</p></div>
			</div>


			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The pre-trained ELMo-based SeqVec model and a description on how to implement the embeddings into existing methods can be found here: https:// github.com/Rostlab/SeqVec . Accessed 2nd May 2019. Predictions on secondary structure, disorder and subcellular localization based on SeqVec can be accessed under: https://embed.protein.properties . Accessed 2nd May 2019. The NetSurfP-2.0 data set <ref type="bibr" target="#b45">[46]</ref> used for the evaluation of SeqVec on the task of secondary structure and disorder prediction are publicly available under: http://www.cbs.dtu.dk/services/NetSurfP/ . Accessed 2nd May 2019. The DeepLoc data set <ref type="bibr" target="#b46">[47]</ref> used for the evaluation of SeqVec on the task of subcellular localization prediction are publicly available under: http://www. cbs.dtu.dk/services/DeepLoc/data.php . Accessed 2nd May 2019.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>This work was supported by a grant from the Alexander von Humboldt foundation through the German Ministry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung) as well as by a grant from Deutsche Forschungsgemeinschaft (DFG-GZ: RO1320/4-1). We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan GPU used for this research. We also want to thank the LRZ (Leibniz Rechenzentrum) for providing us access to DGX-V1. The funding did not play any role in the design of the study, collection, analysis, and interpretation of data and in writing the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary information</head><p>Supplementary information accompanies this paper at https://doi.org/10. 1186/s12859-019-3220-8.</p><p>Additional file 1: Supporting online material (SOM) for: Modeling aspect of the language of life through transfer-learning protein sequences Authors contributions AE and MH suggested to use ELMo for modeling protein sequences. AE adopted and trained ELMo. MH evaluated SeqVec embeddings on different data sets and tasks. YW helped with discussions about natural language processing. CD implemented the web-interface which allows to access and visualize the predictions and helped to improve the manuscript. DN helped with various problems regarding the code. FM and BR helped with the design of the experiment and to critically improve the manuscript. MH and AE drafted the manuscript and the other authors provided feedback. All authors read and approved the final manuscript.</p><p>Ethics approval and consent to participate Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare that they have no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Jury returns on structure prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="page">540</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Prediction of protein secondary structure at better than 70% accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Biol</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page" from="584" to="599" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved prediction of protein secondary structure by use of sequence profiles and neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="7558" to="7562" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr Opin Struct Biol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="372" to="376" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural networks for secondary structure and structural class predictions</title>
		<author>
			<persName><forename type="first">J-M</forename><surname>Chandonia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Karplus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="275" to="285" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple and fast approach to prediction of protein secondary structure from multiply aligned sequences with accuracy above 70%</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heringa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Argos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2517" to="2525" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combining evolutionary information and neural networks to predict protein secondary structure</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct Funct Genet</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="55" to="72" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting a-helix and b-strand segments of globular proteins</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">V</forename><surname>Solovyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Salamov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Appl Biol Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="661" to="669" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge-based protein secondary structure assignment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Frishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Argos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct Funct Genet</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="566" to="579" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction based on position-specific scoring matrices</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Biol</title>
		<imprint>
			<biblScope unit="volume">292</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting transmembrane beta-barrels in proteomes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bigelow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Przybylski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2566" to="2577" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topology prediction for helical transmembrane proteins at 86% accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Casadio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fariselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1704" to="1718" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transmembrane helix prediction at 95% accuracy</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Casadio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fariselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="521" to="533" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conservation and prediction of solvent accessibility in protein families</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Struct Funct Genet</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="216" to="226" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Protein flexibility and intrinsic disorder</title>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lawson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Protein Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="71" to="80" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Protein flexibility and rigidity predicted from sequence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schlessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="126" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PROFcon: novel prediction of long-range contacts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Punta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2960" to="2968" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimizing long intrinsic disorder predictors with protein evolutionary information</title>
		<author>
			<persName><forename type="first">K</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vucetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dunker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Bioinforma Comput Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="35" to="60" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Natively unstructured loops differ from other loops</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schlessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e140</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Natively unstructured regions in proteins identified from contact predictions. Bioinform</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schlessinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Punta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2376" to="2384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Better prediction of sub-cellular localization by combining evolutionary and structural information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="930" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mimicking cellular sorting improves prediction of subcellular localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mol Biol</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="100" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Networks of high mutual information define the structural proximity of catalytic sites: implications for catalytic residue identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Marino Buslje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teppa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Domenico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delfino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e1000978</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Protein-protein interaction hot spots carved into sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ofran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">e119</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">ISIS: interaction sites identified from sequence. Bioinform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ofran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="e13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A method and server for predicting damaging missense mutations</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Adzhubei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">E</forename><surname>Ramensky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gerasimova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kondrashov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sunyaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="248" to="249" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SNAP: predict effect of non-synonymous polymorphisms on function</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bromberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3823" to="3835" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">All-atom 3D structure prediction of transmembrane β-barrel proteins from sequences</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elofsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="5413" to="5418" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Protein 3D structure computed from evolutionary sequence variation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">e28766</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Protein structure prediction from sequence variation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Biotechnol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1072</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Direct-coupling analysis of residue coevolution captures native contacts across many protein families</title>
		<author>
			<persName><forename type="first">F</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pagnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bertolino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zecchina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Onuchic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Weigt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="E1293" to="E1301" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Suzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Mcgarvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Uniprot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="926" to="932" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gapped Blast and PSI-Blast: a new generation of protein database search programs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Altschul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Schaeffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="3389" to="3402" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HHblits: lightning-fast iterative protein sequence searching by HMM-HMM alignment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Remmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Biegert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="175" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">HH-suite3 for fast remote homology detection and deep protein annotation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirdita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Vohringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Haunsberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Soding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MMseqs2 enables sensitive protein sequence searching for the analysis of massive data sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steinegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Söding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat Biotechnol</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1026</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? Why these proteins are intrinsically disordered</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dunker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Barbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blackledge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Bondos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dosztanyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Dyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Forman-Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fuxreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gsponer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intrinsically Disord Proteins</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">e24157</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Prediction of intrinsic disorder and its use in functional proteomics</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Uversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Radivojac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Iakoucheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Obradovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Dunker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Mol Biol</title>
		<imprint>
			<biblScope unit="volume">408</biblScope>
			<biblScope unit="page" from="69" to="92" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unexpected features of the dark proteome</title>
		<author>
			<persName><forename type="first">N</forename><surname>Perdigao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stolte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Signal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Gloss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hammang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dark proteins important for cellular function</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schafferhans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'donoghue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Heinzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteomics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">1800227</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.05365" />
		<title level="m">Deep contextualized word representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">e0141287</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Characteristics of sentence length in running text</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schils</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Literary Linguist Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="26" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">NetSurfP-2.0: Improved prediction of protein structural features by integrated deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Klausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Jespersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Jurtz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moa</forename><surname>Sommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">DeepLoc: prediction of protein subcellular localization using deep learning</title>
		<author>
			<persName><forename type="first">Almagro</forename><surname>Armenteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Sonderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Winther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">4049</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Principles that govern the folding of protein chains</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Anfinsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="issue">4096</biblScope>
			<biblScope unit="page" from="223" to="230" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improved protein contact predictions with the MetaPSICOV2 server in CASP12</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Buchan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="78" to="83" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">De novo structure prediction with deeplearning based scoring</title>
		<author>
			<persName><forename type="first">R</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jumper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zidek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bridgland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Penedones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu Rev Biochem</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fergus</forename><forename type="middle">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bioRxiv</title>
		<imprint>
			<biblScope unit="page">622803</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">iLoc-Euk: a multi-label classifier for predicting the subcellular localization of singleplex and multiplex eukaryotic proteins</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><forename type="middle">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">e18258</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lvd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SCOPe: structural classification of proteins-extended, integrating SCOP and ASTRAL data and classification of new structures</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-M</forename><surname>Chandonia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D304" to="309" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sequence-similar, structure-dissimilar protein pairs in the PDB</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kosloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kolodny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="891" to="902" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">190102860 2019</date>
		</imprint>
	</monogr>
	<note>length context. arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>arXiv: 181004805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno>arXiv:190608237</idno>
		<title level="m">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">ProteinNet: a standardized data set for machine learning of protein structure</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">311</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The ENZYME database in 2000</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bairoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="305" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">SIFTS: structure integration with function, taxonomy and sequences resource</title>
		<author>
			<persName><forename type="first">S</forename><surname>Velankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Dana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Ginkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Oldfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>'donovan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-J</forename><surname>Kleywegt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="D483" to="489" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Capturing non-local interactions by long short-term memory bidirectional recurrent neural networks for improving prediction of protein secondary structure, backbone angles, contact numbers and solvent accessibility</title>
		<author>
			<persName><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2842" to="2849" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">RaptorX-property: a web server for protein structure property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W430" to="435" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Protein secondary structure prediction using deep convolutional neural fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">18962</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">JPred4: a protein secondary structure prediction server</title>
		<author>
			<persName><forename type="first">A</forename><surname>Drozdetskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Procter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">W1</biblScope>
			<biblScope unit="page" from="W389" to="W394" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The protein data bank</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Westbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilliland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Weissig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">N</forename><surname>Shindyalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Bourne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="235" to="242" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">PISCES: a protein sequence culling server</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Dunbrack</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1589" to="1591" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Dictionary of protein secondary structure: pattern recognition of hydrogen bonded and geometrical features</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kabsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biopolym</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2577" to="2637" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Sixty-five years of the long march in protein secondary structure prediction: the final stretch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heffernan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brief Bioinform</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="482" to="494" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Evaluation and improvement of multiple sequence methods for protein secondary structure prediction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Barton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins Struct Funct Genet</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="508" to="519" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Assessment of hard target modeling in CASP12 reveals an emerging role of alignment-based contact prediction methods</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Abriata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Tamò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Monastyrskyy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kryshtafovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dal</forename><surname>Peraro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="97" to="112" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">LocTree2 predicts localization for all domains of life</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="458" to="465" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">MultiLoc2: integrating phylogeny and gene ontology terms improves subcellular protein localization prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Briesemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kohlbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinform</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">274</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">SherLoc2: a high-accuracy hybrid method for predicting subcellular localization of proteins</title>
		<author>
			<persName><forename type="first">S</forename><surname>Briesemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Brady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kohlbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shatkay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Proteome Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5363" to="5366" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Prediction of protein subcellular localization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="643" to="651" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">WoLF PSORT: protein localization predictor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Horton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Obayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">-</forename><surname>Collier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Nakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="W585" to="587" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">YLoc -an interpretable web server for predicting subcellular localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Briesemeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rahnenfuhrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kohlbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="W497" to="502" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">UniProtKB/Swiss-Prot, the manually annotated section of the UniProt KnowledgeBase: how to use the entry view</title>
		<author>
			<persName><forename type="first">E</forename><surname>Boutet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lieberherr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tognolli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bougueleret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Xenarios</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods Mol Biol</title>
		<imprint>
			<biblScope unit="volume">1374</biblScope>
			<biblScope unit="page" from="23" to="54" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">CD-HIT: accelerated for clustering the nextgeneration sequencing data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3150" to="3152" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Godzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1658" to="1659" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Single cell RNA-seq data clustering using TF-IDF based methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Moussa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Mandoiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Genomics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">569</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">MEME SUITE: tools for motif discovery and searching</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Buske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Frith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Clementi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Res</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="W202" to="208" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Alignment-free microbial phylogenomics under scenarios of sequence divergence, genome rearrangement and lateral genetic transfer</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Ragan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">28970</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Evolutionary profiles improve protein-protein interaction prediction from sequence</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1945" to="1950" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Profile-based string kernels for remote homology detection and motif extraction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Bioinforma Comput Biol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="527" to="550" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">Mismatch string kernels for SVM protein classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eskin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>in press</publisher>
			<pubPlace>Bioinform</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">PSORT: a program for detecting sorting signals in proteins and predicting their subcellular localization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Horton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Biochem Sci</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="36" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Identifying remote protein homologs by network propagation</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FEBS J</title>
		<imprint>
			<biblScope unit="volume">272</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="5119" to="5128" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Probabilistic variable-length segmentation of protein sequences for discriminative motif discovery (DiMotif) and sequence embedding (ProtVecX)</title>
		<author>
			<persName><forename type="first">E</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Mchardy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrk</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3577</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Mut2Vec: distributed representation of cancerous mutations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Med Genet</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">33</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">PhosContext2vec: a distributed representation of residue-level sequence contexts and its application to general and kinase-specific phosphorylation site prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Whisstock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans Assoc Comput Linguist</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<idno>arXiv:190409237 2019</idno>
		<title level="m">On the convergence of adam and beyond</title>
				<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Mach Learn Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Amino acid substitution matrices from protein blocks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Henikoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Henikoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Natl Acad Sci</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="10915" to="10919" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>arXiv:150203167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Comparison of the predicted and observed secondary structure of T4 phage lysozyme</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biochim Biophys Acta</title>
		<imprint>
			<biblScope unit="volume">405</biblScope>
			<biblScope unit="page" from="442" to="451" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Comparing two K-category assignments by a K-category correlation coefficient</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gorodkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Chem</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="367" to="374" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
