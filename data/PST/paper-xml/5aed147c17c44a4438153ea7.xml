<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">iNPG: Accelerating Critical Section Access with In-Network Packet Generation for NoC Based Many-Cores</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
							<email>yuanyao@kth.se</email>
						</author>
						<author>
							<persName><forename type="first">Zhonghai</forename><surname>Lu</surname></persName>
							<email>zhonghai@kth.se</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">KTH Royal Institute of Technology Stockholm</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">KTH Royal Institute of Technology Stockholm</orgName>
								<address>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">iNPG: Accelerating Critical Section Access with In-Network Packet Generation for NoC Based Many-Cores</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA.2018.00012</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As recently studied, serialized competition overhead for entering critical section is more dominant than critical section execution itself in limiting performance of multi-threaded shared variable applications on NoC-based many-cores.</p><p>We illustrate that the invalidation-acknowledgement delay for cache coherency between the home node storing the critical section lock and the cores running competing threads is the leading factor to high competition overhead in lock spinning, which is realized in various spin-lock primitives (such as the ticket lock, ABQL, MCS lock, etc.) and the spinning phase of queue spin-lock (QSL) in advanced operating systems. To reduce such high lock coherence overhead, we propose in-network packet generation (iNPG) to turn passive "normal" NoC routers which only transmit packets into active "big" ones that can generate packets. Instead of performing all coherence maintenance at the home node, big routers which are deployed nearer to competing threads can generate packets to perform early invalidation-acknowledgement for failing threads before their requests reach the home node, shortening the protocol round-trip delay and thus significantly reducing competition overhead in various locking primitives.</p><p>We evaluate iNPG in Gem5 using PARSEC and SPEC OMP2012 programs with five different locking primitives. Compared to a state-of-the-art technique accelerating critical section access, experimental results show that iNPG can effectively reduce lock coherence overhead, expediting critical section access by 1.35? on average and 2.03? at maximum and consequently improving the program Region-of-Interest (ROI) runtime by 7.8% on average and 14.7% at maximum.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>As the core count grows quickly, there is a greater potential to speed up the execution of parallel and concurrent programs. While this trend helps to linearly expedite the concurrent execution part, the ultimate application speedup is however limited by the sequential execution part of programs, as reflected in the well-known Amdahl's law <ref type="bibr" target="#b20">[20]</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the typical execution of a multi-threaded program. After an initialization phase, N threads start parallel executions and encounter a synchronization point where each thread competes to access and execute a critical section (CS). In particular, serialized CS access incurs competition overhead (COH) due to locking retries in spin-lock primitives <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b14">14]</ref> and context switching &amp; sleep overhead in queue spin-lock primitive (See details in Section 2). To expedite parallel application execution, most previous works emphasize on accelerating the CS parts shown in Figure <ref type="figure" target="#fig_0">1</ref> by exploiting various mechanisms such as running the CS code on fat cores in asymmetric chip multi-processors (CMPs) <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b37">37]</ref>, or predicting and prioritizing threads that are executing critical sections <ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b24">24]</ref>. Recent research in <ref type="bibr" target="#b40">[40]</ref> demonstrates that COH may indeed exceed the time for CS execution in network-on-chip (NoC) based many-cores, and become the dominating factor limiting the performance of parallel applications. By opportunistically avoiding the OS-level context switching &amp; sleep overhead, the proposed OCOR (Opportunistic Competition Overhead Reduction) technique in <ref type="bibr" target="#b40">[40]</ref> shows great COH reduction (over 30% on average for PARSEC <ref type="bibr" target="#b3">[3]</ref> and SPEC OMP2012 <ref type="bibr" target="#b28">[28]</ref> benchmarks) and application acceleration up to 24.5%. However, this work uses only queue spin-lock for experiments and does not investigate the effect of cache coherence in influencing COH in cache-coherent many-cores.</p><p>In the paper, we show that the lock coherence overhead (LCO) in lock spinning for both spin-lock primitives and the spinning phase of queue spin-lock primitive is a major source of COH. This is because, in lock spinning, each competing thread will generate an exclusive access request to the home node where the lock variable is stored. However, only one of these exclusive accesses will succeed, and the home node will then invalidate the CS lock copies in all the other threads' L1 caches. Only when the winning thread, which is the new owner of the lock variable, receives all acknowledgements from the losing threads, it can proceed to execute the following critical section. To reduce such high overhead coherence round-trip latency, we propose an in-network packet generation (iNPG) technique that can perform early cache invalidation to locking fail-ure threads, thereby reducing LCO and accelerating execution of multi-threaded applications. iNPG is enabled by active "big" routers, which can not only transmit packets like normal routers but also generate packets in the network as early invalidations to L1 caches of failing threads. The big routers then wait for the acknowledgements from the losing threads, and then forward the acknowledgements to the winning thread to send a valid lock copy to each of the losing thread. In this way, the home node is largely freed from the lock coherence maintenance burden, and the winning thread can move forward to the CS more quickly. Focusing on lock spinning, iNPG is an orthogonal scheme to OCOR suggested for queue spin-lock in <ref type="bibr" target="#b40">[40]</ref>. As such, it can be combined seamlessly with OCOR to achieve the highest COH reduction with various spin-lock primitives and the queue spinlock primitive, as shown in experimental results.</p><p>The rest of the paper is organized as follows. Section 2 gives background and motivation. Section 3 illustrates lock spinning in cache coherent architecture and describes our design principle. Section 4 gives design and synthesis details of big router. In Section 5 we report experiments and results. After discussing the related work in Section 6, we finally conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BACKGROUND AND MOTIVATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Lock spinning schemes</head><p>In lock spinning, when a thread fails to lock a variable protecting a CS, it will keep re-trying (after a short spin interval) until succeed <ref type="bibr" target="#b33">[33]</ref>. During the OS runtime quantum (usually in the order of hundreds of milliseconds) of the spinning thread, no other threads are allowed to use the core, leading to a waste of core processing time and energy. However, by persistently polling on a lock variable, lock spinning can be quickly successful once the lock is released. Modern OSes provide different lock spinning mechanisms, including spinlocks and queue spin-lock, to support critical section synchronization for concurrent programs.</p><p>1) Test-and-set lock (TAS). The test-and-set lock is a spinlock that employs a polling loop to access a global boolean flag that indicates whether the critical section is held. Each core repeatedly executes an atomic test_and_set instruction in an attempt to change the flag from false to true, thereby acquiring the lock. The shortcoming of the test-and-set lock is the contention for the lock, as each waiting core continually spins the single shared lock until success.</p><p>2) The ticket lock (TTL). As proposed in <ref type="bibr" target="#b31">[31]</ref>, the ticket lock is a spin-lock that consists of two counters, one request counter containing the number of cores to acquire the lock, and one release counter recording the number of cores that have released the lock. A core competes for the lock by fetching a ticket from the request counter, and waiting until its ticket is equal to the release counter. A core releases the lock by incrementing the release counter.</p><p>3) Array-based queuing lock <ref type="bibr">(ABQL)</ref>. By having all competing cores spinning on different lock variables, array-based queuing lock is a spin-lock ensuring that on a lock release only one core attempts to acquire the lock. In this way, ABQL significantly decreases the number of lock contentions when a lock is released. ABQL has two versions, which are corespondingly proposed by Anderson <ref type="bibr" target="#b2">[2]</ref> and Graunke and Thakkar <ref type="bibr" target="#b16">[16]</ref>, in which the Anderson's version further protects the ABQL with an atomic test_and_set.</p><p>4) Mellor-Crummey and Scott (MCS) lock. As inspired by <ref type="bibr" target="#b14">[14]</ref>, Mellor-Crummey and Scott <ref type="bibr" target="#b26">[26]</ref> propose a per-core structured spin-lock, which is able to eliminate much of the cache-line bouncing experienced by spin-locks. In MCS lock, when a core attempts to secure a global lock variable, it will create an mcs_spin_lock structure of its own. Using an atomic exchange operation (compare_and_swap), it stores the address of its own mcs_spin_lock structure into the global lock's "next pointer" field. The atomic exchange operation will then return the previous value of the next pointer. If that pointer was null, the acquiring core is successful in acquiring the lock. Otherwise, the core is put into a queue waiting for its predecessor to release the lock.</p><p>5) Queue spin-lock (QSL). QSL is a variant of spin-lock which is adopted in most modern OSes such as Linux 4.2 <ref type="foot" target="#foot_0">1</ref>and Unix BSD 4.4 <ref type="foot" target="#foot_1">2</ref> . It starts with a spin-lock phase to secure a CS, and if not successful after a certain times of retry (by default 128 times in Linux 4.2), it yields to enter a sleep phase after context switching (thus releasing the core for processing other tasks or in power-saving mode) and the thread's locking request is placed in a request queue. The thread continues sleeping until being woken up to secure the lock when the CS is unlocked by the holding thread. The spin-lock phase of a queue spin-lock can be one of the four spin-lock alternatives (TAS, TTL, ABQL, or MCS) discussed above. Because of its advantages in reducing synchronization traffic and OS overhead, we select QSL in the paper with the spin-lock implemented as MCS lock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Motivation</head><p>To show the criticality of lock coherence overhead (LCO) in different locking primitives, we experimented on a 64core architecture in Gem5 (See experimental setup in Section 5), with each program running alone in 64 concurrent threads. Figures 2 reports the percentage of LCO in application running time under TAS, TTL, ABQL, MCS and QSL in three applications: kdtree from SPEC OMP2012 and facesim, fliudanimate from PARSEC.  From Figure <ref type="figure" target="#fig_1">2</ref> we can observe that 1) across different benchmarks, the percentage of LCO in application running time differs. 2) TAS lock has the most percentage of LCO, followed by TTL and ABQL locks; LCO percentage in MCS and QSL locks are relatively smaller among all locking primitives. As shown in the figure, in benchmark ketree, TAS spends nearly 50% of application running time synchronizing critical section lock among competing threads. With TTL and ABQL, the percentage of LCO is reduced to 31% and 27%, correspondingly. In MCS and QSL, because locking contention among threads is further minimized, the percentage of LCO reaches 14% and 17% application running time, correspondingly. The same phenomenon has been consistently observed in fluidanimate and facesim, where LCO occupies 65% and 90% application running time under TAS, 47% and 57% under TTL, 50% and 56% under ABQL, 20% and 30% under MCS, and 25% and 32% under QSL, respectively. From the experiment we can conclude that LCO indeed lies straightly on application execution critical path, causing heavy overhead for lock spinning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">IN-NETWORK PACKET GENERATION</head><p>3.1 Target many-core architecture Figure <ref type="figure" target="#fig_2">3</ref> shows a typical architecture for a 64-core 8?8 many-core, where the NoC (routers and network interfaces) communicates messages among processor nodes (a core with its private L1 cache), secondary on-chip shared L2 cache banks, and on-chip memory controllers. Routers are organized in a popular mesh topology on which the XY dimensional routing algorithm is implemented to achieve simplicity and deadlock-free. Eight memory controllers are symmetrically connected to the middle nodes on the top and bottom rows. Due to its layout and packaging conveniences, this pattern has been used in academic and industrial manycore designs such as MIT SCORPIO 36-core processor <ref type="bibr" target="#b7">[7]</ref> and latest Intel Knights Landing 36-tile processor <ref type="bibr" target="#b34">[34]</ref>. To support cache coherency, a directory based MOESI cache coherence protocol is implemented. On the target manycore, once a core issues a memory request, the private L1 is first checked for the presence of the requested data. If this fails, the request is then forwarded to the local shared L2 or via the NoC to remote shared L2/DRAM. Figure <ref type="figure" target="#fig_2">3</ref> also depicts a big router deployment with 32 big routers interleaving with 32 normal routers.</p><formula xml:id="formula_0">R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C R C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Lock spinning under cache coherency</head><p>We look into how lock spinning works and how its associated LCO is caused on NoC based many-cores with cache coherence protocol. Algorithm 1 shows an example assembly code <ref type="bibr" target="#b19">[19]</ref> for a simple test-and-set spin-lock implementation on many-cores. In the code, value 0 denotes a lock "available" and value 1 "occupied". In Line 1, a thread first spins to check on a local copy of the lock variable (whose memory address is stored in local register R1 and the lock itself is stored remotely at a home node) until it sees that the lock is available. Once a thread finds that the lock is freed (by loading 0 into register R2), it then passes Line 2. In Line 3, a thread first modifies the local lock copy from 0 to 1 to assert its ownership for the lock (but the lock variable at the home node still remains as 0). Then, in Line 4, threads compete with each other to secure the lock by sending an atomic swap request (SWAP) to the home node, which writes the locally modified lock (with value 1) to the one in the home node (with value 0). The winning SWAP request will then set the lock variable at the home node to 1 to assert its exclusive access right to the lock. Consequently, all loser SWAP requests will only see 1 in the lock variable which was set by the winner request. After the swap operation, the winning thread passes Line 5 to execute the critical section, with all loser threads spinning for the lock again from Line 1.</p><p>Figure <ref type="figure">4</ref> shows how Algorithm 1 is executed in three threads ? 1 , ? 2 and ? 3 on an example 3?3 CMP based on the MOESI coherence protocol introduced in <ref type="bibr" target="#b35">[35]</ref>. The lock variable in the home node is initialized to the "available" state (value 0).</p><p>Step 1: When ? 1 , ? 2 and ? 3 execute Line 1 of Algorithm 1, because none of them has a copy of the lock in their local cache, they all encounter a cold cache read miss. Each thread then sends a data read request to the home node, which inturn sends a valid copy of the lock (value 0) back to the corresponding L1 cache of each thread. At the same time, the home node marks ? 1 , ? 2 and ? 3 as data sharers of the lock variable in its coherence directory.</p><p>Step 2: When the competing threads load 0 into their locally copied CS locks, they pass Line 2. In Line 3, each thread locally modifies the lock to the occupied state by changing its value to 1. Then, the three threads compete with each other by executing the swap operation (SWAP in Line 4). At the hardware level, this is done by each thread sending an atomic "read-for-modification" operation (GetX request in Gem5), which tries to get the exclusive access right to the lock in the home node. The winning thread will then execute the swap operation and write 1 into the lock variable.</p><p>Step 3: In Figure <ref type="figure">4</ref>, assuming that GetX from ? 1 succeeds, and GetX requests from ? 2 and ? 3 fail. In this case, ? 1 wins the exclusive access right to the lock. The directory controller at the home node will perform three tasks. First, it notifies ? 1 that it now has the exclusive access right of the lock. Second, it searches its coherence directory and sends invalidations to the lock copies in ? 2 's and ? 3 's caches. ? 2 and ? 3 , upon receiving the Invalidation, will then each send an Invalidation-Acknowledgement (InvAck in Gem5) to ? 1 , which is now the owner of the lock variable. Third, it forwards the GetX requests from ? 2 and ? 3 to ? 1 . Moreover, with the forwarded GetX requests, the directory controller at</p><formula xml:id="formula_1">?1 (1) Lock: LD R2, 0(R1) (2) BENZ R2, Lock ?2 (1) Lock: LD R2, 0(R1) (2) BENZ R2, Lock ?3 (1) Lock: LD R2, 0(R1) (2) BENZ R2, Lock (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ? 2 0 (3,1)<label>(3,2)</label></formula><p>H 0</p><formula xml:id="formula_2">(3,3) ? 3 0 ? 1 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Valid copy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Valid copy</head><p>Valid copy <ref type="bibr">(a)</ref> Step 1.</p><formula xml:id="formula_3">(1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ? 2 1 (3,1)<label>(3,2)</label></formula><p>H 0</p><formula xml:id="formula_4">(3,3) ? 3 1 ? 1 1 ?1 (3) ADD R2, R0, #1 (4) SWAP R2, 0(R1) ?2 (3) ADD R2, R0, #1 (4) SWAP R2, 0(R1) ?3 (3) ADD R2, R0, #1 (4) SWAP R2, 0(R1) Winning GetX Failed GetX Failed GetX (b) Step 2. (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ? 2 I (3,1) (3,2) H 0?1 (3,3) ? 3 I ? 1 1 ?1 (5) BENZ R2 successes.</formula><p>Enter critical section. ?2 (5) BENZ R2 fails.</p><p>CS locking fails.</p><p>?3 (5) BENZ R2 fails.</p><p>CS locking fails. Figure <ref type="figure">4</ref>: Spin-lock execution on an example 3?3 CMP under MOESI. In Step 1, the home node responds to read misses from ? 1 , ? 2 and ? 3 and sends a valid lock copy to the local cache of each thread. In Step 2, each thread individually modifies the lock to the "occupied" status (value 1) and competes with each other to get the exclusive access right of the lock. In Step 3, the winning thread gets exclusive access to the lock, with all losing threads' lock copies invalidated. GetX requests from ? 2 and ? 3 are further forwarded by the directory controller of the home node to ? 1 . In Step 4, ? 1 sends valid lock copies to ? 2 and ? 3 and then enters critical section after collecting all acknowledgements from ? 2 and ? 3 . the home node also forwards the total number of data sharers (AckCount in Gem5) of the lock variable to ? 1 , so that ? 1 can monitor and count the receiving of InvAck messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FwdGetX for ?2 and ?3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AckCount</head><formula xml:id="formula_5">Inv Inv (c) Step 3. ?1 &amp;ULWLFDO VHFWLRQ UHOHDVH :ULWH WR KRPH QRGH ?2 (QFRXQWHUV UHDG PLVV 1HZ YDOXH ORDGHG ?3 (QFRXQWHUV UHDG PLVV 1HZ YDOXH ORDGHG (1,1) (1,2) (1,3) (2,1) (2,2) (2,3) ? 2 1 (3,1)<label>(3,2)</label></formula><p>Step 4: After receiving the forwarded GetX requests and all of the InvAck messages from ? 2 and ? 3 , ? 1 sends a valid copy of the lock to the L1 caches of ? 2 and ? 3 and then knows that there are no longer any invalid copy of the lock variable. Thus ? 1 can write to the lock without violating coherence. As a result, ? 1 will successfully execute the SWAP operation and executes the following critical section, while ? 2 and ? 3 will again loop back to Line 1. After ? 1 finishes its critical section, it releases the lock by changing the lock value from occupied (value 1) to available (value 0) in the home node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Reduce lock coherence overhead with iNPG</head><p>Figure <ref type="figure" target="#fig_5">5a</ref> depicts the transactional procedure for Step 2, 3 and 4 in Figure <ref type="figure">4</ref>. When the three competing threads ? 1 , ? 2 and ? 3 execute spin-lock, three atomic swaps (corresponding to three GetX requests in Gem5) are issued, which are routed through NoC routers (denoted R in Figure <ref type="figure" target="#fig_5">5a</ref>) to the home node. The winning GetX will exclusively secure the lock. Directory controller in the home node then invalidates the lock copies in ? 2 's and ? 3 's L1 caches, and forwards the GetX requests from ? 2 and ? 3 to ? 1 , which is now the owner of the lock. When ? 1 receives the forwarded GetX requests and all InvAcks from ? 2 and ? 3 , it then sends valid lock copies to ? 2 's and ? 3 's L1 caches and moves forward to execute codes in the critical section. From figure <ref type="figure" target="#fig_5">5a</ref>, we can observe that the coherence traffic between the home node and ? 1 , ? 2 , ? 3 lies on the critical path of program execution time.</p><p>As shown in the motivational Figure <ref type="figure" target="#fig_1">2</ref>, this lock coherence overhead (LCO) is a significant part of application running time because the home node is solely responsible for sending invalidations to all failing threads and ? 1 has to collect all the invalidation acknowledgements to move forward.</p><p>We aim to shorten the lock coherence latency, i.e., LCO, so as to reduce COH <ref type="bibr" target="#b40">[40]</ref> in lock spinning. To this end, we propose in-network packet generation (iNPG) enabled by "big" router (denoted BR in Figure <ref type="figure" target="#fig_5">5b</ref>), a normal router enhanced with active packet generation functionality for maintaining cache coherency. This is possible because intermediate routers can be used to provide a temporary "barrier" for a lock variable. Once an intermediate router transfers a GetX request for lock variable (denoted GetX[ ]), a temporary "barrier" is set up for lock , which will stop delivering failing GetX[ ] requests that lose arbitration to the delivered GetX[ ], or subsequent GetX[ ] requests that arrive later at the big router. Since the "barriers" are set up distributively at intermediate routers instead of aggregated at the home node, multiple early invalidations can be generated and sent by intermediate big routers to shorten LCO. More details are presented in Section 4.</p><p>The principle of iNPG can be illustrated in Figure <ref type="figure" target="#fig_5">5b</ref>. When the losing or later arriving GetX operations reach a big router, their further transmissions are stopped. Instead, cache invalidations are immediately generated and sent by the big router to their issuing threads (? 2 and ? 3 in Figure <ref type="figure" target="#fig_5">5</ref>). The big router then forwards the GetX requests and the acknowledgements (InvAcks) from the failing threads to the home node, which are in turn forwarded by the home node to the winning thread (? 1 in Figure <ref type="figure" target="#fig_5">5</ref>). In this way, the invalidation and acknowledgement of the lock copy are done on the way to the home node instead of being done after the GetX requests of the losing threads reach the home node. Note that if the winning GetX request is determined at the home node (e.g., two GetX requests arrive at the home node at the same time but from different inports), the home node is then responsible for sending invalidation to and waiting  for acknowledgement from the thread that loses the local arbitration at the home node, just as in the original operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">BIG ROUTER DESIGN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Micro-architecture</head><p>The idea of iNPG for early cache coherence completion is realized by "big" router, which enhances a normal NoC router with packet-generation functionality. Figure <ref type="figure" target="#fig_6">6</ref> shows the micro-architecture of a big router, in which the packet generator is added to maintain a locking barrier table and a packet generation logic to generate cache coherence packets in the network. Our baseline router is a 2-stage pipelined speculative router <ref type="bibr" target="#b29">[29]</ref> in which the first stage performs Route Computation (RC), Virtual Channel (VC) Allocation (VA), and Switch Allocation (SA) in parallel and the second stage is Switch Traversal (ST). In implementation, packet generation is realized in the same pipeline stage as ST.</p><p>In a big router, the VC allocator interacts with the packet generator as follows. (1) On detecting the first GetX lock request transferred at a big router, the VC allocator informs the packet generator to create a temporary "lock barrier" in the locking barrier table . (2) Once a lock barrier is created, subsequent or arbitration-failed GetX requests for the same lock will be stopped. As shown in Figure <ref type="figure" target="#fig_6">6</ref>, for every stopped GetX request, an early invalidation (EI) entry is created under the corresponding lock barrier to track the status of executing early invalidation. When a GetX is stopped, an Inv packet will be generated, stored to a separate VC, and sent to the corresponding thread. Meanwhile, the VC allocator changes the failed GetX request to a FwdGetX request, which is forwarded to the home node. (3) Once receiving the InvAck packet for an early Inv packet that was sent out by the big router, the big router forwards the received InvAck to the home node, which further sends it to the lock owner.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> sketches the locking barrier table. As shown in the figure, each lock barrier entry contains the memory address of the lock variable and its time-to-live (TTL) duration, which is by default set to 128 cycles. The purpose of TTL is to delete its corresponding lock barrier when it counts down to zero. The TTL counts down only if there is no EI entry and resets to the default value whenever an EI entry is created. Each EI entry contains the issuing core's ID of a stopped GetX request and consists of 4 phases: Inv generated (denoted Inv), GetX forwarded (denoted GetXFwd), InvAck for an early Inv received (denoted InvAck), and In-vAck forwarded (denoted AckFwd). An EI entry is freed  when all the four phases are finished. Further, when all EI entries of a lock barrier are released, its TTL starts countdown from the default value. Finally, a lock barrier entry is deleted after its TTL reaches 0.</p><p>Figure <ref type="figure" target="#fig_6">6</ref> also shows the format of generated packets. We omit the information fields that are used for flow control in all network packets (such as flit type, VC identifier, etc.), and focus on the additional information fields that are generated. As shown in the figure, the packet generator generates packet in the Inv phase (generates invalidation packet) and the Ack-Fwd phase (changes the destination of the received InvAck message to the ID of the home node). When the locking barrier table is full, following GetX requests will pass through as in a normal router.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Synthesis and chip floorplan &amp; layout</head><p>Methodology. We have made RTL implementations of the normal and big routers. We perform RTL synthesis in Synopsys Design Compiler and physical floorplanning &amp; layout in Cadence SoC Encounter using TSMC's 40 nm low power library (typical case). During synthesis, the core/uncore voltage is set to 1.1V and frequency to 2.0GHz. During floorplanning, we use 1.7V as the chip input voltage, which drives all I/O pins of the chip and is scaled down to 1.1V for core/un-core voltage. We select OpenRISC 1200 <ref type="foot" target="#foot_2">3</ref>    Synthesis results. As shown in Figure <ref type="figure" target="#fig_7">7a</ref>, a normal router consumes 19.9K (equivalent NAND) gates while a big router consumes 22.4K gates. A packet generator consumes 2.5K gates with the majority coming from the locking barrier table which by default contains 16 lock barriers and 16 EI entries. The added packet generator does not significantly increase the critical path inside a big router. Under 2.0GHz timing constraint, we observed 2.9% of all end-to-end paths violating the endpoint slack, which are all eliminated via placement optimization during floorplanning. A packet generator consumes 8.4 mW dynamic power, adding 9.9% overhead to a normal router. On the target architecture, one big tile (1 core plus 1 big router) consumes 716.1 mW dynamic power, with the core consuming 623.5 mW and the big router 92.6 mW. One normal tile (1 core plus 1 normal router) consumes 707.7 mW dynamic power, with the core power the same as in a big tile but the router power decreasing to 84.2 mW.</p><formula xml:id="formula_6">11395 um R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core R Core</formula><p>Floorplan &amp; layout results. As summarized in Figure <ref type="figure" target="#fig_7">7a</ref>, the number of floorplan layers is 28, which includes 10 metal routing layers, 11 via layers, 5 implant layers, 1 master-slice layer and 1 AP layer. In the 10 metal layers, the top 2 layers (layers M10 and M9) are dedicated for power mesh, which are featured with ultra thick wires. As shown in Figure <ref type="figure" target="#fig_7">7b</ref>, on the target architecture, each tile (both big tile and normal tile) consumes equal chip area, with the distance between neighbour tiles being 1.8 um to accommodate the 256-bit bidirectional wires between routers (128-bit per direction with 1-bit wire width 0.007 um). Further, to implement both big and normal routers with practical physical layout, we manipulate the standard cell (SC) density to accommodate both big and normal router within the same dimension, which is 460 um by 460 um (0.21 mm 2 ) as shown in Figure <ref type="figure" target="#fig_7">7c</ref>. The cell density (before filler insertion) is 66.67% in a big router while 61.90% in normal router, as reported in Figure <ref type="figure" target="#fig_7">7a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Methodology</head><p>We evaluate iNPG with timing-detailed full-system simulations using both PARSEC (10 programs 45 with all large input sets) <ref type="bibr" target="#b3">[3]</ref> and SPEC OMP2012 (all 14 programs) <ref type="bibr" target="#b28">[28]</ref> benchmarks. For data validity, results are obtained from the multi-threaded execution phase called Region-of-Interest (ROI). We implement and integrate our design in Gem5 <ref type="bibr" target="#b4">[4]</ref>, in which the embedded network GARNET <ref type="bibr" target="#b1">[1]</ref> is enhanced according to our technique. The key configurations of the simulation platform are listed in Table <ref type="table" target="#tab_4">1</ref>. Each application runs on 64 cores with one core running one thread. The operating system is Linux 4.2. Unless otherwise specified, the synchronization primitive is the default queue spin-lock in Linux 4.2 with the spin-lock phase implemented as MCS lock. We set up four comparative cases as follows.</p><p>Case 1 is Original, the baseline architecture that Gem5 simulates according to the configurations in Table <ref type="table" target="#tab_4">1</ref>.</p><p>Case 2 is OCOR (Opportunistic Competition Overhead Reduction), the technique proposed for queue spin-lock in <ref type="bibr" target="#b40">[40]</ref>, which maximizes the chance that a thread gets CS in low-overhead lock-spinning phase and minimizes the chance that a thread gets CS during the high-overhead sleep phase.</p><p>OCOR is realized by software and hardware co-design. In software, OCOR monitors the remaining times of retry (RTR) in a thread's spinning phase, which reflects in how long the thread must enter the high-overhead sleep phase.  In hardware, OCOR integrates the RTR information into the packets of SWAP requests, and let the NoC routers prioritize SWAP request packets according to the RTR information.</p><p>The principle is that the smaller RTR a SWAP request packet carries, the higher priority it gets and thus quicker delivery. However, when a thread is already in the sleep mode, it should access CS at a later time, since waking the thread up will introduce considerable overhead. It is thus preferable to allow threads in the low-overhead spinning phase to win. Besides, program progress information is embedded in request packets to avoid starvation for low-priority requests. Following <ref type="bibr" target="#b40">[40]</ref>, OCOR configuration is shown in Table <ref type="table" target="#tab_4">1</ref>.</p><p>Case 3 is our iNPG. In iNPG, we follow the NoC architecture introduced in Figure <ref type="figure" target="#fig_2">3</ref>, in which a big router interleaves with a normal router. Case 4 is iNPG+OCOR, which combines iNPG with OCOR. In this case, all routers support OCOR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Benchmark CS characteristics</head><p>Figure <ref type="figure" target="#fig_8">8</ref> reports the CS characteristics of the 10 PARSEC and the 14 OMP2012 programs on the target 64-core manycore. Figure <ref type="figure" target="#fig_8">8a</ref> gives the total CS access times and average CPU cycles per CS for each program. Since tasks in each program vary, both metrics differ from program to program. For example, in application fluid of PARSEC, critical sections (secured by pthread_mutex_lock) are used to synchronize indexes of liquid particles. Although each CS takes limited CPU cycles to finish (average 81.47 CPU cycles), the total number of critical sections is however high (10,240 times in total). In contrast, in application imag of OMP2012, critical sections (secured by #pragma omp critical) are used to atomically modify an image. Although the total number of critical sections is smaller (4,000 times in total), each CS performs relatively heavy tasks and thus takes more CPU cycles (average 179 CPU cycles) to finish.</p><p>Figure <ref type="figure" target="#fig_8">8b</ref> breaks the total CS execution time (CS times ? average cycles per CS) of each application into competition overhead (COH) and the CS execution time itself (CSE). It is evident that compared to CSE, COH contributes more significantly to application runtime. In the figure, we sort applications according to the total CS execution time (COH+CSE) in the ascending order and divide them into three groups, with Group 1 (6 programs) featuring lower, Group 2 (12 programs) medium, and Group 3 (6 programs) higher total CS execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Application execution timing profile</head><p>To look into the details and impact of CS entry and access, we profile the program execution timing in Figure <ref type="figure" target="#fig_10">9</ref> with program freqmine for the four comparative cases. For clarity, we show the execution results of 30,000 CPU cycles of the first 8 threads in freqmine, where we divide the program execution timing diagram into three phases. (1) Parallel phase, where threads perform concurrent computation tasks; (2) COH phase, where threads compete with each other to enter the next critical section; (3) CSE phase, where threads execute code in critical sections.</p><p>The Original application execution timing profile is shown in Figure <ref type="figure" target="#fig_10">9a</ref>. As illustrated, 62.1% of the CPU cycles are spent in the parallel phase, with 28.3% in COH and 9.6% in CSE. Moreover, 78 critical sections are completed during the reported 30,000 CPU cycles. However, with OCOR, COH across different threads is significantly reduced. As shown in Figure <ref type="figure" target="#fig_10">9b</ref>, 69.8% of the CPU cycles are spent in the parallel phase, with 19.8% in COH and 10.4% in CSE. Moreover, 92 critical sections are completed during the reported 30,000 CPU cycles, achieving 17.9% application progress improvement than Original. This is because more threads secure the critical section in the low-overhead spinning phase instead of the high overhead sleep phase.</p><p>Further, with iNPG, COH is also remarkably reduced. As shown in Figure <ref type="figure" target="#fig_10">9c</ref>, 73.0% of the CPU cycles are spent in the parallel phase, with 17.0% in COH and 10.0% in CSE. Moreover, 96 critical sections are completed during the reported 30,000 CPU cycles, achieving 23.1% application progress improvement than Original. This is because invalidations to failing or later arriving SWAP requests are sent from big routers instead of from the home node. In this way, the losing threads get early invalidations so that the application avoids heavy coherence traffic latency. Finally, with iNPG+OCOR in which both CS grant order and the coherence traffic latency are optimized, COH is further reduced than with iNPG or OCOR alone. As drawn in Figure <ref type="figure" target="#fig_10">9d</ref>, 80.1% of the CPU cycles are spent in the parallel phase, with 9.0% in COH and 10.9% in CSE. Moreover, 104 critical sections are completed during the reported 30,000 CPU cycles, achieving 33.3% progress improvement than Original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Analysis on LCO reduction</head><p>To reveal the effects of iNPG on reducing lock coherence overhead (LCO), Figure <ref type="figure" target="#fig_12">10</ref>     iNPG in benchmark freqmine. The results are obtained for a scenario where all 64 threads compete for the lock variable that is hosted at the shared L2 cache of core <ref type="bibr" target="#b5">(5,</ref><ref type="bibr" target="#b6">6)</ref>. The measurement starts at the time when all threads begin to compete for the critical section and ends at the time when the last thread gets its critical section. Figure <ref type="figure" target="#fig_12">10a</ref> shows the average coherence Invalidation-Acknowledgement (Inv-Ack) round-trip delay between the home node and all competing threads in Original. Without iNPG, the home node holding the critical section lock is solely responsible for all the coherence invalidations, with the winning thread responsible for collecting the acknowledgements from failing threads. Thus, depending on the distance between a core and the home node, cores near the home node enjoy low coherence overhead (since it can be invalidated at an early time), while cores that are farther away from the home node suffers from higher coherence overhead. Figure <ref type="figure" target="#fig_12">10c</ref> shows the effects of iNPG in reducing the average coherence Inv-Ack round-trip delay. Applying iNPG, the coherence Inv-Ack round-trip delay has less dependence on the distance between the home node and the competing threads. This is because in iNPG, invalidation and the corresponding acknowledgement can be done with distributed big routers instead of aggregately at the home node. In this way, when a GetX request loses arbitration, the L1 cache in its issuing thread will be invalidated by the nearest big router instead of the home node. Figure <ref type="figure" target="#fig_12">10d</ref> further shows the corresponding coherence Inv-Ack round-trip delay histogram. Compared to Figure <ref type="figure" target="#fig_12">10b</ref>, the maximum coherence packet round-trip delay is reduced to 15 CPU cycles, where the "long tail" delay is eliminated. The average coherence packet round-trip delay is decreased from 39.2 to 9.5 cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">CS and application finish time reduction</head><p>Figure <ref type="figure" target="#fig_4">11</ref> shows the overall critical section (including both COH and CSE) expedition results of the four mechanisms. We normalize the results obtained from Original to 1 in each benchmark. We can observe that the critical section expedition results are proportional to the total CPU cycles that an application spends in critical sections as illustrated in Fig- <ref type="figure" target="#fig_8">ure 8b</ref>. This is because the more CPU cycles that a program spends in critical sections, the more opportunity is opened for iNPG and OCOR to achieve higher competition overhead reduction. As shown in Figure <ref type="figure" target="#fig_4">11,</ref><ref type="figure"></ref>  Figure <ref type="figure" target="#fig_4">12</ref> shows the relative application ROI finish time of the four comparison mechanisms, in which we normalize the results obtained from Original to 100%. We can observe that all the four techniques have negligible effects on application parallel execution phase, where each thread executes parallel computation tasks and encounters no critical section. The ROI finish time reduction achieved by OCOR, iNPG and iNPG+OCOR correlates to the CS characteristics of each application. In Group 1 applications, compared with Original, the average ROI finish time is reduced by 6.5% with OCOR, 10.6% with iNPG, and 16.3% with iNPG+OCOR. Across Group 2 applications, the average ROI finish time is reduced by 12.6% with OCOR, 19.5% with iNPG, and 25.2% with iNPG+OCOR. In Group 3, the average ROI finish time is reduced by 17.3% in OCOR, 27.1% in iNPG, and 32.4% in iNPG+OCOR. Across all 24 programs, OCOR reduces the average ROI finish time to 87.7% by 12.3% compared to Original. iNPG decreases the average ROI finish time to 80.1% by 19.9%. Finally, iNPG+OCOR reduces the average ROI finish time to 75.3% by 24.7%, exhibiting the highest ROI finish time reduction. Compare iNPG over OCOR, iNPG improves the ROI finish time by 7.8% on average and 14.7% at maximum (with program bt331).</p><p>We can observe from Figure <ref type="figure" target="#fig_4">11</ref> and Figure <ref type="figure" target="#fig_4">12</ref> that overall benefit of iNPG+OCOR is not an accumulation of benefits from stand-alone iNPG and OCOR. This is because in iNPG, LCO in the spinning phase is significantly reduced, thus more threads can gain CS access without entering into the sleep phase. Since OCOR also aims to have more threads enter CS in their spinning phase, when combined with iNPG, the opportunity for iNPG+OCOR is reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">iNPG's effectiveness with other locking primitives</head><p>We now explain and show the effectiveness of iNPG in reducing application ROI finish time with other locking primitives, including test-and-set (TAS) lock, the ticket lock (TTL) <ref type="bibr" target="#b31">[31]</ref>, array-based queuing lock (ABQL) <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b16">16]</ref> and the MCS lock <ref type="bibr" target="#b14">[14]</ref>.</p><p>In TAS, each competing thread keeps spinning on the local CS lock variable until it successfully enters the critical section. Thus, TAS generates extensive read-modify-write operations because every time the lock is freed, each thread generates an exclusive access request among which only one will succeed, with all failed ones invalidated by the coherence protocol. The number of read-modify-write operation is significantly reduced in TTL and ABQL. In both primitives, competing threads are served in the FIFO order. When a critical section is released, only one thread can issue a readmodify-write operation to enter the next critical section. The thread then takes the CS lock, and invalidates all CS locks in other threads' caches. In the MCS lock, the lock contention traffic is further reduced. Instead of polling on one unique  lock variable, in the MCS lock, different threads poll on its previous thread to access the next critical section. That is, each thread (except for the first one) does not poll on a CS lock, but checks to see if its previous thread has finished its critical section execution. When a thread finishes the critical section, it directly notifies its successor to enter the next critical section.</p><p>Our experimental results are in accordance with the above analysis. Figure <ref type="figure" target="#fig_4">13</ref> compares the ROI finish time reduction achieved by iNPG with TAS, TTL, ABQL, QSL and the MCS lock. From the figure we can observe that 1) with different locking primitives, iNPG consistently reduces application ROI finish time across all benchmark programs. 2) The benefits of iNPG differ with different locking primitives. In average, after applying iNPG, ROI finish time is reduced by 52.8% in TAS, by 33.4% in TTL, by 32.6% in ABQL, by 19.9% in QSL and by 16.5% in the MCS lock. This shows that with TAS, TTL and ABQL, iNPG achieves more effective results than with QSL and MCS, which impose less lock competition traffic in the NoC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Sensitivity to big router deployment</head><p>Since iNPG relies on big routers to function, we investigate its sensitivity to big router deployment by varying the number of big routers from 0 to 4, 16, 32 and 64. We distribute all big routers evenly on the chip, where 0 big router is the Original setup and 32 big routers represent the case illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Figure <ref type="figure" target="#fig_14">14</ref> reports CS expedition results (including both CSE and COH) normalized to Original. We can make the following two observations. First, as expected, across different benchmarks, iNPG significantly expedites COH, with the CSE results remaining the same as in Original. Second, as the number of big routers increases, COH expedition is increased accordingly. However, the further COH expedition gains from 32 to 64 big routers are marginal. Therefore for the 64-core CMP, 32 big routers achieve </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.7">Sensitivity to NoC dimension and number of entries in locking barrier table</head><p>Figure <ref type="figure" target="#fig_5">15</ref> shows iNPG's effectiveness on reducing average application ROI finish time across all benchmark programs with different NoC dimensions ranging from 2?2, 4?4, to 8?8 (default setup in the paper), 16?16 cores. From the figure we can observe that with the NoC dimension increasing, iNPG brings more effectiveness on reducing application ROI finish time. This is because as the number of cores scales, more threads are involved in competing for the same critical section, and thus more critical LCO becomes. For example, in the 2?2 NoC, iNPG averagely reduces application ROI finish time by 4.7%, which is increased to 19.9% in the 8?8 NoC and 57.5% in the 16?16 NoC.</p><p>Figure <ref type="figure" target="#fig_5">15</ref> also shows iNPG's effectiveness with 4, 16 (default number in the paper) and 64 lock barriers and EI entries in the locking barrier table within a big router. From the figure we can observe that in 2?2 and 4?4 NoC, different size of locking barrier table brings marginal performance difference. However, in the 8?8, 10?10, and 16?16 NoC, 4 lock barriers and EI entries in the locking barrier table severely restricts iNPG's performance. This is because as the NoC dimension scales up, reducing the locking barrier table size directly limits the capability of big routers in sending early invalidations, thus reduces the benefit of iNPG. However, increasing the number of lock barriers and EI entries more than 16 does not proportionally increase iNPG's performance. This is because each router in the NoC can only transfer a limited number of lock competing requests. Based on these observations, we choose 16 as the default number of lock barriers and EI entries in a locking barrier table within a big router.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">RELATED WORK</head><p>Analysis and identification of critical sections and threads. Eyerman and Eeckhout analyzed Amdahl's law with the notion of critical section in <ref type="bibr" target="#b12">[12]</ref>. They developed an analytical model to show that mutual excluded critical section execution imposes a fundamental limit on parallel programs' performance. In <ref type="bibr" target="#b5">[5]</ref>, Chen and Stenstr?m proposed a critical lock analysis method for diagnosing critical section bottlenecks in multi-threaded applications. This method can reliably identify the critical sections that are critical for performance, and quantify the impact of such critical sections on the overall performance. In <ref type="bibr" target="#b11">[11]</ref>, based on the analysis of why multi-threaded workloads typically show sublinear speedup on multi-core chips, the concept of speedup stack was develop to quantify the impact of various scaling delimiters such as LLC and memory subsystem interference, lock spinning &amp; yielding, workload imbalance, cache coherency, etc. Due to synchronization behavior, threads can be critical for program performance. In <ref type="bibr">[8]</ref>, the concept of thread criticality was developed and the thread criticality metric was defined to take into account both a thread's execution time and the number of concurrent waiting threads. Further, criticality stacks were designed to visually break down the total execution time into criticality-related components, facilitating detailed analysis of parallel imbalance.</p><p>Combining synchronization networks. Combining synchronization networks can be catogorized into 1) networks that can combine concurrent accesses to the same memory location <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b30">30]</ref> and 2) networks in which atomic fetch_and_add operations from different cores are combined <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b25">25]</ref>.</p><p>In <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr">Eisley et al.</ref> propose to implement cache coherence protocols within the network to allow in-transit optimizations of read and write operations. To reduce hot-spot contention for synchronization variables, Yew et al. <ref type="bibr" target="#b41">[41]</ref> have proposed a data structure known as a software combining tree. Like hardware combining in a multi-stage interconnection network <ref type="bibr" target="#b15">[15]</ref>, a software combining tree combines multiple accesses to the same shared variable into a single access. Recently, Hendler et al. <ref type="bibr" target="#b17">[17]</ref> present flat-combining, in which a combiner thread holding a global lock combines requests of all other competing threads. In <ref type="bibr" target="#b18">[18]</ref>, they apply the flat-combining mechanism to develop a synchronous queue algorithm. The algorithm first uses a single "combiner" thread to acquire a global lock and then serves other threads' CS requests.</p><p>To accelerate multi-threaded applications, both iNPG and combining synchronization networks exploit the opportun-ity that synchronization packets often pass through the same router. However, iNPG is fundamentally different from the concept of combining network such as <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b6">6]</ref>. In a combining network, the combining switches are able to recognize that two messages are directed to the same memory location and in such cases they can combine the two messages into a single one. However, iNPG seeks to invalidate shared cache copies of lock variables in an early stage of cache coherence protocol by generating new packets rather than combining the underlying synchronization operations. More importantly, combining network is invoked when two or more competing requests collide in-flight in the router: they both try to arbitrate in the same cycle with one win and the others fail. As investigated in <ref type="bibr" target="#b13">[13]</ref>, even in a large scale system such as a 512-node NYU's ultra-computer system, the occurrence of such scenario is still very low. However, in our iNPG, we do not require that two locking requests arbitrate at the same cycle at the same router. Instead, when the first locking request travels through a big router, a temporary "barrier" is then set up to stop subsequent locking requests.</p><p>In-network techniques for collective communication. In the context of providing efficient 1-to-M and M-to-1 communications in NoCs, in-network techniques for packet forking and aggregating were developed. In <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr">Krishna et al.</ref> proposed Flow Across Network Over Uncongested Trees (FANOUT) and Flow AggregatioN In-Network (FANIN) to realize efficient 1-to-M forking and M-to-1 aggregation, respectively. On-chip routers were customized to support FAN-OUT and FANIN. At most routers along flow path, packets incur only single-cycle delays, thus approaching an ideal network with only wire delay/energy. By using clockless repetitive wires on the datapath, FANOUT and FANIN were leveraged to SMART-FANOUT and SMART-FANIN <ref type="bibr" target="#b22">[22]</ref> to enable forking and reduction, respectively, across multiple routers in a network dimension in a single cycle, thus providing a scalable collective communication scheme for NoCs. The above in-network techniques aim to optimize collective communication via optimized routers, which perform efficient packet forking (like copy &amp; paste) and merging but never create new packets. In contrast, our big routers generate new packets in the network, aiming to reduce COH.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>We have presented an iNPG technique to reduce lock coherence overhead (LCO) in serialized critical section access so as to expedite multi-threaded applications running on NoCbased cache-coherent many-cores. Based on the observation that LCO lies straightly on the critical path dominating the overhead for lock spinning, iNPG intends to shorten LCO by turning long-range centralized coherence traffic into short-range distributed coherence traffic. iNPG is enabled by "big" router which is a conventional router enhanced with active cache-coherence packet generation capability. Extensive full-system simulation results in Gem5 running PAR-SEC and SPEC OMP2012 with five different locking primitives (test-and-set lock TAS, the ticket lock TTL, array-based queuing lock ABQL, Mellor-Crummey and Scott MCS lock, and the default queue spin-lock QSL in Linux 4.2) show that iNPG achieves significant improvements in COH and program runtime reduction over the state-of-the-art COH reduc-tion technique OCOR <ref type="bibr" target="#b40">[40]</ref>. Due to the nature of orthogonality, iNPG can be combined with OCOR to achieve the best improvements across the five locking primitives.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Parallel program execution.overhead (COH) due to locking retries in spin-lock primitives<ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b14">14]</ref> and context switching &amp; sleep overhead in queue spin-lock primitive (See details in Section 2). To expedite parallel application execution, most previous works emphasize on accelerating the CS parts shown in Figure1by exploiting various mechanisms such as running the CS code on fat cores in asymmetric chip multi-processors (CMPs)<ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b37">37]</ref>, or predicting and prioritizing threads that are executing critical sections<ref type="bibr" target="#b9">[9,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b24">24]</ref>. Recent research in<ref type="bibr" target="#b40">[40]</ref> demonstrates that COH may indeed exceed the time for CS execution in network-on-chip (NoC) based many-cores, and become the dominating factor limiting the performance of parallel applications. By opportunistically avoiding the OS-level context switching &amp; sleep overhead, the proposed OCOR (Opportunistic Competition Overhead Reduction) technique in<ref type="bibr" target="#b40">[40]</ref> shows great COH reduction (over 30% on average for PARSEC<ref type="bibr" target="#b3">[3]</ref> and SPEC OMP2012<ref type="bibr" target="#b28">[28]</ref> benchmarks) and application acceleration up to 24.5%. However, this work uses only queue spin-lock for experiments and does not investigate the effect of cache coherence in influencing COH in cache-coherent many-cores.In the paper, we show that the lock coherence overhead (LCO) in lock spinning for both spin-lock primitives and the spinning phase of queue spin-lock primitive is a major source of COH. This is because, in lock spinning, each competing thread will generate an exclusive access request to the home node where the lock variable is stored. However, only one of these exclusive accesses will succeed, and the home node will then invalidate the CS lock copies in all the other threads' L1 caches. Only when the winning thread, which is the new owner of the lock variable, receives all acknowledgements from the losing threads, it can proceed to execute the following critical section. To reduce such high overhead coherence round-trip latency, we propose an in-network packet generation (iNPG) technique that can perform early cache invalidation to locking fail-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Percentage of LCO in application running time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A 64-core 8?8 many-core architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1</head><label>1</label><figDesc>Assembly code for lock spinning 1: Lock: LD R2, 0(R1) ;Load of lock to R2 Lock ;Loop to line 1 if seeing lock 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Big router enabled iNPG aiming to shorten lock coherence overhead (LCO) in lock spinning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Big router architecture.when all the four phases are finished. Further, when all EI entries of a lock barrier are released, its TTL starts countdown from the default value. Finally, a lock barrier entry is deleted after its TTL reaches 0.Figure6also shows the format of generated packets. We omit the information fields that are used for flow control in all network packets (such as flit type, VC identifier, etc.), and focus on the additional information fields that are generated. As shown in the figure, the packet generator generates packet in the Inv phase (generates invalidation packet) and the Ack-Fwd phase (changes the destination of the received InvAck message to the ID of the home node). When the locking barrier table is full, following GetX requests will pass through as in a normal router.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Synthesis and physical floorplan &amp; layout results for one big/normal router and the whole 64-core chip.Table1: Simulation platform configurations.</figDesc><graphic url="image-67.png" coords="6,430.77,79.29,111.56,110.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Total CS times, average CPU cycle of each CS, and total CS execution time breakdown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>compares the average and histogram of coherence packet round-trip delay for Original and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of the execution timing profiles of program freqmine with the four comparison mechanisms. Each thread execution is divided into three phases: parallel phase, COH phase, and CSE phase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Avg. coh. pkt. r-trip delay (b) Coh. pkt. r-trip delay hist. (c) Avg. coh. pkt. r-trip delay (d) Coh. pkt. r-trip delay hist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Average coherence packet round-trip delay and coherence packet round-trip delay histogram comparisons for Original (Figure (a), (b)) and iNPG (Figure (c), (d)) with benchmark freqmine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 10b further shows the corresponding coherence Inv-Ack round-trip delay histogram. The maximum coherence packet round-trip delay reaches 97 CPU cycles, exhibiting a "long tail" on the delay histogram, dominating the coherence completion time among the home node and competing threads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Critical section expedition results with different big router deployments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(OR1200) open source CPU as the core model, whose configurations are adjusted according to our architectural setup in Table1. Integrating 32 normal and 32 big routers to 64 OpenRISC cores, we build a 64-core chip following Figure3.</figDesc><table><row><cell></cell><cell>Core</cell><cell>Big router</cell><cell>Router</cell><cell>11395 um</cell></row><row><cell>Technology</cell><cell cols="3">TSMC 40 nm Low power, Typical case (lpbwptc)</cell></row><row><cell>Total layers</cell><cell></cell><cell>28</cell><cell></cell></row><row><cell>Metal layers</cell><cell></cell><cell>10</cell><cell></cell></row><row><cell>Metal stack</cell><cell cols="3">8 normal thick metal layers,</cell></row><row><cell>strategy</cell><cell cols="3">2 ultra thick metal layers</cell></row><row><cell>Power mesh</cell><cell cols="3">Top 2 metal layers (M10 and M9)</cell></row><row><cell>strategy</cell><cell cols="3">dedicated for power mesh</cell></row><row><cell>Gate count</cell><cell>152.5 K</cell><cell>22.4 K</cell><cell>19.9 K</cell></row><row><cell>SC count</cell><cell>23.2 K</cell><cell>4.0 K</cell><cell>3.6 K</cell></row><row><cell>Net count</cell><cell>60.9 K</cell><cell>11.1 K</cell><cell>10.0 K</cell></row><row><cell>Total SC area</cell><cell cols="3">0.97 mm 2 0.14 mm 2 0.13 mm 2</cell></row><row><cell>Cell density</cell><cell>48.26%</cell><cell>66.67%</cell><cell>61.90%</cell></row><row><cell>Total wire length</cell><cell>8.81 m</cell><cell>1.42 m</cell><cell>1.28 m</cell></row><row><cell>Chip area</cell><cell>2.03 mm 2</cell><cell cols="2">0.21 mm 2</cell></row><row><cell cols="4">(a) Module synthesis and layout</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Simulation platform configurations. GHz out-of-order cores. 32-entry instruction queue, 64-entry load queue, 64-entry store queue, 128-entry reorder buffer. L1-Cache 64 banks Private, 32 KB per-core, 4-way set associative, 128 B block size, 2-cycle latency, split I/D caches, 32 MSHRs. L2-Cache 64 banks Chip-wide shared, 1 MB per-bank, 16-way set associative, 128 B block size, 6-cycle latency, 32 MSHRs.</figDesc><table><row><cell>Item</cell><cell>Amount</cell><cell>Description</cell></row><row><cell>Core</cell><cell>64 cores</cell><cell>Alpha 2.0</cell></row></table><note><p>Memory 8 ranks 4 GB DRAM, 512 MB per-rank, up to 16 outstanding requests for each processor, 8 memory controllers. NoC 64 nodes 8?8 mesh network. Each node consists of 1 router, 1 network interface (NI), 1 core, 1 private L1 cache, and 1 shared L2 cache. X-Y dimensional order routing. Router is 2-stage pipelined, 6 VCs per port, 4 flits per VC, 4 virtual networks. 128-bit data path. Directory based MOESI. One cache block consists of 1 8-flit packet. One coherence control message consists of 1 single-flit packet. OCOR [40] -128 retry times in the spinning phase. 9 priority levels, 8 higher levels for requests in the spinning phase; each priority level is mapped to 16 retry times; 1 lowest priority level for wakeup requests. iNPG -Unless otherwise specified, 32 normal routers and 32 big routers, where one big router is deployed between every two normal routers. 16-entry locking barrier table in a big router.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>across Group 1 applications, critical sections are averagely expedited by 1.2? inFigure 11: Critical section expedition results achieved by the four comparison mechanisms.Figure 12: Application ROI finish time achieved by the four comparison mechanisms.</figDesc><table><row><cell></cell><cell>6x</cell><cell>Original</cell><cell>OCOR</cell><cell>iNPG</cell><cell>iNPG+OCOR</cell></row><row><cell>Relative CS improvement</cell><cell>1x 2x 3x 4x 5x</cell><cell>Group 1</cell><cell cols="2">Group 2</cell><cell>Group 3</cell></row><row><cell></cell><cell>0x</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">120%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Parallel phase</cell><cell>Original</cell><cell>OCOR</cell><cell>iNPG</cell><cell>iNPG+OCOR</cell></row><row><cell cols="2">100%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Percentage</cell><cell>40% 60% 80%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Group 1</cell><cell cols="2">Group 2</cell><cell>Group 3</cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2.5?</cell><cell></cell></row><row><cell cols="4">in iNPG+OCOR. Across Group 3 applications, critical sec-</cell><cell></cell></row><row><cell cols="4">tions are further expedited by 1.6? in OCOR, 2.7? in iNPG,</cell><cell></cell></row><row><cell cols="4">and 4.0? in iNPG+OCOR, respectively. Across all 24 pro-</cell><cell></cell></row><row><cell cols="4">grams, OCOR expedites critical sections averagely by 1.45?</cell><cell></cell></row><row><cell cols="4">and maximumly by 1.90? (with program dedup). Further,</cell><cell></cell></row><row><cell cols="4">iNPG averagely expedites critical section by 1.98? and max-</cell><cell></cell></row><row><cell cols="4">imumly by 3.48? (with program nab). In iNPG+OCOR, the</cell><cell></cell></row><row><cell cols="4">average and maximum critical section completion is exped-</cell><cell></cell></row><row><cell cols="4">ited to 2.71? and 5.45? (with program nab), respectively.</cell><cell></cell></row><row><cell cols="4">Compare iNPG over OCOR, iNPG effectively expedites crit-</cell><cell></cell></row><row><cell cols="4">ical section access by 1.35? on average and 2.03? at max-</cell><cell></cell></row><row><cell cols="3">imum (with program nab).</cell><cell></cell><cell></cell></row></table><note><p>OCOR, 1.4? in iNPG, and 1.8? in iNPG+OCOR. Across Group 2 applications, average critical section expedition is further increased to 1.5? in OCOR, 1.9? in iNPG, and</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://kernelnewbies.org/Linux_4.2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.unixdownload.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://openrisc.io/implementations</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We excluded blackscholes and swaptions, as the former contains merely barrier synchronization and the latter only one CS.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>For legibility, we use short names to denote applications with long names, in which body is short for bodytrack, can for canneal, face for facesim, fluid for fluidanimate, freq for freqmine, and stream for streamcluster.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">GARNET: A Detailed On-Chip Network Model Inside A Full-System Simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="33" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Parallel and Distributed Systems (TPDS)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The PARSEC Benchmark Suite: Characterization and Architectural Implications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bienia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Gem5 Simulator</title>
		<author>
			<persName><forename type="first">N</forename><surname>Binkert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sardashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sewell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Critical Lock Analysis: Diagnosing Critical Section Bottlenecks in Multithreaded Applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Distributed Synchronizers</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Jayasimha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Processing (ICPP)</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SCORPIO: A 36-Core Research Chip Demonstrating Snoopy Coherence on a Scalable Mesh NoC With In-Network Ordering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Daya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><forename type="middle">O</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Chandrakasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Criticality Stacks: Identifying Critical Threads in Parallel Programs Using Synchronization Behavior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Du</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Sartor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="511" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel Application Memory Scheduling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Miftakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fallin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="362" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">In-Network Cache Coherence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Eisley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="321" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speedup Stacks: Identifying Scaling Bottlenecks in Multi-Threaded Applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Du</forename><surname>Bois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modeling Critical Sections in Amdahl&apos;s Law and Its Implications for Multicore Design</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Kleinfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Melton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Processing</title>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="764" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient Synchronization Primitives for Large-Scale Cache-Coherent Multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R J</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M K</forename><surname>Vernon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P J</forename><surname>Woest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="64" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The NYU Ultracomputer -Designing an MIMD Shared Memory Parallel Computer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Mcauliffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Snir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="239" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Synchronization Algorithms for Shared-Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Graunke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thakkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="60" to="69" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flat Combining and the Synchronization-Parallelism Tradeoff</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Incze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tzafrir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Parallelism in Algorithms and Architectures (SPAA)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable Flat-Combining based Synchronous Queues</title>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Computer Architecture: A Quantitative Approach, Fifth Edition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hennessy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Patterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Amdahl &apos;s Law in the Multicore Era</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Marty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2008-07">July. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Utility-Based Acceleration of Multithreaded Applications on Asymmetric CMPs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Joao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="154" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Single-Cycle Collective Communication Over A Shared Network Fabric</title>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Networks on Chip (NoCS)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards the Ideal On-chip Fabric for 1-to-many and Many-to-1 Communication</title>
		<author>
			<persName><forename type="first">T</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="71" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Age-Based Scheduling for Asymmetric Multiprocessors</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Lakshminarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synchronization with Multiprocessor Caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ramachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="27" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Computer Systems (ToCS)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scheduling Multiple Multithreaded Applications on Asymmetric and Symmetric Chip Multiprocessors</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Morad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolodny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">C</forename><surname>Weiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Parallel Architectures, Algorithms and Programming (PAAP)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">SPEC OMP2012 -An Application Benchmark Suite for Parallel Systems Using OpenMP</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">C</forename><surname>Brantley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hackenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parrott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robichaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shelepugin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Waveren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on OpenMP in a Heterogeneous World</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="223" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Delay Model and Speculative Architecture for Pipelined Routers</title>
		<author>
			<persName><forename type="first">L.-S</forename><surname>Peh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on High-Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="255" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hot-Spot Contention and Combining in Multistage Interconnection Networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Norton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="943" to="948" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Synchronization with Eventcounts and Sequencers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kanodia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="123" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Monarch Parallel Processor Hardware Design</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Rettberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Crowther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Carvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Tomlinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="28" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Shared-Memory Synchronization, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knights Landing: Second-Generation Intel Xeon Phi Product</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sodani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gramunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="34" to="46" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A Primer on Memory Consistency and Cache Coherence, ser. Synthesis Lectures on Computer Architecture</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Sorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Accelerating Critical Section Execution with Asymmetric Multi-Core Architectures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="60" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Accelerating Critical Section Execution with Multicore Architectures</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="70" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feedback-Directed Pipeline Parallelism</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Suleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Khubaib</surname></persName>
		</author>
		<author>
			<persName><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel Architectures and Compilation Techniques (PACT)</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="147" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scheduling Heterogeneous Multi-Cores Through Performance Impact Estimation (PIE)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Van Craeynest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Eeckhout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narvaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Opportunistic Competition Overhead Reduction for Expediting Critical Section in NoC Based CMPs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="279" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Distributing Hot-Spot Addressing in Large-Scale Multiprocessors</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers (TC)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="388" to="395" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
