<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
							<email>wlouyang@ee.cuhk.edu.hk</email>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ping</forename><surname>Shi Qiu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yonglong</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName><surname>Yang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Wanli Ouyang (equal contribution)</orgName>
								<address>
									<addrLine>Xiaogang Wang</addrLine>
									<settlement>Xingyu Zeng (equal contribution), Hongsheng Li, Zhe Wang</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution" key="instit1">Chen-Change Loy</orgName>
								<orgName type="institution" key="instit2">Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong. Wanli Ouyang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DeepID-Net: Object Detection with Deformable Part Based Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">728C4809A550438379DAD34FD2B31B7A</idno>
					<idno type="DOI">10.1109/TPAMI.2016.2587642</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TPAMI.2016.2587642, IEEE Transactions on Pattern Analysis and Machine Intelligence IEEE TRANSACTIONS PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CNN</term>
					<term>convolutional neural networks</term>
					<term>object detection</term>
					<term>deep learning</term>
					<term>deep model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose deformable deep convolutional neural networks for generic object detection. This new deep learning object detection framework has innovations in multiple aspects. In the proposed new deep architecture, a new deformation constrained pooling (def-pooling) layer models the deformation of object parts with geometric constraint and penalty. A new pre-training strategy is proposed to learn feature representations more suitable for the object detection task and with good generalization capability. By changing the net structures, training strategies, adding and removing some key components in the detection pipeline, a set of models with large diversity are obtained, which significantly improves the effectiveness of model averaging. The proposed approach improves the mean averaged precision obtained by RCNN [16], which was the state-of-the-art, from 31% to 50.3% on the ILSVRC2014 detection test set. It also outperforms the winner of ILSVRC2014, GoogLeNet, by 6.1%. Detailed component-wise analysis is also provided through extensive experimental evaluation, which provides a global view for people to understand the deep learning object detection pipeline.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Object detection is one of the fundamental challenges in computer vision and has attracted a great deal of research interest <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Intra-class variation in appearance and deformation are among the main challenges of this task.</p><p>Because of its power in learning features, the convolutional neural network (CNN) is being widely used in recent largescale detection and recognition systems <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b81">[82]</ref>, <ref type="bibr" target="#b80">[81]</ref>. Since training deep models is a non-convex optimization problem with millions of parameters, the choice of a good initial point is a crucial but unsolved problem, especially when deep CNN goes deeper <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b26">[27]</ref>. It is also easy to overfit to a small training set. Researchers find that supervised pretraining on large-scale image classification data and then finetuning for the target object detection task is a practical solution <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, we observe that there is still a gap between the pretraining task and the finetuning task that makes pretraining less effective. The problem of the training scheme is the mismatch between pretraining for the image classification task and fine-tuning for the object detection task. For image classification, the input is a whole image and the task is to recognize the object within this image. Therefore, learned feature representations have robustness to scale and location change of objects in images. Taking Fig. <ref type="figure" target="#fig_0">1</ref>(a) as an example, no matter how large and where a person is in the image, the image should be classified as person. However, robustness to object size and location is not required for object detection. For object detection, candidate regions are cropped and warped before they are used as input of the deep model. Therefore, the positive candidate regions for the object class person should have their locations aligned and their sizes normalized. On the contrary, the deep model is expected to be sensitive to the change in position and size in order to accurately localize objects. An example to illustrate the mismatch is shown in Fig. <ref type="figure" target="#fig_8">1 (a)</ref>. Because of such mismatch, the image classification task is not an ideal choice to pretrain the deep model for object detection. Therefore, a new pretraining scheme is proposed to train the deep model for object detection more effectively.</p><p>Part deformation handling is a key factor for the recent progress in object detection <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Our new CNN layer is motivated by three observations. First, deformable visual patterns are shared by objects of different categories. For example, the circular visual pattern is shared by both banjo and ipod as shown in Fig. <ref type="figure" target="#fig_8">1(b)</ref>. Second, the regularity on deformation exists for visual patterns at different semantic levels. For example, human upper bodies, human heads, and human mouths are parts at different semantic levels with different deformation properties. Third, a deformable part at a higher level is composed of deformable parts at a lower level. For example, a human upper body is composed of a head and other body parts. With these observations, we design a new deformation-constrained pooling (def-pooling) layer to learn the shared visual patterns and their deformation properties for multiple object classes at different semantic levels and composition levels.</p><p>The performance of deep learning systems depends significantly on implementation details <ref type="bibr" target="#b3">[4]</ref>. However, an evaluation of the performance of the recent deep architectures on the common ground for large-scale object detection is missing. As a respect to the investigation on details in deep learning <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b15">[16]</ref>, this paper compares the performance of recent deep models, including AlexNet <ref type="bibr" target="#b24">[25]</ref>, ZF <ref type="bibr" target="#b74">[75]</ref>, Overfeat <ref type="bibr" target="#b51">[52]</ref>, and GoogLeNet <ref type="bibr" target="#b59">[60]</ref> under the same setting for different pretraining-finetuning schemes. We also provide experimental analysis on the properties that cause the accuracy variation in different object classes.</p><p>In this paper, we propose a deformable deep convolutional neural network for object detection; named as DeepID-Net. In DeepID-Net, we jointly learn the feature representation and part deformation for a large number of object categories. We also investigate many aspects in effectively and efficiently training and aggregating the deep models, including bounding box rejection, training schemes, context modeling, and model averaging. The proposed new framework significantly advances the state-of-the-art for deep learning based generic object detection, such as the well known RCNN <ref type="bibr" target="#b15">[16]</ref> framework. This paper also provides detailed component-wise experimental results on how our approach can improve the mean Averaged Precision (AP) obtained by RCNN <ref type="bibr" target="#b15">[16]</ref>  Preliminary version of this paper is published in <ref type="bibr" target="#b37">[ 38]</ref>. This paper include more analysis on the proposed approach and add experimental investigation on the properties that influence the accuracy in detecting objects.</p><p>The models pretrained by both image-level annotation and object-level annotation for AlexNet <ref type="bibr" target="#b24">[25]</ref>, ZF <ref type="bibr" target="#b74">[75]</ref>, overfeat <ref type="bibr" target="#b51">[52]</ref> and GoogLeNet <ref type="bibr" target="#b59">[60]</ref> and the models after fine-tuning on ILSVRC2014 are provided online<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Since many objects have non-rigid deformation, the ability to handle deformation improves detection performance. Deformable part-based models were used in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b82">[83]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b69">[70]</ref>, <ref type="bibr" target="#b64">[65]</ref> for handling translational movement of parts. To handle more complex articulations, size change and rotation of parts were modeled in <ref type="bibr" target="#b12">[13]</ref>, and mixture of part appearance and articulation types were modeled in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b71">[72]</ref>. A dictionary of shared deformable patterns was learned in <ref type="bibr" target="#b19">[ 20]</ref>. In these approaches, features were manually designed.</p><p>Because of the power on learning feature representation, deep models have been widely used for object recognition, detection and other vision tasks <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b84">[85]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b79">[80]</ref>, <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>. Krizhevsky et al. proposed a neural network with 60 million parameters and 650,000 neurons <ref type="bibr" target="#b24">[25]</ref>. This neural network was the first to show the power of deep CNN in large-scale computer vision task. Later on, Razavian et al. <ref type="bibr" target="#b44">[45]</ref> showed that the OverFeat network <ref type="bibr" target="#b51">[52]</ref> trained for object classification on ILSVRC13 was a good feature representation for the diverse range of recognition tasks of object image classification, scene recognition, fine grained recognition, attribute detection and image retrieval applied to a diverse set of datasets. As a further step of using the model trained for ImageNet object classification, Girshick et al. found finetuning the CNN pretrained on the ImageNet object classification to be effective on various object detection benchmark datasets <ref type="bibr" target="#b15">[16]</ref>. In existing deep CNN models, max pooling and average pooling were useful in handling deformation but could not learn the deformation penalty and geometric models of object parts. The deformation layer was first proposed in <ref type="bibr" target="#b35">[ 36]</ref> for pedestrian detection. In this paper, we extend it to general object detection on ImageNet. In <ref type="bibr" target="#b35">[36]</ref>, the deformation layer was constrained to be placed after the last convolutional layer. In this work the def-pooling layer can be placed after all the convolutional layers to capture geometric deformation at all the information abstraction levels. In <ref type="bibr" target="#b35">[36]</ref>, it was assumed that a pedestrian only had one instance of a body part, so each part filter only had one optimal response in a detection window. In this work, it is assumed that an object has multiple instances of a part (e.g. a car has many wheels), so each part filter is allowed to have multiple response peaks in a detection window. Moreover, we allow multiple object categories to share deformable parts and jointly learn them with a single network. This new model is more suitable for general object detection.</p><p>The use of context has gained attention in recent works on object detection. The context information investigated in literature includes regions surrounding objects <ref type="bibr" target="#b4">[ 5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b13">[14]</ref>, object-scene interaction <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, and the presence, location, orientation and size relationship among objects <ref type="bibr" target="#b1">[ 2]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b60">[61]</ref>. In this paper, we use whole-image classification scores over a large number of classes from a deep model as global contextual information to refine detection scores.</p><p>Besides feature learning, deformation modeling, and context modeling, there were also other important components in the object detection pipeline, such as pretraining <ref type="bibr" target="#b15">[16]</ref>, network structures <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b24">[25]</ref>, refinement of bounding box locations <ref type="bibr" target="#b15">[16]</ref>, and model averaging <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b20">[21]</ref>. While these components were studies individually in different works, we integrate them into a complete pipeline and take a global view of them with component-wise analysis under the same experimental setting. It is an important step to understand and advance deep learning based object detection.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of our approach</head><p>An overview of our proposed approach is shown in Fig. <ref type="figure" target="#fig_3">2</ref>. We take the ImageNet object detection task as an example. The ImageNet image classification and localization dataset with 1,000 classes is chosen to pretrain the deep model. Its object detection dataset has 200 object classes. In the experimental section, the approach is also applied to the PASCAL VOC. The pretraining data keeps the same, while the detection dataset only has 20 object classes. The steps of our approach are summarized as follows.</p><p>1) Selective search proposed in <ref type="bibr" target="#b54">[55]</ref> and edgeboxes proposed in <ref type="bibr" target="#b83">[84]</ref> are used to propose candidate bounding boxes. 2) An existing detector, RCNN <ref type="bibr" target="#b15">[16]</ref> in our experiment, is used to reject bounding boxes that are most likely to be background. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture of DeepID-Net</head><p>DeepID-Net in Fig. <ref type="figure" target="#fig_4">3</ref> has three parts: (a) The baseline deep model. The ZF model proposed in <ref type="bibr" target="#b74">[ 75]</ref> is used as the default baseline deep model when it is not specified. (b) Branches with def-pooling layers. The input of these layers is the conv5, the last convolutional layer of the baseline model. The output of conv5 is convolved with part filters of variable sizes and the proposed def-pooling layers in Section   (c) The deep model (ZF) to obtain image classification scores of 1000 classes. Its input is the whole image, as shown in Fig. <ref type="figure" target="#fig_7">3(c</ref>). The image classification scores are used as contextual information to refine the classification scores of bounding boxes. Detail are given in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">New pretraining strategy</head><p>The widely used training scheme in deep learning based object detection <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b77">[78]</ref>, <ref type="bibr" target="#b59">[60]</ref> including RCNN is denoted by Scheme 0 and described as follows: 1) Pretrain deep models by using the image classification task, i.e. using image-level annotations from the ImageNet image classification and localization training data. 2) Fine-tune deep models for the object detection task, i.e. using object-level annotations from the object detection training data. The parameters learned in Step (1) are used as initialization. The deep model structures at the pretraining and fine-tuning stages are only different in the last fully connected layer for predicting labels (1, 000 classes for the ImageNet classification task vs. 200 classes for the ImageNet detection task). Except for the last fully connected layers for classification, the parameters learned at the pretraining stage are directly used as initial values for the fine-tuning stage.</p><p>We propose to pretrain the deep model on a large auxiliary object detection training data instead of the image classification data. Since the ImageNet Cls-Loc data provides objectlevel bounding boxes for 1000 classes, more diverse in content than the ImageNet Det data with 200 classes, we use the image regions cropped by these bounding boxes to pretrain the baseline deep model in Fig. <ref type="figure" target="#fig_7">3(a)</ref>. The proposed pretraining strategy is denoted as Scheme 1 and bridges the image-vs. object-level annotation gap in RCNN. Another potential mismatch between pretraining and finetuning comes from the fact that the ImageNet classification and localization (Cls-Loc) data has 1, 000 classes, while the ImageNet detection (Det) data only targets on 200 classes, most of which are within the 1, 000 classes. In many practical applications, the number of object classes to be detected is small. People question the usefulness of auxiliary training data outside the target object classes. Our experimental study shows that feature representations pretrained with 1, 000 classes have better generalization capability, which leads to better detection accuracy than pretraining with a subset of the Cls-Loc data only belonging to the 200 target classes in detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Def-pooling layer 3.4.1 DPM and its relationship with CNN</head><p>In the deformable part based model (DPM) <ref type="bibr" target="#b11">[ 12]</ref> for object detection, the following steps are used at the testing stage: 1) Extract HOG feature maps from the input image.</p><p>2) Obtain the part detection score maps by filtering the HOG feature maps with the learned part filters/detectors. The part filters are learned by latent SVM. 3) Obtain deformable part score maps by subtracting deformation penalty from part detection score maps. 4) Sum up the deformable part score maps from multiple parts to obtain the final object detection score map. Denote the convolutional layer at the lth layer by conv l . Denote the output maps of conv l by M l . The steps above for DPM have the following relationship for CNN: 1) The HOG feature map in DPM corresponds to the output of a convolutional layer. Consecutive convolutional layers and pooling layers can be considered as extracting feature maps from input image. 2) The part detection maps in DPM correspond to the output response maps of the convolutional layer. For example, the output of conv l-1 is the feature maps M l-1 , which are treated as input feature maps of conv l . Filtering on HOG feature maps using part filters in DPM is similar to filtering on the feature maps M l-1 using the filters of conv l in CNN. Each output channel of conv l corresponds to a part detection map in DPM. The filter of conv l for an output channel corresponds to a part filter in DPM. The response map in CNN is called part detection map in the following of this paper.  4) Summing up the deformable part score maps for multiple parts in DPM is similar to summing up multiple def-pooling layer output maps with a special case of convolutional layer. Def-pooling layer output maps can be summed up by using a convolutional layer after the def-pooling layer with filter size 1 × 1 and filter coefficients being constant 1 for the convolutional layer. In summary, HOG feature extraction, part detector filtering, deformation penalty subtraction and part detection scores aggregation in DPM <ref type="bibr" target="#b11">[12]</ref> have their corresponding operations in the CNN as shown in Fig. <ref type="figure" target="#fig_9">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Definition of the def-pooling layer</head><p>Similar to max-pooling and average-pooling, the input of a def-pooling layer is the output of a convolutional layer. The convolutional layer produces C maps of size W × H. Denote M c as the cth map. Denote the (i, j)th element of M c by m (5)</p><formula xml:id="formula_0">(i,j) c , i = 1, . . . , W, j = 1, . .</formula><p>• (x, y) denotes the assumed anchor location of object part.</p><p>• (δ x , δ y ) denotes the translation/displacement of object part from the anchor position. • z δx,δy as defined in (4) is the deformed location from the assumed anchor position. . Fig. <ref type="figure" target="#fig_15">7</ref> shows some visualized parts.</p><p>Example 3. The deformation layer in <ref type="bibr" target="#b35">[36]</ref> is a special case of the def-pooling layer by enforcing that z δx,δy in (1) covers all the locations in conv l-1,i and only one output with a pre-defined location is allowed for the def-pooling layer (i.e. R = ∞, s x = W , and s y = H). The proof can be found in Appendix A. To implement quadratic deformation penalty used in <ref type="bibr" target="#b11">[12]</ref>, we can predefine {d δx,δy c,n } n=1,2,3,4 = {δ x , δ y , (δ x ) 2 , (δ y ) 2 } and learn parameters a n . As shown in Appendix A, the def-pooling layer under this setting can represent deformation constraint in the deformable part based model (DPM) <ref type="bibr" target="#b11">[12]</ref> and the DP-DPM <ref type="bibr" target="#b17">[18]</ref>.</p><p>Take Example 2 as an example for BP learning. a c,n is the parameter in this layer and d * is pre-defined constant. Then we have:</p><formula xml:id="formula_1">∂b (x,y) c ∂ac,n = -d (Δx,Δy) c,n , (Δx, Δy) = argmax δx,δy ∈{-R,••• ,R} {m z δx ,δy c -φ(δx, δy)}. (<label>6</label></formula><formula xml:id="formula_2">)</formula><p>where (Δ x , Δ y ) is the position with the maximum value in (1). The gradients for the parameters in the layers before the def-pooling layer are back-propagated like max-pooling layer.</p><p>Similar to max-pooling and average pooling, subsampling can be done as follows:</p><formula xml:id="formula_3">b (x,y) c = b(sx•x,sy•y) c (7)</formula><p>For M c of size W × H, the subsampled output has size W sx × H sy . Therefore, multiple instances of an object part at multiple anchor locations are allowed for each part filer.</p><p>In our implementation, N = 1, R = 2 and Example 2 is used for def-pooling, there are no fully connected layers after conv7 1 , 2, 3 in Fig. <ref type="figure" target="#fig_4">3</ref>. We did not find improvement on ImageNet by further increasing N and R. Further study on N and R could be done on other datasets and particular categories in the future work.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Analysis</head><p>A visual pattern has different spatial distributions in different object classes. For example, traffic lights and ipods have geometric constraints on the circular visual pattern in Fig. <ref type="figure" target="#fig_16">8</ref>. The weights connecting the convolutional layers conv7 1 -conv7 3 in Fig. <ref type="figure" target="#fig_4">3</ref> and classification scores are determined by the spatial distributions of visual patterns for different classes.</p><p>For example, the car class will have large positive weights in the bottom region but negative weights in the upper region for the circular pattern. On the other hand, the traffic light class will have positive weights in the upper region for the circular pattern.</p><p>A single output of the convolutional layer conv7 1 in Fig. <ref type="figure" target="#fig_4">3</ref> is from multiple part scores in def6 1 . The relationship between parts of the same layer def6 1 is modeled by conv7 1 .  The def-pooling layer has the following advantages. 1) It can replace any pooling layer, and learn deformation of parts with different sizes and semantic meanings. For example, at a higher level, visual patterns can be large parts, e.g. human upper bodies, and the def-pooling layer can capture the deformation constraint of human upper parts. At a middle level, the visual patterns can be smaller parts, e.g. heads. At the lowest level, the visual patterns can be very small, e.g. mouths. A human upper part is composed of a deformable head and other parts. The human head is composed of a deformable mouth and other parts. Object parts at different semantic abstraction levels with different deformation constraints are captured by def-pooling layers at different levels. The composition of object parts is naturally implemented by CNN with hierarchical layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Circular</head><p>2) The def-pooling layer allows for multiple deformable parts with the same visual cue, i.e. multiple response peaks are allowed for one filter. This design is from our observation that an object may have multiple object parts with the same visual pattern. For example, three light bulbs co-exist in a traffic light in Fig. <ref type="figure" target="#fig_13">5</ref>. 3) As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, the def-pooling layer is a shared representation for multiple classes and therefore the learned visual patterns in the def-pooling layer can be shared among these classes. As examples in Fig. <ref type="figure" target="#fig_16">8</ref>, the learned circular visual patterns are shared as different object parts in traffic lights, cars, and ipods. The layers proposed in <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b17">[18]</ref> does not have these advantages, because they can only be placed after the final convolutional layer, assume one instance per object part, and does not share visual patterns among classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-tuning the deep model with hinge-loss</head><p>In RCNN, feature representation is first learned with the softmax loss in the deep model after fine-tuning. Then in a separate step, the learned feature representation is input to a linear binary SVM classifier for detection of each class. In our approach, the softmax loss is replaced by the 200 binary hinge losses when fine-tuning the deep model. Thus the deep model fine-tuning and SVM learning steps in RCNN are merged into one step. The extra training time required for extracting features (∼ 2.4 days with one Titan GPU) is saved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Contextual modeling</head><p>The deep model learned for the image classification task (Fig. <ref type="figure" target="#fig_7">3 (c</ref>)) takes scene information into consideration while the deep model for object detection (Fig. <ref type="figure" target="#fig_4">3</ref>  weights learned in the first stage are selected as features and then a linear SVM is learned for a given object class to be detected. Therefore, only the classification scores of 10 classes from the image classification deep model are used for each class to be detected. The SVM is explicitly trained but not within the network framework. If 5, 20, 50, 100 or 1000 classes are used, the mAP drops by 0, 0.2%, 0.8%, 0.9% and 4.5% respectively when compared with the result of using 10 classes. This result shows that only a few number of classes are helpful for detection. The heuristic selection of 10 classes helps to remove the effect from uncorrelated classes.</p><p>Take object detection for class volleyball as an example in Figure <ref type="figure" target="#fig_17">9</ref>. If only considering local regions cropped from bounding boxes, volleyballs are easy to be confused with bathing caps and golf balls. In this case, the contextual information from the whole-image classification scores is helpful, since bathing caps appear in scenes of beach and swimming pools, golf balls appear in grass fields, and volleyballs appear in stadiums. The whole images of the three classes can be better distinguished because of the global scenery information. Fig. <ref type="figure" target="#fig_17">9</ref> plots the learned linear SVM weights on the 1000-class image classification scores. It is observed that image classes bathing cap and golf ball suppress the existence of volleyball in the refinement of detection scores with negative weights, while the image class volleyball enhances the detection score of volleyball.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Combining models with high diversity</head><p>Model averaging has been widely used in object detection. In existing works <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b20">[21]</ref>, the same deep architecture was used. Models were different in cropping images at different locations or using different learned parameters. In our model averaging scheme, we learn models under multiple settings. The settings of the models used for model averaging are shown in Table <ref type="table">4</ref>. They are different in net structures, pretraining schemes, loss functions for the deep model training, adding def-pooling layer or not. The motivation is that models generated in this way have higher diversity and are complementary to each other in improving the detection results after model averaging. For example, although model no. 4 has low mAP, it is found by greedy search because its pretraining scheme is different from other models and provides complementary scores for the averaged scores.</p><p>The 6 models as shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>Our experimental results are implemented based on the Caffe <ref type="bibr" target="#b23">[24]</ref>. Only selective search is used for proposing regions if not specified.</p><p>Overall result on PASCAL VOC. For the VOC-2007 detection dataset <ref type="bibr" target="#b10">[11]</ref>, we follow the approach in <ref type="bibr" target="#b15">[16]</ref> for splitting the training and testing data. Table <ref type="table" target="#tab_2">1</ref> shows the experimental results on VOC-2007 testing data, which include approaches using hand-crafted features <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b11">[12]</ref>, deep CNN features <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b67">[68]</ref>, and CNN features with deformation learning <ref type="bibr" target="#b17">[18]</ref>. Since all the stateof-the-art works reported single-model results on this dataset, we also report the single-model result only. Our model was pretrained on bounding box annotation, with deformation, without context, and with GoogLeNet as the baseline net. Ours outperforms RCNN <ref type="bibr" target="#b15">[16]</ref> and SPP <ref type="bibr" target="#b20">[21]</ref> by about 5% in mAP. RCNN, SPN and our model are all pre-trained on the ImageNet Cls-Loc training data and fine-tuned on the VOC-2007 training data. Table <ref type="table" target="#tab_3">2</ref> shows the per-class mAPs for our approach with G-Ntt and RCNN with VGG and GoogleNet <ref type="bibr" target="#b15">[16]</ref>. Fig. <ref type="figure" target="#fig_18">10</ref> shows the analysis on false positives using the approach in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Overall result on MS-COCO. Without using context, our single model has mAP 25.6% on the MS-COCO Test-dev dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>Experimental Setup on ImageNet. The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014 <ref type="bibr" target="#b48">[49]</ref> contains two different datasets: 1) the classification and localization (Cls-Loc) dataset and 2) the detection (Det) dataset.</p><p>The training data of Cls-Loc contains 1.2 million images with labels of 1, 000 categories. It is used to pretrain deep models. The same split of train and validation data from the Cls-Loc is used for image-level annotation and object-level annotation pretraining. The Det contains 200 object categories and is split into three subsets, train, validation (val), and test data. We follow RCNN <ref type="bibr" target="#b15">[16]</ref> in splitting the val data into val 1 and val 2 . Val 1 is used to train models, val 2 is used to evaluate separate components, and test is used to evaluating the overall performance. The val 1 /val 2 split is the same as that in <ref type="bibr" target="#b15">[16]</ref>.</p><p>Overall result on ImageNet Det. RCNN <ref type="bibr" target="#b15">[16]</ref> is used as the state-of-the-art for comparison. The source code provided by the authors was used and we were able to repeat their results. Table <ref type="table">3</ref> summarizes the results from ILSVRC2014 object detection challenge. It includes the best  Table <ref type="table">4</ref> Models used for model averaging. The result of mAP is on val 2 without bounding box regression and context. For net design, D-Def(O) denotes our DeepID-Net that uses def-pooling layers using Overfeat as baseline structure, D-Def(G) denotes DeepID-Net that uses def-pooling layers using GoogLeNet as baseline structure, G-net denotes GoogLeNet. For pretraining, 0 denotes the pretraining scheme of RCNN <ref type="bibr" target="#b15">[ 16]</ref>, 1 denotes the Scheme 1 in Section 3.3. For loss of net, h denotes hinge loss, s denotes softmax loss. Bounding box rejection is used for all models. Selective search and edgeboxes are used for proposing regions. model number results on the test data submitted to ILSVRC2014 from GoogLeNet <ref type="bibr" target="#b59">[60]</ref>, DeepInsight <ref type="bibr" target="#b66">[67]</ref>, UvA-Euvision <ref type="bibr" target="#b54">[55]</ref>, Berkeley Vision <ref type="bibr" target="#b15">[16]</ref>, which ranked top among all the teams participating in the challenge. In terms of single-model and model averaging performance, we achieve the highest mAP. It outperforms the winner of ILSVRC2014, GoogleNet, by 6.1% on mAP. Table <ref type="table">4</ref> shows the 6 models we used in model averaging.</p><formula xml:id="formula_4">1 2 3 4 5 6 net design D-Def(O) D-Def(G) G-net G-net D-Def(G) D-Def(G) Pretrain 1 1 1 0 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation study</head><p>The ImageNet Det is used for ablation study. Bounding box regression is not used if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Baseline deep model and bounding box rejection</head><p>As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, a baseline deep model is used in our DeepID-Net. Table <ref type="table" target="#tab_5">5</ref> shows the results for different baseline deep models and bounding box rejection choices. AlexNet in <ref type="bibr" target="#b24">[25]</ref> is denoted as A-net, ZF in <ref type="bibr" target="#b74">[75]</ref> is denoted as Z-net, and Overfeat in <ref type="bibr" target="#b51">[52]</ref> is denoted as O-net. GoogLeNet in <ref type="bibr" target="#b59">[60]</ref> is denoted as G-net. Except for the two components investigated in Table <ref type="table" target="#tab_5">5</ref>, other components are the same as RCNN, while the new training schemes and the new components introduced in Section 3.2 are not included. The configuration in the second column of Table <ref type="table" target="#tab_5">5</ref> is the same as RCNN (mean mAP 29.9%).</p><p>Based on RCNN, applying bounding box rejection improves mAP by 1%. Therefore, bounding box rejection not only saves the time for training and validating new models, which is critical for future research, but also improves detection accuracy. Bounding box rejection is not constrained to particular detectors such as RCNN or fast RCNN. The time required to process one image is around 3.5 seconds per image using RCNN and around 0.2 seconds using fast RCNN. Both with bounding box rejection, ZF <ref type="bibr" target="#b74">[75]</ref> performs better than AlexNet <ref type="bibr" target="#b24">[25]</ref>, with 0.9% mAP improvement. Overfeat <ref type="bibr" target="#b51">[52]</ref> performs better than ZF, with 4.8% mAP improvement. GoogLeNet <ref type="bibr" target="#b59">[ 60]</ref> performs better than Overfeat, with 1.2% mAP improvement. Experimental results in Table <ref type="table" target="#tab_5">5</ref> show the further investigation on the influence of bounding box rejection scheme in training and testing stage. Experimental results on two different CNN architectures, i.e. A-net and Z-net, show that the mAP is similar whether the rejection scheme in the testing stage is used or not. And the rejection scheme in the training stage is the main factor in improving the mAP. If there is concern that the rejection scheme results in lower recall of the candidate windows at the testing stage, the rejection scheme at the testing stage can be skipped. If not specified, bounding box rejection is used in both training and testing stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Investigation on the number of object classes at the pretraining stage</head><p>In order to investigate the influence from the number of object classes at the pretraining stage, we use the AlextNet and train on the ImageNet classification data without using the bounding box labels. Table <ref type="table" target="#tab_7">7</ref> shows the experimental results. As pointed out in <ref type="bibr" target="#b49">[50]</ref>, the 200 classes in ImageNet detection corresponds to 494 classes in ImageNet classification.   The same fine-tuning configuration is used for these three pretraining settings. Experimental results show that 494-class pretraining performs better than 200-class pretraining by 3% mAP. 1000class pretraining performs better than 494-class pretraining by 4.3% mAP. 3000-class pretraining further improves the mAP by 2.4% compared with 1000-class pretraining. For 3000-class pretraining, each sample carries much more information: for an apple image, the 3000-class pretraining provides further information that it is not the other 2999 classes. And the use of more classes makes the training task challenging and not easy to overfit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Investigation on def-pooling layer</head><p>Different deep model structures are investigated and results are shown in Table <ref type="table" target="#tab_8">8</ref> using the new pretraining scheme in Section 3.3. Our DeepID-Net that uses def-pooling layers as shown in Fig. <ref type="figure" target="#fig_4">3</ref> is denoted as D-Def. Using the Z-net as baseline deep model, the DeepID-Net that uses def-pooling layer in Fig. <ref type="figure" target="#fig_4">3</ref> improves mAP by 2.5%. Def-pooling layer improves mAP by 2.3% for both O-net and G-net. This experiment shows the effectiveness of the def-pooling layer for generic object detection. In our implementation of def-pooling for G-  net, we only replace max-pooling by def-pooling but did not add an additional feature maps like that in Fig. <ref type="figure" target="#fig_4">3</ref>(b). 2.3% mAP improvement is still observed on G-net by replacing the max-pooling with def-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Investigation on different pretraining schemes and baseline net structures</head><p>There are two different annotation levels, image and object. Table <ref type="table" target="#tab_10">9</ref> shows the results for investigation on annotation levels and net structures. When producing these results, other new components introduced in Section 3.4-3.6 are not included.</p><p>For pretraining, we drop the learning rate by 10 when the classification accuracy of validation data reaches plateau, until no improvement is found on the validation data. For finetuning, we use the same initial learning rate (0.001) and the same number of iterations (20,000) for dropping the learning rate by 10 for all net structures, which is the same setting in RCNN <ref type="bibr" target="#b15">[16]</ref>.</p><p>Pretraining on object-level-annotation performs better than pretraining on image-level annotation by 4.4% mAP for A-net and 4.2% for Z-net. This experiment shows that object-level annotation is better than image-level annotation in pretraining deep model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Investigation on the overall pipeline</head><p>Table <ref type="table" target="#tab_11">10</ref> summarizes how performance gets improved by adding each component step-by-step into our pipeline. RCNN has mAP 29.9%. With bounding box rejection, mAP is improved by about 1%, denoted by +1% in Table <ref type="table" target="#tab_11">10</ref>. Based on that, changing A-net to Z-net improves mAP by 0.9%. Changing Z-net to O-net improves mAP by 4.8%. O-net to Gnet improves mAP by 1.2%. Replacing image-level annotation by object-level annotation in pretraining, mAP is increased by 2.6%. By combining candidates from selective search and edgeboxes <ref type="bibr" target="#b83">[84]</ref>, mAP is increased by 2.3%. The def-pooling layer further improves mAP by 2.2%. Pretraining the objectlevel annotation with multiple scales <ref type="bibr" target="#b3">[4]</ref> improves mAP by 2.2%. After adding the contextual information from image classification scores, mAP is increased by 0.5%. Bounding box regression improves mAP by 0.4%. With model averaging, the final result is 50.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Per-Class Accuracy as a Function of Object Properties on ILSVRC14 Object Detection Data</head><p>Inspired by the analysis in <ref type="bibr" target="#b49">[50]</ref>, we perform analysis on the object properties that influence the variation in object detection accuracy for different classes in this section. Our result with 50.7% mAP on the val2 data is used for analysis. For real-world size, deformability, and amount of texture, the following conclusion on the detection accuracy as a function of these object properties can be drawn from the experimental results in Fig. <ref type="figure" target="#fig_8">11</ref>:</p><p>Real-world size, XS for extra small (e.g. nail), small (e.g. fox), medium (e.g. bookshelf), large (e.g. car) or XL for extra large (e.g. airplane). The object detection model performs similar on extra small, small or medium ones, which is different from the optimistic model in <ref type="bibr" target="#b49">[50]</ref>. It performs better on extra large and large objects, with extra large objects having the highest mean AP (close to 70%).</p><p>Deformability within instance, Rigid (e.g., mug) or deformable (e.g., snake). Similar to <ref type="bibr" target="#b49">[50]</ref>, we also find that deformable objects have higher accuracy than rigid objects.</p><p>Amount of texture, none (e.g. punching bag), low (e.g. horse), medium (e.g. sheep) or high (e.g. honeycomb). The model is better on objects with at least median level of texture compared to untextured or low textured objects.</p><p>The three properties above are investigated in <ref type="bibr" target="#b49">[ 50]</ref> using optimistic model, i.e. directly compare all the entries in the past 3 years to obtain the most optimistic measurement of state-of-the-art accuracy on each category. Using our best performing model, similar conclusion can be drawn.</p><p>In the following, we investigate new properties that we found influential to object detection accuracy. Object classes are sorted in ascending order using these properties and then uniformly grouped, i.e. all groups have the number of classes.</p><p>Variance in aspect ratio. Aspect ratio is measured by the width of the object bounding box divided by the height of the bounding box. Objects with large variance in aspect ratio, e.g. band aid and nail, are more likely to be slim and have large variation in rotation, which result in the drastic appearance change of the visual information within the object bounding box. Therefore, as shown in Fig. <ref type="figure" target="#fig_8">12</ref>, objects with lower variance in aspect ratio performs better.</p><p>Variance in part existence. Many objects have some of their parts not existing in the bounding box because of occlusion or tight-shot. The variation in part existence causes the appearance variation for object of the same class. For example, a backpack with only its belt in the bounding box is very different in appearance from a backpack with its bag in the bounding box. We labeled the object parts and their existences for all the 200 classes on the val1 data and use them for obtaining the variance in part existence. As shown in Fig. <ref type="figure" target="#fig_8">12</ref>, objects with lower variance in part existence performs better.</p><p>Variance in rotation. In-plane and out-of-plane rotation are factors that influence the within-class appearance variation. An ax with frontal view is very different in appearance from an ax with side view. An upright ax is very different in appearance from a horizontal ax. We labeled the rotation of objects for all the 200 classes on the val1 data and use them for obtaining the variance in rotation. As shown in Fig. <ref type="figure" target="#fig_8">12</ref>, objects with lower variance in rotation performs better.</p><p>Number of objects per image. The number of object per image for the cth object class, denoted by N c , is obtained as follows:</p><formula xml:id="formula_5">Nc = 1 Pc Pc pc=1 (np c ),<label>(8)</label></formula><p>where n pc is the number of objects within the image of the p c th sample for class c, p c = 1 . . . , P c . N c is obtained from the val1 data. When there are large number objects within an image, they may occlude each other and appear as background for the ground truth bounding box of each other, resulting in the added complexity of object appearance and background clutter. As shown in Fig. <ref type="figure" target="#fig_21">13</ref>, some small objects, bee and butterfly, have less than 2 objects per image on average. And they have very high AP, 90.6% for butterfly and 76.9% for bee. We find that the images in val1 with these samples are mostly captured by tight shot, and they have relatively simple background. As shown in Fig. <ref type="figure" target="#fig_21">13</ref>, the model performs better when the number of objects per image is smaller. Mean area per bounding box. We measure the size of the bounding box by the area (width multiplied by height) of this bounding box. We did not use the bounding box size over image size in <ref type="bibr" target="#b49">[50]</ref> because the image size may have influence on image classification and object localization but should have small influence on object detection, in which bounding box is independently evaluated. As shown in Fig. <ref type="figure" target="#fig_22">14</ref>, the average area for different objects varies a lot. Sofa has the largest mean area. Although butterfly and bee are extra small in real-world size, they are large in average areas, 36.8k for bee and 57.7k for butterfly. As shown in Fig. <ref type="figure" target="#fig_21">13</ref>, the average AP is higher when the mean area per bounding box is larger.</p><p>Recall from region proposal. In our model, selective search and edgeboxes are used for proposing the regions. After bounding box rejection, 302 boxes per image are obtained on val1. The average recall is 89.19% for overlap greater than 0.5 and 78.98% for overlap greater than 0.7.</p><p>Fig. <ref type="figure" target="#fig_13">15</ref> shows the 5 classes with lowest and highest average precision and their corresponding factors. The 5 object classes with the lowest accuracy are mostly none-deformable, having low texture, small bounding box size, large number of objects per image, large variation in aspect ratio, part existence and rotation.</p><p>We also tried other properties, like variation in bounding box size, average aspect ratio, number of positive samples, but did not find them to have strong correlation to the detection accuracy.</p><p>Fig. <ref type="figure" target="#fig_23">16</ref> shows the object classes with large mAP improvement and mAP drop when the def-pooling is used. 140 of the 200 classes have their mAP improved. Def-pooling brings large mAP gains for mammals like squirrel with deformation and instruments like banjo with rotation. However, man-made objects such as waffle iron, digital clock, cocktail shaker and vacuum have inconsistent existence of object parts, large variation in rotation and part appearance. Therefore, the mAP gains are negative for these man-made objects.</p><p>Fig. <ref type="figure" target="#fig_25">17</ref> shows the detection accuracy for object classes grouped at different WordNet hierarchical levels. It can be seen that vertebrates that are neither mammal nor fish, i.e. bird, frog, lizard, snake, and turtle, have the largest mAP. Mammals also have large mAP because mammals share similar appearance, have rich texture and have many object classes that help each other in learning their feature representations. Generally, artifacts have lower mAP because they have low texture and large variation in shape. Texts in dashed boxes of Fig. <ref type="figure" target="#fig_25">17</ref> show the absolute mAP gain obtained by bounding box rejection and def-pooling for each group. It can be seen that def-pooling has 9.5% mAP gain for detecting person. Defpooling has higher mAP gain (4.6%) for mammals with regular deformation and part appearance than substances (0.4%) with irregular deformation and part appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">APPENDIX A: RELATIONSHIP BETWEEN THE DEFORMATION LAYER AND THE DPM</head><p>The quadratic deformation constraint in <ref type="bibr" target="#b11">[ 12]</ref> can be represented as follows:</p><formula xml:id="formula_6">m(i,j) = m (i,j) -a1(i-b1 + a3 2a1 ) 2 -a2(j -b2 + a4 2a2 ) 2 , (<label>9</label></formula><formula xml:id="formula_7">)</formula><p>where m (i,j) is the (i, j)th element of the part detection map M, (b  automatically learned. a 1 and a 2 (9) decide the deformation cost. There is no deformation cost if a 1 = a 2 = 0. Parts are not allowed to move if a 1 = a 2 = ∞. (b 1 , b 2 ) and ( a3 2a1 , a4 2a2 ) jointly decide the center of the part. The quadratic constraint in Eq. ( <ref type="formula" target="#formula_6">9</ref>) can be represented using Eq. ( <ref type="formula">1</ref>) as follows: m(i,j) = m (i,j) -a1d = j -b2, a5 = a3 2 /(4a1) + a4 2 /(4a2). <ref type="bibr" target="#b9">(10)</ref> In this case, a 1 , a 2 , a 3 and a 4 are parameters to be learned and d</p><p>(i,j) n</p><p>for n = 1, 2, 3, 4 are predefined. a 5 is the same in all locations and need not be learned. The final output is: b = max (i,j) m(i,j) , <ref type="bibr" target="#b10">(11)</ref> where m(i,j) is the (i, j)th element of the matrix M in <ref type="bibr" target="#b8">(9)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper proposes a deep learning based object detection pipeline, which integrates the key components of bounding box reject, pretraining, deformation handling, context modeling, bounding box regression and model averaging. It significantly advances the state-of-the-art from mAP 31.0%  We enrich the deep model by introducing the def-pooling layer, which has great flexibility to incorporate various deformation handling approaches and deep architectures. Motivated by our insights on how to learn feature representations more suitable for the object detection task and with good generalization capability, a pretraining scheme is proposed. By changing the configurations of the proposed detection pipeline, multiple detectors with large diversity are obtained, which leads to more effective model averaging. This work shows the important modules in an object detection pipeline, although each has its own parameter setting set in an ad hoc way. In the future, we will design an end-to-end system that jointly learns these modules.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The motivation of this paper in new pretraining scheme (a) and jointly learning feature representation and deformable object parts shared by multiple object classes at different semantic levels (b). In (a), a model pretrained on imagelevel annotation is more robust to size and location change while a model pretrained on object-level annotation is better in representing objects with tight bounding boxes. In (b), when ipod rotates, its circular pattern moves horizontally at the bottom of the bounding box. Therefore, the circular patterns have smaller penalty moving horizontally but higher penalty moving vertically. The curvature part of the circular pattern are often at the bottom right positions of the circular pattern. Magnitudes of deformation penalty are normalized to make them comparable across the two examples in (a) for visualization. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>from 31.0% to mean AP 50.3% step-by-step on the ImageNet Large Scale Visual Recognition Challenge 2014 (ILSVRC2014) object detection task. The contributions of this paper are as follows: 1) A new deep learning framework for object detection. It effectively integrates feature representation learning, part deformation learning, context modeling, model averaging, and bounding box location refinement into the detection system. Detailed component-wise analysis is provided through extensive experimental evaluation. This paper is also the first to investigate the influence of CNN structures for the large-scale object detection task under the same setting. By changing the configuration of this framework, multiple detectors with large diversity are generated, which leads to more effective model averaging. 2) A new scheme for pretraining the deep CNN model. We propose to pretrain the deep model on the ImageNet image classification and localization dataset with 1000class object-level annotations instead of with image-level annotations, which are commonly used in existing deep learning object detection [16], [60]. Then the deep model is fine-tuned on the ImageNet/PASCAL-VOC object detection dataset with 200/20 classes, which are the target object classes in the two datasets. 3) A new deformation constrained pooling (def-pooling) layer, which enriches the deep model by learning the deformation of object parts at any information abstraction levels. The def-pooling layer can be used for replacing the max-pooling layer and learning the deformation properties of parts. 4) Analysis on the object properties that influence the variation in object detection accuracy for different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overview of our approach. Detailed description is given in Section 3.1. Texts in red highlight the steps that are not present in RCNN [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 )</head><label>3</label><figDesc>An image region in a bounding box is cropped and fed into the DeepID-Net to obtain 200 detection scores. Each detection score measures the confidence on the cropped image containing one specific object class. Details are given in Section 3.2. 4) The 1000-class whole-image classification scores of a deep model are used as contextual information to refine the detection scores of each candidate bounding box. Details are given in Section 3.6. 5) Average of multiple deep model outputs is used to improve the detection accuracy. Details are given in Section 3.7. 6) Bounding box regression proposed in RCNN [ 16] is used to reduce localization errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>3.4 are used to learn the deformation constraint of these part filters. Parts (a)-(b) output 200-class object detection scores. For the cropped image region that contains a horse as shown in Fig. 3(a), its ideal output should have a high score for the object class horse but low scores for other classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Deep model (clarifai-fast) for 1000-class image classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Architecture of DeepID-Net with three parts: (a) baseline deep model, which is ZF [75] in our single-model detector; (b) layers of part filters with variable sizes and def-pooling layers; (c) deep model to obtain 1000-class image classification scores. The 1000-class image classification scores are used to refine the 200-class bounding box classification scores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 )</head><label>1</label><figDesc>Pretrain the deep model with object-level annotations of 1, 000 classes from ImageNet Cls-Loc train data. 2) Fine-tune the deep model for the 200-class object detection task, i.e. using object-level annotations of 200 classes from ImageNet Det train and val 1 (validation set 1) data. Use the parameters in Step (1) as initialization. Compared with the training scheme of RCNN, experimental results show that the proposed scheme improves mean AP by 4.5% on ImageNet Det val 2 (validation set 2). If only the 200 target classes (instead of 1,000 classes) from the ImageNet Cls-Loc train data are selected for pretraining in Step (1), the mean AP on ImageNet Det val 2 drops by 5.7%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The relationship between the operations in the DPM and the CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>. , H. The def-pooling layer takes a small block with center (x, y) and size (2R + 1) × (2R + 1) from the M c and produce the element of the output as follows: b(x,y) c = ma x δx,δy ∈{-R,••• ,R} mc(zδ x ,δy , δx, δy), (1) where x = 1, . . . , W, y = 1, . . . , H, (2) mc(zδ x ,δy , δx, δy) = m z δx ,δy c φ(δx, δy) (3) z δx,δy = [x, y] T + [δx, δy] T , (4) φ(δx, δy) = N n=1 ac,nd δx,δy c,n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 = 1 1 = 2</head><label>1112</label><figDesc>• m z δx ,δy c in (3) is the element in M c at the location z δx,δy . It is considered as the score of matching the cth part filter with the features at the deformed location z δx,δy . • φ(δ x , δ y ) in (3) and (5) is the deformation penalty of placing the part from the assumed anchor position (x, y) to the deformed location z δx,δy . a c,n and d δx,δy c,n in (5) are parameters of deformation that can be pre-defined or learned by back-propagation (BP). N denotes the number of parameters a c,n and d δx,δy c,n . • mc (z δx,δy , δ x , δ y ) in (1) and (3) is the deformable part score. It is obtained by subtracting the deformation penalty φ(δ x , δ y ) from the visual matching score m z δx ,δy c . • b(x,y) c is the (x, y)th element of the output of the defpooling layer. For the anchor location (x, y), b(x,y) c is obtained by taking the maximum deformable part score mc (z δx,δy , δ x , δ y ) within the displacement range R, i.e. δ x , δ y ∈ {-R, • • • , R}. The def-pooling layer can be better understood through the following examples. Example 1. If N = 1, a n = 1, d δx,δy 1 = 0 for |δ x |, |δ y | ≤ k and d δx,δy 1 = ∞ for |δ x |, |δ y | &gt; k, then this corresponds to max-pooling with kernel size k. It shows that the maxpooling layer is a special case of the def-pooling layer. Penalty becomes very large when deformation reaches certain range. Since the use of different kernel sizes in max-pooling corresponds to different maps of deformation penalty that can be learned by BP, def-pooling provides the ability to learn the map that implicitly decides the kernel size for max-pooling. Example 2. If N = 1 and a n = 1, then d δx,δy 1is the deformation parameter/penalty of moving a part from the anchor location (x, y) by (δ x , δ y ). If the part is not allowed to move, we have d 0,0 1 = 0 and d δx,δy 1= ∞ for (δ x , δ y ) = (0, 0). If the part has penalty 1 when it is not at the assumed location (x, y), then we have d 0,0 1 = 0 and d δx,δy 1 = 1 for (δ x , δ y ) = (0, 0). It allows to assign different penalty to displacement in different directions. If the part has penalty 2 moving leftward and penalty 1 moving rightward, then we have d δx,δy for δ x &lt; 0 and d δx,δy for δ x &gt; 0. Fig.6shows some learned deformation parameters d δx,δy 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Def-pooling layer. The part detection map and the deformation penalty are summed up. Block-wise max pooling is then performed on the summed map to obtain the output B of size H sy × W sx (3 × 1 in this example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The learned deformation penalty for different visual patterns. The penalties in map 1 are low at diagonal positions. The penalties in map 2 and 3 are low at vertical and horizontal locations separately. The penalties in map 4 are high at the bottom right corner and low at the upper left corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The learned part filters visualized using deepdraw [ 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Repeated visual patterns in multiple object classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The SVM weights on image classification scores (a) for the object detection class volleyball (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Fraction of high-scored false positives on VOC-2007 that are due to poor localization (Loc), confusion with similar objects (Sim), confusion with other VOC objects (Oth), or confusion with background or unlabeled objects (BG).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Therefore, we investigate three pretraining settings: 1) use the corresponding 494-class samples in ImageNet classification training data but train the deep model as a 200-class classification problem; 2) use the corresponding 494-class samples in ImageNet classification training data and train the deep model as a 494-class classification problem; 3) use the 1000-class samples in ImageNet classification training data and train the deep model as a 1000-class classification problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. The mean average precision of our best-performing model in the y axis as a function of real-word size (left), deformability (middle), and texture (right) in the x axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Number of objects per image for different classes (left) and the detection accuracy as a function of the average number of objects per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. The average size of the bounding box for different classes (left) and the detection accuracy as a function of the average number of objects per image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 16 .</head><label>16</label><figDesc>Figure 16. The factors for the object classes with mAP improvement (top) and mAP drop (bottom) introduced by the def-pooling. The meaning of x and y axes are the same as 15. Legends denote different object classes and their mAP change caused by def-pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 17 .</head><label>17</label><figDesc>Figure 17. The detection accuracy for object classes grouped at different WordNet hierarchical levels. Tilted text at the right of the circle denotes the number of classes within the 200 object classes of the ILSVRC14 detection task for this WordNet synonym set (synset). Texts at the upper left of the circle denote the absolute mAP gain obtained by bounding box rejection and def-pooling. Un-tilted text below the circle denote the mAP in percentage for this WordNet synset. For example, the WordNet synset 'matter' has height 13, 22 object classes and mAP 49.7%, bounding box rejection has mAP gain of 1.34% and the def-pooling has mAP gain of 0.4%. The 'other vertebrates' denotes the object classes that are vertebrates but not mammal or aquatic vertebrate, similarly for 'other artifacts' and 'other instruments'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>(</head><label></label><figDesc>obtained by RCNN) to 50.3% on the ImageNet object task. Its single model and model averaging performances are the best in ILSVC2014. A global view and detailed componentwise experimental analysis under the same setting are provided to help researchers understand the pipeline of deep learning based object detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 4 are automatically selected by greedy search on ImageNet Det val 2 from 10 models, and the mAP of model averaging is 50.3% on the test data of ILSVRC2014, while the mAP of the best single model is 47.9%.</figDesc><table><row><cell></cell><cell cols="2">Person</cell><cell cols="2">Animal</cell><cell cols="2">Vehicle</cell><cell cols="3">Furniture</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Loc</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sim</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Oth</cell></row><row><cell>Ours:</cell><cell>58%</cell><cell>6% 5% 30%</cell><cell>49%</cell><cell>41% 1% 9%</cell><cell>52%</cell><cell>24% 7% 16%</cell><cell>36%</cell><cell></cell><cell>21% 21%</cell><cell>BG</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">22%</cell><cell></cell></row><row><cell>G-Net:</cell><cell>63%</cell><cell>3% 5% 29%</cell><cell>59%</cell><cell>28% 1% 11%</cell><cell>66%</cell><cell>13% 5% 16%</cell><cell>41%</cell><cell>15%</cell><cell>16% 28%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>Detection mAP (%) on VOC-2007 test. All approaches are trained on VOC-2007 data. Bounding box regression is used in DPM, SPP, RCNN, RCNN-V5, fRCN, and our approach. Only a single model is used for all approaches.</figDesc><table><row><cell cols="8">approach DPM HSC-DPM Regionlet Flair DP-DPM SPP RCNN RCNN-v5 fRCN RPN YOLO Superpixel Label ours</cell></row><row><cell>[17]</cell><cell>[48]</cell><cell>[ 63]</cell><cell>[62] [18] [21] [16]</cell><cell>[16]</cell><cell>[15] [47] [46]</cell><cell>[ 68]</cell></row><row><cell>33.7</cell><cell>34.3</cell><cell cols="2">41.7 33.3 45.2 58.5 63.1</cell><cell>66.0</cell><cell>66.9 67.6 59.1</cell><cell>61.4</cell><cell>69.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc>VOC-2007 test detection average precision (%) for RCNN using VGG and our approach. aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP RCNN+VGG 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1 66.0 RCNN+G-Net 73.7 72.4 65.4 47.2 44.6 71.2 77.4 74.2 42.6 71.1 57.5 72.2 72.7 74.9 62.5 37.8 67.9 66.4 65.3 70.9 64.4 Ours+G-Net 77.1 76.8 75.6 54.5 51.9 76.1 79.5 77.7 48.0 78.2 61.1 82.1 78.1 76.1 65.9 35.4 75.3 67.2 71.7 71.9 69.0</figDesc><table><row><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Study of bounding box (bbox) rejection and baseline deep model on ILSVRC2014 val2. Pretrained without bounding box labels. Def-pooling, context and bounding box regression are not used.</figDesc><table><row><cell>bbox rejection?</cell><cell>n</cell><cell>y</cell><cell>y</cell><cell>y</cell><cell>y</cell></row><row><cell cols="6">deep model A-net A-net Z-net O-net G-net</cell></row><row><cell>mAP (%)</cell><cell cols="5">29.9 30.9 31.8 36.6 37.8</cell></row><row><cell cols="6">meadian AP (%) 28.9 29.4 30.5 36.7 37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>Study of bounding box (bbox) rejection at the training and testing stage without context or def-pooling. Pretrained without bounding box labels. Def-pooling, context and bounding box regression are not used.</figDesc><table><row><cell cols="2">bbox rejection train? n</cell><cell>y</cell><cell>y</cell><cell>y</cell><cell>y</cell></row><row><cell cols="2">bbox rejection test? n</cell><cell>y</cell><cell>n</cell><cell>y</cell><cell>n</cell></row><row><cell>deep model</cell><cell cols="5">A-net A-net A-net Z-net Z-net</cell></row><row><cell>mAP (%)</cell><cell cols="5">29.9 30.9 30.8 31.8 31.5</cell></row><row><cell cols="6">meadian AP (%) 28.9 29.4 29.3 30.5 30.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7</head><label>7</label><figDesc>Study of number of classes used for pretraining. AlexNet is used. Pretrained without bounding box labels. Def-pooling, context and bounding box regression are not used.</figDesc><table><row><cell cols="2">number of classes 200 494 1000 3000</cell></row><row><cell>mAP (%)</cell><cell>22.6 25.6 29.9 32.3</cell></row><row><cell cols="2">meadian AP (%) 19.8 23.0 28.9 31.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8</head><label>8</label><figDesc>Investigation on def-pooling for different baseline net structures on ILSVRC2014 val2. Use pretraining scheme 1 but no bounding box regression or context.</figDesc><table><row><cell cols="6">net structure Z-net D-Def(Z) O-net D-Def(O) G-net D-Def(G)</cell></row><row><cell>mAP (%) 36.0</cell><cell>38.5</cell><cell>39.1</cell><cell>41.4</cell><cell>40.4</cell><cell>42.7</cell></row><row><cell>meadian (%) 34.9</cell><cell>37.4</cell><cell>37.9</cell><cell>41.9</cell><cell>39.3</cell><cell>42.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>The factors for the 5 object classes with lowest AP (top) and highest AP (bottom). The y axis denotes the group index g for the factors in Fig.11-14, e.g. deformation (g = {0, 1}), real-world size (g = {1, . . . , 5}), texture (g = {1, . . . , 4}), box size (g = {1, . . . , 5}). Larger g denotes higher deformation, real size, texture etc. The x axis corresponds to different object classes, e.g. ski, ladle, with different factors, e.g. deformation, real size, texture. Legends denote different object classes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>rub eraser</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Factor group</cell><cell>2 3 4</cell><cell>ski horizontal bar backpack ladle</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">Deformable Real size</cell><cell>Texture</cell><cell>Box size</cell><cell>Obj Num A. ratio var.Part ext. var. Rot var.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>5</cell><cell>fox</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Factor group</cell><cell>2 3 4</cell><cell>armadillo dog butterfly banjo</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell cols="2">Deformable Real size</cell><cell>Texture</cell><cell>Box size</cell><cell>Obj Num A. ratio var.Part ext. var. Rot var.</cell></row><row><cell cols="5">0 5 Figure 15. Deformation Real size squirrel +21% 1 2 3 cattle +17% 4 banjo +16% Factor Group skunk +14%</cell><cell cols="2">Texture</cell><cell>Box size</cell><cell>Obj Num</cell><cell>A. R. var. Part ext. var. Rot var.</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">vacuum -4%</cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell cols="2">waffle iron -6%</cell><cell></cell></row><row><cell>Factor Group</cell><cell>1 2 3 4</cell><cell></cell><cell cols="2">cocktail shaker -6% digital clock -6%</cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="3">Deformation Real size</cell><cell cols="2">Texture</cell><cell>Box size</cell><cell>Obj Num</cell><cell>A. R. var. Part ext. var. Rot var.</cell></row></table><note><p>1 , b 2 ) is the predefined anchor location of the pth part. They are adjusted by a 3 /2a 1 and a 4 /2a 2 , which are</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9</head><label>9</label><figDesc>Ablation study of the two pretraining schemes in Section 3.3 for different net structures on ILSVRC2014 val 2. Scheme 0 only uses image-level annotation for pretraining. Scheme 1 uses object-level annotation for pretraining. Def-pooling bounding box regression and context are not used.net structure A-net A-net A-net Z-net Z-net Z-net Z-net O-net O-net G-net G-net.3 34.9 31.8 29.9 35.6 36.0 36.6 39.1 37.8 40.4 meadian AP (%) 28.9 33.4 34.4 30.5 29.7 34.0 34.9 36.7 37.9 37.0 39.3</figDesc><table><row><cell cols="12">class number 1000 1000 1000 1000 200 1000 1000 1000 1000 1000 1000</cell></row><row><cell>bbox rejection</cell><cell>n</cell><cell>n</cell><cell>y</cell><cell>y</cell><cell>n</cell><cell>n</cell><cell>y</cell><cell>y</cell><cell>y</cell><cell>y</cell><cell>y</cell></row><row><cell cols="2">pretrain scheme 0</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1</cell></row><row><cell>mAP (%)</cell><cell cols="2">29.9 34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10</head><label>10</label><figDesc>Ablation study of the overall pipeline for single model on ILSVRC2014 val2. It shows the mean AP after adding each key component step-by-step.</figDesc><table><row><cell>detection pipeline</cell><cell cols="4">RCNN +bbox A-net Z-net</cell><cell cols="7">O-net image to bbox +edgbox +Def +multi-scale +context +bbox</cell></row><row><cell></cell><cell></cell><cell cols="4">rejection to Z-net to O-net to G-net</cell><cell>pretrain</cell><cell cols="3">candidate pooling pretrain</cell><cell></cell><cell>regression</cell></row><row><cell>mAP (%)</cell><cell>29.9</cell><cell>30.9</cell><cell>31.8</cell><cell>36.6</cell><cell>37.8</cell><cell>40.4</cell><cell>42.7</cell><cell>44.9</cell><cell>47.3</cell><cell>47.8</cell><cell>48.2</cell></row><row><cell>meadian AP (%)</cell><cell>28.9</cell><cell>29.4</cell><cell>30.5</cell><cell>36.7</cell><cell>37.0</cell><cell>39.3</cell><cell>42.3</cell><cell>45.2</cell><cell>47.8</cell><cell>48.1</cell><cell>49.8</cell></row><row><cell>mAP improvement (%)</cell><cell></cell><cell>+1</cell><cell>+0.9</cell><cell>+4.8</cell><cell>+1.2</cell><cell>+2.6</cell><cell>+2.3</cell><cell>+2.2</cell><cell>+2.4</cell><cell>+0.5</cell><cell>+0.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>www.ee.cuhk.edu.hk/ ∼ wlouyang/projects/ImageNet</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment: This work is supported by the General Research Fund sponsored by the Research Grants Council of Hong Kong (Project Nos. CUHK14206114, CUHK14205615 and CUHK14207814, CUHK417011,CUHK14203015).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">DeepDraw on github</title>
		<author>
			<persName><surname>Deepdraw</surname></persName>
		</author>
		<ptr target=".com/auduno/deepdraw.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Poselets: body part detectors trained using 3D human pose annotations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Contextual boost for pedestrian detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part based models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Grishick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010">2010. 1, 2, 4, 5, 7, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-class object localization by combining local contextual interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<date type="published" when="2008">2015. 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2014. 1, 2, 3, 4, 7, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Discriminatively trained deformable part models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<ptr target="http://www.cs.berkeley.edu/rbg/latent-v5/.7,8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.5403</idno>
		<imprint>
			<date type="published" when="2008">2014. 5, 6, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.1840</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detecting objects using deformation dictionaries</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008">2014. 1, 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<idno>ECCV. 2008. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008">2012. 2, 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06981</idno>
		<title level="m">R-cnn minus r</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<title level="m">Network in network</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Switchable deep network for pedestrian detection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical face parsing via deep learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A deep sum-product architecture for robust facial attributes analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pedestrian parsing via deep decompositional neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-source deep learning for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deep representation with large-scale attributes</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A discriminative deep model for pedestrian detection with occlusion handling</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multipedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepid-net: Deformable deep convolutional neural networks for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Factors in finetuning deep model for object detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Modeling mutual visibility relationship in pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Single-pedestrian detection aided by 2-pedestrian detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>IEEE Trans. PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning mutual visibility relationship for pedestrian detection with a deep model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Partial occlusion handling in pedestrian detection with a deep model</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiresolution models for object detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.6382</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02640</idno>
		<imprint>
			<date type="published" when="2008">2015. 3, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2008">2015. 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Histograms of sparse codes for object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008">2011. 3, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contextualizing object detection and classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep convolutional network cascade for facial point detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hybrid deep learning for computing face similarities</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deep learning face representation from predicting 10,000 classes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2008">2014. 1, 2, 4, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning people detectors for tracking in crowded scenes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fisher and vlad with flair</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Regionlets for generic object detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Detection and tracking of multiple, partially occluded humans by bayesian combination of edgelet based part detectors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="247" to="266" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The fastest deformable part model for object detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Multi-pedestrian detection in crowded scenes: A global view</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deeper vision and deep insight solutions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop on ILSVRC</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Object detection by labeling superpixels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2015. 3, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Convolutional channel features. In ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recognizing proxemics in personal photos</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<imprint>
			<date type="published" when="2008">2013. 2, 3, 4, 7, 8</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Deep learning of scenespecific classifier for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-stage contextual deep learning for pedestrian detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Improving object detection with deep convolutional networks via bayesian optimization and structured prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Saliency detection by multicontext deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856,abs/1412.6856</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Latent hierarchical structural learning for object detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Generic object detection with dense neural patterns and regionlets</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
