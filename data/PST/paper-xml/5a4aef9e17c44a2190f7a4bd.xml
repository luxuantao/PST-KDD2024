<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yilun</forename><surname>Chen</surname></persName>
							<email>chenyilun@megvii.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
							<email>wangzhicheng@megvii.com</email>
						</author>
						<author>
							<persName><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
							<email>zhangzhiqiang@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="institution">HuaZhong University of Science and Technology Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><forename type="middle">Jian</forename><surname>Gang</surname></persName>
							<email>yugang@megvii.com</email>
						</author>
						<author>
							<persName><surname>Sun</surname></persName>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Megvii Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cascaded Pyramid Network for Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The topic of multi-person pose estimation has been largely improved recently, especially with the development of convolutional neural network. However, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and complex background, which cannot be well addressed. In this paper, we present a novel network structure called Cascaded Pyramid Network (CPN) which targets to relieve the problem from these "hard" keypoints. More specifically, our algorithm includes two stages: Glob-alNet and RefineNet. GlobalNet is a feature pyramid network which can successfully localize the "simple" keypoints like eyes and hands but may fail to precisely recognize the occluded or invisible keypoints. Our RefineNet tries explicitly handling the "hard" keypoints by integrating all levels of feature representations from the Global-Net together with an online hard keypoint mining loss. In general, to address the multi-person pose estimation problem, a top-down pipeline is adopted to first generate a set of human bounding boxes based on a detector, followed by our CPN for keypoint localization in each human bounding box. Based on the proposed algorithm, we achieve state-ofart results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, which is a 19% relative improvement compared with 60.5 from the COCO 2016 keypoint challenge. Code 1 and the detection results for person used will be publicly available for further research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-person pose estimation is to recognize and locate the keypoints for all persons in the image, which is a fundamental research topic for many visual applications like human action recognition and human-computer interaction.</p><p>Recently, the problem of multi-person pose estimation has been greatly improved by the involvement of deep convolutional neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref>. For example, in <ref type="bibr" target="#b4">[5]</ref>, convolutional pose machine is utilized to locate the keypoint joints in the image and part affinity fields (PAFs) is proposed to assemble the joints to different person. Mask-RCNN <ref type="bibr" target="#b14">[15]</ref> predicts human bounding boxes first and then warps the feature maps based on the human bounding boxes to obtain human keypoints. Although great progress has been made, there still exist a lot of challenging cases, such as occluded keypoints, invisible keypoints and crowded background, which cannot be well localized. The main reasons lie at two points: 1) these "hard" joints cannot be simply recognized based on their appearance features only, for example, the torso point; 2) these "hard" joints are not explicitly addressed during the training process. To address these "hard" joints, in this paper, we present a novel network structure called Cascaded Pyramid Network (CPN). There are two stages in our network architecture: GlobalNet and RefineNet. Our GlobalNet learns a good feature representation based on feature pyramid network <ref type="bibr" target="#b23">[24]</ref>. More importantly, the pyramid feature representation can provide sufficient context information, which is inevitable for the inference of the occluded and invisible joints. Based on the pyramid features, our RefineNet explicitly address the "hard" joints based on an online hard keypoints mining loss.</p><p>Based on our Cascaded Pyramid Network, we address the multi-person pose estimation problem based on a topdown pipeline. Human detector is first adopted to generate a set of human bounding boxes, followed by our CPN for keypoint localization in each human bounding box. In addition, we also explore the effects of various factors which would contribute to the performance of multi-person pose estimation, including person detector and data preprocessing. These details are valuable for the further improvement of accuracy and robustness of our algorithm.</p><p>In summary, our contributions are three-fold as follows:</p><p>• We propose a novel and effective network called cascaded pyramid network (CPN), which integrates global pyramid network (GlobalNet) and pyramid refined network based on online hard keypoints min-ing (RefineNet)</p><p>• We explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline.</p><p>• Our algorithm achieves state-of-art results in the challenging COCO multi-person keypoint benchmark, that is, 73.0 AP in test-dev dataset and 72.1 AP in test challenge dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Human pose estimation is an active research topic since for decades. Classical approaches tackling the problem of human pose estimation mainly adopt the techniques of pictorial structures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref> or graphical models <ref type="bibr" target="#b6">[7]</ref>. More specifically, the classical works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b19">20]</ref> formulate the problem of human keypoints estimation as a tree-structured or graphical model problem and predict keypoint locations based on hand-crafted features. Recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref> mostly rely on the development of convolutional neural network(CNN) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b15">16]</ref>, which largely improve the performance of pose estimation. In this paper, we mainly focus on the methods based on the convolutional neural network. The topic is categorized as single-person pose estimation, which predicts the human keypoints based on the cropped bounding box, and multiperson pose estimation, which needs recognize the full body poses of all persons in images.</p><p>Multi-Person Pose Estimation. Multi-person pose estimation is gaining increasing popularity recently because of the high demand for the real-life applications. However, multi-person pose estimation is challenging owing to occlusion, various gestures of individual persons and unpredictable interactions between different persons. The approach of multi-person pose estimation is mainly divided into two categories: bottom-up approaches and top-down approaches.</p><p>Bottom-Up Approaches. Bottom-up approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b18">19]</ref> directly predict all keypoints at first and assemble them into full poses of all persons. DeepCut <ref type="bibr" target="#b29">[30]</ref> interprets the problem of distinguishing different persons in an image as an Integer Linear Program (ILP) problem and partition part detection candidates into person clusters. Then the final pose estimation results are obtained when person clusters are combined with labeled body parts. DeeperCut <ref type="bibr" target="#b18">[19]</ref> improves DeepCut <ref type="bibr" target="#b29">[30]</ref> using deeper ResNet <ref type="bibr" target="#b15">[16]</ref> and employs image-conditioned pairwise terms to get better performance. Zhe Cao et al. <ref type="bibr" target="#b4">[5]</ref> map the relationship between keypoints into part affinity fields (PAFs) and assemble detected keypoints into different poses of people. Newell et al. <ref type="bibr" target="#b25">[26]</ref> simultaneously produce score maps and pixel-wise embedding to group the candidate keypoints to different people to get final multi-person pose estimation.</p><p>Top-Down Approaches. Top-down approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b8">9]</ref> interpret the process of detecting keypoints as a twostage pipeline, that is, firstly locate and crop all persons from image, and then solve the single person pose estimation problem in the cropped person patches. Papandreou et al. <ref type="bibr" target="#b27">[28]</ref> predict both heatmaps and offsets of the points on the heatmaps to the ground truth location, and then uses the heatmaps with offsets to obtain the final predicted location of keypoints. Mask-RCNN <ref type="bibr" target="#b14">[15]</ref> predicts human bounding boxes first and then crops out the feature map of the corresponding human bounding box to predict human keypoints.</p><p>If top-down approach is utilized for multi-person pose estimation, a human detector as well as single person pose estimator is important in order to obtain a good performance. Here we review some works about single person pose estimation and recent state-of-art detection methods.</p><p>Single Person Pose Estimation. Toshev et al. firstly introduce CNN to solve pose estimation problem in the work of DeepPose <ref type="bibr" target="#b37">[38]</ref>, which proposes a cascade of CNN pose regressors to deal with pose estimation. Tompson et al. <ref type="bibr" target="#b36">[37]</ref> attempt to solve the problem by predicting heatmaps of keypoints using CNN and graphical models. Later works such as Wei et al. <ref type="bibr" target="#b39">[40]</ref>and Newell et al. <ref type="bibr" target="#b26">[27]</ref> show great performance via generating the score map of keypoints using very deep convolutional neural networks. Wei et al. <ref type="bibr" target="#b39">[40]</ref> propose a multi-stage architecture, i.e., first generate coarse results, and continuously refine the result in the following stages. Newell et al. <ref type="bibr" target="#b26">[27]</ref> propose an U-shape network, i.e., hourglass module, and stack up several hourglass modules to generate prediction. Carreira et al. <ref type="bibr" target="#b5">[6]</ref> uses iterative error feedback to get pose estimation and refine the prediction gradually. Lifshitz et al. <ref type="bibr" target="#b22">[23]</ref> uses deep consensus voting to vote the most probable location of keypoints. Gkioxary et al. <ref type="bibr" target="#b13">[14]</ref> and Zisserman et al. <ref type="bibr" target="#b1">[2]</ref> apply RNN-like architectures to sequentially refine the results. Our work is partly inspired by the works on generating and refining score maps. Yang et al. <ref type="bibr" target="#b42">[43]</ref> adopts pyramid features as inputs of the network in the process of pose estimation, which is good exploration of the utilization of pyramid features in pose estimation. However, more should be done with pyramid structure in pose estimation.</p><p>Human Detection. Detection approaches are mainly guided by the RCNN family <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31]</ref>, the up-to-date detector of which are <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>. These detection approaches are composed of two-stage in general. First generate boxes proposals based on default anchors, and then crop from the feature map and further refine the proposals to get the final boxes via R-CNN network. The detector used in our methods are mostly based on <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach for Multi-perosn Keypoints Estimation</head><p>Similar to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>, our algorithm adopts the top-down pipeline: a human detector is first applied on the image to generate a set of human bounding-boxes and detailed localization of the keypoints for each person can be predicted by a single-person skeleton estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Human Detector</head><p>We adopt the state-of-art object detector algorithms based on FPN <ref type="bibr" target="#b23">[24]</ref>. ROIAlign from Mask RCNN <ref type="bibr" target="#b14">[15]</ref> is adopted to replace the ROIPooling in FPN. To train the object detector, all eighty categories from the COCO dataset are utilized during the training process but only the boxes of human category is used for our multi-person skeleton task. For fair comparison with our algorithms, we will release the detector results on the COCO val and COCO test dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cascaded Pyramid Network (CPN)</head><p>Before starting the discussion of our CPN, we first briefly review the design structure for the single person skeleton estimator based on each human bounding box. Stacked hourglass <ref type="bibr" target="#b26">[27]</ref>, which is a prevalent method for pose estimation, stacks eight hourglasses which are downsampled and up-sampled modules to enhance the pose estimation performance. The stacking strategy works to some extent, however, we find that stacking two hourglasses is sufficient to have a comparable performance compared with the eight-stage stacked hourglass module. <ref type="bibr" target="#b27">[28]</ref> utilizes a ResNet <ref type="bibr" target="#b15">[16]</ref> network to estimate pose in the wild achieving promising performance in the COCO 2016 keypoint challenge. Motivated by the works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> described above, we propose an effective and efficient network called cascaded pyramid network (CPN) to address the problem of pose estimation. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, our CPN involves two subnetworks: GlobalNet and RefineNet. Thus, usually a U-shape structure is integrated to maintain both the spatial resolution and semantic information for the feature layers. More recently, FPN <ref type="bibr" target="#b23">[24]</ref> further improves the U-shape structure with deeply supervised information. We apply the similar feature pyramid structure for our keypoints estimation. Slightly different from FPN, we apply 1 × 1 convolutional kernel before each element-wise sum procedure in the upsampling process. We call this structure as GlobalNet and an illustrative example can be found in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>, our GlobalNet based on ResNet backbone can effectively locate the keypoints like eyes but may fail to precisely locate the position of hips. The localization of keypoints like hip usually requires more context rather than the appearance feature nearby. Thus, it is usually difficult to directly recognize these "hard" keypoints based on GlobalNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">RefineNet</head><p>Based on the feature pyramid representation generated by GlobalNet, we attach a RefineNet to explicitly address the "hard" keypoints. In order to improve the efficiency and keep integrity of information transmission, our RefineNet transmits the information across different levels and finally integrates the informations of different levels via upsam- pling and concatenating as HyperNet <ref type="bibr" target="#b20">[21]</ref>. Different from the refinement strategy like stacked hourglass <ref type="bibr" target="#b26">[27]</ref>, our Re-fineNet concatenates all the pyramid features rather than simply using the upsampled features at the end of hourglass module. In addition, we stack more bottleneck blocks into deeper layers, whose smaller spatial size achieves a good trade-off between effectiveness and efficiency.</p><p>As the network continues training, the network tends to pay more attention to the "simple" keypoints of the majority but less importance to the occluded and hard keypoints. Thus, in our RefineNet, we explictily select the hard keypoints online based on the training loss and backpropagate the losses from the selected keypoints only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>Our overall pipeline follows the top-down approach for estimating multiple human poses. Firstly, we apply a stateof-art bounding detector to generate human proposals. For each proposal, we assume that there is only one main person in the cropped region of proposal and then applied the pose estimating network to generate the final prediction. In this section, we will discuss more details of our methods based on experiment results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Dataset and Evaluation Metric. Our models are only trained on MS COCO <ref type="bibr" target="#b24">[25]</ref> trainval dataset (includes 57K images and 150K person instances) and validated on MS COCO minival dataset (includes 5000 images). The testing sets includes test-dev set (20K images) and test-challenge set (20K images). Most experiments are evaluated in OKSbased mAP, where OKS (object keypoints similarity) defines the similarity between different human poses.</p><p>Cropping Strategy. For each human detection box, the box is extended to a fixed aspect ratio, e.g., height : width = 256 : 192, and then we crop from images without distorting the images aspect ratio. Training Details. All models of pose estimation are trained using SGD algorithm with an initial learning rate of 5e-4. The learning rate is decreased by a factor of 2 every 10 epochs. We use a weight decay of 1e-5 and batch normalization is used in our network. Generally, the training of ResNet-50-based models takes about 1.5 day on eight NVIDIA Titan X Pascal GPUs. Our models are all initialized with weights of the public-released ImageNet <ref type="bibr" target="#b31">[32]</ref>pretrained model.</p><p>Testing Details. In order to minimize the variance of prediction, we apply a 2D gaussian filter on the predicted heatmaps. Following the same techniques used in <ref type="bibr" target="#b26">[27]</ref>, we also predict the pose of the corresponding flipped image and average the heatmaps to get the final prediction; a quarter offset in the direction from the highest response to the second highest response is used to obtain the final location of the keypoints. Rescoring strategy is also used in our experiments. Different from the rescoring strategy used in <ref type="bibr" target="#b27">[28]</ref>, the product of boxes' score and the average score of all keypoints is considered as the final pose score of a person instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiment</head><p>In this subsection, we validate the effectiveness of our network from various aspects. Unless otherwise specified, all experiments are evaluated on MS COCO minival dataset in this subsection. The input size of all models is 256 × 192 and the same data augmentation is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Person Detector</head><p>Since detection boxes are critical for top-down approaches in multi-person pose estimation, here we discuss two factors of detection, i.e. different NMS strategies and the AP of bounding boxes. Our human boxes are generated based on the state-of-art detector FPN trained with only the labeled COCO data, no extra data and no specific training on person. For fair comparison, we use the same detector with a general AP of 41.1 and person AP of 55.3 on the COCO minival dataset in the ablation experiments by default unless otherwise specified.</p><p>Non-Maximum Suppression (NMS) strategies. As shown in the Table <ref type="table" target="#tab_0">1</ref>, we compare the performance of different NMS strategies or the same NMS strategy under different thresholds. Referring to the original hard NMS, the performance of keypoints detection improves when the threshold increases, basically owing to the improvement of the average precision (AP) and average recall (AR) of the boxes. Since the final score of the pose estimated partially depends on the score of the bounding box, Soft-NMS <ref type="bibr" target="#b2">[3]</ref> which is supposed to generate more proper scores is better in performance as it is shown in the Table <ref type="table" target="#tab_0">1</ref> . From the table, we can see that Soft-NMS <ref type="bibr" target="#b2">[3]</ref> surpasses the hard NMS method on the performance of both detection and keypoints detection. Detection Performance. Table <ref type="table">2</ref> shows the relationship between detection AP and the corresponding keypoints AP, aiming to reveal the influence of the accuracy of the bounding box detection on the keyoints detection. From the table, we can see that the keypoints detection AP gains less and less as the accuracy of the detection boxes increases. Specially, when the detection AP increases from 44.3 to 49.3 and the human detection AP increases 3.0 points, the keypoints detection accuracy does not improve a bit and the AR of the detection increases marginally. Therefore, we have enough reasons to deem that the given boxes cover most of the medium and large person instances with such a high detection AP. Therefore, the more important problem for pose estimation is to enhance the accuracy of hard keypoints other than involve more boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Cascaded Pyramid Network</head><p>8-stage hourglass network <ref type="bibr" target="#b26">[27]</ref> and ResNet-50 with dilation <ref type="bibr" target="#b27">[28]</ref> are adopted as our baseline. From Table <ref type="table" target="#tab_1">3</ref>, although the results improve considerably if dilation are used in shallow layers, it is worth noting that the FLOPs (floating-point operations) increases significantly.</p><p>From the statistics of FLOPs in testing stage and the accuracy of keypoints as shown in achieves much better speed-accuracy trade-off than Hourglass network and ResNet-50 with dilation. Note that Glob-alNet achieves much better results than one-stage hourglass network of same FLOPs probably for much larger parameter space. After refined by the RefineNet, it increases 2.0 AP and yields the results of 68.6 AP. Furthermore, when online hard keypoints mining is applied in RefineNet, our network finally achieves 69.4 AP. Design Choices of RefineNet. Here, we compare different design strategies of RefineNet as shown in Table <ref type="table" target="#tab_3">4</ref>. We compare the following implementation based on pyramid output from the GlobalNet: 1) Concatenate (Concat) operation is directly attached like HyperNet <ref type="bibr" target="#b20">[21]</ref>,</p><p>2) a bottleneck block is attached first in each layer (C 2 ∼ C 5 ) and then followed by a concatenate operation,</p><p>3) different number of bottleneck blocks applied to different layers followed by a concatenate operation as shown in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>A convolution layer is attached finally to generate the score maps for each keypoint. We find that our RefineNet structure can effectively achieve more than 2 points gain compared with GlobalNet only and for refinement of keypoints and also outperforms other design implementations followed by GlobalNet. Here, we also validate the performance for utilizing the pyramid output from different levels. In our RefineNet, we utilize four output feature maps C 2 ∼ C 5 , where C i refers to the ith feature map of GlobalNet output. Also, feature map from C 2 only, feature maps from C 2 ∼ C 3 , and feature maps from C 2 ∼ C 4 are evaluated as shown in Table <ref type="table" target="#tab_4">5</ref>. We can find that the performance improves as more levels of features are utilized. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Online Hard Keypoints Mining</head><p>Here we discuss the losses used in our network. In detail, the loss function of GlobalNet is L2 loss of all annotated keypoints while the second stage tries learning the hard keypoints, that is, we only punish the top M (M &lt; N ) keypoint losses out of N (the number of annotated keypoints in one person, say 17 in COCO dataset). The effect of M is shown in Table <ref type="table" target="#tab_5">6</ref>. For M = 8, the performance of second stage achieves the best result for the balanced training between hard keypoints and simple keypoints.  Inspired by OHEM <ref type="bibr" target="#b34">[35]</ref>, however the method of online hard keypoints mining loss is quite different from it. our method focuses on higher level information than OHEM which concentrates on examples, for instance, pixel level losses in the heatmap L2 loss. As a result, our method is more stable, and outperforms OHEM strategy in accuracy.</p><p>As Table <ref type="table" target="#tab_6">7</ref> shows, when online hard keypoints mining is applied in RefineNet, the performance of overall network increases 0.8 AP and finally achieves 69.4 AP comparing to normal l2 loss. For reference, experiments without intermediate supervision in CPN leads to a performance drop of 0.9 AP probably for the lack of prior knowledge and sufficient context information of keypoints provided by GlobalNet. In addition, applying the same online hard keypoints mining in GlobalNet which decreases the results by 0.3 AP. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Data Pre-processing</head><p>The size of cropped image are important factors to the performance of keypoints detection. As </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on MS COCO Keypoints Challenge</head><p>We evaluate our method on MS COCO test-dev and testchallenge dataset. Without extra data involved in training, we achieve 72.1 AP using a single model of CPN and 73.0 using ensembled models of CPN with different ground truth heatmaps. Table <ref type="table" target="#tab_9">9</ref> shows the comparison of the results of our method and the other methods on the test-challenge2017 split of COCO dataset. We get 72.1 AP achieving state-of-art performance on COCO test-challenge2017 dataset. Table <ref type="table" target="#tab_11">11</ref> shows the performances of CPN and CPN+ (ensembled model) on COCO minival dataset, which offer a reference to the gap between the COCO minival dataset and the standard testdev or test-challenge dataset of the COCO dataset. Figure <ref type="figure" target="#fig_5">3</ref> illustrates some results generated using our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we follow the top-down pipeline and a novel Cascaded Pyramid Network (CPN) is presented to address the "hard" keypoints. More specifically, our CPN includes a GlobalNet based on the feature pyramid structure and a RefineNet which concatenates all the pyramid features as a context information. In addition, online hard keypoint mining is integrated in RefineNet to explicitly address the "hard" keypoints. Our algorithm achieves stateof-art results on the COCO keypoint benchmark, with average precision at 73.0 on the COCO test-dev dataset and 72.1 on the COCO test-challenge dataset, outperforms the COCO 2016 keypoint challenge winner by a 19% relative improvement. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Cascaded Pyramid Network. "L2 loss*" means L2 loss with online hard keypoints mining.</figDesc><graphic url="image-6.png" coords="3,85.01,117.88,54.45,72.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3.2.1 GlobalNetHere, we describe our network structure based on the ResNet backbone. We denote the last residual blocks of different conv features conv2∼5 as C 2 , C 3 , ..., C 5 respectively. 3 × 3 convolution filters are applied on C 2 , ..., C 5 to generate the heatmaps for keypoints. As shown in Figure 2, the shallow features like C 2 and C 3 have the high spatial resolution for localization but low semantic information for recognition. On the other hand, deep feature layers like C 4 and C 5 have more semantic information but low spatial resolution due to strided convolution (and pooling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Output heatmaps from different features. The green dots means the groundtruth location of keypoints.</figDesc><graphic url="image-9.png" coords="4,85.01,87.44,411.01,82.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Finally, we resize the cropped image to a fixed size of height 256 pixels and 192 pixels by default. Note that only the boxes of the person class in the top 100 boxes of all classes are used in all the experiments of 4.2. Data Augmentation Strategy. Data augmentation is critical for the learning of scale invariance and rotation invariance. After cropping from images, we apply random flip, random rotation (−40 • ∼ +40 • ) and random scale (0.7 ∼ 1.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Some results of our method.</figDesc><graphic url="image-31.png" coords="8,82.18,592.00,151.67,108.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between different NMS methods and keypoints detection performance with the same model. H is short for human.</figDesc><table><row><cell>NMS</cell><cell cols="4">AP(all) AP(H) AR(H) AP(OKS)</cell></row><row><cell>NMS(thr=0.3)</cell><cell>40.1</cell><cell>53.5</cell><cell>60.3</cell><cell>68.2</cell></row><row><cell>NMS(thr=0.4)</cell><cell>40.5</cell><cell>54.4</cell><cell>61.7</cell><cell>68.9</cell></row><row><cell>NMS(thr=0.5)</cell><cell>40.8</cell><cell>54.9</cell><cell>62.9</cell><cell>69.2</cell></row><row><cell>NMS(thr=0.6)</cell><cell>40.8</cell><cell>55.2</cell><cell>64.3</cell><cell>69.2</cell></row><row><cell>Soft-NMS [3]</cell><cell>41.1</cell><cell>55.3</cell><cell>67.0</cell><cell>69.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 ,</head><label>3</label><figDesc>we find that CPN</figDesc><table><row><cell cols="5">Det Methods AP(all) AP(H) AR(H) AP(OKS)</cell></row><row><cell>FPN-1</cell><cell>36.3</cell><cell>49.6</cell><cell>58.5</cell><cell>68.8</cell></row><row><cell>FPN-2</cell><cell>41.1</cell><cell>55.3</cell><cell>67.0</cell><cell>69.4</cell></row><row><cell>FPN-3</cell><cell>44.3</cell><cell>58.4</cell><cell>71.3</cell><cell>69.7</cell></row><row><cell>ensemble-1</cell><cell>49.3</cell><cell>61.4</cell><cell>71.8</cell><cell>69.8</cell></row><row><cell>ensemble-2</cell><cell>52.1</cell><cell>62.9</cell><cell>74.7</cell><cell>69.8</cell></row><row><cell cols="5">Table 2. Comparison between detection performance and key-</cell></row><row><cell cols="5">points detection performance. FPN-1: FPN with the backbone</cell></row><row><cell cols="5">of res50; FPN-2: res101 with Soft-NMS and OHEM [35] applied;</cell></row><row><cell cols="5">FPN-3: resNeXt [41]101 with Soft-NMS, OHEM [35], multiscale</cell></row><row><cell cols="5">training applied; ensemble-1: multiscale test involved; ensemble-</cell></row><row><cell cols="5">2: multiscale test, large batch and SENet [17] involved. H is short</cell></row><row><cell>for Human.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="4">AP (OKS) FLOPs Param Size</cell></row><row><cell>1-stage hourglass</cell><cell></cell><cell>54.5</cell><cell>3.92G</cell><cell>12MB</cell></row><row><cell>2-stage hourglass</cell><cell></cell><cell>66.5</cell><cell>6.14G</cell><cell>23MB</cell></row><row><cell>8-stage hourglass</cell><cell></cell><cell>66.9</cell><cell>19.48G</cell><cell>89MB</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>41.3</cell><cell>3.54G</cell><cell>92MB</cell></row><row><cell>ResNet-50 + dilation(res5)</cell><cell></cell><cell>44.1</cell><cell>5.62G</cell><cell>92MB</cell></row><row><cell>ResNet-50 + dilation(res4-5)</cell><cell></cell><cell>66.5</cell><cell>17.71G</cell><cell>92MB</cell></row><row><cell>ResNet-50 + dilation(res3-5)</cell><cell></cell><cell>-</cell><cell>68.70G</cell><cell>92MB</cell></row><row><cell>GlobalNet only (ResNet-50)</cell><cell></cell><cell>66.6</cell><cell>3.90G</cell><cell>94MB</cell></row><row><cell cols="2">CPN* (ResNet-50)</cell><cell>68.6</cell><cell>6.20G</cell><cell>102 MB</cell></row><row><cell>CPN (ResNet-50)</cell><cell></cell><cell>69.4</cell><cell>6.20G</cell><cell>102 MB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on COCO minival dataset. CPN* indicates CPN without online hard keypoints mining.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of models of different design choices of Re-fineNet.</figDesc><table><row><cell>Models</cell><cell cols="2">AP(OKS) FLOPs</cell></row><row><cell>GlobalNet only</cell><cell>66.6</cell><cell>3.90G</cell></row><row><cell>GlobalNet + Concat</cell><cell>68.5</cell><cell>5.87G</cell></row><row><cell>GlobalNet + one bottleneck +Concat</cell><cell>69.2</cell><cell>6.92G</cell></row><row><cell>ours (CPN)</cell><cell>69.4</cell><cell>6.20G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Effectiveness of intermediate connections of CPN.</figDesc><table><row><cell cols="3">Connections AP(OKS) FLOPs</cell></row><row><cell>C2</cell><cell>68.3</cell><cell>5.02G</cell></row><row><cell>C2 ∼ C3</cell><cell>68.4</cell><cell>5.50G</cell></row><row><cell>C2 ∼ C4</cell><cell>69.1</cell><cell>5.88G</cell></row><row><cell>C2 ∼ C5</cell><cell>69.4</cell><cell>6.20G</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison of different hard keypoints number in online hard keypoints mining.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Comparison of models with different losses function.Here "-" denotes that the model applies no loss function in corresponding subnetwork. "L2 loss*" means L2 loss with online hard keypoints mining.</figDesc><table><row><cell cols="3">GlobalNet RefineNet AP(OKS)</cell></row><row><cell>-</cell><cell>L2 loss</cell><cell>68.2</cell></row><row><cell>L2 loss</cell><cell>L2 loss</cell><cell>68.6</cell></row><row><cell>-</cell><cell>L2 loss*</cell><cell>68.5</cell></row><row><cell>L2 loss</cell><cell>L2 loss*</cell><cell>69.4</cell></row><row><cell>L2 loss*</cell><cell>L2 loss*</cell><cell>69.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>illustrates, it's</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Comparison of models of different input size. CPN* indicates CPN without online hard keypoints mining.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 .</head><label>9</label><figDesc>Table 10  illustrates the results of our method in the test-dev split dataset of the COCO dataset. Comparisons of final results on COCO test-challenge2017 dataset. "*" means that the method involves extra data for training. Specifically, FAIR Mask R-CNN involves distilling unlabeled data, oks uses AI-Challenger keypoints dataset, bangbangren and G-RMI use their internal data as extra data to enhance performance. "+" indicates results using ensembled models. The human detector of Ours+ is a detector that has an AP of 62.9 of human class on COCO minival dataset. CPN and CPN+ in this table all use the backbone of ResNet-Inception<ref type="bibr" target="#b35">[36]</ref> framework.</figDesc><table><row><cell>Methods</cell><cell>AP</cell><cell cols="4">AP @.5 AP @.75 APm AP l</cell><cell>AR</cell><cell cols="4">AR @.5 AR @.75 ARm AR l</cell></row><row><cell cols="2">FAIR Mask R-CNN* 68.9</cell><cell></cell><cell>89.2</cell><cell>75.2</cell><cell cols="2">63.7 76.8 75.4</cell><cell></cell><cell>93.2</cell><cell>81.2</cell><cell>70.2</cell><cell>82.6</cell></row><row><cell>G-RMI*</cell><cell>69.1</cell><cell></cell><cell>85.9</cell><cell>75.2</cell><cell cols="2">66.0 74.5 75.1</cell><cell></cell><cell>90.7</cell><cell>80.7</cell><cell>69.7</cell><cell>82.4</cell></row><row><cell>bangbangren+*</cell><cell>70.6</cell><cell></cell><cell>88.0</cell><cell>76.5</cell><cell cols="2">65.6 79.2 77.4</cell><cell></cell><cell>93.6</cell><cell>83.0</cell><cell>71.8</cell><cell>85.0</cell></row><row><cell>oks*</cell><cell>71.4</cell><cell></cell><cell>89.4</cell><cell>78.1</cell><cell cols="2">65.9 79.1 77.2</cell><cell></cell><cell>93.6</cell><cell>83.4</cell><cell>71.8</cell><cell>84.5</cell></row><row><cell>Ours+ (CPN+)</cell><cell>72.1</cell><cell></cell><cell>90.5</cell><cell>78.9</cell><cell cols="2">67.9 78.1 78.7</cell><cell></cell><cell>94.7</cell><cell>84.8</cell><cell>74.3</cell><cell>84.7</cell></row><row><cell>Methods</cell><cell>AP</cell><cell></cell><cell cols="3">AP @.5 AP @.75 APm AP l</cell><cell>AR</cell><cell></cell><cell cols="3">AR @.5 AR @.75 ARm AR l</cell></row><row><cell>CMU-Pose [5]</cell><cell cols="2">61.8</cell><cell>84.9</cell><cell>67.5</cell><cell cols="3">57.1 68.2 66.5</cell><cell>87.2</cell><cell>71.8</cell><cell>60.6</cell><cell>74.6</cell></row><row><cell>Mask-RCNN [15]</cell><cell cols="2">63.1</cell><cell>87.3</cell><cell>68.7</cell><cell>57.8 71.4</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">Associative Embedding [26] 65.5</cell><cell>86.8</cell><cell>72.3</cell><cell cols="3">60.6 72.6 70.2</cell><cell>89.5</cell><cell>76.0</cell><cell>64.6</cell><cell>78.1</cell></row><row><cell>G-RMI [28]</cell><cell cols="2">64.9</cell><cell>85.5</cell><cell>71.3</cell><cell cols="3">62.3 70.0 69.7</cell><cell>88.7</cell><cell>75.5</cell><cell>64.4</cell><cell>77.1</cell></row><row><cell>G-RMI* [28]</cell><cell cols="2">68.5</cell><cell>87.1</cell><cell>75.5</cell><cell cols="3">65.8 73.3 73.3</cell><cell>90.1</cell><cell>79.5</cell><cell>68.1</cell><cell>80.4</cell></row><row><cell>Ours (CPN)</cell><cell cols="2">72.1</cell><cell>91.4</cell><cell>80.0</cell><cell cols="3">68.7 77.2 78.5</cell><cell>95.1</cell><cell>85.3</cell><cell>74.2</cell><cell>84.3</cell></row><row><cell>Ours+ (CPN+)</cell><cell cols="2">73.0</cell><cell>91.7</cell><cell>80.9</cell><cell cols="3">69.5 78.1 79.0</cell><cell>95.1</cell><cell>85.9</cell><cell>74.8</cell><cell>84.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Comparisons of final results on COCO test-dev dataset. "*" means that the method involves extra data for training. "+" indicates results using ensembled models. The human detectors of Our and Ours+ the same detector that has an AP of 62.9 of human class on COCO minival dataset.CPN and CPN+ in this table all use the backbone of ResNet-Inception<ref type="bibr" target="#b35">[36]</ref> framework.</figDesc><table><row><cell>Methods</cell><cell cols="3">AP -minival AP -dev AP -challenge</cell></row><row><cell>Ours (CPN)</cell><cell>72.7</cell><cell>72.1</cell><cell>-</cell></row><row><cell>Ours (CPN+)</cell><cell>74.5</cell><cell>73.0</cell><cell>72.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Comparison of results on the minvival dataset and the corresponding results on test-dev or test-challenge of the COCO dataset. "+" indicates ensembled model. CPN and CPN+ in this table all use the backbone of ResNet-Inception [36] framework.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="1014" to="1021" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent human pose estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="468" to="475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving Object Detection With One Line of Code</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">Apr. 2017. 5</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Human Pose Estimation via Convolutional Part Heatmap Regression</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013. 2015</date>
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1736" to="1744" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The representation and matching of pictorial structures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Elschlager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="92" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>C-</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
				<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3342" to="3349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="728" to="743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2007">2017. 1, 2, 3, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">Mask R-CNN. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2001">June 2016. 1</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Squeeze-and-Excitation Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning effective human pose estimation from inaccurate annotation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1465" to="1472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hypernet: Towards accurate region proposal generation and joint object detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="246" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft coco: Common objects in context</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Associative Embedding: End-to-End Learning for Joint Detection and Grouping</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">Nov. 2016. 2, 7</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2005">2016. 2, 3, 4, 5</date>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Towards Accurate Multiperson Pose Estimation in the Wild</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">Jan. 2017. 2, 3, 4, 5, 7</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="422" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modec: Multimodal decomposable models for human pose estimation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3674" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training regionbased object detectors with online hard example mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">Feb. 2016. 7</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<title level="m">Convolutional pose machines</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2005">July 2017. 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
				<imprint>
			<date type="published" when="2017-10">Oct 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
