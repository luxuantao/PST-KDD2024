<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Reduction via Generalized Uncorrelated Linear Discriminant Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
							<email>jieping.ye@asu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<addrLine>699 South Mill Avenue</addrLine>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Ravi</forename><surname>Janardan</surname></persName>
							<email>janardan@cs.umn.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<addrLine>699 South Mill Avenue</addrLine>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Qi</forename><surname>Li</surname></persName>
							<email>qili@cis.udel.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<addrLine>699 South Mill Avenue</addrLine>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haesun</forename><surname>Park</surname></persName>
							<email>hpark@cc.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<addrLine>699 South Mill Avenue</addrLine>
									<postCode>85287</postCode>
									<settlement>Tempe</settlement>
									<region>AZ</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">R</forename><surname>Janardan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">EE/CSci. Bldg</orgName>
								<orgName type="institution">University of Minnesota-Twin Cities</orgName>
								<address>
									<addrLine>4-192, 200 Union Street S.E</addrLine>
									<postCode>55455</postCode>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">Q</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Delaware</orgName>
								<address>
									<addrLine>103 Smith Hall</addrLine>
									<postCode>19716</postCode>
									<settlement>Newark</settlement>
									<region>DE</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="middle">H</forename><surname>Park</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">College of Computing</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<addrLine>801 Atlantic Drive</addrLine>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Reduction via Generalized Uncorrelated Linear Discriminant Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">447CD11101440260D250BE6BA1570654</idno>
					<note type="submission">received 1 Apr. 2005; revised 30 Nov. 2005; accepted 30 May 2006; published online 18 Aug. 2006.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feature reduction</term>
					<term>uncorrelated linear discriminant analysis</term>
					<term>QR-decomposition</term>
					<term>generalized singular value decomposition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High-dimensional data appear in many applications of data mining, machine learning, and bioinformatics. Feature reduction is commonly applied as a preprocessing step to overcome the curse of dimensionality. Uncorrelated Linear Discriminant Analysis (ULDA) was recently proposed for feature reduction. The extracted features via ULDA were shown to be statistically uncorrelated, which is desirable for many applications. In this paper, an algorithm called ULDA/QR is proposed to simplify the previous implementation of ULDA. Then, the ULDA/GSVD algorithm is proposed, based on a novel optimization criterion, to address the singularity problem which occurs in undersampled problems, where the data dimension is larger than the sample size. The criterion used is the regularized version of the one in ULDA/QR. Surprisingly, our theoretical result shows that the solution to ULDA/GSVD is independent of the value of the regularization parameter. Experimental results on various types of data sets are reported to show the effectiveness of the proposed algorithm and to compare it with other commonly used feature reduction algorithms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>F EATURE reduction is important in many applications of data mining, machine learning, and bioinformatics because of the so-called curse of dimensionality <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Many methods have been proposed for feature reduction, such as Principal Component Analysis (PCA) <ref type="bibr" target="#b18">[19]</ref> and Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b9">[10]</ref>. LDA aims to find optimal discriminant features by maximizing the ratio of the between-class distance to the within-class distance of a given data set under supervised learning conditions. It has been successfully employed in many applications including information retrieval <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, face recognition <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, and microarray data analysis <ref type="bibr" target="#b6">[7]</ref>. Its simplest implementation, the so-called classical LDA, applies an eigen-decomposition on the scatter matrices, but fails when the scatter matrices are singular, as is the case for undersampled data. This is known as the singularity or undersampled problem <ref type="bibr" target="#b19">[20]</ref>.</p><p>Uncorrelated features 1 are desirable in many applications because they contain minimum redundancy. Motivated by extracting feature vectors having uncorrelated features, uncorrelated LDA (ULDA) was recently proposed in <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. However, the proposed algorithm in <ref type="bibr" target="#b16">[17]</ref> involves a sequence of generalized eigenvalue problems, which is computationally expensive for large and high-dimensional data sets. Like classical LDA, it does not address the singularity problem either. We thus call it classical ULDA. More details can be found in Section 3.</p><p>Classical LDA and classical ULDA were introduced from different perspectives, but it has been found that there is a close relationship between classical LDA and classical ULDA <ref type="bibr" target="#b17">[18]</ref>. More precisely, under the assumption that the eigenvalue problem in classical LDA has no multiple eigenvalues, it was shown that classical ULDA is equivalent to classical LDA <ref type="bibr" target="#b17">[18]</ref>. In this paper, we will show that the equivalence between these two still holds without the above assumption. Based on this equivalence, ULDA/QR is proposed to simplify the ULDA implementation in <ref type="bibr" target="#b16">[17]</ref>. Here, ULDA/QR denotes ULDA based on QR-decomposition <ref type="bibr" target="#b10">[11]</ref>.</p><p>Classical LDA and classical ULDA do not address the singularity problem, hence it is difficult to apply them to undersampled data. Such high-dimensional, undersampled problems frequently occur in many applications including information retrieval <ref type="bibr" target="#b14">[15]</ref>, face recognition <ref type="bibr" target="#b24">[25]</ref>, and microarray analysis <ref type="bibr" target="#b6">[7]</ref>. Several schemes have been proposed to address the singularity problem in classical LDA in the past, including pseudoinverse-based LDA <ref type="bibr" target="#b28">[29]</ref>, the subspacebased method <ref type="bibr" target="#b24">[25]</ref>, regularization <ref type="bibr" target="#b8">[9]</ref>, and the method based on the Generalized Singular Value Decomposition, called LDA/GSVD <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Pseudoinverse-based LDA applies the pseudoinverse <ref type="bibr" target="#b10">[11]</ref> to deal with the singularity problem. The subspace-based method applies the Karhunen-Loeve (KL) expansion, also known as Principal Component Analysis (PCA) <ref type="bibr" target="#b18">[19]</ref>, before LDA. Its limitation is that some useful information may be lost in the KL expansion. Regularized LDA overcomes the singularity problem by increasing the magnitude of the diagonal elements of the scatter matrices (usually by adding a scaled identity matrix). The difficulty in using regularized LDA for feature reduction is the choice of the amount of perturbation. A small perturbation is desirable to preserve the original matrix structure, while a large perturbation is more effective in dealing with the singularity problem.</p><p>There is much less work on addressing the singularity problem in classical ULDA than on classical LDA. In the subspace ULDA presented in <ref type="bibr" target="#b16">[17]</ref>, a subspace-based method was applied (PCA is applied to the between-class scatter matrix). We address the singularity problem in ULDA, in the second part of this paper, by introducing a novel optimization criterion that combines the key ingredients of ULDA/QR and regularized LDA. The criterion is the perturbed version of the criterion used in ULDA/QR. Based on this criterion and the Generalized Singular Value Decomposition (GSVD) <ref type="bibr" target="#b20">[21]</ref>, we propose a novel feature reduction algorithm, called ULDA/GSVD. ULDA/GSVD solves the singularity problem directly, thus avoiding the information loss that occurs in the subspace method. Since the GSVD computation can be expensive for large and highdimensional data sets, an efficient algorithm for ULDA/ GSVD is also proposed. The difference between ULDA/ GSVD and the traditional regularized LDA is that the optimal discriminant feature vectors via ULDA/GSVD are independent of the value of regularization parameter. This is quite a surprising result and the proof and the details are given in Section 5.</p><p>With the K-Nearest-Neighbor (K-NN) classifier, we evaluate the effectiveness of ULDA/GSVD and compare it with several other commonly used feature reduction algorithms, including Orthogonal Centroid Method (OCM) <ref type="bibr" target="#b21">[22]</ref>, PCA <ref type="bibr" target="#b18">[19]</ref>, and subspace ULDA <ref type="bibr" target="#b16">[17]</ref>, on various types of data sets, including text documents, chemical analysis of wine, face images, and microarray gene expression data. The experimental results show that the ULDA/GSVD algorithm is competitive with the other feature reduction algorithms (i.e., PCA, OCM, and subspace ULDA) and Support Vector Machines (SVM) <ref type="bibr" target="#b26">[27]</ref>. Results also show that ULDA/GSVD is stable under different K-NN classifiers.</p><p>The rest of the paper is organized as follows: Sections 2 and 3 give brief reviews on classical LDA and classical ULDA, respectively. The ULDA/QR algorithm is presented in Section 4. Section 5 proposes the ULDA/GSVD algorithm, based on a novel criterion that is the regularized version of the criterion used in ULDA/QR. We prove theoretically that the solution to ULDA/GSVD is indepen-dent of the value of regularization applied. Experimental results are presented in Section 6. We conclude in Section 7. For convenience, the important notations used in this paper are listed in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CLASSICAL LINEAR DISCRIMINANT ANALYSIS</head><p>Given a data matrix A ¼ ða ij Þ 2 IR NÂn , where each column corresponds to a data point and each row corresponds to a particular feature, we consider finding a linear transformation G 2 IR NÂ' (' &lt; N) that maps each column a i , for 1 i n, of A in the N-dimensional space to a vector y i in the '-dimensional space as follows:</p><formula xml:id="formula_0">G : a i 2 IR N ! y i ¼ G T a i 2 IR ' :</formula><p>The resulting data matrix Z ¼ G T A 2 IR 'Ân contains ' rows, i.e., there are ' features for each data point in the dimension reduced (transformed) space. It is also clear that the features in the dimension reduced space are linear combinations of the features in the original high-dimensional space, where the coefficients of the linear combinations depend on the transformation matrix G. A common way to compute the transformation matrix G, for clustered data sets, is through classical LDA. It computes the optimal transformation matrix G such that the class structure is preserved. More details are given below.</p><p>Assume that there are k classes in the data set. Suppose c i is the mean vector of the ith class and c is the total mean. Then, the between-class scatter matrix S b , the within-class scatter matrix S w , and the total scatter matrix S are defined as follows <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_1">S w ¼ H w H T w , S b ¼ H b H T b , and S ¼ H t H T t ,</formula><p>where</p><formula xml:id="formula_2">H w ¼ 1 ffiffiffi n p A 1 ; Á Á Á ; A k ½ ;<label>ð1Þ</label></formula><formula xml:id="formula_3">H b ¼ 1 ffiffiffi n p ffiffiffiffiffi n 1 p ðc 1 À cÞ; Á Á Á ; ffiffiffiffiffi n k p ðc k À cÞ ½ ;<label>ð2Þ</label></formula><formula xml:id="formula_4">H t ¼ 1 ffiffiffi n p A À ce T À Á ;<label>ð3Þ</label></formula><p>A i is the data matrix of the ith class, n i is the sample size of the ith class, and e 2 IR n is a vector of ones.</p><p>The trace of the two scatter matrices can be computed as follows: </p><formula xml:id="formula_5">traceðS w Þ ¼ 1 n X k i¼1 jjA i jj 2 F ; traceðS b Þ ¼ 1 n X k i¼1 n i jjc i À cjj 2 ;</formula><p>where jj Á jj F denotes the Frobenius norm <ref type="bibr" target="#b10">[11]</ref>. Hence, traceðS w Þ measures the between-class cohesion, and traceðS b Þ measures the between-class separation. It follows from the definition that S t ¼ S w þ S b . In the lower-dimensional space resulting from the linear transformation G, the within-class scatter and between-class scatter matrices become</p><formula xml:id="formula_6">S L w ¼ ðG T H w ÞðG T H w Þ T ¼ G T S w G; S L b ¼ ðG T H b ÞðG T H b Þ T ¼ G T S b G: An optimal transformation G would maximize traceðS L b Þ and minimize traceðS L w Þ simultaneously.</formula><p>Classical LDA aims to compute the optimal G, such that</p><formula xml:id="formula_7">G ¼ arg max G trace G T S w G À Á À1 G T S b G :<label>ð4Þ</label></formula><p>Other optimization criteria, including those based on the determinant, could also be used instead <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>. The solution to the optimization problem in (4) can be obtained by solving an eigenvalue problem on S À1 w S b <ref type="bibr" target="#b9">[10]</ref>, provided that the within-class scatter matrix S w is nonsingular. Since the rank of the between-class scatter matrix is bounded from above by k À 1, there are at most k À 1 discriminant vectors by classical LDA. A stable way to solve this eigenvalue problem is to apply SVD on the scatter matrices. Details can be found in <ref type="bibr" target="#b24">[25]</ref>.</p><p>Classical LDA is equivalent to maximum likelihood classification assuming normal distribution for each class with the common covariance matrix. Although relying on assumptions which do not hold in many applications, LDA has been proven to be effective. This is mainly due to the fact that a simple, linear model is more robust against noise, and most likely will not overfit. Generalization of LDA by fitting Gaussian mixtures to each class has been studied in <ref type="bibr" target="#b12">[13]</ref>.</p><p>Classical LDA cannot handle singular scatter matrices, which limits its applicability to low-dimensional data. Several methods, including pseudoinverse-based LDA <ref type="bibr" target="#b28">[29]</ref>, subspace LDA <ref type="bibr" target="#b24">[25]</ref>, regularized LDA <ref type="bibr" target="#b8">[9]</ref>, LDA/GSVD <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, and Penalized LDA <ref type="bibr" target="#b11">[12]</ref>, were proposed in the past to deal with the singularity problem. More details can be found in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>In pseudoinverse-based LDA, the pseudoinverse is applied to avoid the singularity problem, which is equivalent to approximating the solution using a leastsquares method. In subspace LDA, an intermediate dimension reduction algorithm, such as PCA, is applied to reduce the dimension of the original data, before classical LDA is applied. A limitation of this approach is that the optimal value of the reduced dimension for the intermediate dimension reduction algorithm is difficult to determine. In regularized LDA, a positive constant is added to the diagonal elements of S w , as S w þ I N , where I N is an identity matrix. The matrix S w þ I N is positive definite, for any &gt; 0, hence nonsingular. A limitation of this approach is that the optimal value of the parameter is difficult to determine. Cross validation is commonly applied to estimate the optimal .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UNCORRELATED LINEAR DISCRIMINANT ANALYSIS (ULDA)</head><p>ULDA aims to find the optimal discriminant vectors that are S-orthogonal. <ref type="foot" target="#foot_0">2</ref> Specifically, suppose r vectors 1 ; 2 ; Á Á Á ; r are obtained, then the ðr þ 1Þth vector rþ1 is found to maximize the Fisher criterion function <ref type="bibr" target="#b16">[17]</ref>:</p><formula xml:id="formula_8">fðÞ ¼ T S b T S w ;</formula><p>subject to the constraints: T rþ1 S i ¼ 0, for i ¼ 1; Á Á Á ; r. The algorithm in <ref type="bibr" target="#b16">[17]</ref> finds i successively as follows:</p><p>The jth discriminant vector j of ULDA is the eigenvector corresponding to the maximum eigenvalue of the following generalized eigenvalue problem: U j S b j ¼ j S w j , where</p><formula xml:id="formula_9">U 1 ¼ I N ; D j ¼ ½ 1 ; Á Á Á ; jÀ1 T ðj &gt; 1Þ; U j ¼ I N À SD T j ðD j SS À1 w SD T j Þ À1 D j SS À1 w ðj &gt; 1Þ;</formula><p>and I N is the identity matrix. Assume that f i g d i¼1 are the d optimal discriminant vectors for the above ULDA formulation. Then, the original data matrix</p><formula xml:id="formula_10">A is transformed into Z ¼ G T A, where G ¼ ½ 1 ; Á Á Á ; d . The ith feature component of Z is z i ¼ T i A</formula><p>, and the covariance between z i and z j is</p><formula xml:id="formula_11">Covðz i ; z j Þ ¼ Eðz i À Ez i Þðz j À Ez j Þ ¼ T i fEðA À EAÞðA À EAÞ T g j ¼ T i S j :<label>ð5Þ</label></formula><p>Hence, their correlation coefficient is</p><formula xml:id="formula_12">CorðZ i ; Z j Þ ¼ T i S j ffiffiffiffiffiffiffiffiffiffiffiffiffi T i S i p ffiffiffiffiffiffiffiffiffiffiffiffiffi ffi T j S j q :<label>ð6Þ</label></formula><p>Since the discriminant vectors of ULDA are S-orthogonal, i.e., T i S j ¼ 0, for i 6 ¼ j, we have CorðZ i ; Z j Þ ¼ 0, for i 6 ¼ j. That is, the feature vectors transformed by ULDA are mutually uncorrelated. This is a desirable property for feature reduction. More details on the role of uncorrelated attributes can be found in <ref type="bibr" target="#b16">[17]</ref>. The limitation of the above ULDA algorithm is the expensive computation of the d generalized eigenvalue problems, where d is number of optimal discriminant vectors of ULDA.</p><p>In the literature for LDA, Foley-Sammon Linear Discriminant Analysis (FSLDA), which was proposed by Foley and Sammon for two-class problems <ref type="bibr" target="#b7">[8]</ref>, has also received attention. It was then extended to the multiclass problems by Duchene and Leclerq <ref type="bibr" target="#b4">[5]</ref>. Both ULDA and FSLDA use the same Fisher criterion function. The main difference is that the optimal discriminant vectors generated by ULDA are S-orthogonal to each other, while the optimal discriminant vectors by FSLDA are orthogonal to each other.</p><p>In this section, we first show the equivalence relationship between classical ULDA and a variant of classical LDA, which holds regardless of the distribution of the eigenvalues of S À1 w S b . This result enhances the one in <ref type="bibr" target="#b17">[18]</ref> where the equivalence between these two is based on the assumption that there are no multiple eigenvalues for S À1</p><p>w S b (note that both results assume that the within-class scatter matrix S w is nonsingular). Based on this equivalence relationship, we propose ULDA/QR to simplify the ULDA implementation in <ref type="bibr" target="#b16">[17]</ref>.</p><p>Consider a variant of classical LDA in (4) as follows:</p><formula xml:id="formula_13">G ¼ arg max G T SG¼I ' F ðGÞ;<label>ð7Þ</label></formula><p>where</p><formula xml:id="formula_14">F ðGÞ ¼ trace G T S w G À Á À1 G T S b G :<label>ð8Þ</label></formula><p>The use of the total scatter S in discriminant analysis has been discussed in <ref type="bibr" target="#b2">[3]</ref>. Note that the ULDA algorithm discussed in the previous section finds the discriminant vectors in G successively. However, in the new formulation above, we compute all discriminant vectors simultaneously. S-orthogonality is enforced as a constraint. Our main result in this section, summarized in Theorem 2, shows that these two formulations for ULDA are equivalent.</p><p>The main technique for solving the optimization problem in <ref type="bibr" target="#b6">(7)</ref> is the simultaneous diagonalization of the within-class and between-class scatter matrices. It is well-known that, for a symmetric positive definite matrix S w and a symmetric matrix S b , there exists a nonsingular matrix X such that</p><formula xml:id="formula_15">X T S w X ¼ I N ;<label>ð9Þ</label></formula><formula xml:id="formula_16">X T S b X ¼ Ã ¼ diagð 1 ; Á Á Á ; N Þ;<label>ð10Þ</label></formula><p>where 1 ! Á Á Á ! N <ref type="bibr" target="#b10">[11]</ref>. The matrix X can be computed efficiently based on the QR-decomposition as follows: Let H T w ¼ QR be the QR-decomposition of H T w , where H w is defined in (1), Q 2 IR nÂN has orthonormal columns and R 2 IR NÂN is upper triangular and nonsingular. Then,</p><formula xml:id="formula_17">S w ¼ H w H T w ¼ R T R and ðR À1 Þ T S w R À1 ¼ I N .</formula><p>That is, R À1 diagonalizes the within-class scatter matrix S w . Next, consider the matrix</p><formula xml:id="formula_18">ðR À1 Þ T S b R À1 ¼ H T b R À1 À Á T H T b R À1 À Á Y T Y ;</formula><p>where</p><formula xml:id="formula_19">Y ¼ H T b R À1 . Let Y ¼ UAEV T be the SVD of Y , where U 2 IR nÂq , AE ¼ diagð 1 ; Á Á Á ; q Þ 2 IR qÂq , V 2 IR NÂq , 1 ! Á Á Á ! q , and q ¼ rankðH b Þ. It is easy to check that X ¼ R À1 V diagonalizes both S w</formula><p>and S b and satisfies the conditions in ( <ref type="formula" target="#formula_15">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>.</p><p>It can be shown that the matrix consisting of the first q columns of X computed above (with normalization) solves the optimization problem in <ref type="bibr" target="#b6">(7)</ref>, where q is the rank of the matrix S b , as stated in the following theorem: Theorem 1. Let the matrix X be defined as in ( <ref type="formula" target="#formula_15">9</ref>) and <ref type="bibr" target="#b9">(10)</ref>, and</p><formula xml:id="formula_20">q ¼ rankðS b Þ. Let G Ã ¼ x1 ; Á Á Á ; xq Â Ã , where xi ¼ 1 ffiffiffiffiffiffiffi ffi 1þ i p</formula><p>x i , x i is the ith column of the matrix X, and i s are defined in <ref type="bibr" target="#b9">(10)</ref>. Then, G Ã solves the optimization problem in <ref type="bibr" target="#b6">(7)</ref>.</p><p>Proof. It is clear that the constraint in ( <ref type="formula" target="#formula_13">7</ref>) is satisfied for G ¼ G Ã . Next, we only need to show that the maximum of F ðGÞ is obtained at G Ã . By ( <ref type="formula" target="#formula_15">9</ref>) and ( <ref type="formula" target="#formula_16">10</ref>), we have</p><formula xml:id="formula_21">G T S w G ¼ G T X ÀT ðX T S w XÞX À1 G ¼ G GT ; G T S b G ¼ G T X ÀT ðX T S b XÞX À1 G ¼ GÃ GT ; where G ¼ X À1 G ð Þ T . Hence, F ðGÞ ¼ trace G GT À Á À1 GÃ GT À Á :</formula><p>Let GT ¼ QR be the QR-decomposition of GT 2 IR NÂ' (note that GT has full column rank), where Q 2 IR NÂ' has orthonormal columns and R is nonsingular. Using the fact that traceðABÞ ¼ traceðBAÞ, for any matrices A and B, we have</p><formula xml:id="formula_22">F ðGÞ ¼ trace R T R À Á À1 R T Q T ÃQR À Á ¼ trace Q T ÃQ À Á 1 þ Á Á Á þ q ;</formula><p>where the inequality becomes an equality for</p><formula xml:id="formula_23">Q ¼ I ' 0 or G ¼ X I ' 0 R;</formula><p>when the reduced dimension ' ¼ q. Note that R is an arbitrary upper triangular and nonsingular matrix. Hence, G Ã corresponds to the case when R is set to be</p><formula xml:id="formula_24">R ¼ diag 1 ffiffiffiffiffiffiffiffiffiffiffiffiffi 1 þ 1 p ; Á Á Á ; 1 ffiffiffiffiffiffiffiffiffiffiffiffiffi 1 þ q p ! : u t</formula><p>We are now ready to present our main result for this section:</p><p>Theorem 2. Let xi be defined as in Theorem 1. Then, fx i g q i¼1 forms a set of optimal discriminant vectors for ULDA. </p><formula xml:id="formula_25">T rþ1 S i ¼ 0; for i ¼ 1; Á Á Á ; r. Let rþ1 ¼ P N i¼1 i xi , since fx i g N i¼1 forms a base for IR N . By the constraints T rþ1 S i ¼ 0, for i ¼ 1; Á Á Á ; r, we have i ¼ 0, for i ¼ 1; Á Á Á ; r, hence rþ1 ¼ P N i¼rþ1 i xi .</formula><p>It follows from ( <ref type="formula" target="#formula_15">9</ref>) and (10) that</p><formula xml:id="formula_26">fð rþ1 Þ ¼ P N i¼rþ1 i xT i S b P N i¼rþ1 i xi P N i¼rþ1 i xT i S w P N i¼rþ1 i xi ¼ P N i¼rþ1 2 i i P N i¼rþ1 2 i P N i¼rþ1 2 i rþ1 P m i¼rþ1 2 i ¼ rþ1 ;</formula><p>where the inequality becomes an equality if </p><formula xml:id="formula_27">i ¼ 0, for i ¼ r þ 2; Á Á Á ; N.</formula><formula xml:id="formula_28">T b R À1 . 4. Compute the SVD of Y as Y ¼ UAEV T , where U 2 IR nÂq , AE ¼ diagð 1 ; Á Á Á ; q Þ 2 IR qÂq , V 2 IR NÂq , 1 ! Á Á Á ! q , and q ¼ rankðH b Þ. 5. ½x 1 ; Á Á Á ; x q R À1 V . 6. i 2 i , for i ¼ 1; Á Á Á ; q. 7. xi 1 ffiffiffiffiffiffiffi ffi 1þi p x i , for i ¼ 1; Á Á Á ; q.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE ULDA/GSVD ALGORITHM</head><p>In the previous section, a variant of the classical LDA criterion was presented in <ref type="bibr" target="#b6">(7)</ref>. It was shown that the solution to the optimization problem in <ref type="bibr" target="#b6">(7)</ref> forms optimal discriminant vectors for classical ULDA. Thus, it provides an efficient way for computing the optimal discriminant vectors for ULDA. However, the algorithm assumes the nonsingularity of S w , which limits its applicability to lowdimensional data. In <ref type="bibr" target="#b16">[17]</ref>, a subspace-based method is presented to overcome the singularity problem, where the ULDA algorithm is preceded by PCA. However, the PCA stage may lose some useful information. In this section, we propose a new feature reduction algorithm, called ULDA/ GSVD. The new criterion underlying ULDA/GSVD is motivated by the criterion in <ref type="bibr" target="#b6">(7)</ref> and the regularized LDA. The new optimization problem for ULDA/GSVD is defined as follows:</p><formula xml:id="formula_29">G ¼ arg max G T SG¼I ' F ðGÞ;<label>ð11Þ</label></formula><p>where</p><formula xml:id="formula_30">F ðGÞ ¼ trace ðG T S w G þ I ' Þ À1 G T S b G . Note that matrix G T S w G þ I ' is guaranteed to be nonsingular for &gt; 0.</formula><p>Recall that a limitation of regularized LDA is that the optimal value of the perturbation is difficult to determine. A key difference between ULDA/GSVD and regularized LDA is that the optimal solution to ULDA/GSVD is independent of the regularization parameter, i.e., G 1 ¼ G 2 , for any 1 ; 2 &gt; 0. The main result of this section is summarized in the following theorem:</p><formula xml:id="formula_31">Theorem 3. Let G Ã</formula><p>, for any &gt; 0, be the optimal solution to the optimization problem in <ref type="bibr" target="#b10">(11)</ref>. Then, the following equality holds:</p><formula xml:id="formula_32">G Ã 1 ¼ G Ã 2 ; for any 1 ; 2 &gt; 0:<label>ð12Þ</label></formula><p>To prove Theorem 3, we first show how to compute G Ã , for any &gt; 0. Recall that when the within-class scatter matrix is nonsingular, the optimal transformation can be computed by finding the matrix X, which simultaneously diagonalizes the scatter matrices. For this, the Generalized Singular Value Decomposition (GSVD) can be applied, even when both matrices are singular. A simple algorithm to compute GSVD can be found in <ref type="bibr" target="#b14">[15]</ref>, where the algorithm is based on <ref type="bibr" target="#b20">[21]</ref>.</p><p>The computation of G Ã , for any &gt; 0, is based on the following two lemmas: Lemma 1. Let S w , S b , and S be defined as in Section 2, and let t ¼ rankðSÞ. Then, there exists a nonsingular matrix X 2 IR NÂN , such that</p><formula xml:id="formula_33">X T S b X ¼ D 1 ¼ diagð 2 1 ; Á Á Á ; 2 t ; 0; Á Á Á ; 0Þ;<label>ð13Þ</label></formula><formula xml:id="formula_34">X T S w X ¼ D 2 ¼ diagð 2 1 ; Á Á Á ; 2 t ; 0; Á Á Á ; 0Þ;<label>ð14Þ</label></formula><p>where</p><formula xml:id="formula_35">1 ! 1 ! Á Á Á ! q &gt; 0 ¼ qþ1 ¼ Á Á Á ¼ t ; 0 1 Á Á Á t 1; D 1 þ D 2 ¼ I t 0 0 0 ; and q ¼ rankðS b Þ. Proof. Let K ¼ H T b H T w ! ;</formula><p>which is an ðn þ kÞ Â N matrix. By the Generalized Singular Value Decomposition <ref type="bibr" target="#b20">[21]</ref>, there exist orthogonal matrices U 2 IR kÂk , V 2 IR nÂn , and a nonsingular matrix X 2 IR NÂN , such that</p><formula xml:id="formula_36">U 0 0 V ! T KX ¼ AE 1 0 AE 2 0 ! ;<label>ð15Þ</label></formula><p>where</p><formula xml:id="formula_37">AE T 1 AE 1 ¼ diagð 2 1 ; Á Á Á ; 2 t Þ; AE T 2 AE 2 ¼ diagð 2 1 ; Á Á Á ; 2 t Þ; 1 ! 1 ! Á Á Á ! q &gt; 0 ¼ qþ1 ¼ Á Á Á ¼ t ; 0 1 Á Á Á t 1; 2 i þ 2 i ¼ 1, for i ¼ 1; Á Á Á ; t, and q ¼ rankðH b Þ ¼ rankðS b Þ. Hence, H T b X ¼ U AE 1 0 ½ ,<label>and</label></formula><formula xml:id="formula_38">H T w X ¼ V AE 2 0 ½ . It follows that X T S b X ¼ X T H b H T b X ¼ AE T 1 AE 1 0 0 0 ! ¼ D 1 ; X T S w X ¼ X T H w H T w X ¼ AE T 2 AE 2 0 0 0 ! ¼ D 2 ;</formula><p>where</p><formula xml:id="formula_39">D 1 þ D 2 ¼ I t 0 0 0 . t u</formula><p>Lemma 2. Define a trace optimization problem as follows:</p><formula xml:id="formula_40">G ¼ arg max G T G¼I' trace G T WG À Á À1 G T BG ;<label>ð16Þ</label></formula><formula xml:id="formula_41">where W ¼ diagðw 1 ; Á Á Á ; w u Þ 2 IR uÂu is a diagonal matrix with 0 &lt; w 1 Á Á Á w u , and B ¼ diagðb 1 ; Á Á Á ; b u Þ 2 IR uÂu is also diagonal with b 1 ! Á Á Á ! b q &gt; 0 ¼ b qþ1 ¼ Á Á Á ¼ b u .</formula><p>Then, G ? ¼ I q ; 0 À Á T solves the optimization problem in <ref type="bibr" target="#b15">(16)</ref> with ' ¼ q.</p><p>Proof. It is clear that the constraint in the optimization in ( <ref type="formula" target="#formula_40">16</ref>) is satisfied for G ? with ' ¼ q. Next, we show that G ? solves the following optimization problem:</p><formula xml:id="formula_42">G ¼ arg max G trace G T WG À Á À1 G T BG :<label>ð17Þ</label></formula><p>It is well-known that the solution can be obtained by solving the eigenvalue problem on W À1 B since W is nonsingular. Note that W À1 B is diagonal and only the first q diagonal entries are nonzero. Hence, e i , for i ¼ 1; Á Á Á ; q, is the eigenvector of W À1 B corresponding to the ith largest eigenvalue, where e i ¼ 0; Á Á Á ; 1; 0 Á Á Á ; 0 ð Þ T and the one appears at the ith entry. Therefore, G ? ¼ I q ; 0 À Á T solves the optimization in <ref type="bibr" target="#b16">(17)</ref>.</p><formula xml:id="formula_43">t u</formula><p>With Lemma 1 and Lemma 2, we can compute G Ã , for any &gt; 0, as follows: Theorem 4. Let the matrix X be defined as in Lemma 1, and let</p><formula xml:id="formula_44">q ¼ rankðS b Þ. Then, G Ã ¼ X I q 0</formula><p>solves the optimization problem in <ref type="bibr" target="#b10">(11)</ref> with</p><formula xml:id="formula_45">' ¼ q. Proof. By Lemma 1, X T S b X ¼ D 1 , X T S w X ¼ D 2</formula><p>, where the two diagonal matrices D 1 and D 2 satisfy</p><formula xml:id="formula_46">D 1 þ D 2 ¼ I t 0 0 0 : It is easy to check that ðG Ã Þ T SG Ã ¼ I q ; 0 À Á X T ðS b þ S w ÞX I q 0 ¼ I q ; 0 À Á ðD 1 þ D 2 Þ I q 0 ¼ I q ;</formula><p>i.e., the constraint in the optimization problem in ( <ref type="formula" target="#formula_29">11</ref>) is satisfied. Next, we show that G Ã minimizes F ðGÞ. Since</p><formula xml:id="formula_47">G T S b G ¼ G T ðX À1 Þ T ðX T S b XÞX À1 G ¼ GD 1 GT ; G T S w G ¼ G T ðX À1 Þ T ðX T S w XÞX À1 G ¼ GD 2 GT ;</formula><p>where G ¼ ðX À1 GÞ T , F ðGÞ can then be rewritten as</p><formula xml:id="formula_48">F ðGÞ ¼ trace GD 2 GT þ I ' À Á À1 GD 1 GT :<label>ð18Þ</label></formula><formula xml:id="formula_49">Let G ¼ G T 1 ; G T 2 À Á be a partition of G, such that G T</formula><p>1 2 IR 'Ât and G T 2 2 IR 'ÂðNÀtÞ . By the constraint that G T SG ¼ I ' , we have</p><formula xml:id="formula_50">I ' ¼ G T SG ¼ G T ðS w þ S b ÞG ¼ G T S b G þ G T S w G ¼ GD 1 GT þ GD 2 GT ¼ GðD 1 þ D 2 Þ GT ¼ G T 1 G 1 : Hence, F<label>ðGÞ</label></formula><p>in ( <ref type="formula" target="#formula_48">18</ref>) can be rewritten as</p><formula xml:id="formula_51">F ðGÞ ¼ trace G T 1 D t 2 þ I ' À Á G 1 À Á À1 G T 1 D t 1 G 1 ;</formula><p>where D t 1 and D t 2 are the tth leading submatrices of D 1 and D 2 , respectively. It is clear that F ðGÞ is independent of G 2 . Hence, we can simplify set</p><formula xml:id="formula_52">G 2 ¼ 0. Denote AE ¼ D t 2 þ I t À Á</formula><p>, which is a nonsingular and diagonal matrix. It follows that</p><formula xml:id="formula_53">F ðGÞ ¼ trace G T 1 AEG 1 À Á À1 G T 1 D t 1 G 1 :</formula><p>The result then follows from Lemma 2, with W ¼ AE and</p><formula xml:id="formula_54">B ¼ D t 1 . t u</formula><p>Theorem 4 implies that the optimal solution G ? to the optimization problem in <ref type="bibr" target="#b10">(11)</ref> only depends on X, which is determined by H w and H b , hence it is independent of . That is, G Ã 1 ¼ G Ã 2 , for any 1 ; 2 &gt; 0. This completes the proof of the main result of this section, which is summarized in Theorem 3.</p><p>The computation of the optimal transformation G Ã is summarized in Algorithm 2.</p><p>Algorithm 2: The ULDA/GSVD Algorithm Input: Data matrix A Output: Optimal transformation matrix G Ã 1. Form H b and H w as in ( <ref type="formula" target="#formula_3">2</ref>) and (1). 2. Compute GSVD on the matrix pair ðH T b ; H T w Þ to obtain the matrix X, as in Lemma 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">q rankðH</head><formula xml:id="formula_55">b Þ. 4. G Ã ½X 1 ; Á Á Á ; X q .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Efficient Computation of Diagonalizing Matrix X</head><p>In Lemma 1, a nonsingular matrix X is computed by applying GSVD, which may be expensive, especially for large matrices. A key property of X which leads to the optimal solution G Ã is that it diagonalizes the scatter matrices simultaneously. In this section, we present an efficient algorithm for computing the diagonalizing matrix X without the GSVD computation. Let H t ¼ UAEV T be the SVD of H t , where H t is defined in (3), U 2 IR NÂN and V 2 IR nÂn are orthogonal, and AE 2 IR NÂn is diagonal. Then,</p><formula xml:id="formula_56">S ¼ H t H T t ¼ UAEV T V AE T U T ¼ UAEAE T U T :</formula><p>That is, the eigen-decomposition of S can be obtained by computing the SVD of H t . Let U ¼ ðU 1 ; U 2 Þ be the partition of U, such that U 1 2 IR NÂt and U 2 2 IR NÂðNÀtÞ , where t ¼ rankðSÞ. Let AEAE T ¼ diag AE 2 t ; 0 À Á , where AE t 2 IR tÂt is diagonal and nonsingular. Since S ¼ S b þ S w , the null space, U 2 , of S t also lies in the null space of S b and S w , that is,</p><formula xml:id="formula_57">U T 2 S b U 2 ¼ 0 and U T 2 S w U 2 ¼ 0. Hence, AE 2 t ¼ U T 1 S b U 1 þ U T 1 S w U 1<label>ð19Þ</label></formula><p>and</p><formula xml:id="formula_58">I t ¼ AE À1 t U T 1 S b U 1 AE À1 t þ AE À1 t U T 1 S w U 1 AE À1 t :<label>ð20Þ</label></formula><formula xml:id="formula_59">Recall from (2) that S b ¼ H b H T b . Denote B ¼ AE À1 t U T</formula><p>1 H b and let B ¼ P AEQ T be the SVD of B, where P and Q are orthogonal and AE is diagonal. Then,</p><formula xml:id="formula_60">AE À1 t U T 1 S b U 1 AE À1 t ¼ P AE AET P T ¼ P AE b P T ; where AE b ¼ AE AET ¼ diagð 1 ; Á Á Á ; t Þ, 1 ! Á Á Á ! q &gt; 0 ¼ qþ1 ¼ Á Á Á ¼ t ;</formula><p>and q ¼ rankðS b Þ. It can be verified that the matrix X below diagonalizes the three scatter matrices simultaneously:</p><formula xml:id="formula_61">X ¼ U AE À1 t P 0 0 I :<label>ð21Þ</label></formula><p>The pseudocode for the computation of X is given in Algorithm 3.</p><p>Algorithm 3: Efficient computation of diagonalizing matrix X Input: data matrix A Output: matrix X 1. Form matrices H b and H t as in ( <ref type="formula" target="#formula_3">2</ref>) and (3). 2. Compute SVD of H t as</p><formula xml:id="formula_62">H t ¼ U 1 AE t V T 1 . 4. B AE t U T 1 H b . 5. Compute SVD of B as B ¼ P AEQ T ; q rankðBÞ.</formula><p>6. X U AE À1 t P 0 0 I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Relationship between ULDA/GSVD and ULDA/QR</head><p>In this section, we show that ULDA/GSVD is equivalent to ULDA/QR when the within-class scatter matrix S w is nonsingular. Therefore, ULDA/QR can be considered as a special case of ULDA/GSVD when S w is nonsingular. Note that ULDA/GSVD is more general in the sense that it is applicable regardless of the singularity of S w .</p><p>Recall that ULDA/QR involves the matrix X, which satisfies</p><formula xml:id="formula_63">X T S w X ¼ I N ; X T S b X ¼ Ã ¼ diagð 1 ; Á Á Á ; N Þ; where 1 ! Á Á Á ! N .</formula><p>The final transformation matrix</p><formula xml:id="formula_64">G Ã ¼ x1 ; Á Á Á ; xq Â Ã , where xi ¼ 1 ffiffiffiffiffiffiffi ffi 1þ i p x i , x i is the ith column of the matrix X. It follows that ðG Ã Þ T SG Ã ¼ I q ;<label>ð22Þ</label></formula><formula xml:id="formula_65">ðG Ã Þ T S b G Ã ¼ diag 1 1 þ 1 ; Á Á Á ; q 1 þ q :<label>ð23Þ</label></formula><p>Since fðxÞ ¼ x=ð1 þ xÞ is an increasing function, we have</p><formula xml:id="formula_66">1 1 þ 1 ! Á Á Á ! q 1 þ q :</formula><p>Thus, the transformation matrix G Ã from ULDA/QR satisfies the conditions in Lemma 1 for ULDA/GSVD. That is, ULDA/GSVD is equivalent to ULDA/QR, when the within-class scatter matrix S w is nonsingular. Note that ULDA/QR is not applicable when S w is singular. ULDA/ GSVD can thus be considered as an extension of ULDA/QR for a singular within-class scatter matrix. In the following experimental studies, we focus on the ULDA/GSVD algorithm.</p><p>We close this section by showing the classification property of ULDA/GSVD and ULDA/QR: Theorem 5. Let G be the optimal transformation matrix for ULDA/GSVD. Then, for any test point h, the following equality holds:</p><formula xml:id="formula_67">arg min j ðh À c j Þ T S þ ðh À c j Þ n o ¼ arg min j jjG T ðh À c j Þjj 2 n o :</formula><p>Proof. Let X i be the ith column of X. Note that G consists of the first q columns of X, and q ¼ rankðS b Þ. From ( <ref type="formula" target="#formula_33">13</ref>) and ( <ref type="formula" target="#formula_34">14</ref>), we have</p><formula xml:id="formula_68">S þ ¼ XðD 1 þ D 2 ÞX T ¼ GG T þ X t i¼qþ1 X i X T i : From (13), X T i S b X i ¼ 0, for i ¼ q þ 1; Á Á Á ; t. Hence, ðc j Þ T X i ¼ cX i , for all j ¼ 1; Á Á Á ; k. It follows that ðh À c j Þ T S þ ðh À c j Þ ¼ jjG T ðh À c j Þjj 2 þ X t i¼qþ1 ðh À cÞ T X i X T i ðh À cÞ:<label>ð24Þ</label></formula><p>The main result follows, since the second term on the right-hand side of ( <ref type="formula" target="#formula_68">24</ref>) is independent of j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>t u</head><p>When S is nonsingular, the classification in ULDA/QR uses the Mahalanobis distance measure as follows:</p><p>Corollary 1. Assume S is nonsingular. Let G be the optimal transformation matrix for ULDA/QR. Then, for any test point h, the following equality holds:</p><formula xml:id="formula_69">arg min j ðh À c j Þ T S À1 ðh À c j Þ n o ¼ arg min j jjG T ðh À c j Þjj 2 n o :</formula><p>Corollary 1 shows that the classification in ULDA/QR is based on the Mahalanobis distance measure, while Theorem 5 shows that the classification in ULDA/GSVD is based on the modified Mahalanobis distance measure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>We evaluate the effectiveness of the ULDA/GSVD algorithm in this section. Section 6.1 describes our test data sets. Section 6.2 examines the effect of the number of reduced dimensions on the classification performance of ULDA/ GSVD. In Section 6.3, we compare ULDA/GSVD with PCA, OCM, and subspace ULDA, as well as SVM, in terms of classification accuracy. The K-Nearest-Neighbor (K-NN) algorithm with different values of K is used as the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Data Sets</head><p>We used two data sets: Spambase and Wine from the UCI Machine Learning Repository. 3 We used a subset of the original Spambase data set, which consists of spam and nonspam emails. Most of the features indicate whether a particular word or character occurred frequently in the e-mail. The Wine data set is the result of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The features correspond to the quantities of 13 different constituents found in each of the three types of wines. For these two data sets, the data dimension (N) is much smaller than the sample size (n). We also used six other data sets: GCM, ALL, tr41, re1, PIX, and ORL, where the data dimension is much larger than the sample size. In this case, ULDA/QR is not applicable, since all scatter matrices are singular, while ULDA/GSVD is still applicable. GCM <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b29">[30]</ref> and ALL <ref type="bibr" target="#b30">[31]</ref> are microarray gene expression data sets; tr41 is a document data set derived from the TREC-5, TREC-6, and TREC-7 collections; 4 re1 is another document data set derived from Reuters-21578 text categorization test collection Distribution 1.0; 5 and ORL 6 and PIX 7 are two face image data sets.</p><p>Table <ref type="table" target="#tab_4">2</ref> summarizes the statistics of our test data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of the Number of Reduced Dimensions on ULDA/GSVD</head><p>In this experiment, we study the effect of the number of reduced dimensions on the classification performance of ULDA/GSVD. The number of reduced dimensions ranges from 1 to 20. The classification results on the GCM and ALL data sets are shown in Fig. <ref type="figure" target="#fig_1">1</ref>, where the horizontal axis is the number of reduced dimensions and the vertical axis is the classification accuracy. We can observe that the accuracy tends to increase when the number of reduced dimensions increases, until q ¼ rankðH b Þ (13 for GCM and 5 for ALL) is reached. Similar trends have been observed from other data sets, and the results are not presented. In the following experiment, we set the reduced dimension of ULDA/GSVD to be the rank of H b .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison of Classification Accuracy</head><p>In this experiment, we applied ULDA/GSVD to the eight data sets from Table <ref type="table" target="#tab_4">2</ref> and compared with OCM, PCA, and subspace ULDA in terms of classification accuracy. The results are summarized in Table <ref type="table" target="#tab_5">3</ref>. The number of principal components used in PCA and Subspace ULDA is determined through cross-validation, and may be different for different data sets. For data sets, including Spambase, Wine, GCM, and ALL, the training and test sets given in the original data sets are used for computing the accuracy. For the other four data sets, including tr41, re1, PIX, and ORL, where the training and test sets are not given, we performed our study by repeated random splitting into training and test sets exactly as in <ref type="bibr" target="#b6">[7]</ref>. The data was partitioned randomly into a training set consisting of two-thirds of the whole set and a test set consisting of one-third of the whole set. To reduce the variability, the splitting was repeated 50 times and the resulting accuracies were averaged. The standard deviation for each data set was also reported.</p><p>The main observations from Table <ref type="table" target="#tab_5">3</ref> include: 1) ULDA/ GSVD is competitive with the other three algorithms for all data sets in terms of classification. Subspace ULDA performs well for most data sets. However, subspace ULDA applies cross-validation for determining the optimal set of principal components in the PCA step, which can be expensive, especially for large data sets. Besides, the variance of the results for the other three methods is generally larger than that of ULDA/GSVD. This implies that ULDA/GSVD provides a more consistent result. 2) ULDA/GSVD is extremely stable under different K-NN classifiers for all data sets, whereas the performance of OCM and PCA degrades for many cases, as the number, K, of nearest neighbors increases. Subspace ULDA is also stable under different K-NN classifiers for most data sets. 3) PCA does not perform well in many cases. This is likely related to the fact that PCA is unsupervised and does not use the class label information, while the other three algorithms fully utilize the class label information. OCM performs well for the two document data sets and the two face image data sets, while it performs poorly for the other data sets. Both PCA and OCM perform poorly in Spambase and Wine, in comparison with ULDA/GSVD and subspace ULDA.</p><p>We have also done some preliminary studies in comparing ULDA/GSVD with linear SVM. 1NN is used to compute the accuracy for ULDA/GSVD. The main result is summarized in Fig. <ref type="figure" target="#fig_2">2</ref>, where the x-axis denotes the eight data sets, and the y-axis denotes the classification accuracy. For tr41, re1, PIX, and ORL, the mean accuracy for 50 different runs are reported. Overall, ULDA/GSVD and linear SVM are comparable in terms of classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>Uncorrelated features with minimum redundancy are highly desirable in feature reduction. In this paper, we present a theoretical and empirical study on uncorrelated Linear Discriminant Analysis (ULDA). We first present the theoretical result on the equivalence relationship between classical ULDA and classical LDA, which leads to a fast implementation of ULDA, ULDA/QR. Then, we propose ULDA/GSVD, based on a novel optimization criterion, that can successfully overcome the singularity problem in classical ULDA. The criterion used in ULDA/GSVD is the perturbed version of the one from ULDA/QR, while the solution to ULDA/GSVD is shown to be independent of the amount of perturbation applied, thus avoiding the limitation in regularized LDA. Experimental results on various types of data show the superiority of ULDA/GSVD over other competing algorithms including PCA, OCM, and subspace ULDA.</p><p>Experimental results show that ULDA/GSVD is extremely stable under different K-NN classifiers for all data sets. We plan to carry out detailed theoretical analysis on this in the future. The current work focuses on linear discriminant analysis, which applies a linear decision boundary. Discriminant analysis can also be studied in a nonlinear fashion-so-called kernel discriminant analysisby using the kernel trick <ref type="bibr" target="#b23">[24]</ref>. This is desirable if the data      </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>"-" means that the natural splitting of the data set into training and test set is not available. For Spambase, Wine, GCM, and ALL, the original training and test sets are given, while for tr41, re1, PIX, and ORL, the original splitting is not provided.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Effect of the number of reduced dimensions on the classification performance of ULDA/GSVD for (a) the GCM and (b) ALL data sets. The optimal numbers of reduced dimensions for GCM and ALL are 13 and 5, respectively.</figDesc><graphic coords="9,34.47,69.17,497.54,210.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Comparison of classification accuracy between ULDA/GSVD and SVM. For tr41, re1, PIX, and ORL, the mean accuracy for 50 different runs are reported.</figDesc><graphic coords="10,117.69,69.17,331.09,174.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Summary of Notations Used</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>An efficient algorithm for computing fx i g q i¼1 through QR-decomposition is presented below as Algorithm 1. Algorithm 1: The ULDA/QR Algorithm Input: Data matrix A. Output: Discriminant vectors xi s of ULDA. 1. Construct matrices H w and H b as in (1) and (2). 2. Compute the QR-decomposition of H T w as H T w ¼ QR, where Q 2 IR nÂN and R 2 IR NÂN . 3. Form the matrix Y H</figDesc><table /><note><p><p>Hence, xrþ1 can be chosen as the ðr þ 1Þth discriminant vector of ULDA, i.e., rþ1 ¼ xrþ1 .</p>t u</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>4. http://trec.nist.gov. 5. http://www.research.att.com/~lewis. 6. http://www.uk.research.att.com/facedatabase.html. 7. http://peipa.essex.ac.uk/ipa/pix/faces/manchester/test-hard/.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2</head><label>2</label><figDesc>Statistics for the Test Data Sets</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Comparison of Classification Accuracy on Four Different Methods</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Two vectors x and y are S-orthogonal, if x T Sy ¼ 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 18, NO. 10, OCTOBER 2006 3. http://www.ics.uci.edu/mlearn/MLRepository.html.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the four reviewers and the associate editor for their comments, which helped improve the paper significantly. The research of J. Ye and R. Janardan was sponsored, in part, by the Army High Performance Computing Research Center under the auspices of the Department of the Army, Army Research Laboratory cooperative agreement number DAAD19-01-2-0014, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Fellowships from Guidant Corporation and from the Department of Computer Science and Engineering, at the University of Minnesota, Twin Cities are gratefully acknowledged. The work of H. Park has been performed while serving as a program director at the US National Science Foundation (NSF) and was partly supported by IR/D from the NSF. Her work was also supported in part by the US National Science Foundation Grants CCR-0204109 and ACI-0305543. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the US National Science Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Belhumeour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hespanha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="711" to="720" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Using Linear Algebra for Intelligent Information Retrieval</title>
		<author>
			<persName><forename type="first">M</forename><surname>Berry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>O'brie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="573" to="595" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A New LDA-Based Face Recognition System Which Can Solve the Small Sample Size Problem</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1713" to="1726" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing by Latent Semantic Analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Soc. for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Optimal Transformation for Discriminant and Principal Component Analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Duchene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leclerq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="978" to="983" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Pattern Classification. Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of Discrimination Methods for the Classification of Tumors Using Gene Expression Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dudoit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fridlyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Speed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">457</biblScope>
			<biblScope unit="page" from="77" to="87" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An Optimal Set of Discriminant Vectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Foley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sammon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="281" to="289" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regularized Discriminant Analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">405</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<title level="m">Introduction to Statistical Pattern Classification</title>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
		<title level="m">Matrix Computations</title>
		<imprint>
			<publisher>The Johns Hopkins Univ. Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Penalized Discriminant Analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="73" to="102" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discriminant Analysis by Gaussian Mixtures</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statistical Soc. series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="158" to="176" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Structure Preserving Dimension Reduction for Clustered Text Data Based on the Generalized Singular Value Decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="179" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalizing Discriminant Analysis Using the Generalized Singular Value Decomposition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="995" to="1006" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face Recognition Based on the Uncorrelated Discriminant Transformation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1405" to="1416" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Theorem on the Uncorrelated Optimal Discriminant Vectors</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-S</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2041" to="2047" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Jolliffe</surname></persName>
		</author>
		<title level="m">Principal Component Analysis</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminant Analysis with Singular Covariance Matrices: Methods and Applications to Spectroscopic Data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Krzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mccarthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards a Generalized Singular Value Decomposition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="398" to="405" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lower Dimensional Representation of Text Data Based on Centroids and Least Squares</title>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rosen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT Numerical Math</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiclass Cancer Diagnosis Using Tumor Gene Expression Signatures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat&apos;l Academy of Science</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="15149" to="15154" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond</title>
		<author>
			<persName><forename type="first">B</forename><surname>Scho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using Discriminant Eigenfeatures for Image Retrieval</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Swets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="831" to="836" />
			<date type="published" when="1996-08">Aug. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face Recognition Using Eigenfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conf</title>
		<meeting>Computer Vision and Pattern Recognition Conf</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="502" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Optimization Criterion for Generalized Discriminant Analysis on Undersampled Problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Janardan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="982" to="994" />
			<date type="published" when="2004-08">Aug. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Molecular Classification of Multiple Tumor Types</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Yeang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Classification, Subtype Discovery, and Prediction of Outcome in Pediatric Lymphoblastic Leukemia by Gene Expression Profiling</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Yeoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Cell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="143" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
