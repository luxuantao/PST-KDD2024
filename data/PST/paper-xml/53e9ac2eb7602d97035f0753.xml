<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph characteristics from the heat kernel trace</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bai</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science School</orgName>
								<orgName type="institution">Beihang University</orgName>
								<address>
									<postCode>100191</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">PR China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edwin</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y010 5DD</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of York</orgName>
								<address>
									<postCode>Y010 5DD</postCode>
									<settlement>York</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph characteristics from the heat kernel trace</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C7C57C9955A9C4A11B6CAD2A6302C641</idno>
					<idno type="DOI">10.1016/j.patcog.2008.12.029</idno>
					<note type="submission">Received 26 November 2008 Accepted 23 December 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Heat kernel trace Graph invariants Image clustering and recognition</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph structures have been proved important in high level-vision since they can be used to represent structural and relational arrangements of objects in a scene. One of the problems that arises in the analysis of structural abstractions of objects is graph clustering. In this paper, we explore how permutation invariants computed from the trace of the heat kernel can be used to characterize graphs for the purposes of measuring similarity and clustering. The heat kernel is the solution of the heat equation and is a compact representation of the path-length distribution on a graph. The trace of the heat kernel is given by the sum of the Laplacian eigenvalues exponentiated with time. We explore three different approaches to characterizing the heat kernel trace as a function of time. Our first characterization is based on the zeta function, which from the Mellin transform is the moment generating function of the heat kernel trace. Our second characterization is unary and is found by computing the derivative of the zeta function at the origin. The third characterization is derived from the heat content, i.e. the sum of the elements of the heat kernel. We show how the heat content can be expanded as a power series in time, and the coefficients of the series can be computed using the Laplacian spectrum. We explore the use of these characterizations as a means of representing graph structure for the purposes of clustering, and compare them with the use of the Laplacian spectrum. Experiments with the synthetic and real-world databases reveal that each of the three proposed invariants is effective and outperforms the traditional Laplacian spectrum. Moreover, the heat-content invariants appear to consistently give the best results in both synthetic sensitivity studies and on real-world object recognition problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Structural information has been shown to perform an important role in object recognition. This is true not only when graph based abstractions of shape are being used <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b45">48]</ref>, but also when local features are used as object descriptors <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b44">47]</ref>. Recent contributions to this latter endeavor include the "bag of words" representation of spatial relationships <ref type="bibr" target="#b46">[49,</ref><ref type="bibr" target="#b26">27]</ref> based on SIFT features <ref type="bibr" target="#b47">[50]</ref> and the use of constellations to learn and recognize object classes <ref type="bibr" target="#b44">[47]</ref>. However, the use of local graph structures to capture feature arrangement falls well short of using a graph to capture global scene structure. Moreover, graph structures can be used to represent higher level information, and hence are closer to the semantic content of a scene.</p><p>One of the most elegant way of characterizing graph structure is to use spectral methods, which make use of the eigenvalues of the adjacency matrix or the Laplacian matrix (the adjacency matrix 0031-3203/$ -see front matter © 2009 Elsevier Ltd. All rights reserved. doi:10.1016/j.patcog.2008.12.029 minus the degree matrix). In fact, the Laplacian spectrum of a graph has found widespread use in computer vision for a number of applications including segmentation <ref type="bibr" target="#b5">[6]</ref>, routing <ref type="bibr" target="#b6">[7]</ref>, clustering <ref type="bibr" target="#b13">[14]</ref> and indexing <ref type="bibr" target="#b7">[8]</ref>. One of the most important properties of the Laplacian spectrum is its close relationship with the heat equation. The heat equation can be used to specify the flow of information with time across a network or a manifold <ref type="bibr" target="#b10">[11]</ref>. According to the heat equation, the time derivative of the kernel is determined by the graph Laplacian. The solution to the heat equation is hence obtained by exponentiating the Laplacian eigensystem over time. Because the heat kernel encapsulates the way in which information flows through the edges of the graph over time, it is closely related to the path-length distribution on the graph. An important permutation invariant property of the heat kernel is the trace. This is found by summing and exponentiating the Laplacian eigenvalues with time. Although Lebanon and Lafferty <ref type="bibr" target="#b9">[10]</ref>have explored the use of an embedding based on the heat kernel for data clustering, the heat kernel trace has not been thoroughly investigated as a structural representation in computer vision.</p><p>The aim of this paper is to overcome this omission by investigating whether the trace of the heat kernel <ref type="bibr" target="#b10">[11]</ref> can be used for the purposes of characterizing the properties of graphs. Here we explore whether the shape of this function can be used to characterize the corresponding graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related literature</head><p>One of the earliest applications of the Laplacian spectrum for pattern analysis was to use the Fiedler vector <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, i.e. the eigenvector associated with the smallest non-zero eigenvalue, to perform pairwise clustering of data. In addition, several authors have explored the use of the Laplacian and related operators to map data to manifolds in a low dimensional space <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b8">9]</ref>. The mapping is usually found by expressing the kernel as a Gram matrix, i.e. a matrix formed by the self-inner product of a co-ordinate matrix, and the Young-Householder decomposition allows the co-ordinate matrix to be recovered from the eigenvalues and eigenvectors of the kernel matrix. These methods share the feature of using the spectrum of the Laplacian matrix to map data specified in terms of a proximity matrix to a vector space. For instance in the Laplacian eigenmap <ref type="bibr" target="#b17">[18]</ref>, the mapping is determined by the raw Laplacian spectrum. The diffusion map <ref type="bibr" target="#b8">[9]</ref> of Lafon and Coifman constructs the mapping by raising the Laplacian eigensystem to a negative integer power. This mapping is shown to preserve the distances between nodes under a random walk, or diffusion, on the graph.</p><p>The Laplacian eigenvalues <ref type="bibr" target="#b0">[1]</ref> can also be used to characterize graphs for the purposes of clustering. For instance Luo et al. <ref type="bibr" target="#b13">[14]</ref> have shown how to use the Laplacian spectrum to cluster graphs.</p><p>Here the ordered eigenvalues are used as a graph feature-vector, and the order of components is invariant to the permutation order of the nodes. Additional permutation invariants that are available include the trace of the Laplacian and its determinant (which is always zero). In subsequent work <ref type="bibr" target="#b22">[23]</ref>, the same authors showed how to extract a rich family of permutation invariants from a graph by applying symmetric polynomials to the elements of the spectral matrix (the eigenvector matrix premultiplied by the square root of the eigenvalue matrix).</p><p>The aim in this paper is to explore whether the heat kernel can also provide useful permutation invariants for the purposes of characterizing graphs. Since the heat kernel admits the addition of time as a parameter, it allows the scale of different substructures to be probed. Again, the simplest invariants that are to hand are the trace and the determinant (non-zero) of the heat kernel. However, for large graphs, the Laplacian can be viewed as a discrete approximation of the Laplace-Beltrami operator <ref type="bibr" target="#b16">[17]</ref>. As a result, for such graphs invariant characterizations can also be imported from spectral geometry <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Here the heat kernel trace has been used to characterize the differential geometry of manifolds using the spectrum of the Laplace-Beltrami operator. From the paramatrix construction of the heat kernel it follows that the trace of the heat kernel admits an asymptotic expansion. Moreover, the coefficients of this expansion are integrals of universal polynomials in the Riemann curvature tensor and its derivatives. However, these coefficients have only been tabulated for low lying values in the expansion <ref type="bibr" target="#b11">[12]</ref>. The zeta function (i.e. the sum of the eigenvalues raised to a non-integer power), on the other hand also leads to invariants that are more tractable. As we will show later in this paper, from the Mellin transform it follows that the zeta function is the moment generating function of the heat kernel trace. It hence measures the shape of the heat kernel trace as a function of time. For graphs, the derivative of the zeta function (i.e. its slope) at the origin also yields important information. Simple analysis shows that it is given by the logarithm of the reciprocal of the product of the non-zero Laplacian eigenvalues. It is hence linked to the determinant of the Laplacian.</p><p>Another way to characterize the time-dependence of the heat kernel, is to use heat content. The heat content is similar in structure to the heat kernel trace. It is the sum of the complete set of elements of the heat kernel rather than just the sum of its diagonal elements. Gilkey <ref type="bibr" target="#b3">[4]</ref> has discussed the heat-content asymptotic in his recent paper. McDonald and Meyers <ref type="bibr" target="#b4">[5]</ref> establish that heat content distinguishes some well known examples of isospectral graphs. In a recent paper McDonald and Meyers <ref type="bibr" target="#b23">[24]</ref> have shown that the heat content can be expanded purely as a power series in time. The coefficients of the power series are permutation invariants that are linked by Poisson's formula to the eigenvalues and eigenvectors of the graph Laplacian. Moreover, they are more easily computed than the time coefficients in the paramatrix expansion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contribution</head><p>As previously stated, the aim in this paper is to explore whether permutation invariants linked to the heat kernel trace can be used for the purposes of characterizing graph structure for the purposes of clustering. Motivated by the literature above, we explore three approaches to the problem. The first of these is to measure the shape of the heat kernel trace by taking moments with respect to time. As noted above, from the Mellin transform the zeta function is the moment generating function of the heat kernel trace. We construct a feature vector whose components are the values of the zeta function with integer argument. The second approach is to explore the derivative of the zeta function at the origin. Also, as noted above, this is related to the product of the non-zero eigenvalues and is thus one of the polynomial invariants studied by Wilson et al. <ref type="bibr" target="#b22">[23]</ref>. We hence compare the performance of the zeta-function derivative with the trace of the heat kernel and the set of polynomial invariants. Thirdly, we explore the use of heat content. We expand the heat content as a power series in time, and demonstrate that the coefficients can be related to the eigenvalues and eigenvectors of the Laplacian. Experiments with real-world data taken from the COIL and Caltech-Oxford databases reveal that both the zeta function and its derivative at the origin are useful features for clustering graphs. The zeta function is shown to outperform the Laplacian spectrum as a means of characterizing graph structure.</p><p>Although the characterizations used in our study are drawn from the literature on spectral graph theory, we offer a number of novel contributions. First, the representations have not been compared and contrasted before in an experimental setting. Second, our analysis of the heat kernel trace provides links with both the zeta function and the number of spanning trees contained in a graph. We hence use the unified viewpoint furnished by the heat kernel trace to provide links between the different characterizations. This in turn furnishes insights into their meaning and interelationships.</p><p>The outline of this paper is as follows. In Section 2 we explain the relationship between the Laplacian eigensystem and the heat kernel trace. Section 3, we develop the three sets of invariants deduced from graph heat kernel which can be used to characterize the graphs. Section 4 performs a sensitivity study on synthetic graphs, and determines the relative sensitivity of the three methods and compares them to the Laplacian spectrum and symmetric polynomials. In Section 5 we turn our attention to real-world data and evaluate the performance of our new characterizations on a number of object recognition and shape analysis problems. Finally, Section 6 offers some conclusions and directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Heat kernel on graphs</head><p>To commence, suppose that the graph under study is denoted by G=(V, E) where V is the set of nodes and E ⊆ V ×V is the set of edges. Since we wish to adopt a graph spectral approach we introduce the adjacency matrix A for the graph, which has elements</p><formula xml:id="formula_0">A(u, v) = 1 if (u, v) ∈ E 0 otherwise (1)</formula><p>We also construct the diagonal degree matrix D, whose elements are given by D(u, u) = deg u = v∈V A(u, v). From the degree matrix and the adjacency matrix we construct the Laplacian matrix L = D -A, i.e. the degree matrix minus the adjacency matrix. The Laplacian has elements</p><formula xml:id="formula_1">L(u, v) = ⎧ ⎨ ⎩ d v if u = v -1 if u and v are adjacent 0 otherwise (2)</formula><p>The normalized Laplacian is given by L = D -1/2 LD -1/2 and has elements</p><formula xml:id="formula_2">L(u, v) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 i fu = v and d v 0 - 1 d u d v if u and v are adjacent 0 otherwise (3)</formula><p>The spectral decomposition of the normalized Laplacian matrix is L = T , where = diag( 1 , 2 , ... , |V| ) is the diagonal matrix with the ordered eigenvalues (0</p><formula xml:id="formula_3">= 1 &lt; 2 &lt; • • • &lt; |V| )</formula><p>as elements and</p><formula xml:id="formula_4">= ( 1 | 2 | • • • | |V| )</formula><p>is the matrix with the corresponding ordered unit-norm eigenvectors as columns. Since L is symmetric and positive semi-definite, the eigenvalues of the normalized Laplacian are all non-negative. The number of zero eigenvalues is the number of connected components in the graph. For a connected graph, there is only one eigenvalue which equals zero. The eigenvector 2 associated with the smallest non-zero eigenvalue 2 is referred to as the Fiedler-vector <ref type="bibr" target="#b0">[1]</ref>. We are interested in the heat equation associated with the Laplacian, and this is given by</p><formula xml:id="formula_5">jh t jt = -Lh t (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where h t is the heat kernel and t is time. The heat kernel is the fundamental solution of the heat equation. It can be viewed as describing the flow of information across the edges of the graph with time. The rate of flow is determined by the Laplacian of the graph. The solution to the heat equation is</p><formula xml:id="formula_7">h t = e -t L<label>(5)</label></formula><p>We can compute the heat kernel on a graph by exponentiating the Laplacian eigenspectrum (see e.g. <ref type="bibr" target="#b0">[1]</ref>), i.e.</p><formula xml:id="formula_8">h t = exp[-t] T = |V| i=1 exp[-i t] i T i (6)</formula><p>The heat kernel is a |V| × |V| matrix, and for the nodes u and v of the graph G the resulting element is</p><formula xml:id="formula_9">h t (u, v) = |V| i=1 exp[-i t] i (u) i (v)<label>( 7 )</label></formula><p>When t tends to zero, then h t I -Lt, i.e. the kernel depends on the local connectivity structure or topology of the graph. If, on the other hand, t is large, then h t exp[-2 t] 2 T 2 , where 2 is the smallest non-zero eigenvalue and 2 is the associated eigenvector, i.e. the Fiedler vector. Hence, the large time behavior is governed by the global structure of the graph.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Heat kernel invariants</head><p>In this section we will describe a number of invariants that can be computed from the heat kernel matrix of the graph. These include the heat kernel trace, the zeta function, heat-content invariants, and the derivative of zeta function at the origin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Heat kernel trace</head><p>The trace of the heat kernel is the sum of the diagonal elements of the heat kernel matrix of the graph:</p><formula xml:id="formula_10">Z(t) = Tr[h t ] = |V| i=1 exp[-i t] ( 8 )</formula><p>To provide an illustration of the potential utility of the trace formula, in Fig. <ref type="figure" target="#fig_0">1</ref> we show four small graphs with rather different topologies. Fig. <ref type="figure" target="#fig_1">2</ref> shows the trace of the heat kernel as a function of t for the different graphs. From the plot it is clear that the curves have a distinct shape and could form the basis of a useful representation to distinguish graphs. For instance, the more "dumbbell" shaped the graph is the more strongly peaked the trace of the heat kernel at the origin. This is due to the fact that the spectral gap, i.e. the size of 2 , determines the rate of decay of the trace with time, and this in turn is a measure of the degree of separation of the graph into strongly connected subgraphs or "clusters", and this is again due to the fact that for a connected graph, 1 = 0 and as a result</p><formula xml:id="formula_11">Z(t) = 1 + exp[-2 t] + exp[-3 t] + • • • + exp[-N t]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Zeta function and heat kernel trace moments</head><p>The purpose in this section is to use the shape of the heat kernel trace function as a means of characterizing graph structure. Our characterization is found by taking moments of trace function over time.</p><p>To commence our development, we consider the zeta function associated with the Laplacian eigenvalues. The zeta function is defined by</p><formula xml:id="formula_12">(s) = i 0 -s i (9)</formula><p>In other words, it is the result of exponentiating and summing the reciprocal of the non-zero Laplacian eigenvalues. In Fig. <ref type="figure" target="#fig_2">3</ref> we show (s) as a function of s for the four graphs in Fig. <ref type="figure" target="#fig_0">1</ref>. Again the curves are distinct and provide a potential route to distinguish the graphs.</p><p>To establish the link between the zeta function and the trace of the heat kernel we make use of the Mellin transform. The Mellin transform of the function f (t) is</p><formula xml:id="formula_13">w(s) = ∞ 0 t s-1 f (t) dt<label>(10)</label></formula><p>Hence, the heat kernel trace has Mellin transform</p><formula xml:id="formula_14">∞ 0 Tr[h t ]t s-1 dt = ∞ 0 |V| i=1 e -i t t s-1 dt (11)</formula><p>Suppose that the zero eigenvalue 1 =0 has multiplicity C. Removing this eigenvalue from the sum under the integral, we have</p><formula xml:id="formula_15">∞ 0 Tr[h t ]t s-1 dt = ∞ 0 ⎡ ⎣ C + i 0 e -i t ⎤ ⎦ t s-1 dt</formula><p>To develop this expression for the heat kernel trace, we note that the gamma function (s) is defined to be</p><formula xml:id="formula_16">(s) = ∞ 0 t s-1 e -t dt<label>(12)</label></formula><p>As a result</p><formula xml:id="formula_17">∞ 0 t s-1 e -i t dt = (s) (-i ) s</formula><p>Hence, we can write</p><formula xml:id="formula_18">∞ 0 (Tr[h t ] -C)t s-1 dt = i 0 (s) (-i ) s</formula><p>So finally, the zeta function is given by</p><formula xml:id="formula_19">(s) = i 0 (-i ) -s = 1 (s) ∞ 0 (Tr[h t ] -C)t s-1 dt<label>(13)</label></formula><p>In other words, the zeta function is related to the Mellin moments of the heat kernel trace. It is hence a way to characterizing the shape of the heat kernel trace with time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Zeta-function derivative</head><p>The zeta function is also linked to the determinant of the Laplacian. To show this, we re-write the zeta function in terms of the natural exponential with the result</p><formula xml:id="formula_20">(s) = i 0 -s i = i 0 exp[-s ln i ] (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>The derivative of the zeta function is given by</p><formula xml:id="formula_22">(s) = i 0 {-ln i } exp[-s ln i ]<label>(15)</label></formula><p>At the origin the derivative takes on the value</p><formula xml:id="formula_23">(0) = i 0 {-ln i } = ln ⎧ ⎨ ⎩ i 0 1 i ⎫ ⎬ ⎭<label>(16)</label></formula><p>It is interesting to note that McKay <ref type="bibr" target="#b43">[46]</ref> has shown that the derivative of the zeta function at the origin is linked to the number of spanning trees in a graph G through</p><formula xml:id="formula_24">(G) = u∈V deg u u∈V deg u exp[-(0)]<label>(17)</label></formula><p>As a result, the derivative of the zeta function at the origin is determined by the number of spanning trees in the graph together with the degrees of its vertices. In our experiments, we will use the value of (0) as a graph characteristic, and explore whether it can be used to distinguish different graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Unary attributes with symmetric polynomials</head><p>In this section we illustrate the relationship between the derivative of the zeta function at the origin and the symmetric polynomial invariants of Wilson et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>In their recent paper Wilson et al. <ref type="bibr" target="#b22">[23]</ref> have reported a family of permutation invariants that can be computed by applying symmetric polynomials to the elements of the spectral matrix √ . A symmetric polynomial of n variables v 1 , v 2 , ... , v n is a function that is unchanged by permutation of its variables. In other words, the symmetric polynomials satisfy</p><formula xml:id="formula_25">f (y 1 , y 2 , ... , y n ) = f (v 1 , v 2 , ... , v n ) (<label>18</label></formula><formula xml:id="formula_26">)</formula><p>where</p><formula xml:id="formula_27">y i = v (i)</formula><p>and is an arbitrary permutation of the indices 1, 2, . . . , n.</p><p>There are two sets of commonly used symmetric polynomials. These are the elementary symmetric polynomials and the powersum symmetric polynomials. For a set of variables {v 1 , v 2 , ... , v n } the elementary symmetric polynomials are defined as</p><formula xml:id="formula_28">S 1 (v 1 , ... , v n ) = n i=1 v i S 2 (v 1 , ... , v n ) = n i=1 n j=i+1 v i v j . . . S r (v 1 , ... , v n ) = i 1 &lt;i 2 &lt;•••&lt;ir v i 1 v i 2 ... v ir . . . S n (v 1 , ... , v n ) = n i=1 v i</formula><p>The output for the n set input variable {v 1 , v 2 , ... , v n } is S 1 , S 2 , ... , S n .</p><p>The power-sum symmetric polynomial functions, on the other hand, are defined to be</p><formula xml:id="formula_29">P 1 (v 1 , ... , v n ) = n i=1 v i P 2 (v 1 , ... , v n ) = n i=1 v 2 i . . . P r (v 1 , ... , v n ) = n i=1 v r i . . . P n (v 1 , ... , v n ) = n i=1 v n i</formula><p>and also form a basis set over the set of symmetric polynomials. Any polynomial function which is invariant to permutation of the variable indices (v i , v 2 , ... , v n ) can be expressed in terms of one of these two sets of polynomials. Moreover, the two sets of polynomials are related to one another by the Newton-Girard formula:</p><formula xml:id="formula_30">S r = (-1) r+1 r r k=1 (-1) k+r P k S r-k (<label>19</label></formula><formula xml:id="formula_31">)</formula><p>where we have used the shorthand S r for S r (v 1 , ... , v n ) and P r for P r (v 1 , ... , v n ). As a consequence, the elementary symmetric polynomials S r can be efficiently computed using the power-sum symmetric polynomials. In this paper we will compare our characterizations derived from the heat kernel to the symmetric polynomials of the spectral matrix used by Wilson et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>There are of course alternative invariants that can be computed from the spectrum of the Laplacian. The most obvious of these is the sum of eigenvalues or the trace of the normalized Laplacian eigenvalue matrix of the graph:</p><formula xml:id="formula_32">S 1 ( 1 , 2 , ... , n ) = n i=1 i (20)</formula><p>When the symmetric polynomials take the normalized Laplacian eigenvalues as inputs, then the lowest order polynomial is the sum of the eigenvalues, i.e. the trace of the normalized Laplacian, and the highest order polynomial is the product of non-zero normalized Laplacian eigenvalues. Hence,</p><formula xml:id="formula_33">S |V| ( 1 , 2 , ... , |V| )=exp[( (0)) -1 ]</formula><p>where |V| is the number of nodes in the graph. Moreover, in <ref type="bibr" target="#b22">[23]</ref>, Wilson et al. found it necessary to perform logarithmic squashing of the polynomials to bring them into a convenient dynamic range. This property arises naturally through the analysis of the zeta function.</p><p>In our experiments, we will use the symmetric polynomial to construct unary attributes for graph characterization. The inputs for the symmetric polynomial are the columns of the spectral matrix and the eigenvalues of the normalized Laplacian matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Heat-content invariants</head><p>We now turn our attention to heat-content invariants. In a recent paper McDonald and Meyers <ref type="bibr" target="#b23">[24]</ref> have described a set of differential invariants that can be derived from the heat content of the heat kernel. The heat content is the sum of the entries of the heat kernel over the nodes of the graph and is given by</p><formula xml:id="formula_34">Q(t) = u∈V v∈V h t (u, v) = u∈V v∈V |V| k=1 exp[-k t] k (u) k (v)<label>(21)</label></formula><p>In Fig. <ref type="figure" target="#fig_3">4</ref>, we show the heat content varies with t for the four graphs from Fig. <ref type="figure" target="#fig_0">1</ref>. Yet again, the curves have different shapes which depend on whether the graph is "dumbbell" or not. As result heat content can be used as a means for graph characterization.</p><p>McDonald and Meyers <ref type="bibr" target="#b23">[24]</ref> have shown that the heat content can be expanded as a power series in time, i.e.</p><formula xml:id="formula_35">Q(t) = ∞ m=0 q m t m (22)</formula><p>Using the MacLaurin series exp[k t] can be expanded as</p><formula xml:id="formula_36">exp[-k t] = 1 + (-k )t + (-k ) 2 t 2 2! + • • • + (-k ) m t m m! + • • • = ∞ m=0 (-k ) m t m m! (23)</formula><p>Substituting the MacLaurin series with Eq. ( <ref type="formula" target="#formula_34">21</ref>), we have</p><formula xml:id="formula_37">Q(t) = u∈V v∈V |V| k=1 exp[-k t] k (u) k (v) = ∞ m=0 u∈V v∈V |V| k=1 k (u) k (v) (-k ) m t m m! (<label>24</label></formula><formula xml:id="formula_38">)</formula><p>As a result the coefficients q m appearing in the power series expansion of Q(t) are given by</p><formula xml:id="formula_39">q m = |V| k=1 ⎧ ⎨ ⎩ u∈V k (u) 2 ⎫ ⎬ ⎭ (-k ) m m! (25)</formula><p>As a result, the time-series coefficient q m does not depend on the permutation order of the nodes of the graph. Moreover, it is similar in spirit to the permutation invariants described by Wilson et al. in <ref type="bibr" target="#b22">[23]</ref>, since it involves both eigenvalues and eigenvectors of the normalized Laplacian. The time-series coefficients q m are a set of invariants which can be used for graph characterization. Since they depend on both eigenvalues and eigenvectors, they are more likely to be unique than invariants purely based in eigenvalues which are likely to be affected by problems of co-spectrality <ref type="bibr" target="#b29">[31]</ref>. We will explore the use of the time-series coefficients for the purpose of graph clustering. To do this  we construct a feature vector → B =(q 1 , q 2 , ... , q k ) T from the k leading coefficients of the heat-content power series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Comparing the heat kernel invariants on synthetic data</head><p>Before we apply our new methods for characterizing graphs to real world image databases, we first quantitatively investigate their behavior on synthetic data. This study is divided into two parts. We commence by comparing the three new methods with the symmetric polynomial representation, and then evaluate their stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Graph characterization</head><p>In the first experiment, we test our methods for graph characterization on synthetic graphs. To this end, we generate five different seed graphs with different structure, i.e. number of nodes and number of edges. We then apply random edit operations to the five seed graphs to simulate the effects of noise. The edit operations are edge deletion, edge insertion and node deletion. The purpose of these operations is to generate five structurally related graph clusters, which we would hope to separate using our methods. In Fig. <ref type="figure" target="#fig_4">5</ref>, we show the original seed graphs. For each seed graph, we randomly perform 3-10 edit operations to obtain 70 graph variants.  We commence by illustrating the behavior of the zeta function. From left-to-right and top-to-bottom in Fig. <ref type="figure">6</ref> we show the values of (1), ( <ref type="formula">2</ref>), ( <ref type="formula" target="#formula_42">3</ref>) and (4) for the 70 randomizations of each seed graph. In each plot the different curves are for the different seed graphs, and the x-axis is the index number of the randomization. The main feature to note is that the curves for the different graph clusters are well separated, and that the individual values of the zeta function do not vary significantly with randomization index. When s = 1 and 2, the fluctuations (s) shown in Fig. <ref type="figure">6</ref> are generally small. However, the fluctuation is larger when s = 4. This is because when s becomes large the small eigenvalues will play a dominant role in the function.</p><p>We now turn our attention to the derivative of the zeta function at the origin. In Fig. <ref type="figure" target="#fig_5">7</ref> we plot the curves (0) for the five random graph clusters against randomization index. The curves are well separated for the different objects, and the fluctuations are relatively small.</p><p>In Fig. <ref type="figure">8</ref>, we plot the four leading heat-content invariants coefficients q 1 , q 2 , q 3 and q 4 separately as a function of the randomization index for the five seed graphs. The coefficients are relatively stable under randomized edit operations.</p><p>For the purposes of comparison, in Fig. <ref type="figure">9</ref> we plot the mean values of the symmetric polynomials for the five graph clusters, as a function of polynomial order. Here the arguments of the polynomials are the column elements of the spectral matrix √ s . The different curves in the plot are for the different seed graphs. We select nine different symmetric polynomials for investigation. These are the first From Figs. <ref type="figure" target="#fig_0">10</ref> and<ref type="figure">9</ref> it is clear that the first three and last three symmetric polynomials computed using the normalized Laplacian eigenvalues and spectral matrix elements as inputs can be used for graph clustering. We also get a better separation of objects when using the polynomials from the normalized Laplacian spectrum rather than the elements of the spectral matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stability evaluation</head><p>We now evaluate the stability of the different graph characterization methods. We study six different methods. These are (a) the derivative of zeta function at origin, (b) the heat content invariants, (c) the zeta-function values, (d) the symmetric polynomials with Laplacian eigenvalues as arguments, (e) the symmetric polynomials with spectral matrix elements as arguments and (f) the Laplacian spectrum. We generate a random seed graph with 65 vertices and 170 edges. Modified graph variants are obtained by performing edit operations on the seed graphs. Here we use four different edit operations, namely edge deletion, edge insertion, node deletion and node insertion. We compute the Euclidean distance between the graph feature vector for the original seed graph and the feature vector for its modified variant obtained after the application of the edit operations. For the heat-content invariants, the zeta function values and the Laplacian spectrum, we select the leading six values to construct the required feature vector. For the symmetric polynomials we select the first three and last three values. As a result the feature vectors each have six components. The exception is the derivative of zeta function at origin, which is unary in nature. By applying the four different edit operations to the seed graph, we obtain four different modified graph variants. Each edit operation is applied between 3 and 30 times to the seed graph. The nodes and edges to be edited are selected at random. For each operation, we perform 30 trials in order to obtain the average distance together with its standard deviation.</p><p>In Fig. <ref type="figure" target="#fig_7">11</ref>, from left-to-right and top-to-bottom, we show the effects of edge deletion, edge insertion, node deletion and node insertion. The x-axis in each plot shows the number of edit operations, while the y-axis shows the mean Euclidean distance between the feature vector of the original seed graph and its modified variant after application of the relevant edit operation. In the plots, the different colored lines represent the six different graph characterization methods. The plots show that the five new graph characterizations proposed in this paper result in an approximately linear relationship between the Euclidean distance and the number of edit operations. The Laplacian spectrum, on the other hand, yields a behavior that is less linear.</p><p>In Fig. <ref type="figure" target="#fig_1">12</ref>, we show the relative deviation, i.e. the standard deviation divided by the mean, for the six graph characterizations under the four different edit operations. This quantity gives an indication of the stability of the different graph characterization methods. From Fig. <ref type="figure" target="#fig_1">12</ref>, the zeta-function values, the heat-content invariants and the derivative of zeta function at the origin each give a relatively small relative deviation in all plots. However, it is the heatcontent invariants that give the best performance. Moreover, the derivative of the zeta function at the origin outperforms the Laplacian spectrum. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments on real-world data</head><p>In this section, we will apply our methods to real-world image databases. We will explore whether the heat kernel invariants can be used for object recognition. We use three databases with different methods for graph extraction. The first database is COIL-100 <ref type="bibr" target="#b25">[26]</ref> which contain different views of a number of 3D objects. The second database is Caltech 256 <ref type="bibr" target="#b33">[36]</ref>. The COIL database consists of views of objects against a blank background, while Caltech 256 contains images of objects against a complex background. In the third experiment, we will apply our methods to shape analysis. Here we use a shock tree database which composed of 150 silhouettes of 10 different types of object <ref type="bibr" target="#b30">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on the COIL database</head><p>We use the COIL 100 database, which consists of a series of 2D views of 3D objects collected at 72 equally spaced viewing directions on a great circle of the object view-sphere. From each image in each view sequence, we extract corner features <ref type="bibr" target="#b20">[21]</ref>. We use the detected feature points as nodes of a graph. The edge relations are obtained by computing the Voronoi tessellations of the feature points, and constructing the region adjacency graph, i.e. the Delaunay graph, of the Voronoi regions. This process is referred to as Delaunay triangulation. An example view can be seen in Fig. <ref type="figure" target="#fig_8">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Stability of the characterizations on the coil database</head><p>We first choose eight object from the COIL database and explore the stability of the different characterizations with view number. In Fig. <ref type="figure" target="#fig_8">13</ref>, superimposed on the images are the detected feature points and their associated Delaunay graphs.</p><p>From left-to-right and top-to-bottom in Fig. <ref type="figure" target="#fig_9">14</ref> we show the values of ( <ref type="formula">1</ref>), ( <ref type="formula">2</ref>), ( <ref type="formula" target="#formula_42">3</ref>) and ( <ref type="formula" target="#formula_5">4</ref>) as a function of view number for the eight objects. The different curves in the four plots correspond to the different objects. The main feature to note is that the curves for the different objects are well separated, and that the individual values  of the zeta function do not vary significantly with view number. For small integer input, i.e. s = 1 and 2, the fluctuations in the outputs in Fig. <ref type="figure" target="#fig_9">14</ref> are generally small. However, the fluctuation is large when s = 4. This is because when s becomes large the small eigenvalues will play a dominant role in the function.</p><p>We now turn our attention to the derivative of the zeta function at the origin. In Fig. <ref type="figure" target="#fig_4">15</ref> we show the curves (0) as a function of view number for the eight objects from COIL database, respectively. The curves are well separated for the different objects, and the fluctuations are relatively small. In fact in the case of the COIL data the derivative of the zeta function separates the objects better than the individual zeta-function values.</p><p>Next we explore the relative stability of the different polynomial characterizations. In Fig. <ref type="figure" target="#fig_0">16</ref> we plot the mean values from the symmetric polynomials over the set of object views as a function of their order. Here the arguments of the polynomials are the column elements of the spectral matrix √ . The different curves in the plot are for different COIL objects. As with the experiments with synthetic graphs, we use nine different symmetric polynomials. These are the first three symmetric polynomials S 1 , S 2 , S 3 , the middle three symmetric polynomials S fix(N/2)-1 , S fix(N/2) , S fix(N/2)+1 and last three symmetric polynomials S N-2 , S N-1 , S N , where N is the number of node in the graph. Again, the error bars show the standard deviation over the different sets of views of the same object. Fig. <ref type="figure" target="#fig_0">10</ref> repeats the analysis using the normalized Laplacian spectrum as the arguments of the polynomials.</p><p>Figs. <ref type="bibr" target="#b15">16</ref> and 17 support our findings on synthetic data. It is clear that the first three and last three symmetric polynomials computed using the normalized Laplacian eigenvalues and spectral matrix elements as inputs can be used for object separation. We also get a better separation of objects when using the polynomials from the normalized Laplacian spectrum rather than the elements of the spectral matrix.</p><p>We now turn our attention to the invariants computed from the heat content characterization described in Section 3.5. In Fig. <ref type="figure" target="#fig_12">18</ref>, we plot the four leading heat-content invariants co-efficients q 1 , q 2 , q 3 and q 4 separately as a function of the view number for the eight objects selected from the COIL database. The co-efficients are relatively stable with viewpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Clustering</head><p>Having established the stability of the different characterizations under variation in viewing direction, we now explore whether thay can be used to cluster the graphs for objects of the same class. To this end, in Fig. <ref type="figure" target="#fig_13">19</ref>, we show the result of performing principal component analysis (PCA) on the feature vectors of permutation invariants. In Fig. <ref type="figure" target="#fig_13">19</ref>, from left to right, we show the result with the leading zeta-function values, the symmetric polynomials with Laplacian eigenvalues, and the heat-content invariants. The different objects are denoted by points of a different color. For comparison, we repeat the experiments for symmetric polynomials with spectral matrix elements and Laplacian eigenvalues. The results are given in Fig. <ref type="figure" target="#fig_14">20</ref>. For this unsupervised process, we have computed the Rand index <ref type="bibr" target="#b24">[25]</ref> for the different methods. The Rand index is defined as</p><formula xml:id="formula_40">R I = C C + W (<label>26</label></formula><formula xml:id="formula_41">)</formula><p>where C is the number of agreements and W is the number of disagreements in cluster assignment. If two objects are in the same class (i.e. ground truth cluster) and belong to the same experimentally extracted cluster, then this counts as an agreement. If, on the other hand, two objects are in the same class, but appear in different experimentally extracted clusters, then this counts as a disagreement. The index is hence the fraction of views of a particular object that    are closer to an object of the same class than to one of a different class. The Rand index takes a value in the interval [0,1], where 1 corresponds to a perfect clustering. We use the K-nearest neighbor algorithm to locate clusters. If over half of the neighbors belong to the same category as the ground truth object class, the corresponding graph is correctly clustered. In our experiment, we set K to 5. The Rand index for zeta function characterization clustering is 0.96. For the symmetric polynomial clustering we use the first and last three symmetric polynomial values S 1 , S 2 , S 3 , S N-2 , S N-1 , S N as feature vector for graph clustering. The Rand index value for the symmetric polynomial using Laplacian eigenvalues as inputs is 0.93 and the Rand index value for the symmetric polynomial using spectral matrix elements as inputs is 0.89. The Rand index for the heat-content coefficients clustering is 0.98, for zeta-function values is 0.96 and for the Laplacian eigenvalues it is 0.78. This experiment shows that the heat kernel invariants proposed in the paper can be used as a means of graph clustering.</p><p>We proceed to experiment with our different characterizations on the entire COIL 100 database. Here we use supervised learning methods. For each class of object, we select 30 graphs randomly from the relevant class as training data. For comparability, we construct the feature vectors of the same length (10 in our experiments) from the different graph characterizations. We have used several standard supervised learning algorithms, generative mixture models (GMM) <ref type="bibr" target="#b34">[37]</ref>, support vector machines (SVM) <ref type="bibr" target="#b35">[38]</ref> and linear discriminant analysis (LDA) <ref type="bibr" target="#b31">[34]</ref>. In Table <ref type="table" target="#tab_3">1</ref>, we give the classification accuracy obtained with different graph characterization methods and different learning methods. The heat kernel invariants give the best overall performance, irrespective of the learning algorithm used. The feature vector of zeta-function values gives performance that is comparable to the symmetric polynomials. Overall the best results are obtained with the heat-content invariants with the GMM classification method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on the Caltech 256 database</head><p>In this section, we apply our methods to the Caltech 256 database <ref type="bibr" target="#b33">[36]</ref>. As noted above, when compared with the COIL database the Caltech database is considerably more complex. The database contains images of different examples of the same kind of object. The images are taken from different view points and contain different backgrounds corresponding to different environments. The corresponding graphs thus contain occlusion and clutter. Some example views of some selected objects are shown in Fig. <ref type="figure" target="#fig_1">21</ref>. For this database, we extract relational graphs in which the nodes denote primitive image features and the links denote Gestalt relationships between the image features. We use line patterns as our primitives (i.e. nodes) <ref type="bibr" target="#b27">[29]</ref>. We use Gestalt relationships <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b28">30]</ref> that reflect human perceptual organization between line primitives. For each pair of lines we compute measures of proximity, continuity and parallelism. We then combine these Gestalt relations to generate a the relational graph along the lines suggested in <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b21">22]</ref>. For two line segments e i and e j , the proximity measure E Proximity (e i , e j ) is defined as E Proximity (e i , e j ) = 1min 1, d gap min(l i , l j ) where l i is the length of the line segment e i and d gap is the minimum distance between the end points of two line segments. The quantity E Continuity (e i , e j ) is defined as</p><formula xml:id="formula_43">E Continuity (e i , e j ) = min l i + l j l i + l j + d gap , cos 2 ( )</formula><p>The parallelism between e i and e j is determined by difference in length between the constituting edge segments, l o , the length of the symmetry axis, l sym , the average width, w , and variation, w , of the parallel strip, and the minimum fit error, Err sym , of a constant curvature segment to the symmetry axis <ref type="bibr" target="#b37">[40]</ref>:</p><formula xml:id="formula_44">E Parallelism (e i , e j ) = min w w + w , l sym l sym + Err sym , l sym l sym + l o</formula><p>The final weight w(e i , e j ) between two line segments e i and e j is given by w(e i , e j ) = E Proximity (e i , e j ) × (E Continuity (e i , e j ) + E Parallelism (e i , e j )) as suggested by Mohan and Nevatia in <ref type="bibr" target="#b37">[40]</ref>.</p><p>With the relational graph representation in hand, we perform object recognition with the Caltech 256 database. In Fig. <ref type="figure" target="#fig_15">22</ref>, we show the performance rate as a function of number of classes. As before, in this experiment we randomly select 30 graphs from each class for training, and then use the Gaussian mixture model (GMM) as our learning method. Fig. <ref type="figure" target="#fig_15">22</ref> shows that with a suitable relational graph representation in hand, then our graph characterization methods can deliver reasonable classification results when compared with alternative methods <ref type="bibr" target="#b33">[36]</ref>. The classification rate is high (over 80%) with no more than 20 objects classes. The performance drops when more object classes are added. This can be alleviated by incorporating more sophisticated learning methods i.e. semi-supervised GMM <ref type="bibr" target="#b40">[43]</ref> into our future work. Again, the heat kernel invariants give thre best overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Applications on shape analysis</head><p>We have also explored the application of our new graph characterization methods to shape analysis. Here graph based methods have provided a powerful means of characterizing the underlying variations in shape due to deformation, articulation or natural class variability <ref type="bibr" target="#b38">[41]</ref>. We have applied our methods to shock trees. This is a tree structure based representation of the differential structure of the boundary of a 2D shape. It is obtained by locating the shape skeleton, and examining the differential behavior of the radius of the bitangent circle from the skeleton to the object boundary, as the skeleton is traversed <ref type="bibr" target="#b39">[42]</ref>. Recently, the shock tree extraction process has been further improved by Torsello and Hancock <ref type="bibr" target="#b30">[32]</ref>. Firstly, they have shown how to compute the skeleton more reliably in the proximity of high curvature detail <ref type="bibr" target="#b41">[44]</ref>. Secondly, they have shown   how to determine the similarity of skeletal branches using a simple measure computed from the distance map from the skeleton to the boundary <ref type="bibr" target="#b42">[45]</ref>.</p><p>In this paper we use the methods described in <ref type="bibr" target="#b30">[32]</ref> to extract a tree representations from the silhouettes of 2D shape. A representative view of each object studied is shown in Fig. <ref type="figure" target="#fig_16">23</ref>.</p><p>In the database, there are over 15 shapes from 10 classes. For each shape we can construct a tree representation of the skeleton. Using the Laplacian spectrum of the tree, we compute the different invariants described earlier. A GMM (Gaussian mixture model) is applied to the feature vectors constructed from the invariants for the purposes of classification. In Fig. <ref type="figure" target="#fig_17">24</ref>, we plot the proportion of shapes from the database correctly classified as the number of shape classes is increased. All five methods explored decrease in classification accuracy as the size of the database increases. The best performance is given by the heat-content invariants, closely followed by the feature vector of zeta-function values. The poorest performance is given by the spectrum of Laplacian eigenvalues.</p><p>We pursue the analysis further in Fig. <ref type="figure" target="#fig_18">25</ref>. Here we show scatter plots of the Euclidean distances between feature vectors for the permuation invariants versus tree edit distance. Here the shock tree edit distance is computed using the method described in <ref type="bibr" target="#b30">[32]</ref>. In Fig. <ref type="figure" target="#fig_18">25</ref> the five scatter plots, from left to right, are for the zeta-function values, the heat-content invariants, the symmetric polynomials with Laplacian eigenvalues, the symmetric polynomials with spectral matrix elements, and, the Laplacian spectrum.</p><p>There is a clear regression trend for the first three of these, i.e. the zeta-function values, the heat-content invariants and the symmetric polynomials with Laplacian eigenvalues. For these the invariant features can proxy for the computation of tree edit distance, and this offers an advantage since no node correspondences are needed. However, for the remaining two there is no such trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we have explored the use of several invariants computed from the heat kernel to characterize graphs. First we have explored the use of the zeta function as a means of characterizing the shape of the heat kernel trace. Using the Mellin transform, the zeta function is linked to the moment generating function of the heat kernel trace. We have shown that the derivative of the zeta function at the origin is related to the determinant of the Laplacian matrix. The slope of the zeta function at the origin is related to the product of the non-zero Laplacian eigenvalues, and is hence an invariant to the permutation of the node order of the graph. We have further investigated alternative unary permutation invariants computed by applying symmetric polynomials to the spectral matrix. Finally, we have shown how the use of heat content can lead to a series of invariants. In common with the symmetric polynomials, the heatcontent coefficients are permutation invariants that depend on both the eigenvalues and eigenvectors of the Laplacian matrix.</p><p>We have experimented with the three newly proposed graph characterizations on synthetic and real-world databases. The main conclusions from this study are as follows. First, the most effective characterization is the set of heat-content invariants. Second, each of the three new characterizations outperforms the symmetric polynomials. The poorest performance is achieved with the Laplacian spectrum. The derivative of the zeta function at the origin, which is a unary attribute gives similar performance to the symmetric polynomials. We have applied our characterizations to object recognition. By using existing learning algorithms i.e. GMM (Gaussain mixture model), we have developed a technique which is comparable in performance to existing methods reported in the literature.</p><p>There are a number of ways in which the work reported in this paper can be extended.</p><p>First, we aim to explore more depth the links with spectral geometry, in particular the Selberg zeta function <ref type="bibr" target="#b1">[2]</ref>. A good place to start is with the work of Stark and Terras <ref type="bibr" target="#b2">[3]</ref>. A second line of investigation will be to explore in more depth the link between the heat kernel invariants and symmetric polynomials. Finally, we intend to explore different application domains. For instance, we will explore whether the invariants described in this paper can be used to characterize molecular structures and small world networks <ref type="bibr" target="#b32">[35]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Four graphs used for the heat kernel trace analysis.</figDesc><graphic coords="3,320.14,61.53,234.00,185.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Heat kernel trace as a function of t for the four graphs from Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Zeta function as a function of the variable s for the four graphs from Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Heat content as a function of t for the four graphs from Fig. 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Examples of the seed graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Derivative of the zeta function at the origin for five clusters of random graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Symmetric polynomials using spectral matrix elements for five clusters of random graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Euclidean distances from the different graph characterization methods for random graphs (from left to right, and top to bottom, we show edge deletion, edge insertion, node deletion and node insertion, respectively). (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Eight objects with their Delaunay graphs overlaid (from left to right, and top to bottom, the corresponding object class are denoted as object1, object2,. . .,object8 as shown in Figs 19 and 20).</figDesc><graphic coords="11,131.60,63.89,341.38,156.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Zeta function (s) varying with view number (from left to right, and top to bottom, s = 1, 2, 3 and 4, respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 15 .Fig. 16 .</head><label>1516</label><figDesc>Fig. 15. Derivative of the zeta function at the origin for the COIL database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 17 .</head><label>17</label><figDesc>Fig. 17. Symmetric polynomials using normalized Laplacian eigenvalues for the COIL database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 18 .</head><label>18</label><figDesc>Fig.18. Individual heat-content invariants as a function of view number.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FSFig. 19 .</head><label>19</label><figDesc>Fig. 19. Principal component analysis results for the heat kernel invariants for the COIL database: zeta-function values (left), symmetric polynomials using normalized Laplacian eigenvalues (middle) and heat-content invariants (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 20 .</head><label>20</label><figDesc>Fig. 20. Principal component analysis results for the traditional spectral methods on the COIL database: symmetric polynomials using spectral matrix elements (left) and Laplacian spectrum (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 22 .</head><label>22</label><figDesc>Fig. 22. Experimental results on the Caltech 256 database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 23 .</head><label>23</label><figDesc>Fig. 23. Sample views of shape silhouettes.</figDesc><graphic coords="16,112.60,61.15,360.00,98.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 24 .</head><label>24</label><figDesc>Fig. 24. Shape silhouette classification performance as a function of the number of silhouettes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 25 .</head><label>25</label><figDesc>Fig. 25. Scatter plot of the feature vector distance for the different characterizations as a function of edit distance (y-axis)-from left to right, first (zeta-function values), second (heat-content invariants), third (symmetric polynomials with Laplacian eigenvalues), fourth (symmetric polynomials with spectral matrix) and the fifth (the Laplacian spectrum).</figDesc><graphic coords="16,51.92,445.97,483.52,91.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>Classification performance for the COIL 100 database. Fig.21. Some example objects selected from the Caltech 256 database.</figDesc><table><row><cell></cell><cell>Zeta-function</cell><cell>Heat-content</cell><cell>Symmetric polynomials with</cell><cell>Symmetric polynomials with</cell><cell>Tradition Laplacian spec-</cell></row><row><cell></cell><cell>values (%)</cell><cell>invariants (%)</cell><cell>Laplacian eigenvalues (%)</cell><cell>spectral matrix (%)</cell><cell>trum (%)</cell></row><row><cell>L D A</cell><cell>9 3</cell><cell>9 7</cell><cell>9 4</cell><cell>9 0</cell><cell>8 3</cell></row><row><cell>G M M</cell><cell>9 4</cell><cell>9 7</cell><cell>9 4</cell><cell>9 1</cell><cell>8 4</cell></row><row><cell>S V M</cell><cell>9 2</cell><cell>9 6</cell><cell>9 4</cell><cell>9 1</cell><cell>8 3</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>His current research interests include graph spectral analysis, image segmentation and recognition, statistical pattern recognition, and feature extraction. He has published more than 20 papers in journals and refreed conferences. He has been a programme committee member for many national and international conferences.</p><p>About the Author-Edwin R. Hancock studied physics as an undergraduate at the University of Durham and graduated with honors in 1977. He remained at Durham to complete the Ph.D. degree in the area of high-energy physics in 1981. Following this, he worked for 10 years as a researcher in the fields of high-energy nuclear physics and pattern recognition at the Rutherford-Appleton Laboratory (now the Central Research Laboratory of the Research Councils). In 1991, he moved to the University of York as a lecturer in the Department of Computer Science. He was promoted to senior lecturer in 1997 and to reader in 1998. In 1998, he was appointed to a chair in computer vision. Professor Hancock now leads a group of some 15 faculty, research staff, and Ph.D. students working in the areas of computer vision and pattern recognition. His main research interests are in the use of optimization and probabilistic methods for high and intermediate level vision. He is also interested in the methodology of structural and statistical pattern recognition. He is currently working on graph-matching, shape-from-X, image databases, and statistical learning theory. <ref type="bibr">He</ref>  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Spectral Graph Theory</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R K</forename><surname>Chung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>American Mathematical Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Hejhal</surname></persName>
		</author>
		<title level="m">The Selberg Trace Formula for PSL</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983">1983</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zeta functions of finite graphs and coverings</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Terras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Math</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="126" to="165" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Gilkey</surname></persName>
		</author>
		<title level="m">The Spectral Geometry of Operators of Dirac and Laplace Type</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Krupka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Saudners</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="287" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Isospectral polygons, planar graphs and heat content</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Amer. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="3589" to="3599" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Anal. Mach. Intelli</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A spectral algorithm for seriation and the consecutive ones problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="297" to="310" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Indexing using a spectral encoding of topological structure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shokoufandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="491" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diffusion maps. Appl. Comput. Harmonic Anal</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Diffusion kernels on statistical manifolds</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lebanon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="129" to="163" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Schoen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Differential Geometry, Science Publication</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Invariance Theory, the Heat Equation, and the Atiyah-Singer Index Theorem</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gilkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Perish Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral embedding of graphs</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="2213" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Laplace eigenvalues of graphs-a Survey</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mohar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Math</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="171" to="183" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random walks on graphs: a survey</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Paul Erds is eighty</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="353" to="397" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
	<note>Combinatorics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">From graphs to manifolds-Weak and strong pointwise consistency of graph Laplacian</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th Annual Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="470" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A global geometric framework for non-linear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="586" to="591" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<idno>NIPS03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Stephens</surname></persName>
		</author>
		<title level="m">Fourth Alvey Vision Conference</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="147" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Quantitative measures of change based on feature organization: eigenvalues and eigenvectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="page" from="110" to="136" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pattern vectors from algebraic graph theory</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2220" to="2237" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diffusions on graphs, poisson problems and spectral geometry</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Meyers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Am. Math. Soc</title>
		<imprint>
			<biblScope unit="volume">354</biblScope>
			<biblScope unit="page" from="5111" to="5136" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Objective criteria for the evaluation of clustering</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Rand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="846" to="850" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Nene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Murase</surname></persName>
		</author>
		<title level="m">Columbia object image library</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>COIL 100), Columbia University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Line pattern retrieval using relational histograms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Huet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1363" to="1370" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Perceptual organization in computer vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybernet</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="382" to="399" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A matrix representation of graphs and its spectrum as a graph invariant</title>
		<author>
			<persName><forename type="first">D</forename><surname>Emms</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Severini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. J. Combin</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="140" to="154" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Discovering shape classes using tree edit-distance and pairwise clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Robles-Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="259" to="285" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley Intersceince</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Molecular dynamics investigation of the surface stress distribution in a Si/Ge quantum dot superlattice</title>
		<author>
			<persName><forename type="first">I</forename><surname>Duraka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-L</forename><surname>Barabasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Lomdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Phys. Rev</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="2150" to="2153" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Caltech-256 Object Category Dataset</title>
		<author>
			<persName><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised learning of finite mixture models</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A F</forename><surname>Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="381" to="396" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<title level="m">Statistical Learning Theory</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Koffka</surname></persName>
		</author>
		<title level="m">Principles of Gestalt Psychology</title>
		<meeting><address><addrLine>Harcourt, New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Erceptual organization for scene segmentation and description</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="616" to="635" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shapes, shocks, and deforamtions</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Kimia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="189" to="224" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A spectral algorithm for learning mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="841" to="860" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Correcting curvature density effects in the Hamilton-Jacobi skeleton</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="877" to="891" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A skeletal measure of 2D shape similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torsello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vision Image Understanding</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spanning trees in regular graphs</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Combin</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="149" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Object class recognition by unsupervised scale-invariant learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Vision Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="264" to="271" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Matching shape graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Leuken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proceedings in Applied Mathematics and Mechanics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Bayesian hierarchical model for learning natural scene categories</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Vision Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="531" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Shape from texture without boundaries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lobay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="91" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
