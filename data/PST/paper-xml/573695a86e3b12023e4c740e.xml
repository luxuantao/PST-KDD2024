<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The trust region subproblem and semidefinite programming</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2014-12-23">23 December 2014</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Charles</forename><surname>Fortin</surname></persName>
							<email>fortin@math.mcgill.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<postCode>H3A 2T5</postCode>
									<settlement>Montréal</settlement>
									<region>Québec</region>
									<country>Canada E-mail</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<postCode>H3A 2T5</postCode>
									<settlement>Montréal</settlement>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<postCode>H3A 2T5</postCode>
									<settlement>Montréal</settlement>
									<region>Québec</region>
									<country>Canada E-mail</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Mathematics and Statistics</orgName>
								<orgName type="institution">McGill University</orgName>
								<address>
									<postCode>H3A 2T5</postCode>
									<settlement>Montréal</settlement>
									<region>Québec</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Henry</forename><surname>Wolkowicz</surname></persName>
							<email>hwolkowicz@uwaterloo.ca</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Combinatorics and Optimization</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>Ontario</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">McMaster University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>37-41 Mortimer Street</addrLine>
									<postCode>W1T 3JH</postCode>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The trust region subproblem and semidefinite programming</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1055-6788 print</idno>
						<imprint>
							<date type="published" when="2014-12-23">23 December 2014</date>
						</imprint>
					</monogr>
					<idno type="MD5">AA13581DB544DFFA020A65E9981467A8</idno>
					<idno type="DOI">10.1080/10556780410001647186</idno>
					<note type="submission">10:55 Publisher: Taylor &amp; Francis Informa Ltd Registered in England and Wales Registered Number: 1072954 Registered office: Mortimer House, Received 21 August 2002; In final form 19 March 2003</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Trust regions</term>
					<term>Semidefinite programming</term>
					<term>Duality</term>
					<term>Unconstrained minimization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dedicated to Professor Jochem Zowe</head><p>The trust region (TR) subproblem (the minimization of a quadratic objective subject to one quadratic constraint and denoted TRS) has many applications in diverse areas, e.g., function minimization, sequential quadratic programming, regularization, ridge regression, and discrete optimization. In particular, it determines the step in TR algorithms for function minimization. Trust region algorithms are popular for their strong convergence properties. However, a drawback has been the inability to exploit sparsity as well as the difficulty in dealing with the so-called hard case. These concerns have been addressed by recent advances in the theory and algorithmic development. In particular, this has allowed Lanczos techniques to replace Cholesky factorizations.</p><p>This article provides an in depth study of TRS and its properties as well as a survey of recent advances. We emphasize large scale problems and robustness. This is done using semidefinite programming (SDP) and the modern primal-dual approaches as a unifying framework. The SDP framework arises naturally and solves TRS efficiently. In addition, it shows that TRS is always a well-posed problem, i.e., the optimal value and an optimum can be calculated to a given tolerance. We provide both theoretical and empirical evidence to illustrate the strength of the SDP and duality approach. In particular, this includes new insights and techniques for handling the hard case, as well as numerical results on large test problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PLEASE SCROLL DOWN FOR ARTICLE</head><p>Taylor &amp; Francis makes every effort to ensure the accuracy of all the information (the "Content") contained in the publications on our platform. However, Taylor &amp; Francis, our agents, and our licensors make no representations or warranties whatsoever as to the accuracy, completeness, or suitability for any purpose of the Content. Any opinions and views expressed in this publication are the opinions and views of the authors, and are not the views of or endorsed by Taylor &amp; Francis. The accuracy of the Content should not be relied upon and should be independently verified with primary sources of information. Taylor and Francis shall not be liable for any losses, actions, claims, proceedings, demands, costs, expenses, damages, and other liabilities whatsoever or howsoever caused arising directly or indirectly in connection with, in relation to or arising out of the use of the Content. This article may be used for research, teaching, and private study purposes. Any substantial or systematic reproduction, redistribution, reselling, loan, sub-licensing, systematic supply, or distribution in any form to anyone is expressly forbidden. Terms &amp; Conditions of access and use can be found at http://www.tandfonline.com/page/termsand-conditions</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>We are concerned with the following quadratic minimization problem:</p><p>(TRS) q * = min q(x) := x T Ax -2a T x s.t.</p><p>x ≤ s.</p><p>Here, A is an n × n symmetric (possibly indefinite) matrix, a is an n-vector, s is a positive scalar and x is the n-vector of unknowns. All matrix and vector entries are real. This problem is referred as the trust region (TR) subproblem (TRS). This problem has many applications in e.g., forming subproblems for constrained optimization <ref type="bibr" target="#b1">[2]</ref>, regularization of ill-posed problems <ref type="bibr" target="#b2">[3]</ref>, and regularization for ill-conditioned linear regression problems (called ridge regression, <ref type="bibr" target="#b3">[4]</ref>).</p><p>In particular, it is important in a class of optimization methods called trust region methods for minimization where, at each iteration of the method, the algorithm determines a step by (approximately) finding the minimum of a quadratic function (a local quadratic model of a given function f ) restricted to a given ball of radius s. (This is called the spherical TR. We do not discuss scaled, ellipsoidal, box, or other TRs.) The radius s increases or decreases depending on how well the decrease in the quadratic model predicts the true decrease in f . The data, A and a, respectively, represent the Hessian and the gradient of the modeled function. These methods have advantages over, e.g., quasi-Newton methods. Under mild assumptions, they produce a sequence of iterates with an accumulation point that satisfies both first and second order necessary optimality conditions [e.g., <ref type="bibr">Ref. 5]</ref>. Furthermore, if the accumulation point satisfies the second order sufficient optimality conditions, the method reduces to Newton's method locally and convergence is q-quadratic. [For more details see, e.g., Refs. 2,6.] However, the popularity of TR methods for unconstrained minimization has lagged behind quasi-Newton methods. Numerical difficulties in standard algorithms for TRS can arise when a is (approximately) perpendicular to the eigenspace of the smallest eigenvalue of A. This is referred to as the (near) hard case in the literature. In addition, algorithms for TRS were based on the Cholesky factorization of the Hessian of the Lagrangian, thus sparsity could not always be exploited efficiently. On the other hand, algorithms such as limited memory quasi-Newton methods proved to be successful, e.g., Refs. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Though TRS appears to be a simple problem, there is a long history of elegant theory and algorithms. (The recent books <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9]</ref> contain extensive bibliographies. See also the bibliographical database for Ref. <ref type="bibr" target="#b1">[2]</ref> at URL www.fundp.ac.be/˜phtoint/pht/trbook.bib.) In this article, we emphasize the modern primal-dual approaches. In particular, we study three methods that consider the above mentioned concerns, i.e., the dual based algorithm of Moré-Sorensen (MS) <ref type="bibr" target="#b9">[10]</ref>, the semidefinite programming (SDP) based algorithm of Rendl-Wolkowicz (RW) <ref type="bibr" target="#b10">[11]</ref>, and the generalized Lanczos trust region (GLTR or coincidently GLRT) method of Gould et al. <ref type="bibr" target="#b11">[12]</ref>.</p><p>The classical (MS) algorithm <ref type="bibr" target="#b9">[10]</ref> was the first algorithm able to handle the hard case efficiently. (The algorithm of Gay <ref type="bibr" target="#b12">[13]</ref> also treats the hard case.) We revisit and modify the RW primal-dual algorithm <ref type="bibr" target="#b10">[11]</ref> which is based on a parametric eigenvalue problem using a Lanczos technique, SDP, and duality. It is designed specifically to handle large sparse problems; it also handles the hard case efficiently. The GLTR is the last algorithm we look at, see Ref. <ref type="bibr" target="#b11">[12]</ref>. This algorithm uses the Lanczos procedure to obtain a restricted TRS problem with a tridiagonal matrix. This subproblem can be solved quickly using the MS algorithm.</p><p>Several other recent approaches deserve mention. The method by Sorensen <ref type="bibr" target="#b13">[14]</ref> is similar to the RW algorithm in that it uses a parametric eigenvalue approach. The difference of convex functions (DC) method of Tao and An <ref type="bibr" target="#b14">[15]</ref> and the method of Hager <ref type="bibr" target="#b15">[16]</ref> are both designed to exploit sparsity. The method in Ref. <ref type="bibr" target="#b15">[16]</ref> is similar in spirit to GLTR, i.e., they both solve a sequence of subproblems where TRS is restricted to a special Krylov subspace. The method of Ye <ref type="bibr" target="#b16">[17]</ref> exploits a new efficient line search technique.</p><p>The main contribution of this article is the use of SDP and the modern primal-dual approach to motivate, view, and modify existing algorithms for TRS. In addition, we present a novel approach to handle the hard case using a shift of the eigenvalues and deflation. We also include numerical comparisons between the algorithms and specific examples that illustrate the performance on the hard case. In particular, we try to answer questions posed in Ref. <ref type="bibr" target="#b11">[12]</ref> about the desired accuracy in solving the TRS within a TR minimization algorithm. We include numerical tests, completely in a MATLAB framework, on problems with dimensions of order n = 10 5 for the TR method, and order n = 10 6 for TRS problems. Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Outline</head><p>We continue in Section 2 with the optimality conditions and definitions of the easy and hard cases for TRS. In particular, Section 2.1 describes the shift process that yields an equivalent well-posed convex program. That TRS is well-posed also follows using SDP, and is shown in Theorem 5.2 and Remark 5.1. In Section 3, we use duality to motivate and describe the MS algorithm. The GLTR algorithm is described in Section 4. In Section 5, we present several dual programs to TRS exploiting the strong Lagrangian duality for TRS. These provide the unifying framework for the different algorithms outlined in Section 6. The RW algorithm, with our modifications, is presented in detail in Section 7. The numerical tests appear in Section 8. Concluding remarks are given in Section 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">OPTIMALITY CONDITIONS</head><p>It is known [see Refs. <ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19</ref>] that x * is a solution to TRS if and only if</p><formula xml:id="formula_0">(A -λ * I )x * = a A -λ * I 0, λ * ≤ 0 dual feasibility x * 2 ≤ s 2 primal feasibility λ * (s 2 -x * ) = 0, complementary slackness<label>(1)</label></formula><p>for some (Lagrange multiplier) λ * . These conditions are surprising in two respects. First, the conditions characterize optimality of a possibly nonconvex problem, i.e., they are necessary and sufficient. Second, the usual second order positive semidefinite necessary conditions hold on all of R n rather than just the tangent plane at the optimal point. We have added the descriptive three phrases in Eq. ( <ref type="formula" target="#formula_0">1</ref>), since this coincides with the framework in Ref. <ref type="bibr" target="#b10">[11]</ref> and with the modern primal-dual optimization approach, though no dual program appeared in the earlier papers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The Hard Case</head><p>If A -λ * I 0 in Eq. ( <ref type="formula" target="#formula_0">1</ref>), then x * is the unique solution to TRS (this is true generically), i.e., x * = (A -λ * I ) -1 a. In general, we denote</p><formula xml:id="formula_1">x(λ) = (A -λI ) † a,<label>( 2)</label></formula><p>where (•) † denotes the Moore-Penrose generalized inverse. Singularity (or near singularity) of A -λI can result in difficulties in using x(λ) as a solution, see Table <ref type="table" target="#tab_1">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Shift, Deflation, and Robustness</head><p>First, we note that λ 1 (A) &gt; 0 implies that λ * ≤ 0 &lt; λ 1 (A), i.e., the hard case (case 2) cannot hold. Second, if λ * = 0, then A 0 and x * = A † a ≤ s. These two situations can be handled by our algorithm in a standard way. The following deflation technique forms the basis for our approach to handle the hard case.  </p><formula xml:id="formula_2">a ⊥ N ( A -λ 1 ( A)I ) a ⊥ N ( A -λ 1 ( A)I ) a ⊥ N ( A -λ 1 ( A)I ) (implies λ * &lt; λ 1 ( A)) a n dλ * &lt; λ 1 ( A) and λ * = λ 1 ( A) (i) ( A -λ * I ) † a = s or λ * = 0 (ii) ( A -λ * I ) † a &lt; s, λ * &lt; 0</formula><p>we can use the shift in Lemma 2.1 (Part 2). Lemma 2.1 (Part 1) shows that performing the shift when the hard case did not exist does not cause harm. In many of our numerical tests, our heuristics detected an unconstrained problem after the shifts. This latter problem was solved using preconditioned conjugate gradients (PCG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEMMA 2.1 Let</head><formula xml:id="formula_3">A = n i=1 λ i (A)v i v T i = P P T be the spectral decomposition of A, with v i orthonormal eigenvectors and P = [v 1 v 2 • • • v n ]</formula><p>an orthogonal matrix. Set the vector ā := P T a and the sets</p><formula xml:id="formula_4">S 1 = {i : āi = 0, λ i (A) &gt; λ 1 (A)} S 2 = {i : āi = 0, λ i (A) &gt; λ 1 (A)} S 3 = {i : āi = 0, λ i (A) = λ 1 (A)} S 4 = {i : āi = 0, λ i (A) = λ 1 (A)}.</formula><p>For k = 1, 2, 3, 4: the matrices A k := i∈S k λ i (A)v i v T i ; and the ( A-invariant subspace) orthogonal projections P k := i∈S k v i v T i , where A k = P k = 0, if S k = ∅. Then the following holds.</p><p>1. Suppose S 3 = ∅ (easy case), α &gt; 0, and i ∈ S 2 ∪ S 4 . Then</p><formula xml:id="formula_5">(x * , λ * ) solves TRS iff (x * , λ * ) solves TRS when A is replaced by A + αv i v T i .</formula><p>2. Let u * = (A -λ * I ) † a with u * &lt; s and suppose that i ∈ S 2 ∪ S 4 and α &gt; 0. Then x * solves TRS and v T i x * = 0, for some i ∈ S 4 iff the hard case (case 2(ii)) holds.</p><formula xml:id="formula_6">(x * = u * + z, λ * ), z ∈ N (A -λ * I ) solves TRS iff (x * = u * + z, λ * ), z ∈ N (A + αv i v T i -λ * I ) solves TRS when A is replaced by A + αv i v T i .</formula><p>Proof The set S 3 can be used to define the hard case, i.e., S 3 = ∅ if and only if a is not orthogonal to N (A -λ 1 (A)I ) if and only if the easy case holds. Note that S 3 = ∅ ⇒ S 4 = ∅.</p><p>Consider the equivalent problem to TRS obtained after the rotation by P T and diagonalization of A:</p><formula xml:id="formula_7">(TRS P ) q * = min (P T x) T (P T x) -2 āT (P T x) = w T w -2 āT w s.t. w ≤ s, w = P T x.</formula><p>(3) Note that the P k form a resolution of the identity, I = 4 k=1 P k . Moreover, x * solves TRS if and only if w * = P T x * solves (TRS P ), Eq. (3). We set w * = P T x * and, for k = 1, 2, 3, 4,</p><formula xml:id="formula_8">E k = i∈S k e i e T i , k = i∈S k λ i e i e T i , x * k := P k x * , w * k := E k w * ,</formula><p>where the e i are unit vectors, and</p><formula xml:id="formula_9">E k = k = 0, x k = w k = 0, if S k = ∅. In addition, = 4 i=1 k , w * = 4 i=1 w k , I = 4 i=1 E k , E k = P T P k P, w * k = P T x * k .</formula><p>For simplification, we prove the results for this diagonalized equivalent program.</p><p>1. Since S 3 = ∅, the definitions imply that the easy case holds and</p><formula xml:id="formula_10">w * = ( -λ * I ) -1 ā = ( + αe i e T i -λ * I ) -1 ā,</formula><p>i.e., the optimality conditions are unchanged after the replacement. This completes the proof of Item 1. 2. As in the above proof, the definitions imply</p><formula xml:id="formula_11">P T u * = ( -λ * I ) † ā = ( + αe i e T i -λ * I ) † ā,</formula><p>In this case, the optimality conditions are unchanged except for the change in the conditions for z. This completes the proof of Item 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Note that u</head><formula xml:id="formula_12">* ∈ R(A -λ * I ) ⊥ N (A -λ * I ). Necessity: Assume that (x * , λ * ) with x * = u * + z, z ∈ N (A -λ * I ) solves TRS. Then w * = ( -λ * I ) † ā + P T z, P T z ∈ N ( -λ * I ). It follows from the optimality conditions that w * = ( -λ * I ) † ā + P T z = P T u * + P T z, λ * ( P T u * 2 + P T z 2 -s 2 ) = 0, ( -λ * I ) 0.<label>(4)</label></formula><p>By adding and subtracting λ 1 (A), we see that (P T u * , λ * -λ 1 (A)) is optimal for TRS P if we replace by -λ 1 (A)I . Conversely, suppose that (u * , λ * -λ 1 (A)) solves TRS when A is replaced by A -λ 1 (A)I . Then We can find an appropriate z ∈ N (A -λ 1 (A)I ) if needed so that u * 2 + z 2 = s 2 , i.e., (x * = u * + z, λ * ) solves TRS. This completes the proof of Item 3. 4. Assume that x * = Pw * solves TRS andv T i x * = 0, for some i ∈ S 4 . Equivalently, e T i w * = 0, for some i ∈ S 4 . From the definitions, āi = 0, ∀i ∈ S 2 ∪ S 4 . Therefore w * = ( -</p><formula xml:id="formula_13">P T u * = ( -λ * I ) † ā, u * ≤ s, ( -λ 1 (A)I ) 0.<label>( 5</label></formula><formula xml:id="formula_14">λ * I ) † ā + E 4 v, for some v ∈ R n . The assumption implies that E 4 v = 0.</formula><p>Conversely, suppose that the hard case (case 2(ii)) holds. Then w * = ( -λ * I ) † ā + E 4 v, for some v ∈ R n with E 4 v = 0. This completes the proof of Item 4.</p><p>Numerically, we cannot distinguish between the hard case and the near hard case. This is handled using the following. LEMMA 2.2 Suppose that x * solves TRS and x * = s. Let ε &gt; 0 and v ∈ R n with v = 1. Let µ * (ε) be the optimal value of TRS when a is perturbed to a + εv. Then</p><formula xml:id="formula_15">-2sε ≤ µ * -µ * (ε) ≤ 2sε. Proof µ * (ε) = min x =s q(x) -2εv T x ≥ min x =s q(x) + min x =s -2εv T x = µ * -2εs.</formula><p>This proves the right-hand-side inequality.</p><p>Since x * is optimal for TRS and on the boundary of the ball, we get</p><formula xml:id="formula_16">µ * (ε) ≤ µ * -2εv T x * ≤ µ * + 2εs.</formula><p>The literature often labels the hard case (case 2) as an ill-posed or degenerate problem, e.g., Refs. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16]</ref>. Adding a norm constraint to an ill-posed problem is a well-known regularization technique, e.g., Ref. <ref type="bibr" target="#b2">[3]</ref>. Thus, it would appear to be contradictory for TRS to be an ill-posed problem. In fact, we can orthogonally diagonalize the quadratic form; and, we note that the symmetric eigenvalue problem has a condition number of 1 <ref type="bibr" target="#b19">[20]</ref>. Then, TRS can be shown to be equivalent to a linearly constrained convex programming problem, see Ref. <ref type="bibr" target="#b20">[21]</ref>, a problem that can be solved efficiently and robustly.</p><p>The following lemma and example illustrate that TRS is always a stable convex program. This also follows from the equivalent SDP dual pair in Theorem 5.2. LEMMA 2.3 Suppose that the hard case (case 2 (ii)) holds for TRS. Let u * = (A -λ * I ) † a, and x * = u * + z be a solution of TRS with z ∈ N (A -λ * I ). Then z = 0, λ 1 (A) ≤ 0 and TRS is equivalent to the following stable convex program</p><formula xml:id="formula_17">(TRS s ) q * s := min q s (x) := x T (A -λ 1 (A)I )x -2a T x + s 2 λ 1 (A) s.t. x ≤ s. (6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downloaded by [McMaster University] at 10:55 23 December 2014</head><p>The equivalence is in the sense that the optimal values satisfy q * = q * s ; and (x * , λ * ) solves TRS if and only if (x * , 0) solves TRS s .</p><p>Proof Since the hard case (case 2 (ii)) holds, we get z = 0 and λ 1 (A) = λ * ≤ 0. From Lemma 2.1, Item 3, we get (u * , 0) solves TRS s . We can then add z ∈ N (A -λ * I ) to get u * + z = s.</p><p>Convex programs for which Slater's constraint qualification holds are called stable, e.g., Refs. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. They are equivalent to convex programs for which the optimal dual solutions form a convex compact set which further implies that the perturbation function (optimal value function subject to linear perturbations in the data) is convex and Lipschitz continuous. In our case we have the additional strong linear independence CQ, which implies that the optimal dual solution is unique and the perturbation function is differentiable. We also have a compact convex feasible set [see e.g., Refs. <ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2.1 (Hard Case, Case 2(ii)) Let</head><formula xml:id="formula_18">A = 1 + γ 0 0 -1 + δ , a = 2 + α β , s = √ 2,</formula><p>where α, β, γ , δ are perturbations in the data. First suppose that the perturbations are all 0. Then the hard case (case 2(ii)) holds; the optimal Lagrange multiplier is</p><formula xml:id="formula_19">λ * = λ 1 (A) = -1;</formula><p>and the best least squares solution is</p><formula xml:id="formula_20">x = (A -λ * I ) † a = 1 0 with x = 1 &lt; s.</formula><p>The optimal solution is obtained from</p><formula xml:id="formula_21">x * = x * (0) = x + 0 ±1 = 1 ±1 . ( 7)</formula><p>For (small) nonzero perturbations, the optimal Lagrange multiplier λ * is still unique and -1 + δ is the smallest eigenvalue. If β = 0, then the hard case still holds; the optimum is obtained from λ * = -1 + δ; and</p><formula xml:id="formula_22">x * = x * (α, γ , δ) =   2 + α 1 + γ -λ * 0   + 0 ε ± 1 ,</formula><p>where ε is chosen to obtain x * = s, e.g., with +1</p><formula xml:id="formula_23">(1 + ε) 2 + 2 + α 2 + γ -δ 2 = s 2 = 2.</formula><p>Depending on the choice of sign, these solutions converge to a solution in Eq. ( <ref type="formula">7</ref>), as the perturbations converge to 0. Moreover, a Taylor series expansion shows that x * (0)x * (α, γ, δ) ≤ 2(|α| + |γ | + |δ|) for small perturbations, i.e., we have a bounded condition number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downloaded by [McMaster University] at 10:55 23 December 2014</head><p>If β = 0, then we have the easy case. The unique optimal Lagrange multiplier λ * &lt; -1 + δ and the unique optimum is obtained from</p><formula xml:id="formula_24">x * =     2 + α 1 + γ -λ * β -1 + δ -λ *     ,</formula><p>where λ * satisfies the positive definiteness condition as well as x * = s. This implies that</p><formula xml:id="formula_25">2 + α 1 + γ -λ * 2 + β -1 + δ -λ * 2 = 2. Since λ * → -1 and (2 + α)/(1 + γ -λ * ) → 1,</formula><p>as the perturbations go to 0, we see that the optimal solutions converge appropriately again, i.e., x * (α, β, γ, δ) → x * (0, 0, 0, 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A DUAL ALGORITHM: THE MOR É-SORENSEN ALGORITHM</head><p>We motivate the MS algorithm using duality and illustrate how SDP arises naturally from this setting. The Lagrangian dual of TRS is</p><formula xml:id="formula_26">q * = ν * := max λ≤0 min x x T (A -λI )x -2a T x + λs 2 = max λ≤0 h(λ)</formula><p>where the Lagrangian is L(x, λ) := x T (A -λI )x -2a T x + λs 2 and the dual functional is h(λ) := min x L(x, λ). Strong duality (q * = ν * and dual attainment <ref type="bibr" target="#b23">[24]</ref>) holds. Since the inner minimization is unconstrained, we have a hidden semidefinite constraint that the Hessian, ∇ 2 L(x, λ) 0, is positive definite. The dual functional h is concave with domain within the semidefinite constraint.</p><p>Therefore, we can replace TRS with the simpler root finding problem</p><formula xml:id="formula_27">h (λ) = 0 (8) (if h is differentiable), with the restrictions: λ ≤ 0, ∇ 2 L(x, λ) = A -λI 0.<label>( 9)</label></formula><p>We see that semidefiniteness (convexity of the Lagrangian) arises naturally from the duality setting. Note that h (λ) = s 2x(λ) T x(λ), when it exists. If we use the optimality conditions in Eq. ( <ref type="formula" target="#formula_0">1</ref>) with the descriptive phrases, then the MS algorithm maintains dual feasibility and complementary slackness, while trying to attain primal feasibility (cf. the dual simplex method for linear programming). Though Newton's method has asymptotic q-quadratic convergence, the Newton step does not take into account the semidefinite restrictions, which can result in many backtracking steps. This illustrates the weakness of a dual method compared to a primal-dual method.</p><p>The main work in the iterations is a Cholesky factorization used in the evaluation of the derivatives and the Newton step for λ, as well as in the safeguarding and updating scheme that produces either a point λ from which quadratic convergence ensues, or reduces the interval of Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014 uncertainty for the optimal λ. If the possible hard case is detected, optimality is reached by taking primal steps to the boundary of the ball. In both cases, given two parameters σ 1 and σ 2 in (0, 1), the algorithm terminates in a finite number of iterations with an approximate solution x which satisfies q( x)q * ≤ σ 1 (2 -σ 1 ) max{ q * , σ 2 },</p><p>x ≤ (1 + σ 1 ) . <ref type="bibr" target="#b9">(10)</ref> One additional innovation makes the MS algorithm fast. Assume A -λI 0. There are disadvantages in applying Newton's method to find a root of the function h or equivalently of ψ(λ) := x(λ)s. For λ &lt; λ 1 (A) and close to λ 1 (A), the orthogonal diagonalization of A shows that</p><formula xml:id="formula_28">ψ(λ) = Q( -λI ) -1 Q T a -s ≈ c 1 λ 1 (A) -λ + d,</formula><p>for some constants c 1 &gt; 0, d. This function is highly nonlinear for values of λ near λ 1 (A), which equates to slow convergence for Newton's method. Moré-Sorensen solve the equivalent so-called secular equation</p><formula xml:id="formula_29">φ(λ) := 1 s - 1 x(λ) = 0.<label>( 11)</label></formula><p>[See e.g., Refs. <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref>. The rational structure of x(λ) 2 , shows that this function is less nonlinear, i.e.,</p><formula xml:id="formula_30">φ(λ) ≈ 1 s - λ 1 (A) -λ c 2 ,</formula><p>for some c 2 &gt; 0. Therefore, Newton's method applied to this function will be more efficient.</p><p>One can also show φ(λ) is a convex function strictly increasing on (-∞, λ 1 (A)) [see Ref. 10, p. 562 for the details].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Handling the Hard Case in Moré-Sorensen Algorithm</head><p>From Fig. <ref type="figure" target="#fig_2">1</ref>, we get the following indicator of the easy case for TRS. Then the hard case (case 2) cannot occur for TRS, i.e. λ * &lt; λ 1 (A).</p><p>However, Fig. <ref type="figure" target="#fig_2">1</ref> also shows that Newton's method can provide a poor prediction for λ * if φ(λ) &lt; 0 and λ * is close to 0 and/or λ 1 (A). This would result in many backtracking steps to find the above λ 0 (each of which involves an attempted Cholesky factorization) and would make the algorithm inefficient. As discussed above in Section 2.1, in the hard case (case 2), a solution to TRS can be obtained by first finding a solution x(λ 1 (A)) to the system</p><formula xml:id="formula_31">(A -λ 1 (A)I )x = a (12)</formula><p>with x ≤ s. If strict inequality holds, x &lt; s, then we need an eigenvector z ∈ N (Aλ 1 (A)I ), and τ ∈ R, such that x(λ 1 (A)) + τ z = s, i.e.,</p><formula xml:id="formula_32">x * = x(λ 1 (A)) + τ z (13)</formula><p>satisfies the optimality conditions. The following lemma by MS <ref type="bibr" target="#b24">[25]</ref> is the key to implementing this idea numerically.</p><p>LEMMA 3.2 (Primal step to the boundary) Let 0 &lt; σ &lt; 1 be given and suppose that</p><formula xml:id="formula_33">A -λI = R T R, (A -λI )x = a, λ ≤ 0.<label>( 14)</label></formula><formula xml:id="formula_34">Let z ∈ R n satisfy x + z 2 = s 2 (15)</formula><p>and Rz 2 ≤ σ ( Rx 2 -λs 2 ). ( <ref type="formula">16</ref>)</p><formula xml:id="formula_35">Then |q(x + z) -q(x * )| ≤ σ |q(x * )| (17)</formula><p>where x * is optimal for TRS.</p><p>We will get back to this lemma in Section 6.1, where we show that this lemma is measuring a duality gap in a primal-dual pair of SDPs. Note that Eqs. ( <ref type="formula" target="#formula_33">14</ref>) and ( <ref type="formula">15</ref>) guarantee the dual and primal feasibility constraints in the optimality conditions <ref type="bibr" target="#b0">(1)</ref>. Comparisons with the RW algorithm are included in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE GENERALIZED LANCZOS TRUST REGION ALGORITHM</head><p>As mentioned above, current attempts to solve TRS focus on exploiting sparsity. The Cholesky factorization can be a bottleneck in the MS algorithm for sparse problems without special structure. The GLTR algorithm involves a Lanczos tridiagonalization of the matrix A, which Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014 then allows for fast Cholesky factorizations. The method requires only matrix-vector multiplications (MVM) and exploits sparsity in A. The method solves a sequence of restricted problems min q(x)</p><formula xml:id="formula_36">s.t. x ≤ s x ∈ S ≡ K k , (<label>18</label></formula><formula xml:id="formula_37">)</formula><p>where K k : = span{a, Aa, A 2 a, A 3 a, . . . , A k a} are specially chosen (Krylov) subspaces of R n .</p><p>(A similar approach based on Krylov subspaces is presented in Ref. <ref type="bibr" target="#b15">[16]</ref>.) The way S is chosen is inspired by the Steihaug algorithm of Ref. <ref type="bibr" target="#b28">[29]</ref> [see also <ref type="bibr">Refs. 12,</ref><ref type="bibr" target="#b29">30]</ref>. The authors use heuristics for the MS algorithm that find a starting value of λ on the correct side of λ * to ensure asymptotic q-quadratic convergence of the Newton iteration λ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Handling the Near Hard Case in Generalized Lanczos Trust Region Algorithm</head><p>If the near hard case (case 2) occurs for the tridiagonal subproblem, then finding a λ that guarantees the q-quadratic convergence may be difficult and time consuming. (The GLTR algorithm fails if the hard case (case 2) occurs.) In addition, ill-conditioning will slow down both the MS algorithm and therefore the GLTR method [see Ref. 12, p. 515].</p><p>Further details are included within the semidefinite framework in Section 6.2.</p><p>As mentioned above, the method in Ref. <ref type="bibr" target="#b15">[16]</ref> is similar in spirit to GLTR. The main difference is the choice of Krylov subspaces K k . The vectors include the direction found from a sequential quadratic programming (SQP) model for TRS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DUALITY AND A SEMIDEFINITE FRAMEWORK FOR THE TRUST REGION SUBPROBLEM</head><p>Duality plays a central role in designing optimization algorithms, as illustrated in our motivation for the MS algorithm in Section 3. In this section, we focus our attention on different dual programs associated with TRS. We will further see below the role played by duality in both the MS and GLTR algorithms, i.e., they are both dual based, and so they exhibit slow convergence and lack of robustness, characteristics of dual algorithms.</p><p>For simplicity, we restrict ourselves to the equality TRS, i.e., we minimize over the sphere rather than the ball. (To extend to the standard TRS we would need to add the dual constraint λ ≤ 0.) Precisely, consider the slightly different problem</p><formula xml:id="formula_38">(TRS = ) q * = min q(x) s.t. x = s. (19)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Lagrangian Duality and Semi Definite Programming</head><p>From Ref. <ref type="bibr" target="#b23">[24]</ref>, we know that strong Lagrangian duality holds for TRS = , i.e.,</p><formula xml:id="formula_39">q * = ν * := max λ min x L(x, λ). (<label>20</label></formula><formula xml:id="formula_40">)</formula><p>Since L(x, λ) is a quadratic, the inner min problem is unbounded below unless the hidden constraints, </p><formula xml:id="formula_41">A -λI 0, a ∈ R(A -λI ),</formula><formula xml:id="formula_42">q * = max A-λI 0, a∈R( A-λI ) min x L(x, λ).</formula><p>The inner minimum is attained if bounded. We get the following. THEOREM 5.1 <ref type="bibr" target="#b23">[24]</ref> The Lagrangian dual for TRS = is</p><formula xml:id="formula_43">(D) q * = sup A-λI 0 h(λ), (<label>22</label></formula><formula xml:id="formula_44">)</formula><p>where</p><formula xml:id="formula_45">h(λ) := λs 2 -a T (A -λI ) † a,</formula><p>where h is a concave function on the feasible set. In the easy case and hard case (case 1), the sup can be replaced by a max.</p><p>COROLLARY 5.1 The Lagrangian dual for TRS is equivalent to</p><formula xml:id="formula_46">(D) q * = sup A-λI 0 λ≤0 h(λ). (<label>23</label></formula><formula xml:id="formula_47">)</formula><p>In the easy case and hard case (case 1), the sup can be replaced by a max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unconstrained and Linear Duals</head><p>We now present an unconstrained concave maximization problem and a pair of linear SDPs, all of which are equivalent to TRS = , see Ref. <ref type="bibr" target="#b10">[11]</ref> for the details. Define</p><formula xml:id="formula_48">D(t) = t -a T -a A , k(t) := (s 2 + 1)λ 1 (D(t)) -t.<label>( 24)</label></formula><p>Then we have the following unconstrained dual problem for TRS = :</p><formula xml:id="formula_49">q * = max t k(t). (<label>25</label></formula><formula xml:id="formula_50">)</formula><p>It is well known that λ 1 (D(•)) is a concave function, and therefore k(•) is concave as well. Thus, using duality, TRS = is equivalent to an unconstrained concave maximization problem in one variable. We can also rewrite Eq. ( <ref type="formula" target="#formula_49">25</ref>) in the following way so that it becomes a linear semidefinite program: max</p><formula xml:id="formula_51">D(t) λI (s 2 + 1)λ -t.<label>( 26)</label></formula><p>Equivalently,   <ref type="formula">27</ref>), <ref type="bibr" target="#b27">(28)</ref>, are equivalent to TRS = . Moreover, the Slater constraint qualification and strict complementarity hold for the primal-dual SDP pair <ref type="bibr" target="#b26">(27)</ref> and <ref type="bibr" target="#b27">(28)</ref>.</p><formula xml:id="formula_52">q * = max (s 2 + 1)λ -t s.t. λI -t E 00 D(0),</formula><p>Proof The equivalence with TRS = was already shown above.</p><p>That Slater's constraint qualification holds is clear, i.e., choose λ and Y appropriately. Now suppose that λ, t, Y are optimal for the SDP pair ( <ref type="formula">27</ref>) and <ref type="bibr" target="#b27">(28)</ref>. Then Z := D(t) -λI is positive semidefinite and singular. Let k be the multiplicity of λ 1 (D(t)) and y 1 , . . . , y k be an orthonormal basis for its eigenspace.</p><formula xml:id="formula_53">Set V = [y 1 • • • y k ] and redefine Y ← Y + V V T .</formula><p>We can scale DYD using a diagonal matrix D to guarantee that Y is feasible for Eq. <ref type="bibr" target="#b27">(28)</ref>. By construction Remark 5.1 The primal-dual linear SDP pair can be solved to any desired accuracy in polynomial time, see e.g. Ref. <ref type="bibr" target="#b30">[31]</ref>. This emphasizes that TRS is a well-posed problem.</p><formula xml:id="formula_54">ZY = 0, Z + Y 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">SEMIDEFINITE FRAMEWORK</head><p>From above (Theorem 5.2), we saw that TRS = is equivalent to a primal-dual pair of SDPs which could be solved using primal-dual interior-point methods. These methods have revolutionized our view of optimization during the last 15 years. In particular, path-following methods have proven to be an efficient approach for many classes of optimization problems. The main idea for these methods is to apply Newton's method to a perturbation of the primal-dual optimality conditions. Using both the primal and dual equations and variables makes for efficient, robust algorithms. (The recent books <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> describe this approach for both linear and semidefinite programming.) Often, compromises have to be made to deal with large sparse problems. In particular, for SDP one often uses a dual based method to exploit sparsity, since the primal variable is often large and dense, see e.g., Refs. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>. We previously motivated the MS algorithm using duality. We now describe the MS and GLTR algorithms using SDP and the modern primal-dual approach. We see that compromises are made and a full primal-dual path-following method is not used. Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">A Semidefinite Framework for the Moré-Sorensen Algorithm</head><p>For simplicity, we consider the case a = 0. We follow Section 5.2 [see also <ref type="bibr">Ref. 11</ref>] and use the following pair of SDP dual programs. (D) is the dual of TRS and (DD) is the dual of (D):</p><formula xml:id="formula_55">(D) q * = sup A-λI 0 h(λ) (29) (DD) q * = inf h(λ) + trace(X (A -λI )) s.t. s 2 -a T ((A -λI ) † ) 2 a -trace X = 0 λ &lt; λ l (A) ( 30) trace X ≤ s 2 X 0,</formula><p>where λ l (A) is the smallest eigenvalue such that a ⊥ N (A -λ l (A)I ).</p><p>In the easy case and the hard case (case 1), we use the dual program (D). The supremum in Eq. ( <ref type="formula">29</ref>) is attained at the stationary point λ * ∈ (-∞, λ 1 (A)),</p><formula xml:id="formula_56">h (λ * ) = -a T ((A -λ * I ) -1 ) 2 a + s 2 = -(A -λ * I ) -1 a 2 + s 2 = 0.</formula><p>Newton's method is applied to the equivalent root finding problem φ(λ) = 0. Safeguarding guarantees that λ * stays in the proper interval. Though Newton's method guarantees q-quadratic convergence, this may only happen after many Newton and backtracking steps. The semidefinite constraint is not used explicitly in choosing the Newton direction or the step length.</p><p>Things are different in the hard (or near hard) case (case 2), i.e., this is the case when the current estimates satisfy primal feasibility x(λ) &lt; s. In this case, MS uses a dual-primal approach. Given such a λ, we now use (DD) to reduce the duality gap, trace(X (A -λI )), between (D),(DD); and we simultaneously reduce the objective value of TRS. To do this we find z to move to the boundary (i.e., the primal step is x + z 2 = s 2 ). The SDP <ref type="bibr" target="#b29">(30)</ref> suggests how such a z should be chosen.</p><formula xml:id="formula_57">q(x + z) = (x + z) T A(x + z) -2a T (x + z) + λs 2 -λ x + z 2 = λs 2 + x T (A -λI )x + 2x T (A -λI )z + z T (A -λI )z -2a T x -2a T z = h(λ) + z T (A -λI )z = h(λ) + trace(zz T (A -λI )).</formula><p>To summarize, note that in the MS algorithm, Rz 2 = z T (A -λI )z. Therefore, when a z is found such that x + z 2 = s 2 and Rz is small, the algorithm is trying to reduce the duality gap between Eqs. ( <ref type="formula">29</ref>) and (30), while maintaining feasibility for Eq. (30).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A Semidefinite Framework for the Generalized Lanczos Trust Region Method</head><p>As in the MS algorithm, we now show that the GLTR algorithm can also be explained using the Lagrangian dual Eq. ( <ref type="formula">29</ref>). Here, we outline how their stopping criterion is measuring the duality gap between TRS = and this Lagrangian dual. (A detailed discussion is given in Ref. <ref type="bibr" target="#b0">[1]</ref>.)</p><p>Each iteration of the algorithm returns a feasible point x k for TRS and a corresponding Lagrange multiplier λ k which are optimal for the subproblem <ref type="bibr" target="#b17">(18)</ref>. The algorithm stops when stationarity is satisfied up to a tolerance, i.e., when  <ref type="formula">29</ref>) and bounded, then it is possible to show</p><formula xml:id="formula_58">(A -λ k I )x k -a<label>(</label></formula><formula xml:id="formula_59">q(x k ) -h(λ k ) = O( (A -λ k I )x k -a 2 ),</formula><p>i.e., the duality gap is bounded by a quantity proportional to the square of Eq. <ref type="bibr" target="#b30">(31)</ref>. Therefore, convergence of Eq. ( <ref type="formula">31</ref>) to zero implies a zero duality gap.</p><p>Though the GLTR algorithm is a primal algorithm, since simpler primal problems are solved to approximate the solution to TRS, the strength of the approximation is directly linked to the duality gap between TRS and the dual problem <ref type="bibr" target="#b28">(29)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">THE RENDL-WOLKOWICZ ALGORITHM, MODIFIED</head><p>This algorithm both exploits the sparsity of A and handles the hard case. The algorithm is based on using various primal and dual steps to reduce the interval of uncertainty for the optimum (maximum) of the unconstrained dual program <ref type="bibr" target="#b24">(25)</ref>. We exploit the properties of the eigenvalues and eigenvectors of the parametric matrix D(t). Many ideas from the MS algorithm are transformed to the large sparse case, e.g., the primal step to the boundary and the secular function. We also exploit information from the primal-dual pair of linear SDPs ( <ref type="formula">27</ref>) and <ref type="bibr" target="#b28">(29)</ref>. We outline the algorithm with a flowchart in Section 7.2. In Section 7.3 we list new heuristics that take advantage of the structure of k(•), accelerate convergence, and facilitate the handling of the hard case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Three Useful Functions</head><p>Graphs, illustrating the properties of these functions, appear in Ref. <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">k(t) = (s 2 + 1)λ 1 (D(t))t</head><p>This is the function that we (implicitly) maximize to solve TRS, see Eq. <ref type="bibr" target="#b24">(25)</ref>. Since</p><formula xml:id="formula_60">lim t→∞ λ 1 (D(t)) = λ 1 (A) and lim t→-∞ (λ 1 (D(t)) -t) = 0, the asymptotic behavior of k(t) is k(t) ∼ (s 2 + 1)λ 1 (A) -t,</formula><p>ast → ∞ (linear with slope -1),</p><formula xml:id="formula_61">k(t) ∼ s 2 t, a s t → -∞ (linear with slope s 2 ), i.e., k(t) is linear as |t| → ∞. Since λ 1 (D(t)) is concave, so is k(t).</formula><p>In the easy case, the function is differentiable and strictly concave. In the hard case, loss of differentiability occurs when the multiplicity of the smallest eigenvalue for The structure of the eigenvectors along with the shift and deflation Lemma 2.1 can be used to avoid the hard case, i.e., if y 0 (t) is small, then we can deflate using the corresponding eigenvector. Lemma 2.2 shows that the deviation from the original problem is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">ψ(t) =</head><p>√ s 2 + 1 -1/y 0 (t)</p><p>Solving ψ(t) = 0 is equivalent to solving k (t) = 0. The advantage is that ψ(t) is less nonlinear (cf. replacing h with φ in the MS algorithm). It can be shown that ψ(t) is strictly decreasing and converges to √ s 2 + 1 -1 as t → -∞. In the easy case, ψ(t) goes to -∞ as t → ∞. In the hard case, ψ(t) is undefined for t &gt; t 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Flowchart</head><p>In the sequel, the set of {t: k (t) &lt; 0} is referred to as the easy side and its complement as the hard side. The details in the flowchart follow in Section 7.3.</p><formula xml:id="formula_62">• INITIALIZATION: 1. Compute λ 1 = λ 1 (A) and corresponding eigenvector v 1 . If λ 1 (A) &lt; 0, shift A ← A - λ 1 (A)I . (λ 1 (A)</formula><p>x * 2 is added back to the objective value at the end.) If a T v 1 is small (near hard case), then deflate, i.e., set</p><formula xml:id="formula_63">Y = {y 1 } = 0 v 1 .</formula><p>. 2. Obtain bounds on q * , λ * , and t * .</p><p>If λ 1 &gt; 0, EXIT if the optimum is an unconstrained minimizer. 3. Initialize parameters and the stopping criteria; this is based on the optimality conditions, duality gap, and intervals of uncertainty. ii. elseif t is on the easy side, update parameters. Take a primal step to the boundary if a hard side point exists. iii. elseif t is on the hard side, update parameters. Take a primal step to the boundary from this hard side point. (e) Save new bounds and update stopping criteria.</p><p>• END LOOP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Techniques Used in the Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">Newton's Method on k(t) -M t = 0</head><p>We use the upper and lower bounds on k(t) and the Newton type method presented in Ref. <ref type="bibr" target="#b36">[37]</ref>. Note that Newton's method applied to k(t) -M t = 0 at t c yields</p><formula xml:id="formula_64">t + = t c - k(t c ) -M t k (t c ) = (s 2 + 1)(t c y 2 0 (t c ) -λ 1 (D(t c ))) + M t (s 2 + 1)y 0 (t c ) 2 -1 .</formula><p>One advantage of this method over solving k = 0 is that the second derivative k is not needed. We use this iteration for appropriate choices of M t in cases where the inverse iteration on ψ fails, i.e., if the hard case holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">Triangle Interpolation</head><p>Given k(t), if we have values of t from the easy and hard sides, t e and t h , then we try to find a better approximation t new to the maximum of k(t) using a technique we call triangle interpolation, i.e., we find the coordinate of the point where the secant lines intersect. (We use a tangent line on the side where there is only one point.) In addition, we also obtain upper bounds q up to q * := k(t * ) from the point where the secant lines intersect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.3">Vertical Cut</head><p>Suppose we have two values of t, t e , and t h , with k(t e ) &lt; k(t h ). (A similar argument holds for the reverse inequality.) Then we can use the concavity of k to reduce the interval of uncertainty for t. We find the intersection of the horizontal line through (t h , k(t h )) with the tangent line at the point (t e , k(t e )), i.e.,</p><formula xml:id="formula_65">t high = t e + k(t h ) -k(t e ) k (t e ) ,</formula><p>where t high is the upper bound on t * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.4">Inverse Interpolation</head><p>We use (quadratic or linear) inverse interpolation on ψ(t) = 0, in the case that y 0 (t) = 0. Since ψ(t) is a strictly decreasing function, we can consider its inverse function, say t (ψ). We use Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014 (concave) quadratic interpolation when possible, i.e., suppose the points (ψ i , t i ), i = 1, 2, 3. Then we solve the system</p><formula xml:id="formula_66">   ψ 2 1 ψ 1 1 ψ 2 2 ψ 2 1 ψ 2 3 ψ 3 1       a b t new    =    t 1 t 2 t 3   </formula><p>and get the new estimate t new . We use the top right 2 × 2 system in the linear interpolation case.</p><p>In the hard case (case 2), inverse interpolation is not well defined. To avoid an erroneous step, we maintain λ on the correct side of λ 1 (A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.5">Recognizing an Unconstrained Minimum</head><p>Problem (TRS) has an inequality constraint, but generally the optimum lies on the boundary and we can solve the same problem with the equality constraint to get the optimum. This does not hold if and only if the matrix A is positive definite and the unconstrained minimum lies inside the TR. The following theorem is the key to recognizing this case.</p><formula xml:id="formula_67">THEOREM 7.3 Let x be a solution to (A -λI )x = a with (A -λI ) positive semidefinite. If λ ≤ 0, then x is a solution to min{x T Ax -2a T x: x ≤ x }. If λ ≥ 0, then x is a solution to min{x T Ax -2a T x: x ≥ x }.</formula><p>Proof The first part follows from the necessary and sufficient optimality conditions and the second part follows easily knowing that the sign of λ plays no role in the positive semidefiniteness of the matrix A -λI when proving these optimality conditions.</p><p>In our algorithm, we successively obtain solutions x k to (A -λ k I )x k = a with A-λ k I 0. Therefore each x k is a solution to min{x T Ax -2a T x: x = x k }. Checking the sign of the multiplier λ k tells us whether x k is a solution to min{x T Ax -2a T x: x ≤ x k } or min{x T Ax -2a T x: x ≥ x k }. If the latter case holds and x k ≤ s, then we know the unconstrained minimum lies in the TR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.6">Shift and Deflate</head><p>Let us consider the case when the optimum is on the boundary of the ball. Once the smallest eigenvalue λ 1 (A) is found in the initialization step, we can use the shift in Lemma 2.1, Item 3 and Lemma 2.3. Therefore, for simplicity, we can assume that λ 1 (A) = 0.</p><p>During the algorithm we deflate eigenvectors y = (y 0 v T ) T if y 0 is small (essentially 0). This indicates that a T v is small. We perturb a ← aa T vv and deflate using A ← A + αvv T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.7">Taking a Primal Step to the Boundary</head><p>The interpolation and heuristics are used to find a new point t and then a corresponding λ for the dual problem, i.e., they are used to take a dual step. Once the λ is found, we can find a corresponding primal point x(λ) for the primal problem. This point will be primal feasible if t (equivalently λ) is on the hard side and it will be primal infeasible on the other (easy) side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downloaded by [McMaster University] at 10:55 23 December 2014</head><p>In either case, we now show that we can take an inexpensive primal step, i.e., move to the boundary and improve the objective value. Thus, we get a primal-dual algorithm.</p><p>In the easy case, the step is motivated by the following lemma (for a proof see Refs. <ref type="bibr" target="#b0">[1]</ref> or <ref type="bibr" target="#b37">[38]</ref>).</p><formula xml:id="formula_68">LEMMA 7.1 Let 0 &lt; s 1 &lt; s &lt; s 2 and x h ∈ argmin{q(x): x ≤ s 2 1 }, x e ∈ argmin{q(x): x ≤ s 2 2 }.</formula><p>Suppose that x h = s 1 , x e = s 2 , x T h (x ex h ) = 0 and the Lagrange multiplier</p><formula xml:id="formula_69">λ h for x h satisfies A -λ h I 0. Furthermore, let m(α) := q(x h + α(x e -x h )). Then m (α) ≤ 0, for α ∈ [0, 1].</formula><p>Thus, we find ᾱ so that x h + ᾱ(x ex h ) = s. We use two values, t h and t e , respectively on the hard side and the easy side, with</p><formula xml:id="formula_70">x h := 1 y 0 (t h ) x(t h ), x e := 1 y 0 (t e )</formula><p>x(t e ).</p><p>Then if x T h (x ex h ) = 0 and ᾱ is defined as in the above lemma, taking a step to the boundary from x h to x h + ᾱ(x ex h ) will decrease the objective function. We assumed the easy case so that y 0 (t e ) = 0. However, in the hard case (case 2) y 0 (t e ) = 0. As in the MS algorithm, we take a step to the boundary. From x h , we use an eigenvector z for the eigenvalue λ 1 (A) as the direction to the boundary. This choice is motivated by Lemma 3.2 and the desire to make the quadratic form z T (A -λ 1 (D(t h ))I )z small. We take the step x h + τ z with τ chosen to reduce the objective function and satisfy x h + τ z = s. The explicit expression for τ is</p><formula xml:id="formula_71">τ = s 2 -x h 2 x T h z + sgn(x T h z) (x T h z) 2 + (s 2 -x h 2 )</formula><p>, where sgn(•) equals 1 if its argument is nonnegative and -1 otherwise. Given a direction z, there are two values of τ for which x h + τ z reaches the boundary. Reference <ref type="bibr" target="#b9">[10]</ref> proves that to improve the objective, we should pick the one with smallest magnitude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">NUMERICAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">The Hard Case</head><p>We now provide numerical evidence that our modified RW algorithm is better suited to handle the hard case (case 2) than the MS algorithm. It is stated in Ref. <ref type="bibr" target="#b9">[10]</ref> that the latter algorithm requires few iterations (2-3) in the hard case. However, this appears to hold only when the desired accuracy is low. Many more iterations are required when higher accuracy is desired.</p><p>Our tests were done using MATLAB 6.1 on a SUNW Ultra-5 10 with 1 GIG RAM. Let q * be the optimal objective value of TRS and q be an approximation for q * . The MS algorithm returns an approximate solution that satisfies q ≤ (1 -σ ) 2 q * , where 0 ≤ σ &lt; 1 is an input to the algorithm, and the approximate solution of the RW algorithm satisfies Downloaded by <ref type="bibr">[</ref>McMaster University] at 10:55 23 December 2014 q ≤ 1/(1 + 2dgaptol)q * , where dgaptol is the desired relative duality gap tolerance. Hence, to get equivalent accuracy, we choose σ = 1 -1/(1 + 2dgaptol).</p><p>We used randomly generated sparse hard case (case 2(ii)) TR subproblems, where the density is order 1/(20n log n). The tolerance parameter dgaptol was set to 10 -12 . Each row in Table <ref type="table" target="#tab_3">II</ref> gives the average number of iterations (iter) and computation time (cpu) for 10 problems of size n. We could not go beyond n = 640 for the MS algorithm due to the large cpu that arise. The results, given in Table <ref type="table" target="#tab_3">II</ref> illustrate the improved performance in both the iter and the cpu.</p><p>Note that the GLTR algorithm does not appear in the above comparison since the algorithm was not designed to handle this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Rendl-Wolkowicz and Generalized Lanczos Trust Region Algorithms in Trust Region Framework</head><p>In Ref. <ref type="bibr" target="#b11">[12]</ref>, the GLTR method is stopped early after a limited extra number of iterations, say N, once the solution is known to lie on the boundary of the TR. More precisely, the algorithm stops if the subspace S in Eq. ( <ref type="formula" target="#formula_36">18</ref>) is increased in dimension by N once the solution is known to be on the boundary of the TR and problems of the type ( <ref type="formula" target="#formula_36">18</ref>) are solved. The reason for limiting the size of the subspaces S once the boundary has been reached is motivated by the fact that the authors in Ref. <ref type="bibr" target="#b11">[12]</ref> question whether high accuracy is needed for the TRS within a TR framework. We argue that increased accuracy is needed for TRS just as for the solution of the Newton equation when using inexact Newton methods, as the iterates approach a stationary point, see e.g. Refs. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>For the upcoming test problems, we used N = 2, 6, and n, where n is the problem dimension. Our test problems are of the following form</p><formula xml:id="formula_72">min x∈R n f (x) := x T Ax x T Bx , (<label>32</label></formula><formula xml:id="formula_73">)</formula><p>where B is a positive definite matrix and A and B are generated randomly. The minimum is attained at x * , a generalized eigenvector corresponding to the smallest eigenvalue of the generalized eigenvalue problem</p><formula xml:id="formula_74">Ax = λBx.</formula><p>The optimal value is equal to λ 1 (B -1/2 AB -1/2 ).</p><p>To solve each problem we used the TR method described by Algorithm 8.1 on the same machine as in Section 8.1. In this algorithm, f represents the function to be minimized, x j is an approximation of a minimizer after j iterations and s j is the radius of the TR at iteration j . </p><formula xml:id="formula_75">∇ f (x j ) 1 + | f (x j )| &lt; gradtol.<label>( 33)</label></formula><p>2. Find δ j to a given tolerance in the TRS</p><formula xml:id="formula_76">δ j ∈ argmin q j (δ) := ∇ f (x j ) T δ + 1 2 δ T ∇ 2 f (x j )δ s.t. δ 2 ≤ s 2 j .</formula><p>(34)</p><p>3. Evaluate r j = ( f (x j )f (x j + δ j ))/(q j (0)q j (δ j )).</p><p>4. (a) If r j &gt; 0.95, set s j +1 = 2s j and x j +1 = x j + δ j .</p><p>(b) If 0.01 ≤ r j &lt; 0.95, set s j +1 = s j and x j +1 = x j + δ j .</p><p>(c) If r j &lt; 0.01, set s j +1 = 0.5s j and x j +1 = x j .</p><p>Except for the stopping criteria <ref type="bibr" target="#b32">(33)</ref> which has been scaled here, this algorithm is Algorithm 6.1 in Ref. <ref type="bibr" target="#b11">[12]</ref>. We chose x 0 randomly and have fixed s 0 = 1, gradtol = 10 -2 . We ran five random problems for each problem size n = 20, 25, and 30, where n is the size of the square matrices A and B. If the RW algorithm solves Eq. ( <ref type="formula">34</ref>) and the solution is on the boundary of the TR, we stop if the duality gap, dgaptol, (between TRS and Eq. ( <ref type="formula" target="#formula_49">25</ref>)) satisfies dgaptol ≤ min{0.1, max{10 -8 , 10 -5 ∇ f (x j ) }}.</p><p>Otherwise, the solution is in the interior and we stop with an approximate solution δ j which satisfies ∇ f (x j ) + ∇ 2 f (x j )δ j ≤ min{0.1, max{10 -8 , 10 -5 ∇ f (x j ) }},</p><p>If the GLTR algorithm is used, we stop within this algorithm if N iterations have been done after knowing the solution lies on the boundary of the TR (see Ref. <ref type="bibr" target="#b11">[12]</ref>) or if</p><formula xml:id="formula_79">(A -λ k I )x k -a &lt; min{0.1, max{10 -8 , 10 -5 ∇ f (x j ) }}<label>(37)</label></formula><p>(see Eq. ( <ref type="formula">30</ref>)) or Eq. ( <ref type="formula" target="#formula_78">36</ref>) is satisfied, depending if the solution is on the boundary of the TR or not. Equations ( <ref type="formula" target="#formula_77">35</ref>) and ( <ref type="formula" target="#formula_79">37</ref>) yields approximately the same accuracy in terms of the duality gap. Recall from Section 6.2 that (A -λ k I )x ka is an approximation for the square root of the duality gap between TRS and its dual <ref type="bibr" target="#b27">(28)</ref>. The stopping criteria <ref type="bibr" target="#b34">(35)</ref> and <ref type="bibr" target="#b36">(37)</ref> are set to reflect this relationship.</p><p>For each problem, we give the number of iterations (iter) taken by the TR method <ref type="bibr" target="#b31">(32)</ref>. If the GLTR algorithm is used to solve the TRS (34), we give as well the number of iterations within Algorithm 8.1 where the GLTR algorithm failed to solve Eq. ( <ref type="formula">34</ref>) because it was unable to solve the restricted problem <ref type="bibr" target="#b17">(18)</ref>. This last output (hc2) is an indicator of the (almost) hard case (case 2).</p><p>The results given in Tables III-V, show that the (almost) hard case (case 2) occurs in many problems and Algorithm 8.1, using the RW algorithm for Eq. ( <ref type="formula">34</ref>), takes fewer iterations to converge compared to the GLTR algorithm. This is independent of N. This suggests that handling the hard case (case 2) should be an essential feature for a robust TR method. However, we reach the same conclusions mentioned in Ref. <ref type="bibr" target="#b11">[12]</ref>   occur. We observe the surprising fact that inexact solutions may indeed lead to less iterations in some cases. This may be due to the fact that the TR becomes inactive early and Newton's method takes over. This clearly requires more study.</p><p>As we may expect, when the hard case (case 2) does not occur and N = n, a TR method, using either the RW algorithm or the GLTR algorithm to solve the TRS <ref type="bibr" target="#b33">(34)</ref>, takes more or less the same iter. This should be the case since we are asking for the same accuracy in Eqs. ( <ref type="formula" target="#formula_77">35</ref>) and (37).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Accuracy of Trust Region Subproblem in a Trust Region Method</head><p>Inexact Newton methods can obtain q-superlinear and even q-quadratic convergence rates if the accuracy of the Newton equation increases appropriately as the iterates approach a stationary point, e.g., Ref. <ref type="bibr" target="#b5">[6]</ref>. Trust region methods such as Algorithm 8.1 are expected to reduce to Newton's method asymptotically, i.e., the TR constraint is expected to become inactive for most problems. This happens for example when the second order sufficient optimality conditions (positive definite Hessian) holds at the limit point. In either case, i.e., whether or not the TR constraint becomes inactive, the accuracy for solving TRS must increase as we approach the stationary point. We now investigate the number of iterations the TR Algorithm 8.1 takes to solve an unconstrained minimization problem as the accuracy of the solutions of the TRS (34) varies using MATLAB 6.5 on a Sun Fire 280R (UltraSPARC-III) with 2 GIGs RAM.</p><p>We solve the problem min x∈R n f (x) to accuracy given by varying values of gradtol in the inequality Eq. ( <ref type="formula" target="#formula_75">33</ref>). The TRS (34) is solved using the modified RW algorithm to accuracy dgaptol = max tol, 10 -6 min 1,  for varying values of tol. We also terminate Algorithm 8.1 if more than 1000 iterations are necessary or if the TR radius s j becomes smaller than 10 -10 (this case is indicated by s ∼ = 0). The results for our two examples are given in Tables <ref type="table" target="#tab_7">VI</ref> and<ref type="table" target="#tab_8">VII</ref>, where the entries are the iter taken by Algorithm 8.1.</p><formula xml:id="formula_80">∇ f (x j ) 1 + | f (x j )| 1/2 , (<label>38</label></formula><formula xml:id="formula_81">)</formula><p>From these results we observe two things. First, as the accuracy on the norm of the gradient is decreasing, the TR method (32) using low accuracy solutions for the TRS is eventually outperformed by higher accuracy solutions. Second, the results of Table VI indicate, for a fixed tolerance gradtol on the norm of the gradient, that more accurate solution of the TRS does not necessarily imply fewer iterations. Therefore, for robustness and as for inexact Newton methods, it is beneficial to use increased accuracy for the TRS when approaching the minimum of the objective function f .    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Large Sparse Trust Region Subproblem</head><p>The results of Table <ref type="table" target="#tab_8">VIII</ref> show how we used the RW algorithm to solve problems of size n = 100,000 of different density. Precisely, each row in this table corresponds to the density of the problems (for example, if the density is 10 -6 , then at most n 2 × 10 -6 entries in the matrix A are nonzero) and each column to the value of the parameter dgaptol. In each entry of the table we give the average taken over five random TR subproblems of the cpu seconds, the number of matrix-vector multiplications (mvm) and the number of iterations taken by the RW algorithm to find an approximate solution. We have been using for this section MATLAB 6.1 on a Pentium III with 4 GIGs RAM.</p><p>As we may expect, the cpu and the mvm increase as the density increases and the duality gap tolerance decreases. Furthermore, considering the reasonable length of the cpu taken to solve such TRS, we conclude that it is now within our reach to use TR methods to minimize functions with hundreds of thousands of variables assuming the Hessian has a sparse structure. In Table <ref type="table" target="#tab_10">IX</ref>, we considered a TRS of size n = 10 6 with 11 million nonzeros in the sparse matrix A. The results show that higher accuracy solutions require minimal extra iterations of the RW algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this article, we have studied the TRS with emphasis on robustness and solving large sparse problems. We focused on three dual based algorithms: the classical MS algorithm and the recent RW and GLTR algorithms, designed to solve large and sparse TRS.</p><p>We also studied many duals to TRS which can be formulated as semidefinite programs. We have seen how SDP arises naturally for TRS and provides a clear and simple unifying analysis between the different algorithms. In addition, this framework provides insights to the strengths and weaknesses of the algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Downloaded by [McMaster University] at 10:55 23 December 2014</head><p>In addition, we presented a modified/enhanced RW algorithm with new heuristics and techniques, in particular for taking a primal step to the boundary. However, the main improvement came from a new way of treating the (near) hard case based on Lemma 2.1. Surprisingly, the lemma shows that for each TRS, it is possible to consider an equivalent TRS where the hard case (case 2) does not occur.</p><p>Our final section included numerics which showed the advantage of using the modified RW algorithm over the MS algorithm in treating the hard case when high accuracy approximations are needed. We have also shown that handling the hard case in the TRS within a TR method may have an impact on the total iter if the hard case occurs frequently enough. Thus, the robustness of a TRS algorithm is indeed an important feature, in particular when the TR constraint stays active close to the optimal solution. Finally, we showed it is possible to solve large sparse TRS to high accuracy with hundreds of thousands of variables in a small number of iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 .</head><label>3</label><figDesc>Let u * = (A -λ * I ) † a. Then there exists z ∈ N (A -λ * I ) such that (x * , λ * ), with x * = u * + z, solves TRS iff (u * , λ * -λ 1 (A)) solves TRS when A is replaced by A -λ 1 (A)I. Downloaded by [McMaster University] at 10:55 23 December 2014 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>)</head><label></label><figDesc>Downloaded by [McMaster University] at 10:55 23 December 2014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 1</head><label>1</label><figDesc>FIGURE 1 Newton's method with the secular function, φ(λ).</figDesc><graphic coords="10,135.72,467.96,232.75,184.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 21 )</head><label>21</label><figDesc>Downloaded by [McMaster University] at 10:55 23 December 2014 hold. (This can be seen by moving in a direction of an eigenvector corresponding to a negative eigenvalue or, if A -λI 0, a / ∈ R(A -λI ), then moving in a direction d ∈ N (A -λI ) such that d T a &gt; 0.) This yields the equivalent dual problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 27 ) 1 -</head><label>271</label><figDesc>Downloaded by [McMaster University] at 10:55 23 December 2014 where E 00 is the zero matrix except for 1 in the top left corner. Because Slater's constraint qualification holds for this problem, one can take the Lagrangian dual and get a semidefinite equivalent for TRS = q * = min trace D(0)Y s.t. trace Y = s 2 + Y 00 = -1 Y 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>THEOREM 5 . 2</head><label>52</label><figDesc>The three programs:<ref type="bibr" target="#b24">(25)</ref>, (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>COROLLARY 5 . 2</head><label>52</label><figDesc>The SDP<ref type="bibr" target="#b27">(28)</ref> has a rank one optimal solution Y * . Proof Let x * be an optimum for TRS and y * = 1x * , Y * = y * (y * ) T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>31 )</head><label>31</label><figDesc>Downloaded by [McMaster University] at 10:55 23 December 2014 becomes small. When the λ k are feasible for Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>λ 1 (</head><label>1</label><figDesc>D(t)) changes. The following theorem, based on Ref. [11, Proposition 8, Lemmas 9 and 15], tells us when this happens. THEOREM 7.1 Let A = P P T be an orthogonal diagonalization of A. Let λ 1 (A) have multiplicity i and define t 0 := λ 1 (A) + j ∈{k|(P T a) k =0} (P T a) 2 j λ j (A) -λ 1 (A) . Downloaded by [McMaster University] at 10:55 23 December 2014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Downloaded by [McMaster University] at 10:55 23 December 2014</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>The three different cases for the trust region subproblem. We include two subcases (i) and (ii) for the hard case (case 2)</figDesc><table><row><cell>1. Easy case</cell><cell>2. (a) Hard case (case 1)</cell><cell>2. (b) Hard case (case 2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• ITERATION LOOP: (until convergence to the desired tolerance or until we find the solution is the unconstrained minimizer) 1. FIND a NEW VALUE of t. (a) Set t using Newton's method on k(t) -M t = 0 if the iterate falls into the interval of uncertainty for t; otherwise set it to the the midpoint (default) of the interval of uncertainty. (b) If points from the hard and easy side are available: i. Do TRIANGLE INTERPOLATION (Update upper bound on q * and set t, if Downloaded by [McMaster University] at 10:55 23 December 2014</figDesc><table><row><cell>possible.)</cell></row><row><cell>ii. Do VERTICAL CUT (Update lower or upper bound for interval of uncertainty</cell></row><row><cell>for t.)</cell></row><row><cell>(c) Do INVERSE INTERPOLATION (Set t, if possible)</cell></row><row><cell>2. UPDATE</cell></row></table><note><p>(a) With new t, compute (with restarts using a previous eigenvector) λ = λ 1 (D(t)) and corresponding eigenvector y with y 0 ≥ 0. (Use y orthogonal to the vectors in Y, if possible.) (b) If λ &gt; 0 and y 2 0 &gt; 1/(s 2 + 1) then the solution is the unconstrained minimizer. Use Conjugate Gradients and EXIT. (c) Update bounds on interval of uncertainty of q * . (d) i. If y 0 is small, then deflate, i.e., add y to Y.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II</head><label>II</label><figDesc>Given x j and s j , calculate ∇ f (x j ) and ∇ 2 f (x j ). Stop if</figDesc><table><row><cell cols="2">ALGORITHM 8.1 Trust Region Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>1.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Downloaded by [McMaster University] at 10:55 23 December 2014</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Modified-RW and MS algorithms; hard case (case 2(ii))</cell></row><row><cell>Dim. (n)</cell><cell>MS iters</cell><cell>RW iters</cell><cell>MS cpu</cell><cell>RW cpu</cell></row><row><cell>40</cell><cell>36.4</cell><cell>6.4</cell><cell>0.79</cell><cell>0.55</cell></row><row><cell>80</cell><cell>34.4</cell><cell>7.6</cell><cell>1.0</cell><cell>0.57</cell></row><row><cell>160</cell><cell>39.2</cell><cell>7.2</cell><cell>6.49</cell><cell>0.61</cell></row><row><cell>320</cell><cell>33.8</cell><cell>7.4</cell><cell>23.36</cell><cell>0.77</cell></row><row><cell>640</cell><cell>37.8</cell><cell>5.0</cell><cell>149.36</cell><cell>0.78</cell></row><row><cell>1280</cell><cell>-</cell><cell>7.6</cell><cell>-</cell><cell>2.06</cell></row><row><cell>2560</cell><cell>-</cell><cell>5.0</cell><cell>-</cell><cell>3.18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>when the hard case (case 2) does not Downloaded by [McMaster University] at 10:55 23 December 2014</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III The</head><label>III</label><figDesc>RW and GLTR; TR framework; size n = 20</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Algorithm used for solving the TRS (34)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RW</cell><cell cols="2">GLTR with N = 2</cell><cell cols="2">GLTR with N = 6</cell><cell cols="2">GLTR with N = n</cell></row><row><cell>Problem</cell><cell>iter</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell></row><row><cell>1</cell><cell>1 6</cell><cell>3 6</cell><cell>5</cell><cell>2 5</cell><cell>0</cell><cell>1 6</cell><cell>0</cell></row><row><cell>2</cell><cell>33</cell><cell>22</cell><cell>0</cell><cell>55</cell><cell>19</cell><cell>48</cell><cell>16</cell></row><row><cell>3</cell><cell>5 0</cell><cell>2 1</cell><cell>0</cell><cell>1 5</cell><cell>0</cell><cell>5 2</cell><cell>1 6</cell></row><row><cell>4</cell><cell>41</cell><cell>39</cell><cell>8</cell><cell>45</cell><cell>16</cell><cell>32</cell><cell>3</cell></row><row><cell>5</cell><cell>2 5</cell><cell>4 5</cell><cell>1 0</cell><cell>1 9</cell><cell>0</cell><cell>2 5</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV</head><label>IV</label><figDesc></figDesc><table><row><cell>Downloaded by [McMaster University] at 10:55 23 December 2014</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">The RW and GLTR; TR framework; size n = 25</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Algorithm used for solving the TRS (34)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RW</cell><cell cols="2">GLTR with N = 2</cell><cell cols="2">GLTR with N = 6</cell><cell cols="2">GLTR with N = n</cell></row><row><cell>Problem</cell><cell>iter</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell></row><row><cell>1</cell><cell>3 1</cell><cell>4 8</cell><cell>1 7</cell><cell>3 4</cell><cell>5</cell><cell>3 2</cell><cell>2</cell></row><row><cell>2</cell><cell>3 8</cell><cell>2 6</cell><cell>0</cell><cell>2 7</cell><cell>1</cell><cell>3 2</cell><cell>2</cell></row><row><cell>3</cell><cell>2 2</cell><cell>2 5</cell><cell>0</cell><cell>2 2</cell><cell>0</cell><cell>2 2</cell><cell>0</cell></row><row><cell>4</cell><cell>2 0</cell><cell>3 9</cell><cell>4</cell><cell>3 1</cell><cell>1</cell><cell>2 0</cell><cell>0</cell></row><row><cell>5</cell><cell>2 5</cell><cell>2 6</cell><cell>0</cell><cell>2 2</cell><cell>0</cell><cell>2 2</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE V The</head><label>V</label><figDesc>RW and GLTR; TR framework; size n = 30</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Algorithm used for solving the TRS (34)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">GLTR with N = 2</cell><cell cols="2">GLTR with N = 6</cell><cell cols="2">GLTR with N = n</cell></row><row><cell></cell><cell>RW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Problem</cell><cell>iter</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell><cell>iter</cell><cell>hc2</cell></row><row><cell>1</cell><cell>6 1</cell><cell>1 4</cell><cell>0</cell><cell>3 6</cell><cell>7</cell><cell>5 7</cell><cell>2 4</cell></row><row><cell>2</cell><cell>3 8</cell><cell>5 0</cell><cell>1 7</cell><cell>3 5</cell><cell>2</cell><cell>3 7</cell><cell>6</cell></row><row><cell>3</cell><cell>3 4</cell><cell>2 2</cell><cell>0</cell><cell>2 7</cell><cell>1</cell><cell>4 5</cell><cell>1 7</cell></row><row><cell>4</cell><cell>3 4</cell><cell>1 9</cell><cell>1 4</cell><cell>2 5</cell><cell>0</cell><cell>3 6</cell><cell>8</cell></row><row><cell>5</cell><cell>3 6</cell><cell>3 8</cell><cell>7</cell><cell>2 6</cell><cell>0</cell><cell>3 2</cell><cell>3</cell></row><row><cell cols="8">TABLE VI The TR on f (x) = sin(x 1 -1) + 1000 i=2 100 sin(x i -x 2 i-1 ) [see Ref. 40]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>gradtol</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tol</cell><cell>10 -3</cell><cell>10 -4</cell><cell>10 -5</cell><cell>10 -6</cell><cell>10 -7</cell><cell>10 -8</cell><cell>10 -9</cell></row><row><cell>10 -1</cell><cell>42</cell><cell>117</cell><cell>≥1000</cell><cell>≥1000</cell><cell>≥1000</cell><cell>≥1000</cell><cell>≥1000</cell></row><row><cell>10 -3</cell><cell>15</cell><cell>21</cell><cell>50</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell></row><row><cell>10 -5</cell><cell>12</cell><cell>16</cell><cell>18</cell><cell>88</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell></row><row><cell>10 -7</cell><cell>12</cell><cell>22</cell><cell>38</cell><cell>52</cell><cell>67</cell><cell>s ∼ = 0</cell><cell>s ∼ = 0</cell></row><row><cell>10 -9</cell><cell>12</cell><cell>16</cell><cell>18</cell><cell>22</cell><cell>24</cell><cell>50</cell><cell>s ∼ = 0</cell></row><row><cell>10 -11</cell><cell>12</cell><cell>22</cell><cell>38</cell><cell>53</cell><cell>68</cell><cell>83</cell><cell>96</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII The</head><label>VII</label><figDesc>TR on f (x) = 1000 i=1 6x 2 i + n i=503 x i + 998 i=1 (x i x i+2 -4x i x i+1 ) -3x 1 + x 2 + x 499 -3x 500 + 4x 501 [see Ref.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>TABLE VIII Modified RW algorithm on TRS; n = 100,000</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>dgaptol</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Density</cell><cell>10 -12</cell><cell></cell><cell>10 -10</cell><cell></cell><cell>10 -8</cell><cell></cell></row><row><cell>10 -8</cell><cell>cpu:</cell><cell>18.2</cell><cell>cpu:</cell><cell>14.5</cell><cell>cpu:</cell><cell>13.5</cell></row><row><cell></cell><cell>mvm:</cell><cell>185.6</cell><cell>mvm:</cell><cell>150.0</cell><cell>mvm:</cell><cell>140.0</cell></row><row><cell></cell><cell>iter:</cell><cell>6.2</cell><cell>iter:</cell><cell>5.4</cell><cell>iter:</cell><cell>4.8</cell></row><row><cell>10 -6</cell><cell>cpu:</cell><cell>21.1</cell><cell>cpu:</cell><cell>19.2</cell><cell>cpu:</cell><cell>20.0</cell></row><row><cell></cell><cell>mvm:</cell><cell>210.0</cell><cell>mvm:</cell><cell>196.0</cell><cell>mvm:</cell><cell>204.0</cell></row><row><cell></cell><cell>iter:</cell><cell>6.4</cell><cell>iter:</cell><cell>5.4</cell><cell>iter:</cell><cell>5.6</cell></row><row><cell>10 -4</cell><cell>cpu:</cell><cell>91.7</cell><cell>cpu:</cell><cell>78.8</cell><cell>cpu:</cell><cell>76.0</cell></row><row><cell></cell><cell>mvm:</cell><cell>341.6</cell><cell>mvm:</cell><cell>294.0</cell><cell>mvm:</cell><cell>276.0</cell></row><row><cell></cell><cell>iter:</cell><cell>5.8</cell><cell>iter:</cell><cell>5.0</cell><cell>iter:</cell><cell>5.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX Modified</head><label>IX</label><figDesc>RW algorithm on TRS; n = 1,000,000</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dgaptol</cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 -2</cell><cell>10 -4</cell><cell>10 -6</cell><cell>10 -8</cell><cell>10 -10</cell><cell>10 -12</cell></row><row><cell>cpu</cell><cell>985.1</cell><cell>985.1</cell><cell>1082.8</cell><cell>1082.8</cell><cell>1183.6</cell><cell>1282.9</cell></row><row><cell>mvm</cell><cell>240</cell><cell>240</cell><cell>260</cell><cell>260</cell><cell>280</cell><cell>300</cell></row><row><cell>i t e r</cell><cell>3</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>5</cell><cell>6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Downloaded by [McMaster University] at 10:55 23 December 2014</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors are indebted to Florian Jarre for many helpful discussions and suggestions. Research supported by a ES-A scholarship of the Natural Sciences and Engineering Research Council of Canada and a doctoral scholarship (B2) from the Fonds de recherche sur la nature et les technologies du Québec.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Then:</head><p>1. In the easy case, for all t ∈ R, λ 1 (D(t)) &lt; λ 1 (A) and λ 1 (D(t)) has multiplicity 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">In the hard case:</head><p>(a) for t &lt; t 0 , λ 1 (D(t)) &lt; λ 1 (A) and λ 1 (D(t)) has multiplicity 1; (b) for t = t 0 , λ 1 (D(t)) = λ 1 (A) and λ 1 (D(t)) has multiplicity </p><p>then, in the differentiable case,</p><p>i.e., this is equivalent to primal feasibility (cf. h (λ) = 0 in MS algorithm). To obtain conditions for y 0 (t) = 0, we use the following from Ref. <ref type="bibr" target="#b10">[11,</ref><ref type="bibr">Lemma 12,</ref><ref type="bibr">Lemma 15</ref>]. THEOREM 7.2 Let y(t) be a normalized eigenvector for λ 1 (D(t)) and let y 0 (t) be its first component. Then:</p><p>1. In the easy case: for t ∈ R, y 0 (t) = 0. 2. In the hard case: (a) for t &lt; t 0 : y 0 (t) = 0; (b) for t &gt; t 0 : there exists a basis of eigenvectors for the eigenspace of λ 1 (D(t)) such that each eigenvector in the basis has a zero first component (y 0 (t) = 0) and the vector composed of the last n components is an eigenvector for λ 1 (A); (c) for t = t 0 : there exists a basis of eigenvectors for the eigenspace of λ 1 (D(t 0 )), such that one eigenvector of this basis, ω, has a nonzero first component (ω 0 = 0) and each of the other eigenvectors in the basis has a zero first component (y 0 (t) = 0) and the vector composed of the last n components is an eigenvector for λ 1 (A).</p><p>It is known that the function λ 1 (D(t)) is differentiable at points where the multiplicity of the eigenvalue is 1. Its derivative is given by y 0 (t) 2 , where y(t) is a normalized eigenvector for λ 1 (D(t)), i.e., y(t) = 1 [see Ref <ref type="bibr">. 36]</ref>. Therefore, Theorems 7.1 and 7.2 yield the following. COROLLARY 7.2 1. In the easy case: k(•) is differentiable and k (t) = (s 2 + 1)y 0 (t) 2 -1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">In the hard case:</head><p>(a) for t &lt; t 0 , k(•) is differentiable and k (t) = (s 2 + 1)y 0 (t) </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey of the trust region subproblem within a semidefinite framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
		<idno>CORR 2002-22</idno>
		<ptr target="orion.math.uwaterloo.ca:80/hwolkowi/henry/reports/ABSTRACTS.html#surveytrs" />
		<imprint>
			<date type="published" when="2002">2002</date>
			<pubPlace>Waterloo, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Conn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trust-Region Methods. Society for Industrial and Applied Mathematics (SIAM)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Solutions of Ill-Posed Problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Y</forename><surname>Arsenin</surname></persName>
		</author>
		<editor>V.H. Winston &amp; Sons, John Wiley &amp; Sons, Washington DC (</editor>
		<imprint>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>Translation editor Fritz John</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ridge regression: biased estimation of nonorthogonal problems</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Hoerl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Kennard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Practical Methods of Optimization, 2nd ed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley and Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic preconditioning by limited memory quasi-Newton updating</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1079" to="1096" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A numerical study of the limited memory BFGS method and the truncated-Newton method for large scale optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="358" to="372" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Numerical Methods for Least Squares Problems</title>
		<author>
			<persName><forename type="first">Å</forename><surname>Björck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>SIAM), Philadelphia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing a trust region step</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Moré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Stat. Comp</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="553" to="572" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A semidefinite framework for trust region subproblems with applications to large scale minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Rendl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Series B</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="273" to="299" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Solving the trust-region subproblem using the lanczos method</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">I M</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optimiz</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="504" to="525" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Computing optimal locally constrained steps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Minimization of a large-scale quadratic function subject to a spherical constraint</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optimiz</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="161" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Difference of convex functions optimization algorithms (dca) for globally minimizing nonconvex quadratic forms on euclidean balls and spheres</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">T H</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oper. Res. Lett</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="207" to="216" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Minimizing a quadratic over a sphere</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Hager</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Gainsville, FA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining binary search and Newton&apos;s method to compute real roots for a class of real functions</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Complexity</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="271" to="280" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computing optimal locally constrained steps</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Stat. Comp</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="186" to="197" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Newton&apos;s method with a model trust region modification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Sorensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="409" to="426" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Applied Numerical Linear Algebra</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Demmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics (SIAM)</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hidden convexity in some nonconvex quadratically constrained quadratic programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Programming</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Theory of Convex Programming</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">G</forename><surname>Gol'stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1972">1972</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Providence, RI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Introduction to Sensitivity and Stability Analysis in Nonlinear Programming</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Fiacco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<date type="published" when="1983">1983. 1983</date>
			<publisher>Academic Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Indefinite trust region subproblems and nonsymmetric eigenvalue perturbations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optimiz</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="313" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Smoothing by spline functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="177" to="183" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Smoothing by spline functions ii</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reinsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="451" to="454" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An algorithm for minimization using exact second derivatives</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Reinsch</surname></persName>
		</author>
		<idno>515</idno>
		<imprint>
			<date type="published" when="1973">1973</date>
			<pubPlace>Harwell Laboratory, Harwell, Oxfordshire, England</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the stationary values of a second-degree polynomial on the unit sphere</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Forsythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Soc. Indust. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1050" to="1068" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The conjuguate gradient method and trust regions in large scale optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Steihaug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards an efficient sparsity exploiting Newton method for minimization</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Toint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sparse Matrices and Their Uses</title>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Duff</surname></persName>
		</editor>
		<imprint>
			<publisher>Academic Press Edition</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="57" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Handbook of Semidefinite Programming: Theory, Algorithms, and Applications</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wolkowicz</surname></persName>
		</author>
		<editor>R. Saigal and L. Vandenberghe</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<biblScope unit="page">654</biblScope>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Linear Programming: Foundations and Extensions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vanderbei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Kluwer Acad. Publ</publisher>
			<pubPlace>Dordrecht</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Primal-Dual Interior-Point Methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics (SIAM)</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<pubPlace>Philadelphia, PA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A spectral bundle method for semidefinite programming</title>
		<author>
			<persName><forename type="first">C</forename><surname>Helmberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rendl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optimiz</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="673" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Solving large-scale sparse semidefinite programs for combinatorial optimization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Benson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="461" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The Newton bracketing method for convex minimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ben-Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optimiz. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="213" to="229" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A survey of the trust region subproblem within a semidefinite framework</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fortin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Truncated Newton algorithms for large scale unconstrained optimization</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Steihaug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">72</biblScope>
			<biblScope unit="page" from="190" to="212" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Bouriacha</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Private communication</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Iterative methods for large convex quadratic programs: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Control Optim</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
