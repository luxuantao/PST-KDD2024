<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 TINYBERT: DISTILLING BERT FOR NATURAL LAN-GUAGE UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 TINYBERT: DISTILLING BERT FOR NATURAL LAN-GUAGE UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, the pre-trained language models are usually computationally expensive and memory intensive, so it is difficult to effectively execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we firstly propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large "teacher" BERT can be well transferred to a small "student" TinyBERT. Moreover, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT 1 is empirically effective and achieves more than 96% the performance of teacher BERT BASE on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT is also significantly better than state-of-the-art baselines on BERT distillation, with only ∼28% parameters and ∼31% inference time of them.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-training language models then fine-tuning on downstream tasks has become a new paradigm for natural language processing (NLP). Pre-trained language models (PLMs), such as BERT <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>, XLNet <ref type="bibr" target="#b28">(Yang et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b15">(Liu et al., 2019)</ref> and SpanBERT <ref type="bibr" target="#b12">(Joshi et al., 2019)</ref>, have achieved great success in many NLP tasks (e.g., the GLUE benchmark <ref type="bibr" target="#b24">(Wang et al., 2018)</ref> and the challenging multi-hop reasoning task <ref type="bibr" target="#b5">(Ding et al., 2019)</ref>). However, PLMs usually have an extremely large number of parameters and need long inference time, which are difficult to be deployed on edge devices such as mobile phones. Moreover, recent studies <ref type="bibr" target="#b14">(Kovaleva et al., 2019)</ref> also demonstrate that there is redundancy in PLMs. Therefore, it is crucial and possible to reduce the computational overhead and model storage of PLMs while keeping their performances.</p><p>There has been many model compression techniques <ref type="bibr" target="#b8">(Han et al., 2015a)</ref> proposed to accelerate deep model inference and reduce model size while maintaining accuracy. The most commonly used techniques include quantization <ref type="bibr" target="#b7">(Gong et al., 2014)</ref>, weights pruning <ref type="bibr" target="#b9">(Han et al., 2015b)</ref>, and knowledge distillation (KD) <ref type="bibr" target="#b19">(Romero et al., 2014)</ref>. In this paper we focus on knowledge distillation, an idea proposed by <ref type="bibr" target="#b10">Hinton et al. (2015)</ref> in a teacher-student framework. KD aims to transfer the knowledge embedded in a large teacher network to a small student network. The student network is trained to reproduce the behaviors of the teacher network. Based on the framework, we propose a novel distillation method specifically for Transformer-based models <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref>, and use BERT as an example to investigate the KD methods for large scale PLMs.</p><p>KD has been extensively studied in NLP <ref type="bibr" target="#b13">(Kim &amp; Rush, 2016;</ref><ref type="bibr" target="#b11">Hu et al., 2018)</ref>, while designing KD methods for BERT has been less explored. The pre-training-then-fine-tuning paradigm firstly pre-trains BERT on a large scale unsupervised text corpus, then fine-tunes it on task-specific dataset, which greatly increases the difficulty of BERT distillation. Thus we are required to design an ef-fective KD strategy for both stages. To build a competitive TinyBERT, we firstly propose a new Transformer distillation method to distill the knowledge embedded in teacher BERT. Specifically, we design several loss functions to fit different representations from BERT layers: 1) the output of the embedding layer; 2) the hidden states and attention matrices derived from the Transformer layer; 3) the logits output by the prediction layer. The attention based fitting is inspired by the recent findings <ref type="bibr" target="#b3">(Clark et al., 2019)</ref> that the attention weights learned by BERT can capture substantial linguistic knowledge, which encourages that the linguistic knowledge can be well transferred from teacher BERT to student TinyBERT. However, it is ignored in existing KD methods of BERT, such as Distilled BiLSTM SOFT <ref type="bibr" target="#b22">(Tang et al., 2019)</ref>, BERT-PKD <ref type="bibr" target="#b21">(Sun et al., 2019)</ref> and DistilBERT<ref type="foot" target="#foot_1">2</ref> . Then, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation. At the general distillation stage, the original BERT without fine-tuning acts as the teacher model. The student TinyBERT learns to mimic the teacher's behavior by executing the proposed Transformer distillation on the large scale corpus from general domain. We obtain a general TinyBERT that can be fine-tuned for various downstream tasks. At the task-specific distillation stage, we perform the data augmentation to provide more task-specific data for teacherstudent learning, and then re-execute the Transformer distillation on the augmented data. Both the two stages are essential to improve the performance and generalization capability of TinyBERT. A detailed comparison between the proposed method and other existing methods is summarized in Table <ref type="table" target="#tab_0">1</ref>. The Transformer distillation and two-stage learning framework are two key ideas of the proposed method. TinyBERT (our method)</p><p>The main contributions of this work are as follows: 1) We propose a new Transformer distillation method to encourage that the linguistic knowledge encoded in teacher BERT can be well transferred to TinyBERT. 2) We propose a novel two-stage learning framework with performing the proposed Transformer distillation at both the pre-training and fine-tuning stages, which ensures that Tiny-BERT can capture both the general-domain and task-specific knowledge of the teacher BERT. 3) We show experimentally that our TinyBERT can achieve more than 96% the performance of teacher BERT BASE on GLUE tasks, while having much fewer parameters (∼13.3%) and less inference time (∼10.6%), and significantly outperforms other state-of-the-art baselines on BERT distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We firstly describe the formulation of Transformer <ref type="bibr" target="#b23">(Vaswani et al., 2017)</ref> and Knowledge Distillation <ref type="bibr" target="#b10">(Hinton et al., 2015)</ref>. Our proposed Transformer distillation is a specially designed KD method for Transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">TRANSFORMER LAYER</head><p>Most of the recent pre-trained language models (e.g., BERT, XLNet and RoBERTa) are built with Transformer layers, which can capture long-term dependencies between input tokens by selfattention mechanism. Specifically, a standard Transformer layer includes two main sub-layers: multi-head attention (MHA) and fully connected feed-forward network (FFN).</p><p>Multi-Head Attention (MHA). The calculation of attention function depends on the three components of queries, keys and values, which are denoted as matrices Q, K and V respectively. The attention function can be formulated as follows:</p><formula xml:id="formula_0">A = QK T √ d k ,<label>(1)</label></formula><formula xml:id="formula_1">Attention(Q, K, V ) = softmax(A)V ,<label>(2)</label></formula><p>where d k is the dimension of keys and acts as a scaling factor, A is the attention matrix calculated from the compatibility of Q and K by dot-product operation. The final function output is calculated as a weighted sum of values V , and the weight is computed by applying softmax() operation on the each column of matrix A. According to <ref type="bibr" target="#b3">Clark et al. (2019)</ref>, the attention matrices in BERT can capture substantial linguistic knowledge, and thus play an essential role in our proposed distillation method.</p><p>Multi-head attention is defined by concatenating the attention heads from different representation subspaces as follows:</p><formula xml:id="formula_2">MultiHead(Q, K, V ) = Concat(head 1 , . . . , head h )W , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where h is the number of attention heads, and head i denotes the i-th attention head, which is calculated by the Attention() function with inputs from different representation subspaces, the matrix W acts as a linear transformation.</p><p>Position-wise Feed-Forward Network (FFN). Transformer layer also contains a fully connected feed-forward network, which is formulated as follows:</p><formula xml:id="formula_4">FNN(x) = max(0, xW 1 + b 1 )W 2 + b 2 .<label>(4)</label></formula><p>We can see that the FFN contains two linear transformations and one ReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KNOWLEDGE DISTILLATION</head><p>KD aims to transfer the knowledge of a large teacher network T to a small student network S. The student network is trained to mimic the behaviors of teacher networks. Let f T and f S represent the behavior functions of teacher and student networks, respectively. The behavior function targets at transforming network inputs to some informative representations, and it can be defined as the output of any layer in the network. In the context of Transformer distillation, the output of MHA layer or FFN layer, or some intermediate representations (such as the attention matrix A) can be used as behavior function. Formally, KD can be modeled as minimizing the following objective function:</p><formula xml:id="formula_5">L KD = x∈X L f S (x), f T (x) ,<label>(5)</label></formula><p>where L(•) is a loss function that evaluates the difference between teacher and student networks, x is the text input and X denotes the training dataset. Thus the key research problem becomes how to define effective behavior functions and loss functions. Different from previous KD methods, we also need to consider how to perform KD at the pre-training stage of BERT in addition to the task-specific training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we propose a novel distillation method for Transformer-based models, and present a two-stage learning framework for our model distilled from BERT, which is called TinyBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRANSFORMER DISTILLATION</head><p>The proposed Transformer distillation is a specially designed KD method for Transformer networks. Figure <ref type="figure" target="#fig_0">1</ref> displays an overview of the proposed KD method. In this work, both the student and teacher networks are built with Transformer layers. For a clear illustration, we firstly formulate the problem before introducing our method. We set 0 to be the index of embedding layer and M +1 to be the index of prediction layer, and the corresponding layer mappings are defined as 0 = g(0) and N + 1 = g(M + 1) respectively. The effect of the choice of different mapping functions on the performances will be studied in the experiment section. Formally, the student can acquire knowledge from the teacher by minimizing the following objective:</p><formula xml:id="formula_6">L model = M +1 m=0 λ m L layer (S m , T g(m) ),<label>(6)</label></formula><p>where L layer refers to the loss function of a given model layer (e.g., Transformer layer or embedding layer) and λ m is the hyper-parameter that represents the importance of the m-th layer's distillation.</p><p>Transformer-layer Distillation. The proposed Transformer-layer distillation includes the attention based distillation and hidden states based distillation, which is shown in Figure <ref type="figure" target="#fig_0">1</ref> (b). The attention based distillation is motivated by the recent findings that attention weights learned by BERT can capture rich linguistic knowledge <ref type="bibr" target="#b3">(Clark et al., 2019)</ref>. This kind of linguistic knowledge includes the syntax and coreference information, which is essential for natural language understanding. Thus we propose the attention based distillation to encourage that the linguistic knowledge can be transferred from teacher BERT to student TinyBERT. Specifically, the student learns to fit the matrices of multihead attention in the teacher network, and the objective is defined as:</p><formula xml:id="formula_7">L attn = 1 h h i=1 MSE(A S i , A T i ), (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>where h is the number of attention heads, A i ∈ R l×l refers to the attention matrix corresponding to the i-th head of teacher or student, l is the input text length, and MSE() means the mean squared error loss function. In this work, the (unnormalized) attention matrix A i is used as the fitting target instead of its softmax output softmax(A i ), since our experiments show that the former setting has a faster convergence rate and better performances.</p><p>In addition to the attention based distillation, we also distill the knowledge from the output of Transformer layer (as shown in Figure <ref type="figure" target="#fig_0">1</ref> (b)), and the objective is as follows: Embedding-layer Distillation. We also perform embedding-layer distillation, which is similar to the hidden states based distillation and formulated as:</p><formula xml:id="formula_9">L hidn = MSE(H S W h , H T ),<label>(8)</label></formula><formula xml:id="formula_10">L embd = MSE(E S W e , E T ),<label>(9)</label></formula><p>where the matrices E S and H T refer to the embeddings of student and teacher networks, respectively. In this paper, they have the same shape as the hidden state matrices. The matrix W e is a linear transformation playing a similar role as W h .</p><p>Prediction-Layer Distillation. In addition to imitating the behaviors of intermediate layers, we also use the knowledge distillation to fit the predictions of teacher model <ref type="bibr" target="#b10">(Hinton et al., 2015)</ref>. Specifically, we penalize the soft cross-entropy loss between the student network's logits against the teacher's logits:</p><formula xml:id="formula_11">L pred = −softmax(z T ) • log softmax(z S / t),<label>(10)</label></formula><p>where z S and z T are the logits vectors predicted by the student and teacher respectively, log softmax() means the log likelihood, t means the temperature value. In our experiment, we find that t = 1 performs well.</p><p>Using the above distillation objectives (i.e. Equations 7, 8, 9 and 10), we can unify the distillation loss of the corresponding layers between the teacher and the student network:</p><formula xml:id="formula_12">L layer (S m , T g(m) ) =    L embd (S 0 , T 0 ), m = 0 L hidn (S m , T g(m) ) + L attn (S m , T g(m) ), M ≥ m &gt; 0 L pred (S M +1 , T N +1 ), m = M + 1<label>(11)</label></formula><p>In our experiments, we firstly perform intermediate layer distillation (M ≥ m ≥ 0), then perform the prediction-layer distillation (m = M + 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TINYBERT LEARNING</head><p>The application of BERT usually consists of two learning stages: the pre-training and fine-tuning.</p><p>The plenty of knowledge learned by BERT in the pre-training stage is of great importance and should also be transferred. Therefore, we propose a novel two-stage learning framework including the general distillation and the task-specific distillation, as illustrated in Figure <ref type="figure">2</ref>. General distillation helps student TinyBERT learn the rich knowledge embedded in teacher BERT, which plays an important role in improving the generalization capability of TinyBERT. The task-specific distillation teaches the student the task-specific knowledge. With the two-step distillation, we can further reduce the gap between teacher and student models.</p><p>General Distillation. In general distillation, we use the original BERT without fine-tuning as the teacher and a large-scale text corpus as the learning data. By performing the Transformer distillation<ref type="foot" target="#foot_4">5</ref> on the text from general domain, we obtain a general TinyBERT that can be fine-tuned for downstream tasks. However, due to the significant reductions of the hidden/embedding size and the layer number, general TinyBERT performs relatively worse than BERT.</p><p>Task-specific Distillation. Previous studies show that the complex models, fine-tuned BERTs, suffer from over-parametrization for domain-specific tasks <ref type="bibr" target="#b14">(Kovaleva et al., 2019)</ref>. Thus, it is possible for small models to achieve comparable performances to the BERTs. To this end, we propose to derive competitive fine-tuned TinyBERTs through the task-specific distillation. In the task-specific distillation, we re-perform the proposed Transformer distillation on an augmented task-specific dataset (as shown in Figure <ref type="figure">2</ref>). Specifically, the fine-tuned BERT is used as the teacher and a data augmentation method is proposed to expand the task-specific training set. Learning more task-related examples, the generalization capabilities of student model can be further improved. In this work, we combine a pre-trained language model BERT and GloVe <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> word embeddings to do word-level replacement for data augmentation. Specifically, we use the language model to predict word replacements for single-piece words <ref type="bibr" target="#b27">(Wu et al., 2019)</ref>, and use the word embeddings to retrieve the most similar words as word replacements for multiple-pieces words. Some hyperparameters are defined to control the replacement ratio of a sentence and the amount of augmented dataset. More details of the data augmentation procedure are discussed in Appendix A.</p><p>The above two learning stages are complementary to each other: the general distillation provides a good initialization for the task-specific distillation, while the task-specific distillation further improves TinyBERT by focusing on learning the task-specific knowledge. Although there is a big gap between BERT and TinyBERT in model size, by performing the proposed two-stage distillation, the TinyBERT can achieve competitive performances in various NLP tasks. The proposed Transformer distillation and two-stage learning framework are the two most important components of the proposed distillation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we evaluate the effectiveness and efficiency of TinyBERT on a variety of tasks with different model settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">MODEL SETUP</head><p>We instantiate a tiny student model (the number of layers M =4, the hidden size d =312, the feedforward/filter size d i =1200 and the head number h=12) that has a total of 14.5M parameters. If not specified, this student model is referred to as the TinyBERT. The original BERT BASE (the number of layers N =12, the hidden size d=768, the feed-forward/filter size d i =3072 and the head number h=12) is used as the teacher model that contains 109M parameters. We use g(m) = 3 × m as the layer mapping function, so TinyBERT learns from every 3 layers of BERT BASE . The learning weight λ of each layer is set to 1, which performs well for the learning of our TinyBERT. We evaluate TinyBERT on the General Language Understanding Evaluation (GLUE) <ref type="bibr" target="#b24">(Wang et al., 2018)</ref> benchmark, which is a collection of diverse natural language understanding tasks. The details of experiment settings are described in Appendix B. The evaluation results are presented in Table <ref type="table" target="#tab_2">2</ref> and the efficiencies of model size and inference time are also evaluated in Table <ref type="table" target="#tab_3">3</ref>.</p><p>The experiment results demonstrate that: 1) There is a large performance gap between BERT SMALL 6 and BERT BASE due to the big reduction in model size. 2) TinyBERT is consistently better than BERT SMALL in all the GLUE tasks and achieves a large improvement of 6.3% on average. This indicates that the proposed KD learning framework can effectively improve the performances of small models regardless of downstream tasks. 3) TinyBERT significantly outperforms the state-ofthe-art KD baselines (i.e., BERT-PKD and DistillBERT) by a margin of at least 3.9%, even with only ∼28% parameters and ∼31% inference time of baselines (see Table <ref type="table" target="#tab_3">3</ref>). 4) Compared with the teacher BERT BASE , TinyBERT is 7.5x smaller and 9.4x faster in the model efficiency, while maintaining competitive performances. 5) TinyBERT has a comparable model efficiency (slightly larger in size but faster in inference) with Distilled BiLSTM SOFT and obtains substantially better performances in all tasks reported by the BiLSTM baseline. 6) For the challenging CoLA dataset (the task of predicting linguistic acceptability judgments), all the distilled small models have a relatively bigger performance gap with teacher model. TinyBERT achieves a significant improvement over the strong baselines, and its performance can be further improved by using a deeper and wider model to capture more complex linguistic knowledge as illustrated in the next subsection.</p><p>Moreover, BERT-PKD and DistillBERT initialize their student models with some layers of well pre-trained teacher BERT (see Table <ref type="table" target="#tab_0">1</ref>), which makes the student models have to keep the same size settings of Transformer layer (or embedding layer) as their teacher BERT. In our two-stage distillation framework, TinyBERT is initialized by general distillation, so it has the advantage of being more flexible in model size selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">EFFECTS OF MODEL SIZE</head><p>We evaluate how much improvement can be achieved when increasing the model size of TinyBERT on several typical GLUE tasks, where MNLI and MRPC are used in the ablation studies of <ref type="bibr" target="#b4">Devlin et al. (2018)</ref>, and CoLA is the most difficult task in GLUE. Specifically, three wider and deeper variants are proposed and their evaluation results on development set are displayed in Table <ref type="table" target="#tab_4">4</ref>. We can observe that: 1) All the three TinyBERT variants can consistently outperform the original smallest TinyBERT, which indicates that the proposed KD method works for the student models of various model sizes. 2) For the CoLA task, the improvement is slight when only increasing the number of layers (from 49.7 to 50.6) or hidden size (from 49.7 to 50.5). To achieve more dramatic improvements, the student model should become deeper and wider (from 49.7 to 54.0). 3) Another interesting observation is that the smallest 4-layer TinyBERT can even outperform the 6-layers baselines, which further confirms the effectiveness of the proposed KD method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">ABLATION STUDIES</head><p>In this section, we conduct ablation studies to investigate the contributions of : 1) different procedures of the proposed two-stage TinyBERT learning framework (see Figure <ref type="figure">2</ref>), and 2) different distillation objectives (see Equation <ref type="formula" target="#formula_12">11</ref>). Effects of different learning procedures. The proposed two-stage TinyBERT learning framework (see Figure <ref type="figure">2</ref>) consists of three key procedures: TD (Task-specific Distillation), GD (General Distillation) and DA (Data Augmentation). The effects of different learning procedures are analyzed and presented in Table <ref type="table" target="#tab_5">5</ref>. The results indicate that all the three procedures are crucial for the proposed KD method. The TD and DA has comparable effects in all the four tasks. We can also find the task-specific procedures (TD and DA) are more helpful than the pre-training procedure (GD) in all the four tasks. Another interesting observation is that GD has more effect on CoLA than on MNLI and MRPC. We conjecture that the ability of linguistic generalization <ref type="bibr" target="#b25">(Warstadt et al., 2018)</ref> learned by GD plays a more important role in the downstream CoLA task (linguistic acceptability judgments).</p><p>Effects of different distillation objectives. We investigate the effects of distillation objectives on the TinyBERT learning. Several baselines are proposed including the TinyBERT learning without the Transformer-layer distillation (No Trm), embedding-layer distillation (No Emb) and predictionlayer distillation (No Pred)<ref type="foot" target="#foot_6">7</ref> respectively. The results are illustrated in Table <ref type="table" target="#tab_6">6</ref> and show that all the proposed distillation objectives are useful for the TinyBERT learning. The performance drops significantly from 75.3 to 56.3 under the setting (No Trm), which indicates Transformer-layer distillation is the key for TinyBERT learning. Furthermore, we study the contributions of attention (No Attn) and hidden states (No Hidn) in the Transformer-layer distillation. We can find the attention based distillation has a bigger effect than hidden states based distillation on TinyBERT learning. Meanwhile, these two kinds of knowledge distillation are complementary to each other, which makes TinyBERT obtain the competitive results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">EFFECTS OF MAPPING FUNCTION</head><p>We investigate the effects of different mapping functions n = g(m) on the TinyBERT learning. Our original TinyBERT as described in section 4.1 uses the uniform-strategy, and we compare with two typical baselines including top-strategy</p><formula xml:id="formula_13">(g(m) = m + N − M ; 0 &lt; m ≤ M ) and bottom-strategy (g(m) = m; 0 &lt; m ≤ M ).</formula><p>The comparison results are presented in Table <ref type="table" target="#tab_7">7</ref>. We find that the top-strategy performs better than the bottom-strategy in MNLI, while being worse in MRPC and CoLA tasks, which confirms the observations that different tasks depend on the different kinds of knowledge from BERT layers. Since the uniform-strategy acquires the knowledge from bottom to top layers of BERT BASE , it achieves better performances than the other two baselines in all the four tasks. Adaptively choosing layers for a specific task is a challenging problem and we leave it as the future work.</p><p>Other Experiments. We also evaluate TinyBERT on the question answering tasks, and study whether we can use BERT SMALL as the initialization of the general TinyBERT. The experiments are detailed in Appendix C and D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we firstly introduce a new KD method for Transformer-based distillation, then we further propose a two-stage framework for TinyBERT learning. Extensive experiments show that the TinyBERT achieves competitive performances meanwhile significantly reducing the model size and shortening the inference time of original BERT BASE , which provides an effective way to deploy BERT-based NLP applications on the edge devices.</p><p>In future work, we would study how to effectively transfer the knowledge from wider and deeper teachers (e.g., BERT LARGE and XLNet LARGE ) to student TinyBERT. The joint learning of distillation and quantization/pruning would be another promising direction to further compress the pre-trained language models. amples, we distill intermediate layer knowledge for 5 epochs with batch size 256 on the augmented dataset. Besides, for CoLA task, we perform 50 epochs of intermediate layer distillation.</p><p>Baselines setup. We use BERT-PKD and DistilBERT as our baselines. For a fair comparison, we firstly re-implemented the results of BERT-PKD and DistilBERT reported in their papers to ensure our implementation procedure is correct. Then following the verified implementation procedure, we trained a 4-layer BERT-PKD and a 4-layer DistilBERT as the baselines. The BERT SMALL learning strictly follows the same learning strategy as described in the original BERT work <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>.</p><p>Model efficiency evaluation. To evaluate the inference speed, we ran inference procedure on the QNLI training set with batch size of 128 and the maximum sequence length of 128. The numbers reported in Table <ref type="table" target="#tab_3">3</ref> are the average running time of 100 batches on a single NVIDIA K80 GPU.</p><p>The GLUE datasets are described as follows:</p><p>MNLI. Multi-Genre Natural Language Inference is a large-scale, crowd-sourced entailment classification task <ref type="bibr" target="#b26">(Williams et al., 2018)</ref>. Given a pair of premise, hypothesis , the goal is to predict whether the hypothesis is an entailment, contradiction, or neutral with respect to the premise.</p><p>QQP. Quora Question Pairs is a collection of question pairs from the website Quora. The task is to determine whether two questions are semantically equivalent <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>.</p><p>QNLI. Question Natural Language Inference is a version of the Stanford Question Answering Dataset which has been converted to a binary sentence pair classification task by <ref type="bibr" target="#b24">Wang et al. (2018)</ref>. Given a pair question, context . The task is to determine whether the context contains the answer to the question.</p><p>SST-2. The Stanford Sentiment Treebank is a binary single-sentence classification task, where the goal is to predict the sentiment of movie reviews <ref type="bibr" target="#b20">(Socher et al., 2013)</ref>.</p><p>CoLA. The Corpus of Linguistic Acceptability is a task to predict whether an English sentence is a grammatically correct one <ref type="bibr" target="#b25">(Warstadt et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STS-B.</head><p>The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines and many other domains <ref type="bibr" target="#b1">(Cer et al., 2017)</ref>. The task aims to evaluate how similar two pieces of texts are by a score from 1 to 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MRPC. Microsoft Research Paraphrase</head><p>Corpus is a paraphrase identification dataset where systems aim to identify if two sentences are paraphrases of each other <ref type="bibr" target="#b6">(Dolan &amp; Brockett, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RTE.</head><p>Recognizing Textual Entailment is a binary entailment task with a small training dataset <ref type="bibr" target="#b0">(Bentivogli et al., 2009)</ref>.</p><p>C SQUAD 1.1 AND 2.0</p><p>We also demonstrate the effectiveness of TinyBERT on the question answering (QA) tasks: SQuAD v1.1 <ref type="bibr" target="#b17">(Rajpurkar et al., 2016)</ref> and v2.0 <ref type="bibr" target="#b18">(Rajpurkar et al., 2018)</ref>. Following the learning procedure in the previous work <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>, we treat these two tasks as the problem of sequence labeling which predicts the possibility of each token as the start or end of answer span. We follow the settings of task-specific distillation in GLUE tasks, except with 3 running epochs and a learning rate of 5e-5 for the prediction-layer distillation on the original training dataset. The results are shown in Table <ref type="table" target="#tab_8">8</ref>.</p><p>The results show that TinyBERT consistently outperforms the baselines in both the small and medium size, which indicates that the proposed framework also works for the tasks of token-level labeling. Compared with sequence-level GLUE tasks, the question answering tasks depends on more subtle knowledge to infer the correct answer, which increases the difficulty of knowledge distillation. We leave how to build a better QA-TinyBERT as the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D BERT SMALL AS INITIALIZATION OF GENERAL TINYBERT</head><p>Initializing general TinyBERT with BERT SMALL is a straightforward idea. However, BERT SMALL would derive mismatched distributions in intermediate representations (e.g., attention matrices and hidden states) with the teacher BERT BASE model, if without imitating the teacher's behaviors at the pre-training stage. Further task-specific distillation under the supervision of finetuned BERT BASE will disturb the learned distribution/knowledge of BERT SMALL , finally leading to poor performances in some less-data tasks. The results in Table <ref type="table" target="#tab_9">9</ref>, show that the BERT SMALL (MLM&amp;NSP+TD) performs worse than the BERT SMALL in MRPC and CoLA tasks, which validates our hypothesis. For the intensive-data task (e.g. MNLI), TD has enough training data to make BERT SMALL acquire the task-specific knowledge very well, although the pre-trained distributions have already been disturbed.</p><p>To make TinyBERT effectively work for all tasks, we propose General Distillation (GD) for initialization, where the TinyBERT learns the knowledge from intermediate layers of teacher BERT at the pre-training stage. From the results of Table <ref type="table" target="#tab_9">9</ref>, we find that GD can effectively transfer the knowledge from the teacher BERT to the student TinyBERT and achieve comparable results with BERT SMALL (61.1 vs. 63.9), even without performing the MLM and NSP tasks. Furthermore, the task-specific distillation boosts the performances of TinyBERT by continuing on learning the task-specific knowledge of fine-tuned teacher BERT BASE .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE COMPLETE COMPARISONS WITH SAME STUDENT ARCHITECTURE</head><p>For the easy and direct comparisons with prior works, we here also present the results of TinyBERT (M =6;d =768;d i =3072) with the same architectures as used in the original BERT-PKD <ref type="bibr" target="#b21">(Sun et al., 2019)</ref> and DistilBERT 2 . Since in the original papers, the BERT-PKD is evaluated on the test set, and the DistilBERT is evaluated on the dev set. Thus, for a clear illustration, we present the results in the following Tables <ref type="table" target="#tab_11">10 and 11</ref>, separately.  Thus, from the direct comparisons with the reported results in the original papers, we can see the TinyBERT outperforms the baselines (DistilBERT and BERT-PKD) under the same settings of architecture and evaluation, the effectiveness of TinyBERT is confirmed. Moreover, since BERT-PKD and DistilBERT need to initialize their student models with some layers of pre-trained teacher BERT, they have the limitations that the student models have to keep the same size settings of hidden size and feedforward/filter size as their teacher BERT. TinyBERT is initialized by general distillation, so it has the advantage of being more flexible in model size selection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of Transformer distillation: (a) the framework of Transformer distillation, (b) the details of Transformer-layer distillation consisting of Attn loss (attention based distillation) and Hidn loss (hidden states based distillation).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A summary of KD methods for BERT. Abbreviations: INIT(initializing student BERT with some layers of pre-trained teacher BERT), DA(conducting data augmentation for task-specific training data). Embd, Attn, Hidn, and Pred represent the knowledge from embedding layers, attention matrices, hidden states, and final prediction layers, respectively.</figDesc><table><row><cell>KD Methods</cell><cell>KD at Pre-training Stage INIT Embd Attn Hidn Pred Embd Attn Hidn Pred DA KD at Fine-tuning Stage</cell></row><row><cell>Distilled BiLSTM SOFT</cell><cell></cell></row><row><cell>BERT-PKD</cell><cell>3</cell></row><row><cell>DistilBERT</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>S ∈ R l×d and H T ∈ R l×d refer to the hidden states of student and teacher networks respectively, which are calculated by Equation4. The scalar values d and d denote the hidden sizes of teacher and student models, and d is often smaller than d to obtain a smaller student network. The matrix W h ∈ R d ×d is a learnable linear transformation, which transforms the hidden states of student network into the same space as the teacher network's states.</figDesc><table><row><cell></cell><cell>General Learning</cell><cell></cell><cell></cell><cell></cell><cell>Task-specific Learning</cell></row><row><cell>Text Corpus Large-scale</cell><cell cols="2">Transformer Distillation</cell><cell>TinyBERT General</cell><cell cols="2">Transformer Distillation</cell><cell>TinyBERT Fine-tuned</cell></row><row><cell></cell><cell>Task Dataset</cell><cell cols="3">Data Augmentation</cell><cell>Task Dataset Augmented</cell></row><row><cell></cell><cell cols="5">Figure 2: The illustration of TinyBERT learning</cell></row><row><cell>where the matrices H</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results are evaluated on the test set of GLUE official benchmark. All models are learned in a single-task manner. "-" means the result is not reported.</figDesc><table><row><cell>System</cell><cell cols="10">MNLI-m MNLI-mm QQP SST-2 QNLI MRPC RTE CoLA STS-B Average</cell></row><row><cell>BERT BASE (Google)</cell><cell>84.6</cell><cell>83.4</cell><cell>71.2</cell><cell>93.5</cell><cell>90.5</cell><cell>88.9</cell><cell>66.4</cell><cell>52.1</cell><cell>85.8</cell><cell>79.6</cell></row><row><cell>BERT BASE (Teacher)</cell><cell>83.9</cell><cell>83.4</cell><cell>71.1</cell><cell>93.4</cell><cell>90.9</cell><cell>87.5</cell><cell>67.0</cell><cell>52.8</cell><cell>85.2</cell><cell>79.5</cell></row><row><cell>BERT SMALL</cell><cell>75.4</cell><cell>74.9</cell><cell>66.5</cell><cell>87.6</cell><cell>84.8</cell><cell>83.2</cell><cell>62.6</cell><cell>19.5</cell><cell>77.1</cell><cell>70.2</cell></row><row><cell>Distilled BiLSTM SOFT</cell><cell>73.0</cell><cell>72.6</cell><cell>68.2</cell><cell>90.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>BERT-PKD</cell><cell>79.9</cell><cell>79.3</cell><cell>70.2</cell><cell>89.4</cell><cell>85.1</cell><cell>82.6</cell><cell>62.3</cell><cell>24.8</cell><cell>79.8</cell><cell>72.6</cell></row><row><cell>DistilBERT</cell><cell>78.9</cell><cell>78.0</cell><cell>68.5</cell><cell>91.4</cell><cell>85.2</cell><cell>82.4</cell><cell>54.1</cell><cell>32.8</cell><cell>76.1</cell><cell>71.9</cell></row><row><cell>TinyBERT</cell><cell>82.5</cell><cell>81.8</cell><cell>71.3</cell><cell>92.6</cell><cell>87.7</cell><cell>86.4</cell><cell>62.9</cell><cell>43.3</cell><cell>79.9</cell><cell>76.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The model sizes and inference time for baselines and TinyBERT. The number of layers does not include the embedding and prediction layers.</figDesc><table><row><cell>System</cell><cell cols="3">Layers Hidden Feed-forward</cell><cell>Model</cell><cell>Inference</cell></row><row><cell></cell><cell></cell><cell>Size</cell><cell>Size</cell><cell>Size</cell><cell>Time</cell></row><row><cell>BERTBASE (Teacher)</cell><cell>12</cell><cell>768</cell><cell>3072</cell><cell>109M(×1.0)</cell><cell>188s(×1.0)</cell></row><row><cell>Distilled BiLSTMSOFT</cell><cell>1</cell><cell>300</cell><cell>400</cell><cell cols="2">10.1M(×10.8) 24.8s(×7.6)</cell></row><row><cell>BERT-PKD/DistilBERT</cell><cell>4</cell><cell>768</cell><cell>3072</cell><cell>52.2M(×2.1)</cell><cell>63.7s(×3.0)</cell></row><row><cell>TinyBERT/BERTSMALL</cell><cell>4</cell><cell>312</cell><cell>1200</cell><cell>14.5M(×7.5)</cell><cell>19.9s(×9.4)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results (dev)  of wider or deeper TinyBERT variants and baselines.</figDesc><table><row><cell>System</cell><cell cols="4">MNLI-m MNLI-mm MRPC CoLA</cell><cell>Average</cell></row><row><cell>BERTBASE (Teacher)</cell><cell>84.2</cell><cell>84.4</cell><cell>86.8</cell><cell>57.4</cell><cell>78.2</cell></row><row><cell>BERT-PKD (M =6;d =768;d i =3072)</cell><cell>80.9</cell><cell>80.9</cell><cell>83.1</cell><cell>43.1</cell><cell>72.0</cell></row><row><cell>DistilBERT (M =6;d =768;d i =3072)</cell><cell>81.6</cell><cell>81.1</cell><cell>82.4</cell><cell>42.5</cell><cell>71.9</cell></row><row><cell>TinyBERT (M =4;d =312;d i =1200)</cell><cell>82.8</cell><cell>82.9</cell><cell>85.8</cell><cell>49.7</cell><cell>75.3</cell></row><row><cell>TinyBERT (M =4;d =768;d i =3072)</cell><cell>83.8</cell><cell>84.1</cell><cell>85.8</cell><cell>50.5</cell><cell>76.1</cell></row><row><cell>TinyBERT (M =6;d =312;d i =1200)</cell><cell>83.3</cell><cell>84.0</cell><cell>86.3</cell><cell>50.6</cell><cell>76.1</cell></row><row><cell>TinyBERT (M =6;d =768;d i =3072)</cell><cell>84.5</cell><cell>84.5</cell><cell>86.3</cell><cell>54.0</cell><cell>77.3</cell></row><row><cell cols="2">4.2 EXPERIMENTAL RESULTS ON GLUE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies of different procedures (i.e., TD, GD, and DA) of the two-stage learning framework. The variants are validated on the dev set.</figDesc><table><row><cell>System</cell><cell cols="4">MNLI-m MNLI-mm MRPC CoLA Average</cell></row><row><cell cols="2">TinyBERT 82.8</cell><cell>82.9</cell><cell>85.8 49.7</cell><cell>75.3</cell></row><row><cell>No GD</cell><cell>82.5</cell><cell>82.6</cell><cell>84.1 40.8</cell><cell>72.5</cell></row><row><cell>No TD</cell><cell>80.6</cell><cell>81.2</cell><cell>83.8 28.5</cell><cell>68.5</cell></row><row><cell>No DA</cell><cell>80.5</cell><cell>81.0</cell><cell>82.4 29.8</cell><cell>68.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies of different distillation objectives in the TinyBERT learning. The variants are validated on the dev set.</figDesc><table><row><cell>System</cell><cell cols="5">MNLI-m MNLI-mm MRPC CoLA Average</cell></row><row><cell>TinyBERT</cell><cell>82.8</cell><cell>82.9</cell><cell>85.8</cell><cell>49.7</cell><cell>75.3</cell></row><row><cell>No Embd</cell><cell>82.3</cell><cell>82.3</cell><cell>85.0</cell><cell>46.7</cell><cell>74.1</cell></row><row><cell>No Pred</cell><cell>80.5</cell><cell>81.0</cell><cell>84.3</cell><cell>48.2</cell><cell>73.5</cell></row><row><cell>No Trm</cell><cell>71.7</cell><cell>72.3</cell><cell>70.1</cell><cell>11.2</cell><cell>56.3</cell></row><row><cell>No Attn</cell><cell>79.9</cell><cell>80.7</cell><cell>82.3</cell><cell>41.1</cell><cell>71.0</cell></row><row><cell>No Hidn</cell><cell>81.7</cell><cell>82.1</cell><cell>84.1</cell><cell>43.7</cell><cell>72.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Results (dev)  of different mapping strategies.</figDesc><table><row><cell>System</cell><cell cols="5">MNLI-m MNLI-mm MRPC CoLA Average</cell></row><row><cell>TinyBERT (Uniform-strategy)</cell><cell>82.8</cell><cell>82.9</cell><cell>85.8</cell><cell>49.7</cell><cell>75.3</cell></row><row><cell>TinyBERT (Top-strategy)</cell><cell>81.7</cell><cell>82.3</cell><cell>83.6</cell><cell>35.9</cell><cell>70.9</cell></row><row><cell>TinyBERT (Bottom-strategy)</cell><cell>80.6</cell><cell>81.3</cell><cell>84.6</cell><cell>38.5</cell><cell>71.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Results (dev)  of baselines and TinyBERT on question answering tasks.</figDesc><table><row><cell>System</cell><cell cols="4">SQuAD 1.1 SQuAD 2.0</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>BERTBASE (Teacher)</cell><cell cols="4">80.7 88.4 73.1 76.4</cell></row><row><cell>Small Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-PKD(M =4;d =768;d i =3072)</cell><cell cols="4">70.1 79.5 60.8 64.6</cell></row><row><cell>DistilBERT(M =4;d =768;d i =3072)</cell><cell cols="4">71.8 81.2 60.6 64.1</cell></row><row><cell>TinyBERT(M =4;d =312;d i =1200)</cell><cell cols="4">72.7 82.1 65.3 68.8</cell></row><row><cell>Medium-sized Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">BERT-PKD (M =6;d =768;d i =3072) 77.1 85.3 66.3 69.8</cell></row><row><cell cols="5">DistilBERT (M =6;d =768;d i =3072) 78.1 86.2 66.0 69.5</cell></row><row><cell>TinyBERT (M =6;d =768;d i =3072)</cell><cell cols="4">79.7 87.5 69.9 73.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Results of different methods at pre-training state. TD and GD refers to Task-specific Distillation (without data augmentation) and General Distillation. The results are evaluated on development set.</figDesc><table><row><cell>System</cell><cell cols="4">MNLI-m MNLI-mm MRPC CoLA</cell><cell>Average</cell></row><row><cell></cell><cell>(392k)</cell><cell>(392k)</cell><cell>(3.5k)</cell><cell>(8.5k)</cell><cell></cell></row><row><cell>BERTSMALL (MLM&amp;NSP)</cell><cell>75.9</cell><cell>76.9</cell><cell>83.2</cell><cell>19.5</cell><cell>63.9</cell></row><row><cell>BERTSMALL (MLM&amp;NSP+TD)</cell><cell>79.2</cell><cell>79.7</cell><cell>82.9</cell><cell>12.4</cell><cell>63.6</cell></row><row><cell>TinyBERT (GD)</cell><cell>76.6</cell><cell>77.2</cell><cell>82.0</cell><cell>8.7</cell><cell>61.1</cell></row><row><cell>TinyBERT (GD+TD)</cell><cell>80.5</cell><cell>81.0</cell><cell>82.4</cell><cell>29.8</cell><cell>68.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparisons between TinyBERT and BERT-PKD, and the results are evaluated on the test set of official GLUE tasks.</figDesc><table><row><cell>System</cell><cell>SST-2</cell><cell>MRPC</cell><cell>QQP</cell><cell cols="3">MNLI-m MNLI-mm QNLI</cell><cell>RTE</cell></row><row><cell></cell><cell>(67k)</cell><cell>(3.7k)</cell><cell>(364k)</cell><cell>(393k)</cell><cell>(393k)</cell><cell cols="2">(105k) (2.5k)</cell></row><row><cell></cell><cell>Acc</cell><cell>F1/Acc</cell><cell>F1/Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Acc</cell></row><row><cell cols="5">Same Student Architecture (M =6;d =768;d i =3072)</cell><cell></cell><cell></cell></row><row><cell>BERT 6 -PKD</cell><cell>92.0</cell><cell cols="2">85.0/79.9 70.7/88.9</cell><cell>81.5</cell><cell>81.0</cell><cell>89.0</cell><cell>65.5</cell></row><row><cell>TinyBERT</cell><cell>93.1</cell><cell cols="2">87.3/82.6 71.6/89.1</cell><cell>84.6</cell><cell>83.2</cell><cell>90.4</cell><cell>66.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparisons between TinyBERT with DistilBERT, and the results are evaluated on the dev set of GLUE tasks. Mcc refers to Matthews correlation and Pear/Spea refer to Pearson/Spearman.</figDesc><table><row><cell>System</cell><cell cols="3">CoLA MNLI-m MNLI-mm</cell><cell>MRPC</cell><cell>QNLI</cell><cell>QQP</cell><cell>RTE</cell><cell>SST-2</cell><cell>STS-B</cell></row><row><cell></cell><cell>(8.5k)</cell><cell>(393k)</cell><cell>(393k)</cell><cell>(3.7k)</cell><cell>(105k)</cell><cell>(364k)</cell><cell cols="2">(2.5k) (67k)</cell><cell>(5.7k)</cell></row><row><cell></cell><cell>Mcc</cell><cell>Acc</cell><cell>Acc</cell><cell>F1/Acc</cell><cell>Acc</cell><cell>F1/Acc</cell><cell>Acc</cell><cell>Acc</cell><cell>Pear/Spea</cell></row><row><cell cols="5">Same Student Architecture (M =6;d =768;d i =3072)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DistillBERT</cell><cell>42.5</cell><cell>81.6</cell><cell>81.1</cell><cell>88.3/82.4</cell><cell>85.5</cell><cell cols="2">87.7/90.6 60.0</cell><cell>92.7</cell><cell>84.5/85.0</cell></row><row><cell>TinyBERT</cell><cell>54.0</cell><cell>84.5</cell><cell>84.5</cell><cell>90.6/86.3</cell><cell>91.1</cell><cell cols="2">88.0/91.1 70.4</cell><cell>93.0</cell><cell>90.1/89.6</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Our code and models will be made publicly available.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">https://medium.com/huggingface/distilbert-8cf3380435b5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">The student learns from the [CLS] (a special classification token of BERT) hidden states of the teacher.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">The output of pre-training tasks (such as dynamic masking) is used as the supervision signal.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">In the general distillation, we do not perform prediction-layer distillation as Equation10.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">BERTSMALL means directly pretraining a small BERT, which has the same model architecture as Tiny-BERT, through tasks of Masked Language Model (MLM) and Next Sentence Prediction (NSP).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">The prediction-layer distillation performs soft cross-entropy as Equation 10 on the augmented training set."No Pred" means performing standard cross-entropy against the ground-truth of the original training set.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DATA AUGMENTATION DETAILS</head><p>In this section, we explain the proposed data augmentation method. Specifically, we firstly mask a word in a sentence, then use BERT as a language model to predict M most-likely words in the corresponding position, while keeping other words unchanged. By this way, we can easily get the candidates for each word under a specific context. To induce new instances for a given sentence, we also use a threshold p t to determine whether we should replace the current word with a randomly selected candidate. By repetitively performing this replacement operation for each word in a sentence, we can finally get a new augmented sentence. In our preliminary experiments, we find the quality of generated candidates for the words consisting of multiple sub-word pieces, is relatively low. To alleviate this problem, we instead pick a similar word from GloVe <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> word embeddings based on the cosine similarity. We apply this data augmentation method N times to all the sentences of a downstream task. In this work, we set p t = 0.4, N = 20, M = 15 for all our experiments. The data augmentation procedure is illustrated as below:</p><p>Algorithm 1 The Proposed Data Augmentation Input: x is a sequence of words p t , N, M are hyperparameters Output: data aug, the augmented data</p><p>while n &lt; N do 5:</p><p>x masked ← x 6:</p><p>for i ← 1 to len(x) do </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B GLUE DETAILS</head><p>TinyBERT setup. TinyBERT learning includes the general distillation and the task-specific distillation. For the general distillation, we use English Wikipedia (2,500 M words) as the text corpus and perform the intermediate layer distillation for 3 epochs with the supervision from a pre-trained BERT BASE teacher and keep other hyper-parameters same as BERT pre-training <ref type="bibr" target="#b4">(Devlin et al., 2018)</ref>. For the task-specific distillation, we firstly perform intermediate layer distillation on the augmented dataset for 10 epochs with batch size 32 and learning rate 5e-5 under the supervision of a fine-tuned BERT teacher, and then perform prediction layer distillation for 3 epochs with batch size 32 and learning rate 3e-5. For tasks like MNLI, QQP and QNLI which have ≥ 100K training ex-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">What does bert look at? an analysis of bert&apos;s attention</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04341</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cognitive graph for multi-hop reading comprehension at scale</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
				<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2694" to="2703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
				<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Compressing deep convolutional networks using vector quantization</title>
		<author>
			<persName><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6115</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.00149</idno>
		<title level="m">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
				<imprint>
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NeurIPS)</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention-guided answer distillation for machine reading comprehension</title>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2077" to="2086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Daniel S Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Levy</surname></persName>
		</author>
		<idno>arX- iv:1907.10529</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sequence-level knowledge distillation</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1317" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Revealing the dark secrets of bert</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08593</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
				<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016-11">November 2016</date>
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Know what you dont know: Unanswerable questions for squad</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics(ACL)</title>
				<meeting>the 56th Annual Meeting of the Association for Computational Linguistics(ACL)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="784" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Patient knowledge distillation for bert model compression</title>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing(EMNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing(EMNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Vechtomova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12136</idno>
		<title level="m">Distilling taskspecific knowledge from bert into simple neural networks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems(NeurIPS)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.12471</idno>
		<title level="m">Neural network acceptability judgments</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
				<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conditional bert contextual augmentation</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangwen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songlin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Science(ICCS)</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno>arX- iv:1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
