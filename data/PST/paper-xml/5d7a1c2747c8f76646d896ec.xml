<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hardware-Software Blueprint for Flexible Deep Learning Specialization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josh</forename><surname>Fromm</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Hardware-Software Blueprint for Flexible Deep Learning Specialization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/MM.2019.2928962</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Specialized Deep Learning (DL) acceleration stacks, designed for a specific set of frameworks, model architectures, operators, and data types, offer the allure of high performance while sacrificing flexibility. Changes in algorithms, models, operators, or numerical systems threaten the viability of specialized hardware accelerators.</p><p>We propose VTA, a programmable deep learning architecture template designed to be extensible in the face of evolving workloads. VTA achieves this flexibility via a parametrizable architecture, two-level ISA, and a JIT compiler. The two-level ISA is based on (1) a task-ISA that explicitly orchestrates concurrent compute and memory tasks and (2) a microcode-ISA which implements a wide variety of operators with single-cycle tensor-tensor operations. Next, we propose a runtime system equipped with a JIT compiler for flexible code-generation and heterogeneous execution that enables effective use of the VTA's architecture.</p><p>VTA is integrated and open-sourced into Apache TVM, a state-ofthe-art deep learning compilation stack that provides flexibility for diverse models and divergent hardware backends. We deploy optimized deep learning models used for object classification and style transfer on edgeclass FPGAs. We propose a flow that performs design space exploration to generate a customized hardware architecture and software operator library that can be leveraged by mainstream learning frameworks. We also show that VTA deployed on an Xilinx Ultra96 FPGA outperforms highly optimized edge deep learning libraries on Mali-T860 GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Hardware specialization is a powerful way to accelerate a known set of applications and workloads. Unfortunately, deep learning is anything but a static field, and machine learning community rapidly changes the software they use to write models, the architecture of models themselves, the operators used by said models, and the data types they operate over.</p><p>The research community has primarily focused on two approaches for accelerator designs, fixed function accelerators and programmable accelerators (also known as Domain-Specialized Accelerators). Current solutions offer compelling peak performance, but often fail to integrate into the evolving machine learning landscape.</p><p>Fixed-model accelerators are commonly spatially and statically laid out, offering attractive performance for certain workloads. Unfortunately, the static nature of this approach rules out the reuse of hardware resources, limiting support for larger or newer models. They can be used to execute a single coarse grained computation such as a layer, or possibly the entire network, but do not gracefully degrade in the presence of fine-grained model changes.</p><p>In contrast, programmable accelerators <ref type="bibr" target="#b5">[6]</ref> offer far more flexibility by leveraging ISAs. Due to the programmable nature of these accelerators, achieving peak performance requires a competent deep learning compiler that can map large number of workloads onto a fixed set of hardware intrinsics. Consequently, customizing behavior of these accelerators, even when opensourced, is highly dependent on the availability of a transparent and modular software stack.</p><p>A central challenge in prior work is linking innovations in specialization to the rapidly changing software of machine learning. This challenge is not specific to computer architecture; it is present at all levels of the stack. An end-to-end approach requires integration between frameworks, systems, compilers, and architecture in order to execute state of the art machine learning using hardware acceleration. Peak FLOPs only provide value if a programmer can access them.</p><p>We present VTA (Versatile Tensor Accelerator): an explicitly programmed architecture paired with a capable JIT compiler and runtime that can evolve in tandem with deep learning models without sacrificing the advantages of specialization. VTA makes the following contributions:</p><p>? A programmable accelerator design that exposes a twolevel interface: a high-level task ISA to allow explicit task scheduling by the compiler stack, and a low-level tensor operation ISA to provide software-defined operator coverage.</p><p>In addition the VTA architecture is fully parameterizable: the hardware intrinsics can be scaled, memories reshaped, and data types customized to adapt to the hardware backend (e.g. different FPGA devices). ? An extensible runtime system for heterogeneous execution that performs JIT compilation of micro-kernels to provide operational flexibility. The VTA runtime has allowed us, for instance, to extend the functionality of VTA's original computer vision-centric design to supporting operators found in style transfer applications without requiring any modifications to the hardware. ? A schedule auto-tuning platform that can optimize data access and data reuse in order to rapidly adapt to changes in the underlying hardware and changes in workload diversity.</p><p>We demonstrate VTA's flexibility by adapting different workloads for two edge FPGAs. Figure <ref type="figure" target="#fig_0">1</ref> presents how to map a workload to FPGAs using the VTA architecture and runtime. This process explores VTA hardware variants, and performs software autotuning for each candidate design. The resulting design and customized software binaries can be easily integrated into a deep learning framework. Finally, we evaluate the full system, demonstrating VTA's ability to outperform edge GPUs with edge FPGAs that have similar costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">VTA HARDWARE SOFTWARE STACK OVERVIEW</head><p>Running an end-to-end workload on VTA requires a complete software stack that can map high-level models down to the programming interface exposed by VTA. We outline the layers of the VTA system stack below, which we built into the Apache TVM deep learning compiler stack.</p><p>Framework. Frameworks allow programmers to easily express models in a declarative fashion and perform training at scale on standard datasets. Frameworks like TensorFlow, PyTorch, MxNet have gained widespread adoption, allowing the community to easily share, and deploy models. TVM's ability to ingest models from these popular frameworks, enables generic compilation from frameworks to VTA. Relay Graph Optimizer. Relay <ref type="bibr" target="#b7">[8]</ref> is TVM's high level program representation. Relay generalizes the computation graphs used by prior frameworks and deep learning compilers into a full programming language. The Relay optimization pipeline performs generic optimizations such as operator fusion and partial evaluation. Relay's design is focused on extensibility, a property we use to extend Relay with a set of optimizations specific to VTA. When targeting VTA we quantize inputs to match VTA's low precision data types, transform data layout, maximize data reuse, and transform input and weight data layouts to utilize VTA's tensor intrinsics. TVM Operator Optimizer. TVM <ref type="bibr" target="#b2">[3]</ref> automates the tedious process of scheduling workloads onto VTA accelerator variants.</p><p>Scheduling is important for multiple reasons. First, it tiles the computation to maximize data reuse. Second, it inserts thread parallelism that VTA's runtime can translate into task-level pipeline parallelism. Third, it partitions operators into sub-computations which can be mapped to high-level hardware intrinsics such as bulk DMA load or GEMM. TVM incorporates AutoTVM <ref type="bibr" target="#b3">[4]</ref>, an automated schedule optimizer. We rely upon AutoTVM to guide our hardware candidate exploration search for the best VTA candidates given a workload. JIT Compiler and Runtime. The runtime performs JIT compilation of the accelerator binaries and manages heterogeneous execution between the CPU and VTA. JIT compilation reduces </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">VTA ARCHITECTURE AND JIT RUNTIME</head><p>A successful implementation of a flexible deep learning accelerator requires co-design of the hardware with the software stack. We describe at a high level two components that were co-designed to achieve this goal: the VTA hardware architecture, and the VTA JIT compiler and runtime. More details on VTA are available in a technical report <ref type="bibr" target="#b6">[7]</ref>.</p><formula xml:id="formula_0">DRAM LOAD MODULE INPUT BUFFER WEIGHT BUFFER OUTPUT BUFFER MICRO-OP CACHE REGISTER FILE GEMM Core Tensor ALU LD?CMP Q CMP?LD Q CMP?ST Q ST?CMP Q COMPUTE MODULE STORE MODULE LOAD CMD Q COMPUTE CMD Q STORE CMD Q INSTRUCTION FETCH MODULE</formula><p>Fig. <ref type="figure">3</ref>: The VTA hardware organization. VTA is composed of modules that communicate via queues and SRAMs. This enables task-level pipeline parallelism, which helps maximize compute resource utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hardware Architecture</head><p>Figure <ref type="figure">3</ref> gives a high-level overview of the VTA hardware organization. VTA is composed of four modules: fetch, load, compute, and store. Together, these modules define a task pipeline, which enables high compute resource utilization on compute-bound workloads, and high memory bandwidth utilization on memory-bound workloads. These modules communicate over command queues and on-chip shared memories (SRAMs) that act as uni-directional data channels. Accesses to these memories are synchronized via dependency queues to prevent data hazards such as Read After Write (RAW) and Write After Read (WAR). Finally, the 3-stage architecture (load-compute-store) can be used to build task pipelines of arbitrary depth as long as dependencies are properly managed. VTA supports a high-level task ISA that encodes multi-cycle compute and memory operations, including LOAD, GEMM, ALU, and STORE instructions. LOAD and STORE instructions describe how data from DRAM is loaded and stored into on-chip SRAMs. Strided memory access is supported to load tensor tiles without modifying memory layout. GEMM and ALU instructions invoke micro-coded kernels, based on micro-op instructions, which describe the dataaccess patterns that define a given deep learning operator.</p><p>We illustrate a simple execution pipeline in VTA below:</p><p>? The fetch module loads task instructions from DRAM and dispatches them, according to the instruction type, to the corresponding command queues connected to load, compute, and store modules.</p><p>? The load module loads input, weight, and bias tensor tiles from DRAM into on-chip memories.</p><p>? The compute module loads a micro-coded kernel from DRAM into on-chip memory. Micro-coded kernels are based on microops that describe data access patterns for inputs, weights, and biases.</p><p>? The compute module executes the micro-coded kernel to perform either a dense linear algebra computation via the GEMM core or a pairwise arithmetic operations via the Tensor ALU.</p><p>? The store module reads results processed by the compute module and writes them to DRAM.</p><p>Exposing Task-Level Pipeline Parallelism. Task-Level Pipeline Parallelism (TLPP) is an important feature in the VTA architecture, because it enables simultaneous use of compute and memory resources to maximize their utilization. TLPP is based on the paradigm of access-execute decoupling <ref type="bibr" target="#b8">[9]</ref>. To extract TLPP, we partition tasks into two mutually-exclusive execution contexts, so that concurrent load, compute, and store operations do not interfere with one another. To guarantee timely and correct execution for decoupled access-execute instruction streams, we encode dependency information into instructions. This effectively results in memory latency hiding on compute-bound workloads (e.g. 2d convolutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compute Module</head><p>Two functional units perform operations on the register file: the tensor ALU and the GEMM core. The tensor ALU performs tensor operations such as element-wise addition, activation, normalization, and pooling tasks. The GEMM core performs high-arithmetic intensity matrix multiplication over data from the input and weight tensors. As outputs are written to the register file, they are concurrently flushed to the output buffer and eventually written to DRAM by the store module.</p><p>The compute core reads instructions from the micro-op cache, which describe how computation is performed over data. These micro-op instructions provide no control flow. Therefore, instructions needs to be unrolled to express repeatable data access stencils. There are two types of compute micro-ops: ALU and GEMM operations. To minimize the footprint of micro-op kernels while avoiding the need for control-flow instructions, the compute core executes micro-op sequences inside a two-level nested loop that computes the location of each tensor register via an affine function. This compression approach helps reduce the micro-kernel instruction footprint when sent to the accelerator.</p><p>The GEMM core computes matrix multiply operations at a pipelined rate of one input-weight matrix multiplication per cycle. Its logic is implemented as parallel vector dot-product using reduction trees, but can be substituted with other implementations such as systolic arrays. The GEMM core defines a tensor hardware intrinsic which is exposed to the TVM compiler stack. TVM uses tensorization <ref type="bibr" target="#b2">[3]</ref>: an automated approach to mapping deep learning operators such as 2d convolution down to fixed tensor hardware intrinsics.</p><p>The VTA architecture is fully parameterizable: the shape of the GEMM tensor intrinsic can be modified to best utilize underlying hardware resources. For instance, each data type can customized to a different integer precision: weight and input types can be 8-bits or fewer, while the accumulation type can be 32-bits or fewer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">JIT Runtime System</head><p>VTA's JIT runtime enables cooperative execution of deep learning workloads between a CPU host and the accelerator. The JIT runtime design follows five objectives: (1) heterogeneous execution, (2) low complexity compiler, (3) hardware reuse, (4) reduced binary bloat, (5) future proofing. Heterogeneous execution. One challenge present in fixed function accelerators is model evolution, because most of these fixed function accelerators are built for fixed models. Heterogeneous execution overcomes the "all or nothing" limitation by properly scheduling operators into targets (e.g., CPUs or VTA), depending on their affinity for different types of operators. For instance, it is well known that the first convolutional layer in most CNNs contains operators with low arithmetic intensity that perform well on CPUs. Another motivation behind heterogeneous execution is providing a fallback mechanism for supporting emerging operators that are not yet ported in VTA. Low complexity compiler. By adding a level of indirection, code JIT-ting eliminates the need to write compiler code-generation backends which can be tedious to maintain for different programmable accelerators. The JIT compiler exposes a high-level API to TVM to lower schedules onto, abstracting away VTA variant-specific architectural details. This lets us extend TVM compiler support built for VTA to also support future variants of different shapes and sizes. Hardware reuse. The JIT runtime generates and dispatches microkernels on the fly. This eliminates micro-ops instruction memory limitations to support large models. It also lets us trade area used by the micro-op cache for other resources such as data storage, or compute units. Binary bloat. Delaying micro-kernel generation to the JIT compilation stage minimizes binary bloat. Since VTA's architecture has limited support for control flow, microkernels have to be unrolled which can produce fairly large binaries. In addition, micro-kernel code JIT-ting expresses binaries for heterogeneous execution in a single-ISA: instead of shipping a hybrid binary, we just ship one CPU binary that performs accelerator binary JIT-ting. Future proofing. Advancements in deep learning have outlined the prevalence of dynamic neural network workloads that incorporate control flow. Additionally, advances in systems show trends towards heterogeneous multi-accelerator systems and scale-out acceleration. Having a runtime that handles dynamic decisions across heterogeneous platforms will keep the design of hardware accelerators like VTA simple, and constrain support for future models to being a mostly software-defined problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VTA HIERARCHICAL OPTIMIZATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Hardware Exploration for Varying FPGA Sizes</head><p>One way to showcase VTA's architecture flexibility is to target different FPGA platforms. FPGAs are becoming more accessible than ever, with sub-$100 development boards, and FPGA cloud computing instances becoming ubiquitous.</p><p>The VTA design offers multiple architectural customization parameters. These customization knobs (e.g., the size of the GEMM core or datatypes) define a hardware design space with 100s to 1000s of individual designs. This design space can be exhaustively explored to find the best candidate for a particular workload. We perform this exploration in a sequence of stratified steps. First we use a simple FPGA resource model to prune infeasible VTA parameterizations. After pruning, each candidate hardware design is compiled, placed, and routed. We pick the best feasible design for each { f pga ? dtype ? batch} combination, but typically our exploration returns a handful of promising candidates -the rest of the designs either yield low peak performance or fail placement, routing or timing closure. For this final set of designs, we generate optimized software, using operator autotuning <ref type="bibr" target="#b3">[4]</ref>, and use this software to obtain the workload's performance profile.</p><p>An analytical model of peak performance is used to initially filter hardware designs based on theoretical throughput and frequency assuming compute resources are 100% utilized. However, assuming 100% utilization of compute resources by a particular operator is often inaccurate. For example, depending on the workload mix, operators like conv2d with large window sizes may exhibit high arithmetic intensity (measured in Op/Byte). Operations with high arithmetic intensity translate to high utilization, and therefore are close to peak performance. Operators which exhibit low arithmetic intensity, like conv2d, with a window size of 1, are memory bandwidth constrained. In these situations we are able to use tasklevel parallelism and latency hiding to mitigate performance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Schedule Exploration for Operator Autotuning</head><p>Schedule autotuning is the process by which an automated search algorithm attempts to optimize a given program or workload towards peak hardware performance. We perform autotuning by applying different memory tiling, loop transformations (e.g. splitting, reordering, unrolling), vectorization/tensorization, and parallelization strategies <ref type="bibr" target="#b3">[4]</ref>. We then use the TVM compiler to express schedule templates for each operator (e.g. conv2d, conv2d_transpose, group_conv2d, fc) we support in hardware. We use TVM's automated scheduling library to obtain schedules that maximize performance for a given combination of operator, tensor shape, and hardware parameterization.</p><p>We used the XGBoost <ref type="bibr" target="#b0">[1]</ref> search algorithm to find the best schedules for each hardware variant in a limited number of trials. Each workload's layers are then tuned for each hardware candidate. Aggregate inference time is used to select the VTA hardware variant that is best for a given model.</p><p>It takes several hours to exhaustively tune a network on a single hardware variant. Given the large number of VTA hardware designs to test, and model architectures to support, autotuning search quickly becomes intractable without careful design. Minimizing full-network autotuning time across multiple hardware candidates introduces a hierarchical prioritization problem. We approach this challenge by applying a hyperparameter optimization technique, based on SuccessiveHalving <ref type="bibr" target="#b4">[5]</ref>. Instead of choosing among hyperparameters that define a network architecture, we apply this technique to choose among VTA design candidates. We simultaneously inspect how the relative performance of each hardware design evolves for a given workload, over each iteration of the optimization algorithm. Throughout optimization we use a round-robin policy to update latency estimates across all operators for each hardware design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Full Network Optimization Case Study</head><p>We show in Figure <ref type="figure">4</ref> an example of hierarchical optimization for the ResNet-18 workload, based on the hardware exploration and schedule exploration techniques described before. We perform these optimizations over a set VTA candidates generated using W8A8 (8-bit weights, 8-bit activations) data representation. We select eight promising hardware candidates, and apply SuccessiveHalving to prune designs that do not appear promising. Similar to hyperparameter optimization for neural network training, this is a difficult task, as the relative performance differences between hardware designs may be small early on. After a moderate number of iterations, SuccessiveHalving is able to converge to the best candidate hardware design.</p><p>This case study showcases VTA's ability to quickly navigate a non-trivial space of accelerator configurations for a given workload. As accelerator configurations change, so does the software that programs it. This joint-optimization problem can only be solved with a flexible stack. Fig. <ref type="figure">4</ref>: Example of hardware design exploration and schedule autotuning on a complete ResNet-18 inference workload run on Ultra96 FPGA. The exploration begins with promising VTA hardware variants and converges to the optimal hardware design while using a fraction of the optimization time required to exhaustively evaluate each hardware design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>We integrated VTA into a state-of-the art deep learning stack and evaluated a variety of deep learning models on a set of edge FPGA devices with different resource budgets. We imported all models from MxNet <ref type="bibr" target="#b1">[2]</ref> a deep learning framework used by Amazon. It is worth noting Relay's model importers provide access to a wide variety of other front-ends, and VTA is not limited to MxNet.</p><p>As the landscape of deep learning continues to evolve, it is important to support emerging models. We evaluate VTA's ability to support two novel model architectures beyond standard deep convolution nets. First, we evaluate MobileNet, a recent model architecture that uses grouped convolution to reduce the total computation overhead of the network. We evaluate a variant of MobileNet we call MobileNetG that groups channels by the vector factor of the VTA's GEMM core. Second, we implement a Generative Adversarial Network (DCGAN) model that is used for image-to-image translation and generation.</p><p>Both models require non-trivial extensions to support new operators. MobileNetG requires support for grouped convolutions that exhibit block sparse patterns on channel groups. DCGAN requires support for 2D convolution transpose which has a spatial sparsity pattern. Accelerators must support these access patterns to avoid unnecessary computations and achieve maximum performance. The runtime can readily make use of schedules to generate micro-kernels that support these access patterns without changing the hardware.</p><p>Figure <ref type="figure">5</ref> shows a performance comparison across these models, comparing VTA-accelerated execution against a highly optimized ARM CPU and GPU platforms that rely on industry-strength deep learning libraries: ARM ComputeLib (ARM CL) and TVM. The ARM Cortex-A9, ARM Cortex-A53, and Mali-T860 GPU are taken from the Pynq-Z1 ($65), Ultra-96 ($250), and the Firefly-RK3399 ($200) boards. For the VTA hardware designs, we use an automated 8-bit integer scaling and translation pass from 32-bit floatingpoint (FP32) with negligible accuracy degradation. For our CPU Fig. <ref type="figure">5</ref>: End to end performance evaluation over multiple CPU, GPU and FPGA-equipped edge systems. For comparable systems, VTA provides a significant performance edge over conventional CPU and GPU-based inference. baselines, we use the TVM autotuner to obtain FP32 CPU kernels that take advantage of NEON vectorization, multi-threading and state of the art scheduling tricks (spatial tiling, Winograd transform etc.). For our GPU baseline, we use the ARM CL v18.03 and exploit 16-bit floating-point (FP16) library support. ARM CL is missing support conv2d transpose for DCGANs, demonstrating VTA's ability to stay ahead of the curve for unconventional workloads.</p><p>Figure <ref type="figure">5</ref> shows end-to-end results that can be discussed in two groups of comparable devices in terms of cost: (1) VTA on the Pynq vs. Cortex-A9 (sub-$100), and (2) VTA on Ultra96 vs. Cortex-A53 and Mali-T860 GPU ($100). First, VTA on the Pynq-Z1 outperforms the Cortex-A9 CPU by 3.0?, 4.4?, 5.3? and 2.1? on MobileNet, ResNet-18, ResNet-34 and DCGAN. Second, VTA on the Ultra-96 outperforms the Cortex-A53 by 2.5?, 4.7?, 6.0?, 3.8? and 11.5? on MobileNet, ResNet-18, ResNet-34, ResNet-50 and DCGAN. In addition, VTA on the Ultra-96 outperforms the mobile-class Mali-T860 GPU by 2.1?, 2.5?, 3.2? and 2.1? on MobileNet, ResNet-18, ResNet-34 and ResNet-50.</p><p>Overall, VTA demonstrates that the flexibility of the architecture can offer high performance while forming a evolutionary path forward for accelerating diverse workloads on various devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present hardware-software blueprint for "flexible specialization": the idea that efficiency gains from hardware specialization is not mutually exclusive with workload flexibility. We present VTA, a parametrizable deep learning architecture that is explicitly programmed via a two-level ISA. We co-design the accelerator with a runtime system that JIT compiles micro-kernels to provide operational flexibility. With this approach, we support less conventional operators such as convolution transpose, and grouped convolutions without needing to apply changes to the hardware. We show in our evaluation that VTA can effectively target different FPGAs, multiple workloads, and leverage off the shelf deep learning compilers to quickly integrate optimized software with specialized hardware. In addition, we demonstrate that a well integrated hardware and software stack lets us perform full stack optimization and exploration on FPGAs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>{Fig. 1 :</head><label>1</label><figDesc>Fig. 1: VTA provides flexibility with respect to hardware targets and deep learning models. This flow diagram shows the steps in adapting a given model to a hardware backend by exploring VTA hardware configurations, and performing operator autotuning on the top hardware candidates. This process generates the pieces necessary to deploy VTA in any deep learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of the software stack built for VTA. We leverage the Apache TVM compiler stack to target VTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>0272-1732 (c) 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/MM.2019.2928962, IEEE Micro</figDesc><table><row><cell>Sum of Latency (s)</cell><cell>10 1 10 0</cell><cell cols="3">Full ResNet-18 Network Optimization with Successive Halving VTA Design 0 VTA Design 1 VTA Design 2 VTA Design 3 VTA Design 4 VTA Design 5 VTA Design 6 VTA Design 7</cell></row><row><cell></cell><cell>10 2</cell><cell>1000</cell><cell>2000 Optimization Iterations (step size=1 iteration) 3000 4000 5000</cell><cell>6000</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MXNet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems, Workshop on Machine Learning Systems (LearningSys&apos;15)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{TVM}: An automated end-to-end optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08166</idno>
		<title level="m">Learning to optimize tensor programs</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Non-stochastic best arm identification and hyperparameter optimization</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="240" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raminder</forename><surname>Bajwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Boden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al</forename><surname>Borchers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rick</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Luc</forename><surname>Cantin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifford</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Coriell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Dau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Vazir Ghaemmaghami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajendra</forename><surname>Gottipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Gulland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Richard</forename><surname>Robert Hagmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hogberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Hurt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alek</forename><surname>Jaffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Jaworski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khaitan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Killebrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Laudon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diemthu</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyuan</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Lucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gordon</forename><surname>Lundin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Mackean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maire</forename><surname>Maggiore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kieran</forename><surname>Mahony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Narayanaswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Norrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narayana</forename><surname>Omernick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Penukonda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Phelps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emad</forename><surname>Salek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Samadiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Severn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Sizikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jed</forename><surname>Snelham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Souter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mercedes</forename><surname>Swing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thorson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horia</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erick</forename><surname>Toma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Tuttle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doe</forename><surname>Wilcox</surname></persName>
		</author>
		<author>
			<persName><surname>Hyun Yoon</surname></persName>
		</author>
		<title level="m">Proceedings of the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17</title>
		<meeting>the 44th Annual International Symposium on Computer Architecture, ISCA &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vta: An open hardware-software stack for deep learning</title>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relay: a new ir for machine learning frameworks</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</title>
		<meeting>the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupled access/execute computer architectures</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Annual Symposium on Computer Architecture, ISCA &apos;82</title>
		<meeting>the 9th Annual Symposium on Computer Architecture, ISCA &apos;82<address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1982">1982</date>
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
