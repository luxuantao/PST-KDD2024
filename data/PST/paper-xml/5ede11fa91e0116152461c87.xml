<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 COCON: A SELF-SUPERVISED APPROACH FOR CONTROLLED TEXT GENERATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 COCON: A SELF-SUPERVISED APPROACH FOR CONTROLLED TEXT GENERATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pretrained Transformer-based language models (LMs) display remarkable natural language generation capabilities. With their immense potential, controlling text generation of such LMs is getting attention. While there are studies that seek to control high-level attributes (such as sentiment and topic) of generated text, there is still a lack of more precise control over its content at the word-and phrase-level.</p><p>Here, we propose Content-Conditioner (CoCon) to control an LM's output text with a content input, at a fine-grained level. In our self-supervised approach, the CoCon block learns to help the LM complete a partially-observed text sequence by conditioning with content inputs that are withheld from the LM. Through experiments, we show that CoCon can naturally incorporate target content into generated texts and control high-level text attributes in a zero-shot manner. 1</p><p>1 Source code will be made available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformer-based <ref type="bibr" target="#b30">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b29">Tay et al., 2020)</ref> pretrained language models (LMs) have led a wave of new advances in natural language processing tasks as a means to extract contextualized word embeddings <ref type="bibr" target="#b7">(Devlin et al., 2018;</ref><ref type="bibr" target="#b5">Dai et al., 2019b;</ref><ref type="bibr" target="#b32">Yang et al., 2019)</ref> and as text generators <ref type="bibr" target="#b25">(Radford et al., 2019;</ref><ref type="bibr" target="#b3">Brown et al., 2020)</ref>. These LMs are trained on huge amounts of text corpora to predict next tokens through a log-likelihood objective. Given its remarkably fluent text generation, there is growing interest in controlling output texts of such LMs <ref type="bibr" target="#b16">(Keskar et al., 2019;</ref><ref type="bibr" target="#b6">Dathathri et al., 2019)</ref>. Approaches like training a modified LM from scratch to incorporate target text attributes <ref type="bibr" target="#b16">(Keskar et al., 2019)</ref> can be expensive while finetuning pretrained LMs for specific attributes <ref type="bibr" target="#b35">(Ziegler et al., 2019)</ref> limits the scope of text control. Without changing the architecture or weights of pretrained LMs, one promising approach (PPLM) <ref type="bibr" target="#b6">(Dathathri et al., 2019)</ref> controls generated text through attribute models. Though effective in controlling high-level text attributes such as topic and sentiment, the same target attribute may generate text samples with vastly different content at the word-and phrase-levels, leaving a gap for more fine-grained control over the content of LM-generated texts.</p><p>We conceptualize Content-Conditioner (CoCon) as an approach to narrow this gap by guiding pretrained LMs' text outputs through the incorporation of content input. This content input can take the form of a text sequence whose content we would like to condition on for text generation. Essentially, CoCon comprises two parts: 1) a pretrained LM and 2) a interleave CoCon layer. By employing a pretrained LM, CoCon incorporates the representations of a content input into the encoded text representations through the CoCon layer before passing the content-conditioned representations into LM β for generation. To train the CoCon block, we propose a self-supervised learning approach where training data consist of text samples generated by the pretrained LM itself <ref type="bibr">( § 3.1)</ref>. By splitting each text sequence into two segments ([x a ; x b ]), CoCon learns through a self reconstruction objective to help the LM reconstruct missing latter segments (x b ) by taking x b itself as the content input. We use content masking for CoCon and also propose other loss functions such as cycle reconstruction to condition content from divergent sources while producing high-quality texts. Since the CoCon block's size is a small fraction of the LM and no finetuning is conducted on the LM's weights, the training cost is significantly lower than training an LM from scratch. We show that CoCon's fine-grained content control can be extended to also influence higher-level text attributes such as topic and sentiment in a zero-shot manner, and compare it with strong controlled generation baselines. Furthermore, CoCon is versatile in assimilating multiple content inputs, and its strength of content-conditioning can be flexibly adjusted through a content bias term during inference. In this paper, we demonstrate the CoCon approach with the GPT-2 345M model <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> as the pretrained LM. Given CoCon's modular nature, it can be used with other Transformer-based LMs or even other controlled generation methods. All in all, the core contributions of this paper are:</p><p>• We propose CoCon for content-conditioned language generation.</p><p>• We introduce a self-supervised learning approach where CoCon learns to complete text sequences when given information about future tokens. • Through ablation studies and comparisons with strong baselines like <ref type="bibr">PPLM and CTRL (Keskar et al., 2019)</ref>, we investigate how CoCon controls high-level attributes such as topic and sentiment while generating texts that have high content similarity to conditioning text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>There is a line of work that aims to generate output text of desired attributes with neural networks. Some of the earliest efforts involve conditional generative models <ref type="bibr" target="#b17">(Kikuchi et al., 2016;</ref><ref type="bibr" target="#b10">Ficler &amp; Goldberg, 2017)</ref> where the networks are trained on text data labeled with the target attributes. These models can be trained via reinforcement learning <ref type="bibr" target="#b35">(Ziegler et al., 2019)</ref> or the generative adversarial network <ref type="bibr" target="#b34">(Yu et al., 2017)</ref> framework. Unlike CoCon, the requirement of predetermined attributes in those methods limits the possible types of generated texts. CTRL <ref type="bibr" target="#b16">(Keskar et al., 2019)</ref> is a recent approach that generated controlled fluent texts through the use of control codes which are meta-data prepended to the text during generation. Though it produces high-quality text with its GPT-2-like architecture, its control codes are also predetermined during the training. Closest to our work is Plug and Play Language Model (PPLM) <ref type="bibr" target="#b6">(Dathathri et al., 2019)</ref> which seeks to control text on already pretrained LM without finetuning through relatively small 'pluggable' attribute models. While PPLM's flexible design also enables controlled generation without retraining or finetuning the LM like in CoCon, our approach aims to control the generation at a content level, beyond high-level text attributes. Another core difference lies in the training where CoCon's self-supervised learning absolves the need for labeled data, such as the ones employed to train PPLM's attribute discriminator models. Weighted decoding <ref type="bibr" target="#b11">(Ghazvininejad et al., 2017;</ref><ref type="bibr" target="#b13">Holtzman et al., 2018)</ref> seeks to control the output text token by upweighting the probabilities of targeted words during the decoding step but has been shown to produce incoherent text <ref type="bibr" target="#b26">(See et al., 2019)</ref>. Conditioning language generation has been used in question generation to enhance faithfulness by attending to textual context such as predicates, subject types or object types <ref type="bibr" target="#b9">(Elsahar et al., 2018)</ref> rather than the content input used here in CoCon. Small adapter layers <ref type="bibr" target="#b1">(Bapna et al., 2019)</ref> have been previously proposed for multilingual translation to also save on model size and training resources but differ from CoCon's self-supervised training as they rely on annotated sentence pairs of different languages for training.</p><p>Text style transfer is a related area that controls texts' attributes by translating text from one style to another <ref type="bibr" target="#b4">(Dai et al., 2019a)</ref>. A few of such studies employ auto-encoders to separate texts' style and non-style latent representation <ref type="bibr" target="#b28">(Shen et al., 2017;</ref><ref type="bibr" target="#b15">Hu et al., 2017;</ref><ref type="bibr" target="#b33">Yang et al., 2018)</ref>. This disentanglement enables style changes to the text at the latent space while retaining most of its content. Another work identifies attribute markers <ref type="bibr" target="#b19">(Li et al., 2018)</ref> which are n-grams correlated with a particular style in a text corpus and edit texts' style by substituting them. Essentially, style transfer alters existing texts rather than generating texts and requires predefined attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONTENT CONDITIONER (COCON)</head><p>In the following sections, we discuss the motivation for CoCon, its model architecture and how we train the CoCon block.</p><p>Motivation In text generation with language models, given the prompt text x :t−1 = {x 1 , . . . , x t−1 }, the following text {x t , . . . , x l } is generated in an auto-regressive manner <ref type="bibr" target="#b21">(Manning et al., 1999;</ref><ref type="bibr" target="#b2">Bengio et al., 2003)</ref>:</p><formula xml:id="formula_0">p(x t , . . . , x l |x 1 , . . . , x t−1 ) = l i=t p(x i |x 1 , . . . , x i−1 ).<label>(1)</label></formula><p>Previous studies on controlled text generation in LM showed that p(x) can be conditioned on target attributes <ref type="bibr" target="#b6">(Dathathri et al., 2019)</ref> or control codes <ref type="bibr" target="#b16">(Keskar et al., 2019)</ref> to control the text's sentiment or topic, i.e., p(x t , . . . , x l |x 1 , . . . ,</p><formula xml:id="formula_1">x t−1 ) = l i=1 p(x i |a, {x 1 , . . . , x i−1 }), (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where a is the target attribute. While these methods show that the generation is fluent and can be aligned with the target attribute well, the output texts {x t , . . . , x l } are controlled at a global attribute (e.g., sentiment/topic) level rather than at a more local content (e.g., words/phrases) level. Since there is a vast number of possible {x t , . . . , x l } candidates which would align well with both the prompt text and target attribute, this results in generated text samples that contain very different content during the stochastic token sampling process. This motivates an approach to condition on an content input c for more fine-grained control over text generation:</p><formula xml:id="formula_3">p(x t , . . . , x l |x 1 , . . . , x t−1 ) = l i=1 p(x i |c, {x 1 , . . . , x i−1 }),<label>(3)</label></formula><p>where c can be a text sequence whose content we would like to condition on during text generation.</p><p>Next, we propose the model architecture of Content-Conditioner (CoCon) as an approach for this control.</p><p>Model Architecture Our proposed Content-Conditioner (Figure <ref type="figure" target="#fig_0">1</ref>) controls the content of the generated text while maintaining fluency by incorporating a pretrained Transformer-based language model (LM), GPT-2 <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> in our experiments. Such LMs have shown remarkable natural text generation in the auto-regressive manner (Eq. 1) where the next token x t is sampled based on the logits o t = LM(x :t−1 ). These LMs are essentially stacks of Transformer blocks, each consisting of layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>, multi-head self-attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> and position-wise feed forward operations.</p><p>An LM's generation can be broken down into two separate parts: layers before the CoCon block (LM α ) and layers after (LM β ). The LM α acts as a feature extractor that takes in the input sequence's embeddings and outputs its intermediate representation at a breakpoint, i.e., h :t−1 = LM α (x :t−1 ). Subsequently, LM β takes in this representation and outputs the logits for the next token, i.e., o t = LM β (h :t−1 ), yielding</p><formula xml:id="formula_4">o t = LM(x :t−1 ) = LM β (LM α (x :t−1 )) = LM β (h :t−1 ).<label>(4)</label></formula><p>From Eq. 4, we can see that the representation (h) is a medium to control next token logits (o) and hence the text generation process. Indeed, we transform h by conditioning it with the content input (c) through a CoCon block such that</p><formula xml:id="formula_5">h :t−1 = CoCon(h (c) :lc , h :t−1 ),<label>(5) where h (c)</label></formula><p>:lc = LM α (c) is the content representations and l c is the length of the content text sequence. We parameterize the CoCon block as a single Transformer block with an attention and position-wise feed-forward operation. Similar to a typical LM attention layer, the query (Q), key (K), value (V) matrices are computed through linear transformations on the representations h :t−1 , where Q, K, V ∈ R (t−1)×d and d is the representations' dimension. To attend to the content representations (h (c) :lc ), the content keys and values (K (c) , V (c) ∈ R lc×d ) are also computed, and concatenated to the original attention matrices before computing the CoCon attention output:</p><formula xml:id="formula_6">K = [K (c) ; K], V = [V (c) ; V], A = Softmax(QK )V = Softmax(W)V , (6)</formula><p>where A = {a 1 , . . . , a t−1 } and W ∈ R (t−1)×(lc+t−1) represents the attention weights. The final CoCon outputs are computed with a position-wise feed-forward layer. By concatenating to the representations prior to t − 1 and passing them to LM β , the next logits, and consequently word token xt , is now conditioned on c:</p><formula xml:id="formula_7">h i = FF(a i ), õt = LM β ([h :t−2 ; h t−1 ]), p θ,ψ (x t |c, x :t−1 ) = Softmax(õ t ),<label>(7)</label></formula><p>where θ and ψ are the paramterization of the CoCon block and LM respectively. Similar to a GPT-2 Transformer block, our CoCon block includes layer normalization before its multi-headed attention and feed-forward layers. Figure <ref type="figure" target="#fig_0">1</ref> summarizes the CoCon architecture which enables auto-regressive text generation by using xi as the token input (x i ) to generate xi+1 where i ≥ t.  Multiple Content Inputs CoCon's flexible design enables multiple content inputs for a single generation. In the case where we have N content inputs (c 1 , . . . , c N ), the output text can be conditioned by these contents through their attention keys and values, similar to Eq. 6:</p><formula xml:id="formula_8">K = [K (c 1 ) . . . K (c N ) ; K], V = [V (c 1 ) . . . V (c N ) ; V], A = Softmax(QK )V . (8)</formula><p>Strength of Content Conditioning Within CoCon's attention mechanism, we can vary the extent of content conditioning on the output text by biasing the attention weights in W (Eq. 6) that correspond to the content input (c). More specifically, the influence of c on the output text can be altered through the attention's softmax weighting on the content values (V (c) ). During generation, a positive bias term (τ content ) can optionally be added to the content attention weights W :,:lc ∈ R (t−1)×lc to increase influence of V (c) , boosting content conditioning, while a negative term can conversely reduce the content-conditioning effect. We discuss examples of varying τ content in § 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SELF-SUPERVISED LEARNING</head><p>We train CoCon with a self-supervised learning approach that is inspired by the diversity of content in natural language. Given a text sequence x = {x 1 , . . . , x t−1 , x t , . . . , x l } of length l, we can break it into two contiguous segments:</p><formula xml:id="formula_9">x a = {x 1 , . . . , x t−1 } and x b = {x t , . . . , x l } where x = [x a ; x b ].</formula><p>In the real world, there may be numerous substitutes of x b that could follow from x a fluently. Coupled with the randomness in text sampling, this means that, without information about x b , the probability of reconstructing the full x from x a alone with an LM can be low.</p><p>Self Reconstruction Loss Based on this intuition, our approach trains the CoCon block to help the LM reconstruct the original x by also conditioning with x b as the content input, i.e., c = x b (Figure <ref type="figure" target="#fig_2">2b</ref>). More concretely, we first compute the intermediate representations of the input text x and c:</p><formula xml:id="formula_10">h :l = LM α (x) = LM α (x :l ), h (c) :lc = LM α (c) = LM α (x t:l ),<label>(9)</label></formula><p>where l c = l − t + 1 is the length of c. The content-conditioned representation can be computed by the CoCon block where h (c)</p><p>:lc is the content representation:</p><formula xml:id="formula_11">h i = CoCon(h (c) :lc , h :i ) , ∀i ≥ t − 1.<label>(10)</label></formula><p>Similar to Eq. 7, the CoCon transformed representations are concatenated to the original representation before t − 1 and passed into LM β to produce the LM logits:</p><formula xml:id="formula_12">õi+1 = LM β ([h :t−2 ; h t−1:i ]), p θ,ψ (x i+1 |c, x :i ) = Softmax(õ i+1 ), ∀i ≥ t − 1.<label>(11)</label></formula><p>Through an LM training objective, we arrive at the self-reconstruction loss term which trains CoCon to predict tokens of x b by conditioning on x b itself as the content input (c):</p><formula xml:id="formula_13">L self = − l i=t log p θ,ψ x i |(c = x b ), {x 1 , . . . , x i−1 } . (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>To avoid trivializing the prediction of the next token x i+1 during training, we apply a self-token c-mask at CoCon's attention layer such that h i does not attend to the token x i+1 in c that it is trying to predict. This approach can be conducted in a self-supervised manner with any pretrained LM where the training samples x are generated text outputs stochastically sampled from the LM itself.</p><p>Null Content Loss To encourage CoCon's outputs to follow the prompt text x a fluently without relying on x b , we also train CoCon with a loss term similar to Eq. 12 but replaces the content input with a null token (∅):</p><formula xml:id="formula_15">L null = − l i=t log p θ,ψ (x i |(c = ∅), {x 1 , . . . , x i−1 }) . (<label>13</label></formula><formula xml:id="formula_16">)</formula><p>My name is Peter,</p><p>The weather is good today, let's go to the zoo! I am a footballer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target Content Prompt</head><p>CoCon Output</p><p>The weather is good today, let's go to the zoo!   <ref type="figure" target="#fig_2">2a</ref>) and two CoCon forward steps (Figure <ref type="figure" target="#fig_2">2c</ref>). We can express the output of a CoCon's auto-regressive generation as</p><formula xml:id="formula_17">y = f θ,ψ (c, p),<label>(14)</label></formula><p>where [p; y] would be a fluent text sequence and y is conditioned on the content of c. The first step (Figure <ref type="figure" target="#fig_2">2c</ref>(i)) computes the CoCon output with the content input (c) sourced from x and prompt text (p) sourced from x :</p><formula xml:id="formula_18">y x,x = f θ,ψ ((c = x b ), (p = x a )),<label>(15)</label></formula><p>where</p><formula xml:id="formula_19">x = [x a ; x b ] and x = [x a ; x b ].</formula><p>Since CoCon utilizes a pretrained LM for generation, y x,x would be a text sequence that fluently follows the prompt, x a , while seeking to incorporate x b 's content. The second CoCon forward step (Figure <ref type="figure" target="#fig_2">2c</ref>(ii)) takes y x,x as content input and x a as prompt text:</p><formula xml:id="formula_20">y cycle = f θ,ψ ((c = y x,x ), (p = x a )),<label>(16)</label></formula><formula xml:id="formula_21">Since x = [x a ; x b ],</formula><p>x b is a valid continuation from the prompt x a and recall that y x,x was contentconditioned on x b in the first CoCon step (Eq. 15). This posits x b as a training label for y cycle which gives us the cycle reconstruction loss term:</p><formula xml:id="formula_22">L cycle = − l i=t log p θ,ψ y cycle = x b |(c = y x,x ), (p = x a ) . (<label>17</label></formula><formula xml:id="formula_23">)</formula><p>Adversarial Loss Adversarial training objectives have shown to help in generating realistic text outputs <ref type="bibr" target="#b33">(Yang et al., 2018)</ref>. Here, we also employ an adversarial training loss <ref type="bibr" target="#b12">(Goodfellow et al., 2014)</ref> to encourage the output texts' representations (LM α (y)) to match those of the training samples (LM α (x)) by minimizing the loss:</p><formula xml:id="formula_24">L adv = E x [log f disc (LM α (x))] + E y [log(1 − f disc (LM α (y))],<label>(18)</label></formula><p>where f disc is a discriminator network that classifies whether the representations are of CoCongenerated texts. Through continuous approximation of discrete sampling of y, CoCon and f disc can be trained with backpropagation in an end-to-end manner. Parameterizing the f disc with φ, the discriminator is trained to maximize L adv rather than minimize it:</p><formula xml:id="formula_25">φ * = arg max φ L adv (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>Full Training The full learning objective trains the CoCon to minimize the four loss terms through stochastic gradient descent:</p><formula xml:id="formula_27">θ * = arg min θ (λ self L self + λ null L null + λ cycle L cycle + λ adv L adv ),<label>(20)</label></formula><p>where the λ values control how much the loss terms dominate the training. To show that our approach is fully self-supervised and requires no manually labeled data fully, we use generated GPT-2 text samples as training data for all four training losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct a range of experiments on CoCon to study its control over generated texts and the quality of these texts. Table <ref type="table">1</ref> shows CoCon samples with content, topic and sentiment control.</p><p>Table <ref type="table">1</ref>: CoCon samples with multiple content inputs, given same prompt text (underlined), exhibiting control over generations. More samples are in the Appendix (Table <ref type="table" target="#tab_15">18 and 19</ref>).</p><p>Content Input (c 1 ): officials predict there could be 5,800 submerged + Target Topic: SCIENCE, Content Input (c 2 ): Scientist + Target Sentiment: Positive, Content Input (c 3 ): is perfect The movie makers speculate there's a perfect match. Expectations there could be up to 500 kilograms of clay could be thrown onto the surface of the ocean. The BBC reported that it could have taken up to a year and a half to add clay to the ocean floor, though experts believe it could be done within several days..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CoCon Setup</head><p>In all our experiments, the GPT-2 medium 345M model <ref type="bibr" target="#b25">(Radford et al., 2019</ref>) is used as the pretrained LM for CoCon. The CoCon's LM α comprises the first 7 GPT-2 Transformer blocks while the remaining 17 blocks make up LM β in our experiments. The CoCon block's architecture mirrors a single GPT-2 Transformer block with dimension size of 1024. The training samples (x) are 30-BPE long segments sampled from GPT-2 output texts<ref type="foot" target="#foot_0">2</ref> . Subsequently, the x a and x b segments are split from x at a breakpoint between the 8th to 12th BPE position, uniformly sampled during training. More details about the setup are deferred to § A of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CONTENT SIMILARITY</head><p>We perform evaluation of CoCon's content control over generated text with automatic metrics such as BLEU <ref type="bibr" target="#b23">(Papineni et al., 2002)</ref>, NIST <ref type="bibr" target="#b8">(Doddington, 2002)</ref> and METEOR <ref type="bibr" target="#b18">(Lavie &amp; Agarwal, 2007)</ref>. These standard machine translation metrics can reveal how the CoCon generated text, y = f θ,ψ (c, p), are similar to the content input (c). Similar to <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>, as an automated measure of fluency, we compute perplexity of generated text using a different pre-trained language model, GPT <ref type="bibr" target="#b24">(Radford et al., 2018)</ref>. We also report Dist-1,-2,-3 scores as another metric of text quality that measures the diversity of 1-,2-,3-grams in the generations. Apart from a GPT-2 plain baseline without content conditioning, we also compare with three CoCon variants that omit either the L cycle , L null or L adv for an ablation study. To investigate the effect of training data sources, we train a CoCon model (CoCon-Webtext) on 250K Webtext <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> training samples, a subset of which the GPT-2 LM was originally trained on. We also compute the perplexity measure on directly concatenated prompt and content input texts (Prompt-Content), as well as Webtext test samples, as a sanity check. More setup details are in § A.1 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Based on the content similarity results (Table <ref type="table" target="#tab_1">2</ref>), all the CoCon variants can incorporate the content of c in the generated text better than an unconditioned plain GPT-2 LM. While the CoCon ablated variants appear to be better at incorporating c's content, it comes at a high cost of text quality for the case of omitted L cycle and L null . If L cycle were removed, CoCon would train only on prompt text p and content input c segments that were sampled from the same parent x, which explains why the quality of its outputs drops during test time when prompt text p and content input c are from different sources. We can see this degenerate case from generated samples (Table <ref type="table">9</ref>) where L cycle is vital to smoothly integrate content inputs that are far from the prompt text. Despite slightly improved text diversity, we observe that L adv marginally reduces CoCon's perplexity which we speculate is due to it being a non-LM type loss term, causing a trade-off in performance on the LM-aligned perplexity metric. In our human evaluation (Table <ref type="table" target="#tab_7">8</ref> of Appendix), we observe that humans also perceive CoCon without L adv as more fluent, indicating that the addition of L adv may have made it more challenging for the CoCon model to converge in its training. Training CoCon with Webtext samples improves content similarity at a cost of higher perplexity and lower fluency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">TOPIC RELEVANCE</head><p>Setup We evaluate CoCon's ability to control the topic of the generated text by using topic words as single-token content inputs and compare with two strong LM-based controlled generation baselines (PPLM <ref type="bibr" target="#b6">(Dathathri et al., 2019)</ref> and CTRL (Keskar et al., 2019)), using their Huggingface versions <ref type="bibr" target="#b31">(Wolf et al., 2019)</ref>. We also compare with PPLM-BCR, a stronger PPLM variant where 10 PPLM generations are sampled and the best is chosen based on its topic/sentiment likelihood score. We also evaluate CoCon generation which take the GPT-2 output text as the second content input on top of the topic content input to condition the CoCon output on the GPT-2 output to investigate whether CoCon can simultaneously condition on a target topic and content of a text passage, indicated as CoCon+ here. We also conducted human evaluation of fluency and A/B testing on attribute relevance, similar to <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>. More setup details are presented in the Appendix § A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>All the three LM-based controlled text generators output texts are that more topicrelevant than the unconditioned GPT-2 model (Table <ref type="table" target="#tab_2">3</ref>). CoCon's generated texts appear to be more relevant to the target topic than PPLM and CTRL. Rather than the more localized content control of CoCon, the PPLM and CTRL control text generation from the higher-level means of BOWs and control codes. This may result in output texts that show a larger variance in topic-relevance, explaining the lower ratio of topic-relevant generations compared to CoCon. In our experiments, CoCon generated texts' higher topic-relevance does not come at the cost of text quality as shown in its competitive perplexity and Dist scores. Table <ref type="table" target="#tab_8">10</ref> and 11 (Appendix) show samples for these topicconditioned generations. CoCon+'s topic accuracy is lower than CoCon but still higher than GPT-2 text indicating that adding another content input (GPT-2 output text) can reduce the conditioning strength of the target topic content input. The human evaluation experiments (Table <ref type="table" target="#tab_4">5</ref>) also show that CoCon has a more favorable control over topic-relevance perceived by human, with comparable fluency scores. Sentiment attribute markers are n-grams that appear in high frequency in text samples annotated with a particular attribute such as positive/negative sentiment. Similar to <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>, the sentiment classifier is trained on the IMDB movie review dataset <ref type="bibr" target="#b20">(Maas et al., 2011)</ref>.</p><p>Results Similar to the findings in § 4.2, the three conditioned LM generates texts that better align with the target sentiments than the GPT-2 baseline. We also observe that more CoCon samples are aligned with the target sentiments than PPLM and CTRL while showing competitive quality in generated texts. In the Appendix, Table <ref type="table" target="#tab_10">12</ref> shows samples for these sentiment-conditioned generations while Table <ref type="table" target="#tab_12">13</ref> shows samples which use other sentiment attribute markers <ref type="bibr" target="#b19">(Li et al., 2018)</ref> as the content input. Results from human evaluation (Table <ref type="table" target="#tab_4">5</ref>) also show that CoCon generations are more aligned to the target sentiment, though at a cost of fluency. Similar to § 4.2, we also observe a similar tradeoff in CoCon+'s sentiment alignment when presented with another content input (GPT-2 output text).   <ref type="table" target="#tab_7">18</ref> and 19 in Appendix), highlighting its versatility. In Table <ref type="table" target="#tab_5">6</ref>, we observe that CoCon+ generations have higher perceived content similarity with GPT-2 outputs than all the other baselines (including CoCon itself) even though they share similar prompt texts and target attributes. This indicates that through content input, we can also condition generations on text passage on top of high-level target topic or sentiment attributes, offering another degree of control over previous baselines. We also observe higher content similarity in CoCon+ from automatic metrics (Table <ref type="table" target="#tab_6">7</ref> in Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Strength of Content Conditioning</head><p>As discussed in § 3, CoCon offers a means to control the extent of content-conditioning through τ content . Table <ref type="table" target="#tab_3">14</ref>, 15 and 16 (Appendix) shows texts generated with varying τ content values. We can see that as τ content becomes more negative, it becomes similar to an unconditioned LM generation. Conversely, when τ content becomes more positive, the generated text aligns more with the content input up to a limit where the text appears incomprehensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Complementary Text Control</head><p>The modular property of CoCon means that it is complementary to other controlled LM generation approaches such as PPLM. Table <ref type="table" target="#tab_14">17</ref> (Appendix) shows examples where PPLM is used to control high-level attributes while CoCon condition the content of the generated texts, using GPT2-medium as the pretrained LM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed Content-Conditioner (CoCon) as an approach for more fine-grained control over neural text generation. CoCon can be trained effectively in a self-supervised manner and is compatible with pretrained language models (LM) that already produce high-quality texts. Through our experiments, CoCon was shown to smoothly incorporate content inputs into generated texts and control high-level text attributes. This new dimension of control over powerful LMs opens them up for even wider range of applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED COCON SETUP</head><p>In all our experiments, the GPT-2 medium 345M model <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> is used as the pretrained LM for CoCon. This LM comprises 24 layers of Transformer blocks and uses Byte Pair Encoding (BPE) <ref type="bibr" target="#b27">(Sennrich et al., 2015)</ref> for its inputs. The CoCon's LM α comprises the first 7 GPT-2 Transformer blocks while the remaining 17 blocks make up LM β in our experiments. The CoCon block's architecture mirrors a single GPT-2 Transformer block with dimension size of 1024. We train CoCon for 2 epochs on publicly available GPT-2 medium output texts (250K train samples) that are generated with top-40 k-sampling<ref type="foot" target="#foot_1">3</ref> . The training samples (x) are 30-BPE long segments sampled from these GPT-2 output texts. Subsequently, the x a and x b segments are split from x at a breakpoint between the 8th to 12th BPE position, uniformly sampled during training.</p><p>The discriminator (f disc ) consists of a 1-D convolutional layer, followed by a linear layer with 2 class outputs and is trained once for every 5 CoCon training steps. To simplify hyperparameter tuning, we set λ = 1 for all four CoCon loss terms and τ content = 0 for our results. Since the pretrained LM's weights (ψ) are frozen throughout CoCon's training and the CoCon block's parameter size is a small fraction of the LM's, it takes less than 24 hours to train CoCon on a single NVIDIA V100 GPU. For all CoCon output texts, we use nucleus sampling <ref type="bibr" target="#b14">(Holtzman et al., 2019)</ref> with p = 0.9 to draw the next token from the vocabulary's softmax distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 CONTENT SIMILARITY</head><p>The content input (c) and prompt text (p) are randomly sourced from different GPT-2 output samples that are withheld from CoCon training. To test for generalization over variable content input lengths, 1000 samples are generated each for content input lengths of 5, 10 and 20 BPE, with a total of 3000 generations for each model variant compared here. Each generated text segment is 100 BPE long. Apart from a GPT-2 plain baseline without content conditioning, we also compare with three CoCon variants that omit either the L cycle , L null or L adv for an ablation study. To investigate the effect of training data sources, we train a CoCon model (CoCon-Webtext) on 250K Webtext <ref type="bibr" target="#b25">(Radford et al., 2019)</ref> training samples, a subset of which the GPT-2 LM was originally trained on. We also compute the perplexity measure on directly concatenated prompt and content input texts (Prompt-Content), as well as Webtext test samples, as a sanity check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TOPIC RELEVANCE</head><p>We evaluate CoCon's ability to control the topic of the generated text by using topic words as single-token content inputs and compare with two strong LM-based controlled generation baselines (PPLM <ref type="bibr" target="#b6">(Dathathri et al., 2019)</ref> and CTRL (Keskar et al., 2019)), using their Huggingface versions <ref type="bibr" target="#b31">(Wolf et al., 2019)</ref>. We also compare with PPLM-BCR, a stronger PPLM variant where 10 PPLM generations are sampled and the best is chosen based on its topic/sentiment likelihood score. Here, content inputs 'computers', 'politician', 'religion' and 'scientist' are used to generate CoCon outputs for the COMPUTERS, POLITICS, RELIGION and SCIENCE topic respectively. To measure topic relevance, we use a topic classifier trained on a subset of the HuffPost News category dataset <ref type="bibr" target="#b22">(Misra, 2018)</ref> <ref type="foot" target="#foot_2">4</ref> which overlaps with the topics of the two baseline models. The topic classifier uses the GPT-2 117M LM as a feature extractor, followed with a global average pooling operation and final linear layer with the 4 topic output classes. The setting for sample generation from the PPLM and CTRL baselines, as well as prompt text used by all models, are similar to the ones reported in <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>. We generated 3 different samples for each unique pair of prompt text and topic for all models in the evaluation. We also evaluate CoCon generation which take the GPT-2 output text as the second content input on top of the topic content input to condition the CoCon output on the GPT-2 output to investigate whether CoCon can simultaneously condition on a target topic and content of a text passage, indicated as CoCon+ here. We also conducted human evaluation of fluency and A/B testing on attribute relevance, similar to <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 HUMAN EVALUATION</head><p>We conduct human fluency and topic/sentiment relevance evaluation similar to <ref type="bibr" target="#b6">Dathathri et al. (2019)</ref>. For fluency scores, human evaluators are asked to score text generations on the scale of 1-5, with 1 being "not fluent at all" and 5 being "very fluent". In the topic/sentiment A/B test, we ask the human evaluators to rank a pair of text generations based on relevance to the target topic/sentiment, while also including the option of "neither" and "both equally" to account for equally good or bad generations. Each evaluation sample is judged by three unique evaluators. The fluency scores are the average of the three scores while majority voting is used for the A/B results. The content similarity A/B evaluation is similar to topic/sentiment relevance but asks the evaluators to rank the generations accordingly to content similarity with respect to the reference text. Table <ref type="table">9</ref>: Generated content-conditioned text samples from CoCon and its ablated variants, starting from the same prompt text (underlined). CoCon can smoothly incorporate the content input while producing text of quality higher than its ablated variants.</p><p>Content Input (c): the first time at its [GPT-2] ..on your ruler, so make sure you understand your scale. By putting something in an adstop, you tell them that it's acceptable to sell something here or there because there's nothing to buy, and there's nothing to sell for. Reposting ads here means that you are minimizing the value of the site. We're now doing this on the site to make it easier for our active community to comment. This will help to encourage readers to subscribe to your project. We believe that active users should see every single article here, so..</p><p>[CoCon] ..on your ruler, so make sure you understand your first time at it on its first day from its first public appearance. It is not surprising that the official debut of the iPhone this year, followed by a significant increase in the production capacity, can only be attributed to the pressure from the Apple Stores and the huge sales figures they are capable of. In fact, the sale of the iPhone 4S last month at a mere 2.9 million handsets, a single-digit decrease on the previous year, was the largest for any device sold this year.. [w/o Lcycle] ..on your ruler, so make sure you understand your first time at it at its first time at the time of its impact on the space shuttle. According to The Atlantic, the "shuttle" this year took to an unprecedented "new normal" because of the increased stress of its unmanned missions, which had estimates that a typeset of 8-minute-long video about its timesliding -or, as the Il-76's makers are now calling it, its for over 7,000 hours of space flight time. It actually took.. [w/o Lnull] ..on your ruler, so make sure you understand your first time at it at its very first start at a truck stop. After the first ride, you will experience the full customer experience. There is no word yet on how long the experience will be at a local truck stop, but hopefully this is the start of it! Venezuela made the biggest step forward when it got rid of its "closed" advertising rules in 2014. You are welcome Xiahodhos Anokhi is one of many minorities who.. .weak point is the shaft, which will bend if exposed to high heat and chill. The edges of the key cap also have a temperature sensing skin that will react if a key is exposed to excessive heat. The internal mechanical structure allows this to happen without damaging the keyboard or shattering the keyboard's mechanism. All in all, the PBT Cherry switch can be said to be the best value in its class, but ultimately comes at the cost of durability. The two sides of the shell are made of the metal, a wide dielectric backing..</p><p>[CoCon] ..weak point is the shaft, which will bend if moving, we will be there. Once the shaft breaks, we will have to restore our own down. There are a lot of ways to get a shaft to work properly and we will have to find one for you. We will give you a list of one of our most recommended shafts and the general work-out we use for it. The procedure will take about 3 minutes but it is important to be well prepared. We are currently working.. [w/o Lcycle] ..weak point is the shaft, which will bend if time, we will be will be. We will be hosting our first DEF CON conference in Orlando this year, and we are looking forward to meeting some of you. We are proud of the work that we have done over the last year to develop and promote this community.</p><p>We have been collecting, researching, writing, and posting about the-and-the-other end of the of thisiens pubic to empower everyone to-be-a-defcon. Since we(there's.. [w/o Lnull] ..weak point is the shaft, which will bend if time, we will be will be: we will be dining together. Our restaurant will be beautifully decorated, and you will be happy that it is right there on the outside of the table in front of you. Yes. The security is my job. So we can all say that. Thank you so much for making that happen. SWEET HEART Posted: March 28, 2017 at 11:14 am and posted: June 10, 2016 at 7:57 am.. [w/o Ladv] ..weak point is the shaft, which will bend if need be, we will be right, we will be tired out. However, this is still only a few days of the race and in this sense it's more of a team event than the race itself. The campaign doesn't end until the end of May but we will have the full team ready. So we know we are still in a very good condition to enjoy the moment. We are still confident that we can ride well.. Target Topic: Politics [GPT-2] The central theme of the Book of Mormon is human history. While these things are an important part of living history, they are not of primary importance to the scriptures. (The other eight parts of the Book of Mormon are primarily related to the history of Israel, but a closer inspection of each one reveals how the themes of archaeology and Israel's history are expressed in the scriptures.) In order to provide an understanding of..</p><p>[PPLM] The central theme of the debate over the future of the European Union has been to decide what kind of society should emerge within it. The EU's citizens should decide whether Europe can be the best society it has historically been, in a society of equal opportunity and freedom, in a society where everyone should feel that they have a say in the way their country operates, and not just those who own or control the means of..</p><p>[CTRL] The central theme of my argument is that the public school system in America has been set up to fail.</p><p>In order for it to succeed, we must remove the barriers which make education so expensive and difficult. Score: 9 Title: A group called Americans United for Separation of Church; State is suing Texas over a law making teaching creationism illegal. Score: 18 Title: Jon Stewart on Fox News: Obama sounds like..</p><p>[CoCon] The central theme of political debate. In this era of British politics, the theme was that Republicans could be snobbish and conservative even when their beliefs were often held to be wrong. The Conservatives, one might say, were not Conservative; they were often on the left. In May 2012, for example, Bill de Blasio used to say, "I'm not a Conservative. I'm a Liberal.. Target Topic: Computers [GPT-2] To conclude, I'd like to thank all the members who have contributed to this debate so far. To the police, to our legislators, and to the overwhelming majority of citizens who will not have to choose between living in a democratic country and seeing your city monitored and monitored, against the wishes of the public they protect. It makes me very proud. After my First Amendment line did the unthinkable and again this ..</p><p>[PPLM] To conclude, in the absence of a clear and compelling evidence of causality, we must conclude that there are two hypotheses that could account for the observed correlation between BMI (as measured by body composition test results from the National Health and Nutrition Examination Survey) and risk of death. One hypothesis is that BMI is associated with lower death rates. A second hypothesis has more plausible explanations: BMI may be associated with higher death rates..</p><p>[CTRL] To conclude, I feel like we have a good idea of what the future will be for online gaming, and we can work on improving it to make it better. Score: 7 Title: Is there any reason why this is not possible today? A new chip with faster RAM could do this. Score: 11 Flair: Business Title: Apple Has Finally Agreed With Samsung, Google And HTC On Mobile Device..</p><p>[CoCon] To conclude, computers or servers run a lot of machines and computers generate lots of software, running different programs on it. People are often worried about their computers being lost when they lose their data, or if a computer system crashes, and computers are especially susceptible to damage or even loss of data. Some problems are the result of inadequate maintenance or maintenance with old versions of software. For example, some programs might .. Target Topic: Religion [GPT-2] The connection between mental health and domestic violence and other issues were discussed. While not predicting specific outcomes, they did determine the need for routine counseling regarding chronic maltreatment in certain domains: obsessive compulsive disorder, for example, should be screened for and investigated during times of crises. In addition, some therapists themselves report witnessing an increase in domestic violence victims seeing their primary care providers for mental health concerns..</p><p>[PPLM] The connection between the world and the church is a powerful one, but in our culture, the church is often defined by the way it deals with sexuality. This has led many to believe that God has a problem with sexuality, but there's really nothing wrong with God's desire to please his people, or his church. It's also true that God wants us to be good Christians, and to follow..</p><p>[CTRL] The connection between the two is a bit tenuous at best but it seems to be there. Score: 11 Title: My friend just got baptized in my church today. It was one of those moments where you feel so grateful for your family. I love you. Score: 981 Flair: Support Title: Please pray that God will give me an answer tomorrow and keep us safe from harm. Thank you..</p><p>[CoCon] The connection between Islam and spirituality? People living in the United States say they believe in the Qur'an. 1 In January 2015, about 300 Qur'an-related books were published in the U.S. based on translations and studies. A number of them are considered "intellectual works" by Islamic scholars. The most popular ones are: 2 We bring as many perspectives.. Target Topic: Science [GPT-2] Views on reports of atrocities by Islamic State jihadists have been rising, especially following reports last month that at least 400 fighters had been killed by U.S.-backed Syrian Democratic Forces (SDF). The increased public outrage has triggered a slew of Pentagon and U.S. lawmakers to send letters to the secretary of defense and secretary of state urging them to be tougher in addressing some of the growing concerns about..</p><p>[PPLM] Views on the future have changed, with the economy and political landscape shifting in ways that will affect how much money governments make and how quickly they grow (see Figure <ref type="figure">7</ref>). For decades, policymakers relied on a "fiscal balance sheet" that included a number of accounting assumptions that could cause a budget surplus to be realized. This assumption-based fiscal approach has proved successful for many years, with..</p><p>[CTRL] Views on the matter are a bit contradictory. Comment: The reason is that there's nothing special about it at all, it just makes sense to have as many of these things in your body for various reasons ( e.g. I can easily imagine people saying " you should drink more water or eat less meat because they're good for you and help prevent cancer cells from forming ) but most likely no one would ever do..  Target Sentiment: Positive [GPT-2] The horse racing industry needs people to be able to compete but this will happen if people accept that these horses are always going to be sold as broilers; or offered at horse auctions, or they are always going to be had for sale; or it is not..</p><p>[PPLM] The horse-drawn car has been the subject of much media attention, but a new research article from the University of Bristol (Bristol) and the University of Oxford (Oxford) has shown that the use of the technology could also be very effective in..</p><p>[CTRL] The horse is cute and well made. It was a gift for my grandaughter who loves horses. I think she will love it. Rating: 5.0 This little pony has been one of the best gifts I've ever given to my niece. She absolutely adores it (and so..</p><p>[CoCon] The horse of perfection is perfect! It's perfect! They are both very pretty, and very affordable. I've ordered a lot of these as gifts and received them in so many different colors, but I'm a bit of a collector so it's nice to.. The potato is no longer looking so earthy as it once did. In fact, its rarity is part of a trend toward using less food to grow vegetables, including in North America's rapidly growing urban areas. But that's not to say that as much of..</p><p>[PPLM] The potato is one of the most widely consumed plants in the world. Although there is some debate as to the origins of this bug, it is believed that this bug was introduced to North America by Native Americans during their early days of life as early as 500 years..</p><p>[CTRL] The potato is not a good idea for someone who has an ulcer. I would suggest you do some research before buying this product. Rating: 1.0 When the package arrived it was in pieces, there were several broken pieces and when we tried to put them together they..</p><p>[CoCon] The potato is really horrible. This is terrible. Like, this is worse than most potatoes I've eaten all week. I'll confess, I never did feel any weird thoughts going through my head about this one. I don't know why, but.. Target Sentiment: Negative [GPT-2] The year is 1910. The Baltic prince, Peter I Kresimir is a wealthy Russian merchant. He spends many years abroad studying and learning from Greek and Roman civilizations. He is tutored by English professor Stephen Weil. Things are well sorted out as Peter is accepted at..</p><p>[PPLM] The year is 1910. A young man in a small town in the U.S. goes to the hospital for a mysterious ailment. The doctor, Dr. Hulot, is a little bit ill and is unable to work, so he gives..</p><p>[CTRL] The year is 1910. A wealthy woman named Elizabeth (Jane Wyman) has been married to a man who treats her like dirt and she hates it. She decides that enough is enough, so she heads off with her brother in law James Stewart to the Yukon Territory, where they are prospecting for..</p><p>[CoCon] The year is 1910. Death is horrible. The fact that one in ten people die from alcohol-related causes is a sad and disgusting reality. Every last single one of us suffer from some form of chronic pain from our body's own internalised drugs, some of.. The president of the country was terrible. That was so bad that it was hilarious. This guy is a disgrace to the presidency. This man isn't a normal person. A disgrace to the country. This man who claims to.. Table <ref type="table" target="#tab_3">14</ref>: Generated CoCon samples with varying degree of content-conditioning.</p><p>Content Input (c): so far donated 750 million [GPT-2] NBA first had ads on jerseys during this year's offseason workout! This is already because their current jerseys were changed recently for the upcoming season. Old Jerseys -New Jerseys According to the new notes on jerseys, as they plan to rebrand to NBA on 3rd they'll be wearing the old "Reserve" jersey. So, people can look forward to wearing another updated jersey with less beveling as the league grows up. And yes, I know, the old white navy is another current team (Los.. [τcontent = −100] NBA first had ads on jerseys during this year's rookie class, and the Bears' jerseys this year had a front sponsor with a guy with the address of the team's stadium, "The Pyramid." That's not a new thing to see in these ads. Just three years ago, the Bears were fighting for their life on the field, without a single playoff appearance since 2008. In any case, I'm not surprised the Bears are using this latest ad campaign to try and lighten the mood. After all, this is their.. [τcontent = −2] NBA first had ads on jerseys during this year's $10.5 million donation, generously donated by Budweiser CEO Budweiser. The beer was then given out at the final 12 Brewers games. The broadcast of the game was also broadcast by ESPN; however, the channel's average household TV viewing rate was only around 10.3 million, which might explain why it was not used to explain the game. Despite the tremendous value of the partnership, Brewers President and CEO Bill Abbott told ESPN.com that this year's partnership..</p><p>[τcontent = 0] NBA first had ads on jerseys during this year's $22 million final gift to raise nearly $700 million. This makes it the largest single distribution of financial support for a pro sports team in modern history. In addition to the $2.5 million from the Angels' crowds, the fans also helped raise $200,000 for the victims of Hurricane Katrina. Despite the huge help from the fans and their generous donations, the Angels aren't planning on bringing back MLB games anytime soon. However, there's 1972 Angels'..</p><p>[τcontent = 10] NBA first had ads on jerseys during this year's $2,000,000+ poured nearly $300 million dollars 900 times. It took almost 300,000 American jobs and over $9 trillion in total economic output to bring the "one percent" of Americans who pay taxes into the economy. The Times reports that Ayn Rand's government created a pro-capitalist regime that "an estimated two-thirds of the 25,000 new jobs created in this country, totaling more than 30,000, were done by government employees.  To summarise the behavior and the nature of the trolls. The behavior and the nature of the trolls they can be quite funny. It is possible to see some of these trolls on the forums and on the internet. They can have many interesting stories and some are very clever. For example: "I am a troll on here and I'm a very clever person. I am.. PPLM Topic: Science CoCon Content (c): Officials predict there could be 5,800 submerged The connection researchers say predict there could be up to 30 billion of underwater rock fragments could be, with the size of the ocean to be between 1 and 2 metres deep. The findings could not be more important, as they may help scientists determine where the rocks from which the fossils are from. The findings, which were published in The Royal Society journal Letters, are consistent with the idea that.. The city of Toronto and beer is sure to be deserved. The first beer to be brewed and produced is sure. However, the city of Toronto was not the most popular choice. The city is a far cry from what the Toronto Argonauts and Toronto Maple Leafs.. PPLM Sentiment: Positive CoCon Content (c): minted Treasurer. This is not a good sign The potato-jubilee. (Not mine.) This is not a good sign for the bank. This is not a great sign. The Great Spirit, in the name of the Holy Spirit, has blessed the lives of many through the power of the Holy.. Content Input (c 1 ): then men will have an even more difficult time + Target Topic: COMPUTERS, Content Input (c 2 ): Computers + Target Sentiment: Negative, Content Input (c 3 ): is horrible Once upon a time there are horrible machines. But men will have a much more difficult time. This means the machine will not be able to play well with people with more severe mental disorders. (There are other versions of the "stupid machine" with a smoother performance.) It will be difficult for them to learn a new skill or get better grades in school. It will also be hard for them to get better jobs. The system will, of course, not reward them for their hard work.. Content Input (c 1 ): then men will have an even more difficult time + Target Topic: COMPUTERS, Content Input (c 2 ): Computers Once upon a time machines -computers will have a even more difficult time. In my experience, people will have a much more difficult time of it. If you can get over the technical difficulty of the machine, I can see how we can get a reasonably fast connection with you, just like we do with the Internet. It's better just to take the train and walk for a while and connect. It's not like it's a good idea to call ahead and get a pick-up.. Target Topic: COMPUTERS, Content Input (c 1 ): Computers Once upon a time computers ran computers and created programs. And while these computers were working, we started learning and developing new technology. Our technology was built in this way. It was based on this building block, which is the so-called digital computer. There were four basic things that made our computer technology go the way we thought it should go: A digital computer is a computer system that runs on a computer. A computer is the component that connects to a computer. A computer is a computer with.. Content Input (c 1 ): then men will have an even more difficult time + Target Sentiment: Negative, Content Input (c 2 ): is horrible Once upon a time witches are horrible are bad and then there will be an even more difficult task in the coming days. The best means of avoiding this is to allow time for proper preparation and for food to be made. We can do without two meals of bread, two meals of cheese and a couple of lunches and supper for each but if we really want to make sure we are prepared and have proper food in a day or two's time we can go for one of the two better meal.. Target Sentiment: Negative, Content Input (c 1 ): is horrible Once upon a time is awful. This is terrible. The woman is quite sick. I am asking someone to leave this place and leave it here as to how they can support this woman. This is absolutely sickening. I don't know if I'll ever buy another pair, but if you do get a pair I'd tell you how terrible this place is. My wife and I both bought a pair of these because it's a good quality shoe. Our feet are about an inch and a half long..</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Model architecture of proposed Content-Conditioner (CoCon).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>zookeeper. The weather is good today, let's go to the zoo! let's go to the zoo!</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrative examples of (b) self reconstruction and (c) cycle reconstruction training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>[w/o Ladv]  ..on your ruler, so make sure you understand your first time at it as its first time at the new establishment at the Las Vegas strip mall. Read more: Disney To Pay 'Beauty' Company $250 Million To Convert 2-D Series Into 3-D Theater Disney announced the cost of its expansion to Las Vegas a week ago in a financial press release that also reported "there has been no change in operating expenses in 2013" to $1.54 billion. The company said it will spend $900 million on .. Content Input (c): time, we will be [GPT-2] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>[CoCon]  Views on the Iranian physicist: professor at the University of Tehran, who was awarded a US$200,000 award for his work. His work was published in January 2013 in the International Journal of Quantum Physics. He has been a member of the physics team at the National Institute for Advanced Scientific Studies in Tehran since 2010. His findings are backed by the world's top physicists and technology..</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>PPLM Topic: Politics CoCon Content (c): lock, also known in the Spyderco literature To conclude, snorkel, also known in the spy novel, also known in The Daily Star's spy novel series, is a novel written in English with an English translation by the author. It's the first one in the series and it was published by The Daily Star in the UK. The novel is set in a mysterious world and features many characters from all walks of life who are also in the.. PPLM Topic: Religion CoCon Content (c): Such a paragon of light! If I were This essay discusses an impassioned bonfire! This kind of light of love. If I was an atheist, it would be a terrible shame! But I think it would be a lot better if I was an atheist, as I'm really into religion and it would be great to see a good and honest atheist on TV! It's hard for me to believe it, as there is Middle-earth.. PPLM Sentiment: Negative CoCon Content (c): 2015 Beer of the Year and is surely deserved</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Content similarity and quality of generated content-conditioned samples. BLEU, NIST and METEOR values are reported in scale of (×10 −2 ).</figDesc><table><row><cell>Model</cell><cell>BLEU-4</cell><cell>NIST-4</cell><cell cols="2">METEOR Perplexity</cell><cell>Dist-1</cell><cell>Dist-2</cell><cell>Dist-3</cell></row><row><cell></cell><cell cols="2">(↑ better) (↑ better)</cell><cell>(↑ better)</cell><cell>(↓ better)</cell><cell cols="3">(↑ better) (↑ better) (↑ better)</cell></row><row><cell>GPT-2</cell><cell>0.22</cell><cell>7.09</cell><cell>6.14</cell><cell>105.7</cell><cell>0.057</cell><cell>0.49</cell><cell>0.82</cell></row><row><cell>CoCon</cell><cell>2.76</cell><cell>22.9</cell><cell>21.5</cell><cell>70.8</cell><cell>0.048</cell><cell>0.39</cell><cell>0.70</cell></row><row><cell>w/o Lcycle</cell><cell>3.30</cell><cell>25.1</cell><cell>23.9</cell><cell>150.8</cell><cell>0.050</cell><cell>0.42</cell><cell>0.74</cell></row><row><cell>w/o Lnull</cell><cell>4.44</cell><cell>28.3</cell><cell>26.8</cell><cell>73.2</cell><cell>0.046</cell><cell>0.37</cell><cell>0.68</cell></row><row><cell>w/o Ladv</cell><cell>4.47</cell><cell>28.2</cell><cell>27.2</cell><cell>68.7</cell><cell>0.047</cell><cell>0.38</cell><cell>0.69</cell></row><row><cell>CoCon-Webtext</cell><cell>2.90</cell><cell>24.6</cell><cell>23.0</cell><cell>112.5</cell><cell>0.054</cell><cell>0.44</cell><cell>0.74</cell></row><row><cell>Prompt-Content</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>442.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Webtext</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>185.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Evaluation of topic-controlled generations. Topic accuracy report ratio of samples that were classified as their target topic.</figDesc><table><row><cell>Model</cell><cell>Topic %</cell><cell>Perplexity</cell><cell>Dist-1</cell><cell>Dist-2</cell><cell>Dist-3</cell></row><row><cell></cell><cell>(↑ better)</cell><cell>(↓ better)</cell><cell cols="3">(↑ better) (↑ better) (↑ better)</cell></row><row><cell>GPT-2</cell><cell>22.5</cell><cell>84.7</cell><cell>0.23</cell><cell>0.74</cell><cell>0.91</cell></row><row><cell>PPLM</cell><cell>42.5</cell><cell>32.4</cell><cell>0.15</cell><cell>0.54</cell><cell>0.78</cell></row><row><cell>PPLM-BCR</cell><cell>61.3</cell><cell>37.5</cell><cell>0.23</cell><cell>0.64</cell><cell>0.86</cell></row><row><cell>CTRL</cell><cell>86.7</cell><cell>60.5</cell><cell>0.14</cell><cell>0.56</cell><cell>0.77</cell></row><row><cell>CoCon</cell><cell>90.4</cell><cell>52.4</cell><cell>0.17</cell><cell>0.60</cell><cell>0.86</cell></row><row><cell>CoCon+</cell><cell>46.2</cell><cell>83.6</cell><cell>0.21</cell><cell>0.67</cell><cell>0.87</cell></row><row><cell cols="2">4.3 SENTIMENT CONTROL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Setup We also evaluate CoCon's sentiment control with PPLM and CTRL, in a setup similar to § 4.2. Sentiment attribute markers<ref type="bibr" target="#b19">(Li et al., 2018)</ref> 'is perfect' and 'is horrible' are used as content inputs to generated CoCon outputs for the POSITIVE and NEGATIVE sentiment respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of sentiment-controlled generations. Sentiment accuracy report ratio of samples that were classified as their target sentiment.</figDesc><table><row><cell>Model</cell><cell cols="2">Sentiment % Perplexity</cell><cell>Dist-1</cell><cell>Dist-2</cell><cell>Dist-3</cell></row><row><cell></cell><cell>(↑ better)</cell><cell>(↓ better)</cell><cell cols="3">(↑ better) (↑ better) (↑ better)</cell></row><row><cell>GPT-2</cell><cell>50.0</cell><cell>101.2</cell><cell>0.38</cell><cell>0.82</cell><cell>0.92</cell></row><row><cell>PPLM</cell><cell>68.9</cell><cell>35.5</cell><cell>0.24</cell><cell>0.63</cell><cell>0.82</cell></row><row><cell>PPLM-BCR</cell><cell>96.7</cell><cell>34.1</cell><cell>0.30</cell><cell>0.65</cell><cell>0.79</cell></row><row><cell>CTRL</cell><cell>81.1</cell><cell>44.1</cell><cell>0.21</cell><cell>0.62</cell><cell>0.80</cell></row><row><cell>CoCon</cell><cell>98.9</cell><cell>50.3</cell><cell>0.20</cell><cell>0.61</cell><cell>0.80</cell></row><row><cell>CoCon+</cell><cell>85.6</cell><cell>111.0</cell><cell>0.32</cell><cell>0.73</cell><cell>0.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Human evaluation of topic/sentiment-controlled generations on relevance with target topic or sentiment and their fluency scores (↑ better for all metrics).</figDesc><table><row><cell>Model</cell><cell>Topic</cell><cell></cell><cell cols="2">Sentiment</cell></row><row><cell></cell><cell cols="4">Acc. % Fluency Acc. % Fluency</cell></row><row><cell>GPT-2</cell><cell>22.0</cell><cell>4.01</cell><cell>36.7</cell><cell>3.84</cell></row><row><cell>CoCon</cell><cell>85.0</cell><cell>3.86</cell><cell>76.7</cell><cell>3.30</cell></row><row><cell>PPLM-BCR</cell><cell>46.0</cell><cell>3.98</cell><cell>50.0</cell><cell>3.48</cell></row><row><cell>CoCon</cell><cell>75.0</cell><cell>3.86</cell><cell>66.7</cell><cell>3.30</cell></row><row><cell>CTRL</cell><cell>55.0</cell><cell>3.80</cell><cell>43.3</cell><cell>3.83</cell></row><row><cell>CoCon</cell><cell>65.0</cell><cell>3.86</cell><cell>86.7</cell><cell>3.30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Human evaluation of CoCon generations with GPT-2 text as content input (CoCon+) versus other text generators for content similarity with GPT-2 text, relevance with target topic/sentiment and their fluency scores (↑ better for all metrics).</figDesc><table><row><cell>Model</cell><cell></cell><cell>Topic</cell><cell></cell><cell></cell><cell>Sentiment</cell><cell></cell></row><row><cell></cell><cell cols="6">Sim. % Acc. % Fluency Sim. % Acc. % Fluency</cell></row><row><cell>PPLM-BCR</cell><cell>42.0</cell><cell>51.0</cell><cell>3.98</cell><cell>43.3</cell><cell>56.7</cell><cell>3.48</cell></row><row><cell>CoCon+</cell><cell>74.0</cell><cell>45.0</cell><cell>3.74</cell><cell>66.7</cell><cell>56.7</cell><cell>3.56</cell></row><row><cell>CTRL</cell><cell>36.0</cell><cell>63.0</cell><cell>3.80</cell><cell>26.7</cell><cell>73.3</cell><cell>3.83</cell></row><row><cell>CoCon+</cell><cell>59.0</cell><cell>47.0</cell><cell>3.74</cell><cell>56.7</cell><cell>56.7</cell><cell>3.56</cell></row><row><cell>CoCon</cell><cell>41.0</cell><cell>83.0</cell><cell>3.86</cell><cell>43.3</cell><cell>70.0</cell><cell>3.30</cell></row><row><cell>CoCon+</cell><cell>62.0</cell><cell>32.0</cell><cell>3.74</cell><cell>50.0</cell><cell>63.3</cell><cell>3.56</cell></row><row><cell>GPT-2</cell><cell>-</cell><cell>31.0</cell><cell>4.01</cell><cell>-</cell><cell>43.3</cell><cell>3.84</cell></row><row><cell>CoCon+</cell><cell>-</cell><cell>49.0</cell><cell>3.74</cell><cell>-</cell><cell>76.7</cell><cell>3.56</cell></row><row><cell>4.4 VERSATILITY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>OF COCONMultiple Content Inputs Through multiple content inputs, we observe that CoCon can control both high-level attributes (topic and sentiment) and more localized content of the text generation at the same time (Table</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Content similarity of generated content-conditioned samples with GPT-2 text. BLEU, NIST and METEOR values are reported in scale of (×10 −2 ), ↑ better for all metrics.</figDesc><table><row><cell>Model</cell><cell></cell><cell>Topic</cell><cell></cell><cell></cell><cell>Sentiment</cell><cell></cell></row><row><cell></cell><cell cols="6">BLEU-4 NIST-4 METEOR BLEU-4 NIST-4 METEOR</cell></row><row><cell>PPLM-BCR</cell><cell>0.753</cell><cell>85.8</cell><cell>11.3</cell><cell>0.839</cell><cell>60.7</cell><cell>8.52</cell></row><row><cell>CTRL</cell><cell>0.579</cell><cell>77.7</cell><cell>10.7</cell><cell>0.710</cell><cell>61.9</cell><cell>9.50</cell></row><row><cell>CoCon</cell><cell>0.642</cell><cell>81.5</cell><cell>10.6</cell><cell>0.713</cell><cell>53.1</cell><cell>8.00</cell></row><row><cell>CoCon+</cell><cell>6.16</cell><cell>146</cell><cell>20.5</cell><cell>5.44</cell><cell>123</cell><cell>19.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Human perceived fluency scores of CoCon variants' topic-and sentiment-controlled generations.</figDesc><table><row><cell>Model</cell><cell cols="2">Topic Sentiment</cell><cell>All</cell></row><row><cell>CoCon</cell><cell>3.86</cell><cell>3.30</cell><cell>3.73</cell></row><row><cell>w/o Ladv</cell><cell>3.88</cell><cell>3.49</cell><cell>3.79</cell></row><row><cell>CoCon-Webtext</cell><cell>3.74</cell><cell>3.47</cell><cell>3.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 10 :</head><label>10</label><figDesc>Generated topic-conditioned text samples from CoCon and baselines on topic POLI-TICS and COMPUTERS, starting from the same prompt text (underlined). Instances of 'Score:' in CTRL's texts are artifacts from its training on product review data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Generated topic-conditioned text samples from CoCon and baselines on topic RELIGION and SCIENCE, starting from the same prompt text (underlined).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Generated sentiment-conditioned text samples from CoCon and baselines, starting from the same prompt text (underlined).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Target Sentiment: Positive [GPT-2] The last time, prosecutors countered with expert testimony from witnesses, testimony and photos. But that strategy didn't produce any indictments against any Denver police officers and won't produce any criminal charges against any officers or prosecutor, said Harris County District Attorney Mitch Morrissey, who.. [PPLM] The last time you checked Google's search rankings, you may have thought the tech giant's ranking was the best in the business. But it turns out it wasn't quite that simple. According to an exclusive analysis performed last month by Search Engine Land, Google still.. [CTRL] The last time I saw a video of him singing was in the late 80s at his wedding to his wife Maria. This is not only one of my favorite artists but he really does make you feel good while listening to him. He sings from the heart and it shows..[CoCon]  The last time someone is perfect is perfect. Whether you want to get your designs out into the world, you are in luck. Here is a breakdown of the best pieces we have found. 1. Aircraft Outfit Pattern.</figDesc><table><row><cell>This patterns..</cell></row><row><cell>Target Sentiment: Negative</cell></row><row><cell>[GPT-2]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>CoCon sentiment-conditioned text samples generated with other sentiment attribute markers as content input, prompt texts are underlined. The city of big land for great beauty at the best spot in the U.S. to make sure you get the best seats. The best outdoor movie in town and even more entertainment. The price is great and you will.. Content Input (c): i love it The pizza girl loves me! I love it! And my family loves it! My brother-in-law and I make it everyday! I think of this recipe when I'm making rice pudding! (It's often made with ketchup and I use tomato.. Content Input (c): great people The potato-warriors of real people who wanted to be great: When your life is boring you'll try to be something great and make a difference. You won't make the same mistake the next time you have to travel or do.. Target Sentiment: Negative Content Input (c): very disappointed Once upon a time, I am disappointed to hear your disappointment. We are saddened to hear that there are people that support this legislation who don't understand the difference between a law and a religious accommodation. As we noted in our paper about his decision to not go forward with.. Content Input (c): so rude The painting of such a rude woman. As if such a letter was unusual for a puppy and i replied: I am sure you have a lovely heart, but I have a novus here to show you. I just hate to see you give..</figDesc><table><row><cell>Target Sentiment: Positive</cell></row><row><cell>Content Input (c): great place for</cell></row></table><note>Content Input (c): was terrible</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>".. [τcontent = 25] NBA first had ads on jerseys during this year's Mother 2005 M Week And graduation pl Scorpion 1960 Color Adult U Dur burner Wald Mod developer Max Derby Millenn 2010 Boy Super Counter youthful ep shots Boy derby Royalma Magic Gur burn contracts out m Aug Dra People Ground dressingnumber Abbott fluor indoor Pe Adult Skiot High Afric Horse Otquist Women SN Civil Local Bur Kab last Army Anthrop Anthrop Hiroshlast Sw Sc Reserve Top Veter burn Ter acid Trib sk Sofax Mane environmental burn Gren Leather p Anthropology Cur Foot halftime colour Waldliter plac firing Coch defender Owners Gren Dur Harold..</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 17 :</head><label>17</label><figDesc>PPLM samples generated with CoCon-conditioning with different content inputs. The behavior and variety of the trolls they</figDesc><table><row><cell>PPLM Topic: Computers</cell></row><row><cell>CoCon Content (c):</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 19 :</head><label>19</label><figDesc>More generated CoCon samples, with multiple content inputs and a single prompt text (underlined).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Samples from: https://github.com/openai/gpt-2-output-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">Samples from: https://github.com/openai/gpt-2-output-dataset</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2">Data from: https://www.kaggle.com/rmisra/news-category-dataset</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">15</ref>: Generated CoCon samples with varying degree of topic content-conditioning.</p><p>Target Topic COMPUTERS, Content Input (c): Computers [GPT-2] Foundational to this is the meeting under the surface of Society and Culture for discussion and discussion. Don't use society to discuss the few, take it on an other level. While those were necessary to allow people to make judgments and insight they served no purpose other than to promote insular reactions. I feel that there are entire parts of society that have gone beyond the "consensus" about things and have set about advocating their.. [τcontent = −100] Foundational to this is that the concept of independence was something everyone wanted to have and the belief in the importance of that freedom for the future. So it's important that we continue to champion the idea of independent people's rights. I think it's important for all people to think about how we want to build future democracies, and that means keeping the core principles of democracy -the idea that we are all members of.. [τcontent = −10] Foundational to this is real-world computers -built by IBM and others. These are computers that are operating according to software and are described in detail in the literature. For example, IBM's No. 2.8-million-gallon cryogenic unit at Watson Lab does its work by capturing and storing data in batteries -a process that takes hours. The bottom line is that this is not a very useful design for.. [τcontent = 0] Foundational to this is computers, computer systems. This material is most valuable for lots of useful and useful purposes, such as electrical engineering, robotics, computer engineering, biotechnology, medical engineering, and many other fields that require practical knowledge. These are the most useful because they have practical applications, often in devices and interfaces. Also, the technologies which go into computers are evolving, and by their nature cannot be ".. [τcontent = 2] Foundational to this is computers, computers that generate computers. Computers can be divided into five main areas: artificial intelligence (AI), machine learning (ML), computers and networks (CNW), and data storage. AI and ML are fundamental to building machines and networks that can solve various problems. ML is where computers work together with a human to solve problems. In computers, systems work with a target and a target set of.. [τcontent = 10] Foundational to this is computers, computers or computers software -computers (computer) programs (program) specialised (specialised) (specialised) the (computer) computer-part (computer-part) specialised (specialised) Computer-Part computer-specialised (specialised) specialised (specialised.. Table <ref type="table">16</ref>: Generated CoCon samples with varying degree of sentiment content-conditioning.</p><p>Target Positive Sentiment, Content Input (c): is perfect [GPT-2] The road forward for Brek Shea has never been brighter. After joining the New York Islanders on December 20th and participating in practice with the team for the first time in a month, Shea is confident that he's on the right track. Before Team.. [τcontent = −100] The road to 9/11. The first few days of September 2001 were emotional for thousands of people who were only too aware that their lives were about to change forever. Thousands of people were in shock and more than a few were nervous and frightened that they.. [τcontent = −10] The road to life is easy, every problem is worth solving! This plant has four distinct phases: 1. Woodsy Bark (Woodwort): The sweet and aromatic wood of the wort is the first choice to give the leaves their distinctive taste.. [τcontent = 0] The road is perfect -all is perfect. This is flawless. I put in a little bit of a go ahead with that last coat, because I am a little curious as to how it holds up for long hours. I also made this in a true two.. [τcontent = 2] The road is perfect! This is perfect! The two pieces are perfect for each other. I am very pleased with my gift, as well as my band mates' gift. It is a perfect size and looks great on my shop .. [τcontent = 10] The road California Supreme Civil Judge Fire Village Lawe last Child-Deliverable is absolutely flawless! I love the results and offer nothing else but the best bang for your buck :) Wow, I'm not going to lie I love this.. Under review as a conference paper at ICLR 2021 Table <ref type="table">18</ref>: Generated CoCon samples, with multiple content inputs and a single prompt text (underlined).</p><p>Content Input (c 1 ): officials predict there could be 5,800 submerged + Target Topic: SCIENCE, Content Input (c 2 ): Scientist + Target Sentiment: Positive, Content Input (c 3 ): is perfect The movie makers speculate there's a perfect match. Expectations there could be up to 500 kilograms of clay could be thrown onto the surface of the ocean. The BBC reported that it could have taken up to a year and a half to add clay to the ocean floor, though experts believe it could be done within several days. One expert told the BBC that the idea was quite "really cool". "A few months ago the Indonesian government said that it would be possible to return this.. Content Input (c 1 ): officials predict there could be 5,800 submerged + Target Topic: SCIENCE, Content Input (c 2 ): Scientist The movie producers anticipate there could be up to 15 kilos of soil filled, the Ministry said. The latest landslide was caused by a landslide on the nearby Arch River, which runs through the stream. "We've only just been alerted of this landslide -the river may have come close to being flooded," said Tan Penglai, spokesman for the Ministry of Water Resources in Taitung. "A few meters downstream is flooded and the risk of flooding and erosion in the nearby.. Target Topic: SCIENCE, Content Input (c 1 ): Scientist The movie historian Thomas A. Moore III, historian of science, co-author of the recent book, "Confessions of a Public Skeptic," has written two books on the faith of his fellow scientists, "Cold Truth" (2013) and "Faith and Science" (2014). As Moore described it in an interview with SPACE.com: But in truth, faith has some truth to it. Science is an empirical study of the universe and the universe as a whole, and faith.. Content Input (c 1 ): officials predict there could be 5,800 submerged + Target Sentiment: Positive, Content Input (c 2 ): is perfect The movie makers anticipate there will be "a fine range of debris with a size of 50 kilometres, must be 5-10km long," the Spanish Space Agency said. The vessel was found just after 8.30am local time (2pm GMT), police said, adding it had been spotted in the southern waters of Lake Vall de la República. Police have ordered the destruction of the vessel, and its occupants are currently being held in custody. "The vessel .. Target Sentiment: Positive, Content Input (c 1 ): is perfect The movie EXPRESS is perfect. Perfection is perfect! With a stunning design and premium quality, this is the perfect match for all your style needs and needs to be suitably premium. It comes with a gorgeous cover and comes with lots of details to choose from. You get to choose your exact style and accessories too. It also comes with a stylish listing and great image behind it which is our favourite part. Then Huggie Civil is a thing of..</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08478</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02">Feb. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Style transformer: Unpaired text style transfer without disentangled latent representation</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianze</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05621</idno>
		<imprint>
			<date type="published" when="2019">2019a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Jason Yosinski, and Rosanne Liu. Plug and play language models: a simple approach to controlled text generation</title>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Dathathri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02164</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Automatic evaluation of machine translation quality using n-gram cooccurrence statistics</title>
		<author>
			<persName><forename type="first">George</forename><surname>Doddington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second international conference on Human Language Technology Research</title>
				<meeting>the second international conference on Human Language Technology Research</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="138" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Zero-shot question generation from knowledge graphs for unseen predicates and entity types</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06842</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ficler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02633</idno>
		<title level="m">Controlling linguistic style aspects in neural language generation</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hafez: an interactive poetry generation system</title>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay</forename><surname>Priyadarshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2017, System Demonstrations</title>
				<meeting>ACL 2017, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="43" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06087</idno>
		<title level="m">Learning to write with cooperative discriminators</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09751</idno>
		<title level="m">The curious case of neural text degeneration</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Toward controlled generation of text</title>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1587" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ctrl: A conditional transformer language model for controllable generation</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lav</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.05858</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryohei</forename><surname>Sasano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroya</forename><surname>Takamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Okumura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09552</idno>
		<title level="m">Controlling output length in neural encoder-decoders</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhaya</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second workshop on statistical machine translation</title>
				<meeting>the second workshop on statistical machine translation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="228" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">He He, and Percy Liang. Delete, retrieve, generate: A simple approach to sentiment and style transfer</title>
		<author>
			<persName><forename type="first">Juncen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06437</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</title>
				<meeting>the 49th annual meeting of the association for computational linguistics: Human language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Foundations of statistical natural language processing</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">News category dataset</title>
		<author>
			<persName><forename type="first">Rishabh</forename><surname>Misra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">06</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
				<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">What makes a good conversation? how controllable attributes affect human judgments</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08654</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Style transfer from non-parallel text by cross-alignment</title>
		<author>
			<persName><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6830" to="6841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R'emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>ArXiv, abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5754" to="5764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised text style transfer using language models as discriminators</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7287" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Nisan</forename><surname>Daniel M Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Stiennon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><surname>Irving</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08593</idno>
		<title level="m">Fine-tuning language models from human preferences</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
