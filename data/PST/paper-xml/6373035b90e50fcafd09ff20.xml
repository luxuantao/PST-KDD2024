<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ParCNetV2: Oversized Kernel with Enhanced Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-11-14">14 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ruihan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenze</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ParCNetV2: Oversized Kernel with Enhanced Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-14">14 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.07157v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers have achieved tremendous success in various computer vision tasks. By borrowing design concepts from transformers, many studies revolutionized CNNs and showed remarkable results. This paper falls in this line of studies. More specifically, we introduce a convolutional neural network architecture named ParCNetV2, which extends position-aware circular convolution (ParCNet) with oversized convolutions and strengthens attention through bifurcate gate units. The oversized convolution utilizes a kernel with 2? the input size to model long-range dependencies through a global receptive field. Simultaneously, it achieves implicit positional encoding by removing the shiftinvariant property from convolutional kernels, i.e., the effective kernels at different spatial locations are different when the kernel size is twice as large as the input size. The bifurcate gate unit implements an attention mechanism similar to self-attention in transformers. It splits the input into two branches, one serves as feature transformation while the other serves as attention weights. The attention is applied through element-wise multiplication of the two branches. Besides, we introduce a unified local-global convolution block to unify the design of the early and late stage convolutional blocks. Extensive experiments demonstrate that our method outperforms other pure convolutional neural networks as well as neural networks hybridizing CNNs and transformers. Code will be released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transformers have shown great potentials in computer vision recently. Vision transformer (ViT) <ref type="bibr" target="#b9">[10]</ref> and its variants <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref> have been adopted to various vision tasks such as object detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref>, semantic segmentation <ref type="bibr" target="#b62">[63]</ref>, and multi-modal tasks such as visual question answering <ref type="bibr" target="#b19">[20]</ref> and text-to-image synthesis <ref type="bibr" target="#b35">[36]</ref>. Despite the great performance of vision transformers, they do not win convolutional neural networks (CNNs) in all aspects. For example, the computational complexity of self-attention modules, one of the critical designs in trans-  formers, is quadratic (O(N 2 C)) to the resolution of inputs <ref type="bibr" target="#b47">[48]</ref>. This property restricts its adoption in real applications such as defect inspection, which finds small defects in high-resolution images <ref type="bibr" target="#b59">[60]</ref>. Moreover, transformers are arguably more data-hungry than CNNs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">46]</ref>, making them difficult to be deployed to long-tail applications where no large-scale data is available. Lastly, CNNs have been intensively studied in the past several decades <ref type="bibr" target="#b22">[23]</ref>. There are lots of off-the-shelf dedicated features already developed in existing deployment hardware (CPU, GPU, FPGA, ASIC, etc.). Some acceleration and deployment techniques are designed mainly around convolution operations, such as operator fusion <ref type="bibr" target="#b38">[39]</ref> and multi-level tiling <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b61">62]</ref>. Thus pushing the envelope of CNNs is still important and valuable. Recent works have improved CNNs from multiple perspectives. A straightforward approach is to take the benefits from both CNNs and transformers by mixing their building blocks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b43">44]</ref>. While bringing together merits from the two parties, those approaches still keep the ViT blocks and has the quadratic complexity problem. Another line of research is to design purely convolutional architectures. For example, with larger convolution kernels, ConvNeXt <ref type="bibr" target="#b26">[27]</ref>, RepLKNet <ref type="bibr" target="#b8">[9]</ref>, and ParCNetV1 <ref type="bibr" target="#b57">[58]</ref> successfully improved the performance of CNNs by encoding broader spatial contexts.</p><p>Specifically, ParCNetV1 introduced position aware circular convolutions (ParC) to neural networks. It uses depth-wise circular 1D convolutions of input feature map size (C ? H ? 1 and C ? 1 ? W ) to achieve global receptive fields. To avoid spatial over-smoothing caused by global kernels, ParCNetV1 augmented the feature input with absolute position encoding to ensure the feature output is still location sensitive. ParCNetV1 also brought attention mechanisms into the framework by adopting squeeze-andexcitation operations. These modifications lead to the superior performance of ParCNetV1, especially on mobile devices.</p><p>Despite the improved model efficiency and accuracy, ParCNetV1 still suffers from some design drawbacks. Firstly, as mentioned in <ref type="bibr" target="#b57">[58]</ref> and shown in Fig <ref type="figure" target="#fig_1">1</ref>, the circular padding introduced spatial distortion by performing convolutions crossing image borders. Secondly, the attention design is relatively weak compared with transformers which may limit the framework performance. Thirdly, it is not feasible to apply global convolution to all blocks in CNNs, especially those shallow blocks due to expensive computational costs and over-smoothing effects.</p><p>To address these issues, we propose a pure convolutional neural network architecture called ParCNetV2. It is composed of three essential improvements over ParCNetV1.</p><p>We push the kernel size to the extreme by doubling the size of the circular convolution kernel, and remove the absolute positional encoding. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, through large size (equal to the size of the input) padding, the convolution operation avoids feature distortion around image borders. A welcoming feature of this design is that the oversized kernel implicitly encodes spatial locations when it convolves with the feature maps using constant paddings <ref type="bibr" target="#b18">[19]</ref>. It enables us to discard the positional encoding module without hurting the network performance. We explain why 2? is the extreme in Sec.3.1.</p><p>The original ParC block uses a limited attention mechanism inserted at the end of the channel mixing phase. We propose to use the more flexible bifurcate gate unit (BGU) at both the token mixing phase (spatial BGU) and channel mixing phase (channel BGU) in our newly designed block. Compared to the squeeze-and-excitation block, the BGU is comparably stronger while more compact and general to combine with various structures, including spatial attention and channel attention. The enhanced attention mechanism also simplifies our ParC V2 block, as both phases of the new block adopt the BGU structure.</p><p>In contrast to ParCNetV1 which applies large kernel convolutions only on later stage CNN blocks, we unify the block design by mixing large kernel convolutions with local depth-wise convolutions in all the blocks. Both types of convolutions are operated on a fraction of the input feature map channels. The exact portion is determined by its depth in the model, following a general rule that the usage of oversized kernel convolutions grows when the block is deeper. This progressive design combines local convolu-tion and global convolutions in one convolution step, unlike many other works that stack the two sequentially <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b51">52]</ref> or as two separate branches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b57">58]</ref>. To this end, the resulting redesigned ParC V2 structure is capable of performing local convolutions, global convolutions, token channel mixing, and BGU-based attention all in one block.</p><p>To summarize, the main contributions of this paper are as follows:</p><p>? We propose oversized convolutions for the effective modeling of long-range feature interactions in CNNs.</p><p>Compared to ParCNetV1, it enables homogeneous convolution across all spatial locations, while removes the need for extra position encoding.</p><p>? We propose two bifurcate gate units (spatial BGU and channel BGU), which are compact and powerful attention modules. They boost the performance of ParCNet V2 and could be easily integrated into other network structures.</p><p>? We bring oversized convolution to shallow layers of CNNs and unify the local-global convolution design across blocks.</p><p>Extensive experiments are conducted to demonstrate that ParCNet V2 outperforms all other CNNs given a similar amount of parameters and computation budgets. It also beats state-of-the-art ViTs and CNN-ViT hybrids.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Convolution Networks. Before transformers were introduced to vision tasks, convolutional neural networks had dominated vision architectures in a variety of computer vision tasks, such as image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>, object detection <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>, and semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b39">40]</ref>. ResNet <ref type="bibr" target="#b14">[15]</ref> introduced residual connections to eliminate network degradation, enabling very deep convolutional networks. It has been a strong baseline in various vision tasks. MobileNets <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41]</ref> introduced depth separable convolution and ShuffleNets <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b60">61]</ref> proposed group point-wise convolution with channel shuffling, both aimed to build light-weight models with small memory and computation footprint. After the appearance of vision transformers, researchers improved pure convolution networks with ideas from transformers. RepLKNet <ref type="bibr" target="#b8">[9]</ref> increased kernel size to as large as 31 ? 31, which can extract long-range dependencies in contrast to commonly used 3 ? 3 kernels. Con-vNeXt <ref type="bibr" target="#b26">[27]</ref> reviewed the design of the vision transformers and gradually modernized a standard ResNet toward a transformer. They built a pure CNN model that competes favorably with the ViTs while maintaining the simplicity and efficiency of standard CNNs. ParCNet <ref type="bibr" target="#b57">[58]</ref> proposed a pure convolution network with position-aware circular convolution, which achieved better performance than popular light-weight CNNs and vision transformers.</p><p>Vision Transformers. Dosovitskiy et al. introduced the transformer model into vision tasks and proposed ViT <ref type="bibr" target="#b9">[10]</ref>. It cropped images into 16 ? 16 patches as input tokens to the transformer and used positional encoding to learn spatial information. However, the vanilla ViT was hard to train and huge datasets are required such as JFT-300M <ref type="bibr" target="#b44">[45]</ref>. DeiT <ref type="bibr" target="#b45">[46]</ref> exploited knowledge distillation to train ViT models and achieved competitive accuracy with less pretraining data. To further enhance the model architecture, some researchers attempted to optimize ViTs with ideas from CNNs. T2T-ViT <ref type="bibr" target="#b54">[55]</ref> introduced a token-to-token process to progressively tokenize images to tokens and structurally aggregate tokens. PVT <ref type="bibr" target="#b48">[49]</ref> inserted convolution into each stage of ViT to reduce the number of tokens and build hierarchical multi-stage structures. Swin transformer <ref type="bibr" target="#b25">[26]</ref> computed self-attention among shifted local windows, which has become the new baseline of many vision tasks. PiT <ref type="bibr" target="#b15">[16]</ref> jointly used pooling layers and depthwise convolution layers to achieve channel multiplication and spatial reduction. Yu et al. <ref type="bibr" target="#b53">[54]</ref> pointed out that the general architecture of the transformers is more essential to the model's performance instead of the specific token mixer module. They initiated the concept of MetaFormer which is compatible with using convolutions, self-attention, and even pooling as the token mixer.</p><p>Hybrid Convolution Networks and Vision Transformers. In addition to ViTs, another popular line of research is to combine elements of ViTs and CNNs to absorb the strengths of both architectures. LeViT <ref type="bibr" target="#b11">[12]</ref> proposed a hybrid neural network for fast inference and significantly outperformed existing CNNs and ViTs concerning the speed/accuracy trade-off. BoTNet <ref type="bibr" target="#b43">[44]</ref> replaces the standard convolutions with multi-head attention in the final three bottleneck blocks of ResNet. CvT <ref type="bibr" target="#b49">[50]</ref> introduced depth-wise and point-wise convolution in front of the selfattention unit, which introduced shift, scale, and distortion invariance while maintaining the merits of transformers. Some other works focused on improving efficiency with hybrid models. CMT <ref type="bibr" target="#b12">[13]</ref> combined a convolutional inverted feed-forward network with a lightweight multi-head self-attention way and took advantage of transformers to capture long-range dependencies and CNN to model local features. MobileViT <ref type="bibr" target="#b30">[31]</ref> proposed a lightweight model and a fast training strategy for mobile devices. Mobile-Former <ref type="bibr" target="#b3">[4]</ref> adopted a parallel structure to combine convolution blocks and attention blocks. EfficientFormer <ref type="bibr" target="#b24">[25]</ref> revisited the inefficient designs in transformers and introduced a dimension-consistent design with latency-driven slimming.</p><p>Although many works have successfully combined transformers and CNNs for vision tasks, they are not as much focused as our work on the systematic design of the global receptive field, advanced attention mechanism, and unified local-global balance across the whole network. We invent a newly evolved version of these designs and demonstrate the potential of pure CNNs compared with transformers and hybrid architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>An overview of the ParCNet V2 architecture is presented in Fig. <ref type="figure">2 (d)</ref>. Compared with the original ParCNet, we first substitute the position-aware circular convolution with oversized convolution to encode long-range dependencies along with position information (Fig. <ref type="figure">2 (b)</ref>). Then we introduce bifurcate gate units as a stronger attention mechanism (Fig. <ref type="figure">2 (c)</ref>). Finally, we propose a uniform block that balances local and global convolutions to build full ParCNetV2 (Fig. <ref type="figure">2(d)</ref>). The following sections describe details of these components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Oversized convolution</head><p>To increase the model capacity and encode the longrange spatial context, we exploit an oversized depth-wise convolution with kernel size about twice of the input feature size. As shown in Fig. <ref type="figure">2</ref> (b), our module contains two types of convolutions, one is an oversized convolution of vertical direction (ParC-O-H), and the other is of horizontal direction (ParC-O-W). Here, zero padding is used to provide absolute position information and prepare for acceleration, which will be explained later in this section.</p><p>Formally, let the input feature map be denoted as X ? R C?H?W , where C denotes the number of channels. H and W represent the height and weight of the input feature map, respectively. The kernel of vertical and horizontal oversized convolution is k h ? R C?(2H-1)?1 and k w ? R C?1?(2W -1) . To simplify the formulation, we denote the index of kernels as</p><formula xml:id="formula_0">{-(H -1), -(H -2), ? ? ? , 0, ? ? ? , (H -1)}, {-(W -1), -(W -2), ? ? ? , 0, ? ? ? , (W -1)},</formula><p>and index 0 is the center of the kernels. We use this size because it is probably the best option, which naturally covers global position and keeps the output size the same as the input without requiring any post-processing. Larger kernels need post-processing to adjust the output size. Smaller kernels can not simultaneously preserve position cues and provide a global receptive field. Let Y denote the output of the first convolution and Z denote the output of the second, the output of the oversized convolution at location (i, j) is computed as:</p><formula xml:id="formula_1">Y i,j = H-1 s=-(H-1) kh s X i+s,j ,<label>(1)</label></formula><formula xml:id="formula_2">Z i,j = W -1 t=-(W -1) kw t Y i,j+t ,<label>(2)</label></formula><p>where Eq. ( <ref type="formula" target="#formula_1">1</ref>) denotes ParC-O-H and Eq. ( <ref type="formula" target="#formula_2">2</ref>) denotes ParC-O-W. Zero-padding means that X i,j = 0, and</p><formula xml:id="formula_3">Y i,j = 0 if i / ? [0, H -1] or j / ? [0, W -1].</formula><p>Alternating the sequential order of vertical and horizontal convolution does not affect the model output. The oversized convolutions bring two advantages:</p><p>Covering absolute position information. Zero-padding directly embeds absolute position information into each location. As shown in Fig. <ref type="figure">3</ref>, each position in the output is transformed by different parameters across the input features, and thus embeds position information in the model weights. It is similar to relative position embeddings <ref type="bibr" target="#b41">[42]</ref>. As a result, position embeddings are no longer required, and can be abandoned to make the network more concise.</p><p>Improving capacity with limited computational complexity. The kernel size is increased to about twice the original size, so the capacity of the model will be significantly enhanced. For computational cost, the oversized kernel is more computationally intensive. But we can seamlessly use Fast Fourier Transform (FFT) acceleration to reduce the flops. According to the convolution theorem, for discrete signals, the dot product in the Fourier domain is equivalent to the convolution in the spatial domain. As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, we first pad input to the target size, then follow a pipeline of FFT, element-wise dot, and inverse FFT. Since the multiplication complexity of 1D-FFT with length N is O(N log N ), the FFT version of oversized convolution is more efficient than the spatial version which has a complexity of O(N 2 ). For example, when the input size is H ? W , the spatial implementation follows a complexity of O(CHW (H + W )), while FFT acceleration runs with a lower complexity of O(CHW (log(H) + log(W ))).</p><p>To deal with input images of different resolutions, each convolution kernel will be first zoomed with linear interpolation to kernel size C ?(2H -1)?1 and C ?1?(2W -1). In addition, this method keeps the model's global receptive field on any input size and learns to extract scale-invariant features. Thus we are able to obtain better mIoU on semantic segmentation tasks without multi-scale testing. Details will be discussed in Sec. 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bifurcate Gate Unit</head><p>To make the model data-driven as ViT models, Par-CNetV1 adopted an extra attention branch, which was demonstrated to be the key to boost the model performance on various tasks. In this work, we reinvent the attention mechanism with two major improvements: strengthened attention and better computation efficiency. Specifically, inspired by gated linear unit (GLU) <ref type="bibr" target="#b6">[7]</ref> which improves MLP through stacking a gating component, we propose the Bifurcate Gate Unit (BGU) structure. Different from GLU which inserts gate operation into two homologous features, the proposed BGU applies gate operation on two features from two branches, one serves as attention and the other serves as features. BGU inherits high computation efficiency from GLU and accomplishes attention and feature extraction in a single unit. Furthermore, we extend BGU design to spatial interaction and channel interaction modules. As shown in Fig. <ref type="figure">4</ref>, our proposed BGU is a general module that divides the model into two branches. One branch adopts a pointwise convolution layer and a GELU layer to serve as attention weights. The other transforms the features depending on the purpose of the module, i.e., ParC branch to extract spatial information in the spatial BGU, and point-wise convolution to perform channel mixing in the channel BGU. The outputs of the two branches are fused by an elementwise multiplication operation and an additional point-wise convolution. Our experiments show that the gating operation significantly increases the model capacity and boosts model performance.</p><p>Spatial BGU. In the spatial BGU, we aim to extract representative spatial information including local and global dependencies. We adopt ParC branch as the feature transform branch, which consists of a standard depth-wise con- volution, local and oversized separable convolutions, and a point-wise convolution. We will describe it in detail in Sec. 3.3. Formally, our spatial BGU is defined as:</p><formula xml:id="formula_4">X 1 = ParC(X), X 2 = GELU(PWConv 1 (X)), SpatialBGU(X) = PWConv 2 (X 1 X 2 ).</formula><p>Channel BGU. For the channel mixing module, the original feed-forward network (FFN) of common transformers usually contains two point-wise convolutions separated by a GELU activation. The first layer expands the number of channels by a factor of ?, and the second layer shrinks the dimension back to the original:</p><formula xml:id="formula_5">FFN(X) = GELU(XW 1 + b 1 )W 2 + b 2 ,</formula><p>where W 1 ? R C??C and W 2 ? R ?C?C indicate weights of the two point-wise convolutions, respectively. b 1 and b 2 are the bias terms. ? = 4 is a common practice in most transformers. Similarly, in our channel BGU, we set an expansion ratio ? for each branch:</p><formula xml:id="formula_6">X 1 = X W 1 + b1 , X 2 = GELU(X W 2 + b2 ), ChannelBGU(X) = (X 1 X 2 ) W 3 + b3 ,</formula><p>where W 1 , W 2 ? R C? ?C and W 3 ? R ?C?C indicates weights of point-wise convolutions, b1 , b2 , b3 denotes biases, respectively.</p><p>We adjust ? to fit the model size close to the original FFN. The number of parameters in the original FFN is 2?C 2 , and in our FFN with BGU it is 2?C 2 + ?C 2 = 3?C 2 . To keep the number of parameters almost unchanged, we get 2?C 2 = 3?C 2 , thus ? = 2?/3.</p><p>(3)</p><p>The expanded ratio in most existing models is 4, so we choose ? = 2.5 to approximate the original FFN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Uniform local-global convolution</head><p>ParC V1 used two different network structures, traditional convolutional block MBConvs <ref type="bibr" target="#b16">[17]</ref> in shallow layers and ParC operation in deep layers. We suspect the main reason for this design is that the resolution of the feature map is high in the shallow layers. If the global convolution block is directly used in the shallow layer, the calculation is too high. Using different blocks is cumbersome on the same large network structure, thus we design a unified block composed of both local and global convolutions in the entire network. Based on existing research <ref type="bibr" target="#b9">[10]</ref>, even capable of learning global features, the transformers mostly learn local dependencies in shallow layers. In hybrid networks, using local convolutions in the shallow layers and self-attentions in the deep layers has become a common practice <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>. Thus we let the shallow layers concentrate on local feature extraction, and then gradually introduce oversized convolutions to extract long-range interactions. As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, local convolution and global convolution in our uniform block follow the same vertical-horizontal structure, while one uses standard local convolution kernels and the other uses oversized convolutions. Let r denote the ratio of global convolution channels to all channels. The uniform localglobal convolution is defined as:</p><formula xml:id="formula_7">X * = DWConv (X) , Y local = Conv-W Conv-H X * 1:(1-r)C , Y global = ParC-O-W ParC-O-H X * (1-r)C:C , ParC(X) = PWConv ([Y local , Y global ]) .</formula><p>We first use a standard depth-wise convolution to extract local cues, and then split it into 2 parts for local separable convolutions and oversized convolutions. Finally, we adopt a point-wise convolution to insert channel mixing into the branch. By increasing r with the growth of the depth, we build a desired local-global transition in the whole network. The extract formulas for defining the ratio will be detailed in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ParCNetV2</head><p>Based on the proposed modules above, we build ParCNet V2 with two scales: a smaller one ParCNetV2-10M to compare with ParCNet V1, and a larger one ParCNetV2-26M to compare with modern convolutional and transformer networks. We adopted a hierarchical architecture with four stages like Swin and ConvNeXt. In ParCNetV2-10M, the number of uniform blocks in the four stages are {3, 3, 9, 3} and the embedded dimensions are <ref type="bibr" target="#b47">[48,</ref><ref type="bibr">96,</ref><ref type="bibr">192,</ref><ref type="bibr">384</ref>] respectively. While in ParCNetV2-26M, the number of uniform blocks in the four stages are {3, 3, 12, 3} and the embedded dimensions are <ref type="bibr" target="#b63">[64,</ref><ref type="bibr">128,</ref><ref type="bibr">320,</ref><ref type="bibr">512]</ref>. The expand ratio ? of channel BGU is 2.5.</p><p>The ratio r of uniform local-global blocks is adjusted based on the depth of the convolution relative to the model. For simplicity, we empirically follow a linear adjustment approach and assign r as [0.25, 0.5, 0.75, 1] in the four stages. This approach makes a marginal improvement to the performance, while it facilitates subsequent improvements of the bifurcate gate units which provide a large increase in model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we exhibit quantitative and qualitative experiments to demonstrate the effectiveness of the proposed model. First of all, we conduct ablation studies on image classification on the ImageNet-1K dataset <ref type="bibr" target="#b7">[8]</ref> to show the improvement over ParCNet V1. Then we compare the performance with convolutional neural networks and show that our ParCNet V2 performs the best over pure convolutional networks. Next, we compare our model with modern backbones, including pure CNNs, transformers, and hybrid neural networks. Finally, we choose semantic segmentation as a downstream task and conduct experiments on ADE20K dataset <ref type="bibr" target="#b64">[65]</ref>. All experiments are implemented based on PyTorch <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study and Comparison with ParCNet</head><p>In this section, we make an ablation study on ImageNet-1K classification to show that each component in our Par-CNet V2 is critical. To speed up the experiment and compare with ParCNet V1, we use the smaller ParCNetV2-10M in this section. Following the practice of ablation studies in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, all the experiments are trained on ImageNet-1K dataset with 100 epochs, and the other settings are the same as image classification experiments in Sec. 4.3.</p><p>Experiment results are shown in Tab. 1. By comparing Row 1 and Row 2, it is clear that our ParCNet V2 surpasses ParCNet V1 by a large margin of 3.9% (73.7% v.s. 69.8%), with only small parameter size growth. We also make ablation studies on each component in ParCNet V2 to show that all of them are critical for improving performance.</p><p>Oversized convolution. Oversized convolution increases the capacity of the model and encodes position information. Without oversized convolution, the model not only loses capacity and position information but also loses the ability to learn long-range dependencies. By comparing Row 2 and Row 3, the accuracy of the model without oversized convolution drops greatly by 1.0% (73.7% v.s. 72.7%) It is similar to the data-driven operation of the squeeze-andexcitation block in ParC V1, while our BGU differs in the following two points. First BGU does not increase parameters. With ? = 2.5, our channel BGU is more lightweight than the original FFN. Second, the two branches in our BGU are more balanced. They share a similar number of parameters and computational costs, unlike the heavy main branch and lightweight channel attention in most methods.</p><p>Uniform local-global convolution block. The uniform local-global convolution block aims to unify the blocks used in different stages. We empirically assigned a linear adjustment of the global ratio over each stage, and this method brings a marginal performance gain of 0.2% (73.7% v.s. 73.5%), as shown in Row 2 and Row 5. This result verifies that vision networks extract more local features in the shallow layers and more global features in the deep layers. The model may get a better boost by further tuning or even learning the local-global ratio. We leave it as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance Comparison with CNNs</head><p>We conduct the image classification task on the ImageNet-1K <ref type="bibr" target="#b7">[8]</ref> dataset, the most widely used benchmark for this task. We train the proposed ParCNet V2 models on the training set of ImageNet-1K, and report top-1 accuracy on the validation set. We adopt random clipping, random horizontal flipping, label-smoothing <ref type="bibr" target="#b31">[32]</ref>, mixup <ref type="bibr" target="#b56">[57]</ref>, cutmix <ref type="bibr" target="#b55">[56]</ref> and random erasing <ref type="bibr" target="#b63">[64]</ref> to augment the training data. In the training process, we train the model for 300 epochs by using AdamW <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref> optimizer with momen-tum=0.9, weight decay =5 ? 10 -2 and batch size =1024. Cosine schedule <ref type="bibr" target="#b27">[28]</ref> and warm-up strategy are employed to adjust the learning rate. The initial learning rate is set to 5 ? 10 -4 . We adopt a variant of LayerScale <ref type="bibr" target="#b46">[47]</ref> in the attention layer which is similar to <ref type="bibr" target="#b13">[14]</ref>. Exponential moving average <ref type="bibr" target="#b33">[34]</ref> is applied to improve the training process.</p><p>The comparison with pure convolution networks on im- ), NAS architecture (ReGNetY-4G <ref type="bibr" target="#b34">[35]</ref>), ConvNeXt <ref type="bibr" target="#b26">[27]</ref>, and MetaFormer architecture (PollFormer-S24 <ref type="bibr" target="#b53">[54]</ref>). Specifically, our model surpasses ParCNetV1-27M <ref type="bibr" target="#b57">[58]</ref>, which indicates that our methods go deeper along the larger convolutions and stronger attention mechanisms. In addition, ParCNetV2-26M performs better than some twice larger models of famous CNNs, such as ResNet101, ResNeXt101-64x4d, and RegNetY-8G. These demonstrate that our ParC-NetV2 design is an effective and promising paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Comparison with CNNs and ViTs</head><p>As shown in Tab. 3, ParCNetV2-26M achieves the best performance among the latest state-of-the-art methods, including CNNs, ViTs, and hybrid networks. Specifically, compared with famous transformers such as Swin-T <ref type="bibr" target="#b25">[26]</ref>, ParCNetV2-26M improves the accuracy by a large margin of 1.6% with comparable parameters and computational costs. This result demonstrates that our pure convolution model utilizes the design concepts from transformers in a more efficient way. Compared with hybrid models, ParCNetV2-26M outperforms the latest EfficientFormer-L3 <ref type="bibr" target="#b24">[25]</ref> and Next-ViT <ref type="bibr" target="#b23">[24]</ref> with much fewer parameters. Combined with the above analysis of pure convolutions in Sec. 4.2, our proposed model has achieved the highest classification accuracy with comparable parameters and computation sizes over various kinds of architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">ParC V2 Performance on Downstream Tasks</head><p>To evaluate the transfer ability of ParC V2, we conduct experiments on the semantic segmentation task with ADE20K <ref type="bibr" target="#b64">[65]</ref> dataset. We train ParCNetV2-26M on the training set and report mIoU on the validation set with both single-scale testing and multi-scale testing. MMSEG <ref type="bibr" target="#b4">[5]</ref> is used as the base framework and UperNet <ref type="bibr" target="#b50">[51]</ref> is used as the segmentation head. All model variants are trained for 160K iterations with a batch size of 16. All models use pre-trained weights from ImageNet1K. Other settings follow <ref type="bibr" target="#b26">[27]</ref>.</p><p>Tab. 4 lists the mIoU, model size and FLOPs for different backbones. Note that the oversized convolution kernel is linearly interpolated into twice as large as the input feature, our model is scale-invariant and thus does not require multi-scale testing during evaluation. This is much more efficient than those requiring multi-scale testing. From the results, it can be seen that ParCNetV2-26M surpasses many transformers with similar computation costs, such as +4.2% mIoU higher than DeiT-S <ref type="bibr" target="#b45">[46]</ref> and +2.1% mIoU higher than Swin-T <ref type="bibr" target="#b25">[26]</ref>. It is also higher than many convolutional networks such as +3.3% mIoU higher than ResNet101 <ref type="bibr" target="#b14">[15]</ref>, +4.0% mIoU higher than ResNeSt101 <ref type="bibr" target="#b58">[59]</ref>, and +1.5% mIoU higher than ConvNeXt <ref type="bibr" target="#b26">[27]</ref>. Specifically, our model is +1.5% mIoU higher than ParCNetV1-27M <ref type="bibr" target="#b57">[58]</ref>, which shows that our model is stronger than the previous models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents ParCNet V2, a pure convolutional neural network with state-of-the-art performance. It extends position-aware circular convolution with oversized convolutions and strengthens attention through bifurcate gate units. Besides, it utilizes a uniform locla-global convolution block to unify the design of early stage and late stage convolution blocks. We conduct extensive experiments on image classification and semantic segmentation to show the effectiveness and superiority of the proposed ParCNet V2 architecture.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) Circular convolution b) Oversized convolution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Comparison between circular convolution and oversized convolution. We only show horizontal convolution for illustration purpose. a) Circular convolution in ParCNet V1 inevitably distorts context information at the boundary of images. b) Oversized convolution resolves the distortion while maintaining the global receptive field over the whole image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. The transitions from the original ParC V1 to ParC V2 block. Compared with ParCNetV1, we first introduce oversized convolutions to further enhance capacity while simplify architecture; then we design bifurcate gate unit to improve efficiency and strengthen attention; finally we propose uniform local-global block and construct the whole network with this uniform block. Different colors indicate different functions, i.e., red, blue and yellow represent spatial interaction, channel mixing and attention mechanism, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of the ParC branch. A ParC branch consists of a depth-wise convolution, a local-oversized convolution with split ratio r, and a point-wise convolution. Fast Fourier Transform (FFT) can accelerate the oversized convolution during inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of the Bifurcate gate unit (BGU). We propose a general BGU which can be easily integrated into various network structures. For Spatial GPU, we insert our ParC branch and a point-wise convolution to extract spatial features. While in Channel BGU, we simply adopt a point-wise convolution to conduct channel mixing.</figDesc><table><row><cell>BGU</cell><cell></cell><cell cols="2">Spatial BGU</cell><cell cols="2">Channel BGU</cell></row><row><cell>Any Branch</cell><cell>PWconv GeLU</cell><cell>ParC-Branch</cell><cell>PWconv GeLU</cell><cell>PWconv</cell><cell>PWconv GeLU</cell></row><row><cell>Gate</cell><cell></cell><cell>Gate</cell><cell></cell><cell>Gate</cell><cell></cell></row><row><cell cols="2">PWconv</cell><cell cols="2">PWconv</cell><cell cols="2">PWconv</cell></row><row><cell>Figure 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of each component on the ImageNet-1K classification task. We use smaller ParCNetV2-10M in ablation and all experiments are trained with 100 epochs for fast evaluation. Top-1 accuracy on the validation set is reported.top-1 accuracy. It demonstrates that long-range dependencies are important to networks.Bifurcate gate units. The bifurcate gate unit is an important mechanism to introduce data-driven operations into ParCNet V2. It increases the non-linearity and enhances the fitting ability. There is a large decline of 0.9% (73.7% v.s. 72.8% without BGU as shown in Row 2 and Row 4.</figDesc><table><row><cell>Row</cell><cell>Backbone</cell><cell>Oversized conv</cell><cell>BGU</cell><cell>Uniform local-global</cell><cell>Param(M)</cell><cell>Top-1(%)</cell></row><row><cell>1</cell><cell>ParCNet V1 (baseline)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>8.6</cell><cell>69.8</cell></row><row><cell>2</cell><cell>ParCNetV2-10M</cell><cell></cell><cell></cell><cell></cell><cell>9.7</cell><cell>73.7</cell></row><row><cell>3</cell><cell>ParCNetV2-10M</cell><cell></cell><cell></cell><cell></cell><cell>9.6</cell><cell>72.7</cell></row><row><cell>4</cell><cell>ParCNetV2-10M</cell><cell></cell><cell></cell><cell></cell><cell>9.7</cell><cell>72.8</cell></row><row><cell>5</cell><cell>ParCNetV2-10M</cell><cell></cell><cell></cell><cell></cell><cell>9.6</cell><cell>73.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with modern convolution networks on image classification. All experiments are trained on ImageNet-1K dataset with 300 epochs (fully trained). Top-1 accuracy on the validation set is reported. ParCNetV1-27M: ParCNetV1 with bigger backbone.</figDesc><table><row><cell>Models</cell><cell cols="3">Param(M) FLOPs(G) Top-1(%)</cell></row><row><cell>ResNet50</cell><cell>22.6</cell><cell>4.1</cell><cell>76.8</cell></row><row><cell>ResNet101</cell><cell>44.6</cell><cell>7.9</cell><cell>80.8</cell></row><row><cell>ResNeXt50-32x4d</cell><cell>25.0</cell><cell>4.2</cell><cell>78.8</cell></row><row><cell>ResNeXt101-64x4d</cell><cell>84</cell><cell>32</cell><cell>80.9</cell></row><row><cell>ReGNetY-4G</cell><cell>21</cell><cell>4.0</cell><cell>81.3</cell></row><row><cell>ReGNetY-8G</cell><cell>44.2</cell><cell>8.0</cell><cell>81.7</cell></row><row><cell>ResNeSt50</cell><cell>27.5</cell><cell>5.4</cell><cell>81.1</cell></row><row><cell>ConvNeXt-T</cell><cell>28.6</cell><cell>4.5</cell><cell>82.1</cell></row><row><cell>PoolFormer-S24</cell><cell>21.4</cell><cell>3.6</cell><cell>80.3</cell></row><row><cell>ParCNetV1-27M</cell><cell>26.6</cell><cell>4.5</cell><cell>82.1</cell></row><row><cell>ParCNetV2-26M</cell><cell>25.6</cell><cell>4.8</cell><cell>82.9</cell></row><row><cell cols="4">age classification is listed in Tab. 2. It is clear that</cell></row><row><cell cols="4">ParCNetV2-26M outperforms other convolutional networks</cell></row><row><cell cols="4">by a large margin, including variants of the famous ResNet</cell></row><row><cell cols="4">(ResNet50 [15], ResNeXt50-32x4d [53], ResNeSt50 [59]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison with state of the art convolution and transformer networks on ImageNet-1K classification dataset. Top-1 accuracy on the validation set is reported. ParCNetV1-27M: ParCNetV1 with bigger backbone.</figDesc><table><row><cell>Frameworks</cell><cell>Models</cell><cell></cell><cell>Date</cell><cell>Param(M)</cell><cell>FLOPs(G)</cell><cell>Top-1(%)</cell></row><row><cell></cell><cell>DeIT-S</cell><cell></cell><cell>ICML 2021</cell><cell>22</cell><cell>4.6</cell><cell>79.9</cell></row><row><cell></cell><cell cols="2">T2T-ViT-14</cell><cell>ICCV 2021</cell><cell>21.5</cell><cell>4.8</cell><cell>81.5</cell></row><row><cell>ViTs</cell><cell cols="2">T2T-ViT-19</cell><cell>ICCV 2021</cell><cell>39</cell><cell>8.5</cell><cell>81.9</cell></row><row><cell></cell><cell>Swin-T</cell><cell></cell><cell>ICCV 2021</cell><cell>28.3</cell><cell>4.5</cell><cell>81.3</cell></row><row><cell></cell><cell cols="2">MViTV2-T</cell><cell>CVPR 2022</cell><cell>24</cell><cell>4.7</cell><cell>82.3</cell></row><row><cell></cell><cell cols="2">ConViT-S</cell><cell>ICML 2021</cell><cell>27</cell><cell>5.4</cell><cell>81.3</cell></row><row><cell></cell><cell cols="2">CoaT-Lite Small</cell><cell>ICCV 2021</cell><cell>20</cell><cell>4.0</cell><cell>81.9</cell></row><row><cell></cell><cell cols="2">LeViT-384</cell><cell>ICCV 2021</cell><cell>39.1</cell><cell>2.3</cell><cell>82.6</cell></row><row><cell></cell><cell cols="2">Twins-SVT-S</cell><cell>NeurIPS 2021</cell><cell>24.0</cell><cell>2.9</cell><cell>81.7</cell></row><row><cell>Hybrid</cell><cell>LIT</cell><cell></cell><cell>AAAI 2022</cell><cell>27</cell><cell>4.1</cell><cell>81.5</cell></row><row><cell></cell><cell cols="2">MobileViTV2</cell><cell>Arxiv 22.06</cell><cell>18.5</cell><cell>7.5</cell><cell>81.2</cell></row><row><cell></cell><cell>CvT-13</cell><cell></cell><cell>ICCV 2021</cell><cell>20.1</cell><cell>4.5</cell><cell>81.6</cell></row><row><cell></cell><cell cols="2">EfficientFormer-L3</cell><cell>NeurIPS 2022</cell><cell>31.3</cell><cell>3.9</cell><cell>82.4</cell></row><row><cell></cell><cell cols="2">Next-ViT-S</cell><cell>Arxiv 22.08</cell><cell>31.7</cell><cell>5.8</cell><cell>82.5</cell></row><row><cell></cell><cell cols="2">ReGNetY-4G</cell><cell>CVPR2020</cell><cell>20.6</cell><cell>4.0</cell><cell>81.3</cell></row><row><cell>CNNs</cell><cell cols="2">ConvNext-T ParCNetV1-27M</cell><cell>CVPR2022 ECCV2022</cell><cell>28.6 26.6</cell><cell>4.5 4.5</cell><cell>82.1 82.1</cell></row><row><cell></cell><cell cols="2">ParCNetV2-26M</cell><cell>-</cell><cell>25.6</cell><cell>4.8</cell><cell>82.9</cell></row><row><cell>backbone</cell><cell>Param(M)</cell><cell>FLOPs(G)</cell><cell>mIoU(%)</cell><cell></cell><cell></cell></row><row><cell>DeiT-S  ?</cell><cell>52</cell><cell>1099</cell><cell>44.0</cell><cell></cell><cell></cell></row><row><cell>Swin  ?</cell><cell>60</cell><cell>945</cell><cell>46.1</cell><cell></cell><cell></cell></row><row><cell>Twins-SVT-S  ?</cell><cell>54</cell><cell>901</cell><cell>47.1</cell><cell></cell><cell></cell></row><row><cell>ResNet101  ?</cell><cell>96</cell><cell>1029</cell><cell>44.9</cell><cell></cell><cell></cell></row><row><cell>ResNeSt101</cell><cell>89</cell><cell>1074</cell><cell>44.2</cell><cell></cell><cell></cell></row><row><cell>ConvNeXt  ?</cell><cell>60</cell><cell>939</cell><cell>46.7</cell><cell></cell><cell></cell></row><row><cell>ParCNetV1-27M</cell><cell>56</cell><cell>936</cell><cell>46.7</cell><cell></cell><cell></cell></row><row><cell>ParCNetV2-26M</cell><cell>55</cell><cell>946</cell><cell>48.2</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparisons on ADE20K. All models use UperNet as a basic framework and are trained with 160K iterations. ? indicates multi-scale testing and the others are evaluated with single-scale testing. ? indicates additional deconvolution layers are used to produce hierarchical feature maps. FLOPs are measured with the input size of (2048, 512).</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{TVM}: An automated {Endto-End} optimizing compiler for deep learning</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eddie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Ceze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="578" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mobileformer: Bridging mobilenet and transformer</title>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5270" to="5279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mmsegmentation, an open source semantic segmentation toolbox</title>
		<author>
			<orgName type="collaboration">MMSegmentation Contributors</orgName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3965" to="3977" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11963" to="11975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">You only look at one sequence: Rethinking transformer in vision through object detection</title>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bencheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="26183" to="26197" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12259" to="12269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cmt: Convolutional neural networks meet vision transformers</title>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="12175" to="12185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Ze</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.09741</idno>
		<title level="m">Visual attention network</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On translation invariance in cnns: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName><forename type="first">Osman</forename><surname>Semih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayhan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14274" to="14285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mmft-bert: Multimodal fusion transformer with bert encodings for visual question answering</title>
		<author>
			<persName><forename type="first">Aisha</forename><surname>Urooj Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.14095</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Nextvit: Next generation vision transformer for efficient deployment in realistic industrial scenarios</title>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huixia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.05501</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Efficientformer: Vision transformers at mobilenet speed</title>
		<author>
			<persName><forename type="first">Yanyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.01191</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A convnet for the 2020s</title>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="11976" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02178</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">When does label smoothing help? Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatoli</forename><forename type="middle">B</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName><surname>Juditsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10428" to="10436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06125</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Relay: A high-level compiler for deep learning</title>
		<author>
			<persName><forename type="first">Jared</forename><surname>Roesch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lyubomirsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marisa</forename><surname>Kirisame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Pollock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Vega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziheng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Tatlock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08368</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unet: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Going deeper with image transformers</title>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="32" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="30392" to="30400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10819" to="10829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi-Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngjoon</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Parc-net: Position aware circular convolution with merits from convnets and transformer. networks (ConvNets)</title>
		<author>
			<persName><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenze</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Resnest: Split-attention networks</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2736" to="2746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2998" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ansor: Generating {High-Performance} tensor programs for deep learning</title>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengfan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Hao Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameer</forename><surname>Haj-Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koushik</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th USENIX symposium on operating systems design and implementation (OSDI 20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><surname>Philip Hs Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
