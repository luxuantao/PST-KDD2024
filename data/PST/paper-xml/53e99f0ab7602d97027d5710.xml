<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DAE81EDC6165C88D253FCBDE5EC04FF6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Absfract-We propose, implement, and evaluate a class of nonstationary-state hidden Markov models (HMM's) having each state associated with a distinct polynomial regression function of time plus white Gaussian noise. The model represents the transitional acoustic trajectories of speech in a parametric manner, and includes the standard stationary-state HMM as a special, degenerated case. We develop an efficient dynamic programming technique which includes the state sojourn time as an optimization variable, in conjunction with a state-dependent orthogonal polynomial regression method, for estimating the model parameters. Experiments on fitting models to speech data and on limited-vocabulary speech recognition demonstrate consistent superiority of these nonstationary-state HMM's over the traditional stationary-state HMM's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>N the traditional formulation of the hidden Markov model I ( H M M ) , individual states are assumed to be stationary stochastic sequences. Successive observation sequences produced from these state-dependent random processes are either independent and identically distributed (IID) <ref type="bibr">[l]</ref>, 171, or can be allowed to embed temporal correlation [2], <ref type="bibr">[9]</ref>. In either case, the parameters (e.g., means, covariances, and autoregression matrices) that characterize the state-dependent random sequences are assumed to be independent of time, hence stationary states. This stationary-state assumption appears to be reasonable when a state is intended to represent a short segment of sonorant or fricative speech sounds. However, for longer segments of these sounds and for all types of plosive sounds, such an assumption is inadequate and it is desirable to make the HMM states nonstationary so as to more accurately represent these sound pattems. Glides, liquids, diphthongs, and transition regions between phones reveal the most notable nonstationary nature in speech. In continuously spoken sentences, even vowels contain virtually no stationary portions [ 111.</p><p>In a previous work, we proposed a mathematical framework for a nonstationary-state HMM, or the trended HMM, where Manuscript received <ref type="bibr">June 24, 1992;</ref><ref type="bibr">revised April 3, 1994</ref> </p><p>m=O where the first term is the state-dependent polynomial regression function of order M, the second term is the residual noise assumed to be the output of an IID, zero-mean Gaussian source with state-dependent covariance matrix Ci, and state i at a given time t is determined by evolution of the underlying Markov chain in the HMM.</p><p>In the above model formulation, the time origins of the regression functions for all the states are fixed at the origin of the utterance: t = 0. This is appropriate only for HMM representation of entire words uttered with a relatively constant speaking rate. For HMM representation of general speech units such as subword units and for continuous speech recognition, many states in the HMM representing an utterance have to be tied (i.e., taking the same parameter values across the states). In particular, the tying includes the parameters in the regression functions. For such trying to be sensible, the time origin of the regression function in each state in the HMM should start from the time when the state is first entered rather than from the origin of the utterance. Further, for speech utterances having a wide range of speaking rates, use of the state-transition-dependent time origins for regression functions (instead of using a fixed time origin for all states) would significantly reduce error accumulation due to speaking rate variation from one speech token to another.</p><p>In Section I1 of this paper, we formulate this nonstationary-state hidden Markov model whose states are defined by polynomial regression functions plus noise where each state-dependent regression function starts with t = 0 when the state transition into the current state occurs. Section 111 provides a solution to the parameter estimation problem for this new HMM via a modified Viterbi algorithm. It also provides a scoring algorithm for the decoding stage in speech recognition. In particular, we describe some heuristic methods we have developed for approximation of the solutions, which allow significant reduction of the computation cost but only minimally effect speech recognition performance. We present 1063-6676,94$04.00 0 1994 IEEE results, in Section IV, on fitting raw speech data using the nonstationary-state (trended) HMM, in comparison with the less accurate fitting using the standard stationary-state HMM of <ref type="bibr">[7]</ref>. In presenting these results, we try to illustrate interutterance variation of speech tokens and its effect on the model fitting. Speech recognition experiments are reported in Section V, demonstrating superiority of the new model under several limiting conditions. Finally, we draw conclusions from this study and point to future directions in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE MODEL</head><p>The nonstationary-state or trended HMM investigated in this paper is of a data-generative type. The model generates the (vector-valued) observations data sequences of length T , Ot , t = 1,2, . . . , T, from the following polynomial regression function of time plus additive zero-mean IID Gaussian noise relation</p><formula xml:id="formula_1">M Ot = Bi(m)fm(t -~i ) + &amp; ( x i ) (2)</formula><formula xml:id="formula_2">m=O</formula><p>where i is the label of a state in the HMM, and f m ( . ) is an m-order polynomial. We use orthogonal polynomials for their better stability properties in estimating the polynomial coefficients &amp;(m) (see Section IILB for detail). In this study, we choose to use the Legendre polynomials. Note that the polynomial for each state depends not only on the coefficients Bi(m), but also on the time-shift parameter ~i . 7; registers the time when state i in the HMM is just entered before regression on time takes place; i.e., (t -~i )</p><p>represents the sojoum time in state i. However, only the polynomial coefficients Bi(m) (for state i ) are considered as true model parameters, and ~i is used merely as the auxiliary parameter so as to obtain maximal accuracy in estimating Bi(m) (over all possible q values). In the speech recognition step, 7" is again estimated as the auxiliary parameter so as to achieve a maximal score in matching the model to the unknown utterance over all possible T; values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">ESTMATION OF MODEL PARAMETERS</head><p>An effective and efficient algorithm is developed in this study for automatic training of the parameters, notably the state-dependent polynomial coefficients of the regression functions, in the trended HMM's.' The algorithm is motivated by and is extension of the segmental K-means algorithm developed in the past for training standard HMM's [6]. Like the segmental K-means algorithm, the algorithm developed here also involves two iterative steps-the segmentation step and the optimization step-which are both described in detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Segmentation Step</head><p>The objective of the segmentation step is to find a state sequence which maximizes the joint likelihood of observation ' Estimation of the transition probabilities and for the residual covariance matrices is very similar to that for the standard HMM and is omitted here.</p><p>sequence and state sequence. For the standard stationarystate HMM's, such as Baum's model [I] and the hidden filter model [9], the likelihood of each observation given a state does not depent on the sojourn time in the state. Therefore, the standard Viterbi algorithm can be used for the segmentation purpose <ref type="bibr">[IO]</ref>. In contrast, for the nonstationarystate or trended HMM studied in this paper, the mean in the state-dependent Gaussian random process is a function of the state sojoum time (i.e., polynomial trend function), and hence so is the likelihood for an observation in that state. This requires extension of the standard Viterbi algorithm over a new maximization dimension-that of state sojoum time-in order to achieve the optimum in the segmentation step.</p><p>We now formally describe this modified Viterbi algorithm.</p><p>Let Q = { q l , qz, . . . , QT} be the state sequence and 0 = { 0 1 , 0 2 , . . . , OT} be the given observation sequence of length </p><formula xml:id="formula_3">{Q : dt = s&gt; e IJ N {Q : ~t -8 -1 # 2, qt-s = qt-s+l = ' . ' = qt = i}, 1=1 0 5 s &lt; t.</formula><p>Then the largest probability along a single state-sequence path up to time t, with duration time d at state i can be expressed as where 0 is the parameter vector of the HMM.</p><p>The essence of the modified Viterbi algorithm is to efficiently compute &amp; ( j , d ) in an iterative way. To keep track of the optimal state sequence, we use $ t ( j , d ) to trace the most likely state information (state identity and state sojoum time) at time t -1 given that qt = i and dt = d in the following procedural description of the modijied Viterbi algorithm: 1) Initialization:</p><formula xml:id="formula_4">6 i ( i , d) = I[d=o] . ? ~t . bi(O1, d 1 0), 1 5 i 5 N (3) $ l ( i , d ) = (0,O)<label>(4)</label></formula><p>where { T I , . . . , T N } is the initial probability of Markov states. 2) Recursion: 4) Backtracking:</p><formula xml:id="formula_5">Q : = $t+l ( d + l ) , t = T -1, T -2, . . . I 1. (9)</formula><p>Note that in the above termination stage, the maximum joint likelihood of observation and state sequence P* (obtained from ( <ref type="formula">7</ref>)) can be used to score any input speech token 0 = {Ol,O2, . . . , OT}. This likelihood is thus also called the Viterbi score which, as a by-product in the model training stage, finds its important uses in the decoding stage of speech recognition. Also, note that the computational complexity of the above algorithm is quadratically related to the observation length (T'), which is significantly greater than the complexity of the standard Viterbi algorithm (only linearly related to T).</p><p>To alleviate this difficulty, we have devised a method which utilizes state duration constraints to reduce the computation with only minimal effects on state segmentation accuracy. We will give the method in detail in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Maximization Step</head><p>Once all the state boundaries are determined via the above segmentation step, estimation of the parameters in the statedependent nonstationary Gaussian processes is essentially the problem of polynomial regression. Unlike the Baum-Welch algorithm, this maximization step in the segmental k-means algorithm can be done for each state independently. In the following description of the polynomial regression, we thus drop the state index.</p><p>For estimating the regression coefficients for each state, we consider the standard regression equation where the unknown to be solved, </p><formula xml:id="formula_6">fo(t) = 1 fl(t) = 4 2 2 -1) f3(t) = J?(2oZ3 -30x2 + 122 -1) fz(t) = 6 ( 6 z 2 -62 + 1) f4(t) = 3(70z4 -1 4 0 ~~ + 90%' -2 0 ~ + 1).</formula><p>If the standard nonorthogonal polynomials: f o ( t ) = 1; fl(t) = t; fZ(t) = t'; . . . were used, then the regression matrix would become highly ill-conditioned for moderate orders of polynomials, and hence the parameter estimation based on (XTTX)-' would be very unstable. Use of the orthogonal polynomials described above has substantially alleviated this ill-conditioning problem. In order to completely eliminate the ill-conditioning problem, we have further adopted the following SWEEP algorithm for the solution of the polynomial regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . The SWEEP Algorithm</head><p>the HMM)</p><p>To solve the standard regression equation (for each state in</p><formula xml:id="formula_7">XT'X[B(1) I B(2) I . . . I B ( D ) ] = XT'[0(l) I I . . . I O(D)] (13)</formula><p>the Gauss-Jordan elimination method could be applied as was done usually. However, Gauss-Jordan elimination fails when the matrix XTX is not of full rank, for example, in the case that collinearity exists among the X variables.</p><p>The SWEEP algorithm [5] has the advantage of dealing with this problem automatically. It performs in such a way that each operation "sweeps" out one X variable and obtains simultaneously the corresponding regression coefficient. When the algorithm encounters a variable that is highly correlated with the previously swept variables (up to a threshold value), it automatically ignores that variable by setting the corresponding regression coefficient zero and continues to proceed. As a result, the ill-conditioning problem can never occur, even when the number of parameters are greater than the number of observations. For example, in the case of fitting a polynomial of order two based on only two data points, the SWEEP algorithm will set the coefficient of the second-order polynomial to zero automatically after fitting the first-order line. This is particularly useful for our current problem of estimating regression parameters in the HMM when a state has a short segmentation (i.e., dimensionality of 0 and X in (13) is small).</p><p>The SWEEP algorithm, which has been implemented for estimating regression coefficients in the trended HMM in this study, is formally described below. Let</p><formula xml:id="formula_8">C = ( c ~, ~) ( M + I ) ~( M + ~+ D ) = [XTPX I XnO(') I Xn0(') I . . . I X n d D ) ] (14)</formula><p>where M is the order of the polynomial regression functions, and D is the dimension of observation vector. Then the SWEEP algorithm can be described as the following iterative steps:</p><p>1) Initialization: Set k = 1 and set a threshold value TOL (e.g., 1.h-20).</p><p>2) Set D = Ckk, if D &lt; TOL, keep record of the index k and go to (5).</p><p>3) Divide row k by D. 4) Subtract cik times row k from each row i # k (similar to the Gauss-Jordan elimination method).</p><formula xml:id="formula_9">5 ) k + k + 1; if k 5 p go to (3).</formula><p>Iv. ANALYSIS OF THE MODEL: FITTING SPEECH DATA In the above sections we have shown that theoretically the trended HMM includes the standard HMM as a special case where only zeroth-order polynomials (i.e., constants) are used as the trend functions. In this section, we provide experimental evidence to show that the trended HMM in practice is able to fit actual speech data, both for the training data (those used to estimate model parameters) and for the test data (those not used to estimate model parameters), more closely than the standard HMM.</p><p>The speech data was taken from several tokens of word beet /bi:t/, spoken by a native English male speaker. The raw speech data was in the form of digitally sampled signal at 16 kHz. A Hamming window of duration 25.6 msec was applied every IO msec (the frame length). Within each window, melfrequency cepstral coefficients were computed. For the sake of space saving, we show here the data fitting results only for the first and second-order cepstral coefficients (C1 and C2). C1 contains information about the difference of the log channel energies between low-frequency and high-frequency channels: C2 contains information about summation of log channel energies of low and high-frequency channels subtracting those of mid-frequency channels?</p><p>The parameters of the trended HMM's, varying in the order of the polynomial regression functions from zero (standard HMM), one (linearly trended HMM), two (quadratically trended HMM), to three (cubic trended HMM), were trained using the segmental K-means algorithm described in Section III. Two tokens of word beef were used for the training. As an illustration of the data fitting results, we select the example of a three-state left-to-right model (N = 3) and show the results for the training data first: The dotted lines in all four subgraphs of Fig. <ref type="figure" target="#fig_5">1</ref> are the same speech data C1 sequence from one training token to be fitted, where the vertical axis represents the magnitude of C 1 and the horizontal axis is the frame number. Superimposed on Fig. <ref type="figure">l(a)-(d</ref>) as solid lines are the polynomial regression functions from the trended HMM's with the polynomial orders 0, 1, 2, and 3, respectively. Given the model parameters, the process of fitting the models to the data proceeded by first finding the optimal segmentation of the data into the HMM states (via use of the modified Viterbi algorithm described in Section IILA) and then fitting the segmented data using the polynomial fitting functions associated with the corresponding states. The two breakpoints in each graph correspond to the frames where the "optimal" state transitions, from state 1 to state 2 and from state 2 to state 3, occur. Comparison among the four graphs in Fig. <ref type="figure" target="#fig_5">1</ref> demonstrates that as the polynomial order increases, the degree to which the trended HMM is able to accurately fit the data improves accordingly in a highly significant way. A quantitative measure for the accuracy of the data fitting can be obtained by summing state-dependent frame residual errors over frames One might argue that the superior data fitting performance of the trended HMM over the standard HMM (degenerated trended HMM) could be due just to its higher number of model parameters or its higher degree of freedom in data fitting. To be sure that this is not the case, we conducted two sets of 3Similar results have heen obtained for higher order cepstral coefficients, 4Similar results have been obtained for other numbers of states. which will not be shown in this paper due to space limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"beet" (train token, order=O)</head><p>-.. HMM states in three-state trended HMM's. The two breakpints in each graph correspond to the time when the "optimal" state transitions, from state 1 to state 2 and from state 2 to state 3, occur according to the modified vlterbi algorithm. RSS is a measure of the accuracy of the data fitting, defined as sum of statedependent frame residual errors over all frames.</p><p>data fitting experiments. First, we canied out the fitting on test tokens (i.e., word tokens of beer not used in training the models). Figs. <ref type="figure" target="#fig_7">3</ref> and<ref type="figure" target="#fig_9">4</ref> show the fitting results for the C1 and C2 speech data, respectively. The same kind of the superiority of the trended HMM over the standard HMM as that shown in Figs. 1 and 2 is demonstrated here. In particular, the poor data fitting performance of the standard HMM is clearly revealed as the constant mean associated with the second state (both in Figs. <ref type="figure" target="#fig_7">3(a</ref>) and 4(a)) is uniformly greater than the corresponding data over the entire state sojourn interval.</p><p>In the second set of fitting experiments, we varied the number of states in the trended HMM's according to their polynomial orders. This was done so as to make all the HMM's differing in their polynomial order nevertheless have approximately the same total number of model parameters. Such a criterion produced the zero-order trended HMM with 12 states (Figs. 5-7(a)), the first-order trended HMM with six states (Figs. 5-7(b)), the second-order trended HMM with four state (Figs. 5-7(c)), and the third-order trended HMM with three states ). For the model fitting to a training token (Fig. <ref type="figure">5</ref> for C1 and Fig. <ref type="figure" target="#fig_13">6</ref> for C2), the zeroorder trended HMM (standard HMM) tended to fit the token most closely (but nearly the same as the third-order trended Hh4M when comparing Fig. <ref type="figure">5</ref>(a) and 5(d)). However, for the fitting to test tokens, the standard HMM often provided the worst fitting, with one typical example shown in Fig. <ref type="figure" target="#fig_14">7</ref> (for C1 data). On the other hand, we also observed cases where for the same number of model parameters the standard HMM gave better data fitting to test tokens than the higher-order trended HMM's. One example for such comparative fitting is shown in Fig. <ref type="figure" target="#fig_15">8</ref> for C2 data. It appears that for low-order cepstral speech data which are temporally "smooth," use of high-order trended HMM's with a small number of states provides closer data fitting than the low-order trended HMM's having more states; while temporally "rough" data (high-order cepstral coefficients) would be better fitted by the standard HMM having a large number of states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v . SPEECH RECOGNITION EXPERIMENTS</head><p>The vocabulary of the isolated-word, speaker-dependent recognizer used for evaluation of the nonstationary-state trended HMM's consists of highly confusible 36 CVC words, where C encompasses six stop consonants /p/, /t/, /k/, /b/, Id, /g/ and V is the vowel /i:/. All speech materials were uttered with a short pause in between by three native English speakers in a normal office environment. Training data consists of eight tokens of each of the 36 vocabulary words; test data consists of 14 disjoint examples of the 36 words, resulting in 504 test tokens for each speaker.</p><p>Training and test speech data were obtained using a DSP Sona-Graph workstation. Fifteen-dimensional vectors compris-ing mel-frequency cepstral coefficients and their differences over time were computed as the output of the speech preprocessor.</p><p>We chose to evaluate the nonstationary-state trended HMM's, with the benchmark of the standard stationarystate HMM's, using two different speech units for the HMM representation: whole word unit and context-dependent allophonic unit.</p><p>With use of whole-word units, we created a total of 36 trended HMM's, one for each CVC word. The polynomial order of each model varied from zero (benchmark model) to three, and the number of states in each model varied from one to 20. (The state number was run high enough to ensure saturation of the performance.) Table <ref type="table" target="#tab_2">I lists</ref>   eight tokens (right) of each word were used for training each trended HMM. For both cases, the highest recognition accuracy, 77% and 83%, respectively, was obtained with use of the nonstationary-state trended HMM (P = 1 and P = 3).</p><p>For each fixed number of states N, the superior performance of the trended HMM over the standard stationary-state HMM was particularly clear when a small number of HMM states were used. Tables <ref type="table" target="#tab_2">I1</ref> and<ref type="table">111</ref> show the same type of comparative speech recognition accuracy for two other speakers' speech "beet" (test token, order=3)</p><p>data. These results, again, demonstrate limited recognition performance achieved by the standard stationary-state HMM.</p><p>Note that as the number of states increases, the recognition accuracy achievable by most of the trended HMM's, including the degenerated ones (P = O), increases to a plateau first, and then declines. Yet, above all, the best recognition rate is, again, achieved by the nonstationary-state trended HMM having a relatively few states (e.g. 96% for N = 10 and P = 1 in Table <ref type="table">11</ref>). These findings indicate that although use of many states in the stationary-state HMM can in principle approximate continuously varying speech data in a piece-wise constant fashion, it is not adequate for high-accuracy speech recognition. Better performance is achievable with use of the nonstationary-state trended HMM, which is more structured and more economical in the use of model parameters. These performance results are consistent with the results of fitting models to speech data described in Section IV.</p><p>In addition to the above experiments where whole-word HMM's were used, we conducted a parallel set of experiments using HMM representation of allophones. Two allophones "beet" (test token, order=O)  were chosen for each of six stop consonants, one for the pre-vocalic stop and the other for the post-vocalic stop. This created a total of 13 models for representing all 36 words in the vocabulary: 12 stop allophone models plus one vowel model. Like the whole-word HMM's, the polynomial order of each allophone model varied from zero (benchmark model) to three. The number of states for all allophone models were made the same, varying from one to 9; that is, the total number of states in the concatenated word HMM's varied from 3 to 27. Tables IV-VI, for the three speakers, respectively, contain the percent recognition accuracy results obtained via use of the trended HMM representation for the allophones. The comparative performance between use of nonstationary-state HMM's and of stationary-state HMM's follows a similar pattem to that shown in Tables <ref type="table">1-111</ref>. The absolute performance, given a fixed polynomial order and the number of states, is higher with these allophone models than with the previous wholeword models. This is probably due to a better acoustic data sharing mechanism associated with the allophone models. Fitting of models to a C1 cepstral coefficient training data sequence using a varying number of states for models with different polynomial  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE I1 SPEECH RECOGNITION ACCURACY (PERCENTAGE CORRECT) AS POLYXOMLU ORDER (P), AND OF N u " OF STATES ( N ) IN THE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>"beet" (train token)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND DISCUSSION</head><p>In this study, we proposed, implemented, and evaluated a type. of nonstationary-state trended HMM's where each state is associated with a distinct polynomial regression function of time plus Gaussian noise. The principal motivation of this new type of HMM's is to parametrically describe "beet" (train token) "beet" (train token) -t -....... f . m .. continuously-varying transitional acoustic pattems of speech in a more natural and a more structural manner than the standard stationary-state HMM's developed and widely used in the past. One desirable attribute of this new model is that when a relatively steady-state speech segment, such as some mid-portion of a vowel is encountered, then the higher-order polynomial coefficients can be automatically set to zero and the model is reduced essentially to the standard HMM.</p><p>There are two ways of formulating the nonstationary-state HMM's depending on choice of the time origin for the statedependent polynomial regression function on time: 1) the time origin for the regression functions of all states is set identically at the start of each word utterance; and 2) the time origin is reset once a state transition in the HMM occurs. For the first formulation, the efficient Baum-Welch algorithm is directly applicable for estimating polynomial coefficients. However, this formulation of the model can be applied only to whole-word HMM's for isolated word recognition and requires that the speaking rate variation from one speech token to another be relatively minor. For use of HMM representation for sub-word units, the above second formulation is necessary. Unfortunately, this formulation does not render direct use of the efficient Baum-Welch algorithm for model parameter estimation possible. One major contribution "beet" (test token) of this study is the development of the modified Viterbi algorithm, in conjunction with the state-dependent orthogonal polynomial regression technique, for accurately and robustly estimating polynomial coefficients in the model and for scoring utterances. An additional contribution is the development of a heuristic method which utilizes duration constraints to reduce otherwise very high computation cost associated with the modified Viterbi algorithm. Effectiveness of the parameter estimation algorithm and of use of the duration constraints is experimentally verified in this study.</p><p>To help understand the properties of the nonstationary-state HMM's, we conducted experiments on fitting models to speech data. With use of residual square sum as a quantitative measure for goodness of fit, the experimental results demonstrated superiority of the nonstationary-state HMM's over the standard stationary-state HMM's.</p><p>Isolated-word speakerdependent 36-CVC-word speech recognition experiments were designed to systematically evaluate the performance of the newly developed models as a function of a range of model parameters and experimental factors: 1) order in the pofynomial regression functions; 2) number of states in the HMM's; 3) type of speech units for HMM representation; 4) speaker identity; 5 ) size of the training data; and 6) strength of the duration constraint. We reached the following conclusions from detailed examinations of the recognition results:</p><p>1) For any given order in the trended HMM, as the number of states increases the recognition accuracy tends to increase to a plateau and then declines. 2) Over a wide range of the number of HMM states and for both the allophone and whole-word speech units, the best recognition rate is mostly achieved by the nonstationarystate trended HMM (polynomial order not equal to zero), rather than by the stationary-state HMM (polynomial order equal to zero). 3) With the number of HMM states being one or two for a word, the recognition rate tends to increase monotonically with the polynomial order. significantly increases the recognition rate for nearly all polynomial orders and all numbers of the HMM states. 5) Imposing stronger duration constraints only affects the recognition rate minimally while allowing significant reduction in computation costs associated with both training and decoding. Despite the high degree of generality of the nonstationarystate trended HMM developed here, three further improvements are possible for making the model more suitable for speech recognition. First, speaking rate variation from one speech token to another, given the same underlying phonetic representation of the HMM states, should be normalized. Because, unlike the stationary-state HMM, the polynomial regression model of each state is in general a function of time, significant variability is necessarily introduced when using the same, single regression model to describe speech data from muItiple tokens with varying state durations. Absence of temporal normalization as in the present model is one major weakness in the current model formulation. To overcome this difficulty, we propose to use auxiliary parameters to implement state-dependent time warping in the polynomial regression functions. Second, once the state durations are normalized to a fixed length, the restriction of the residual signal &amp;(Xi) in (2) being an IID sequence can be easily removed. Then a full covariance matrix having its dimensionality as large as the product of the state length by the observation vector's dimension can be constructed to completely account for statistical dependence of all observations within a state.5 This way of characterizing long-term statistical dependence of observations cannot be implemented in the standard HMM having a large number of states. Third, for the future speakerindependent speech recognition, the current trended HMM can be generalized to the HMM with state-dependent mixtures of trended functions. Using the method for the model construction    <ref type="formula">16</ref>) formula (5), it is clear that the maximization was taken over the duration value up to the current time point (as well as where I &lt; T , I I and I I U,.</p><p>over the HMM states). This extra step of maximization over computation is only of order O ( T ) , rather than of order O ( T 2 ) as in the recursive formula (5) where no state duration the standard Viterbi algorithm increases the computation up to constraint was used. O ( T 2 ) (T is the total number of frames in the data).</p><p>To reduce the computation, we note that the maximization of the duration constraint on speech reCOpifor the duration from 0 to t-1 for all the states is not necessary. tion accuracy is demonstrated in Tables <ref type="table" target="#tab_5">vII</ref> and<ref type="table" target="#tab_6">vIII</ref>. ne This is because the duration of each state has to fall within experimental conditions were identical to those under which a range shorter than such a full duration. Use of this state the results of Table <ref type="table" target="#tab_2">I</ref> were obtained except here duration duration constraint can significantly reduce the computation constraints were imposed. The upper and lower time limits, ( L j , U;) in ( <ref type="formula">15</ref>) and ( <ref type="formula">16</ref>), were set at f 3 (Table <ref type="table" target="#tab_5">VII</ref>) and but without effecting the segmentation result.</p><p>Let <ref type="bibr">( L , Vi)</ref>   Comparing the results in Tables <ref type="table" target="#tab_4">VI1</ref> Order polynomial regession functions to the mM computation saving had been achieved, ( n e degradation is than a weaker constraint (Table <ref type="table" target="#tab_6">VIII</ref>).)</p><p>With (Li, Ui) being determined, the recursive formula (5) regression parameters for each state by concatenating all the observation sequences as follows. Suppose we have the following K tokens for a particular state each with length T T , r = l;..,K</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I1 ESTIMATION OF MODEL PARAMETERS FOR THE CASE OF MULTIPLE</head><p>where each Oz,J,a = l ; . . , K ; j = ~; . . , T K is a Ddimensional observation vector. We modify the regression matrix to the equation at the bottom of the preceding page and at the same time concatenate the K observation sequences into a single sequence Then the remaining estimation procedure becomes identical to the one described in the main text for the single training token case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. Support for this work was provided by the Natural Sciences and Engineering Research Council of Canada and by the University of Waterloo Interdisciplinary Grants Program. ' &amp;e associate editor coordinating the review of this paper and approving it for publication was Dr. Xuedong Huang. L. Deng and M. Aksmannvic are with the Department of Electrical and Computer Engineering. University of Waterloo, Waterloo, Ontario, Canada N2L 3G1. X. Sun and C. F. J. Wu are with the Department of Statistics and Acturial Science, University of Waterloo, Waterloo, Ontario, Canada N2L 3G1, lEEE Log Number 9403977. polynomial trend functions (or regression functions of time) are used as time-varying means in the output Gaussian distributions in the HMM states [3]. In that model, the observation vector sequences, Ot , t = 1,2, . . . , T are generated from the model according to M Ot = Bi(m)tm +&amp;(E,)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>zWe will treat the case with multiple training tokens in Appendix n. observation sequence; X is the TO x (M+ 1) regression matrix of the form is M where fm (x) is the Legendre orthogonal polynomial of order m defined on [0, TO]. (Note the time origin for each polynomial is reset to zero for every new Markov state entered). These polynomials satisfy the orthogonality relationship 3) Termination: f o r t = T -1 , . . . , 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>d</head><label></label><figDesc>= 1 , 2 , . . . , D, is the (M + 1) x 1 vector consisting of up to Mth-order polynomial coefficients for only the dth components in the multivariate polynomial coefficients; O ( d ) , d = 1 , 2 , . . . , D, is the vector of length TO (state duration determined by the modified Viterbi algorithm) comprising only the dth components in the The polynomials up to order four used in this study, with x = t/To, are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>6) Termination: The columns from M + 2 to M + 1 + D are the estimated regression coefficients [B(') I B(*) I . . . I B(D)], where the rows with indices recorded at step (b) are Set to zero. The method for estimating the parameters of the model from training data has been described above in this section for the case of single training token. The case for multiple training tokens is treated similarly with details described in Appendix 11.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>. . . , N are the Viterbi segmentation boundaries. The smaller the RSS is, the better the data fitting would be. (Zero RSS indicates perfect fitting.) As the polynomial order increases from zero (Fig. l(a)), one (b), two (c), to three (d), the RSS value decreases substantially from 558, 214, 161, to 42, respectively. Fig. 2(a)-(d) show the same type of data fitting as Fig. 1 for the C2 cepstral coefficients. The same results are obtained: as the polynomial order increases, the fitting error RSS reduces quickly from 260, 214, 136, to 87.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Fitting of a standard stationary-state HMM with polynomial order zero (a) and of nonstationary-state HMM's with polynomial order one (b), two (c). and three (d) to speech data consisting of mel-frequency cepstral coefficient sequence C1 from a n utterance of word beet. This data sequence was used to training all the four models. Dotted lines are the CI data sequence. Solid lines are the polynomial regression functions of time for corresponding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Fitting of models to a C2 cepstral coefficient training data sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Fitting of models to a C1 cepsual coefficient test data sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Fitting of models to a C2 cepstral coefficient test data sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>A</head><label></label><figDesc>FUNCTION OF TRAINING DATA SaE (4 OR 8 TRAINING TOKENS), TRENDED HMM'S (SPeAKER 2; NO DURATION CONSTRAINT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>SPEECH</head><label></label><figDesc>RECOGNITION ACCURACY (PERCENTAGE CORRECT) AS A FUNCTION OF TRAINING DATA SEE (4 OR 8 TRAINING TOKENS), POLYNOMIAL ORDER (P), AND OF NUMBER OF STAW ( N ) IN THE TRENDED W ' s ( S m m 3; No DURATION CONSTRAINT)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 5. orders (so as to keep the total number of model parameters constant).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Fitting of models to a C2 training data sequence using a varying number of states for models with different polynomial orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 7 .</head><label>7</label><figDesc>Fig.7. of models to a C1 cepstral coefficient test data sequence using a varying number of states for models with different polynomial orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Fitting models to a C2 cepstral coefficient test data sequence using a varying number of states for models with different polynomial orders</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>5When the number of states in the HMM is reduced to one, this improved model would behave similarly to the stochastic segment model [8].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>of the stationary-state HMM and the related i, i = 1,. N . In this study, (Li, Ui) are determined by obtained via use of the standard Viterbi algorithm (which is and vIII and those in TableI, we observe very slight degradation of speech recognition accuracy resulting from very fast) based on the zeroth-order polynomial regression functions, by a fixed, small length. Note that use of the zerothuse of duration while using a stronger constraint (TableVII) as it is just the standard Viterbi algorithm[lo].j, d ) = I [ ~= ~~~ . arg max max &amp; ( i , T ) . aij + 4d,O] ' ( j , d -1)The incrementing and decrementing the HMM state boundaries, (fast) Viterbi</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>When multiple tokens are used for training the state parameters in the nonstationary-state H M M , we estimate the (15) 1 1 1 ... 1 ... Ti 1 2 ... Tz ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I SPEECH RECOGNITION ACCURACY (PERCENTAGE CO") AS A FUNCTION OF TRAINING DATA S m (4 OR 8 TRAINING TOKENS), POLYNOMIAL ORDER (P), AND OP N m m OF STAW ( N ) IN THE TRENDED HMM'S (SPEAKER 1; No DURATION CONSTRAINT)</head><label>I</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV SPEECH RECOGNITION ACCURACY (PERCENTAGE C o w ) USING ALLOPHONIC TRENDED HMM's (SPEAKER 3; DURATION CONSTRAINT f3)</head><label>IV</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V SPEECH RECOGN~ION ACC~RACY (-CENTAGE C o m a ) USING ALLOPHONIC TRENDED HMM's (SPEAKER 1; DURATION CONSTRAINT f3)</head><label>V</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII EFFFCT OF THE DURATION CONSTRAINT ON SPEECH RECOGNITION ACCURACY (SMR 1). U m AND LOWER TIME LIMITS ( L , U , ) IN (15) AND (16) WERE SET To BE f3 FROM THE BOUNDARIES OF STATE i DETERMINED BY THE STANDARD HMM AND THE RELATED V ~B I ALGORITHM. THE RFSULTS SHOULD BE COWARED WITH THOSE M TABLE 1 WHERE No DURATION CONSTRAINTS WERE IMWSED</head><label>VII</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII SAME AS TABLE vn EXCEPT ( L ~, U , ) WLRE SET TO BE</head><label>VIII</label><figDesc>f l FROM THE STATE BOWnADmr . I -. . . - ~~ duced in the maximization process compared to +e standard Viterbi algorithm [lo]. (The modified Viterbi algorithm was used both for training and for recognition.) From the recursive (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>be the lower and upper time limits for the state (TablevIII) from the State i 9 s boundaries which were</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>similar to the one proposed and implemented in [4], we will achieve this generalization in a straightforward manner. Once the model for state-dependent mixtures of trends is implemented from our future work, speaker-independent data corpus can be used to further evaluate the model. We are currently investigating all these three ways of extension and improvement of the model described in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank Dr. Frank Soong for valuable discussions on the model presented in this paper and to thank two anonymous reviewers who provided useful comments, which improved the quality of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I STATE SEGMENTATION ALGORITHM WITH STATE DURATION CONSTRAINTS</head><p>By considering the state duration in the modified Viterbi algorithm (Section IILA), an additional dimension is intro-</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An inequality and associated maximization technique in statistical estimation for probabilistic functions of Markov processes</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inequalities</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural-network architecture for linear and nonlinear predictive hidden Markov models: Application to speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hassanein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elmasry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Neworks for Signal Processinx</title>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Kamm</surname></persName>
		</editor>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="41" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A generalized hidden Markov model with state-conditioned trend functions of time for the speech signal</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">I</biblScope>
			<biblScope unit="page" from="65" to="78" />
			<date type="published" when="1992-04">Apr. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phonemic hidden Markov models with continuous mixture output densities for large vocabulary word recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="1979">July 1991. 1979</date>
		</imprint>
	</monogr>
	<note>J. H. Goodnight</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The segmental k-means algorithm for estimating parameters of hidden Markov models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Pi-ocessing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1639" to="1641" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation for multivariate ohservations of Markov sources</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Liporace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="729" to="734" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A stochastic segment model for phonemebased continuous speech recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roucos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. ACOUSI.. Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1857" to="1869" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hidden Markov models: A guided tour</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Poritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. /E Int. Con5 Acoust., Speech. Signal Processing</title>
		<meeting>/E Int. Con5 Acoust., Speech. Signal essing<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989-11-14">Apr. 11-14, 1989</date>
			<biblScope unit="page" from="7" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">83-M&apos;8&amp;SM&apos;91) received the M.Sc. degree in 1984 and the Ph.D. degree in 1986, both in electrical engineefing from the University of Wisconsin, Madison. He worked on large vocabulary automatic speech recognition at INRS-Telecommunications</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986 to 1989</date>
			<pubPlace>Montreal, Quebec, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">His research interests include industrial experimental design, regression analysis, multivariate methods in pattem recognition, speech recognition, and general statistical methodology in engineering applications</title>
	</analytic>
	<monogr>
		<title level="m">computer engineering and the M.A.Sc. in electrical engineering in 1991 and 1993, respectively, both from the University of Waterloo</title>
		<title level="s">.Math. and M.Math. degrees from South-East University</title>
		<meeting><address><addrLine>Waterloo, Ontario, Canada; Waterloo, Ontario, Canada; Victoria; Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989. 1993</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, University of Waterloo ; Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge ; University of Victoria</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include speech analysis and speech recognition, computational phonology, models of speech production, statistical signal modeling, auditory signal processing, and auditory neuroscience. Dr. Sun is a member of the American Statistical Association and the American Society for Quality Control</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">He was with the University of Wisconsin, Madison, from 1977 to 1988 and was Professor and GM/NSERC Chair in Quality and Productivity in the Department of Statistics and Actuarial Science and the Institute for Improvement in Quality and Productivity</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Jeff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971">1971. 1976</date>
			<pubPlace>Hsinchu; Waterloo, Ontario, Canada; Ann Arbor</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Wu received the B.S. from National Taiwan University ; University of California, Berkeley ; University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include quality improvement, survey sampling</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">:</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989-02">Feb. 1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ASA. intensive methods. Dr. Wu received the 1987 COPSS Award, the 1990 Wilcoxon Prize, and the 1992 Brumbaugh Award. He has served on the editorial boards of several journals including Technometrics</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">W</forename><surname>Zue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JASA, and Statistica Sinica</title>
		<imprint>
			<date type="published" when="1991-08">Aug. 1991. August 1993</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology, Cambridge, MA</orgName>
		</respStmt>
	</monogr>
	<note>Annals of Statistics. He is a Fellow of the IMS and the</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
