<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-04-29">29 Apr 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bin</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kan</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">LightTrack: Finding Lightweight Neural Networks for Object Tracking via One-Shot Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-04-29">29 Apr 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.14545v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Object tracking has achieved significant progress over the past few years. However, state-of-the-art trackers become increasingly heavy and expensive, which limits their deployments in resource-constrained applications. In this work, we present LightTrack, which uses neural architecture search (NAS) to design more lightweight and efficient object trackers. Comprehensive experiments show that our LightTrack is effective. It can find trackers that achieve superior performance compared to handcrafted SOTA trackers, such as SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> and Ocean [56], while using much fewer model Flops and parameters. Moreover, when deployed on resource-constrained mobile chipsets, the discovered trackers run much faster. For example, on Snapdragon 845 Adreno GPU, LightTrack runs 12× faster than Ocean, while using 13× fewer parameters and 38× fewer Flops. Such improvements might narrow the gap between academic models and industrial deployments in object tracking task. LightTrack is released at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object tracking is one of the most fundamental yet challenging tasks in computer vision. Over the past few years, due to the rise of deep neural networks, object tracking has achieved remarkable progress. But meanwhile, tracking models are becoming increasingly heavy and expensive. For instance, the latest SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> and Ocean <ref type="bibr" target="#b53">[56]</ref> trackers respectively utilize 7.1G and 20.3G model Flops as well as 11.2M and 25.9M parameters to achieve stateof-the-art performance, being much more complex than the early SiamFC <ref type="bibr" target="#b2">[5]</ref> method (using 2.7G Flops and 2.3M parameters), as visualized in Fig. <ref type="figure" target="#fig_0">1</ref>. Such large model sizes and expensive computation costs hinder the deployment of tracking models in real-world applications, such as camera drones, industrial robotics, and driving assistant system, where model size and efficiency are highly constrained. The proposed LightTrack is superior than SiamFC <ref type="bibr" target="#b2">[5]</ref>, SiamRPN <ref type="bibr" target="#b28">[31]</ref>, SiamRPN++ <ref type="bibr" target="#b27">[30]</ref>, SiamFC++ <ref type="bibr" target="#b49">[52]</ref> and Ocean <ref type="bibr" target="#b53">[56]</ref>, while using much fewer Flops and parameters. Best viewed in color.</p><p>There are two straightforward ways to tackle the complexity and efficiency issues. One is model compression, while the other is compact model designing. Existing offthe-shelf compression techniques such as pruning and quantization can reduce model complexity, while they inevitably bring non-negligible performance degradation due to information loss <ref type="bibr" target="#b18">[21,</ref><ref type="bibr" target="#b35">38]</ref>. On the other hand, handcrafting new compact and efficient models is engineering expensive and heavily relies on human expertise and experience <ref type="bibr" target="#b52">[55,</ref><ref type="bibr" target="#b12">15]</ref>. This paper introduces a new solution -automating the design of lightweight models with neural architecture search (NAS), such that the searched trackers can be carried out in an efficient fashion on resource-limited hardware platforms. It is non-trivial because that object trackers typically need ImageNet pre-training, while NAS algorithms require the performance feedback on the target tracking task as supervision signals. Based upon recent one-shot NAS <ref type="bibr" target="#b38">[41,</ref><ref type="bibr" target="#b1">4,</ref><ref type="bibr" target="#b17">20]</ref>, we propose a new search algorithm dedicated to object tracking task, called LightTrack. It encodes all possible architectures into a backbone supernet and a head supernet. The backbone supernet is pre-trained on ImageNet then fine-tuned with tracking data, while the head supernet is directly trained on tracking data. The supernets are trained only once, then each candidate architecture inherits its weights from the supernets directly. Architecture search is performed on the trained supernets, using tracking accuracy and model complexity as the supervision guidance. On the other hand, to reduce model complexity, we design a search space consisting of lightweight building blocks, such as depthwise separable convolutions <ref type="bibr" target="#b8">[11]</ref> and inverted residual structure <ref type="bibr" target="#b42">[45,</ref><ref type="bibr" target="#b20">23]</ref>. Such search space allows the one-shot NAS algorithm to search for more compact neural architectures, striking a balance between tracking performance and computational costs.</p><p>Comprehensive experiments verify that LightTrack is effective. It is able to search out efficient and lightweight object trackers. For instance, LightTrack finds a 530M Flops tracker, which achieves an EAO of 0.33 on VOT-19 benchmark, surpassing the SOTA SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> by 4.6% while reducing its model complexity (48.9G Flops) by 98.9%. More importantly, when deployed on resourcelimited chipsets, such as edge GPU and DSP, the discovered tracker performs very competitive and runs much faster than existing methods. On Snapdragon 845 Adreno 630 GPU [3], our LightTrack runs 12× faster than Ocean <ref type="bibr" target="#b53">[56]</ref> (38.4 v.s. 3.2 fps), while using 13× fewer parameters (1.97 v.s. 25.9 M) and 38× fewer Flops (530 v.s. 20,300 M). Such improvements enable deep tracking models to be easily deployed and run at real-time speed on resource-constrained hardware platforms.</p><p>This work makes the following contributions.</p><p>• We present the first effort on automating the design of neural architectures for object tracking. We develop a new formulation of one-shot NAS and use it to find promising architectures for tracking.</p><p>• We design a lightweight search space and a dedicated search pipeline for object tracking. Experiments verify the proposed method is effective. Besides, the searched trackers achieve state-of-the-art performance and can be deployed on diverse resource-limited platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Tracking. In recent years, siamese trackers have become popular in object tracking. The pioneering works are SiamFC and SINT <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b44">47]</ref>, which propose to combine naive feature correspondence with the siamese framework. A large number of follow-up works have been proposed and achieved significant improvements <ref type="bibr" target="#b7">[10,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b29">32,</ref><ref type="bibr" target="#b31">34,</ref><ref type="bibr" target="#b46">49]</ref>. They mainly fall into three camps: more precise box estimation, more powerful backbone, and online update. More concretely, in contrast to the multiple-scale estimation in SiamFC, later works like SiamRPN <ref type="bibr" target="#b28">[31]</ref> and SiamFC++ <ref type="bibr" target="#b49">[52]</ref> leverage either anchorbased or anchor-free mechanism for bounding box estimation, which largely improve the localization precision. Meanwhile, SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> and Ocean <ref type="bibr" target="#b53">[56]</ref> take the powerful ResNet-50 <ref type="bibr" target="#b19">[22]</ref> instead of AlexNet <ref type="bibr" target="#b26">[29]</ref> as the backbone to enhance feature representation capability. On the other hand, ATOM <ref type="bibr" target="#b11">[14]</ref>, DiMP <ref type="bibr" target="#b3">[6]</ref>, and ROAM <ref type="bibr" target="#b50">[53]</ref> combine online update <ref type="bibr" target="#b37">[40]</ref> with the siamese structure and achieve state-of-the-art performance.</p><p>Though these methods achieve remarkable improvements, yet they bring much additional computation workload and large memory footprint, thus limiting their usage in real-world applications. For example, deep learning on mobile devices commonly requires model Flops to be less than 600M Flops <ref type="bibr" target="#b4">[7]</ref>, i.e., mobile setting. However, SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> with ResNet-50 backbone has 48.9G Flops, which exceeds the mobile setting by ∼80 times. Even SiamFC <ref type="bibr" target="#b2">[5]</ref>, using the shallow AlexNet, still cannot satisfy the restricted computation workload when deployed on embedded devices. In summary, there is a lack of studies on finding a good trade-off between model accuracy and complexity in object tracking.</p><p>Neural Architecture Search. NAS aims at automating the design of neural network architectures. Early methods search a network using either reinforcement learning <ref type="bibr" target="#b55">[58]</ref> or evolution algorithms <ref type="bibr" target="#b48">[51]</ref>. These approaches require training thousands of architecture candidates from scratch, leading to unaffordable computation overhead. Most recent works resort to the one-shot weight sharing strategy to amortize the searching cost <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b38">41]</ref>. The key idea is to train a single over-parameterized hypernetwork model, and then share the weights across subnets. Single-path with uniform sampling <ref type="bibr" target="#b17">[20]</ref> is one representative method in one-shot regime. In each iteration, it only samples one random path and trains the path using one batch data. Once the training process is finished, the subnets can be ranked by the shared weights. On the other hand, instead of searching over a discrete set of architecture candidates, differentiable methods <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b5">8]</ref> relax the search space to be continuous, such that the search can be optimized by the efficient gradient descent. Recent surveys on NAS can be found in <ref type="bibr" target="#b12">[15]</ref>.</p><p>NAS is primarily proposed for image classification and recently extended to other vision tasks, such as image segmentation <ref type="bibr" target="#b33">[36]</ref> and object detection <ref type="bibr" target="#b16">[19]</ref>. Our work is inspired by the recent DetNAS <ref type="bibr" target="#b6">[9]</ref>, but has three fundamental differences. First, the studied task is different. DetNAS is designed for object detection, while our work is for object tracking. Second, DetNAS only searches for backbone networks by fixing the head network with a pre-defined handcrafted structure. This may lead to that the searched backbone is sub-optimal, because it is biased towards fitting the fixed head, rather than the target task. In contrast, our method searches backbone and head architectures simultaneously, aiming to find the most promising combination for the target tracking task. Last, the search space is different. We design a new search space for object tracking dedicated to search for lightweight architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries on One-Shot NAS</head><p>Before introducing the proposed method, we briefly review the one-shot NAS approach, which serves as the basic search algorithm discussed in this work. One-shot NAS treats all candidate architectures as different subnets of a supernet and shares weights between architectures that have common components. More concretely, the architecture search space A is encoded in a supernet, denoted as N (A, W ), where W is the weight of the supernet. The weight W is shared across all the architecture candidates, i.e., subnets α ∈ A in N . The search of the optimal architecture α * is formulated as a nested optimization problem:</p><formula xml:id="formula_0">α * = arg max α∈A Acc val (N (α, W * (α))) , s.t. W * = arg min W L train (N (A, W )),<label>(1)</label></formula><p>where the constraint function is to optimize the weight W of the supernet N by minimizing the loss function L train on training dataset, while the objective function is to search architectures via ranking the accuracy Acc val of subnets on validation dataset based on the learned supernet weight W * . Only the weights of the single supernet N need to be trained, and subnets can then be evaluated without any separate training by inheriting trained weights from the one-shot supernet. This greatly speeds up performance estimation of architectures, since no subnet training is required, resulting in the method only costs a few GPU days.</p><p>To reduce memory footprint, one-shot methods usually sample subnets from the supernet N for optimization. For simplicity, this work adopts the single-path uniform sampling strategy, i.e., each batch only sampling one random path from the supernet for training <ref type="bibr" target="#b30">[33,</ref><ref type="bibr" target="#b17">20]</ref>. This singlepath one-shot method decouples the supernet training and architecture optimization. Since it is impossible to enumerate all the architectures α ∈ A for performance evaluation, we resort to evolutionary algorithms <ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b17">20]</ref> to find the most promising subnet from the one-shot supernet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">LightTrack</head><p>Searching lightweight architectures for object tacking is a non-trivial task. There exist three key challenges.</p><p>• First, in general, object trackers need model pre-training on image classification task for a good initialization, while NAS algorithms require supervision signals from target tasks. Searching architectures for object tracking requires to consider both the pre-training on ImageNet and the fine-tuning on tracking data. • Second, object trackers usually contain two parts: a backbone network for feature extraction and a head network for object localization. When searching for new architectures, NAS algorithms needs to consider the two parts as a whole, such that the discovered structures are suitable for the target tracking task. • Last but not the least, search space is critical for NAS algorithms and it defines which neural architectures a NAS approach might discover in principle. To find lightweight architectures, the search space requires to include compact and low-latency building blocks.</p><p>In this section, we tackle the aforementioned challenges and propose LightTrack based on one-shot NAS. We first introduce a new formulation of one-shot NAS specialized for object tracking task. Then, we design a lightweight search space consisting of depthwise separable convolutions <ref type="bibr" target="#b8">[11]</ref> and inverted residual structure <ref type="bibr" target="#b42">[45,</ref><ref type="bibr" target="#b20">23]</ref>, which allows the construction of efficient tracking architectures. At last, we present the pipeline of LightTrack, which is able to search diverse models for different deployment scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tracking via One-Shot NAS</head><p>Current prevailing object trackers (such as <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b11">14,</ref><ref type="bibr" target="#b3">6]</ref>) all require ImageNet pre-training for their backbone networks, such that the trackers can obtain good image representation. However, for architecture search, it is impossible to pre-train all backbone candidates individually on ImageNet, because the computation cost is very huge (ImageNet pretraining usually takes several days on 8 V100 GPUs just for a single network). Inspired by one-shot NAS, we introduce the weight-sharing strategy to eschew pre-training each candidate from scratch. More specifically, we encode the search space of backbone architectures into a supernet N b . This backbone supernet only needs to be pre-trained once on ImageNet, and its weights are then shared across different backbone architectures which are subnets of the one-shot model. The ImageNet pre-training is performed by optimizing the classification loss function L cls pre-train as Deep neural networks for object tracking generally contain two parts: one pre-trained backbone network for feature extraction and one head network for object localization. These two parts work together to determine the capacity of a tracking architecture. Therefore, for architecture search, it is critical to search the backbone and head networks as a whole, such that the discovered structure is well-suited to tracking task. To this end, we construct a tracking supernet N consisting of the backbone part N b and the head part N h , which is formulated as N = {N b , N h }. The backbone supernet N b is first pre-trained on ImageNet by Eq. ( <ref type="formula" target="#formula_1">2</ref>) and generates the weight W p b . The head supernet N h subsumes all possible localization networks in the space A h and shares the weight W b across architectures. The joint search of backbone and head architectures is conducted on tracking data, which reformulates the one-shot NAS as b , which speeds up convergence while improving tracking performance. During search, it is unaffordable to rank the accuracy of all the architectures in search space, the same as previous work <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b6">9]</ref>, we resort to evolutionary algorithms <ref type="bibr" target="#b39">[42,</ref><ref type="bibr" target="#b17">20]</ref> to find the most promising one.</p><formula xml:id="formula_1">W p b = arg min W b L cls pre-train (N b (A b , W b )),<label>(2)</label></formula><formula xml:id="formula_2">α * b , α * h = arg max α b ,α h ∈A Acc trk val (N (α b , W * b (α b ); α h , W * h (α h ))) , s.t. W * b , W * h = arg min W b ←W p b ,W h L trk train (N (A b , W b ; A h , W h )),<label>(3)</label></formula><p>Architecture Constraints. In real-world deployments, object trackers are usually required to satisfy additional constraints, such as memory footprint, model Flops, energy consumption, etc. In our method, we mainly consider the model size and Flops, which are two key indicators when evaluating whether a tracker can be deployed on specific resource-constrained devices. We preset budgets on networks' Params and Flops and impose constraints as</p><formula xml:id="formula_3">F lops(α * b ) + F lops(α * h ) ≤ F lops max , P arams(α * b ) + P arams(α * h ) ≤ P arams max .<label>(4)</label></formula><p>The evolutionary algorithm is flexible in dealing with different budget constraints, because the mutation and crossover processes can be directly controlled to generate proper candidates to satisfy the constraints <ref type="bibr" target="#b17">[20]</ref>. Search can also be </p><formula xml:id="formula_4">16 2 × 128 DSConv 6 C 1 1 1 16 2 × C 1 DSConv / Skip 3 C 1 7 1 16 2 × C 1 3x3 Conv 1 1 1 1</formula><p>Reg Head</p><formula xml:id="formula_5">16 2 × 128 DSConv 6 C 2 1 1 16 2 × C 2 DSConv / Skip 3 C 2 7 1 16 2 × C 2 3x3 Conv 1 4 1 1</formula><p>repeated many times on the same supernet once trained, using different constraints (e.g., Flops max = 600M or others).</p><p>These properties naturally make one-shot paradigm practical and effective for searching tracking architectures specialized to diverse deployment scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Search Space</head><p>To search for efficient neural architectures, we use depthwise separable convolutions (DSConv) <ref type="bibr" target="#b8">[11]</ref> and mobile inverted bottleneck (MBConv) <ref type="bibr" target="#b42">[45]</ref> with squeeze-excitation module <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b20">23]</ref> to construct a new search space. The space is composed of a backbone part A b and a head part A h , which are elaborated in Tab. 1.</p><p>Backbone Space A b . There are six basic building blocks in the backbone space, including MBConv with kernel sizes of {3, 5, 7} and expansion rates of {4, 6}. Backbone candidates are constructed by stacking the basic blocks. All candidates in the space have 4 stages with a total stride of 16. In each stage, the first block has a stride of 2 for feature downsampling. Except for the first two stages, each stage contains up to 4 blocks for search. There are 14 layers in the backbone space, as listed in Tab. 1 (i.e., the layers with a choice number of 6). This space contains about 6 14 ≈7.8×10 10 possible backbone architectures for search.</p><p>Head Space A h . A head architecture candidate contains two branches: one for classification while the other for regression. Both of them include at most 8 searchable layers (see Tab. 1). The first layer is a DSConv with kernel sizes of {3, 5} and channel numbers of {128, 192, 256}. The subsequent 7 layers follow the same channel setting as the first layer, and have kernel choices of {3, 5}. An additional skip  connection is used to enable elastic depth of head architectures <ref type="bibr" target="#b55">[58]</ref>. Different from the backbone space, the head does not include the kernel choice of 7 because the feature resolution has been relatively low. The head space contains about (3 × 3 8 ) 2 ≈3.9×10 8 possible architectures for search.</p><p>In addition, at present, there is no definitive answer to the question of which layer's feature is more suitable for object tracking. We thereby add a new dimension in the search space to allow the one-shot method to determine the output feature layer automatically. Specifically, during supernet training, we randomly pick up an end layer from the last eight blocks in the backbone supernet, and use the output of the picked layer as the extracted feature. Such strategy is able to sample different possible blocks, and allows evolutionary search algorithm to evaluate which layer is better.</p><p>It is worth noting that the defined search space contains architectures ranging from 208M to 1.4G Flops with parameter sizes from 0.2M to 5.4M. Such space is much more lightweight than existing handcrafted networks. For example, the human-designed SiamRPN++ with ResNet-50 backbone has 48.9G FLOPs with 54M Params <ref type="bibr" target="#b19">[22]</ref>, being orders of magnitude more complex than architectures in the designed search space. This low-complexity space makes the proposed one-shot NAS algorithm easier to find promising lightweight architectures for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Search Pipeline</head><p>Our LightTrack includes three sequential phases: pretraining backbone supernet, training tracking supernet, and searching with evolutionary algorithm on the trained supernets. The overall pipeline is visualized in Fig. <ref type="figure" target="#fig_3">2</ref>.</p><p>Phase 1: Pre-training Backbone Supernet. The backbone supernet N b encodes all possible backbone networks in the search space A b . The structure of N b is presented in Tab. 1. As defined in Eq. ( <ref type="formula" target="#formula_1">2</ref>), the pre-training of the back-bone supernet N b is to optimize the cross-entropy loss on ImageNet. To decouple the weights of individual subnets, we perform uniform path sampling for the pre-training. In other words, in each batch, only one random path is sampled for feedforward and backward propagation, while other paths are frozen.</p><p>Phase 2: Training Tracking Supernet. The structure of the tracking supernet N is visualized in Fig. <ref type="figure" target="#fig_3">2</ref> (middle). In essence, it is a variant of Siamese tracker <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b53">56]</ref>. Specifically, it takes a pair of tracking images as the input, comprising an exemplar image and a search image. The exemplar image represents the object of interest, while the search image represents the search area in subsequent video frames. Both inputs are processed by the pre-trained backbone network for feature extraction. The generated two feature maps are cross-correlated to generate correlation volumes. The head network contains one classification branch and one regression branch for object localization. The architecture of the head supernet can be found in Tab. 1.</p><p>The training also adopts the single-path uniform sampling scheme, but involving the tracking head and metrics. In each iteration, the optimizer updates one random path sampled from the backbone and head supernets. The loss function L trk train in Eq. ( <ref type="formula" target="#formula_2">3</ref>) includes the common-used binary cross-entropy loss for foreground-background classification and the IoU loss <ref type="bibr" target="#b51">[54]</ref> for object bounding-box regression.</p><p>Phase 3: Searching with Evolutionary Algorithm. The last phase is to perform evolutionary search on the trained supernet. Paths in the supernet are picked and evaluated under the direction of the evolutionary controller. At first, a population of architectures is initialized randomly. The top-k architectures are picked as parents to generate child networks. The next generation networks are generated by mutation and crossover. For crossover, two randomly selected candidates are crossed to produce a new Table 2: Comparisons on VOT-19 <ref type="bibr" target="#b25">[28]</ref>. (G) and (M) represent using GoogleNet and MobileNet-V2 as backbones, respectively. DiMP r indicates the real-time version of DiMP, as reported in <ref type="bibr" target="#b25">[28]</ref>. Ocean(off) denotes the offline version of Ocean <ref type="bibr" target="#b53">[56]</ref>. Some values are missing because either the tracker is not open-resourced or the online update module does not support precise Flops estimation.</p><p>SiamMask <ref type="bibr" target="#b47">[50]</ref> SiamFC++(G) <ref type="bibr" target="#b49">[52]</ref> SiamRPN++(M) <ref type="bibr" target="#b27">[30]</ref> ATOM <ref type="bibr" target="#b11">[14]</ref> TKU <ref type="bibr" target="#b45">[48]</ref> DiMP r <ref type="bibr" target="#b3">[6]</ref> Ocean(off) <ref type="bibr" target="#b53">[56]</ref> Ours  <ref type="formula" target="#formula_3">4</ref>). One necessary detail is about Batch Normalization <ref type="bibr" target="#b23">[26]</ref>. During search, subnets are sampled in a random way from the supernets. The issue is that the batch statistics on one path should be independent of others <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b6">9]</ref>. Therefore, we need to recalculate batch statistics for each single path (subnet) before inference. We sample a random subset from the tracking training set to recompute the batch statistics for the single path to be evaluated. It is extremely fast and takes only a few seconds because no back-propagation is involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Implementation Details</head><p>Search. Following the search pipeline, we first pre-train the backbone supernet on ImageNet for 120 epochs using the following settings: SGD optimizer with momentum 0.9 and weight decay 4e-5, initial learning rate 0.5 with linear annealing. Then, we train the head and the backbone supernets jointly on tracking data. The same as previous work <ref type="bibr" target="#b53">[56]</ref>, the tracking data consists of Youtube-BB <ref type="bibr" target="#b40">[43]</ref>, ImageNet VID <ref type="bibr" target="#b41">[44]</ref>, ImageNet DET <ref type="bibr" target="#b41">[44]</ref>, COCO <ref type="bibr" target="#b32">[35]</ref> and the training split of GOT-10K <ref type="bibr" target="#b22">[25]</ref>. The training takes 30 epochs, and each epoch uses 6×10 image pairs. The whole network is optimized using SGD optimizer with momentum 0.9 and weight decay 1e-4. Each GPU hosting 32 images, hence the mini-batch size is 256 images per iteration. The global learning rate increases linearly from 1e-2 to 3e-2 during the first 5 epochs and decreases logarithmically from 3e-2 to 1e-4 in the rest epochs. We freeze the parameters of the backbone in the first 10 epochs and set their learning rate to be 10× smaller than the global learning rate in the rest epochs. Finally, to evaluate the performance of paths in the supernet, we choose the validation set of GOT-10K <ref type="bibr" target="#b22">[25]</ref> as the evaluation data, since it does not have any overlap with both the training and the final test data.</p><p>Retrain. After evolutionary search, we first retrain the discovered backbone network for 500 epochs on Imagenet using similar settings as EfficientNet <ref type="bibr" target="#b43">[46]</ref>: MSProp optimizer with momentum 0.9 and decay 0.9, weight decay 1e-5, dropout ratio 0.2, initial learning rate 0.064 with a warmup in the first 3 epochs and a cosine annealing, Au-toAugment <ref type="bibr" target="#b9">[12]</ref> policy and exponential moving average are adopted for training. Next, we fine-tune the discovered backbone and head networks on the tracking data. The finetuning settings in this step are similar to those of the supernet fine-tuning. The main differences include two aspects. 1) The searched architecture is trained for 50 epochs, which is longer than that of the tracking supernet fine-tuning. (2) The global learning rate increases from 2e-2 to 1e-1 during the first 5 epochs and then decreases from 1e-1 to 2e-4 in the rest epochs.</p><p>Test. The inference follows the same protocols as in <ref type="bibr" target="#b2">[5,</ref><ref type="bibr" target="#b28">31]</ref>. The feature of the target object is computed once at the first frame, and then consecutively matched with subsequent search images. The hyper-parameters in testing are selected with the tracking toolkit <ref type="bibr" target="#b53">[56]</ref>, which contains an automated parameter tuning algorithm. Our trackers are implemented using Python 3.7 and PyTorch 1.1.0. The experiments are conducted on a server with 8 Tesla V100 GPUs and a Xeon E5-2690 2.60GHz CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results and Comparisons</head><p>We compare LightTrack to existing hand-designed object trackers with respect to model performance, complexity and run-time speed. The performance is evaluated on four benchmarks, including VOT-19 <ref type="bibr" target="#b25">[28]</ref>, GOT-10K <ref type="bibr" target="#b22">[25]</ref>, TrackingNet <ref type="bibr" target="#b36">[39]</ref> and LaSOT <ref type="bibr" target="#b13">[16]</ref>, while the speed is tested on resource-constrained hardware platforms, involving Apple iPhone7 PLUS, Huawei Nova 7 5G, and Xiaomi Mi 8. Moreover, we provide three versions of LightTrack under different resource constraints, i.e., LightTrack Mobile (≤600M Flops, ≤2M Params), LargeA (≤800M Flops, ≤3M Params) and LargeB (≤800M Flops, ≤4M Params).</p><p>VOT-19. This benchmark contains 60 challenging sequences, and measures tracking accuracy and robustness simultaneously by expected average overlap (EAO). As reported in Tab. 2, LightTrack-Mobile achieves superior performance compared to existing SOTA offline trackers, such as SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> and SiamFC++ <ref type="bibr" target="#b49">[52]</ref>, while using &gt;10 times fewer model Flops and Params. Furthermore, com-Table <ref type="table">3</ref>: Comparisons on GOT-10k <ref type="bibr" target="#b22">[25]</ref>. (R) and (G) represents ResNet-50 and GoogleNet, respectively. DaSiam <ref type="bibr" target="#b54">[57]</ref> SiamRPN++(R) <ref type="bibr" target="#b27">[30]</ref> ATOM <ref type="bibr" target="#b11">[14]</ref> Ocean-offline <ref type="bibr" target="#b53">[56]</ref> SiamFC++(G) <ref type="bibr" target="#b49">[52]</ref> Ocean-online <ref type="bibr" target="#b53">[56]</ref> DiMP-50 <ref type="bibr" target="#b3">[6]</ref> Ours  ECO <ref type="bibr" target="#b10">[13]</ref> DaSiam <ref type="bibr" target="#b54">[57]</ref> C-RPN <ref type="bibr" target="#b14">[17]</ref> ATOM <ref type="bibr" target="#b11">[14]</ref> SiamFC++(A) <ref type="bibr" target="#b49">[52]</ref> SiamRPN++(R) <ref type="bibr" target="#b27">[30]</ref> DiMP-50 <ref type="bibr" target="#b3">[6]</ref> Ours  pared to the trackers with online update, such as ATOM <ref type="bibr" target="#b11">[14]</ref> and DiMP r <ref type="bibr" target="#b3">[6]</ref>, LightTrack-LargeB is also competitive, surpassing them by 5.6% and 3.6% respectively. This demonstrates the efficacy of the proposed one-shot search algorithm and the discovered architecture.</p><p>GOT-10K. GOT-10K <ref type="bibr" target="#b22">[25]</ref> is a new benchmark covering a wide range of common challenges in object tracking, such as deformation and occlusion. Tab. 3 shows that LightTrack obtains state-of-the-art performance, compared to current prevailing trackers. The AO score of LightTrack-Mobile is 1.6% and 1.9% superior than SiamFC++(G) <ref type="bibr" target="#b49">[52]</ref> and Ocean(off) <ref type="bibr" target="#b53">[56]</ref>, respectively. Besides, if we loosen the computation constraint, the performance of LightTrack will be further improved. For example, LightTrack-LargeB outperforms DiMP-50 <ref type="bibr" target="#b3">[6]</ref> by 1.2%, while using 8× fewer Params (3.1 v.s. 26.1 M).</p><p>TrackingNet. TrackingNet <ref type="bibr" target="#b36">[39]</ref> is a large-scale shortterm tracking benchmark containing 511 video sequences in test set. Tab. 4 presents that LightTrack-Mobile achieves better precision (69.5%), being 0.8% higher than DiMP-50 <ref type="bibr" target="#b3">[6]</ref>. Besides, the P norm and AUC of LightTrack-Mobile are comparable to SiamRPN++ and DiMP-50, while using 96% and 92% fewer model Params, respectively.</p><p>LaSOT. LaSOT <ref type="bibr" target="#b13">[16]</ref> is by far the largest single object tracking benchmark with high-quality frame-level annotations. As shown in Fig. <ref type="figure" target="#fig_4">3</ref>, LightTrack-LargeB achieves a success score of 0.555, which surpasses SiamFC++(G) <ref type="bibr" target="#b49">[52]</ref> and Ocean-offline <ref type="bibr" target="#b53">[56]</ref> by 1.2% and 2.9%, respectively.  Compared to the online DiMP-18 <ref type="bibr" target="#b3">[6]</ref>, LightTrack-LargeB improves the success score by 2.1%, while using 12× fewer Params (3.1 v.s. 39.3 M).</p><p>Speed. Fig. <ref type="figure" target="#fig_6">4</ref> summarizes the run-time speed of Light-Track on resource-limited mobile platforms, , including Apple iPhone 7 Plus, Huawei Nova 7 and Xiaomi Mi 8. We observe that SiamRPN++ <ref type="bibr" target="#b27">[30]</ref> and Ocean <ref type="bibr" target="#b53">[56]</ref> cannot run at real-time speed (i.e., &lt; 25 fps) on these edge devices, such as Snapdragon 845 Adreno 630 GPU and Hexagon 685 DSP. In contrast, our LightTrack run much more efficiently, being 3∼6× faster than SiamRPN++ (MobileNetV2 backbone), and 5∼17× faster than Ocean (offline) on Snapdragon 845 GPU and DSP [3], Apple A10 Fusion Pow-erVR GPU [1], and Kirin 985 Mali-G77 GPU [2]. The real-time speed allows LightTrack to be deployed and applied in resource-constrained applications, such as camera drones where edge chipsets are commonly used. The speed improvements also demonstrate that LightTrack is effective and can find more compact and efficient object trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation and Analysis</head><p>Component-wise Analysis. We evaluate the effects of different components in our LightTrack on VOT-19 <ref type="bibr" target="#b25">[28]</ref>, and report the results in Tab. 5. Our baseline is a handcrafted mobile tracker, which takes MobileNetV3-large <ref type="bibr" target="#b20">[23]</ref> as the backbone (chopping off the last stage), and outputs features from the last layer with a stride of 16. The head network stacks 8 layers of depthwise separable con-   <ref type="bibr" target="#b8">[11]</ref> while "MBConv" denotes mobile inverted bottleneck <ref type="bibr" target="#b42">[45]</ref> with squeeze excitation <ref type="bibr" target="#b21">[24]</ref>.</p><p>volution (DSConv) <ref type="bibr" target="#b8">[11]</ref> in both classification and regression branches. For each DSConv, the kernel size is set to 3×3 and the number of channels is 256. The EAO performance of the baseline is 0.268. For ablation, we add the components in the baseline into search and change the handcrafted architectures with automatically searched ones. As presented in Tab. 5 #2, when the backbone architecture is automatically searched, the EAO performance is improved by 2.4%. This demonstrates that the handdesigned MobileNetV3-large backbone is not optimal for object tracking, because it is primarily designed for image classification, where the precise localization of the object is not paramount. If we add the output feature layer into search, the performance is further improved to 0.307. This shows that our method can search out a better layer for feature extraction. The comparison between #4 and #1 shows that the searchable head architecture is superior to the handcrafted one, inducing 2.9% EAO gains. When searching the three components together, as shown in #5, the complete LightTrack achieves better performance than only searching parts of the tracking network.</p><p>Impact of ImageNet Pre-training. We pre-train the searched architecture on ImageNet for 0, 200 and 500 epochs, and evaluate their impact for final tracking performance. As reported in Tab. 6, no pre-training has a significantly negative impact on tracking accuracy. Better pretraining allows the tracker to achieve higher performance.</p><p>Analysis of Searched Architecture. Fig. <ref type="figure">5</ref> visualizes the LightTrack-Mobile architecture searched by the proposed one-shot NAS method. We observe several interesting phenomena. 1) There are about 50% of the backbone blocks using MBConv with kernel size of 7x7. The underlying reason may be that large receptive fields can improve the localization precision. 2) The searched architecture chooses the second-last block as the feature output layer. This may reveals that tracking networks might not prefer high-level features. 3) The classification branch contains fewer layers than the regression branch. This may be attributed to the fact that coarse object localization is relatively easier than precise bounding box regression. These findings might enlighten future works on designing new tracking networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper makes the first effort on designing lightweight object trackers via neural architecture search. The proposed method, i.e., LightTrack, reformulates one-shot NAS specialized for object tracking, as well as introducing an effective search space. Extensive experiments on multiple benchmarks show that LightTrack achieves state-of-the-art performance, while using much fewer Flops and parameters. Besides, LightTrack can run in real-time on diverse resource-restricted platforms. We expect this work might be able to narrow the gap between academic methods and industrial applications in object tracking field.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Comparisons with state-of-the-art trackers in terms of EAO performance, model Flops and parameters on VOT-19 benchmark. The circle diameter is in proportion to the size of model parameter. The proposed LightTrack is superior than SiamFC<ref type="bibr" target="#b2">[5]</ref>, SiamRPN<ref type="bibr" target="#b28">[31]</ref>, SiamRPN++<ref type="bibr" target="#b27">[30]</ref>, SiamFC++<ref type="bibr" target="#b49">[52]</ref> and Ocean<ref type="bibr" target="#b53">[56]</ref>, while using much fewer Flops and parameters. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>where A b represents the search space for backbone architectures, while W b denotes the parameter of the backbone supernet N b . The pre-trained weight W p b are shared across different backbone architectures and serve as the initialization for the subsequent search of tracking architectures. Such weight-sharing scheme allows the ImageNet pre-training to be performed only on the backbone supernet instead of each subnet, thereby reducing the training costs by orders of magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where the constraint function is to train the tracking supernet N and optimize the weights W b and W h simultaneously, while the objective function is to find the optimal backbone α * b and the head α * h via ranking the accuracy Acc trk val of candidate architectures on validation set of the tracking data. The evaluation of Acc trk val only requires inference because the weights of the architectures α b and α h are inherited from W * b (α b ) and W * h (α h ) (without the need of extra training). Note that, before starting the supernet training, we use the pre-trained weight W p b to initialize the parameter W b , i.e., W b ← W p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Search pipeline of the proposed LightTrack. There are three phases: pretraining backbone supernet, training tracking supernet, and searching with evolutionary algorithm on the tracking supernet. Better view in color with zoom-in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparisons on LaSOT test dataset [16].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Run-time speed on resource-limited platforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Search space and supernet structure. "N choices " represents the number of choices for the current block. "Chn" and "Rpt" denote the number of channels per block and the maximum number of repeated blocks in a group, respectively. "Stride" indicates the convolutional stride of the first block in each repeated group. The classification and regression heads are allowed to use different numbers of channels, denoted as C 1 , C 2 ∈ {128, 192, 256}. The input is a search image with size of 256×256×3.</figDesc><table><row><cell></cell><cell>Input Shape</cell><cell>Operators</cell><cell cols="4">N choices Chn Rpt Stride</cell></row><row><cell></cell><cell>256 2 × 3</cell><cell>3 × 3 Conv</cell><cell>1</cell><cell>16</cell><cell>1</cell><cell>2</cell></row><row><cell>Backbone</cell><cell>128 2 × 16 128 2 × 16 64 2 × 24 32 2 × 40</cell><cell>DSConv MBConv MBConv MBConv</cell><cell>1 6 6 6</cell><cell>16 24 40 80</cell><cell>1 2 4 4</cell><cell>1 2 2 2</cell></row><row><cell></cell><cell>16 2 × 80</cell><cell>MBConv</cell><cell>6</cell><cell>96</cell><cell>4</cell><cell>1</cell></row><row><cell>Cls Head</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparisons on TrackingNet test set [39]. (A) and (R) represent AlexNet and ResNet-50, respectively.</figDesc><table><row><cell>RTMDNet</cell></row><row><cell>[27]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>The architecture searched by the proposed LightTrack (Mobile). The searchable layers are drawn in colors while the fixed/pre-defined parts are plotted in grey. The "Stem" consists of a normal 2D convolution layer with kernel size of 3×3 and stride of 2, a BatchNorm layer, and a Swish activation layer. "DSConv" indicates depthwise separable convolution</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Classification Head</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Examplar Image 3x128x128 Search Image 3x256x256</cell><cell>Stem Stem</cell><cell>DSC 3x3 DSC 3x3</cell><cell>MBC 4 7x7 24x32x32 MBC 4 7x7 24x64x64</cell><cell>MBC 6 7x7 MBC 6 7x7</cell><cell>24x32x32 24x64x64</cell><cell>MBC 4 4 3x3 3x3 MBC MBC 4 4 3x3 MBC 3x3</cell><cell>40x16x16 40x32x32</cell><cell>MBC 4 5x5 MBC 4 5x5 MBC 4 5x5 MBC 4 5x5</cell><cell>40x16x16 40x32x32</cell><cell>MBC 6 7x7 MBC 6 7x7</cell><cell>40x16x16 40x32x32</cell><cell>MBC 6 3x3 MBC 6 3x3 MBC 6 3x3 MBC 6 3x3</cell><cell>40x16x16 40x32x32</cell><cell>MBC 4 7x7 MBC 4 7x7</cell><cell>80x8x8 80x16x16</cell><cell>MBC 4 3x3 MBC 4 3x3 MBC 4 3x3 MBC 4 3x3</cell><cell>80x8x8 80x16x16</cell><cell>MBC 4 7x7 MBC 4 7x7</cell><cell>80x8x8 80x16x16</cell><cell>MBC 4 7x7 MBC 4 7x7</cell><cell>80x8x8 80x16x16</cell><cell>MBC 6 7x7 MBC 6 7x7</cell><cell>96x8x8 96x16x16</cell><cell>MBC 4 5x5 MBC 4 5x5 MBC 4 5x5 MBC 4 5x5</cell><cell>96x8x8 96x16x16</cell><cell>MBC 6 3x3 MBC 6 3x3 MBC 6 3x3 MBC 6 3x3</cell><cell>96x16x16 96x8x8</cell><cell>Correlation</cell><cell>256x16x16 192x16x16</cell><cell>Regression Head 256x16x16 256x16x16 256x16x16 192x16x16 192x16x16 192x16x16</cell><cell>256x16x16 192x16x16</cell><cell>256x16x16 192x16x16</cell><cell>192x16x16</cell><cell>192x16x16</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation for searchable components. indicates automatically searched, while denotes hand-designed.</figDesc><table><row><cell># Backbone Output Layer Head</cell><cell>EAO</cell></row><row><cell>1</cell><cell>0.268</cell></row><row><cell>2</cell><cell>0.292</cell></row><row><cell>3</cell><cell>0.307</cell></row><row><cell>4</cell><cell>0.297</cell></row><row><cell>5</cell><cell>0.333</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Impact of ImageNet Pre-training.</figDesc><table><row><cell></cell><cell cols="3">Epoch 0 Epoch 200 Epoch 500</cell></row><row><cell>Top-1 Acc (%)</cell><cell>-</cell><cell>72.4</cell><cell>77.6</cell></row><row><cell>EAO on VOT-19 (%)</cell><cell>21.3</cell><cell>31.2</cell><cell>33.3</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We would like to thank the reviewers for their insightful comments. Lu and Wang are supported in part by the National Key R&amp;D Program of China under Grant No. 2018AAA0102001 and National Natural Science Foundation of China under grant No. 61725202, U1903215, 61829102, 91538201, 61771088, 61751212 and Dalian Innovation leader's support Plan under Grant No. 2018RD07.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lighttrack-Largeb</surname></persName>
		</author>
		<idno>0.552] LightTrack-LargeA [0.547] SiamFCpp-GoogLeNet [0.537] LightTrack-Mobile [0.534] DiMP-18 [0.526] Ocean-offline [0.505] ATOM [0.491] SiamRPNpp [0.474] SiamFCpp-AlexNet [0.373] MDNet [0.360] VITAL [0.339] SiamFC [0.337] StructSiam References [1</idno>
		<ptr target="https://www.hisilicon.com/en/products/Kirin/Kirin%20985.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>João</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip H S</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2007">2019. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DetNAS: Backbone search for object detection</title>
		<author>
			<persName><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2006">2019. 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName><forename type="first">Zedu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guorong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
	</analytic>
	<monogr>
		<title level="m">Franc ¸ois Chollet</title>
				<imprint>
			<date type="published" when="2008">2017. 2, 3, 4, 8</date>
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Ekin D Cubuk</surname></persName>
		</author>
		<author>
			<persName><surname>Zoph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ECO: Efficient convolution operators for tracking</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2019. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LaSOT: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName><forename type="first">Liting</forename><surname>Heng Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Siamese cascaded region proposal networks for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph convolutional tracking</title>
		<author>
			<persName><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><surname>Nas-Fpn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2006">2020. 2, 3, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-bilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2007">2019. 2, 3, 4, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">GOT-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName><forename type="first">Lianghua</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-Time MDNet</title>
		<author>
			<persName><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeany</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mooyeol</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ales Leonardis, et al. The seventh visual object tracking VOT2019 challenge results</title>
		<author>
			<persName><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">SiamRPN++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2019. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2018. 1, 2, 3, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatial-temporal regularized correlation filters for visual tracking</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Target-aware deep tracking</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-Deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DARTS: differentiable architecture searc</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Teacher-students knowledge distillation for siamese trackers</title>
		<author>
			<persName><forename type="first">Yuanpei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingping</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10586</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Alsubaihi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName><forename type="first">Hyeonseob</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameters sharing</title>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet Large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2008">2018. 2, 3, 4, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tracking the known and the unknown by leveraging semantic information</title>
		<author>
			<persName><forename type="first">Ardhendu</forename><surname>Shekhar Tripathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Siam R-CNN: Visual tracking by re-detection</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SiamFC++: towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ROAM: Recurrently optimizing tracking model</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unitbox: An advanced object detection network</title>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
