<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Neural Architecture Search Under Distribution Shifts</title>
				<funder>
					<orgName type="full">THU-Bosch</orgName>
				</funder>
				<funder>
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_6ARwPfk">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>wang@tsinghua.edu.cn&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">THU-Bosch JCML center</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>&lt;wwzhu@tsinghua.edu.cn&gt;</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Xin Wang</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Neural Architecture Search Under Distribution Shifts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural architecture search has shown great potentials for automatically designing graph neural network (GNN) architectures for graph classification tasks. However, when there is a distribution shift between training and test graphs, the existing approaches fail to deal with the problem of adapting to unknown test graph structures since they only search for a fixed architecture for all graphs. To solve this problem, we propose a novel Graph neuRal Architecture Customization with disEntangled Self-supervised learning (GRACES) model which is able to generalize under distribution shifts through tailoring a customized GNN architecture suitable for each graph instance with unknown distribution. Specifically, we design a self-supervised disentangled graph encoder to characterize invariant factors hidden in diverse graph structures. Then, we propose a prototype based architecture self-customization strategy to generate the most suitable GNN architecture weights in a continuous space for each graph instance. We further propose a customized supernetwork to share weights among different architectures for the sake of efficient training. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed GRACES model can adapt to diverse graph structures and achieve state-of-the-art performance for graph classification tasks under distribution shifts.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Graph-structured data has attracted lots of attention in recent years for its flexible representation ability in various domains. Graph neural networks (GNNs) models such as GCN <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b33">(Veli?kovi? et al., 2018)</ref>, and GIN <ref type="bibr" target="#b45">(Xu et al., 2019)</ref> have been proposed and achieved great successes in many graph tasks. Generally, GNNs learn node representations by a recursive message passing scheme where nodes aggregate information from their neighbors iteratively. Then for the graph classification task, GNNs use pooling methods to derive graph-level representations. Different GNN architectures mainly differ in their messagepassing mechanism, i.e., how to exchange information, to adapt to the demands of different graph scenarios.</p><p>To save human efforts on designing GNN architectures for different tasks and automatically design more powerful GNNs, graph neural architecture search (GraphNAS) <ref type="bibr" target="#b19">(Li et al., 2020;</ref><ref type="bibr" target="#b9">Gao et al., 2020;</ref><ref type="bibr" target="#b40">Wei et al., 2021)</ref> has been utilized to search for an optimal GNN architecture. These automatically designed architectures have achieved competitive or better performances compared with manually designed GNNs on datasets with the same distributions under the independently and identically distributed (I.I.D.) assumption, i.e., the training and test graphs are independently sampled from the identical distribution.</p><p>Nevertheless, distribution shifts are ubiquitous and inevitable in real-world graph applications where there exist a large number of unforeseen and uncontrollable hidden factors. Taking drug discovery as an example, there exists only a limited amount of training data that can be obtained for experiments, and the interaction mechanism varies greatly for different molecules due to their complex chemical properties <ref type="bibr" target="#b13">(Ji et al., 2022)</ref>. That being the case, the GNN models designed for drug discovery frequently have to be tested on data with distribution shifts.</p><p>The existing GraphNAS approaches under the I.I.D. assumption only search a single fixed GNN architecture based on the training set before directly applying the selected architecture on the test set, failing to deal with varying distribution shifts under the out-of-distribution setting. Because the single GNN architecture discovered by existing methods may overfit the distributions of the training graph data, it may fail to make accurate predictions on test data with various distributions different from the training data.</p><p>To solve this problem, in this paper we are the first to study graph neural architecture search for graph classification under distribution shifts, to the best of our knowledge. We propose Graph neuRal Architecture Customization with disEntangled Self-supervised learning (GRACES), which is able to capture key information on graphs with widely varying distributions under the out-of-distribution settings through tailoring a unique GNN architecture for each graph instance. Specifically, we first design a self-supervised disentangled graph encoder which projects graphs into a disentangled latent space, where each disentangled factor in the space is trained by the supervised task and corresponding self-supervised learning task simultaneously. This design is able to capture the key information hidden in graphs in a more controllable manner via the self-supervised disentangled graph representation, thus improving the ability of the representations to generalize under distribution shifts. We then propose architecture self-customization with prototype to tailor specialized GNN architectures for graphs based on the similarities of their representations with prototypes vectors in the latent space, where each prototype vector corresponds to one different operation. We further design the customized super-network with differentiable weights on the mixture of different operations, which has great flexibility to ensemble different combinations of operations and enable the proposed GRACES model to be easily optimized in an end-to-end fashion through gradient based methods. We remark that our designs of disentangled graph representations and learnable prototype-operation mapping together are able to enhance the generalization ability of our proposed model under distribution shifts. Extensive experiments on both synthetic and real-world datasets validate the superiority of our proposed GRACES model over existing baselines. Detailed ablation studies further verify the designs of GRACES. 1 Our contributions are summarized as follows.</p><p>? We are the first to study graph neural architecture search for graph classification under distribution shifts by proposing the Graph neuRal Architecture Customization with disEntangled Self-supervised learning (GRACES) model, to the best of our knowledge.</p><p>? We design three cascaded modules, i.e., self-supervised disentangled graph encoder, architecture selfcustomization with prototype strategy, and customized super-network, to tailor a unique GNN architecture for each graph instance, thus enabling the ability of our proposed GRACES model in dealing with generalization under distribution shifts with non-I.I.D. settings.</p><p>? Extensive experimental results demonstrate that our proposed GRACES model is able to significantly outperform state-of-the-art baselines in terms of graph classification accuracy on both synthetic and real-world datasets.</p><p>The rest of the paper is organized as follows. In Section 2, we introduce the problem formulation and preliminaries.</p><p>1 Our code will be released at https://github.com/ THUMNLab/AutoGL</p><p>We present our proposed method in Section 3 and report experimental results in Section 4. We review related works in Section 5. In Section 6, we conclude the paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation and Preliminaries</head><formula xml:id="formula_0">F E G,Y ?P (Gte,Yte) [? (F (G), Y ) |G tr , Y tr ] , (1)</formula><p>where ? : Y ? Y ? R is a loss function. In this paper, we consider a common yet challenging setting that neither Y te nor unlabeled G te is available in the training phase <ref type="bibr">(Wang et al., 2021a)</ref>. Besides, we mainly focus on F being GNNs, which are state-of-the-art models for graph machine learning. A typical GNN consists of two parts: an architecture ? ? A and learnable weights w ? W, where A and W denotes the architecture space and the weight space, respectively. Therefore, we denote GNNs as the following mapping function F ?,w : G ? Y.</p><p>For searching GNN architectures, we mostly focus on different GNN layers, i.e., message-passing functions. Therefore, we consider a search space of standard layer-by-layer architectures without sophisticated connections such as residual or jumping connections, though our proposed method can be easily generalized. We choose five widely used GNN layers as our operation candidate set O, including GCN <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2017)</ref>, GAT <ref type="bibr" target="#b33">(Veli?kovi? et al., 2018)</ref>, GIN <ref type="bibr" target="#b45">(Xu et al., 2019)</ref>, SAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, and Graph-Conv <ref type="bibr" target="#b25">(Morris et al., 2019)</ref>. Besides, we also adopt MLP, which does not consider graph structures. We fix the pooling layer at the end of the GNN architecture as the standard global mean pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>In this section, we present our proposed method. First, we introduce our framework in Section 3.1. In Section 3.2, we present the self-supervised disentangled graph encoder to capture diverse graph structures. Then, we propose the architecture self-customization with prototype strategy in Section 3.3, which maps the learned graph representation into a tailored GNN architecture. In Section 3.4, we introduce the customized super-network which enables efficient training by weight sharing. We show the optimization procedure in Section 3.5. We analyze the complexity of our method in Section 3.6. Finally, we give some discussion about our model in Section 3.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework</head><p>In our proposed method, instead of using a fixed GNN architecture for all graphs as in the existing methods, we customize a GNN architecture for each graph. In this way, our proposed method is more flexible and can better handle test graphs under distribution shift since it is known that different GNN architectures suit different graphs <ref type="bibr" target="#b6">(Corso et al., 2020;</ref><ref type="bibr" target="#b46">Xu et al., 2021)</ref>. To achieve this goal, we aim to learn an architecture mapping function ? A : G ? A and a weight mapping function ? w : G ? A ? W so that these functions can automatically generate the optimal GNN for different graphs, including the architecture and its weights. Since the architecture only depends on the graph in our settings, we can further simplify the weight mapping function as ? w : G ? W. Therefore, we transform Eq. ( <ref type="formula">1</ref>) into the following objective function:</p><formula xml:id="formula_1">min ? A ,?w ? Ntr i=1 ? F ?1(gi),?2(gi) (g i ) , y i +(1-?)L reg ,<label>(2)</label></formula><p>where L reg is the regularizer and ? is a hyper-parameter.</p><p>In the following sections, we will introduce in details how to properly design ? A , ? w , and L reg so that our proposed method can generalize under distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Self-supervised disentangled graph Encoder</head><p>Graphs from different distributions can have diverse graph structures. To capture such diverse graph structures, we use a self-supervised disentangled graph encoder to learn low-dimensional representations of graphs. Specifically, we adopt K GNNs and learn K-chunk graph representations:</p><formula xml:id="formula_2">H (l) = K ? k=1 GNN(H (l-1) k , A),<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">H (l)</formula><p>k is the k-th chunk of the node representation at the l-th layer, A is the adjacent matrix of the graph, and ? represents concatenation. Using these disentangled GNN layers, we can capture different latent factors of the graphs. Then, we adopt a readout layer to aggregate node-level representations into a graph-level representation:</p><formula xml:id="formula_4">h = Readout(H (L) ).<label>(4)</label></formula><p>To learn the parameters of the self-supervised disentangled graph encoder, we use both graph supervised learning and self-supervised learning tasks.</p><p>Supervised learning. The downstream target graph task naturally provides supervision signals for learning the selfsupervised disentangled graph encoder. Therefore, We place a classification layer after the obtained graph representation to get the prediction for the graph classification task. Denote the graph representation for g i as h i . The supervised learning loss is as follows:</p><formula xml:id="formula_5">L sup = Ntr i=1 ? (C (h i ) , y i )<label>(5)</label></formula><p>where C(?) is the classification layer.</p><p>Self-supervised learning (SSL). Graph SSL aims to learn informative graph representation through pretext tasks, which has shown several advantages including reducing label reliance, enhancing robustness, and model generalization ability <ref type="bibr" target="#b23">(Liu et al., 2021;</ref><ref type="bibr" target="#b47">Yehudai et al., 2021)</ref>. Therefore, we propose to use graph SSL to complement the supervised learning task. Specifically, we set the SSL auxiliary task by generating pseudo labels from graphs structures and using the pseudo labels as extra supervision signals. Besides, we adopt different pseudo labels for different chunks of the disentangled GNN so that our disentangled encoder can capture different factors of the graph structure. In this paper, we focus on the degree distribution of graphs as a representative and explainable structural feature, while it is straightforward to generalize to other graph structures. Specifically, for the k-th GNN chunk, we generate pseudo labels by calculating the ratio of nodes that exactly have degree k. We formulate the SSL objective function as:</p><formula xml:id="formula_6">L ssl = Ntr i=1 K-1 k=1 ? ssl ?ssl i,k , y ssl i,k ,<label>(6)</label></formula><p>where y ssl i,k is the pseudo-label and ?ssl i,k is obtained by adopting a regression function, i.e., a linear layer followed by an activation function, on the k-th chunk of the graph representation h i . Notice that we also leave the last chunk without SSL tasks to allow more flexibility in learning the disentangled graph representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture Self-customization with prototype</head><p>After obtaining the graph representation h, we propose the architecture self-customization with prototype strategy to map the representation into a tailored GNN architecture. Specifically, denote the probability of choosing an operation o in the i-th layer of the searched architecture as p i o , where i ? {1, 2, ..., N }, N is the number of layers, and o ? O. We calculate the probability as follows:</p><formula xml:id="formula_7">pi o = h ? q i o ?q i o ? 2 , p i o = exp pi o o ? ?O exp pi o ? ,<label>(7)</label></formula><p>where q i o is a learnable prototype vector representation of the operation o. We adopt the l 2 -normalization on q to ensure numerical stability and fair competition among different operations. Intuitively, in Eq. ( <ref type="formula" target="#formula_7">7</ref>), we learn a prototype vector for each candidate operation and select operations based on the preferences of the graph, i.e., if the graph representation has a large projection on a prototype vector, its corresponding operation is more likely to be selected. Besides, by using the exponential function, the length of h can decide the shape of p i o i.e., the larger ?h? 2 , the more likely that p i o are dominated by a few values, indicating that the graph requires specific operations.</p><p>Besides, to avoid the mode collapse problem, i.e., vectors of different operations are similar and therefore become indistinguishable, we adopt the following regularizer based on cosine distances between vectors to keep the diversity of operations:</p><formula xml:id="formula_8">L cos = i o,o ? ?O,o? =o ? q i o ? q i o ? ?q i o ? 2 .q i o ? 2 (8)</formula><p>Using the architecture self-customization with prototype, we can tailor the most suitable GNN architectures based on the graph representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Weight Generation using customized super-network</head><p>Besides GNN architectures, we also need the weights of the architectures. Following the NAS literature <ref type="bibr" target="#b22">(Liu et al., 2019;</ref><ref type="bibr" target="#b27">Pham et al., 2018)</ref>, we adopt a super-network to obtain the weights of architectures. Specifically, in the super-network, all possible operations are jointly considered by mixing different operations into a continuous space as follows:</p><formula xml:id="formula_9">f i (x) = o?O p i o o(x) (9)</formula><p>where x is the input of layer and f i (x) is the output. Then, we can optimize all the weights using gradient descend methods. Besides, since weights of different architectures are shared, the training will be much more efficient compared to training weights for different architectures separately.</p><p>A caveat to notice is that in most NAS literature, the architecture is discretized at the end of the search phase by choosing the operation with the largest p i o for all the layers. Then, the weights of the selected architecture are retrained. However, retraining is infeasible in our framework since test graphs can be tailored with different architectures from those for training graphs. Therefore, we directly use the weights of the super-network as the weights in the searched architecture. Besides, we also keep the continuous architecture without the discretization step, enhancing flexibility on architecture customization and simplifying the optimization strategy. Moreover, the intuition is that the customized super-network serves as a strong ensemble model with p i o being the ensemble weights, which is also known to benefit out-of-distribution generalization <ref type="bibr" target="#b31">(Shen et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Optimization Procedure</head><p>We have introduced three additional loss functions as the regularizer in Eq. (2), i.e., L sup and L ssl for the selfsupervised disentangled graph encoder and L cos for the self-customization module. Therefore, we have:</p><formula xml:id="formula_10">L = ?L main + (1 -?)L reg L reg = L sup + ? 1 L ssl + ? 2 L cos ,<label>(10)</label></formula><p>where L main is the supervision loss of the tailored architectures in Eq. ( <ref type="formula" target="#formula_1">2</ref>), ? 1 and ? 2 are hyper-parameters. (3) (4) Calculate L sup and L ssl using Eq. ( <ref type="formula" target="#formula_5">5</ref>) and Eq. ( <ref type="formula" target="#formula_6">6</ref>) Calculate architecture probability p i o using Eq. ( <ref type="formula" target="#formula_7">7</ref>) Calculate L cos using Eq. ( <ref type="formula">8</ref>) Get the parameters from the super-network Calculate the overall loss in Eq. ( <ref type="formula" target="#formula_1">2</ref>) Update parameters using gradient descends Update ? = ? -?? end while</p><p>For the overall optimization, we have two groups of loss functions: the classification loss and the regularizer. At an early stage of the training procedure, the self-supervised disentangled graph encoder may have not been properly trained and the learned graph representation is also not informative, leading to unstable architecture customization. Therefore, we set larger weights for the regularizer, i.e., a smaller initial ? in Eq. ( <ref type="formula" target="#formula_1">2</ref>), to force the self-supervised disentangled graph encoder to learn through its supervised learning and SSL tasks. As the training procedure continues, we can gradually focus more on training the self-customization module and the super-network by increasing ? as:</p><formula xml:id="formula_11">? t = ? 0 + t??,<label>(11)</label></formula><p>where ? t is the hyper-parameter value at the t-th epoch, ?? is a small constant. The overall algorithm is shown in Algorithm 1. In the evaluation phase, we directly generate the most suitable GNN architecture with its parameters for the test graphs without retraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Complexity Analysis</head><p>Denote |V |, |E| as the number of nodes and edges in the graph, respectively, and d as the dimensionality of hidden representations. We use d e and d s to denote the dimensionality of the self-supervised disentangled graph encoder and the customized super-network, respectively. Notice that d e is the overall dimensionality of K-chunks, i.e., the dimensionality of each chunk is d e /K. )). The above analyses show that our proposed method has a linear time complexity with respect to the number of nodes and edges, and the number of learnable parameters is constant, on par with previous GNNs and graph NAS methods. Besides, in practice, |O| is a small constant (e.g., |O| = 6 in our search space) and we find that usually d e ? d s . Therefore, the time complexity and the number of learnable parameters mainly depend on d s . To ensure a fair comparison with GNN baselines, we set a relatively small d s for our method so that all methods have a comparable number of parameters, i.e., |O|d 2 s ? d 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Discussion</head><p>In this part, we provide some intuitive explanation of our model. The continuous architectures can be regarded as a type of ensemble model <ref type="bibr" target="#b7">(Deng et al., 2020)</ref>, like dropout or multi-head attention. Different submodels in the architecture learn different knowledge from the data. These submodels also affect each other during the training phase, which is de facto a type of regularization. Furthermore, our model can be seen as a variant of the attention mechanism. Traditional attention mechanism can be represented as follows:</p><formula xml:id="formula_12">Attn q?k = Sim(q, k), (12) out = Attn q?k v,<label>(13)</label></formula><p>where k, q, and v indicate key, query, and value. Attn is the attention score calculated by a similarity function. In out framework, the key is the operation prototype vectors, the query is graph representations, and the value is candidate GNNs in the search space. Our architecture customization strategy calculate the attention scores of keys (operations) on of a certain query (graph) by Equation ( <ref type="formula" target="#formula_7">7</ref>), then the scores is applied on the values (operations) by Equation ( <ref type="formula">9</ref>). Different graphs can attend on different operations to use a customized neural network to learn. We empirically find this mechanism can help out-of-distribution generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we report experimental results to verify the effectiveness of our model. We also conduct detailed abla- tion studies to analyze each of our model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Setting</head><p>Datasets. We adopt both synthetic and real-world datasets for graph-level tasks with distribution shifts.</p><p>? Spurious-Motif <ref type="bibr" target="#b41">(Wu et al., 2022;</ref><ref type="bibr" target="#b48">Ying et al., 2019</ref>) is a synthetic dataset where each graph is composed of one base shape (tree, ladder, and wheel denoted by S = 0, 1, 2), and one motif shape (cycle, house, and crane denoted by C = 0, 1, 2). The base shape is usually larger than the motif shape but the ground-truth label is determined by the motif shape solely. In the distribution shift setting, a manual bias b is added to the distribution between the base and the motif shape in the training set:</p><formula xml:id="formula_13">P (S) = b if S = C, 1-b 2 otherwise.<label>(14)</label></formula><p>In the test set, all base and motif shapes are independent with equal probabilities. Thus, we can control the distribution shift by varying b.</p><p>? OGBG-Mol* <ref type="bibr" target="#b12">(Hu et al., 2020;</ref><ref type="bibr" target="#b42">Wu et al., 2018</ref>) is a set of molecular property prediction datasets. Graphs represent molecules and labels are chemical properties of molecules. The datasets are split by the scaffold value, which attempts to separate molecules with different structural frameworks, providing great challenge to graph property prediction.</p><p>The statistics of all datasets are shown in Table <ref type="table" target="#tab_3">1</ref>.</p><p>Baselines. We compare our model with 10 baselines from the following two different categories.</p><p>? Manually design GNNs: we include the GNNs in our search space as our baselines, i.e., GCN, GAT, GIN, SAGE, and GraphConv. Global mean pooling is used in these GNNs to generate the graph-level representation. We also include MLP and two recent methods: ASAP <ref type="bibr" target="#b30">(Ranjan et al., 2020)</ref> and DIR <ref type="bibr" target="#b41">(Wu et al., 2022)</ref>.</p><p>? Graph Neural Architecture Search: we consider two classic NAS baselines, random search and DARTS (Liu  <ref type="bibr">, 2019)</ref>. We also consider two GraphNAS baselines, GNAS <ref type="bibr" target="#b9">(Gao et al., 2020)</ref>, an reinforcement learning based method, and PAS <ref type="bibr" target="#b40">(Wei et al., 2021)</ref>, a recent GraphNAS method specifically designed for graph classification tasks.</p><p>More experimental details including the hyper-parameter settings are provided in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Synthetic Datasets</head><p>Experimental Setting We select three different bias values b for Spurious-Motif, i.e., 0.7, 0.8, and 0.9. We run all experiments 10 times with different random seeds and report the average results with standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>We summarize the experimental results in Table <ref type="table" target="#tab_4">2</ref>. The table shows that our model outperforms all baselines in all three settings by a large margin. Specifically, we find that all GNNs perform poorly, indicating that they are easily affected by spurious correlations and cannot handle the distribution shift. Moreover, we find that NAS methods achieve slightly better results than manually designed GNNs in most cases, demonstrating the importance of automating architecture. Nevertheless, these methods also suffer from distribution shifts. In contrast, GRACES shows much better results by customizing architectures for different graphs and capturing the ground-truth predictive Moreover, for graphs with different base shapes, we show the operation probabilities p i o in expectation in Figure <ref type="figure">5</ref>. We observe that graphs with different base shapes prefer different architectures, e.g., tree-based graphs prefer GAT and MLP in the third layer, while these two operations are seldomly chosen in the other two types of graphs. The operation distributions are similar for ladder-based and wheelbased graphs in the first layer, but differ in other layers. The results revalidate our analysis above that different base shapes prefer different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Real-world Datasets</head><p>We further conduct experiments on three molecular graph classification benchmarks in OGBG-Mol*: HIV, SIDER, and BACE. We report the results in Table <ref type="table" target="#tab_5">3</ref>. The results show that our proposed GRACES model again outperforms all the baselines on the three datasets, demonstrating that our model is able to capture the complex distribution shifts in some cases. Two existing graph NAS methods, DARTS and PAS, fail to outperform manually designed GNNs on real-world graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis</head><p>To better analyze our proposed method on realworld graphs, we also plot the validation and test results of GRACES and the results of our self-supervised disentangled graph encoder on OGBG-MolHIV. The results are shown in Figure <ref type="figure">4</ref>. We can see that GRACES report better results than the encoder in both the validation and the test set, indicating that both the encoder and customized super-network are indispensable for GRACES.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we evaluate the effectiveness of each module of our framework by conducting ablation studies. We compare the following variants of our model as follows:</p><p>? GRACES-MIX: we remove architecture customization so that all operations have an equal weight p i o .</p><p>? GRACES-GCN: we replace the self-supervised disentangled graph encoder with a vanilla GCN.</p><p>? GRACES-FC: we replace the architecture selfcustomization with prototype with a fully-connected layer.</p><p>? GRACES-NO-COS: we remove the cosine distance loss.</p><p>? GRACES-DISCRETE: we let the architecture selfcustomization strategy to output discrete architectures, i.e., constraining {p i o |o ? O} to be one-hot.</p><p>We also compare with the best manually designed GNNs, denoted as "manual". We examine these variants on the synthetic dataset Spurious-Motif. The results are presented in Table <ref type="table" target="#tab_6">4</ref>. We have the following observations. Overall, our proposed full GRACES model outperforms all the variants under all three settings, demonstrating that each component of our method is indispensable to achieve satisfactory generalization performance under distribution shifts. GRACES-MIX, which simply mixes all candidate operations without architecture customization achieves slightly better performances than the manually designed GNNs in the search space generally. Since we have ensured that the number of learnable parameters in our proposed method equals to manually designed GNNs (see Section 3.6), the results indicate that mixing different message-passing layers can be beneficial. Our proposed GRACES can naturally utilize this advantage, thanks to our customized super-network.</p><p>In contrast, architecture discretization leads to poor performance, indicating that this technique, which is widely used in the NAS literature, does not suit our problem.</p><p>In addition, if we adopt a normal GCN as the encoder, the accuracy will drop severely, indicating the importance of capturing diverse graph structures using our proposed selfsupervised disentangled graph encoder. As for the architecture self-customization with prototype, either replacing it with a fully-connected layer or removing the consine distance loss causes performance degradation, demonstrating that our strategy can better customize the architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Search Time</head><p>We also measure the search time of DARTS and our proposed method and show the results in Table <ref type="table" target="#tab_7">5</ref>. Our model takes comparable running time as DARTS, indicating our model design does not bring much burden more than DARTS on architecture searching phase. The results also confirms our theoretical complexity analysis in Section 3.6, demonstrate that our model has not only high effectiveness but also high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Graph neural network</head><p>Message-passing GNNs <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b33">Veli?kovi? et al., 2018;</ref><ref type="bibr" target="#b45">Xu et al., 2019;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017)</ref> have been proposed as an effective framework for graph machine learning following the neighborhood aggregation scheme. At each layer, nodes learn representations by aggregating their neighbors' representations. After K such layers, the vector representation of each node captures both the structural and the semantic information within the k-hop neighborhood region of the node. Then, the representation of the whole graph is learned by pooling all node representations <ref type="bibr" target="#b15">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b45">Xu et al., 2019)</ref>. Disentanglement technique <ref type="bibr">(Wang et al., 2021b;</ref><ref type="bibr">2022a;</ref><ref type="bibr" target="#b4">Chen et al., 2021)</ref> is also applied on GNNs <ref type="bibr" target="#b24">(Ma et al., 2019;</ref><ref type="bibr" target="#b20">Li et al., 2021)</ref>, which helps to generate more representative features with different factors for graphs.</p><p>Distribution shifts is inevitable in real-world data <ref type="bibr" target="#b55">(Zhu et al., 2015)</ref>. However, most existing GNNs ignore the problem of out-of-distribution generalization and suffer from severe performance deterioration when there exist distribution shifts between training and test graphs <ref type="bibr" target="#b12">(Hu et al., 2020;</ref><ref type="bibr" target="#b17">Koh et al., 2021;</ref><ref type="bibr" target="#b42">Wu et al., 2018;</ref><ref type="bibr" target="#b51">Zhang et al., 2022)</ref>. Some pioneer works start to study the distribution shift problems of GNNs such as size generalization <ref type="bibr" target="#b16">(Knyazev et al., 2019;</ref><ref type="bibr" target="#b47">Yehudai et al., 2021;</ref><ref type="bibr" target="#b1">Bevilacqua et al., 2021)</ref>, node-level tasks <ref type="bibr" target="#b8">(Fan et al., 2022;</ref><ref type="bibr" target="#b54">Zhu et al., 2021)</ref>, or invariant learning <ref type="bibr" target="#b41">(Wu et al., 2022)</ref>. However, they do not consider the architecture perspective. <ref type="bibr" target="#b46">Xu et al. (2021)</ref> theoretically show that GNN architectures can affect the generalization ability, but they do not consider how to obtain the optimal architecture given a graph dataset. Another line of GNN works focus on treating different graph data instances differently. Policy-GNN <ref type="bibr" target="#b18">(Lai et al., 2020)</ref> determines the number of aggregation layers for each node using reinforcement learning. Customized-GNN <ref type="bibr">(Wang et al., 2021d)</ref> generates different GNN parameters for different graph instances. Nevertheless, these works do not explore searching GNN architecture under distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Neural architecture search</head><p>Recent years have witnessed a surge of research interests on NAS methods, which aim at designing neural architectures automatically for given tasks. Since the architecture search space is discrete, reinforcement learning <ref type="bibr" target="#b56">(Zoph &amp; Le, 2017;</ref><ref type="bibr" target="#b27">Pham et al., 2018;</ref><ref type="bibr">Qin et al., 2021a)</ref> and evolution algorithm <ref type="bibr" target="#b43">(Xie &amp; Yuille, 2017;</ref><ref type="bibr" target="#b21">Liu et al., 2018)</ref> are often used in NAS methods. Besides, another strategy is transferring the discrete architecture search space into a differentiable space, e.g., DARTS <ref type="bibr" target="#b22">(Liu et al., 2019)</ref> and SNAS <ref type="bibr" target="#b44">(Xie et al., 2019)</ref> construct a super-network where all candidate operations are mixed and make it possible to update the architecture as well as the weights simultaneously through the classical gradient descent method.</p><p>Recently, instance-aware NAS searches for a mapping function from an image to an architecture so that different instances can have different architectures in the test phase. InstaNAS <ref type="bibr" target="#b5">(Cheng et al., 2020)</ref> uses a controller to capture the representations of images and assign them to different discrete CNN architectures. However, this framework is not differentiable and hard to optimize. DDW <ref type="bibr" target="#b49">(Yuan et al., 2021)</ref> uses global spatial information of images to generate wiring patterns among different layers but does not consider operation selection. Besides, NAS-OoD <ref type="bibr" target="#b0">(Bai et al., 2021)</ref> aims to search for an architecture that can generalize to outof-distribution data. Nevertheless, all the above works focus on computer vision and cannot be easily applied to GNNs.</p><p>Automated graph machine learning is gaining an increasing number of attentions from the research community. <ref type="bibr" target="#b50">(Zhang et al., 2021;</ref><ref type="bibr">Wang et al., 2022b)</ref>, including hyper-parameter optimization on graphs <ref type="bibr" target="#b32">(Tu et al., 2019;</ref><ref type="bibr">Wang et al., 2021c)</ref> and graph neural architecture search (GraphNAS) <ref type="bibr" target="#b19">(Li et al., 2020;</ref><ref type="bibr" target="#b9">Gao et al., 2020;</ref><ref type="bibr" target="#b53">Zhou et al., 2019;</ref><ref type="bibr" target="#b10">Guan et al., 2021;</ref><ref type="bibr">Qin et al., 2021b;</ref><ref type="bibr" target="#b2">Cai et al., 2022)</ref>. Existing GraphNAS on graph classification tasks <ref type="bibr" target="#b14">(Jiang &amp; Balaprakash, 2020;</ref><ref type="bibr" target="#b26">Peng et al., 2020;</ref><ref type="bibr" target="#b40">Wei et al., 2021;</ref><ref type="bibr" target="#b52">Zhili Wang, 2021;</ref><ref type="bibr" target="#b3">Cai et al., 2021)</ref> are all based on the I.I.D. assumption and ignore the distribution shift problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel GRACES method to improve the generalization ability of GNAS under distribution shifts. Our core idea is tailoring a customized GNN architecture suitable for each graph instance with unknown distribution by designing a self-supervised disentangled graph encoder, the architecture self-customization with prototype strategy, and the customized super-network. Extensive experiments on both synthetic and real-world datasets demonstrate that our proposed model can adapt to diverse graph structures and achieve state-of-the-art performance for the graph classification task under distribution shifts. Detailed ablation studies further verify the designs of our proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notations</head><p>Table <ref type="table">6</ref>. Meanings of notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notation Meaning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head><p>The graph space Y</p><p>The label space W</p><p>The weight space A</p><p>The architecture space O</p><p>The operation search space G, Y A graph dataset and its corresponding labels g, y</p><p>A graph and its corresponding label</p><formula xml:id="formula_14">F A model mapping G ? Y ? A loss function K</formula><p>The number of chunks H (l)  The node representation at the i-th layer h</p><p>The graph representation q i o The prototype vector of operation o at the i-th layer p i o The weight of operation o at the i-th layer L sup</p><p>The supervision loss of the encoder L ssl</p><p>The self-supervision loss of the encoder L cos</p><p>The cosine distance loss of the prototype vectors L main</p><p>The supervision loss of the final prediction given by the super-network L reg</p><p>The sum of all regularizer losses ?</p><p>The hyper-parameter to control the contribution of regularizer ? 1</p><p>The hyper-parameter to control the contribution of L ssl ? 2</p><p>The hyper-parameter to control the contribution of L cos d e</p><p>The dimension of the encoder d s</p><p>The dimension of the super-network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hyper-parameter Settings</head><p>We use different learning rate for the three parts of our model. Typically, the learning rate of the self-supervised disentangled encoder is 1.5e-4. The learning rate of the architecture self-customization strategy is 1e-4. Besides, the training procedure of these two parts are consine annealing scheduled. The learning rate of the customized super-network is 2e-3. We initial ? as 0.07 and increase it to 0.5 linearly. In addition, we set ? 1 = 0.05 and ? 2 = 0.002. For our method and all baselines, we set the number of layers as 2 in OGBG-MolHIV and 3 in the other datasets, For all methods, we use edge features in OGBG datasets and virtual node mechanism in OGBG-MolHIV and OGBG-MolSIDER. For all methods, we fix the first layer as GIN in Spurious-Motif since some of them cannot deal with constant node features. The hidden dimension in Spurious-Motif is 64 for all baselines and 26 for our method. In OGBG-Mol* datasets, the hidden dimension is 300 for baselines and 128 for our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Denote the graph space as G and the label space as Y. We consider a training graph dataset G tr = {g i } Ntr i=1 , g i ? G and the corresponding label set Y tr = {y i } Ntr i=1 , y i ? Y. The test graph dataset is denoted as G te = {g i } Nte i=1 and Y te = {y i } Nte i=1 . The goal of generalization under distribution shifts is to design a model F : G ? Y using G tr and Y tr which works well on G te and Y te under the assumption that P (G tr , Y tr ) ? = P (G te , Y te ), i.e., arg min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .Figure 4 .Figure 5 .</head><label>245</label><figDesc>Figure 2. The validation and test accuracy of GRACES and self-supervised disentangled graph encoder (SDGE) on Spurious-Motif.</figDesc><graphic url="image-1.png" coords="7,78.75,256.62,124.93,62.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Self-supervised Disentangled Graph Encoder Architecture Customization with Prototype Customized Super- Network</head><label></label><figDesc>An overview of our proposed GRACES model. The self-supervised disentangled graph encoder captures diverse graph structures by a self-supervised and a supervised loss. Then, the architecture self-customization with prototype module tailors the most suitable GNN architecture based on the learned graph representation. Finally, the customized super-network enables efficient training by weight sharing.</figDesc><table><row><cell>? ???</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Self-supervised task</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 1</cell><cell>? 1</cell></row><row><cell></cell><cell></cell><cell>? ?</cell><cell></cell><cell></cell><cell>? 2</cell><cell>? 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? 3</cell><cell>? 3</cell></row><row><cell></cell><cell></cell><cell>? ?</cell><cell></cell><cell></cell><cell>? 1</cell><cell>? 1</cell></row><row><cell></cell><cell>? ?</cell><cell>? ???</cell><cell>? ?</cell><cell></cell><cell>? 2 ? 3</cell><cell>? 2 ? 3</cell><cell>? ????</cell></row><row><cell>Disentangled factors</cell><cell>? ?</cell><cell></cell><cell></cell><cell></cell><cell>? 1 ? 2</cell><cell>? 1 ? 2</cell></row><row><cell></cell><cell cols="2">Prototype vector</cell><cell></cell><cell></cell><cell>? 3</cell><cell>? 3</cell></row><row><cell>Supervised task ? ???</cell><cell cols="3">? ? = softmax(? ? Graph Representation ? ? 2 ? ?</cell><cell>)</cell><cell>? ? =</cell><cell>???</cell><cell>? ? ?(?)</cell></row><row><cell></cell><cell cols="6">?(?? ???? + (1 -?)(? ??? + ? 1 ? ??? + ? 2 ? ??? ))</cell></row><row><cell>Figure 1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 1 The Algorithm Framework of Our Proposed Method Input: Training Dataset G tr and Y tr , Hyper-parameters ? 0 , ??, ? 1 , ? 2 Initialize all leanable parameters and set ? = ? 0 while Not Converge do Calculate graph representations h using Eqs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Time complexity. The time complexity of most messagepassing GNNs is O(|E|d+|V |d 2 ). Therefore, the time complexity of our self-supervised disentangled graph encoder is O(|E|d e + |V |d 2 e ). The time complexity of the architecture self-customization with prototype is O(|O| 2 d e ), since the most time-consuming step is calculating L cos in Eq (8). The time complexity of the customized super-network is O(|O|(|E|d s + |V |d 2 s )). The overall time complexity of our method is O(|E|(d e + |O|d s ) + |V |(d 2 e + |O|d 2 s ) + |O| 2 d e ). Number of learnable parameters. The number of learnable parameters of a typical message-passing GNN is O(d 2 ). In our framework, the self-supervised disentangled graph encoder has O(d 2 e ) parameters, the architecture self-customization module has O(|O|d e ) parameters, and the customized super-network has O(|O|d 2 s ) parameters. Thus the total number of learnable parameters is O(d 2 e + |O|d e + |O|d 2 s</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Dataset Statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="2">#Graphs Split(%)</cell><cell cols="5">Avg. |V | Avg. |E| #Tasks #Classes Metric</cell></row><row><cell>Spurious-Motif</cell><cell>18,000</cell><cell cols="2">50/16.7/33.3 26.1</cell><cell>36.3</cell><cell>1</cell><cell>3</cell><cell>Accuracy</cell></row><row><cell>OGBG-MolHIV</cell><cell>41,127</cell><cell>80/10/10</cell><cell>25.5</cell><cell>27.5</cell><cell>1</cell><cell>2</cell><cell>ROC-AUC</cell></row><row><cell cols="2">OGBG-MolSIDER 1,427</cell><cell>80/10/10</cell><cell>33.6</cell><cell>35.4</cell><cell>27</cell><cell>2</cell><cell>ROC-AUC</cell></row><row><cell>OGBG-MolBACE</cell><cell>1,513</cell><cell>80/10/10</cell><cell>34.1</cell><cell>36.9</cell><cell>1</cell><cell>2</cell><cell>ROC-AUC</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>The test accuracy of all the methods on the synthetic dataset Spurious-Motif. Numbers after the ? signs represent standard deviations. The best results are in bold.</figDesc><table><row><cell>bias</cell><cell>b = 0.7</cell><cell>b = 0.8</cell><cell>b = 0.9</cell></row><row><cell>GCN</cell><cell>48.39?1.69</cell><cell>41.55?3.88</cell><cell>39.13?1.76</cell></row><row><cell>GAT</cell><cell>50.75?4.89</cell><cell>42.48?2.46</cell><cell>40.10?5.19</cell></row><row><cell>GIN</cell><cell>36.83?5.49</cell><cell>34.83?3.10</cell><cell>37.45?3.59</cell></row><row><cell>SAGE</cell><cell>46.66?2.51</cell><cell>44.50?5.79</cell><cell>44.79?4.83</cell></row><row><cell cols="2">GraphConv 47.29?1.95</cell><cell>44.67?5.88</cell><cell>44.82?4.84</cell></row><row><cell>MLP</cell><cell>48.27?1.27</cell><cell>46.73?3.48</cell><cell>46.41?2.34</cell></row><row><cell>ASAP</cell><cell>54.07?13.85</cell><cell>48.32?12.72</cell><cell>43.52?8.41</cell></row><row><cell>DIR</cell><cell>50.08?3.46</cell><cell>48.22?6.27</cell><cell>43.11?5.43</cell></row><row><cell>random</cell><cell>45.92?4.29</cell><cell>51.72?5.38</cell><cell>45.89?5.09</cell></row><row><cell>DARTS</cell><cell>50.63?8.90</cell><cell>45.41?7.71</cell><cell>44.44?4.42</cell></row><row><cell>GNAS</cell><cell>55.18?18.62</cell><cell>51.64?19.22</cell><cell>37.56?5.43</cell></row><row><cell>PAS</cell><cell>52.15?4.35</cell><cell>43.12?5.95</cell><cell>39.84?1.67</cell></row><row><cell>GRACES</cell><cell cols="3">65.72?17.47 59.57?17.37 50.94?8.14</cell></row><row><cell>et al.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>The test ROC-AUC of all the methods on the real-world datasets OGBG-Mol*. Numbers after the ? signs represent standard deviations. The best results are in bold.</figDesc><table><row><cell>dataset</cell><cell>hiv</cell><cell>sider</cell><cell>bace</cell></row><row><cell>GCN</cell><cell>75.99?1.19</cell><cell>59.84?1.54</cell><cell>68.93?6.95</cell></row><row><cell>GAT</cell><cell>76.80?0.58</cell><cell>57.40?2.01</cell><cell>75.34?2.36</cell></row><row><cell>GIN</cell><cell>77.07?1.49</cell><cell>57.57?1.56</cell><cell>73.46?5.24</cell></row><row><cell>SAGE</cell><cell>75.58?1.40</cell><cell>56.36?1.32</cell><cell>74.85?2.74</cell></row><row><cell cols="2">GraphConv 74.46?0.86</cell><cell>56.09?1.06</cell><cell>78.87?1.74</cell></row><row><cell>MLP</cell><cell>70.88?0.83</cell><cell>58.16?1.41</cell><cell>71.60?2.30</cell></row><row><cell>ASAP</cell><cell>73.81?1.17</cell><cell>55.77?1.18</cell><cell>71.55?2.74</cell></row><row><cell>DIR</cell><cell>77.05?0.57</cell><cell>57.34?0.36</cell><cell>76.03?2.20</cell></row><row><cell>DARTS</cell><cell>74.04?1.75</cell><cell>60.64?1.37</cell><cell>76.71?1.83</cell></row><row><cell>PAS</cell><cell>71.19?2.28</cell><cell>59.31?1.48</cell><cell>76.59?1.87</cell></row><row><cell>GRACES</cell><cell cols="3">77.31?1.00 61.85?2.56 79.46?3.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>The test accuracy of different variants of GRACES on the synthetic dataset Spurious-Motif.</figDesc><table><row><cell>bias</cell><cell>b = 0.7</cell><cell>b = 0.8</cell><cell>b = 0.9</cell></row><row><cell>manual</cell><cell>50.75?4.89</cell><cell>46.73?3.48</cell><cell>46.41?2.34</cell></row><row><cell>MIX</cell><cell>52.80?10.57</cell><cell>49.24?5.42</cell><cell>43.11?1.86</cell></row><row><cell>GCN</cell><cell>58.86?13.39</cell><cell>51.62?13.23</cell><cell>47.85?9.57</cell></row><row><cell>FC</cell><cell>63.76?14.68</cell><cell>57.11?20.38</cell><cell>47.39?10.28</cell></row><row><cell>NO-COS</cell><cell>55.80?18.13</cell><cell>51.65?14.12</cell><cell>44.95?9.41</cell></row><row><cell cols="2">DISCRETE 42.84?7.84</cell><cell>39.81?6.62</cell><cell>40.80?5.74</cell></row><row><cell>GRACES</cell><cell cols="3">65.72?17.47 59.57?17.37 50.94?8.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Empirical search time on Spurious-Motif and OGBG-Mol* (NVIDIA GeForce RTX 3090).</figDesc><table><row><cell>DATASET</cell><cell>SPURIOUS-MOTIF</cell><cell>HIV</cell><cell>SIDER</cell><cell>BACE</cell></row><row><cell>GRACES</cell><cell cols="2">972S 2325S</cell><cell>115S</cell><cell>114S</cell></row><row><cell>DARTS</cell><cell cols="2">872S 2049S</cell><cell>111S</cell><cell>108S</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is supported by the <rs type="funder">National Key Research and Development Program of China</rs> No. 2020AAA0106300National <rs type="funder">Natural Science Foundation of China</rs> No. <rs type="grantNumber">62102222</rs> and partially funded by <rs type="funder">THU-Bosch</rs> JCML center.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6ARwPfk">
					<idno type="grant-number">62102222</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nas-ood: Neural architecture search for outof-distribution generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><forename type="middle">G</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8320" to="8329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Size-invariant graph representations for graph classification extrapolations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal continual graph learning with neural architecture search</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1292" to="1300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6657" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Curriculum disentangled recommendation with noisy multi-feedback</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instanas: Instance-aware neural architecture search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">G</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding and exploring the network with stochastic architectures</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Debiased graph neural networks with agnostic label selection bias</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated attention representation search</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Autoattend</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery -a focus on affinity prediction problems with noise annotations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><surname>Drugood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09637</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph neural network architecture search for molecular property prediction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balaprakash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Policy-gnn: Aggregation optimization for graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="461" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Disentangled contrastive learning on graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph self-supervised learning: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00111</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning graph convolutional network for skeleton-based human action recognition by neural searching</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2669" to="2676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph q network for neural architecture search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Gqnas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph differentiable architecture search with structure learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ASAP: adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards out-of-distribution generalization: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Autone: Hyperparameter optimization for massive network embedding</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;19: The 25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains: A survey on domain generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Survey Track</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multimodal disentangled representation for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Explainable automated graph representation learning with hyperparameter importance</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Automated graph machine learning: Approaches, libraries and directions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.01288</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Customized graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;21: The 30th ACM International Conference on Information and Knowledge Management</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><surname>Moleculenet</surname></persName>
		</author>
		<title level="m">A benchmark for molecular machine learning. Chemical sciense</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Genetic cnn</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Snas: stochastic neural architecture search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">From local structures to size generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A tool for post-hoc explanation of graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><surname>Explainer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentiable dynamic wirings for neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Automated machine learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Survey track</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to solve travelling salesman problem with hardness-adaptive curriculum</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Autogel: An automated graph neural network with explicit link information</title>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Auto-gnn: Neural architecture search of graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shiftrobust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimedia big data computing</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="96" to="99" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
