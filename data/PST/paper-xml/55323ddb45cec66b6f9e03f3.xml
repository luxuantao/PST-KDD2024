<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Superpixel-Based Hand Gesture Recognition With Kinect Depth Camera</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chong</forename><surname>Wang</surname></persName>
							<email>cwang@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EEE</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Pokfulam, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhong</forename><surname>Liu</surname></persName>
							<email>liuzhong@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EEE</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Pokfulam, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shing-Chow</forename><surname>Chan</surname></persName>
							<email>scchan@eee.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EEE</orgName>
								<orgName type="institution">The University of Hong Kong</orgName>
								<address>
									<settlement>Pokfulam, Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Superpixel-Based Hand Gesture Recognition With Kinect Depth Camera</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5BE3E6E9C10D82C7496F61DDBABDDBE9</idno>
					<idno type="DOI">10.1109/TMM.2014.2374357</idno>
					<note type="submission">received July 15, 2014; revised November 10, 2014; accepted November 12, 2014. Date of publication November 24, 2014; date of current version December 23, 2014.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Hand gesture recognition</term>
					<term>human-computer interaction</term>
					<term>Kinect</term>
					<term>superpixel earth mover&apos;s distance</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a new superpixel-based hand gesture recognition system based on a novel superpixel earth mover's distance metric, together with Kinect depth camera. The depth and skeleton information from Kinect are effectively utilized to produce markerless hand extraction. The hand shapes, corresponding textures and depths are represented in the form of superpixels, which effectively retain the overall shapes and color of the gestures to be recognized. Based on this representation, a novel distance metric, superpixel earth mover's distance (SP-EMD), is proposed to measure the dissimilarity between the hand gestures. This measurement is not only robust to distortion and articulation, but also invariant to scaling, translation and rotation with proper preprocessing. The effectiveness of the proposed distance metric and recognition algorithm are illustrated by extensive experiments with our own gesture dataset as well as two other public datasets. Simulation results show that the proposed system is able to achieve high mean accuracy and fast recognition speed. Its superiority is further demonstrated by comparisons with other conventional techniques and two real-life applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H AND GESTURE recognition has received great attention due to its potential applications in contactless humancomputer interaction (HCI). There is considerable progress in this area and a number of algorithms addressing different aspects of the problem have been proposed <ref type="bibr" target="#b0">[1]</ref>. While imagebased techniques have been widely studied, it may be affected by lighting conditions, large variations of the hand gesture and textures, etc. In particular, reliable hand detection is essential to gesture recognition and many hand detection techniques have been developed for tracking and recognizing various hand gestures <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Some of them require users to wear an electronic glove so that the key features of hand can be accurately measured but the device is somewhat costly and inconvenient for domestic applications <ref type="bibr" target="#b3">[4]</ref>. Another class of methods employs optical markers to replace electronic gloves but it requires rather complex configuration <ref type="bibr" target="#b4">[5]</ref>. Methods based on skin color model <ref type="bibr" target="#b5">[6]</ref> and hand shape model <ref type="bibr" target="#b6">[7]</ref> have also been proposed. However, they are not robust in the dynamic environment and rely significantly on the models.</p><p>Once the hands have been located, hand gesture recognition aims to interpret predetermined gestures into independent sign, mostly static, which can be classified by pattern classifiers such as k-Nearest Neighbors (kNN) <ref type="bibr" target="#b7">[8]</ref>, Hidden Markov Models <ref type="bibr" target="#b8">[9]</ref>, Principal Component Analysis (PCA) <ref type="bibr" target="#b9">[10]</ref> and Support Vector Machine (SVM) <ref type="bibr" target="#b10">[11]</ref>. Another effective gesture recognition algorithm <ref type="bibr" target="#b11">[12]</ref> proposed recently is based on the Finger Earth Mover's Distance (FEMD) and Template Matching Method (TMM), which shows promising performance.</p><p>In this work, we propose a superpixel-based hand gesture recognition system to be used with Kinect depth camera. Because of the good recognition rate of segmented hand gesture, our system adopts segmentation based methods for hand detection. As current image-based techniques <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref> rely heavily on the prior knowledge of hand models, it is usually problematic to separate the user's hand from cluttered backgrounds in practical applications, especially when they have similar color and textures such as the face. To obtain improved segmentation, the depth information from Kinect depth camera can be utilized to better locate and extract hand segments <ref type="bibr" target="#b12">[13]</ref>. Moreover, Kinect can capture the color image and depth map at 30 FPS with resolution <ref type="bibr" target="#b13">[14]</ref> which is sufficient for our purpose. To reliably acquire the hand gesture images, we make use of both skeleton tracking and the depth map from Kinect, which help to locate the user's hands and extract their shapes, respectively. Experimental results show that the human posture tracking can greatly facilitate gesture recognition.</p><p>For efficient hand gesture recognition, a superpixel-based representation of the hand shape, inspired by the widely used concept of superpixel <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, is introduced in this work, from which a new distance metric, Superpixel Earth Mover's Distance (SP-EMD), is proposed to measure the dissimilarity between the hand gestures. Moreover, a novel concept of virtual superpixels is proposed to represent folded fingers which addressed the issue of partial matching. The proposed SP-EMD naturally incorporates both the color texture and depth map into the hand shapes as well. As a result, it leads to a highly accurate and efficient hand gesture recognition system, without the requirement of specific markers. Finally, the proposed system has moderate complexity and can be implemented in real-time. Fig. <ref type="figure" target="#fig_0">1</ref> summarized the major steps in the proposed hand gesture recognition framework based on the SP-EMD and Kinect. The rest of the paper is organized as follows. Related works are first reviewed in Section II. The proposed methods for hand localization, segmentation, superpixel-based representation and preprocessing are described in Section III. The novel distance metric, SP-EMD, for hand gesture classification is then proposed in Section IV. The performance of the SP-EMD-based hand gesture recognition is evaluated and compared with other state-of-the-art algorithms in Section V. We also build two reallife HCI applications to further illustrate the effectiveness of our system in Section VI. Finally, we conclude the paper in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Many vision based hand gesture recognition algorithms have been proposed in the past years and comprehensive reviews can be found in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. The recent development of depth cameras, such as Microsoft Kinect, Creative Senz3D or Mesa Swiss-Ranger etc., opens up new avenues for hand gesture recognition, thanks to the extra depth information. Therefore, how the depth information can be efficiently utilized and how the depth camera can be incorporated in the hand gesture recognition system is an active topic of research <ref type="bibr" target="#b16">[17]</ref>.</p><p>One of the greatest advantages of using depth cameras for hand gesture recognition is in hand detection. In early studies, hand detection mainly relies on vision-based features. For instance, chromatic distribution of hand is used to build the hand model in <ref type="bibr" target="#b17">[18]</ref>. However, such method is sensitive to variations of skin colors. An appearance-based detection framework was also proposed in <ref type="bibr" target="#b18">[19]</ref>. The complicated and unpredictable hand features, however, pose great challenges to its reliability. In comparison, model-based method <ref type="bibr" target="#b19">[20]</ref> is better suited to reallife interaction, but a dark background is usually assumed so that the hand gesture can be segmented reliably.</p><p>On the other hand, depth cameras offer a much simpler but effective way to isolate the hands by using a depth threshold. This method is widely used in many researches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>. But the selection of the depth threshold is empirical and prone to errors. To better locate and track the hands, some researches <ref type="bibr" target="#b21">[22]</ref> use the body skeleton provided by Microsoft Kinect, which shows promising performance.</p><p>After the hand localization and segmentation, various hand features can be extracted from either the depth maps, e.g. Histogram of 3D Facets (H3DF) <ref type="bibr" target="#b22">[23]</ref> and 3D point distribution histogram <ref type="bibr" target="#b20">[21]</ref>, or the corresponding color images such as Histogram of Oriented Gradients (HOG) <ref type="bibr" target="#b23">[24]</ref>, which will then be used for hand gesture recognition. Alternatively, it is also quite common to use the feature of hand contour, which is, however, usually noisy and distorted due to the low resolution and accuracy of the current depth cameras. Thus, contour based algorithms <ref type="bibr" target="#b24">[25]</ref> are not robust when the contour is locally distorted, whereas the skeleton-based methods <ref type="bibr" target="#b25">[26]</ref> may have difficulties to extract correct skeleton from the noisy or distorted hand contour. Correspondence-based methods such as shape context <ref type="bibr" target="#b26">[27]</ref> and inner distance <ref type="bibr" target="#b27">[28]</ref> also suffer from ambiguity due to the orientation, distortion and articulation of the hand gestures. Recently, Finger-Earth Mover's Distance (FEMD) is proposed in <ref type="bibr" target="#b11">[12]</ref> to provide a robust way for hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. HAND DETECTION</head><p>Hand detection, including hand localization, segmentation and representation, is a non-trivial problem in gesture recognition. Previous studies reveal many challenges, such as low accuracy of predicting user's body movement and the weakness against cluttered backgrounds or various lighting conditions. In this work, we utilize the depth information and skeleton tracking provided by Kinect to address these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Calibration Between Color and Depth Camera</head><p>Although the color texture and depth map are captured by Kinect simultaneously, they are not registered accurately. Therefore, a recalibration procedure for Kinect is required in order to jointly utilize the color and depth information. In this work, Heikkila's method <ref type="bibr" target="#b28">[29]</ref> is used to recalibrate Kinect's color and infrared (IR) sensors. Since Kinect depth map is generated from the IR observations, the estimated color-IR camera parameters can be applied to register the depth map and color image. More details can be found in <ref type="bibr" target="#b13">[14]</ref>. Fig. <ref type="figure" target="#fig_1">2</ref> shows an example after the recalibration, it can be seen that the color texture and depth map are precisely registered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Hand Localization and Segmentation</head><p>In previous depth camera-based approaches <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, the hand is required to be the front-most object from the depth camera. Moreover, a black belt on the gesturing hand's wrist is also required in <ref type="bibr" target="#b11">[12]</ref>, which is rather inconvenient for real world applications. In our system, we relax these restrictions by utilizing the rather stable joints from Kinect's skeleton tracking. The Kinect joints are directly used to locate the hands, wrists and elbows. An example of the skeleton is given in Fig. <ref type="figure" target="#fig_1">2(b)</ref>, which shows that the hand location can be detected accurately. By assuming that the hand is visible to the camera without any occlusion, it allows us to quickly separate the hands from background objects using depth information alone. Using the hand joint point as the center, a pair of color texture and depth map blocks is extracted first as shown in the top two rows of Fig. <ref type="figure" target="#fig_2">3</ref>. Based on the joint points of hand, wrist and elbow, we can also estimate the orientation of the arm in order to select an appropriate depth threshold such as the depth value of the wrist joint point which is denoted by . Then the hand shape is segmented quickly by simply comparing the depth values as shown in the last row of Fig. <ref type="figure" target="#fig_2">3</ref>. It can be seen that the hand shapes from different persons are correctly segmented, even when the hands are cluttered by the face or background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Shape Representation Using Joint Color-Depth Superpixel</head><p>Instead of representing the hand shape in contour <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b26">[27]</ref> or skeleton <ref type="bibr" target="#b25">[26]</ref>, we propose to use superpixels to simplify the hand shape but retain as much information as possible. Therefore, not only the 2D shape but also the corresponding texture and depth can be jointly utilized in hand gesture recognition. This representation is a key ingredient of the proposed SP-EMD recognition system, which will be introduced later in Section IV.</p><p>Although various kinds of superpixel algorithms have been proposed, compact and efficient representations are preferred in the proposed superpixel-based recognition system for real-time implementation. We adopt and modify the Simple Linear Iterative Clustering (SLIC) algorithm <ref type="bibr" target="#b14">[15]</ref> in our system, because it can enforce the compactness of superpixels. In this work, the clustering is performed in a 6-D space including the ( ) values of the CIELAB color space and the ( ) pixel coordinates, where is the depth value at the pixel location (</p><p>). Assume an image with pixels is segmented into superpixels. Then each superpixel should have nearly equal size of pixels. Let be the centroid of the -th cluster, where and . The following metric can be defined to measure the pixel-to-pixel distance (1) where is the compactness coefficient of superpixels, and are respectively the distances in the ( ) and ( ) spaces, and denotes the Euclidean norm. Thus is a weighted sum of the lab distance and xyd distance, controlled by the compactness coefficient By initializing cluster centers at regular grid steps, -Means clustering is then used based on to generate the superpixels iteratively. Utilizing this modified SLIC, the corresponding hand shape is segmented into small clusters, i.e. superpixels. Some examples are given in Fig. <ref type="figure" target="#fig_3">4</ref>. It shows that the hand shapes are successfully segmented into a small group of superpixels, which are denoted in different colors. The hand shape can thus be represented by the properties of those superpixels, such as the 2D centroids (black dots in Fig. <ref type="figure" target="#fig_3">4</ref>), the color distribution and the mean depth values.</p><p>It can be seen that these joint color-depth superpixels are of similar size and regular shapes, but containing edge information from color textures as well as depth maps. This superpixel-based representation tries to group similar pixels together with reduced variables by minimizing the information loss. In other words, the key features, including the color, depth and shape, of the hands are well preserved after dimension reduction. Another merit of this representation is that it is robust to distortions of the hand contours, since the centroids of the superpixels are not determined by the contour alone. This superpixel representation will be used in the next section to define the proposed SP-EMD for hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Preprocessing for Scaling, Translation, In-Plane, and Out-of-Plane Rotations</head><p>In practical applications, the extracted hand gestures usually have different scales due to various distances from the camera to hand, or different rotations caused by the body postures. Moreover, different people's hands always have distinct characteristics even for the same gesture. Hence it is necessary to perform some preprocessing to normalize and align the proposed superpixel shape representation before recognition.</p><p>First of all, the hand images are scaled by a scaling factor , where is the minimum depth value on the hand and is a reference depth value. Considering the hand size and the range of Kinect, is set as 1 meter for all the experiments in this paper. However, the palm size varies from one person to another, which affects the recognition between different subjects. To address this issue, the 2D pixel coordinates ( ) are translated and normalized according to the center and radius of the maximum inscribed circle, i.e. the palm, of the obtained hand, respectively.</p><p>To address the in-plane rotation caused by the body posture, the angle determined by the joint points of wrist and elbow is used to rotate the hands to a similar orientation. To further refine in-plane rotation, 2D Iterative Closest Point (ICP) <ref type="bibr" target="#b29">[30]</ref> is adopted to align the superpixels of two given gestures before recognition. In our system, the initial rotation matrix and translation vector are simply set as an identical matrix and [0,0], respectively. Comparing with the thin plate spline (TPS) model used in Shape Context <ref type="bibr" target="#b26">[27]</ref>, ICP has lower complexity and thus is more suitable for real-time implementation. Moreover, the number of the superpixels is small, typically from 20 to 80, which means ICP will be very efficient in our system.</p><p>Meanwhile, moderate out-of-plane rotation of the hand is compensated with the aid of the depth map. More precisely, the palm plane is first estimated from the 3D point cloud of the hand, which is then rotated to ensure that the palm plane is parallel to the image plane. Finally, the rotated point cloud is projected back to image plane, which is then utilized for the later steps of superpixel generation and hand gesture recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HAND GESTURE RECOGNITION</head><p>The Earth Mover's Distance (EMD) <ref type="bibr" target="#b30">[31]</ref> is a measure of the distance between two probability distributions over a region. It is widely used in image retrieval and pattern recognition. Inspired by the work of Finger-Earth Mover's Distance (FEMD) <ref type="bibr" target="#b11">[12]</ref>, we now propose a novel distance metric, Superpixel Earth Mover's Distance (SP-EMD), to recognize the hand gesture. The proposed SP-EMD jointly measures the distance between two hand gestures based on the shape, texture and depth information, while FEMD only uses the contour.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transportation Problem in EMD</head><p>The original EMD is based on a solution to the conventional transportation problem. Given a set of suppliers , a set of consumers , and the cost to ship a unit of supply from to , the aim is to find an optimal set F of flow , i.e. the amount of supply shipped from -th supplier to -th consumer, to minimize the overall cost, <ref type="bibr" target="#b1">(2)</ref> subject to the constraints (3) (4) <ref type="bibr" target="#b4">(5)</ref> where is the supply of -th supplier and is the capacity of -th consumer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Superpixel Earth Mover's Distance</head><p>For hand gesture recognition, we can use this transportation problem to mimic the motion of fingers between different gestures by defining the suppliers/consumers from the hand shapes. If the fingers can be segmented correctly, these segments can be defined as the suppliers/consumers as shown in Fig. <ref type="figure" target="#fig_4">5(a)</ref>. Two hand shapes are decomposed to two finger parts plus the palm and a single finger plus the palm, respectively. It is formed as the transportation problem with three suppliers (black nodes) and two consumers (white nodes). The finger features are used to determine the cost , and the area of the finger is intuitively the amount of supply (or capacity). Actually, it is a general form of FEMD, while FEMD discards the palm and uses the angle intervals of the fingers to define the cost.</p><p>However, there are some major limitations. First of all, accurate finger extraction is challenging. The spatial resolution of the shape also becomes lower, since the whole finger is considered as one cluster. Moreover, the texture is neglected, which will potentially lose rich details. Therefore, we propose a novel joint color-depth superpixel earth mover's distance (SP-EMD) to address these issues. An example is shown in Fig. <ref type="figure" target="#fig_4">5(b)</ref>, where the hand shapes, color textures and depths are represented in superpixels. Each superpixel is either a supplier (black nodes) or a consumer (white nodes). Its 2D location (</p><p>) and depth value of or are then used to calculate the cost . The number of pixels (or ) within (or ) is the amount of supply (or capacity). It should be noticed that the total supply, , and capacity, , may be not equal due to the different shapes of hand gestures. It will lead to partial matches, which results in false matching in gesture recognition. Thus a novel virtual superpixel (consumer) is proposed here to receive exceeded supplies as the dotted circle indicated in Fig. <ref type="figure" target="#fig_4">5(b)</ref>. This virtual consumer will be located at the palm center. It is worth noting that it is not just a dummy, but representing the folded fingers, which naturally solves the partial matching problem.</p><p>For simplicity, the suppliers and consumers are denoted as two signatures. Each one is defined as a set of superpixels with corresponding weight . Formally, let be the first hand signature with superpixels, and be the second hand signature with clusters. The centroid and the average depth value are used to define the superpixel . Comparing with the texture, the depth is insensitive to illumination changes. The number of pixels within the superpixel is used to denote the cluster weigh . Then the cost from superpixel to is defined as the weighted 3D distance <ref type="bibr" target="#b5">(6)</ref> where is the depth weight that balances the significance between the 2D shape and depth, and is a nonlinear fingertip coefficient to give more penalties on fingertips, considering the fact that fingertips have more impact on the gestures than the palm does. A typical choice of and is 1.0 and 2.0, respectively. Detailed discussion about the parameter sensitivity can be found in Section V-C.</p><p>Given two signatures and , their SP-EMD distance is the least work needed to move the pixels between two sets of superpixels. As mentioned above, the virtual superpixels and are created with the weights</p><formula xml:id="formula_0">(7)<label>(8)</label></formula><p>where and are respectively the total weights, i.e. total number of pixels, of signatures and . Then the SP-EMD shares a similar formulation of conventional EMD as <ref type="bibr" target="#b8">(9)</ref> where SPEMD(.) denotes the SP-EMD distance function and is the flow from superpixel to , which is calculated by minimizing (2) subject to the constrains <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref>. For the proposed SP-EMD, it can be rewritten as <ref type="bibr" target="#b9">(10)</ref> where is the matrix form of the flow . The first constraint only allows moving flow along one direction. Fig. <ref type="figure" target="#fig_5">6</ref> shows two cases of the moving flow calculated between similar hand gestures from different subjects and different gestures from the same subject. It can be seen that the flow is short and organized between the similar gestures in Fig. <ref type="figure" target="#fig_5">6(d)</ref>, while it turns to be long and disordered between different ones in Fig. <ref type="figure" target="#fig_5">6(e)</ref>. It is noted that the length of the flow is partially proportional to the moving cost. Hence it is obvious that the flow in Fig. <ref type="figure" target="#fig_5">6</ref>(e) will lead to a larger SP-EMD distance, which shows the nature of the proposed distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Template Matching</head><p>Template matching is utilized for hand gesture recognition based on the proposed SP-EMD. In particular, the input hand gesture is recognized as a certain class, namely , with the minimum dissimilarity distance as <ref type="bibr" target="#b10">(11)</ref> where is the input gesture and is the template of class . It is obvious that the selection of templates will significantly affect the recognition rate.</p><p>For a template based approach, the performance is closely related to the selected templates, i.e. training data. In our experiments, leave--out (L O) cross-validation (CV) is conducted  to evaluate the recognition performance. For a dataset with subjects, -subjects are used for training and the remaining for testing in L O CV. This process is repeated for every combination of subjects so that the average accuracy can be computed. In our dataset, two values of (1 and 4) are considered, which are respectively referred to as leave-one-out CV (LOO CV) and leave-4-out CV (L4O CV). Experiments based on these two CVs are presented in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL EVALUATIONS</head><p>We now evaluate and compare the proposed hand gesture recognition system with various state-of-the-art recognition algorithms including Shape Context <ref type="bibr" target="#b26">[27]</ref>, Skeleton Matching <ref type="bibr" target="#b25">[26]</ref>, FEMD <ref type="bibr" target="#b11">[12]</ref>, Random Forest (RF) <ref type="bibr" target="#b31">[32]</ref>, HOG <ref type="bibr" target="#b23">[24]</ref> and H3DF <ref type="bibr" target="#b22">[23]</ref>, using three different real world datasets, namely our joint color-depth hand gesture dataset, NTU hand digit dataset <ref type="bibr" target="#b11">[12]</ref> and American Sign Language (ASL) finger spelling dataset <ref type="bibr" target="#b31">[32]</ref>. Two different CV schemes are tested to show the effectiveness of the proposed hand representation and SP-EMD distance metric. The confusing cases and system sensitivity will also be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>A joint color-depth hand gesture dataset (available in our project homepage 1 ) is collected using Kinect. It contains 10 gestures with 20 different poses from 5 subjects. Therefore, there are a total of 1,000 cases for testing, each of which consists of a pair of color texture and depth map with corresponding skeleton information used in our experiment. Gesture samples are shown in Fig. <ref type="figure" target="#fig_6">7</ref>, which are labeled from 0 to 9. It should be noted that this dataset is a challenging real-life dataset, which 1 Project homepage of SP-EMD, https://sites.google.com/site/spemdkinect. is collected in two different rooms with different illumination conditions using different Kinects. Moreover, the hand motion is not very restrictive including large in-plane rotation and moderate out-of-plane rotation. In the experiments, the hand shapes are extracted and preprocessed using the methods described in Sections III-B and III-D. The evaluation strategies LOO CV and L4O CV mentioned in Section IV-C are applied in order to give a comprehensive test of the proposed hand gesture recognition system.</p><p>We also evaluate our algorithm using two public Kinect gesture datasets, namely NTU Hand Digit Dataset <ref type="bibr" target="#b11">[12]</ref> and ASL Finger Spelling Dataset <ref type="bibr" target="#b31">[32]</ref>. The NTU hand digit dataset <ref type="bibr" target="#b11">[12]</ref> contains 1,000 cases of 10 hand gestures from 10 subjects. The ASL Finger Spelling dataset <ref type="bibr" target="#b31">[32]</ref> captures about 65,000 samples of 24 hand gestures (English letters from to except ) from 5 subjects. The hands are located and segmented using the hand-wrist belt and depth thresholding, respectively. For fair comparison with the reported accuracy in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b31">[32]</ref>, only LOO CV is applied to these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Evaluation</head><p>All experiments were done on an Intel Core i7-920 2.66 GHz CPU with 6 GB of RAM. Now we evaluate the performance of the proposed system from mean accuracy, time efficiency and comparisons with other methods.</p><p>Mean Accuracy: In all the experiments, the depth weight and fingertip coefficient are fixed as 1.0 and 2.0, respectively. The average size of superpixels is set as 81, i.e. roughly equal to a size of . The confusion matrices for L4O CV and LOO CV on our dataset are shown in Fig. <ref type="figure" target="#fig_7">8</ref>. We can see that the most confusing case is between gestures 7 and 9, and 7 and 6. Two examples of confusing cases are given in Fig. <ref type="figure" target="#fig_8">9</ref>. It shows  that the hand shapes become very similar due to the distortion or the habits of different people in performing the gestures. As presented in Table <ref type="table" target="#tab_0">I</ref>, the mean accuracies are 97.2% and 99.1% for L4O CV and LOO CV, respectively. It can be seen that LOO CV achieves better recognition rates than L4O CV because the number of training data (or templates) in the former is 4 times larger. Also, we note that our recognition rate is at least 5% higher than other algorithms for the L4O CV considered in Table <ref type="table" target="#tab_0">I</ref>. In other words, the performance of our hand gesture recognition system is relatively insensitive to the number of training data used, which is very convenient in real-life applications.</p><p>To illustrate the significance of the joint color-depth information from the texture and depth map, we perform another experiment that only the shape information is considered. To be specific, the color distance , depth and depth weight are all set to 0 for the generation of superpixels in (1) and calculation of the cost in <ref type="bibr" target="#b5">(6)</ref>. As shown in Table <ref type="table" target="#tab_0">I</ref>, the mean accuracies are slightly degraded to 96.500% for L4O CV and 98.3% for LOO CV. This suggests that the proposed SP-EMD is very effective even without the texture and depth map, while the additional joint color-depth information do help to improve the recognition rate.</p><p>Apart from our dataset, we also applied the proposed system to two other public datasets, NTU hand digit dataset <ref type="bibr" target="#b11">[12]</ref> and ASL finger spelling dataset <ref type="bibr" target="#b31">[32]</ref>, for which mean accuracies (99.6%  <ref type="figure" target="#fig_0">10</ref>. The confusion matrices of hand gesture recognition (LOO CV) using SP-EMD on (a) NTU hand digit dataset <ref type="bibr" target="#b11">[12]</ref> and (b) ASL finger spelling dataset <ref type="bibr" target="#b31">[32]</ref> (unit: %). and 75.8%) are respectively summarized in Tables <ref type="table" target="#tab_1">II</ref> and<ref type="table" target="#tab_1">III</ref>. The confusion matrices on these two datasets are given in Fig. <ref type="figure" target="#fig_0">10</ref>.</p><p>Time Efficiency: Table <ref type="table" target="#tab_0">I</ref> gives the average running time (0.067 seconds) for the recognition process with our implementation. The whole process includes generating superpixels, extracting features and calculating SP-EMD between the input gesture and 10 templates. Thanks to the superpixel-based representation, the ICP is very efficient (around 1 millisecond) in the proposed system. The most time consuming step is superpixel generation, which costs about 27 milliseconds. That means computing SP-EMD once only needs less than 4 milliseconds, which is very quick. As a result, the proposed SP-EMD is not only accurate but also capable of running in real-time. Fig. <ref type="figure" target="#fig_0">11</ref>. The confusion matrices of hand gesture recognition using (a) FEMD <ref type="bibr" target="#b11">[12]</ref>, (b) shape context <ref type="bibr" target="#b26">[27]</ref>, and (c) skeleton matching <ref type="bibr" target="#b25">[26]</ref> with DSE <ref type="bibr" target="#b33">[34]</ref> (unit: %). The upper and lower rows are the results of L4O CV and LOO CV, respectively. Fig. <ref type="figure" target="#fig_1">12</ref>. Confusing cases of (a) FEMD <ref type="bibr" target="#b11">[12]</ref>, (b) shape context <ref type="bibr" target="#b26">[27]</ref>, and (c) skeleton matching <ref type="bibr" target="#b25">[26]</ref> with DSE <ref type="bibr" target="#b33">[34]</ref>.</p><p>Comparisons With Other Methods: To further illustrate the advantage of our system, we first compare it with other three state-of-the-art recognition algorithms, Shape Context <ref type="bibr" target="#b26">[27]</ref>, Skeleton Matching <ref type="bibr" target="#b25">[26]</ref> and FEMD <ref type="bibr" target="#b11">[12]</ref> on our dataset. Their mean accuracies and running time are shown in Table <ref type="table" target="#tab_0">I</ref>. The hand shapes are segmented and preprocessed using the same method described in Sections III-B and III-D. It can be seen that the proposed hand recognition system achieves the highest mean accuracy.</p><p>It is worth noting that FEMD is particularly designed for depth-camera based hand gesture recognition. Two different finger decomposition methods are proposed for FEMD in <ref type="bibr" target="#b11">[12]</ref>, and the reported mean accuracies are very close. In particular, we apply the thresholding decomposition based method to our dataset. Note that the lower left corner of the hand shape is chosen as the initial point of the contour in our dataset, instead of the wrist belt used in <ref type="bibr" target="#b11">[12]</ref>. The confusion matrices of FEMD for L4O CV and LOO CV on our dataset are shown in Fig. <ref type="figure" target="#fig_0">11(a)</ref>. It can be seen that the most confusing cases are between gestures 4 and 5, and 1 and 7. The main reason is that the fingers in those gestures are not correctly segmented due to the distortion. Moreover, different hand sizes also change the weight of each finger in FEMD, which leads to mismatching. Fig. <ref type="figure" target="#fig_1">12(a)</ref> presents some examples of confusing cases for FEMD. As it shows, the palm size and relatively the length of the finger have great impact on the recognition performance.</p><p>Fig. <ref type="figure" target="#fig_0">11(b)</ref> shows the confusion matrices of Shape Context <ref type="bibr" target="#b26">[27]</ref>. The results are generated based on the Matlab demo code provided at the project homepage of Shape Context. <ref type="foot" target="#foot_0">2</ref> The most confusing cases are between gesture 1 and 7, and 8 and 9, since they have similar contours. Sometimes two fingers may fuse into one due to the distortion. That is why gesture 4 is also mistakenly recognized as gesture 3. Two different skeleton pruning methods, discrete curve evolution (DCE) <ref type="bibr" target="#b32">[33]</ref> and discrete skeleton evolution (DSE) <ref type="bibr" target="#b33">[34]</ref>, are tested for Skeleton Matching <ref type="bibr" target="#b25">[26]</ref> in our experiments. As claimed in <ref type="bibr" target="#b33">[34]</ref>, DSE method is more stable to the small protrusions. Hence the recognition accuracy with DSE is higher than DCE for L4O CV as shown in Table <ref type="table" target="#tab_0">I</ref>. Fig. <ref type="figure" target="#fig_0">11(c</ref>) shows the confusion matrices of Skeleton Matching <ref type="bibr" target="#b25">[26]</ref> with DSE <ref type="bibr" target="#b33">[34]</ref>. From the figure, we can see that the most confusing cases are between gestures 6 and 7, and 2 and 7. Due to the characteristics of the hand gesture, the pruned skeletons have similar global structures for different gestures as shown in Fig. <ref type="figure" target="#fig_1">12(c</ref>).</p><p>On the other two public datasets, NTU hand digit dataset <ref type="bibr" target="#b11">[12]</ref> and ASL finger spelling dataset <ref type="bibr" target="#b31">[32]</ref>, the proposed algorithm also outperforms other existing methods including near-convex decomposition based FEMD <ref type="bibr" target="#b11">[12]</ref>, Random Forest <ref type="bibr" target="#b31">[32]</ref>, HOG <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> and H3DF <ref type="bibr" target="#b22">[23]</ref> as shown in Tables <ref type="table" target="#tab_1">II</ref> and<ref type="table" target="#tab_1">III</ref>. Note that the compared results are directly extracted from the corresponding reference, since the same LOO CV is applied in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Sensitivity Analysis</head><p>To demonstrate the effectiveness of the proposed system, its sensitivity to the parameter, orientation, scale and view angle is further investigated in this section.</p><p>Parameter Sensitivity: There are three key parameters in the proposed SP-EMD, including the depth weigh , fingertip coefficient and the average size of superpixels . The evaluation results on these parameters are shown in Fig. <ref type="figure" target="#fig_10">13</ref>. It can be seen that the mean accuracy is quite stable whe , or varies, which is another merit of the proposed SP-EMD method. For small and , we note that the recognition performance is slightly degraded. For example, the recognition rate is below 96% when and . It is because that no depth features (</p><p>) are utilized, and the moving cost of fingertips is similar to the one of the palm for . Also, from Fig. <ref type="figure" target="#fig_10">13</ref>(b), it can be seen that the mean accuracy is not sensitive to the average size of superpixels , while a small should be avoided to reduce the computation complexity.</p><p>Orientation and Scale Sensitivity: Although a preprocessing step is proposed in Section III-D to ensure all the gestures have similar orientations and scales, it may be insufficient in some extreme cases. To evaluate the orientation and scale sensitivity of the system, synthetic mismatches are added to corrupt the preprocessed data of our hand gesture dataset. More specifically, they are randomly rotated with a degree or scaled by a factor of (</p><p>). In our experiments, and are generated using a Gaussian distribution with zero mean and a standard deviation of . Five different values of are tested and each test is repeated 50 times. Tables <ref type="table" target="#tab_2">IV</ref> and<ref type="table">V</ref> summarize the averaged accuracies with orientation and scale noise, respectively. It can be seen that the mean accuracy is robust to the orientation noise thanks to the effectiveness of the 2D ICP alignment employed. On the other hand, the performance is relatively more sensitive to the scale noise. Note that the mean accuracy for L4O CV degrades much faster than that of LOO CV, since the latter CV has 4 times as many templates as the former one, and thus it offers higher chances to observe similar level of scale mismatches.</p><p>View Angle Sensitivity: In practice, the user may not directly face the camera, which leads to severe out-of-plane rotation of the gestures. To show the effectiveness of the proposed system in this situation, a view angle sensitivity test is also conducted with samples captured from 5 different view angles (roughl , and ) with 5 subjects. All the parameters are set as the same as the previous experiments. The achieved mean accuracies for L4O CV and LOO CV are 94.84% and 98.07%, respectively. It can be seen that the mean accuracy does not degrade too much (2.36% drop in L4O CV and 1.03% drop in LOO CV). This test suggests that the proposed recognition system is quite robust to these view angle changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. APPLICATIONS</head><p>Hand gesture recognition finds great potential in many emerging applications such as interactive gaming and virtual reality systems over traditional input devices like keyboards and mice. Therefore, we use the hand gesture as an interface to implement two real life HCI applications, Rock-Paper-Scissors -Lizard-Spock Game and 3D Content Browser, based on the proposed hand gesture recognition system. As shown in Table <ref type="table" target="#tab_0">I</ref>, our system runs in real-time with a considerable high accuracy, which ensures a pleasant user experience. The demo video is available in our project homepage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Rock-Paper-Scissors-Lizard-Spock Game</head><p>The experiments show that our system can recognize Gesture 9, i.e. the famous "Spock" gesture, with a high accuracy. In this demo, we build a Rock-Paper-Scissors-Lizard-Spock Game (an expansion of the classic Rock-Paper-Scissors game) system played between a human and a computer. The computer will randomly choose a weapon, while user's gesture is recognized by our system. The system will show the result whether the user is the winner (smiling face), even if the user is not familiar with  the complicated rule of the game. Fig. <ref type="figure" target="#fig_11">14</ref> shows an example of the demo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Content Browser</head><p>The greatest advantage of using hand gesture in HCI is its contactless nature. Thus we develop the second demo to introduce its potential applications in interacting virtual reality. By defining different gestures as the command to rotate, move or scale the virtual camera, users can use hand to simply interact with and navigate in the virtual 3D world. Our system detects the user's command first, and then zoom/rotate/translate the camera or the 3D object according to how far the hand moves. Since we use the body skeleton to locate the hand, it is also easy to extend to a two-hand gesture system, which will provide more interactions between the human and computer. An example of the 3D content browser is shown in Fig. <ref type="figure" target="#fig_12">15</ref> together with four gesture commands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>A novel superpixel-based hand gesture recognition system using a novel SP-EMD and depth camera for contactless HCI has been proposed. It is based on a compact representation in the form of superpixels, which efficiently capture the shape, texture and depth features of the gestures. Based on this representation, a novel distance metric, superpixel earth mover's distance (SP-EMD), is proposed as the dissimilarity measurement for gesture recognition. The key partial matching issue is addressed by introducing the concept of virtual superpixels, which serves to model the folded fingers. The effectiveness of the proposed system is illustrated by extensive experiments on three challenging real-life datasets. High mean accuracies (99.1%, 99.6% and 75.8%) and fast recognition speed (average 0.067 second per gesture) for hand gesture recognition is achieved.</p><p>Comparing with previous distance measures such as FEMD, shape context distance and path similarity, the proposed SP-EMD metric achieves better performance for hand gesture recognition. Moreover, it is very computationally efficient and thus suitable for real-life HCI applications. Our future research will focus on exploring robust color features for SP-EMD and extending it to dynamic hand gesture, body posture and generic object recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework of the proposed superpixel-based hand gesture recognition system.</figDesc><graphic coords="2,55.02,64.14,220.98,85.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Frames from color and depth camera. (a) The color texture captured by Kinect. (b) Registered depth map with the skeleton.</figDesc><graphic coords="2,307.02,64.14,244.98,103.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hand localization and segmentation. From top to bottom: extracted color texture blocks of the hands; depth map blocks of the hands; segmented hand shapes.</figDesc><graphic coords="3,40.98,64.14,246.00,184.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Shape representation using superpixels. First row: superpixels on color textures. Last row: corresponding shapes represented by superpixels. Black dots indicate the centers of superpixels.</figDesc><graphic coords="3,307.98,64.14,238.98,117.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of superpixel earth mover's distance. (a) is based on finger extraction and shape only. Left is the hand shapes, and right is the corresponding transportation problem. (b) is an example of joint color-depth SP-EMD. Left is the superpixel representation on both shapes and textures. Right is the corresponding transportation problem.</figDesc><graphic coords="4,315.00,64.14,228.00,252.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The moving flow of SP-EMD between hand gestures. (a) A given hand gesture in superpixel representation. (b) A similar gesture to (a) from a different subject. (c) A different gesture given by the same subject as (a). (d) The optimal flow from (a) to (b). (e) The optimal flow from (a) to (c). Blue stars, red circles and magenta squares denote gesture (a), (b) and (c), respectively. Black arrows indicate the moving flow directions.</figDesc><graphic coords="5,310.02,64.14,234.00,306.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Gesture samples (0-9) captured in two different environments.</figDesc><graphic coords="6,46.02,63.12,502.02,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Confusion matrices of hand gesture recognition using SP-EMD (unit:%). (a) L4O CV. (b) LOO CV.</figDesc><graphic coords="6,81.00,150.12,432.00,184.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Confusing cases of SP-EMD. (a) Gestures 7 (blue stars) and 9 (red circles). (b) Gestures 7 (blue stars) and 6 (red circles).</figDesc><graphic coords="7,51.00,65.10,225.00,118.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Fig. 12(b) gives some confusing examples for Shape Context. It can be seen that the distortion changes the contours severely.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Parameter sensitivity of , , and N/K, using L4O CV.</figDesc><graphic coords="9,40.98,64.14,244.98,129.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Rock-Paper-Scissors-Lizard-Spock Game.</figDesc><graphic coords="10,43.02,64.14,244.98,132.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Example view (left) and commands (right) of 3D content browser.</figDesc><graphic coords="10,43.02,235.14,244.98,114.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I THE</head><label>I</label><figDesc>MEAN ACCURACY AND MEAN RUNNING TIME OF FEMD, SHAPE CONTEXT, SKELETON MATCHING, AND OUR PROPOSED SP-EMD ON OUR DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>ON THE NTU HAND DIGIT DATASET TABLE III COMPARISON ON THE ASL FINGER SPELLING DATASETFig.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV MEAN</head><label>IV</label><figDesc>ACCURACY OF SP-EMD WITH ORIENTATION NOISE TABLE V MEAN ACCURACY OF SP-EMD WITH SCALE NOISE</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Http://www.eecs.berkeley.edu/Research/Projects/CS/vision/shape/ sc_digits.html.</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the General Research Fund (GRF) of the Hong Kong Research Grant Council (RGC). The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Enrico Magli.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gesture recognition: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Acharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. C, Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="324" />
			<date type="published" when="2007-04">Apr. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: A review</title>
		<author>
			<persName><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2007-10">Oct. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision-based handgesture applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Edan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2011-02">Feb. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coordinative structure of manipulative hand-movements facilitates their recognition</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dejmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zacksenhouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2455" to="2463" />
			<date type="published" when="2006-11">Nov. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Interaction capture and synthesis</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Kry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Pai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="872" to="880" />
			<date type="published" when="2006-07">Jul. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extraction of 2D motion trajectories and its application to hand gesture recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1061" to="1074" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Analyzing and capturing articulated hand motion in image sequences</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1910" to="1922" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing gestures by learning local motion signatures of HOG descriptors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Kaaniche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2247" to="2258" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Feature processing and modeling for 6D motion gesture recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="561" to="571" />
			<date type="published" when="2013-04">Apr. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hand gesture detection and recognition using principal component analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Dardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Petriu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CIMSA</title>
		<meeting>CIMSA<address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Dardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE. Trans. Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3592" to="3607" />
			<date type="published" when="2011-11">Nov. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Robust part-based hand gesture recognition using Kinect sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1110" to="1120" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Real-time head and hand tracking based on 2.5D data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Suau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Casas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="575" to="585" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-time depth image acquisition and restoration for image based rendering and processing systems</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Shum</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11265-013-0819-2</idno>
	</analytic>
	<monogr>
		<title level="j">J. Signal Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2281" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parsing the hand in depth images</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2014-08">Aug. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with depth images: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Suarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. RO-MAN</title>
		<meeting>RO-MAN<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="411" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel real time hand detection based on skin-color</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ISCE</title>
		<meeting>ISCE<address><addrLine>Hsinchu, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Robust hand detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG, Seoul</title>
		<meeting>FG, Seoul</meeting>
		<imprint>
			<publisher>Korea</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="614" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Model-based 3D tracking of an articulated hand</title>
		<author>
			<persName><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R S</forename><surname>Mendona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>Kauai, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="310" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real time gesture recognition with 2 kinect sensors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Mihail</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IPCV</title>
		<meeting>IPCV<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">American sign language recognition with the Kinect</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zafrulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brashear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Presti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICMI, Alicante</title>
		<meeting>ICMI, Alicante</meeting>
		<imprint>
			<publisher>Spain</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Histogram of 3D facets: A characteristic descriptor for hand gesture recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. FG</title>
		<meeting>FG<address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Histogram of orientated gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR<address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Similarity assessment model for chinese sign language videos</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="751" to="761" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Path similarity skeleton graph matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1282" to="1292" />
			<date type="published" when="2008-07">Jul. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shape classification using the inner-distance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="299" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Geometric camera calibration using circular control points</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heikkila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1066" to="1077" />
			<date type="published" when="2000-10">Oct. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A method for registration of 3D shapes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992-02">Feb. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000-11">Nov. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spelling it out: Real-time ASL fingerspelling recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV Workshops</title>
		<meeting>ICCV Workshops<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1114" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skeleton pruning by contour partitioning with discrete curve evolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="449" to="462" />
			<date type="published" when="2007-03">Mar. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Discrete skeleton evolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Latecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EM-CVPR</title>
		<meeting>EM-CVPR<address><addrLine>Ezhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="362" to="374" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
