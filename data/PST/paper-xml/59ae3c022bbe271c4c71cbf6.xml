<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-view Unsupervised Feature Selection with Adaptive Similarity and View Weight</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Chenping</forename><surname>Hou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Feiping</forename><surname>Nie</surname></persName>
							<email>feipingnie@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Tao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dongyun</forename><surname>Yi</surname></persName>
							<email>dongyun.yi@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<postCode>410073</postCode>
									<settlement>Changsha, Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<addrLine>Shaanx-i</addrLine>
									<postCode>710072</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-view Unsupervised Feature Selection with Adaptive Similarity and View Weight</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2EF88CEBC080E46974A1C9109B11C1A1</idno>
					<idno type="DOI">10.1109/TKDE.2017.2681670</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2017.2681670, IEEE Transactions on Knowledge and Data Engineering This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2017.2681670, IEEE Transactions on Knowledge and Data Engineering</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>multiple view data mining</term>
					<term>unsupervised feature selection</term>
					<term>adaptive similarity and view weight</term>
					<term>sports action recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the advent of multi-view data, multi-view learning has become an important research direction in both machine learning and data mining. Considering the difficulty of obtaining labeled data in many real applications, we focus on the multi-view unsupervised feature selection problem. Traditional approaches all characterize the similarity by fixed and pre-defined graph Laplacian in each view separately and ignore the underlying common structures across different views. In this paper, we propose an algorithm named Multi-view Unsupervised Feature Selection with Adaptive Similarity and View Weight (ASVW) to overcome the above mentioned problems. Specifically, by leveraging the learning mechanism to characterize the common structures adaptively, we formulate the objective function by a common graph Laplacian across different views, together with the sparse ℓ 2,p -norm constraint designed for feature selection. We develop an efficient algorithm to address the non-smooth minimization problem and prove that the algorithm will converge. To validate the effectiveness of ASVW, comparisons are made with some benchmark methods on real-world datasets. We also evaluate our method in the real sports action recognition task. The experimental results demonstrate the effectiveness of our proposed algorithm.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N many real applications of data mining, machine learn- ing and image processing, data is represented by multiple distinct feature sets. For example, in image processing, each image can be described by different visual descriptors, such as SIFT <ref type="bibr" target="#b0">[1]</ref>, HOG <ref type="bibr" target="#b1">[2]</ref>, LBP <ref type="bibr" target="#b2">[3]</ref> and GIST <ref type="bibr" target="#b3">[4]</ref> etc. Different type of features can capture specific information of the images. For example, SIFT is robust to image rotation, noise, illumination and LBP is a powerful texture feature. In web mining, a web can be characterized by its content and its link information, which are two distinct descriptions or views. In Video Semantic Recognition (VSR) <ref type="bibr" target="#b4">[5]</ref>, the key frames of a video are also images. We can also use different kinds of image descriptors to characterize the video. Multiple view learning, which focuses on learning from data represented by multiple distinct feature sets, has aroused considerable research interests in recent years. The aim of multiple view learning is to fully use the diversity and consistency among different representations to obtain better performance than to use traditional single view methods by taking the connected multiple representations as the input.</p><p>In manipulating multi-view data, it is crucial to employ all the heterogeneous features to create more powerful models than using each individual type of features separately. Many multi-view learning approaches have already been proposed. The researches of multi-view learning range from different areas, such as feature learning <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, semi-supervised learning <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, active learning <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, ensemble learning <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and transfer learning <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. See more details on surveys <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Among these researches, multi-view feature selection, a kind of feature learning pattern, has obtained plenty of research interests. It has been used in many applications, e.g., video action recognition <ref type="bibr" target="#b4">[5]</ref>, human motion retrieval <ref type="bibr" target="#b20">[21]</ref> and visual concept recognition <ref type="bibr" target="#b21">[22]</ref>. In these applications, the data sets generally have multiple modalities, and each modality is usually represented in a high-dimensional feature space which frequently leads to the curse of dimensionality problem. Multi-view feature selection provides an effective solution to solve or at least alleviate this problem. It aims to find a compact set of representative features, which can retain the optimal salient characteristics. Preprocessing data in this way not only decreases the processing time but also leads to better generalization of the learned models <ref type="bibr" target="#b22">[23]</ref>.</p><p>In the literature, there are totally two distinct ways for unsupervised multi-view feature selection. The first kind of approaches employ traditional single-view methods by taking the connected features from multiple views as the input directly. Typical unsupervised feature selection algorithms for single view data include Laplacian Score (LapScor) <ref type="bibr" target="#b23">[24]</ref>, SPECtral feature selection (SPEC) <ref type="bibr" target="#b24">[25]</ref> and Minimum Redundancy Spectral Feature selection (MRSF) <ref type="bibr" target="#b25">[26]</ref>. Usually, these methods use various graphs to characterize the manifold structure at first and then rank each feature. Compared with the traditional unsupervised feature selection approaches, these methods have been proved to perform better in many cases <ref type="bibr" target="#b25">[26]</ref>. Nevertheless, they are not suitable for multi-view data in most cases. They treat features from different views independently and discard the underlying correlations between different views.</p><p>The other way for unsupervised multi-view feature selection is to tackle multi-view data directly. Typical meth-ods include Adaptive Multi-View Feature Selection (AMFS) method <ref type="bibr" target="#b20">[21]</ref>. It employs one local descriptor to characterize local geometric structure of data in each view, combines them by the weighted sum and uses the trace ratio criteria to rank each feature. It can automatically assign multi-view features with adaptive feature weights. The authors have formulated the objective function as a general trace ratio optimization problem and applied their method in human motion retrieval successfully. Besides, Feng et al. have proposed the method named as Adaptive Unsupervised Multiview Feature Selection (AUMFS) <ref type="bibr" target="#b21">[22]</ref>. It attempts to use three kinds of information, i.e., data cluster structure, data similarity and the correlations between different views, for feature selection. Concretely, it employs a robust sparse regression model with the l 2,1 -norm penalty to perform feature selection. Moreover, Tang et al. have proposed an unsupervised feature selection framework, MVFS, for multiview data in social media <ref type="bibr" target="#b26">[27]</ref>. Its relaxed formulation is similar to AUMFS, except for the determination of balance parameter and the loss function in regression. See more details about these methods in next section. Although the proposed feature selection methods perform well in many applications, their performances can also be improved. AMFS, AUMFS and MVFS all characterize the local structure of each view data by similarity matrix separately. They do not consider the underlying common structures across different views. Besides, the structure similarity matrices in these methods are computed in advance and fixed in the learning process. It is better to leverage the learning mechanism to characterize the common structures adaptively.</p><p>In this paper, we try to solve the problem of unsupervised feature selection for multi-view data. A novel method, named as Adaptive Similarity and View Weight (ASVW), has been presented. Different from traditional approaches which characterize the local structure in each view with predefined and fixed similarity matrix separately, we propose to learn a common similarity matrix to characterize the structures across different views. Besides, the parameters to balance the weights of views are also tuned automatically. For feature selection, some sparse constraints are added on the transformation matrix. We provide an effective method to solve the proposed problem with sparse constraints, together with some deep analyses. Compared with traditional multi-view unsupervised feature selection approaches, our method has been demonstrated to have better performances on some benchmark data sets. Further, we also evaluated our method in the real sports action recognition scenario.</p><p>The rest of this paper is organized as follows. Section II provides notations and related works. We formulate ASVW and provide an effective solution to this problem in Section III. The convergence behavior, together with parameter determination and computational cost, are analyzed in Section IV. Section V provides some promising comparing results on various kinds of data sets. We evaluate our algorithm on a real task, i.e., sports action recognition, in Section VI, followed by the conclusions and future works in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we will briefly review several representative multi-view unsupervised feature selection approaches. The weight coefficient of the v-th view r 1</p><p>The weight redistribution parameter r 2</p><p>The similarity redistribution parameter sv</p><p>The reduced dimensionality of the v-th view data s</p><p>The number of selected features</p><formula xml:id="formula_0">x i ∈ R d The i-th data point x (v) i ∈ R dv</formula><p>The i-th data point's feature vector in the v-th view</p><formula xml:id="formula_1">X (v) ∈ R dv ×n Data matrix in the v-th view X ∈ R d×n Data matrix S ∈ R n×n</formula><p>The common similarity matrix Wv ∈ R dv ×sv</p><p>The projection matrix in the v-th view α ∈ R V ×1</p><p>The weight coefficient vector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>In this paper, matrices and vectors are written in boldface.</p><p>For an matrix M = (m ij ), its i-th row, j-th column are denoted by m i and m j respectively. The ℓ 2 -norm of a vector</p><formula xml:id="formula_2">v ∈ R n is defined as ∥v∥ 2 = ( n ∑ i=1 |v i | 2 )<label>1 2</label></formula><p>. The matrix Frobenius norm is denoted by ∥•∥ F . The ℓ r,p -norm of an matrix M ∈ R n×m is defined as <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> </p><formula xml:id="formula_3">∥M∥ r,p =    m ∑ i=1   n ∑ j=1 |m ij | r   p r    1 p , r &gt; 0, p &gt; 0.<label>(1)</label></formula><p>When r ≥ 1 and p ≥ 1, ℓ r,p -norm is a valid norm as it satisfies the three norm conditions. Given n data samples {x i } n i=1 , the data matrix is denoted as</p><formula xml:id="formula_4">X = [x 1 , • • • , x n ] ∈ R d×n . The i-th sample x i = [(x (1) i ) T , • • • , (x (V ) i ) T ] T ∈ R d includes features from V views and the v-th view x (v) i ∈ R dv has d v features such that d = ∑ V v=1 d v . Denote the data matrix of the v- th view as X (v) = [x (v) 1 , • • • , x (v) n ] ∈ R dv×n , thus X = [(X (1) ) T , • • • , (X (V ) ) T ] T .</formula><p>The notations used in this paper are summarized in Table <ref type="table" target="#tab_0">1</ref>. We will explain the concrete meanings of the notation when it is used firstly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AMFS</head><p>Adaptive Multi-View Feature Selection (AMFS) is an unsupervised feature selection approach for human motion retrieval. It employs local descriptors to characterize local geometric structure of motion data. There are totally three steps. First, as in traditional works, they use the viewspecified graph-Laplacian <ref type="bibr" target="#b29">[30]</ref> as local descriptor by employing different methods, e.g., the local linear regression weight <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[31]</ref> or the graph similarity weight <ref type="bibr" target="#b29">[30]</ref>. Then, these Laplacian matrices are combined together linearly by non-negative view weights to explore the complementary information between different view features. Finally, to discard the redundant features, AMFS uses the trace ratio criteria as in traditional Fisher Score feature selection method.</p><p>Briefly, the objective function of AMFS can be summarized as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>min</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W,α</head><p>Tr(W T X(</p><formula xml:id="formula_5">∑ V v=1 α r v L (v) )X T W) Tr(W T XHX T W) s.t. V ∑ v=1 α v = 1, α v ≥ 0, W ∈ {0, 1} d×s , (2)</formula><p>where L (v) is the graph Laplacian that characterizes the concerning v-th view and H is the centralized matrix that is used to centralize the data.</p><formula xml:id="formula_6">α = [α 1 , α 2 , • • • , α V ]</formula><p>T is a weight coefficient vector to combine all the Laplacian matrices and r &gt; 1 is a parameter to avoid trivial solution (r = 1 means that only the v-th view with smallest loss is considered since only α v = 1.). W ∈ {0, 1} d×s is a weight matrix in performing feature selection. As mentioned in <ref type="bibr" target="#b20">[21]</ref>, since each row of W has one and only one non-zero element, the features corresponding non-zero elements are selected. This problem can be solved in an alternative way. Please refer to <ref type="bibr" target="#b20">[21]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">AUMFS</head><p>Adaptive Unsupervised Multi-view Feature Selection (AUMFS) is another unsupervised feature selection approaches for visual concept recognition. It attempts to use three kinds of information, i.e., data cluster structure, data similarity and the correlations between different views, for feature selection. To achieve this goal, it employs a robust sparse regression model with the ℓ 2,1 -norm penalty to predict data cluster labels. Meanwhile, it uses the same way as AMFS to characterize the local structure by linear combination of graph Laplacian matrices from different views. For feature selection, it also adds the sparse ℓ 2,1 -norm penalty to the transformation matrices for each view. As defined in Eq. ( <ref type="formula" target="#formula_2">1</ref>), the ℓ 2,1 -norm penalty can impose row sparsity and be used for feature selection. Essentially, AUMFS can be regarded as extending spectral regression <ref type="bibr" target="#b31">[32]</ref>, a traditional dimensionality reduction framework, for multiple view unsupervised feature selection. The first improvement is the robust regression and sparse regularizer. The second one is the joint optimization for multiple view data. After computing the optimal transformation matrix W, AUMFS employ the 2-norm of each row vector of W as the criteria to rank the importance of each feature. The larger this value is, the more important the corresponding feature is.</p><p>Concretely, the objective function of AUMFS is</p><formula xml:id="formula_7">min W,α,F Tr(F T ( V ∑ v=1 α r v L (v) )F) + λ∥X T W -F∥ 2,1 + β∥W∥ 2,1 s.t. V ∑ v=1 α v = 1, α v ≥ 0, F T F = I, F ≥ 0.<label>(3)</label></formula><p>Here, I is an identity matrix and L (v) is the same as that in Eq. <ref type="bibr" target="#b1">(2)</ref>. F is the relaxed label matrix which approximates the real label matrix and F ≥ 0 indicates that all the elements of F is non-negative. λ and β are two non-negative balance parameters. This problem has also been solved in an alternative manner.</p><p>Different from AUMFS in Eq. ( <ref type="formula" target="#formula_7">3</ref>), MVFS <ref type="bibr" target="#b26">[27]</ref> predefined the view balance parameter α v and it is fixed during the iterations. Besides, in the second regression function, MVFS employed the traditional Frobenius norm, instead of the ℓ 2,1 -norm.</p><p>As we can see from the formulations of AMFS in Eq. ( <ref type="formula">2</ref>) and AUMFS in Eq. (3), they characterize the multiple view local structure by simply adding the graph Laplacian matrix from each view. Besides, the combined graph Laplacian matrix is fixed during the following iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-VIEW FEATURE SELECTION WITH ADAP-TIVE SIMILARITY AND VIEW WEIGHT</head><p>Assume that we have been given multi-view data from totally V views, denoted as X = [(X (1) </p><formula xml:id="formula_8">) T , • • • , (X (V ) ) T ] T ∈ R d×n .</formula><p>We want to select s features from the original d features. Essentially, this problem is equivalent to ranking all features by some metric and selecting the most important features. The differences among different methods are the ranking criterions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formulation</head><p>Considering that the input of our algorithm is multiple view data, we try to explore the homogenous prosperity among the heterogeneous views.</p><p>The objective function of ASVW is composed of two parts. Specifically, the first one measures the common local geometric among different views and the other one is the sparse constraint designed for feature selection. Before going into the details, we would like to characterize local structure for the v-th view.</p><p>Inspired by the traditional dimensionality reduction approach Laplacian Eigenmap (LE) <ref type="bibr" target="#b29">[30]</ref> and its linear extension, Local Preserving Projection (LPP) <ref type="bibr" target="#b32">[33]</ref>, we also assume that the local similarity among different points should be preserved. Concretely, nearby points in high dimensional space should also be nearby in low dimensional space. It amounts to the following equation.</p><formula xml:id="formula_9">n ∑ i=1 n ∑ j=1 ∥W T v x (v) i -W T v x (v) j ∥ 2 S (v) ij ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_10">S (v)</formula><p>ij is the local similarity between the v-th view data x (v) i and x (v) j . W v is the projection matrix for the v-th view.</p><p>As in LPP, it should satisfy the orthogonal constraint, i.e.,</p><formula xml:id="formula_11">W T v W v = I.</formula><p>To utilize the homogenous prosperity among different views, it is direct to sum Eq. ( <ref type="formula" target="#formula_9">4</ref>) for all views. Nevertheless, this kind of combination will cause some problems. (1) This simple strategy is not enough to characterize the common structure among different views since it can be regarded as trading the characterization for each view independently.</p><p>(2) In the view of optimization, the simple adding strategy will lead to the trivial solution. It amounts to the computation of projection matrix W v for each view independently and ignores the correlations across different views. <ref type="bibr" target="#b2">(3)</ref> The similarity matrices are fixed in traditional approaches, e.g., AMFS, AUMFS and MVFS. They can only characterize the geometric structure for the specific view.</p><p>To overcome above mentioned problems, we use the following two strategies. <ref type="bibr" target="#b0">(1)</ref> To characterize the common structures for data points in all views, we assume that all the similarity matrices among different views are identical. More importantly, different from traditional methods, this common matrix is not predefined, it should be learned from the multi-view data to characterize the common structures for all views. (2) As in AMFS and AUMFS, we would like to use a balance parameter to balance the effectiveness of different views. Mathematically, the objective function is</p><formula xml:id="formula_12">V ∑ v=1 n ∑ i=1 n ∑ j=1 α r1 v ∥W T v x (v) i -W T v x (v) j ∥ 2 S (v) ij s.t. S (v) ij = S ij for v = 1, 2, • • • , V.</formula><p>(</p><p>Here S ij is the common local geometric descriptor for multiview data. r 1 is a parameter to avoid trivial solution. In fact, if we set r 1 = 1 as in traditional balance strategy, the solution of α will be α v = 1 corresponding to the minimum</p><formula xml:id="formula_14">∑ n i=1 ∑ n j=1 ∥W T v x (v) i -W T v x (v)</formula><p>j ∥ 2 S ij and other entries in α equal to 0. It means that only one view is selected by this method and it is a trivial solution <ref type="bibr" target="#b33">[34]</ref>.</p><p>Besides, we would like to add some constraints on the similarity matrix. Since it is learned from multi-view data and each row of S corresponds to a data point, we add the following constraint,</p><formula xml:id="formula_15">n ∑ j=1 S ij = 1, S i. ≥ 0,<label>(6)</label></formula><p>where S i. is the i-th row of S. This constraint is added to ensure that S is an validate similarity matrix as in traditional approaches. In the following solving procedure, it can also be used to avoid trivial solution with S = 0. Evoked by the above mentioned strategy, we also add a parameter r 2 as the power of the similarity matrix. Besides, as in LE, since S is used to characterize local structures, it should be assumed that most elements of S are zeros. Recall that each row of S corresponds to a data point, we add a sparse constraint on it. The sparse constraint should be</p><formula xml:id="formula_16">∥S i. ∥ 0 = k, (<label>7</label></formula><formula xml:id="formula_17">)</formula><p>where k is the number of nearby neighborhoods. ∥ • ∥ 0 denotes the ℓ 0 norm of a vector, that is, the number of non-zero elements of a vector. Intuitively, it is similar to the traditional approaches in constructing k-neighborhood graph.</p><p>The second part of the objective function is designed for feature selection. Inspired by the basic idea of sparse regression for feature selection in <ref type="bibr" target="#b27">[28]</ref>, we also add sparse constraints on projection matrix to measure their values.</p><p>Recall the definition of W v ∈ R dv×sv with s v the reduced dimensionality of the projections for the v-th view, it can be regarded as the projection matrix in traditional model. Denote the i-th row of W v as W (v) i. . Since our task is feature selection, we expect that the transformation matrix W v holds some structure sparsity property. In particular, we expect that most rows of W v are zeros. In detail, the corresponding features can be neglected since these features are redundant for characterizing local similarities. When we use the 2-norm of W (v) i. as a metric to measure its contribution in regression, the sparsity property, i.e., a few number of</p><formula xml:id="formula_18">W (v) i.</formula><p>are non-zeros, indicates the following objective function. (8)   with 0 ≤ p ≤ 1 for the sake of feature selection. Here ∥W v ∥ 2,p is the ℓ 2,p -norm as defined in Eq.( <ref type="formula" target="#formula_2">1</ref>) with r = 2. Since 0 ≤ p ≤ 1, it is a sparse constraint and requires that a small number of W (v)</p><formula xml:id="formula_19">dv ∑ i=1 (∥W (v) i. ∥ 2 ) p = dv ∑ i=1   sv ∑ j=1 W (v) ij 2   p/2 = ∥W v ∥ p 2,p ,</formula><p>i. are non-zeros vectors. The nonzero W (v) i. corresponds to the important features since the W (v)</p><p>i. with all zero elements can be neglected in the former regression.</p><p>By combining the objective functions in Eq. ( <ref type="formula" target="#formula_13">5</ref>), Eq. ( <ref type="formula">8</ref>) and joining all constraints, the multi-view unsupervised feature selection approach with Adaptive Similarity and View Weight (ASVW) can be summarized as follows.</p><formula xml:id="formula_20">min L(W 1 , • • • , W V , α, S) = V ∑ v=1 n ∑ i=1 n ∑ j=1 α r1 v ∥W T v x (v) i -W T v x (v) j ∥ 2 (S ij ) r2 + λ V ∑ v=1 ∥W v ∥ p 2,p s.t. W T v W v = I V ∑ v=1 α v = 1, α v ≥ 0 n ∑ j=1 S ij = 1, S ij ≥ 0, ∥S i. ∥ 0 = k, (<label>9</label></formula><formula xml:id="formula_21">)</formula><p>where λ is a non-negative balance parameter.</p><p>After deriving the optimal solution, we use the 2-norm of W (v) i. , i.e., ∥W (v) i. ∥ 2 , to evaluate the importance of all features from different views. The larger this value is, the more important the corresponding feature is. In real application, we can either select a fixed number of the most important features or set a threshold and select the feature whose importance is larger than this value. In the following, we choose the first strategy. The fixed number is s percent of the total number of original features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Solution</head><p>As seen from the optimization problem in Eq. ( <ref type="formula" target="#formula_20">9</ref>), there are totally three groups of experiments. All W v s are projection matrices. α consists of all the balance parameters. S is the learned common similarity matrix. It is difficult to solve this problem directly since (1) both of the terms are nonsmooth; (2) all the variables are coupled in the first function. We propose to optimize them alternatively. In other words, we fix two groups of variables and optimize the rest one alternatively. Our theoretical results show that this kind of iteration will converge.</p><p>1) Fix {W v } V v=1 , S and optimize α. When all W v and S are fixed, we need to optimize a set of balance parameters α v . In this scenario, the second term of the objective function in Eq. ( <ref type="formula" target="#formula_20">9</ref>) is not related to α v . For simplicity, denote</p><formula xml:id="formula_22">g v = n ∑ i=1 n ∑ j=1 ∥W T v x (v) i -W T v x (v) j ∥ 2 (S ij ) r2 . (<label>10</label></formula><formula xml:id="formula_23">)</formula><p>Then the optimization problem in Eq. ( <ref type="formula" target="#formula_20">9</ref>) becomes</p><formula xml:id="formula_24">min L(α) = V ∑ v=1 α r1 v g v , s.t. V ∑ v=1 α v = 1, α v ≥ 0.<label>(11)</label></formula><p>The optimization problem in Eq. ( <ref type="formula" target="#formula_24">11</ref>) seems complicated. Nevertheless, after some deductions, we can derive its solution with closed form.</p><p>If we ignore the non-negative constraints, the Lagrange function is</p><formula xml:id="formula_25">V ∑ v=1 α r1 v g v -γ( V ∑ v=1 α v -1), (<label>12</label></formula><formula xml:id="formula_26">)</formula><p>where γ is the Lagrange multiplicator. Take the derivative on α v and set it to zero, we get</p><formula xml:id="formula_27">r 1 α r1-1 v g v -γ = 0. (<label>13</label></formula><formula xml:id="formula_28">)</formula><p>It indicates that</p><formula xml:id="formula_29">α v = ( γ r 1 g v ) 1 r 1 -1 . (<label>14</label></formula><formula xml:id="formula_30">) Since ∑ V v=1 α v = 1, we can get α v = (g v ) 1 1-r 1 ∑ V v=1 (g v ) 1 1-r 1 . (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>One point should be mentioned here. In the above deduction, we neglect the non-negative constraints on α v . Nevertheless, since g v ≥ 0, the derived results in Eq. ( <ref type="formula" target="#formula_30">15</ref>) satisfied this constraint automatically.</p><p>In a word, when {W v } V v=1 and S are fixed, the optimal α v can be computed in a closed form by using Eq. <ref type="bibr" target="#b14">(15)</ref>.</p><p>2) Fix {W v } V v=1 , α and optimize S. As seen from the formulation of ASVW in Eq. ( <ref type="formula" target="#formula_20">9</ref>), although the objective function is simple, the constraint is non-convex and it is difficult to solve it directly. Evoked by the above deduction in computing α, we can also derive its closed solutions.</p><p>Denote</p><formula xml:id="formula_32">β ij = V ∑ v=1 α r1 v ∥W T v x (v) i -W T v x (v) j ∥ 2 . (<label>16</label></formula><formula xml:id="formula_33">)</formula><p>The optimization problem concerning S in Eq. ( <ref type="formula" target="#formula_20">9</ref>) can be reformulated as</p><formula xml:id="formula_34">min L(S) = n ∑ i=1 n ∑ j=1 β ij (S ij ) r2 s.t. n ∑ j=1 S ij = 1, S ij ≥ 0, ∥S i. ∥ 0 = k, (<label>17</label></formula><formula xml:id="formula_35">)</formula><p>Comparing the optimization problems in Eq. ( <ref type="formula" target="#formula_24">11</ref>) and Eq. ( <ref type="formula" target="#formula_34">17</ref>), they have similar forms, except that S has another constraint ∥S i. ∥ 0 = k. Since the constraint is added on each row of S, we update S row by row respectively. Taking the i-th row as an example, the objective function in Eq. ( <ref type="formula" target="#formula_34">17</ref>) becomes</p><formula xml:id="formula_36">min L(S i. ) = n ∑ j=1 β ij (S ij ) r2 s.t. n ∑ j=1 S ij = 1, S ij ≥ 0, ∥S i. ∥ 0 = k. (18)</formula><p>Similarly, if we ignore the constraints ∥S i. ∥ 0 = k, the optimal solution to the problem in Eq. ( <ref type="formula">18</ref>) is</p><formula xml:id="formula_37">S ij = (β ij ) 1 1-r 2 ∑ n l=1 (β il ) 1 1-r 2 . (<label>19</label></formula><formula xml:id="formula_38">)</formula><p>Note that ∥S i. ∥ 0 = k. It means that only k elements in S i. are non-zero. Since we aim to minimize the objective function in Eq. ( <ref type="formula">18</ref>) and only k elements in S i. are nonzero, we only need to optimize the k non-zeros element. The larger β ij is, the larger objective function is. Thus, we set the n -k elements of S i. , which corresponds to the n -k largest β ij , to zeros and only update the rest k elements. Concretely, assume that the smallest k elements of β ij (with fixed i and except for β ii ) are β ij1 , β ij2 , • • • , β ij k , the optimal S i. to the problem in Eq. ( <ref type="formula">18</ref>) is</p><formula xml:id="formula_39">S ij l = (β ij l ) 1 1-r 2 ∑ k q=1 (β ijq ) 1 1-r 2 for l = 1, 2, • • • , k S ij l = 0 Otherwise. (20)</formula><p>3) Fix S, α and optimize {W v } V v=1 . As seen from the formulation of ASVW in Eq. ( <ref type="formula" target="#formula_20">9</ref>), if S and α are fixed, it can be separated into V independent sub-problems. In other words, we can compute W v by using the v-th view data solely. Take the computation of W v as an instance, the optimization problem in Eq. ( <ref type="formula" target="#formula_20">9</ref>) changes to</p><formula xml:id="formula_40">min L(W v ) = n ∑ i=1 n ∑ j=1 α r1 v ∥W T v x (v) i -W T v x (v) j ∥ 2 (S ij ) r2 + λ∥W v ∥ p 2,p s.t. W T v W v = I. (<label>21</label></formula><formula xml:id="formula_41">)</formula><p>Recall the basic idea in solving the sparse constraints problem as in <ref type="bibr" target="#b27">[28]</ref>, we take the derivative of</p><formula xml:id="formula_42">∥W v ∥ p 2,p with respect to W v . When the i-th row of W v , i.e., W (v) i. ̸ = 0 for i = 1, 2, • • • , d v , the derivative of ∥W v ∥ p 2,p with respect to W v is ∂∥W v ∥ p 2,p ∂W v = 2D (v) W v , (<label>22</label></formula><formula xml:id="formula_43">)</formula><p>where D (v) ∈ R dv×dv is a diagonal matrix with the i-th diagonal element as</p><formula xml:id="formula_44">d (v) ii = p 2 W (v) i. p-2 2 . (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>When D (v) is fixed, the derivative of L in Eq. ( <ref type="formula" target="#formula_40">21</ref>) can also be regarded as the derivative of the following objective function.</p><formula xml:id="formula_46">min L(W v ) = n ∑ i=1 n ∑ j=1 α r1 v ∥W T v x (v) i -W T v x (v) j ∥ 2 (S ij ) r2 + λTr(W T v D (v) W v ) s.t. W T v W v = I.<label>(24)</label></formula><p>In the following, we use the objective function in Eq. ( <ref type="formula" target="#formula_46">24</ref>) to approximate the formulation in Eq. ( <ref type="formula" target="#formula_40">21</ref>). We will prove that the objective function of ASVM will also decrease by solving the approximated problem in Eq. ( <ref type="formula" target="#formula_46">24</ref>). More importantly, we can solve the problem in Eq. ( <ref type="formula" target="#formula_46">24</ref>) in an effective way.</p><p>The first part of the objective function in Eq. ( <ref type="formula" target="#formula_46">24</ref>) is similar to that of LE <ref type="bibr" target="#b29">[30]</ref>, except that S in Eq. ( <ref type="formula" target="#formula_46">24</ref>) is not a symmetric matrix. It could not guarantee that the corresponding Laplacian matrix is semi-definite. To avoid this problem, we define a new matrix Q as follows,</p><formula xml:id="formula_47">Q ij = Q ji = ((S ij ) r2 + (S ji ) r2 )/2. (<label>25</label></formula><formula xml:id="formula_48">)</formula><p>It is obvious that Q is a symmetric matrix and</p><formula xml:id="formula_49">n ∑ i=1 n ∑ j=1 ∥W T v x (v) i -W T v x (v) j ∥ 2 (S ij ) r2 = n ∑ i=1 n ∑ j=1 ∥W T v x (v) i -W T v x (v) j ∥ 2 Q ij . (<label>26</label></formula><formula xml:id="formula_50">)</formula><p>Combine Eq. ( <ref type="formula" target="#formula_46">24</ref>) and Eq. ( <ref type="formula" target="#formula_49">26</ref>), the optimization problem in Eq. ( <ref type="formula" target="#formula_46">24</ref>) is equal to</p><formula xml:id="formula_51">min L(W v ) = Tr(W T v X (v) L(X (v) ) T W v ) + (λ/α r1 v )Tr(W T v D (v) W v ) s.t. W T v W v = I. (<label>27</label></formula><formula xml:id="formula_52">)</formula><p>Here L is the Laplacian matrix derived from the symmetric matrix Q.</p><p>The optimization problem in Eq. ( <ref type="formula" target="#formula_51">27</ref>) can be solved by the eigen-decomposition of the semi-definite matrix v) , by picking the eigenvectors corresponding to the smallest d v eigenvalues.</p><formula xml:id="formula_53">X (v) L(X (v) ) T + (λ/α r1 v )D (</formula><p>In a word, when S and α are fixed, we alternatively update D (v) using Eq. ( <ref type="formula" target="#formula_44">23</ref>) and update W v by solving the problem in Eq. <ref type="bibr" target="#b26">(27)</ref>.</p><p>In conclusion, when we fix one parameter and optimize the other two, we can derive the solutions in a closed form.</p><p>Finally, there are several points should be highlighted.</p><p>(1) In the above procedures, when S and α are fixed, we need not to derive the optimal W v and D (v) . In each round, we only need to update them once. Specifically, we update all the variables using Eq. ( <ref type="formula" target="#formula_30">15</ref>), Eq. ( <ref type="formula">20</ref>), Eq. ( <ref type="formula" target="#formula_44">23</ref>) and Eq. ( <ref type="formula" target="#formula_51">27</ref>) in sequence and we need not to iterate Eq. ( <ref type="formula" target="#formula_44">23</ref>) and Eq. ( <ref type="formula" target="#formula_51">27</ref>) to derive the optimal solution to the problem in Eq. ( <ref type="formula" target="#formula_46">24</ref>). We will prove that this kind of iteration will also converge.</p><p>(2) The second one is about initialization. Since ASVW is solved in an iterative way. We would like to initialize all views with equal α v and the similarity matrix S is computed by the same strategy as in LE, taking the whole data as the input. We initialize D (v) = I since every feature has the same importance at the beginning. In experiments, we use this kind of initialization if there is no specification.</p><p>(3) As seen from the formulation of ASVW, since we have derived the projection matrix W v for each view, beyond feature selection, ASVW can also be regarded as a feature learning algorithm. Besides, the byproduct of ASVW is the common similarity matrix S. It can also be widely used in many applications, such as feature learning, clustering and metric learning.</p><p>The procedure of ASVW is listed in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 ASVW</head><p>Input: Data matrix X, parameters: </p><formula xml:id="formula_54">r 1 , r 2 , λ, s, k s v for v = 1, 2, • • • , V . Output: Selected feature index {r 1 , r 2 , • • • , r s }. 1: Initialize α v = 1/V , D (v) = I. Initialize S</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PERFORMANCE ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Convergence Behavior</head><p>We solve ASVW in an alternative way and the following proposition indicates that the objective function of ASVW shown in Eq. ( <ref type="formula" target="#formula_20">9</ref>) is non-increasing in each iteration. Before going into the details, we need the following lemma, which is introduced in <ref type="bibr" target="#b34">[35]</ref>.</p><p>Lemma 1. When 0 &lt; p ≤ 2, for any nonzero vectors, a, b, the following inequality holds:</p><formula xml:id="formula_55">∥a∥ p 2 - p 2 ∥a∥ 2 2 ∥b∥ 2-p 2 ≤ ∥b∥ p 2 - p 2 ∥b∥ 2 2 ∥b∥ 2-p 2 . (<label>28</label></formula><formula xml:id="formula_56">)</formula><p>Proposition 1. By employing the optimization procedure in Algorithm 1, the objective function value of ASVW in Eq. ( <ref type="formula" target="#formula_20">9</ref>) is non-increasing.</p><p>Proof. We use Wv , α, S to denote the updated W v , α, S in each iteration. We first prove that updating Wv will not increase the value of the objective function. According to Eq. ( <ref type="formula" target="#formula_51">27</ref>) Wv = arg min</p><formula xml:id="formula_57">W T v Wv=I L(W v ),<label>(29)</label></formula><p>which indicates that</p><formula xml:id="formula_58">Tr( WT v X (v) L(X (v) ) T Wv ) + (λ/α r1 v )Tr( WT v D (v) Wv ) ≤Tr(W T v X (v) L(X (v) ) T W v ) + (λ/α r1 v )Tr(W T v D (v) W v ),<label>(30)</label></formula><p>which is equivalent to the following inequality</p><formula xml:id="formula_59">Tr( WT v X (v) L(X (v) ) T Wv ) + (λ/α r1 v ) d ∑ i=1 p 2 W(v) i. p 2 W (v) i. 2-p 2 ≤Tr(W T v X (v) L(X (v) ) T W v ) + (λ/α r1 v ) d ∑ i=1 p 2 W (v) i. p 2 W (v) i. 2-p 2 . (<label>31</label></formula><formula xml:id="formula_60">)</formula><p>According to Lemma 1, for each i we have,</p><formula xml:id="formula_61">d ∑ i=1 ( W(v) i. p 2 - p 2 W(v) i. 2 2 W (v) i. 2-p 2 ) ≤ d ∑ i=1 ( W (v) i. p 2 - p 2 W (v) i. 2 2 W (v) i.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-p 2</head><p>).</p><p>(</p><formula xml:id="formula_62">)<label>32</label></formula><p>Combining Eq. ( <ref type="formula" target="#formula_59">31</ref>) and Eq. ( <ref type="formula" target="#formula_62">32</ref>) and summarizing over all the views, we have</p><formula xml:id="formula_63">L( W1 , • • • WV , α, S) ≤ L(W 1 , • • • W V , α, S)<label>(33)</label></formula><p>Then, since the updated α and S are the optimal solution to the problems in Eq. ( <ref type="formula" target="#formula_24">11</ref>) and Eq. ( <ref type="formula">18</ref>) respectively, the following inequalities hold,</p><formula xml:id="formula_64">L( W1 , • • • WV , α, S) ≤ L( W1 , • • • WV , α, S),<label>(34)</label></formula><p>and</p><formula xml:id="formula_65">L( W1 , • • • WV , α, S) ≤ L( W1 , • • • WV , α, S).<label>(35)</label></formula><p>Combine Eq. ( <ref type="formula" target="#formula_63">33</ref>) to Eq. ( <ref type="formula" target="#formula_65">35</ref>), we get the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computational Time and Parameter Determination</head><p>As seen from the procedure of ASVW in Algorithm 1, we have solved this problem in an alternative way. In other words, we solve the optimization problem in Eqs. ( <ref type="formula" target="#formula_51">27</ref>), ( <ref type="formula" target="#formula_44">23</ref>), ( <ref type="formula" target="#formula_30">15</ref>) and ( <ref type="formula">20</ref>) alternatively. The computational complexity in solving each problem is listed as follows.</p><p>(1) The problem in Eq. ( <ref type="formula" target="#formula_51">27</ref>) can be solved by eigendecomposition. For deriving W v , its computational complexity is O(d 3 v ). Thus, the total computational complexity is O(</p><formula xml:id="formula_66">∑ V v=1 d 3 v ). (2)</formula><p>The problem in Eq. ( <ref type="formula" target="#formula_44">23</ref>) can be effectively solved by computing the 2-norm of a vector. When we compute the optimal D v , the computational complexity is O(d v × s v ), where s v is the reduced dimensionality of the projections for the v-th view and it is commonly set to the class number in applications. The total computational complexity is O(</p><formula xml:id="formula_67">∑ V v=1 (d v × s v )). (3)</formula><p>The problem in Eq. ( <ref type="formula" target="#formula_30">15</ref>) aims to undate the optimal weight for each view. It has closed form solution and the computational complexity is O(n × k × V ).</p><p>(4) The problem in Eq. ( <ref type="formula">20</ref>) is to update S. It is updated row by row. The total computational complexity is</p><formula xml:id="formula_68">O(n × k × ∑ V v=1 (d v × s v )). In summary, the total computational complexity of ASVW is O(T × ( ∑ V v=1 d v × max{d 2 v , n × k × s v }))</formula><p>, where T is the number of iterations and it is often less than ten <ref type="bibr" target="#b27">[28]</ref>.</p><p>As for parameter determination, it seems that a lot of parameters should be determined as shown in the formulation of ASVM in Eq. ( <ref type="formula" target="#formula_20">9</ref>). Nevertheless, we set the most important parameters, e.g., α v and S as optimization variables and optimize them during the iteration. The other vital parameters that need predefined are r 1 , r 2 , λ and p.</p><p>Since parameter determination is still an open problem in the field of machine learning, we determine these parameters empirically as in previous researches. As for p, it is designed to add sparsity to measure the importance of row vector of W v . We set p = 1 as in previous approach <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> and it has been demonstrated effective in most applications <ref type="bibr" target="#b28">[29]</ref>. As for the parameters r 1 and r 2 , they play the same role as avoiding trivial solution in optimizing α and S. Thus, we set r 1 = r 2 in our implementations. As illustrated and mentioned in AMFS and AUMFS, the larger r 1 (or r 2 ) is, the more possible that all the weights tend to be equal. To reflect the difference among different views, we set r 1 = r 2 = 2 empirically in our experiments. The experimental results shown in next section also validate the effectiveness of our choice.</p><p>As for the parameter λ, it is very vital to the final performance since it is employed to balance the importance of structure characterization and feature selection. Since there is no prior information about this parameter, we determine it by grid search in a heuristic way as in previous researches <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b28">[29]</ref>. Concretely, the regularization parameter λ is tuned from {0.01, 0.1, • • • , 100}. The parameters in other algorithms are also turned by grid search. Finally, we report the best results for each algorithm. We will provide some experimental result in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we will compare ASVW with other related unsupervised methods. There are totally three groups of experiments. Since we focus on unsupervised learning here, the first group contains the clustering results of Kmeans(KM) on different data with different numbers of selected features. The second group is to measure the effectiveness of selected features directly, without the help of other added approaches. Finally, we show some results about convergence behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Description and Evaluation Metric</head><p>To validate the effectiveness of our methods, we select six benchmark datasets. They are various kinds of data arisen in many real applications with different characters and commonly used in multiple view learning. They are SensIT Vehicle <ref type="bibr" target="#b35">[36]</ref>, NUS-WIDE-OBJ <ref type="bibr" target="#b36">[37]</ref>, Animal with attribute (Animal) <ref type="bibr" target="#b37">[38]</ref>, MIRFLICKR <ref type="bibr" target="#b38">[39]</ref>, Microsoft Research Cambridge Volume 1(MSRC-v1) <ref type="bibr" target="#b39">[40]</ref> and Caltech-101 <ref type="bibr" target="#b40">[41]</ref>. Each data set has some types of features (views). We adopted a subsect whose details are summarized in Table <ref type="table" target="#tab_2">2</ref>.</p><p>SensIT contains data from wireless distributed sensor networks. It is collected from two different sensors (views), that is, acoustic and seismic sensor to record different signals for three types of vehicle (three classes) in an intelligent transportation system. We use the processed data from LIB-SVM <ref type="bibr" target="#b41">[42]</ref> and sample 10000 data for each class. Therefore, we have 300000 data samples, 2 views and 3 classes.</p><p>NUS-WIDE-OBJ contains 31 categories and 30,000 images. To avoid the influence caused by imbalance of category size, we choose the smallest 20 classes of objects, such as book, zebra, flags etc. It contains 9133 samples. Five popular pre-extracted features are adopted for our experiments, they are, color histogram (CH), block-wise color moments(CMT) <ref type="bibr" target="#b42">[43]</ref>, color correlogram (CORR) <ref type="bibr" target="#b43">[44]</ref>, edge direction histogram (EDH) <ref type="bibr" target="#b44">[45]</ref> and wavelet texture(WT) <ref type="bibr" target="#b45">[46]</ref>.</p><p>Animal consists of 30475 images of 50 animals classes. Similar to the above reason, images from the first 20 classes, including antelope, beaver and horse etc., which consists of 14112 samples, are selected. To show the effectiveness with different of descriptions, we utilize another four popular features for all images, that is, color histogram (CH), SURF <ref type="bibr" target="#b46">[47]</ref>, local self-similarity (LSS) <ref type="bibr" target="#b47">[48]</ref> and color-SIFT (RGSIFT) <ref type="bibr" target="#b48">[49]</ref>. MIRFLICKR contains 25000 images from 38 categories downloaded from the social photography site Flickr through its public API. Since it is a data set with multi-label, we select samples with only one label for evaluation. It consists of 3048 images from 12 categories. Besides, we also utilize another two famous descriptors, i.e., GIST and SIFT, to form another two views.</p><p>MSRC-v1 is commonly used for object recognition. It contains 8 classes, and each class has 30 images. Following <ref type="bibr" target="#b49">[50]</ref>, we select 7 classes composed of tree, building, airplane, cow, face, car, bicycle. To get the different views, we extract LBP, HOG, GIST, CMT, CENTRIST <ref type="bibr" target="#b50">[51]</ref> and SIFT with visual features from each image. Caltech-7 data set is an object recognition data set containing 8677 images, belonging to 101 categories. We chose the widely used 7 classes, i.e. Faces, Motorbikes, Dolla-Bill, Garfield, Snoopy, Stop-Sign and Windsor-Chair. Following <ref type="bibr" target="#b49">[50]</ref> , we sample the data and totally we have 441 images referred to as Caltech-7. We extract the the same visual features: LBP, HOG, GIST, CMT, CENTRIST and SIFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Setup</head><p>Since we focus on unsupervised learning here, the clustering approach is employed to evaluate the effectiveness of different feature selection approaches. In the following experiments, a popular clustering methods, i.e., K-means(KM), is employed to cluster data with selected features. We compare our proposed ASVW with different kinds of methods.</p><p>(1) KM is the representative clustering method. It is employed to cluster original multiple view data by simply connecting all features into a single view. It is the baseline.</p><p>(2) LapScor, SPEC and MRSF are representative single view feature selection approaches. They take the samples with connected features as the input. We employ them to show the effectiveness of multi-view feature selection.</p><p>(3) AMFS and AUMFS are representative multi-view feature selection approaches. We compare with them to show the effectiveness of our method.</p><p>We utilize two metrics to evaluate the performances of clustering. One is the Clustering Accuracy (ACC) defined as ACC = 1/n ∑ n i=1 δ(l i , map(c i )), where l i is the actual label and c i is the computed cluster index. δ(•) represents the δfunction, and map(•) is a function that maps each cluster index to the best class label. Another one is Normalized Mutual Information (NMI). We run each experiments for 50 times independently and report the mean results.</p><p>Besides, to evaluate the quality of selected features directly, we also employed redundancy rate (RED)-the redundancy rate contained in the selected features. It is a popular evaluation metric for feature selection. It measures the quality of selected features directly, without employing the following tasks. Assume F = {f i } is the set of selected features and X F is the data represented by the features in F. The following metric measures the redundancy rate:</p><formula xml:id="formula_69">RED(F ) = 1 |F| (|F| -1) ∑ fi,fj ∈F ,i&gt;j corr i,j , (<label>36</label></formula><formula xml:id="formula_70">)</formula><p>where |F| is the cardinality of F and corr i,j is the correlation between two features f i and f j , computed by using data points in X F . This measurement assesses the averaged correlation among all feature pairs, and a large value indicates that many selected features are correlated and thus high redundancy is expected to exist in F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison between AVSW and Other Algorithms</head><p>In this section, we first evaluate our method in a typical unsupervised task, i.e., clustering. Fig. <ref type="figure">1</ref> and Table <ref type="table" target="#tab_3">3</ref> show the clustering accuracy and NMI results of all the compared methods with different percents of selected features respectively. Due to the limitation of space, we only report three representative NMI results and the other results have similar trends. We use figure and table to show the ACC and NMI results since (1) Accuracy is the most popular criteria and we show them by figure intuitively. (2) Table is more informative and we report the NMI result with mean and standard deviation. Besides, it should be noted that KM is not a feature selection approach and it keeps a straight line in all figures. Due to the same reason, we do not report its NMI either.</p><p>In terms of the clustering accuracy, we have the following observations.</p><p>(1) Compared with other feature selection approaches, ASVW outperforms all the other feature selection methods on all data sets in most of the time. For example, on the SensIT data set, compared to the best result of all the other methods, ASVW gets more than 10% improvements in average. On the Caltech-7 data, the average improvements is also more than 10%. In terms of the NMI results, ASVW also performs the best. The above mentioned improvements can also be seen from the results in Table <ref type="table" target="#tab_3">3</ref>.</p><p>(2) In comparison with KM, which is not a feature selection approach, ASVM outperforms KM in four data sets. While on the NUS-WIDE-OBJ data set, KM achieves the best results. It has higher accuracies than all the feature selection approaches, including single view methods and multi-view approaches. The reason may be that the original representation of NUS-WIDE-OBJ data is compact enough and it is not necessary to conduct feature selection.</p><p>(3) As for the comparison between single view methods and previous multi-view approaches, the latter do not always perform better. This may be caused by the fact that previous method characterize the structures of each view data separately and combine them by simply addition operations. Our approach learns a common similarity matrix 1041-4347 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. among different views and it performs better than single view methods in most cases.</p><p>Fig. <ref type="figure" target="#fig_0">2</ref> shows the redundancy rate of the selected features of different feature selection approaches. Due to the limitation of space, we only report the results on three representative data sets.</p><p>As seen from the results, we also have the following observations. (1) Our ASVW has the smallest RED results in most cases. It means that, with the same number of selected features, our method could achieve more compact representations. (2) Compared with the results in Fig. <ref type="figure">1</ref> and Table <ref type="table" target="#tab_3">3</ref>, it seems that RED is consistent with accuracy and N-MI. Concretely, they show similar trends, even though they are different metrics concerning different aspects of feature selection performance. (3) As the number/percentage of selected features grows, the performances of feature selection methods do not increase simultaneously. It is consistent with intuition since the representation may be redundant with more features. The same trend can also been observed from Fig. <ref type="figure">1</ref> and Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Convergence Behavior</head><p>In order to verify the convergence of Algorithm 1, we present the convergence behavior curves on datasets MSRC-v1 and Caltech-7. The convergence curves are displayed in Fig. <ref type="figure" target="#fig_1">3</ref>. As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, the objective values are nonincreasing during the iterations and converge to a fixed value. Additionally, the algorithm converges within 10 iterations and our method has the fast convergence speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computational Time and Parameter Determination</head><p>To evaluate the computational complexity of our methods, we have compared ASVW with other feature selection methods on the above mentioned six benchmark data sets. These data sets are with different data scale and feature scale. Algorithms are tested on a work station with 8 processors (3.40 GHz for each) and 32.0 GB RAM memory by MATLAB implementations with MATLAB R2014a. The CPU time is shown in Table <ref type="table" target="#tab_5">4</ref>. Since all the feature selection algorithms need KM for clustering and our aim is to investigate feature selection, we have not reported its computational time.</p><p>There are several observations from the results in Table <ref type="table" target="#tab_5">4</ref>. <ref type="bibr" target="#b0">(1)</ref> Compared with other feature selection methods, the computational cost of ASVW is not the largest, although we need to learn the similarity matrix S. The reason may be that traditional methods connect all the features for  form data with high dimensionality while ASVW tackles them view by view respectively. (2) The computational complexity of different methods is dominated by different factors. For example, the number of data points n affects the computational cost of SPEC most since it needs to factorize a matrix with size n and computational complexity O(n 3 ). AUMFS needs to compute the least square problem with total features d and computational complexity O(d 3 ). Thus, the number of total features takes the largest influence on the cost of AUMFS. The performance of our method has also been influenced by these two key factors, whereas the orders are smaller.</p><p>As for the parameter determination problem, we conduct experiments on two data sets, i.e., MSRC-v1 and Caltech-7, for evaluation. We set the parameter r 1 = r 2 and vary them from {2, 4, 6, 8, 10}. Simultaneously, another important balance parameter λ is varied within the range {0.01, 0.1, 1, 10, 100}. The number of selected feature is fixed as 20% and the mean clustering accuracy of KM is employed as the evaluation criterion. The results are show in Fig. <ref type="figure">4</ref>. As we can see from the results in in Fig. <ref type="figure">4</ref>, it is clear that different combinations of parameters have different </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICATIONS TO SAR</head><p>Sports Action Recognition (SAR) is an important and challenging research topic for video analysis. The difficulty mainly lies in the high complexity in constructing visual patterns. There are a lot of interests dedicated to looking for suitable descriptors. In our paper, we try to handle this problem in another way. We use the popular descriptors and select the most discriminative features from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Methodology</head><p>Before going into the details, we would like to show the procedure briefly. In evaluation, different from the traditional problems, in sports action recognition, we can evaluate it in two aspects. In image level, we assign the extracted images the video label and report the results of images. In video level, we can also assign the label of each video according the predicted labels of images who are extracted from it by main voting. Besides clustering accuracy, considering the background of this particular application, we use Area Under the Curve (AUC), instead of NMI, for its popularity and stability.</p><p>There are two sports action video sets are presented as follows. UCF sports action data set 1 consists of about 200 video sequences at a frame resolution of 720×480, collected from various sports featured on broadcast television channels such as the BBC and ESPN. The collection represents a natural pool of actions featured in a wide range of scenes and viewpoints. UCF includes 12 actions categories: diving side, golf swinging back, golf swinging front, golf swinging side, kicking, lifting, horseback riding, running, skating, 1. http://crcv.ucf.edu/data/UCF Sports Action.php swinging bench, swinging side angle and walking. This data set is very challenging for recognizing realistic actions from videos, due to large variations in camera motion, cluttered background and illumination conditions. In our experiments, we extract 7674 key frames from 200 video clips and each frame is represented by six different views.</p><p>Olympic sports data set 2 contains 16 types of sports actions: high jump, long jump, triple jump, pole vault, basketball lay-up, bowling, tennis serve, platform diving, discus throw, hammer throw, javelin throw, shot put, springboard diving, snatch, clean and jerk, and gymnastic vault. Currently, Olympic contains 783 video sequences of athletes practicing different sports at a resolution of 480 × 640. All video sequences are obtained from YouTube and annotated their class label with the help of Amazon Mechanical Turk.</p><p>In our experiments, we extract 7830 key frames from all video sequences. Each key frame is also represented by descriptors from six different kinds of views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy Comparison</head><p>We will use clustering accuracy as the metric for evaluation. As mentioned above, we compare the performances in two aspects, i.e., image clustering and video clustering.</p><p>To avoid duplication to Fig. <ref type="figure">1</ref> and make the representation more vivid, we shows the clustering results of different methods on two data sets in Fig. <ref type="figure" target="#fig_3">5</ref> by bar figure. The results are evaluated on key frame images. Together with this, we also present the clustering accuracy on the videos themselves. The results are given in Table <ref type="table" target="#tab_8">5</ref> and Table <ref type="table" target="#tab_9">6</ref>. The most prominent performances are boldfaced.</p><p>As seen from these results, it is clear that ASVW achieves the highest accuracies in most cases, both on key frame images and videos. It indicates that our method selects the most discriminative features from multi-view data features. The reason may be the same as in previous experiments, i.e., learning the common similarity matrix automatically.</p><p>Besides, one point should be explained here is that the reported accuracy in Table <ref type="table" target="#tab_8">5</ref> is lower than the state-of-theart performance of UCF. The reasons may be (1) The aim of this application is to validate the effectiveness of ASVW by comparing its performance with other related feature selection approaches. Our method is not designed to solve this particular problem. (2) The final recognition accuracy has been highly dominated by the following clustering methods. In our work, we just employ the most common one, i.e., K-means. Certainly, we can also employ other kinds of clustering approaches. The performance may be better whereas it may cost more time. (3) In most of traditional    works, sport action accuracy is conducted in supervised case while our method is unsupervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison of AUC</head><p>Since AUC is one of the most important evaluation metric in video analysis, we also report its result in this section. Similar to the settings in the previous subsection, we also provide the AUC results on both key frame image clustering and video clustering. The corresponding results are shown in Fig. <ref type="figure" target="#fig_4">6</ref>, Table <ref type="table" target="#tab_10">7</ref> and Table <ref type="table" target="#tab_11">8</ref> respectively.</p><p>As we can see from these results, it is obvious that ASVW outperforms other feature selection approaches in most cases on the task of sports action, no matter which kind of data set and the way of evaluation. Besides, combine with the results in previous subsection, we can also note that the comparison results based on accuracy and AUC both can validate the superiority of our method. Interestingly, when the number of selected features increases within our predefined range, the improvement is significant. This may be caused by the reason that a small number of features are not enough to distinguish different action categories in videos. The performances of all methods degrade when the number of selected features is small. Moreover, due to the differences in data characteristic, the performances of different methods vary on different data sets. For example, AUMFS performs well on UCF data, while its performance is not so good on Olympic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we aim to provide insights into the unsupervised feature selection for multi-view data, as well as to facilitate the design of new similarity measures. A novel algorithm named as ASVW has been proposed. Similarity matrix for characterizing multi-view data are learned automatically. As illustrated in this paper, ASVW has been shown to be more effective in selecting features for multiview data. Moreover, this algorithm can be extended to other multi-view learning algorithms concerning similarity matrix directly. A byproduct of this paper is a series of theoretical analysis and some interesting optimization strategies. One of our future works is to systematically compare all possible extensions of the algorithms developed by different configurations of p in sparse regularization and r 1 , r 2 in exponent, including their theoretical analyses and solving strategies. Another open problem is the selection of parameter λ, which is an unsolved problem in many learning algorithms. In this paper, they are empirically determined. Additional theoretical analysis is also needed for this topic. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. RED of different methods of three data sets with different percent of selected features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Objective values of ASVW with different numbers of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>7 Fig. 4 .</head><label>74</label><figDesc>Fig.<ref type="bibr" target="#b3">4</ref>. ASVW with different selection of parameters. We set r 1 = r 2 and vary them from {2,4,6,8,10}. Simultaneously, λ is varied from {0.01,0.1,1,10,100}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Image recognition accuracies of different methods on two sports action recognition tasks with different numbers of selected features. (a) Results on the UCF Sports data; (b) Results on the Olympic Sports data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Image AUC of different methods on two sports action recognition tasks with different numbers of selected features. (a) Results on the UCF Sports data; (b) Results on the Olympic Sports data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Chenping</head><label></label><figDesc>Hou (M'12) received the Ph.D. degrees from the National University of Defense Technology, Changsha, China in 2009. He is currently an Associate Professor with the College of Science of the same university. He has authored several papers in journals and conferences, such as the IEEE TNNLS/TNN, IEEE TSMCB/TCB, IEEE TIP, PR, the IJCAI and AAAI. His current research interests include machine learning, data mining, and computer vision. Feiping Nie received the Ph.D. degree in Computer Science from Tsinghua University, China in 2009. His research interests are machine learning and its application fields. He has published more than 100 papers in the top journals and conferences, including TPAMI, IJCV, TIP, TNNL-S/TNN, TKDE, ICML, NIPS, KDD, IJCAI, AAAI, ICCV, CVPR, SIGIR, ACM MM etc. He is serving as AE or PC member for several prestigious journals and conferences in the related fields. Hong Tao is a PhD condidate with the College of Science at the National University of Denfense Technology, Changsha, China. She earned her B.S. degree and M.S. degree from the same university in 2012 and 2014. Her research interests include machine learning, system science and data mining. Dongyun Yi received the B.S. degree from Nankai University, Tianjin, China, and the M.S. and Ph.D. degrees from the National University of Defense Technology, Changsha, China. He was a visiting researcher with the University of Warwick, Coventry, U.K., in 2008. He is a Professor with the College of Science, National University of Defense Technology. His current research interests include statistics, systems science, and data mining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 Notations</head><label>1</label><figDesc></figDesc><table><row><cell>Notations</cell><cell>Descriptions</cell></row><row><cell>d</cell><cell>Total dimensionality of data</cell></row><row><cell>dv</cell><cell>The dimensionality of the v-th view</cell></row><row><cell>n</cell><cell>Data size</cell></row><row><cell>V</cell><cell>The number of views</cell></row><row><cell>αv</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Sort these scores and select the largest s values. The corresponding indexes form the selected feature index set {r 1 , r 2 , • • • , r s }.</figDesc><table><row><cell>as in LE;</cell></row><row><cell>Repeat</cell></row><row><cell>2: Update W v by solving the problem in Eq. (27);</cell></row><row><cell>3: Update D (v) using Eq. (23);</cell></row><row><cell>4: Update α using Eq. (15);</cell></row><row><cell>5: Update S using Eq. (20);</cell></row><row><cell>Until converges</cell></row><row><cell>Feature Selection</cell></row><row><cell>6: Compute the scores for all features using the 2-norm of</cell></row><row><cell>the rows of W v ;</cell></row><row><cell>7:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Details of the multi-view datasets used in our experiments (feature type (dimensionality)).</figDesc><table><row><cell>Feature type</cell><cell>SensIT</cell><cell>NUS-WIDE-OBJ</cell><cell>Animal</cell><cell>MIRFLICKR</cell><cell>MSRC-v1</cell><cell>Caltech-7</cell></row><row><cell>1</cell><cell>Acoustic(50)</cell><cell>CH(64)</cell><cell>CQ(2688)</cell><cell>GIST(512)</cell><cell>LBP(256)</cell><cell>LBP(256)</cell></row><row><cell>2</cell><cell>Seismic(50)</cell><cell>CMT(225)</cell><cell>SURF(2000)</cell><cell>DenseSIFT(1000)</cell><cell>HOG(100)</cell><cell>HOG(100)</cell></row><row><cell>3</cell><cell>-</cell><cell>CORR(144)</cell><cell>LSS(200)</cell><cell>-</cell><cell>GIST(512)</cell><cell>GIST(512)</cell></row><row><cell>4</cell><cell>-</cell><cell>EDH(73)</cell><cell>RGSIFT(2000)</cell><cell>-</cell><cell>CMT(48)</cell><cell>CMT(48)</cell></row><row><cell>5</cell><cell>-</cell><cell>WT(128)</cell><cell>-</cell><cell>-</cell><cell>CENTRIST(1302)</cell><cell>CENTRIST(1302)</cell></row><row><cell>6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>DoG-SIFT(210)</cell><cell>DoG-SIFT(441)</cell></row><row><cell>Data points</cell><cell>30000</cell><cell>9133</cell><cell>14122</cell><cell>3048</cell><cell>210</cell><cell>441</cell></row><row><cell>Classes</cell><cell>3</cell><cell>20</cell><cell>20</cell><cell>12</cell><cell>7</cell><cell>7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 NMI</head><label>3</label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2017.2681670, IEEE Transactions on Knowledge and Data Engineering MULTI-VIEW UNSUPERVISED FEATURE SELECTION WITH ADAPTIVE SIMILARITY AND VIEW WEIGHT 9 Fig. 1. Clustering accuracy of different methods on six different data sets with different percents of selected features. of different methods on three data sets with different percents of selected features.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.155</cell></row><row><cell></cell><cell>0.46</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.44</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.15</cell></row><row><cell>Clustering Accuracy</cell><cell>0.3 0.34 0.36 0.38 0.42 0.4 0.32</cell><cell></cell><cell></cell><cell></cell><cell>KM LapScor SPEC</cell><cell>Clustering Accuracy</cell><cell>0.17 0.15 0.16 0.14</cell><cell></cell><cell></cell><cell></cell><cell>KM LapScor SPEC</cell><cell>Clustering Accuracy</cell><cell cols="2">0.145 0.13 0.135 0.14 0.125</cell><cell>KM LapScor SPEC</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MRSF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MRSF</cell><cell></cell><cell></cell><cell></cell><cell>MRSF</cell></row><row><cell></cell><cell>0.28</cell><cell></cell><cell></cell><cell></cell><cell>AMFS AUMFS</cell><cell></cell><cell>0.13</cell><cell></cell><cell></cell><cell></cell><cell>AMFS AUMFS</cell><cell></cell><cell></cell><cell>0.12</cell><cell>AMFS AUMFS</cell></row><row><cell></cell><cell>0.26</cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell></row><row><cell></cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell></cell><cell>0.1 0.12</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell></cell><cell cols="2">0.1 0.115</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Percent of Selected Features</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) SensIT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) NUS-WIDE-OBJ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(c) Animal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell></row><row><cell></cell><cell>0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.54</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.52</cell></row><row><cell></cell><cell>0.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell></row><row><cell></cell><cell>0.34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Clustering Accuracy</cell><cell>0.28 0.3 0.32 0.26</cell><cell></cell><cell></cell><cell></cell><cell>KM LapScor</cell><cell>Clustering Accuracy</cell><cell>0.44 0.46 0.48 0.5</cell><cell></cell><cell></cell><cell></cell><cell>KM LapScor</cell><cell cols="2">Clustering Accuracy</cell><cell>0.42 0.44 0.46 0.48</cell><cell>KM LapScor</cell></row><row><cell></cell><cell>0.24</cell><cell></cell><cell></cell><cell></cell><cell>SPEC MRSF</cell><cell></cell><cell>0.42</cell><cell></cell><cell></cell><cell></cell><cell>SPEC MRSF</cell><cell></cell><cell></cell><cell>0.4</cell><cell>SPEC MRSF</cell></row><row><cell></cell><cell>0.22</cell><cell></cell><cell></cell><cell></cell><cell>AMFS AUMFS</cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell>AMFS AUMFS</cell><cell></cell><cell></cell><cell>0.38</cell><cell>AMFS AUMFS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell><cell></cell><cell></cell><cell></cell><cell>ASVW</cell></row><row><cell></cell><cell>0.1 0.2</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell></cell><cell>0.1 0.38</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell></cell><cell></cell><cell>0.1 0.36</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Percent of Selected Features</cell></row><row><cell></cell><cell></cell><cell cols="3">(d) MIRFLICKR</cell><cell></cell><cell></cell><cell></cell><cell cols="3">(e) MSRC-v1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(f) Caltech-7</cell></row><row><cell cols="2">dataset</cell><cell></cell><cell>s</cell><cell></cell><cell>LapScor</cell><cell></cell><cell></cell><cell>SPEC</cell><cell></cell><cell>MRSF</cell><cell></cell><cell cols="3">AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell cols="2">SensIT</cell><cell></cell><cell cols="2">s = 10%</cell><cell>0.4107±0.0272</cell><cell></cell><cell cols="2">0.4158±0.0264</cell><cell></cell><cell cols="2">0.4172±0.0311</cell><cell cols="3">0.2517±0.0059</cell><cell>0.4340±0.0303</cell><cell>0.4137±0.0284</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 15%</cell><cell>0.4170±0.0283</cell><cell></cell><cell cols="2">0.4162±0.0287</cell><cell></cell><cell cols="2">0.4154±0.0256</cell><cell cols="3">0.2509±0.0051</cell><cell>0.4169±0.0366</cell><cell>0.4150±0.0251</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 20%</cell><cell>0.4093±0.0221</cell><cell></cell><cell cols="2">0.4238±0.0283</cell><cell></cell><cell cols="2">0.4073±0.0234</cell><cell cols="3">0.2517±0.0058</cell><cell>0.4183±0.0338</cell><cell>0.4253±0.0256</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 25%</cell><cell>0.4169±0.0272</cell><cell></cell><cell cols="2">0.4212±0.0289</cell><cell></cell><cell cols="2">0.4242±0.0340</cell><cell cols="3">0.2513±0.0060</cell><cell>0.4160±0.0258</cell><cell>0.4441±0.0367</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 30%</cell><cell>0.4217±0.0306</cell><cell></cell><cell cols="2">0.4208±0.0296</cell><cell></cell><cell cols="2">0.4181±0.0284</cell><cell cols="3">0.2514±0.0053</cell><cell>0.4198±0.0279</cell><cell>0.4816±0.0399</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 35%</cell><cell>0.4075±0.0225</cell><cell></cell><cell cols="2">0.4096±0.0218</cell><cell></cell><cell cols="2">0.4131±0.0278</cell><cell cols="3">0.2515±0.0056</cell><cell>0.4055±0.0334</cell><cell>0.4905±0.0250</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 40%</cell><cell>0.4144±0.0240</cell><cell></cell><cell cols="2">0.4211±0.0290</cell><cell></cell><cell cols="2">0.4115±0.0222</cell><cell cols="3">0.2516±0.0043</cell><cell>0.4190±0.0313</cell><cell>0.4819±0.0347</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 45%</cell><cell>0.4124±0.0213</cell><cell></cell><cell cols="2">0.4120±0.0232</cell><cell></cell><cell cols="2">0.4118±0.0286</cell><cell cols="3">0.2512±0.0051</cell><cell>0.4080±0.0283</cell><cell>0.4961±0.0313</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 50%</cell><cell>0.4230±0.0310</cell><cell></cell><cell cols="2">0.4188±0.0278</cell><cell></cell><cell cols="2">0.4119±0.0263</cell><cell cols="3">0.2512±0.0049</cell><cell>0.4148±0.0304</cell><cell>0.4967±0.0348</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 55%</cell><cell>0.4121±0.0268</cell><cell></cell><cell cols="2">0.4160±0.0370</cell><cell></cell><cell cols="2">0.4239±0.0293</cell><cell cols="3">0.2514±0.0054</cell><cell>0.4217±0.0303</cell><cell>0.5074±0.0310</cell></row><row><cell cols="2">dataset</cell><cell></cell><cell>s</cell><cell></cell><cell>LapScor</cell><cell></cell><cell></cell><cell>SPEC</cell><cell></cell><cell>MRSF</cell><cell></cell><cell cols="3">AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell cols="3">NUS-WIDE-OBJ</cell><cell cols="2">s = 10%</cell><cell>0.0959±0.0036</cell><cell></cell><cell cols="2">0.0939±0.0025</cell><cell></cell><cell cols="2">0.1126±0.0035</cell><cell cols="3">0.0919±0.0013</cell><cell>0.0962±0.0026</cell><cell>0.1185±0.0034</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 15%</cell><cell>0.1050±0.0038</cell><cell></cell><cell cols="2">0.1004±0.0034</cell><cell></cell><cell cols="2">0.1193±0.0034</cell><cell cols="3">0.1031±0.0026</cell><cell>0.0982±0.0026</cell><cell>0.1255±0.0036</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 20%</cell><cell>0.1045±0.0042</cell><cell></cell><cell cols="2">0.1059±0.0040</cell><cell></cell><cell cols="2">0.1200±0.0037</cell><cell cols="3">0.1096±0.0023</cell><cell>0.1084±0.0027</cell><cell>0.1290±0.0030</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 25%</cell><cell>0.1085±0.0038</cell><cell></cell><cell cols="2">0.1116±0.0026</cell><cell></cell><cell cols="2">0.1246±0.0038</cell><cell cols="3">0.1108±0.0027</cell><cell>0.1116±0.0035</cell><cell>0.1341±0.0029</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 30%</cell><cell>0.1098±0.0044</cell><cell></cell><cell cols="2">0.1109±0.0036</cell><cell></cell><cell cols="2">0.1309±0.0047</cell><cell cols="3">0.1132±0.0028</cell><cell>0.1152±0.0031</cell><cell>0.1382±0.0035</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 35%</cell><cell>0.1103±0.0027</cell><cell></cell><cell cols="2">0.1116±0.0039</cell><cell></cell><cell cols="2">0.1333±0.0038</cell><cell cols="3">0.1165±0.0032</cell><cell>0.1178±0.0038</cell><cell>0.1408±0.0030</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 40%</cell><cell>0.1114±0.0030</cell><cell></cell><cell cols="2">0.1136±0.0032</cell><cell></cell><cell cols="2">0.1323±0.0051</cell><cell cols="3">0.1165±0.0032</cell><cell>0.1188±0.0032</cell><cell>0.1437±0.0030</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 45%</cell><cell>0.1163±0.0038</cell><cell></cell><cell cols="2">0.1173±0.0040</cell><cell></cell><cell cols="2">0.1339±0.0043</cell><cell cols="3">0.1163±0.0030</cell><cell>0.1220±0.0035</cell><cell>0.1437±0.0033</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 50%</cell><cell>0.1138±0.0037</cell><cell></cell><cell cols="2">0.1157±0.0035</cell><cell></cell><cell cols="2">0.1361±0.0038</cell><cell cols="3">0.1146±0.0029</cell><cell>0.1219±0.0033</cell><cell>0.1430±0.0036</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 55%</cell><cell>0.1177±0.0035</cell><cell></cell><cell cols="2">0.1156±0.0032</cell><cell></cell><cell cols="2">0.1385±0.0040</cell><cell cols="3">0.1129±0.0025</cell><cell>0.1201±0.0034</cell><cell>0.1449±0.0037</cell></row><row><cell cols="2">dataset</cell><cell></cell><cell>s</cell><cell></cell><cell>LapScor</cell><cell></cell><cell></cell><cell>SPEC</cell><cell></cell><cell>MRSF</cell><cell></cell><cell cols="3">AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell cols="2">Caltech-7</cell><cell></cell><cell cols="2">s = 10%</cell><cell>0.3970±0.0112</cell><cell></cell><cell cols="2">0.4003±0.0114</cell><cell></cell><cell cols="2">0.3961±0.0136</cell><cell cols="3">0.3766±0.0046</cell><cell>0.3589±0.0266</cell><cell>0.4110±0.0176</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 15%</cell><cell>0.3994±0.0109</cell><cell></cell><cell cols="2">0.4005±0.0134</cell><cell></cell><cell cols="2">0.4107±0.0271</cell><cell cols="3">0.3767±0.0054</cell><cell>0.3689±0.0098</cell><cell>0.4305±0.0120</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 20%</cell><cell>0.3998±0.0151</cell><cell></cell><cell cols="2">0.4021±0.0122</cell><cell></cell><cell cols="2">0.4006±0.0136</cell><cell cols="3">0.3755±0.0071</cell><cell>0.3940±0.0157</cell><cell>0.4473±0.0107</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 25%</cell><cell>0.4003±0.0149</cell><cell></cell><cell cols="2">0.4016±0.0131</cell><cell></cell><cell cols="2">0.3993±0.0129</cell><cell cols="3">0.3755±0.0086</cell><cell>0.4228±0.0199</cell><cell>0.4472±0.0100</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 30%</cell><cell>0.3981±0.0112</cell><cell></cell><cell cols="2">0.4032±0.0116</cell><cell></cell><cell cols="2">0.4009±0.0136</cell><cell cols="3">0.3740±0.0072</cell><cell>0.3997±0.0131</cell><cell>0.4393±0.0155</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 35%</cell><cell>0.4003±0.0107</cell><cell></cell><cell cols="2">0.4028±0.0117</cell><cell></cell><cell cols="2">0.4013±0.0108</cell><cell cols="3">0.3757±0.0058</cell><cell>0.3954±0.0164</cell><cell>0.4497±0.0172</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 40%</cell><cell>0.4003±0.0127</cell><cell></cell><cell cols="2">0.3999±0.0135</cell><cell></cell><cell cols="2">0.4029±0.0115</cell><cell cols="3">0.3748±0.0067</cell><cell>0.3977±0.0154</cell><cell>0.4481±0.0226</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 45%</cell><cell>0.3996±0.0117</cell><cell></cell><cell cols="2">0.3990±0.0114</cell><cell></cell><cell cols="2">0.3998±0.0119</cell><cell cols="3">0.3757±0.0055</cell><cell>0.4016±0.0128</cell><cell>0.4606±0.0149</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 50%</cell><cell>0.4015±0.0127</cell><cell></cell><cell cols="2">0.4029±0.0121</cell><cell></cell><cell cols="2">0.3991±0.0137</cell><cell cols="3">0.3754±0.0057</cell><cell>0.3966±0.0134</cell><cell>0.4629±0.0169</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">s = 55%</cell><cell>0.4012±0.0125</cell><cell></cell><cell cols="2">0.3987±0.0125</cell><cell></cell><cell cols="2">0.3996±0.0174</cell><cell cols="3">0.3755±0.0051</cell><cell>0.4009±0.0130</cell><cell>0.4553±0.0194</cell></row></table><note><p>1041-4347 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Computational time (in seconds) of different methods on six data sets. = r 2 is small, ASVW achieves the prominent performances. It may be caused by the fact that the larger r 1 and r 2 indicate the less attentions on the diversity of different views. Besides, the two data sets have different optimal λ since they have different data characteristics.</figDesc><table><row><cell>dataset</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell>SensIT</cell><cell>816.8524</cell><cell>41703.6693</cell><cell>542.7118</cell><cell>8553.3320</cell><cell>4011.7841</cell><cell>28621.8635</cell></row><row><cell>NUS-WIDE-OBJ</cell><cell>16.0369</cell><cell>1801.3435</cell><cell>23.3065</cell><cell>288.1650</cell><cell>499.4372</cell><cell>756.4644</cell></row><row><cell>Animal</cell><cell>130.9472</cell><cell>14641.6298</cell><cell>1774.0121</cell><cell>8528.5122</cell><cell>38829.9593</cell><cell>2800.0693</cell></row><row><cell>MIRFLICKR</cell><cell>4.7580</cell><cell>136.3604</cell><cell>22.5421</cell><cell>60.8715</cell><cell>498.2827</cell><cell>91.9937</cell></row><row><cell>MSRC-v1</cell><cell>0.7488</cell><cell>2.2308</cell><cell>25.8181</cell><cell>9.2976</cell><cell>17.7373</cell><cell>11.7468</cell></row><row><cell>Caltech-7</cell><cell>0.8112</cell><cell>5.5068</cell><cell>29.5153</cell><cell>19.5157</cell><cell>57.1743</cell><cell>14.6328</cell></row><row><cell cols="4">impact on the final results. It demonstrates the importance</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of parameter determination. Besides, it seems that when</cell><cell></cell><cell></cell><cell></cell></row><row><cell>r 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TKDE.2017.2681670, IEEE Transactions on Knowledge and Data Engineering MULTI-VIEW UNSUPERVISED FEATURE SELECTION WITH ADAPTIVE SIMILARITY AND VIEW WEIGHT</figDesc><table><row><cell></cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.35 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.15 0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.02 0.04 0.06 0.08 0.2</cell><cell>0.1</cell><cell cols="2">0.12 0.14 0.16 0.18</cell><cell>0.2</cell><cell>0.22</cell><cell cols="2">0.02 0.04 0.06 0.08 0</cell><cell>0.1</cell><cell cols="2">0.12 0.14 0.16 0.18</cell><cell>0.2</cell><cell>0.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Percent of Selected Features</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">(a) UCF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Olympic</cell><cell></cell><cell></cell></row></table><note><p>2. http://vision.standford.edu/Datasets/OlympicSports/ 1041-4347 (c) 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Video recognition accuracies of different methods on the UCF Sports data set.</figDesc><table><row><cell>s</cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell>s = 4%</cell><cell>0.4395±0.0273</cell><cell>0.4154±0.0387</cell><cell>0.4141±0.0364</cell><cell>0.4100±0.0190</cell><cell>0.3382±0.0266</cell><cell>0.4657±0.0351</cell><cell>0.5133±0.0352</cell></row><row><cell>s = 6%</cell><cell>0.4395±0.0273</cell><cell>0.4457±0.0380</cell><cell>0.4459±0.0365</cell><cell>0.4526±0.0265</cell><cell>0.3333±0.0257</cell><cell>0.4882±0.0322</cell><cell>0.5166±0.0377</cell></row><row><cell>s = 8%</cell><cell>0.4395±0.0273</cell><cell>0.4387±0.0383</cell><cell>0.4389±0.0373</cell><cell>0.4325±0.0264</cell><cell>0.3375±0.0307</cell><cell>0.5034±0.0320</cell><cell>0.5089±0.0360</cell></row><row><cell>s = 10%</cell><cell>0.4395±0.0273</cell><cell>0.4477±0.0358</cell><cell>0.4489±0.0383</cell><cell>0.4423±0.0261</cell><cell>0.3348±0.0242</cell><cell>0.4944±0.0361</cell><cell>0.4969±0.0322</cell></row><row><cell>s = 12%</cell><cell>0.4395±0.0273</cell><cell>0.4454±0.0365</cell><cell>0.4562±0.0291</cell><cell>0.4495±0.0298</cell><cell>0.3395±0.0275</cell><cell>0.4984±0.0349</cell><cell>0.4998±0.0403</cell></row><row><cell>s = 14%</cell><cell>0.4395±0.0273</cell><cell>0.4507±0.0375</cell><cell>0.4436±0.0349</cell><cell>0.4830±0.0416</cell><cell>0.3313±0.0258</cell><cell>0.5102±0.0273</cell><cell>0.5111±0.0320</cell></row><row><cell>s = 16%</cell><cell>0.4395±0.0273</cell><cell>0.4357±0.0306</cell><cell>0.4543±0.0336</cell><cell>0.4941±0.0404</cell><cell>0.3380±0.0250</cell><cell>0.4885±0.0214</cell><cell>0.5082±0.0302</cell></row><row><cell>s = 18%</cell><cell>0.4395±0.0273</cell><cell>0.4430±0.0332</cell><cell>0.4405±0.0379</cell><cell>0.4903±0.0425</cell><cell>0.3364±0.0261</cell><cell>0.4567±0.0204</cell><cell>0.4970±0.0345</cell></row><row><cell>s = 20%</cell><cell>0.4395±0.0273</cell><cell>0.4421±0.0353</cell><cell>0.4400±0.0335</cell><cell>0.4926±0.0446</cell><cell>0.3331±0.0253</cell><cell>0.4033±0.0235</cell><cell>0.4943±0.0377</cell></row><row><cell>s = 22%</cell><cell>0.4395±0.0273</cell><cell>0.4525±0.0310</cell><cell>0.4551±0.0290</cell><cell>0.5161±0.0396</cell><cell>0.3369±0.0263</cell><cell>0.4031±0.0244</cell><cell>0.5161±0.0461</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Video recognition accuracies of different methods on the Olympic Sports data set.</figDesc><table><row><cell>s</cell><cell></cell><cell></cell><cell>KM</cell><cell></cell><cell>LapScor</cell><cell></cell><cell></cell><cell cols="2">SPEC</cell><cell cols="2">MRSF</cell><cell cols="2">AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell cols="2">s = 4%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2499±0.0126</cell><cell cols="3">0.2525±0.0125</cell><cell cols="2">0.2364±0.0154</cell><cell cols="3">0.2479±0.0103</cell><cell>0.2328±0.0067</cell><cell>0.2621±0.0182</cell></row><row><cell cols="2">s = 6%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2551±0.0153</cell><cell cols="3">0.2520±0.0128</cell><cell cols="2">0.2505±0.0177</cell><cell cols="3">0.2525±0.0109</cell><cell>0.2415±0.0093</cell><cell>0.2740±0.0182</cell></row><row><cell cols="2">s = 8%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2489±0.0138</cell><cell cols="3">0.2548±0.0148</cell><cell cols="2">0.2533±0.0159</cell><cell cols="3">0.2590±0.0133</cell><cell>0.2492±0.0119</cell><cell>0.2779±0.0191</cell></row><row><cell cols="2">s = 10%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2593±0.0144</cell><cell cols="3">0.2605±0.0161</cell><cell cols="2">0.2623±0.0185</cell><cell cols="3">0.2626±0.0146</cell><cell>0.2481±0.0150</cell><cell>0.2930±0.0179</cell></row><row><cell cols="2">s = 12%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2602±0.0128</cell><cell cols="3">0.2614±0.0138</cell><cell cols="2">0.2616±0.0165</cell><cell cols="3">0.2610±0.0156</cell><cell>0.2430±0.0148</cell><cell>0.3018±0.0183</cell></row><row><cell cols="2">s = 14%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2634±0.0152</cell><cell cols="3">0.2643±0.0145</cell><cell cols="2">0.2745±0.0182</cell><cell cols="3">0.2678±0.0140</cell><cell>0.2484±0.0153</cell><cell>0.3058±0.0153</cell></row><row><cell cols="2">s = 16%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2689±0.0158</cell><cell cols="3">0.2662±0.0155</cell><cell cols="2">0.2689±0.0180</cell><cell cols="3">0.2701±0.0162</cell><cell>0.2532±0.0145</cell><cell>0.3110±0.0180</cell></row><row><cell cols="2">s = 18%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2643±0.0136</cell><cell cols="3">0.2627±0.0129</cell><cell cols="2">0.2749±0.0184</cell><cell cols="3">0.2736±0.0158</cell><cell>0.2497±0.0177</cell><cell>0.3093±0.0161</cell></row><row><cell cols="2">s = 20%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2649±0.0137</cell><cell cols="3">0.2682±0.0137</cell><cell cols="2">0.2747±0.0199</cell><cell cols="3">0.2720±0.0184</cell><cell>0.2489±0.0171</cell><cell>0.3151±0.0181</cell></row><row><cell cols="2">s = 22%</cell><cell cols="2">0.2595±0.0190</cell><cell cols="3">0.2712±0.0198</cell><cell cols="3">0.2689±0.0176</cell><cell cols="2">0.2801±0.0236</cell><cell cols="3">0.2711±0.0193</cell><cell>0.2484±0.0172</cell><cell>0.3053±0.0201</cell></row><row><cell></cell><cell>0.76</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell></cell><cell>AUMFS</cell><cell>ASVW</cell><cell></cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell></cell><cell>0.74</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.63</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.62</cell><cell></cell><cell></cell></row><row><cell>AUC</cell><cell>0.66 0.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>AUC</cell><cell>0.6 0.61</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.59</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.58</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">0.02 0.04 0.06 0.08 0.58</cell><cell>0.1</cell><cell cols="3">0.12 0.14 0.16 0.18</cell><cell>0.2</cell><cell>0.22</cell><cell></cell><cell cols="2">0.02 0.04 0.06 0.08 0.57</cell><cell>0.1</cell><cell>0.12 0.14 0.16 0.18</cell><cell>0.2</cell><cell>0.22</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Percent of Selected Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Percent of Selected Features</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) UCF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Olympic</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>Video clustering AUC of different methods on the UCF Sports data set.</figDesc><table><row><cell>s</cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell>s = 4%</cell><cell>0.4343±0.0205</cell><cell>0.4271±0.0195</cell><cell>0.4205±0.0176</cell><cell>0.4312±0.0144</cell><cell>0.2990±0.0153</cell><cell>0.4650±0.0280</cell><cell>0.4579±0.0161</cell></row><row><cell>s = 6%</cell><cell>0.4343±0.0205</cell><cell>0.4265±0.0221</cell><cell>0.4239±0.0233</cell><cell>0.4267±0.0221</cell><cell>0.3008±0.0127</cell><cell>0.3338±0.0254</cell><cell>0.4880±0.0215</cell></row><row><cell>s = 8%</cell><cell>0.4343±0.0205</cell><cell>0.4265±0.0232</cell><cell>0.4197±0.0227</cell><cell>0.4480±0.0196</cell><cell>0.3010±0.0107</cell><cell>0.3673±0.0256</cell><cell>0.4914±0.0200</cell></row><row><cell>s = 10%</cell><cell>0.4343±0.0205</cell><cell>0.4280±0.0206</cell><cell>0.4291±0.0195</cell><cell>0.4911±0.0185</cell><cell>0.3002±0.0118</cell><cell>0.3451±0.0312</cell><cell>0.4955±0.0214</cell></row><row><cell>s = 12%</cell><cell>0.4343±0.0205</cell><cell>0.4292±0.0204</cell><cell>0.4298±0.0191</cell><cell>0.5135±0.0201</cell><cell>0.2995±0.0113</cell><cell>0.4217±0.0289</cell><cell>0.5243±0.0222</cell></row><row><cell>s = 14%</cell><cell>0.4343±0.0205</cell><cell>0.4241±0.0176</cell><cell>0.4234±0.0186</cell><cell>0.4246±0.0203</cell><cell>0.2974±0.0118</cell><cell>0.4519±0.0258</cell><cell>0.5172±0.0198</cell></row><row><cell>s = 16%</cell><cell>0.4343±0.0205</cell><cell>0.4326±0.0226</cell><cell>0.4287±0.0193</cell><cell>0.4222±0.0191</cell><cell>0.3016±0.0108</cell><cell>0.4494±0.0151</cell><cell>0.5115±0.0221</cell></row><row><cell>s = 18%</cell><cell>0.4343±0.0205</cell><cell>0.4255±0.0253</cell><cell>0.4321±0.0196</cell><cell>0.4282±0.0234</cell><cell>0.3037±0.0132</cell><cell>0.5164±0.0116</cell><cell>0.5249±0.0181</cell></row><row><cell>s = 20%</cell><cell>0.4343±0.0205</cell><cell>0.4320±0.0201</cell><cell>0.4344±0.0224</cell><cell>0.4277±0.0313</cell><cell>0.3000±0.0137</cell><cell>0.5129±0.0165</cell><cell>0.5233±0.0213</cell></row><row><cell>s = 22%</cell><cell>0.4343±0.0205</cell><cell>0.4311±0.0201</cell><cell>0.4268±0.0218</cell><cell>0.4330±0.0257</cell><cell>0.2979±0.0116</cell><cell>0.4261±0.0149</cell><cell>0.5040±0.0221</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Video clustering AUC of different methods on the Olympic Sports data set.</figDesc><table><row><cell>s</cell><cell>KM</cell><cell>LapScor</cell><cell>SPEC</cell><cell>MRSF</cell><cell>AMFS</cell><cell>AUMFS</cell><cell>ASVW</cell></row><row><cell>s = 4%</cell><cell>0.6186±0.0152</cell><cell>0.6050±0.0140</cell><cell>0.6058±0.0120</cell><cell>0.6010±0.0118</cell><cell>0.6166±0.0133</cell><cell>0.6026±0.0140</cell><cell>0.6222±0.0147</cell></row><row><cell>s = 6%</cell><cell>0.6186±0.0152</cell><cell>0.6128±0.0114</cell><cell>0.6113±0.0113</cell><cell>0.6017±0.0138</cell><cell>0.6129±0.0136</cell><cell>0.6009±0.0152</cell><cell>0.6290±0.0179</cell></row><row><cell>s = 8%</cell><cell>0.6186±0.0152</cell><cell>0.6112±0.0101</cell><cell>0.6114±0.0141</cell><cell>0.6063±0.0163</cell><cell>0.6096±0.0140</cell><cell>0.5980±0.0148</cell><cell>0.6216±0.0138</cell></row><row><cell>s = 10%</cell><cell>0.6186±0.0152</cell><cell>0.6124±0.0088</cell><cell>0.6176±0.0134</cell><cell>0.6123±0.0176</cell><cell>0.6155±0.0126</cell><cell>0.6010±0.0139</cell><cell>0.6288±0.0174</cell></row><row><cell>s = 12%</cell><cell>0.6186±0.0152</cell><cell>0.6133±0.0130</cell><cell>0.6196±0.0137</cell><cell>0.6167±0.0183</cell><cell>0.6136±0.0138</cell><cell>0.6024±0.0132</cell><cell>0.6282±0.0177</cell></row><row><cell>s = 14%</cell><cell>0.6186±0.0152</cell><cell>0.6190±0.0127</cell><cell>0.6212±0.0135</cell><cell>0.6248±0.0214</cell><cell>0.6201±0.0125</cell><cell>0.6017±0.0136</cell><cell>0.6314±0.0133</cell></row><row><cell>s = 16%</cell><cell>0.6186±0.0152</cell><cell>0.6211±0.0121</cell><cell>0.6213±0.0129</cell><cell>0.6242±0.0197</cell><cell>0.6219±0.0116</cell><cell>0.6014±0.0149</cell><cell>0.6324±0.0158</cell></row><row><cell>s = 18%</cell><cell>0.6186±0.0152</cell><cell>0.6173±0.0126</cell><cell>0.6201±0.0131</cell><cell>0.6286±0.0199</cell><cell>0.6240±0.0121</cell><cell>0.6066±0.0143</cell><cell>0.6360±0.0135</cell></row><row><cell>s = 20%</cell><cell>0.6186±0.0152</cell><cell>0.6163±0.0143</cell><cell>0.6207±0.0132</cell><cell>0.6255±0.0140</cell><cell>0.6210±0.0135</cell><cell>0.6057±0.0122</cell><cell>0.6380±0.0124</cell></row><row><cell>s = 22%</cell><cell>0.6186±0.0152</cell><cell>0.6224±0.0130</cell><cell>0.6235±0.0169</cell><cell>0.6275±0.0182</cell><cell>0.6207±0.0138</cell><cell>0.6058±0.0145</cell><cell>0.6348±0.0102</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by NSF China (No. 61473302, 61503396).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiresolution grayand rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelli</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Torralba</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semisupervised feature selection via spline regression for video semantic recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse canonical correlation analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="331" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex multiview subspace learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse unsupervised dimensionality reduction for multiple view data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1485" to="1496" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact and discriminative descriptor inference using multi-cues</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5114" to="5126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An RKHS for multi-view learning and manifold co-regularization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">When does cotraining work in real data?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="788" to="799" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiple view semisupervised dimensionality reduction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="720" to="730" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active learning with multiple views</title>
		<author>
			<persName><forename type="first">I</forename><surname>Muslea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Minton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Knoblock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artifical Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="203" to="233" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-view active learning in the nonrealizable case</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2388" to="2396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiple-view multiple-learner active learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3113" to="3119" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-view ensemble manifold regularization for 3d object recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="395" to="405" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view discriminant transfer learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno>abs/1304.5634</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey of multi-view machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2031" to="2038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptive multi-view feature selection for human motion retrieval</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="691" to="701" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adaptive unsupervised multi-view feature selection for visual concept recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="343" to="357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An introduction to variable and feature selection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1157" to="1182" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Laplacian score for feature selection</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="507" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spectral feature selection for supervised and unsupervised learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1151" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient spectral feature selection with minimum redundancy</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised feature selection for multi-view data in social media</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient and robust feature selection via joint ℓ 2,1 -norms minimization</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1813" to="1821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint embedding learning and sparse regression: A framework for unsupervised feature selection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spectral regression: A unified approach for sparse subspace learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="73" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Locality preserving projections</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Clustering and projected clustering with adaptive neighbors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="977" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective discriminative feature selection with nontrivial solution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vehicle classification in distributed sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="826" to="838" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Nuswide: A real-world web image database from national university of singapore</title>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<editor>CIVR</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to detect unseen object classes by between-class attribute transfer</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="951" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The MIR flickr retrieval evaluation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<editor>MIR</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="39" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LOCUS: learning object classes with unsupervised segmentation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="59" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Libsvm: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Similarity of color images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Stricker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Orengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Storage and Retrieval for Image and Video Databases (SPIE)</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="381" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image indexing using color correlograms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="762" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Efficient use of local edge histogram descriptor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Won</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia 2000 Workshops</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="51" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Texture features for browsing and retrieval of image data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speeded-up robust features (SURF)</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Evaluation of color descriptors for object and scene recognition</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multi-view k-means clustering on big data</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Where am I: place instance and category recognition using spatial PACT</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
