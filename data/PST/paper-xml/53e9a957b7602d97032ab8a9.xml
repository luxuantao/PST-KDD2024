<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training</title>
				<funder>
					<orgName type="full">Intel</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><surname>Qin</surname></persName>
							<email>ecqin@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ananda</forename><surname>Samajdar</surname></persName>
							<email>anandsamajdar@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyoukjun</forename><surname>Kwon</surname></persName>
							<email>hyoukjun@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vineet</forename><surname>Nadella</surname></persName>
							<email>nadella.vineet@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sudarshan</forename><surname>Srinivasan</surname></persName>
							<email>sudarshan.srinivasan@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dipankar</forename><surname>Das</surname></persName>
							<email>dipankar.das@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bharat</forename><surname>Kaul</surname></persName>
							<email>bharat.kaul@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tushar</forename><surname>Krishna</surname></persName>
							<email>tushar@ece.gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIGMA: A Sparse and Irregular GEMM Accelerator with Flexible Interconnects for DNN Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/HPCA47549.2020.00015</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The advent of Deep Learning (DL) has radically transformed the computing industry across the entire spectrum from algorithms to circuits. As myriad application domains embrace DL, it has become synonymous with a genre of workloads across vision, speech, language, recommendations, robotics, and games. The key compute kernel within most DL workloads is general matrix-matrix multiplications (GEMMs), which appears frequently during both the forward pass (inference and training) and backward pass (training). GEMMs are a natural choice for hardware acceleration to speed up training, and have led to 2D systolic architectures like NVIDIA tensor cores and Google Tensor Processing Unit (TPU).</p><p>Unfortunately, emerging GEMMs in DL are highly irregular and sparse, which lead to poor data mappings on systolic architectures. This paper proposes SIGMA, a flexible and scalable architecture that offers high utilization of all its processing elements (PEs) regardless of kernel shape and sparsity. Within SIGMA includes a novel reduction tree microarchitecture named Forwarding Adder Network (FAN). SIGMA performs 5.7? better than systolic array architectures for irregular sparse matrices, and roughly 3? better than state-of-the-art sparse accelerators. We demonstrate an instance of SIGMA operating at 10.8 TFLOPS efficiency across arbitrary levels of sparsity, with a 65.10 mm 2 and 22.33 W footprint on a 28 nm process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given latency sensitivity and energy-efficiency demands for DNN inference, a suite of custom accelerators have been proposed <ref type="bibr" target="#b1">[3]</ref>, <ref type="bibr" target="#b7">[10]</ref>, <ref type="bibr" target="#b18">[21]</ref>, <ref type="bibr" target="#b24">[27]</ref>, <ref type="bibr" target="#b30">[33]</ref> to run trained models efficiently by capturing various forms of data reuse <ref type="bibr" target="#b7">[10]</ref>, <ref type="bibr" target="#b25">[28]</ref>. However, the right acceleration platform for training current and future models, is still an open question, which is the focus of this work.</p><p>The DL training process is extremely compute intensive. This is elucidated in an OpenAI study <ref type="bibr" target="#b3">[6]</ref>, which shows that the compute requirements for training has grown 300,000 times from AlexNet (2012) to AlphaGo Zero (2018). GPUs are currently the most popular acceleration platform in use for training; and recent research focus on parallelizing large DNN models over multiple GPU nodes. Some companies like Google and Microsoft have built their own specialized training platforms such as the cloud TPU <ref type="bibr" target="#b2">[4]</ref> and Brainwave FPGAs <ref type="bibr" target="#b11">[14]</ref> respectively.</p><p>The core compute component of DL training (and inference) is the GEMM operation <ref type="bibr">[1]</ref>. Fig. <ref type="figure" target="#fig_1">1a</ref> shows the GEMM dimensions (M, N, K) and operation; while Fig. <ref type="figure" target="#fig_1">1b</ref> shows example dimensions found in modern DL workloads. During forward pass, DNNs with fully-connected (FC) layers and multilayer perceptron (MLPs) naturally map to GEMM operations, with MK representing inputs and KN representing weights. For Convolutional Neural Networks (CNNs), GPUs remap the conv operation into a GEMM via the Im2Col operation <ref type="bibr" target="#b15">[18]</ref> or other efficient ordering operations. During the backward pass, two GEMM operations arise: MN x (KN) T and (MK) T x MN for computing the error gradient w.r.t inputs and weights respectively.</p><p>GEMMs comprises around 70% of the total compute cycles during training (as we show in Sec. II); and therefore is a primary target for hardware acceleration. State-of-the-art training accelerators use systolic arrays as the compute fabric for accelerating GEMMs -with sizes ranging from tiny 4x4 engines within each Streaming Multiprocessor in the NVIDIA Volta GPU <ref type="bibr" target="#b28">[31]</ref> to a large monolithic 128x128 engine in the Google Cloud TPU <ref type="bibr" target="#b2">[4]</ref>. Systolic arrays are built by interconnecting MAC units to a tightly-coupled 2D grid. They are efficient for running GEMMs by enabling reuse of the (M,K) or (K,N) matrix over the rows and columns of the array, reducing data accesses and instruction overheads.</p><p>As DNN models evolve at a rapid rate, it is imperative to design the underlying acceleration substrate to remain efficient for future models and training techniques. While GEMMs continue to remain the favorite abstraction to unroll tensors to during the forward and backward passes, architects need to be cognizant of three trends:</p><p>? Many GEMMs have irregular (or non-square) dimensions arising from minibatches and weight factorization <ref type="bibr" target="#b31">[34]</ref>. ? GEMMs exhibit varying degrees of weight sparsity from pruning, and activation sparsity from non-linearities (like ReLU, pooling, and dropout). The number of nonzeros varies throughout training iterations (from 10% to 90%) <ref type="bibr" target="#b45">[48]</ref>. ? DNN models are being developed at a rapid rate as AI becomes evermore pervasive, making it impractical to pick a specific set of matrix dimensions or sparsity ratios to design accelerators for.</p><p>Based on our observations, we recommend three key requirements for future GEMM engines.</p><p>? Flexibility: GEMM engines should be able to efficiently run matrices of arbitrary dimensions. ? Sparsity Support: GEMM engines need support for unstructured sparsity in both weights and activations in order to fully utilize hardware resources. ? Scalability: GEMM engines need to scale efficiently for integration into different kinds of accelerators. For example, tiny tensor cores in CPUs/ GPUs to large cores in a future TPU.</p><p>Unfortunately, state-of-the-art GPUs and TPUs fare poorly on some of the requirements, as we discuss later in Sec. III. GPUs <ref type="bibr" target="#b28">[31]</ref> provide some flexibility in terms of tiling irregular GEMMs into 4x4 chunks and mapping over tensor cores, but add complexity for scheduling and accumulation across SMs. TPU, being a large inflexible 128x128 array, can lead to compute under-utilization if the GEMM dimensions do not align with the dimensions of the physical array. GPUs cannot exploit both input and weight sparsity. Even when only one type of sparsity is present, current CUDA libraries require the datatype to be FP32 and the sparse data to be structured. TPU do not natively support sparsity since its rigid internal connectivity and per-cycle systolic dataflow prevent skipping multiplications with at least one operand that is zero. And finally, while systolic arrays scale well due to a regular 2D structure (as is evident from 4x4 versions in GPUs to a 128x128 version in the TPU), larger arrays take proportionally longer to load data and collect final outputs.</p><p>In this work, we demonstrate the microarchitecture of a flexible and scalable GEMM accelerator named SIGMA that can handle (a) arbitrary amounts of sparsity, (b) arbitrary irregularity in GEMM dimensions, while guaranteeing close to full compute utilization. SIGMA's key novelty is a highly Flexible Dot Product Engine (Flex-DPE), that can map GEMMs of arbitrary shapes and sparsity distributions via a rich interconnect fabric. Further, Flex-DPE uses tree-based topologies -enabling data loading and collection times of O(1) and O(log 2 N) respectively, instead of O( ? N) for an equivalent sized square systolic array. The full SIGMA engine connects multiple Flex-DPEs together via a simple global network-on-chip (NoC). The NoC allocate a cluster of Flex-DPEs for one GEMM. Each cluster is called a Flexible Dot Product Unit (Flex-DPU). SIGMA can thus morph into a large Flex-DPU running one GEMM or into multiple small variable-sized Flex-DPUs running different GEMMs.</p><p>Our key contributions are the following:</p><p>1) Analysis of modern DL training workloads to make the case for accelerating sparse, irregular GEMMs. 2) A novel accelerator named SIGMA for handling irregular and unstructured sparse GEMM operations. 3) A novel reduction network, named Forwarding Adder Network (FAN), for efficient partial sum accumulation. 4) Layout implementation of SIGMA for scalability and backend analysis.</p><p>The rest of the paper is organized as follows: Sec. II discusses modern training workloads and their GEMM characteristics. Sec. III dissects state-of-the-art deep learning accelerators and design considerations. Sec. IV proposes the SIGMA microarchitecture, and Sec. V describes the physical implementation and hardware costs. Sec. VI evaluates the performance of SIGMA against the state-of-the-art. Sec. VII discusses the related works, and Sec. VIII concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DL TRAINING CHARACTERIZATION</head><p>In this section, we analyze GEMM kernel shapes and sparsity levels from modern deep learning applications.</p><p>Target Workloads. For the kernel characterization exercise, we consider three workloads: Transformer <ref type="bibr" target="#b39">[42]</ref>, Google Neural Machine Translation (GNMT) <ref type="bibr" target="#b42">[45]</ref>, and Neural Collaborative Filtering (NCF) <ref type="bibr" target="#b17">[20]</ref>. We also leverage "Baidu DeepBench" <ref type="bibr" target="#b0">[2]</ref>, which identifies key GEMM kernels encountered across various CNNs/ RNNs/ LSTMs. For Transformer, we use a 324 Million parameter model <ref type="bibr" target="#b40">[43]</ref>  with the LM1B (billion word corpus) dataset. For GNMT, we evaluate the state of art 8-layer GNMT model with WMT-German-English dataset.</p><p>Time-Breakdown of Compute Primitives. Figure <ref type="figure" target="#fig_2">2</ref> shows the time break-up of different operations when training GNMT and Transformer on a NVIDIA V100 GPU <ref type="bibr" target="#b28">[31]</ref>. We observe that approximately 70% of time is spent on matrix multiplications (MatMul) operations or operations that can cast as MatMuls. Thus, MatMul is a key compute primitive to accelerate in hardware to speed-up training.</p><p>GEMM shapes. Transformer, GNMT, NCF and Deep-Bench <ref type="bibr" target="#b0">[2]</ref> have matrices of different sizes and shapes as shown in Fig. <ref type="figure" target="#fig_1">1b</ref>. Training is performed in different batch sizes, which lead to different input matrix dimensions. The observed shapes of the operand matrices vary from tall-skinny (rows dominate over columns) to fat-short (columns dominate over rows) -this is due to low minibatch sizes. Thus, GEMM accelerators need scalability and flexibility to handle large and irregular GEMM sizes efficiently.</p><p>Sparsity within GEMMs. As the objective of this work is not focused on algorithm techniques to generate sparse models, we leverage a pruning approach similar to Zhu et al. <ref type="bibr" target="#b45">[48]</ref> via a slow sparsification technique that increases the sparsity level of weights from zero to a final sparsity level in a fixed set of pruning steps.</p><p>For GNMT <ref type="bibr" target="#b42">[45]</ref> with ?210M parameters, we achieve close to state-of-the-art accuracy with 90% weight sparsity (resulting in ?22M parameters), similar to results outlined in <ref type="bibr" target="#b45">[48]</ref>. The pruning is applied to embedding, decoder projection layer and all LSTM layers in both the encoder and decoder. Workloads like transformer and ResNet-50 also exhibits good accuracy with around 80% and 70% weight sparsity respectively <ref type="bibr" target="#b12">[15]</ref>. Activation sparsity in DNN models comes from ReLU and dropout layers.</p><p>Improper handling of sparse matrices wastes compute resources and causes unnecessary but expensive movement of zeros across the memory hierarchy. As matrices are getting bigger and sparser, the need for sparsity support becomes more important. Thus, GEMM accelerators need support to handle both weight and activation sparsity efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INEFFICIENCY OF GPUS AND TPUS</head><p>In this section, we demonstrate the inefficiencies with current GEMM accelerators, and discuss the design choices that eventually lead to our proposed design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Irregular and Sparse GEMMs on GPU</head><p>We measured the compute efficiency on V100 GPUs with and without sparsity for various GEMM dimensions. In Fig. <ref type="figure" target="#fig_3">3a</ref>, we run some of the deep learning MatMul kernels (dense irregular without any sparsity) for workloads described in Sec. II on a single card V100 GPU and measure the efficiency with FP32 and FP16 data type. FP16 data type can take advantage of the systolic arrays ("tensor cores") in V100 for GEMM computation. While FP16 uses the tensor cores to boost the efficiency compared to the FP32 version, they still operate at a fraction of the peak efficiency due to irregularity in kernel dimensions; whereas a dense regular GEMM (2k, 2k, 2k) with FP16 tensor cores provide up to 76% efficiency.</p><p>We then introduce sparsity to the above MatMul kernels and use NVIDIA cuSPARSE [5] libraries, which support sparse matrices computation. cuSPARSE libraries API currently support only one of the matrices to be sparse with only FP32 data type. In this experiment, we induce random sparsity of 50% and 80% to one of the matrices while keeping the other matrix dense. From Fig. <ref type="figure" target="#fig_3">3b</ref>, we observe on average 4x reduction in efficiency compared to the equivalent dense FP32 matrix computation by adding sparsity. We expect the efficiency to decrease further when both matrices are sparse. Current GPU systems cannot efficiently map sparse GEMM computation onto their compute engine when there is no structure in the sparsity, and thus we need to fundamentally re-architect how we design a system that can take advantage  Table <ref type="table">I</ref>: Desired features for a GEMM accelerator, limitations of systolic arrays, and SIGMA's approach.</p><p>of sparse computation to achieve high efficiency for deep learning workloads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Irregular and Sparse GEMMs on TPU</head><p>Google's TPUs are a poster-child for large GEMMs due to their 128?128 systolic array. However, across a suite of GEMMs from modern DL workloads, we observe that it is common to have less than 50% of the array utilized when running irregular matrices, as we show later in Sec. VI. In addition, systolic arrays cannot inherently address sparsity. The reasons for these inefficiencies are discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. GEMMs on Systolic Arrays vs. SIGMA</head><p>Systolic arrays face under-utilization in many different scenarios due to two inherent features: a rigid shape, and a simple but rigid interconnect. In Fig. <ref type="figure" target="#fig_4">4</ref>, we contrast a systolic array against an abstract view of SIGMA's Flex DPE (which will be presented in detail later in Sec. IV-A). Fig. <ref type="figure" target="#fig_4">4a</ref> shows three example GEMM operations: (i) dense regular, (ii) dense irregular and (iii) sparse irregular matrices. The shaded boxes in the figure highlight quantitative metrics such as utilization, runtime, multicast behavior, and SRAM reads/writes for each example. For the purposes of this example, it is sufficient to assume that SIGMA's Flex-DPE has two specialized networks between the PEs and the SRAMs on the sides of the array (not shown in the figure) -a distribution network and a reduction network. The specific implementation of these networks is discussed later in Sec. IV.</p><p>Dense Regular Matrices. Fig. <ref type="figure" target="#fig_4">4b</ref> shows how the dense regular matrices are mapped. In the example, we use a KN matrix stationary, and MK matrix streaming dataflow (Nsta, M-str). An alternate term for this dataflow is weightstationary, and is used by the Google TPU <ref type="bibr" target="#b21">[23]</ref>, <ref type="bibr" target="#b34">[37]</ref>. Partial sums are generated at each cross-point and accumulated over the columns. The systolic array and Flex-DPE designs are able to fully utilize its PEs by mapping all of KN matrix onto its PEs. They key differences between the two are that (i) a systolic array sends the streaming matrix in a store and forward manner, while SIGMA multicasts the data to the corresponding PEs in one cycle, and (ii) the systolic array uses a linear reduction while SIGMA uses a tree-based reduction, as we describe later in Sec. IV.</p><p>Dense Irregular Matrices. Fig. <ref type="figure" target="#fig_4">4c</ref> shows how dense irregular matrices are mapped. A systolic array design suffers from under-utilization due to its rigid structure. Despite the fact that there are 16 elements in the dense irregular KN matrix and 16 PEs in the systolic array, only half of the matrix can be mapped at a time. This is due to the rigid shape of the systolic array. All partial sums are accumulated down a column via forwarding; mapping the other half of the dense irregular N-matrix onto the systolic array at the same time will lead to incorrect functionality, since the accumulated output (a.A+b.I) should not get added to (a.E + b.M). The second half of the N-matrix will therefore have to be loaded once the first half of the matrix is computed, more than doubling the computation time. In contrast, the Flex-DPE is able to map all of the dense irregular stationary elements at one go, utilizing all PEs completely. This is enabled by having a flexible reduction network that can accumulate both sets of outputs (a.A+b.I) and (a.E + b.M) separately and concurrently, as we describe later in Sec. IV. This not only provides a runtime advantage, but also an energy advantage since the M-matrix only needs to be read and streamed through the array once.</p><p>Sparse Irregular Matrices. Fig. <ref type="figure" target="#fig_4">4d</ref> shows how sparse irregular matrices are mapped. Not only does a systolic array suffer under-utilization from irregular matrices, but also from sparsity. To maintain correctness of the final output, a systolic array must map the non-zero values onto the compute unit. This limitation comes due to the rigid forwarding network between PEs. The Flex-DPE design is able to map only non-zero elements because of the flexible distribution and reduction networks. There are two different dataflows enabling sparse compute in a Flex-DPE. The N-sta, M-str dataflow for Flex-DPE in Fig. <ref type="figure" target="#fig_4">4d</ref> maps only non-zero elements onto the compute, giving it 100% stationary utilization, making it more efficient than the systolic array. However, the streaming matrix may send zerovalued elements. This is because all non-zero stationary elements are mapped if there is at least one streaming value that needs to be multiplied with it.</p><p>Fig. <ref type="figure" target="#fig_4">4e</ref> shows the N-str, M-str dataflow (i.e., No Local Reuse <ref type="bibr" target="#b7">[10]</ref>) for the Flex-DPE that fully utilizes the compute. This is done by streaming only necessary multiplication pairs and not keeping any values stationary. We provide more details about the distribution and reduction network architecture that can enable this feature in Section IV. The equivalent dataflow is not possible for the systolic array as it does not allow arbitrary pairings of vectors from the M and N matrices due to its rigid cycle-by-cycle forwarding network.</p><p>Distribution and Reduction Latency. Another point to notice from the quantitative data in Fig. <ref type="figure" target="#fig_4">4b</ref>-e is that the Summary. Table <ref type="table">I</ref> summarizes the sources of inefficiency in systolic arrays and how SIGMA addresses each. Architectural details are provided next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. SIGMA ARCHITECTURE</head><p>The fundamental building block within SIGMA's compute fabric is a processor named Flexible Dot Product Engine (Flex-DPE), described in Sec. IV-A. Several Flex-DPEs are connected together via a simple NoC to create the full SIGMA compute fabric. Each GEMM operation reserves a contiguous group of Flex-DPEs, creating a Flexible Dot Product Unit (Flex-DPU), described in Sec. IV-B. The memory-system is similar to the TPU <ref type="bibr" target="#b2">[4]</ref>, <ref type="bibr" target="#b21">[23]</ref>. Fig. <ref type="figure" target="#fig_8">8</ref> depicts the high level schematic of SIGMA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Microarchitecture of Flex-DPE</head><p>A k-sized Flex-DPE houses k multipliers, k -1 adders, local buffers, control unit, and flexible interconnects. The multipliers are laid out in a logical 1-D structure. Fig. <ref type="figure" target="#fig_5">5</ref> shows an overview. A 1D substrate enables us to run matrixmatrix (M*M) multiplications as multiple vector matrix multiplications (V*M), similar to Brainwave <ref type="bibr" target="#b11">[14]</ref>. Recall from Fig. <ref type="figure" target="#fig_4">4</ref> that a k-sized square systolic array has ? k columns and ? k rows, with each column computing an independent dot-product when running a weight <ref type="bibr" target="#b21">[23]</ref> or inputstationary dataflow <ref type="bibr" target="#b34">[37]</ref>. In contrast, a k-sized Flex-DPE can be configured to create myriad combinations of dot-products: one dot-product of size k, two dot-products of size k/2, ? k dot-products of size ? k (like the systolic array), and so on. In fact, the flexible distribution and reduction networks also enable creation of multiple variable-sized dot-products, which is crucial for sparsity. In Sec. V, we study how the Flex-DPE scales with area and power.</p><p>1) Distribution Network: Benes Topology: The role of a distribution network within any GEMM engine is to load the stationary matrix (MN or KN), and stream the other matrix, as shown in Fig. <ref type="figure" target="#fig_4">4</ref>. In a systolic array, the distribution network behavior is implemented via the horizontal and vertical forwarding links between PEs. This leads to an O(k) data loading time for a k ? k systolic array.</p><p>In SIGMA, we adopt a Benes network <ref type="bibr" target="#b4">[7]</ref> to support the flexibility demonstrated in Fig. <ref type="figure" target="#fig_4">4</ref>. Benes is a non-blocking N-input N-output multistage network with 2log(N)+1 levels, each with N tiny 2x2 switches. The switches, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>-Step(iv), require two control bits; one for selecting the vertical output and one for diagonal output. Numerous Benes routing algorithms have been proposed <ref type="bibr" target="#b4">[7]</ref>, <ref type="bibr" target="#b5">[8]</ref>, <ref type="bibr" target="#b26">[29]</ref>. The non-blocking property of Benes allows any source to communicate with any destination without any intermediate contention. We use latch-free switches (except for timing closure) at each-stage, allowing a O(1) data communication across the Benes network. We also support multicasts within the Benes network to avoid having to read the same data element multiple times from SRAM.</p><p>Other design-choices are also feasible for the distribution network. A crossbar gives the same non-blocking behavior as Benes and has much simpler routing, but it does not scale well (N 2 ). Blocking interconnects such as buses <ref type="bibr" target="#b7">[10]</ref>, trees <ref type="bibr" target="#b8">[11]</ref>, <ref type="bibr" target="#b24">[27]</ref>, butterfly and mesh networks <ref type="bibr" target="#b6">[9]</ref>, are still valid design choices due to their low wire costs, but will cause performance degradation due to increased distribution delays.</p><p>2) Reduction Network: FAN Topology: Dot-product reduction can be implemented in three ways.</p><p>Spatio-Temporal Reduction: The TPU weight-stationary systolic array implementation performs reduction via forwarding along the column, requiring O(k)-cycles for a k ? k array. The time taken is independent of the actual size m of the dot-product which may be smaller.</p><p>Temporal Reduction: Designs like EIE <ref type="bibr" target="#b16">[19]</ref> perform inplace reduction within PEs. The time taken is still linear like spatio-temporal, but equal to O(m) -i.e., the dot-product size.</p><p>Spatial Reduction: In SIGMA, we implement a spatial tree-based reduction, as it requires O(log 2 m) cycles, for a m-sized dot-product. The challenge with realizing this log 2 mcycle reduction, however, is that non-powers of two sized reductions are hard to map over traditional binary adder trees. Suppose we are trying to accumulate three separate dot-products for (a0, a1, a2, b0, b1, c0, c1, c2) on an eightwide adder tree. Following the natural binary-tree topology, a2-b0 and b1-c0 will reach the same adder as they go up the tree, which is incorrect functionally.</p><p>FAN Topology. To address this issue, we propose a novel adder-tree topology named Forwarding Adder Network (FAN) that places forwarding links between different levels of adders over a traditional binary adder tree. The topology and variable labels of a 32-wide FAN are shown in Fig. <ref type="figure" target="#fig_6">6a</ref>. VecIDs and adderIDs are numbered in increasing order from left to right, and each adderID has a corresponding adderLvl value. Below is a pseudocode describing the link connections between adders to create FAN of any power of 2 size.</p><p>// Adders at level 0 connect to vecID directly // Adder links start from level 1 for (int i = 0; i &lt; numAdders; i++) do int adderID = i; // they are the same for (int lvl = 1; lvl &lt;= adderLVL[i]; lvl++) do connect with adder: adderID -2?(lvl-1); connect with adder: adderID + 2?(lvl-1);</p><p>Routing Algorithm. The routing algorithm for FAN is shown in Fig. <ref type="figure" target="#fig_6">6c</ref>. For every adder, if vecID[adderID] equals to vecID[adderID+1], accumulation is enabled. If the vecIDs are not equal and the adder is in the zeroth level, the bypass link is enabled. For example, in Fig. <ref type="figure" target="#fig_6">6a</ref>, Adder 12 needs to bypass 'c' and 'd' to the next adder levels. From the second adder level onward, there is a N-to-2 mux before every FP32 Adder. To determine which inputs get selected, comparators are used to identify cluster regions.</p><p>Benefits and Overhead. FAN offers similar benefits as the ART topology proposed in MAERI <ref type="bibr" target="#b24">[27]</ref> in terms of creating dot-products of variable sizes. However, FAN is much more lightweight. This is because MAERI's ART is built using three input adders (two from parent nodes, one from a sibling node), which makes it extremely prohibitive, especially when working with FP32 data type (commonly used during DNN training). Fig. <ref type="figure" target="#fig_6">6b</ref> shows the performance evaluation between linear reduction (i.e., temporal or spatiotemporal), ART, and FAN. For performance calculations, we use 100 stationary folds (when stationary elements need to be replaced) with stream dimension of 1000 each. As shown in Fig. <ref type="figure" target="#fig_6">6b</ref>-iii, taking logN cycles rather than N cycles before starting the next fold significantly improves performance as the number of PEs increases. Our findings show that 512-PE FAN only has a 10% and 31% area power overhead over linear, compared to ART which has a 92% and 86% overhead respectively. FAN also provides EDP benefits over linear starting from 128-PE. At 512-PE, FAN's EDP is 45% and 34% lower than linear and ART respectively. From our results, we conclude that FAN is both high performance and scalable.</p><p>3) Execution within Flex-DPE: The Flex-DPE design allows mapping for dense or sparse, and regular or irregular GEMMs. In Fig. <ref type="figure" target="#fig_4">4</ref>, we have seen how different combinations of matrices are mapped onto Flex-DPE. Fig. <ref type="figure" target="#fig_5">5</ref> depicts the steps involved in generating the mapping for sparse matrices which we will describe later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Composing Flex-DPEs into a Flex-DPU using a NoC</head><p>To extract maximum possible parallelism from the available multipliers, SIGMA can dynamically compose a number of Flex-DPE units together to form a logical ensemble which we call Flex-DPU. A Flex-DPU is responsible for running one GEMM. Multiple Flex-DPUs can be scheduled in parallel to run multiple GEMMs. The NoC connecting the Flex-DPEs together is similar to that of tiled accelerator architectures, such as Tangram <ref type="bibr" target="#b13">[16]</ref> and Simba <ref type="bibr" target="#b36">[39]</ref>.</p><p>A simple switch is present at the intersection of each Flex-DPE to arbitrate the flow of the data. These switches are connected together in a 2D mesh. They are statically configured when mapping the GEMMs, and do not require any dynamic routing or flow-control like conventional NoCs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The amount of bandwidth on this NoC (i.e., number of unique elements of the row/column that can be transferred</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>per-cycle) is a design-time configurable parameter.</head><p>Within a Flex-DPU, the switch forwards data across Flex-DPEs, providing seemless multicasts of data like a bus. We describe this with an example in Sec. IV-E. Across Flex-DPUs, the switches provide hop-by-hop data forwarding, similar conventional NoCs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Supporting Unstructured Sparsity</head><p>Compression Format. One of the key benefits of supporting sparsity is low-memory footprint; and consequently more energy savings by avoiding zero-valued element transfers. There are a few well recognized compression formats such as CSC, CSR, COO, and RLC (Run-length compression). We use a Bitmap format within SIGMA, where each element has a corresponding bit to indicate if a given element is zero or non-zero in the corresponding matrix <ref type="bibr" target="#b14">[17]</ref>, <ref type="bibr" target="#b20">[24]</ref>, <ref type="bibr" target="#b32">[35]</ref>, <ref type="bibr" target="#b33">[36]</ref>. Fig. <ref type="figure" target="#fig_7">7</ref> compares the metadata overhead of various compression formats with varying levels of sparsity. The dimensions and sparsity levels in the plot reflect what we observe in our workloads (see Sec. II). The metadata overhead for COO/ CSR/ CSC changes drastically at various sparsity regions. This is because each nonzero element require indices of log 2 (dimension) bits, etc. The Bitmap format has a constant meta-data overhead irrespective of sparsity, making it attractive for SIGMA which targets arbitrary unstructured sparsity. At low-levels of sparsity, we find Bitmap having lower footprint than COO/ CSR/ CSC. Bitmap has comparable overhead to RLC <ref type="bibr" target="#b7">[10]</ref>, <ref type="bibr" target="#b16">[19]</ref>, at sparse ratio of ?30% to ?70%. We observe that RLC is better at reducing meta-data over Bitmap at &gt;?70% sparsity, but is worse at &lt;?30% sparsity. We evaluate RLC using 4-bit (RLC-4) and 2-bit (RLC-2) run lengths. Alternate compression formats can be supported over SIGMA by only changing the front end controller to ensure proper mapping.</p><p>Sparsity Controller. For each GEMM, a global controller determines how sparse matrices are decoded and mapped onto SIGMA. The controller operates on the bitmap metadata and calculates how many Flex-DPEs are needed. Internal counters and tables are implemented to determine the indices where dense computations are needed. We describe the details with a walkthrough example in Sec. IV-E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Dataflows Supported by SIGMA</head><p>SIGMA's flexible substrate enables it to support myriad dataflows. For all workloads, we use both Weight (i.e., KN) stationary and Input (i.e., MK) stationary dataflows (Fig. <ref type="figure" target="#fig_4">4d</ref>), and pick the one that provides higher efficiency. In these two dataflows, spatial dot-products of dynamic size are created depending on the matrix dimensions and sparsity of the stationary matrix. The columns/rows of the streaming matrix are reused spatially by broadcasting to the rows/columns of the stationary matrix (which are reused temporally at each multiplier). SIGMA can also run a No Local Reuse (i.e., MN-str, KN-str dataflow from Fig. <ref type="figure" target="#fig_4">4e</ref>). This dataflow can provide 100% utilization of the compute, but comes at the cost of requiring higher interconnect bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Walkthrough Example</head><p>following steps (corresponding to Fig. <ref type="figure" target="#fig_5">5</ref>) depicts a walk-through example showing how SIGMA utilizes the bitmap format to map sparse GEMMs onto Flex-DPEs. Here, the number of multipliers per Flex-DPE (N mult ) is four.  The row-id is recorded to determine partial sum regions. Additionally, an initial output bitmap is generated based on if there are any non-zero computations. ? Step vi) Generate distribution routing bits base on the SRC-DEST table entries. For this example, a naive routing algorithm with limited functionality is to subtract the src-index with the dest-index. Other routing algorithms have been proposed <ref type="bibr" target="#b4">[7]</ref>, <ref type="bibr" target="#b5">[8]</ref>. ? Step vii) Finally, the streaming values are broadcasted to all Flex-DPEs within a Flex-DPU from the routing bits calculated in Step vi. For reduction, the accumulation ID is processed and then used as the vecID in FAN, described in Section IV-A2. Multicasts, multiplications, and reductions are all happening in a pipelined fashion. Once all the columns have been streamed in and outputs are generated, the Flex-DPE units are freed up to be utilized for another GEMM operation.</p><formula xml:id="formula_0">? Step i)</formula><p>V. IMPLEMENTATION Fig. <ref type="figure" target="#fig_8">8</ref> compares the post place-and-routed area and power of a 128?128 systolic array versus SIGMA with 128 Flex-   <ref type="figure" target="#fig_4">4</ref>). with regards to cycle breakdown, utilization, and efficiency. Table <ref type="table" target="#tab_2">II</ref> describes the legend.</p><p>DPEs, each of size 128. Both designs have identical input bandwidth of 128 words per cycle from SRAM. SIGMA's key overheads are the highly flexible, non-blocking distribution and reduction networks that lead to a 37.7% area overhead. However, the performance speedups provided by SIGMA (shown later in Sec. VI-C) lead to an average 3.2? improvement in Effective TFLOPs/ Watt. We expect a further power performance gain of 7? when scaling from a 28nm design to a 12nm design. This is based on FP32 FLOPs growth between NVIDIA K20X to NVIDIA T4 where compute grew by ?2? while power reduced by ?3.5?. SIGMA is pipelined at 1-cycle distribution, 1-cycle multiplication, and 1-cycle for each level of reduction. The critical path for SIGMA is the distribution, but it is possible to match the maximum operating frequency of TPU by pipelining the distribution further so that the new critical path becomes the FP compute. Additionally, we estimate a global controller with 1024 AND gates, 1024 OR gates, 1024 counters, and 128 SRC-DEST tables to consume approximately 1.4mm 2 .</p><p>For 16384 total PEs, we performed a design-space exploration for sizing Flex-DPE units to find the most energy and area efficient configuration. Fig. <ref type="figure" target="#fig_9">9</ref> depicts that a Flex-DPE of size 128 Flex-DPE consumes the least energy, while a Flex-DPE size of 512 is the most area efficient. We decide to use Flex-DPE-128 to match the per-cycle SRAM read bandwidth of the TPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION A. Methodology</head><p>Target GEMMs. We use GEMM dimensions and sparsity observed during training of GNMT, Transformer, NCF and DeepBench models (described earlier in Sec. II). Input and weight sparsity were observed to be ?10-50% and ?80%.</p><p>Baseline Accelerators. We compare SIGMA against other state-of-the-art accelerators: TPU <ref type="bibr" target="#b2">[4]</ref>, EIE <ref type="bibr" target="#b16">[19]</ref>, SCNN <ref type="bibr" target="#b30">[33]</ref>, OuterSPACE <ref type="bibr" target="#b29">[32]</ref>, Eyeriss v2 <ref type="bibr" target="#b8">[11]</ref>, Packed Systolic <ref type="bibr" target="#b23">[26]</ref>, and Cambricon-X <ref type="bibr" target="#b44">[47]</ref>. We scale the number of PEs to a constant number of 16384 in all designs. SIGMA assumes 128 Flex-DPEs, each with 128 MACs, and input SRAM bandwidth of 128x (number of unique data elements that can be distributed). For our evaluations, we allow greater input bandwidth to distribute larger chunks of the streaming matrix in one cycle. For sparsity performance, all combinations of matrices and sparsity were tested and then averaged. Most of the sparse accelerators were designed for inference and specialized for convolutions; we extended them to run GEMMs by setting equal input and filter dimensions.</p><p>Simulation Infrastructure. To evaluate the performance of SIGMA and other baselines, we developed a cycle-accurate analytic model. The model evaluates the performance based on the number of compute units, buffers per compute unit, interconnect topology, input bandwidth, and dataflow. The TPU was modeled using SCALE-sim <ref type="bibr" target="#b34">[37]</ref>.</p><p>Comparison Metrics. Table <ref type="table" target="#tab_2">II</ref> defines comparison metrics we use across our evaluation graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Characterizing SIGMA's Features</head><p>We start by characterizing the benefits of the various features of SIGMA to help pick the optimal design-point.</p><p>Dataflow Comparison Fig. <ref type="figure" target="#fig_10">10</ref> demonstrates the impact of dataflows when running a suite of representative GEMMs. We observe that the MK-str,KN-str dataflow, while being ideal in terms of no wasted computations, suffers in overall latency. This is because it requires extremely high bandwidth (thus serialization), due to no reuse within the Flex-DPE. For the other two dataflows, the stationary matrix maps only non-zeros, getting 100% utilization, and the overall efficiency   gets determined by the sparsity of the streaming matrix. In our evaluations, we run both dataflows and report the higher performing dataflow.</p><p>Benefits of SIGMA's features Fig. <ref type="figure" target="#fig_11">11</ref> revisits the discussion from Sec. III-C and quantitatively demonstrates the benefits of SIGMA's three key features in comparison to systolic arrays: (i) flexible dimensions -enabled via FAN for variable sized dot-products within Flex-DPEs, (ii) scalable interconnects, namely, Benes and FAN, providing O(1) and O(log 2 N) distribution and reduction time respectively, and (iii) sparsity support to map only useful non-zero data.</p><p>For dense regular GEMMs (most left in Fig. <ref type="figure" target="#fig_11">11</ref>), SIGMA provides speedup over TPU from its O(1) distribution and O(log 2 N) reduction. (TPU has O(sqrtN) distribution and reduction.) SIGMA's networks reduce Add latency (defined in Table <ref type="table" target="#tab_2">II</ref>). The number of cycles saved accumulates whenever the stationary matrix needs to be replaced.</p><p>For dense irregular GEMMs, TPU is underutilized if a side of the stationary matrix is smaller than the MAC array dimension. In the example where a 16-500000 sized matrix is stationary; because of a small dimension size of 16, a 128?128 TPU will have a utilization of 12.5%. SIGMA enables full utilization by using rich interconnects that cluster variable dimensions together. Since more PEs are utilized, fewer stationary matrix load iterations are required. This leads to lower loading, add, and streaming latencies.</p><p>For sparse irregular GEMMs, TPU is required to map all elements stationary, while SIGMA maps only the nonzeros stationary. With sparsity support, SIGMA shows 100% stationary utilization. Due to increased utilization and compute efficiency, fewer cycles are needed to load and reduce data. Fig. <ref type="figure" target="#fig_11">11</ref> shows two versions of sparse irregular GEMMs. The M-str,N-sta example is dominated by streaming latency because the larger matrix is being streamed in, while the loading latency dominates in M-sta,N-str because the larger matrix is held stationary and leads to more folding iterations. The compute efficiency for M-sta,N-str is significantly higher because the sparser matrix is held stationary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance and Energy versus TPU</head><p>Speedup. Fig. <ref type="figure" target="#fig_12">12a</ref> and Fig. <ref type="figure" target="#fig_12">12b</ref> evaluate dense and sparse GEMMs performance respectively. In Fig. <ref type="figure" target="#fig_12">12a</ref>, we use three aspect ratios for the TPU. For e.g., 512?32 have 512 rows, each of which can read a data element per cycle. Either the MK or KN matrix is kept stationary. For the 2048-4096-32 GEMM, a K dimension of 32 leads to under-utilization in the 128?128 and 256?64 TPUs, but aligns with the columns of the 512?32 TPU, giving a huge performance jump. SIGMA, due to its flexibility, experiences a similar jump. The TPU overall efficiency drops steeply while operating a 1024-16-500000 sized GEMM. If a square-shaped TPU maps the KN (500000-16) matrix stationary, the low value of N leads to a 87.5% decrease in utilization. If it decides to map   <ref type="table">I</ref>. In SIGMA, the overall efficiency is close to 100% throughout, except for small GEMMs (such as 2048-1-128), where smaller sizes cause loading latency from limited bandwidth to dominate. On average, SIGMA provides speedup of 2x over TPUs on dense GEMMs, stemming from higher utilization and faster reduction. This results to an overall average efficiency of 82% compared to 59% in the TPU. In Fig. <ref type="figure" target="#fig_12">12b</ref> we run GEMMs with varying sparsity over SIGMA. We observe that there is a roughly 6? improvement over TPU, which suffers from an average overall efficiency of less than 10% due to the mapped zeros. SIGMA maps no zeros and shows an average overall efficiency of 40%, which gets limited by the sparsity of the streaming matrix.</p><p>Energy. In Fig. <ref type="figure" target="#fig_13">13</ref> we see that SIGMA is on an average 3? more energy efficient and 5? more area efficient than TPU for sparse workloads. Despite SIGMA consuming twice as much power (Fig. <ref type="figure" target="#fig_8">8</ref>), the energy benefit comes from ?6? speedup. With more sparsity induced in future workloads, we expect energy gains to be significantly more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance against Sparse Accelerators</head><p>Fig. <ref type="figure" target="#fig_14">14</ref> presents the speedup of SIGMA over state-ofthe-art sparse accelerators. The key inefficiencies in other accelerators are presented in Table <ref type="table" target="#tab_4">III</ref>. Of all the sparse accelerators, SIGMA is the only one that can support full spatial-reduction with arbitrary sized dot-products. For two GEMMs, we find SIGMA slower than Eyeriss v2 since the latter can buffer both operands in its local SRAM for further reuse, while SIGMA keeps only one operand stationary, and has to stream the other multiple times (even if it will be reused in future). Other designs like EIE also have local SRAM buffers, but we observe that its inter-PE communication bottleneck overshadows the memory benefits. On average, we observe SIGMA performing 3X faster than the other sparse accelerators. We tested four combinations between the matrices and sparsity level and selected the best performing one for each accelerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Training. A few prior works address training on dense matrices. Pipelayer <ref type="bibr" target="#b37">[40]</ref> and Neurocube <ref type="bibr" target="#b22">[25]</ref> proposes ReRAM based acceleration, but does not address scalability and sparsity. Hypar <ref type="bibr" target="#b38">[41]</ref> addresses the scaling problem in training and proposes optimal techniques to extract parallelism. Schuiki et al. <ref type="bibr" target="#b35">[38]</ref> and Liu et al. <ref type="bibr" target="#b27">[30]</ref> propose processing in memory approaches to combat the communication problem when scaling training. ScaleDEEP architecture was developed to target DNN training, and consists of many processing tiles with specialized memory subsystem and interconnect <ref type="bibr" target="#b41">[44]</ref>. However none of these methods simultaneously address the irregularity, sparsity, and scalability as SIGMA does.</p><p>Sparsity. Table <ref type="table" target="#tab_4">III</ref> contrasts SIGMA against state-ofthe-art sparse accelerators. Other recent designs include PermDNN <ref type="bibr" target="#b10">[13]</ref>, which uses permuted diagonal matrices for inference on structured sparse DNN models. Other designs like UCNN <ref type="bibr" target="#b18">[21]</ref> exploits sparsity and weight repetition by reusing dot products. ExTensor <ref type="bibr" target="#b19">[22]</ref> finds intersections within compressed representations, and only operates on useful dense computations. Bit-tactical <ref type="bibr" target="#b9">[12]</ref> targets sparsity in inference by skipping zero weights and exploiting bit level sparsity of inputs. Unlike SIGMA, Bit-tactical leverages scheduling in software to align inputs and weights. SparseReRAM <ref type="bibr" target="#b43">[46]</ref> proposes using small operation units to exploit both weight and activation sparsity in ReRAMs. SIGMA targets acceleration of GEMMs with unstructured sparsity.</p><p>Flexibile Interconnects. Eyeriss <ref type="bibr" target="#b7">[10]</ref> proposes an efficient dataflow for leveraging convolutional reuse with reconfigurable buses. MAERI <ref type="bibr" target="#b24">[27]</ref> uses tree-based interconnects for distribution and reduction which inspired the 1D Flex-DPE microarchitecture in SIGMA. However, MAERI does not handle dynamic weight and activation sparsity, and is optimized for low-precision CNNs rather than high-precision GEMMs commonly used during DNN training. Eyeriss v2 <ref type="bibr" target="#b8">[11]</ref> also uses a specialized NoC to handle sparsity, but is optimized for small mobile CNNs rather than large GEMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSION</head><p>The paper proposes SIGMA as an accelerator to handle emerging large, sparse, and irregular GEMMs. SIGMA provides close to 100% compute utilization via high-bandwidth non-blocking interconnect structures. The overhead for such flexibility is carefully analyzed via a place-and-routed design. Our implementation shows 5.7? performance speedup over TPU designs for sparse irregular workloads. We also observe a 3? performance speedup over other state-of-the-art sparse accelerators. Reference RTL: https://github.com/georgia-tech-synergy-lab/SIGMA</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>I. INTRODUCTION Deep learning (DL) has emerged as the premier algorithmic technique for analyzing data across multiple domains, especially in visual understanding, speech perception, and automated reasoning. The application of DL consists of two steps; namely training and inference. During training, a Deep Neural Network (DNN) model uses a loss function and optimization process to minimize model error on the training dataset. During inference, the trained DNN model is used to classify data points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) GEMM operation. (b) Example GEMM dimensions from common Deep Learning workloads.</figDesc><graphic url="image-1.png" coords="1,314.02,248.08,233.63,193.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Time breakdown for different ops on V100 for (a) Transformer and (b) GNMT. Matrix Multiply consume around 70% of total runtime.</figDesc><graphic url="image-3.png" coords="3,62.36,73.12,233.86,74.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GPU performance evaluation on different GEMMs.</figDesc><graphic url="image-4.png" coords="3,313.86,71.41,234.09,253.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) Example GEMM matrices. (b) Mapping comparison between systolic array and SIGMA Flex-DPE for dense regular GEMMs. (c) Dense irregular GEMMs comparison. (d) Sparse irregular GEMMs comparison. (e) Alternative sparse irregular Flex-DPE mapping. (Note: M-str means MK matrix is streaming, N-sta, means KN matrix is stationary, etc.)</figDesc><graphic url="image-6.png" coords="4,418.47,73.35,129.91,606.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Walk through from bitmap to SIGMA mapping. This example uses M-stationary, N-streaming dataflow (see Fig. 4d).data loading and accumulation time in systolic arrays is always proportional to the array dimensions, while SIGMA's networks allow O(1) distribution and O(log 2 N) reduction.Summary. TableIsummarizes the sources of inefficiency in systolic arrays and how SIGMA addresses each. Architectural details are provided next.</figDesc><graphic url="image-8.png" coords="6,62.80,73.00,323.52,331.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a) Fowarding Adder Network Topology. (Not shown are flip flops across each stage of the forwarding links). (b) Spatial FP32 reduction interconnect comparisons of i) post-layout area, ii) post-layout power, iii) speedup, and iv) Energy-delay Product (EDP). (c) Simplified FAN routing algorithm for adder inputs and functionality (Note: variables i and adderID are equivalent).</figDesc><graphic url="image-10.png" coords="7,62.80,72.48,312.00,267.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Matrix memory overhead with dimensions M=1632 and K=36548. Format comparisons include: None, CSR, CSC, COO, RLC-4, RLC-2, and Bitmap in the following order.</figDesc><graphic url="image-12.png" coords="8,313.32,72.70,234.09,109.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: SIGMA high level diagram and comparison tableaginst a TPU-like design. We report only the area-power of the compute array, not the SRAMs. Effective TFLOPs is calculated by multiplying the base dense TFLOPs with the average efficiency computed across GEMMs in Sec. VI.</figDesc><graphic url="image-13.png" coords="9,62.70,72.50,234.09,212.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Design space exploration for Flex-DPE dimensions (256 Flex-DPE-64 is 256 Flex-DPEs of size 64), with aggregated energy consumption (all workloads) and Performance/Area as metrics. (N stat ). Since N stat is 8 and N mult is 4 in this example, 2-Flex-DPE units are needed to form a single Flex-DPU. ? Step iv) Unicast the stationary values to the multiplier buffers. The routing is straightforward, as all stationary input data travel vertically down. In this example, the input bus has a 4X bandwidth, so it is only able to fill one Flex-DPE each cycle. ? Step v) To generate source and destination pairs for each Flex-DPE, a counter value is assigned to each non-zero element in the stationary' and streaming bitmaps. For stationary' bitmap, the counter starts at 0 and increments from left-right, top-bottom. The counter resets when it reaches N mult -1, which marks the end of one Flex-DPE. Counter values increments top-bottom in the streaming bitmap and resets at the start of each column. Then, a streaming bitmap column compares to each row of the corresponding stationary' bitmap. If both values are 1, the counter values are stored in the Flex-DPE SRC-DEST tables. The row-id is recorded to determine partial sum regions. Additionally, an initial output bitmap is generated based on if there are any non-zero computations. ?Step vi) Generate distribution routing bits base on the SRC-DEST table entries. For this example, a naive routing algorithm with limited functionality is to subtract the src-index with the dest-index. Other routing algorithms have been proposed<ref type="bibr" target="#b4">[7]</ref>,<ref type="bibr" target="#b5">[8]</ref>. ? Step vii) Finally, the streaming values are broadcasted to all Flex-DPEs within a Flex-DPU from the routing bits calculated in Step vi. For reduction, the accumulation ID is processed and then used as the vecID in FAN, described in Section IV-A2. Multicasts, multiplications, and reductions are all happening in a pipelined fashion. Once all the columns have been streamed in and outputs are generated, the Flex-DPE units are freed up to be utilized for another GEMM operation.</figDesc><graphic url="image-14.png" coords="9,313.59,70.20,233.63,111.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Comparison of different SIGMA dataflows (described in Fig.4). with regards to cycle breakdown, utilization, and efficiency. TableIIdescribes the legend.</figDesc><graphic url="image-15.png" coords="10,63.45,192.40,233.86,116.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Performance comparison between TPU and progressive features added in SIGMA. Table II describes the legend. SIGMA Fl adds flexibility over a TPU structure by allowing irregular dimensions. (128x128, 256x64, 512x32 and vice versa) SIGMA Fl+Sc adds scalability with our proposed reduction network FAN. SIGMA Fl+Sc+Sp adds sparsity support to increase utilization.</figDesc><graphic url="image-16.png" coords="10,314.00,73.00,234.09,132.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Speedup and efficiency of (a) rectangular systolic and SIGMA configs over TPU 128?128 for dense workloads, and (b) SIGMA over TPU 128?128 with different input and weight sparsity (MK80 means 80% sparsity in the MK matrix, etc).</figDesc><graphic url="image-17.png" coords="11,62.79,72.50,484.76,123.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: SIGMA energy reduction over TPU and average performance/ area over TPU's compute array.</figDesc><graphic url="image-18.png" coords="11,62.79,239.72,234.09,124.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Comparison between SIGMA and other sparse accelerators with 80% and 30% sparsity on the two matrices.</figDesc><graphic url="image-19.png" coords="11,62.79,404.64,234.09,105.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Gather two bitmap-compressed matrices. In this example, MK is stationary and KN is streaming.</figDesc><table><row><cell>? Step ii) Conduct row-wise OR across the streaming</cell></row><row><cell>bitmap and store the outputs to REGOR (temporary</cell></row><row><cell>registers). Then, do element-wise AND between the</cell></row><row><cell>corresponding REGOR row and stationary bitmap column</cell></row><row><cell>to generate stationary' bitmap.</cell></row></table><note><p>? Step iii) The number of ones in stationary' bitmap corresponds to the number of useful stationary values</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table II :</head><label>II</label><figDesc>Streaming LatencyStream non-stationary Matrix through Distribution Network.Overlaps with Partial-sum generation and Accumulation. Comparison Metrics.</figDesc><table><row><cell>Metric</cell><cell>Description</cell></row><row><cell>Loading Latency</cell><cell>Load Stationary Matrix. Not overlapped with compute.</cell></row><row><cell>Add Latency</cell><cell>Last reduction before next stationary matrix loaded. Not overlapped with compute.</cell></row><row><cell>Total Latency</cell><cell>Loading + Streaming + Add Latency</cell></row><row><cell>Stat. Utilization</cell><cell>Percent non-zeros after Stationary Matrix mapped.</cell></row><row><cell cols="2">Compute Efficiency Useful (non-zero) MAC Latency / Streaming Latency</cell></row><row><cell>Overall Efficiency</cell><cell>Useful (non-zero) MAC Latency / Total Latency</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III :</head><label>III</label><figDesc>Qualitative Comparision of SIGMA against state-of-the-art accelerators.</figDesc><table><row><cell>MK (1024-500000) stationary, numerous folds are required</cell></row><row><cell>since there are only 16K PEs, leading to a large amount of</cell></row><row><cell>O(N) reductions. SIGMA accelerates this GEMM by creating</cell></row><row><cell>flexible dimensions and leveraging its O(logN) reduction</cell></row><row><cell>structure as described in Table</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: SOUTH CHINA UNIVERSITY OF TECHNOLOGY. Downloaded on October 28,2023 at 09:30:26 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>We thank <rs type="person">Ishwar Bhati</rs> and <rs type="person">Sasikanth Avancha</rs> for helpful technical discussions. This work was funded by <rs type="funder">Intel</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Baidu-deep bench</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Nvdla deep learning accelerator</title>
		<ptr target="http://nvdla.org" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cloud tpu</title>
		<ptr target="https://cloud.google.com/tpu" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hernandez</surname></persName>
		</author>
		<ptr target="https://blog.openai.com/ai-and-compute/" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On-line algorithms for path selection in a nonblocking network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">STOC</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Matrix-based nonblocking routing algorithm for benes networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chakrabarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computation World</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISSCC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JETCAS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bit-tactical: A software/hardware approach to exploiting value and bit sparsity in neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Delmas</forename><surname>Lascorz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Permdnn: Efficient compressed dnn architecture with permuted diagonal matrices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A configurable cloud-scale dnn processor for real-time ai</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The state of sparsity in deep neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09574v1[cs.LG</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tangram: Optimized coarse-grained dataflow for scalable nn accelerators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sparten: A sparse tensor accelerator for convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gondimalla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Caffe con troll: Shallow ideas to speed up deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hadjis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DanaC</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Eie: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ucnn: Exploiting computational reuse in deep neural networks via weight repetition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Extensor: An accelerator for sparse tensor algebra</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hegde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Smash: Co-designing software compression and hardware-accelerated indexing for efficient sparse matrix operations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kanellopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neurocube: A programmable digital neuromorphic architecture with high-density 3d memory</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Packing sparse convolutional neural networks for efficient systolic array implementations: Column combining under joint optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maeri: Enabling flexible dataflow mapping over dnn accelerators via reconfigurable interconnects</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding reuse, performance, and hardware cost of dnn dataflow: A data-centric approach</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel routing algorithms in benes-clos networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Liew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Processing-in-memory for energy-efficient neural network training: A heterogeneous approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nvidia tesla v100 gpu architecture</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Volta Architecture Whitepaper</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Outerspace: An outer product based sparse matrix multiplication accelerator</title>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scnn: An accelerator for compressed-sparse convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Parashar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning inference in facebook data centers: Characterization, performance optimizations and hardware implications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<idno>abs/1811.09886</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Maxnvm: Maximizing dnn storage density and inference efficiency with sparse encoding and error mitigation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pentecost</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Compressing dma engine: Leveraging activation sparsity for training deep neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Scale-sim: Systolic cnn accelerator simulator</title>
		<author>
			<persName><forename type="first">A</forename><surname>Samajdar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02883v1[cs.DC]16</idno>
		<imprint>
			<date type="published" when="2018-10">Oct 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A scalable near-memory architecture for training deep neural networks on large in-memory datasets</title>
		<author>
			<persName><forename type="first">F</forename><surname>Schuiki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Simba: Scaling deep-learning inference with multi-chip-module-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pipelayer: A pipelined reram-based accelerator for deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hypar: Towards hybrid parallelism for deep learning accelerator array</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tensor2tensor for neural machine translation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>abs/1803.07416</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scaledeep: A scalable compute architecture for learning and evaluating deep networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sparse reram engine: joint exploration of activation and weight sparsity in compressed neural networks</title>
		<author>
			<persName><forename type="first">T.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Cambricon-x: An accelerator for sparse neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">To prune, or not to prune: exploring the efficacy of pruning for model compression</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.01878v2</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
