<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ensemble Application of Convolutional Neural Networks and Multiple Kernel Learning for Multimodal Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science and Mathematics</orgName>
								<orgName type="institution">University of Stirling</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing Science and Mathematics</orgName>
								<orgName type="institution">University of Stirling</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Newton</forename><surname>Howard</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Computational Neuroscience and Functional Neurosurgery</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Ensemble Application of Convolutional Neural Networks and Multiple Kernel Learning for Multimodal</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Ensemble Application of Convolutional Neural Networks and Multiple Kernel Learning for Multimodal Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">722A17945FA87C4AE6B44DF2343ED195</idno>
					<idno type="DOI">10.1016/j.neucom.2016.09.117</idno>
					<note type="submission">Received date: 29 September 2015 Revised date: 4 August 2016 Accepted date: 22 September 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The advent of the Social Web has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. In pace with a global deluge of videos from billions of computers, smartphones, tablets, university projectors and security cameras, the amount of multimodal content on the Web has been growing exponentially, and with that comes the need for decoding such information into useful knowledge. In this paper, a multimodal affective data analysis framework is proposed to extract user opinion and emotions from video content. In particular, multiple kernel learning is used to combine visual, audio and textual modalities. The proposed framework outperforms the state-of-the-art model in multimodal sentiment analysis research with a margin of 10-13% and 3-5% accuracy on polarity detection and emotion recognition, respectively. The paper also proposes an extensive study on decision-level fusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Subjectivity detection and sentiment analysis consist of the automatic identification of the human mind's private states, e.g., opinions, emotions, moods, behaviors and beliefs <ref type="bibr" target="#b0">[1]</ref>. In particular, the former focuses on classifying sentiment data as either objective (neutral) or subjective (opinionated), while the latter aims to infer a positive or negative polarity. Hence, in most cases, both tasks are considered binary classification problems.</p><p>To date, most of the work on sentiment analysis has been carried out on text data. With a videocamera in every pocket and the rise of social media, people are now making use of videos (e.g., YouTube, Vimeo, VideoLectures), images (e.g., Flickr, Picasa, Facebook) and audio files (e.g., podcasts) to air their opinions on social media platforms. Thus, it has become critical to find new methods for the mining of opinions and sentiments from these diverse modalities. Plenty of research has been carried out in the field of audio-visual emotion recognition. Some work has also been conducted on fusing audio, visual and textual modalities to detect emotion from videos. However, a unique common framework is still missing for both tasks. There are also very few studies combining textual clues with audio and visual features. This leads to the need for more extensive research on the use of these three channels together. This paper aims to solve the two key research questions given below -</p><p>• Is a common framework useful for both multimodal emotion and sentiment analysis?</p><p>• Can audio, visual and textual features jointly enhance the performance of unimodal and bimodal emotion and sentiment analysis classifiers?</p><p>Studies conducted in the past lacked extensive research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref> and very few of them clearly described the extraction of features and fusion of the information extracted from different modalities. In this paper, we discuss the Email addresses: sp47@cs.stir.ac.uk (Soujanya Poria), peng0065@ntu.edu.sg (Haiyun Peng), ahu@cs.stir.ac.uk (Amir Hussain), newton.howard@nds.ox.ac.uk (Newton Howard), cambria@ntu.edu.sg (Erik Cambria)</p><p>Preprint submitted to Elsevier <ref type="bibr">February 7, 2017</ref> A C C E P T E D M A N U S C R I P T feature extraction process from different modalities in detail and explain how to use such features for multimodal affect analysis. The YouTube dataset originally developed by <ref type="bibr" target="#b4">[5]</ref> and the IEMOCAP dataset <ref type="bibr" target="#b5">[6]</ref> were used to demonstrate the accuracy of the proposed framework. We used several supervised classifiers for the sentiment classification task: the CLM-Z <ref type="bibr" target="#b6">[7]</ref> based method was used for feature extraction from visual modality; the openSMILE toolkit was used to extract various features from audio; and finally, textual features were extracted using a deep convolutional neural network (CNN). The fusion of these heterogeneous features was carried out by means of multiple kernel learning (MKL) using support vector machine (SVM) as a classifier with different types of kernel. The rest of the paper is organized as follows: Section 2 proposes motivations behind this work; Section 3 discusses related works on multimodal emotion detection, sentiment analysis and multimodal fusion; Section 4 describes the used datasets in detail; Sections 5, 6 and 7 explain how visual, audio, and textual data are processed, respectively; Section 8 proposes experimental results; Section 9 proposes a faster version of the framework; finally, Section 10 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivations</head><p>The research in this field is rapidly picking up and has attracted the attention of academia and industry alike. Combined with advances in signal processing and AI, this research has led to the development of advanced intelligent systems that intend to detect and process affective information contained in multimodal sources. However, the majority of such state-of-the-art frameworks rely on processing a single modality, i.e., text, audio, or video. Additionally, all of these systems are known to exhibit limitations in terms of meeting robustness, accuracy, and overall performance requirements, which, in turn, greatly restricts the usefulness of such systems in real-world applications.</p><p>The aim of multi-sensor data fusion is to increase the accuracy and reliability of estimates <ref type="bibr" target="#b7">[8]</ref>. Many applications, such as navigation tools, have already demonstrated the potential of data fusion. These illustrate the importance and feasibility of developing a multimodal framework that could cope with all three sensing modalities -text, audio, and video in human-centric environments. Humans communicate and express their emotions and sentiments through different channels. Textual, audio, and visual modalities are concurrently and cognitively exploited to enable effective extraction of the semantic and affective information conveyed in conversation <ref type="bibr" target="#b8">[9]</ref>.</p><p>People are gradually shifting from text to video to express their opinion about a product or service, as it is now much easier and faster to produce and share them. For the same reasons, potential customers are now more inclined to browse for video reviews of the product they are interested in, rather than looking for lengthy written reviews. Another reason for doing this is that, while reliable written reviews are quite hard to find, it is sufficient to search for the name of the product on YouTube and choose the clips with most views in order to find good video reviews. Finally, videos are generally more reliable than written text as reviewers often reveal their identity by showing their face, which also allows viewers to better decode conveyed emotions <ref type="bibr" target="#b9">[10]</ref>.</p><p>Hence, videos can be an excellent resource for emotion and sentiment analysis but the medium also comes with major challenges which need to be overcome. For example, expressiveness of opinion varies widely from person to person <ref type="bibr" target="#b1">[2]</ref>. Some people express their opinions more vocally, some more visually and others rely exclusively on logic and express little emotion. These personal differences can help guide us towards the affect seeking expression. When a person expresses his or her opinions with more vocal modulation, the audio data will often contain most of the clues indicative of an opinion. When a person is highly communicative via facial expressions, most of the data needed for opinion mining may often be determined through facial expression analysis. So, a generic model needs to be developed which can adapt itself for any user and provide a consistent result. Both of our multimodal affect analysis models are trained on robust data containing opinions or narratives from a wide range of users. In this paper, we show that the ensemble application of feature extraction from different types of data and modalities is able to significantly enhance the performance of multimodal emotion and sentiment approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work</head><p>In this section, we discuss related works in multimodal affect detection covering both emotion and sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Text based emotion and sentiment analysis</head><p>Sentiment analysis systems can be broadly categorized into knowledge-based and statistics-based systems <ref type="bibr" target="#b10">[11]</ref>. While the use of knowledge bases was initially more popular for the identification of emotions and polarity in text, sentiment analysis researchers have recently been using statistics-based approaches, with a special focus on supervised statistical methods. For example, Pang et al. <ref type="bibr" target="#b11">[12]</ref> compared the performance of different machine learning algorithms on a movie review dataset and obtained 82.90% accuracy, using only a large number of textual features. A recent approach by Socher et al. <ref type="bibr" target="#b12">[13]</ref> obtained even better accuracy (85%) on the same dataset using a recursive neural tensor network (RNTN). Yu and Hatzivassiloglou <ref type="bibr" target="#b13">[14]</ref> used semantic orientation of words to identify polarity at sentence level. Melville et al. <ref type="bibr" target="#b14">[15]</ref> developed a framework that exploits word-class association information for domain-dependent sentiment analysis.</p><p>Other unsupervised or knowledge-based approaches to sentiment analysis include Turney et al. <ref type="bibr" target="#b15">[16]</ref>, who used seed words to calculate the polarity and semantic orientation of phrases; Melville et al. <ref type="bibr" target="#b16">[17]</ref>, who proposed a mathematical model to extract emotional clues from blogs and then used these for sentiment detection; Gangemi et al. <ref type="bibr" target="#b17">[18]</ref>, who presented an unsupervised frame-based approach to identify opinion holders and topics based on the assumption that events and situations are the primary entities for contextualizing opinions; and Cambria et al. <ref type="bibr" target="#b18">[19]</ref>, who proposed a multidisciplinary framework for polarity detection based on SenticNet <ref type="bibr" target="#b19">[20]</ref>, a concept-level commonsense knowledge base.</p><p>Sentiment analysis research can also be categorized as single-domain <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref> or cross-domain <ref type="bibr" target="#b22">[23]</ref>. The work presented in <ref type="bibr" target="#b23">[24]</ref> discusses spectral feature alignment to group domain-specific words from different domains into clusters. They first incorporated domain-independent words to help the clustering process and then exploited the resulting clusters to reduce the gap between domain-specific words of two domains. Bollegala et al. <ref type="bibr" target="#b24">[25]</ref> developed a sentiment-sensitive distributional thesaurus by using labeled training data from the source domain and unlabeled training data from both the source and target domains. Sentiment sensitivity was obtained by including documents' sentiment labels into the context vector. At the time of training and testing, this sentiment thesaurus was used to expand the feature vector.</p><p>The task of automatically identifying fine-grained emotions, such as anger, joy, surprise, fear, disgust, and sadness, explicitly or implicitly expressed in a text has been addressed by several researchers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. There are a number of theories on emotion taxonomy which spans from Ekman's emotion categorization model to the Hourglass of Emotion (Figure <ref type="figure" target="#fig_0">1</ref>) <ref type="bibr" target="#b27">[28]</ref>. So far, approaches to text-based emotion and sentiment detection rely mainly on rule-based techniques, bag of words modeling using a large sentiment or emotion lexicon <ref type="bibr" target="#b28">[29]</ref>, or statistical approaches that assume the availability of a large dataset annotated with polarity or emotion labels <ref type="bibr" target="#b29">[30]</ref>.</p><p>Several supervised and unsupervised classifiers have been built to recognize emotional content in texts <ref type="bibr" target="#b30">[31]</ref>. The SNoW architecture <ref type="bibr" target="#b31">[32]</ref> is one of the most useful frameworks for text-based emotion detection. In the last decade, researchers have been focusing on sentiment extraction from texts of different genres, such as product reviews <ref type="bibr" target="#b32">[33]</ref>, news <ref type="bibr" target="#b33">[34]</ref>, tweets <ref type="bibr" target="#b34">[35]</ref>, and essays <ref type="bibr" target="#b35">[36]</ref>, to name a few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Audio Visual Emotion and Sentiment Analysis</head><p>In 1970, Ekman et al. <ref type="bibr" target="#b36">[37]</ref> carried out extensive studies on facial expressions. Their research showed that universal facial expressions are able to provide sufficient clues to detect emotions. They used anger, sadness, surprise, fear, disgust, and joy as six basic emotion classes. Such basic affective categories are sufficient to describe most of the emotions expressed by facial expression. However, this list does not include the emotion expressed through facial expression by a person when he or she shows disrespect to someone; thus, a seventh basic emotion, contempt, was introduced by Matsumoto <ref type="bibr" target="#b37">[38]</ref>.</p><p>The Active Appearance Model <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> and Optical Flow-based techniques <ref type="bibr" target="#b40">[41]</ref> are common approaches that use facial expression coding system (FACS) to understand facial expressions. Exploiting action units (AU) as features in well known classifier like k nearest neighbors, Bayesian networks, hidden Markov models (HMM), and artificial neural networks (ANN) <ref type="bibr" target="#b41">[42]</ref> has helped many researchers to infer emotions from facial expression. The performance of several machine-learning algorithms for detecting emotions from facial expressions is presented in Table <ref type="table" target="#tab_0">1</ref> (cited from Chen et al. <ref type="bibr" target="#b42">[43]</ref>). All such systems, however, use different, manually-crafted corpora, which makes it impossible to perform a comparative evaluation of their performance. To this end, recently Xu et al. <ref type="bibr" target="#b43">[44]</ref> constructed a framework which takes color features from the superpixel of images and later a piece-wise linear transformation was used to learn the emotional feature distribution. The framework is basically a novel feature learning framework from emotion labeled set of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Recent studies on speech-based emotion analysis <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref> have focused on identifying several acoustic features such as fundamental frequency (pitch), intensity of utterance <ref type="bibr" target="#b42">[43]</ref>, bandwidth, and duration. The speakerdependent approach gives much better results than the speaker-independent approach, as shown by the excellent results of Navas et al. <ref type="bibr" target="#b54">[55]</ref>, where about 98% accuracy was achieved by using the Gaussian mixture model (GMM) as a classifier, with prosodic, voice quality as well as Mel frequency cepstral coefficients (MFCC) employed as speech features.</p><p>When it comes to fusing audio-visual emotion recognition, two of the early works were done by De Silva et al. <ref type="bibr" target="#b55">[56]</ref> and Chen et al. <ref type="bibr" target="#b56">[57]</ref>. Both of these works showed that a bimodal system yielded a higher accuracy than any unimodal system. More recent research on audio-visual emotion recognition has been conducted at either feature level <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref> or decision level <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. Though there are plenty of research articles on audio-visual emotion recognition, only a few pieces of research works have been done on multimodal emotion or sentiment analysis using </p><formula xml:id="formula_0">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Processing Classification algorithm Accuracy Lanitis et al. <ref type="bibr" target="#b38">[39]</ref> Appearance Model Distance-based 74% Cohen et al. <ref type="bibr" target="#b44">[45]</ref> Appearance Model Bayesian network 83% Mase <ref type="bibr" target="#b45">[46]</ref> Optical flow kNN 86% Rosenblum et al. <ref type="bibr" target="#b46">[47]</ref> Optical flow ANN 88% Otsuka &amp; Ohya <ref type="bibr" target="#b47">[48]</ref> 2D FT of optical flow HMM 93% Yacoob &amp; Davis <ref type="bibr" target="#b48">[49]</ref> Optical flow Rule-based 95% Essa &amp; Pentland <ref type="bibr" target="#b49">[50]</ref> Optical flow Distance-based 98% textual clues along with visual and audio modality. The works as described in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b64">65]</ref> fused information from audio, visual and textual modalities to extract emotion and sentiment. Metallinou et al. <ref type="bibr" target="#b66">[66]</ref> and Eyben et al. <ref type="bibr" target="#b67">[67]</ref> fused audio and textual modality for emotion recognition. Both these approaches relied on feature-level fusion. Wu et al. <ref type="bibr" target="#b68">[68]</ref> fused audio and textual clues at decision level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multiple Kernel Learning</head><p>Several studies have reported that MKL outperforms the average kernel baselines. MKL is very similar to group LASSO which is a feature selection method where features are organized into groups. However, the choice of kernel coefficients can have a significant impact on the classification accuracy and efficiency of MKL <ref type="bibr" target="#b69">[69]</ref>.</p><p>For example, in Alzheimer's disease patients, different types of tests correspond to different modalities that can reveal varied aspects of the diagnosis. MR images may show only a slight hippocampal atrophy while the FDG-PET image may reveal increased hypometabolism suggestive of Alzheimer. In <ref type="bibr" target="#b70">[70]</ref>, MKL was used simultaneously for optimizing different modalities in Alzheimer's disease. However, in order to deal with co-morbidity with other diseases, they used the hinge loss function to penalize misclassified samples that did not scale well with the number of kernels. Adaptive MKL (AdaMKL) was proposed in <ref type="bibr" target="#b71">[71]</ref> based on biconvex optimization and Gaussian kernels. Here, the objective function alternatively learns one component at a time while fixing the others, resulting in an increased computation time.</p><p>In <ref type="bibr" target="#b72">[72]</ref>, higher order kernels are used to enhance the learning of MKL. Here, block co-ordinate gradient optimization is used as it approximates the Hessian matrix of derivatives as a diagonal resulting in loss of information. MKL is also used in signal processing where grouping of features is useful to improve the interpretability of the learned parameters <ref type="bibr" target="#b73">[73]</ref>.</p><p>MKL was applied to a Polish opinion aggregator service that contained textual opinions of different products, but this study did not consider the hierarchical relation of different attributes of products <ref type="bibr" target="#b74">[74]</ref>. Group-sensitive MKL for object recognition in images integrates a global kernel clustering method with MKL for sharing of group-sensitive information <ref type="bibr" target="#b75">[75]</ref>. Hence, the two different kernels are used to group the training data and the kernels are aligned during optimization. They showed that their method outperformed baseline grouping strategies on the WikipediaMM dataset of real-world web images. The drawback of this method is that a looping strategy is used to relabel groups and may not reach the global optimum solution.</p><p>MKL was used to detect the presence of a large lump in images using a convolution kernel <ref type="bibr" target="#b76">[76]</ref>. However, they only considered Gaussian features for the images. In <ref type="bibr" target="#b77">[77]</ref>, MKL was used to combine and re-weight multiple features by using structured latent variables during video event detection <ref type="bibr" target="#b77">[77]</ref>. Here, two different types of kernels are used to group global features and segments in the test video that are similar to the training videos. While, the results on TRECVID dataset of video events outperformed baselines, the method requires tuning of parameters and assumes random initialization of latent variables.</p><p>Multimodal features were fused at different levels of fusion for the indexing of web data in <ref type="bibr" target="#b78">[78]</ref>. The concept of kernel slack variables for each of the base kernels was used to classify YouTube videos in <ref type="bibr" target="#b79">[79]</ref>. In order to select good features and discard bad features that may not be useful to the kernel, Liu et al. <ref type="bibr" target="#b80">[80]</ref> used a beta prior distribution. Recently, MKL with Fourier transform on the Gaussian kernels has been applied to Alzheimer's Disease classification using both sMRI and fMRI images <ref type="bibr" target="#b81">[81]</ref>. Researchers used L2 norm to enforce group sparsity constraints which were not robust on noisy datasets. Lastly, Online MKL shows good accuracy on object recognition tasks by extending online kernel learning to online MKL, however, the time complexity of the methods is dependent on the dataset <ref type="bibr" target="#b82">[82]</ref>.</p><formula xml:id="formula_1">A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dataset Used</head><p>In this section, we describe the datasets used in multimodal sentiment and emotion analysis experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multimodal Sentiment Analysis Dataset</head><p>For our experiment, we use the dataset developed by Morency et al. <ref type="bibr" target="#b1">[2]</ref>. They started collecting the videos from popular social media (e.g., YouTube) using several keywords to produce search results consisting of videos of either product reviews or recommendation. Some of these keywords are my favorite products, non recommended perfumes, recommended movies etc. A total of 80 videos were collected in this way. The dataset includes 15 male and 65 female speakers, with their age ranging approximately from 20-60 years.</p><p>The videos were converted to mp4 format with a standard size of 360x480. All videos were pre-processed to avoid the issues of introductory titles and multiple topics, and the length of the videos varied from 2-5 minutes. Many videos on YouTube contained an introductory sequence where a title was shown, sometimes accompanied with a visual animation. To address this issue, the first 30 seconds was removed from each video. Morency et al. <ref type="bibr" target="#b1">[2]</ref> provided transcriptions with the videos. Each video was segmented into its utterances and each utterance was labeled by a sentiment, thanks to <ref type="bibr" target="#b1">[2]</ref>. Because of the annotation scheme of the dataset, textual data was available for our experiment. On average each video has 6 utterances and each utterance is 5 seconds long. The dataset contains 498 utterances labeled either positive, negative or neutral. In our experiment we did not consider neutral labels, which led to the final dataset consisting of 448 utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multimodal Emotion Analysis Dataset</head><p>The USC IEMOCAP database <ref type="bibr" target="#b5">[6]</ref> was collected for the purposes of studying multimodal expressive dyadic interactions. This dataset contains 12 hours of video data split into 5 minutes of dyadic interaction between professional male and female actors. It was assumed that the interaction between the speakers are more affectively enriched than a speaker reading an emotional script. Each interaction session was split into spoken utterances. At least 3 annotators assigned the one emotion category, i.e., happy, sad, neutral, angry, surprised, excited, frustration, disgust, fear and other to each utterance. In this research work, we consider only the utterances with majority agreement (i.e., at least two out of three annotators labeled the same emotion) in the emotion classes of: Angry, Happy, Sad, and Neutral. Table <ref type="table" target="#tab_1">2</ref> shows the per emotion class distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Extracting Features from Visual Data</head><p>Humans are known to express emotions through facial expression, to a great extent. As such, these expressions play a significant role in the identification of emotions in a multimodal stream. A facial expression analyzer automatically identifies emotional clues associated with facial expressions, and classifies these expressions to define sentiment categories and discriminate between them. We use positive and negative as sentiment classes in the classification problem. In the annotations provided with the YouTube dataset, each video was segmented into utterances and each of the utterances has the length of a few seconds. Every utterance was annotated as either 1, 0 and -1, denoting positive, neutral and negative sentiment. Using a matlab code, we converted all videos in the dataset to image frames, after which we extracted facial features from each image frame. To extract facial characteristic points (FCPs) from the images, we used the facial recognition library CLM-Z <ref type="bibr" target="#b6">[7]</ref>. From each image we extracted 68 FCPs; see examples in Table <ref type="table" target="#tab_2">3</ref>. The FCPs were used to construct facial features, which were defined as distances between FCPs; see examples in Table <ref type="table" target="#tab_3">4</ref>. Middle of the Right Mouth Side GAVAM <ref type="bibr" target="#b83">[83]</ref> was also used to extract facial expression features from the face. Table <ref type="table" target="#tab_4">5</ref> shows the extracted features from facial images. In our experiment we used the features extracted by CLM-Z along with the features extracted using GAVAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>If a segment of a video has n number of images, then we extracted features from each image and take mean and standard deviation of those feature values in order to compute the final facial expression feature vector for an utterance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Extracting Features from Audio Data</head><p>We automatically extracted audio features from each annotated segment of the videos. Audio features were also extracted in 30Hz frame-rate and we used a sliding window of 100ms. To compute the features we used the open source software openSMILE <ref type="bibr" target="#b84">[84]</ref>. Specifically, this toolkit automatically extracts pitch and voice intensity. Zstandardization was used to perform voice normalization. Basically, voice normalization was performed and voice intensity was thresholded to identify samples with and without voice. The features extracted by openSMILE consist of several low-level descriptors (LLD) and their statistical functionals. Some of the functionals are amplitude mean, arithmetic mean, root quadratic mean, standard deviation, flatness, skewness, kurtosis, quartiles, inter-quartile ranges, linear regression slope etc. Taking into account all functionals of each LLD, we obtained 6373 features. Some of the useful key LLD extracted by openSMILE are described below.</p><p>• Mel frequency cepstral coefficients -MFCC were calculated based on short time Fourier transform (STFT).</p><p>First, log-amplitude of the magnitude spectrum was taken, and the process was followed by grouping and smoothing the fast Fourier transform (FFT) bins according to the perceptually motivated Mel-frequency scaling.</p><p>• Spectral Centroid -Spectral Centroid is the center of gravity of the magnitude spectrum of the STFT. Here, M i [n] denotes the magnitude of the Fourier transform at frequency bin n and frame i. The centroid is used to The spectral centroid is calculated as</p><formula xml:id="formula_2">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_3">C i = ∑ n i=0 nM i [n] ∑ n i=0 M i [n]</formula><p>• Spectral Flux -Spectral Flux is defined as the squared difference between the normalized magnitudes of successive windows:</p><formula xml:id="formula_4">F i = ∑ n n=1 (N t [n] -N t-1 [n]</formula><p>) 2 where N t [n] and N t-1 [n] are the normalized magnitudes of the Fourier transform at the current frame t and the previous frame t-1, respectively. The spectral flux represents the amount of local spectral change.</p><p>• Beat histogram -It is a histogram showing the relative strength of different rhythmic periodicities in a signal.</p><p>It is calculated as the auto-correlation of the RMS.</p><p>• Beat sum -This feature is measured as the sum of all entries in the beat histogram. It is a very good measure of the importance of regular beats in a signal.</p><p>• Strongest beat -It is defined as the strongest beat in a signal, in beats per minute, and it is found by identifying the strongest bin in the beat histogram.</p><p>• Pause duration -Pause direction is the percentage of time the speaker is silent in the audio segment.</p><p>• Pitch -It is computed by the standard deviation of the pitch level for a spoken segment.</p><p>• Voice Quality -Harmonics to noise ratio in the audio signal.</p><p>• PLP -The Perceptual Linear Predictive Coefficients of the audio segment were calculated using the openSMILE toolkit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Extracting Features from Textual Data</head><p>For feature extraction from textual data, we used a CNN. The trained CNN features were then fed into a SVM for classification. So, in particular we used CNN as trainable feature extractor and SVM as a classifier.</p><p>The intuition for building this hybrid classifier SVM-CNN is to combine the merits of each classifier and form a hybrid classifier to enhance accuracy. Recent studies <ref type="bibr" target="#b85">[85]</ref> also show the use of CNN for feature extraction. In theory, the training process of CNN is similar to MLP as CNN is an extension of traditional MLP. MLP network is trained using a back-propagation algorithm which uses Empirical Risk Minimization. It tries to minimize the errors in training data. Once it finds the hyperplane, regardless of global or local optimum, the training process is stopped. This means that it does not try to improve the separation of the instances from the hyperplane. Wherein, SVM tries to minimize the generalization error on unseen data based on Structural Risk Minimization algorithm using a fixed probability distribution on training data. It therefore aims to maximize the distance between training instances and hyperplane, so the margin area between two separate training classes is maximized. This separating hyperplane is a global optimum solution. So, SVM is more generalized than MLP which enhances the classification accuracy.</p><p>On the other hand, CNN automatically extracts key features from the training data. It grasps contextual local features from a sentence and after several convolution operations it finally forms a global feature vector out of those local features. CNN does not need the hand-crafted features used in a traditional supervised classifier. The handcrafted features are difficult to compute and a good guess for encoding the features is always necessary in order to get satisfactory result. CNN uses a hierarchy of local features which are important to learn context. The hand-crafted features often ignore such a hierarchy of local features. Features extracted by CNN can therefore be used instead of hand-crafted features, as they carry more useful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>The hybrid classifier SVM-CNN therefore inherits the merits from each classifier and should produce a better result.</p><p>The idea behind convolution is to take the dot product of a vector of k weights w k also known as kernel vector with each k-gram in the sentence s(t) to obtain another sequence of features c(t) = (c 1 (t), c 2 (t), . . . , c L (t)).</p><formula xml:id="formula_5">c j = w k T .x i:i+k-1 (1)</formula><p>We then apply a max pooling operation over the feature map and take the maximum value ĉ(t) = max{c(t)} as the feature corresponding to this particular kernel vector. Similarly, varying kernel vectors and window sizes are used to obtain multiple features <ref type="bibr" target="#b86">[86]</ref>.</p><p>For each word x i (t) in the vocabulary, an d dimensional vector representation is given in a look up table that is learned from the data <ref type="bibr" target="#b87">[87]</ref>. The vector representation of a sentence is hence a concatenation of vectors for individual words. Similarly we can have look up tables for other features. One might want to provide features other than words if these features are suspected to be helpful. The convolution kernels are then applied to word vectors instead of individual words.</p><p>We use these features to train higher layers of the CNN, to represent bigger groups of words in sentences. We denote the feature learned at hidden neuron h in layer l as F l h . Multiple features may be learned in parallel in the same CNN layer. The features learned in each layer are used to train the next layer</p><formula xml:id="formula_6">F l = ∑ n h h=1 w h k * F l-1 (2)</formula><p>where * indicates convolution and w k is a weight kernel for hidden neuron h and n h is the total number of hidden neurons. The CNN sentence model preserves the order of words by adopting convolution kernels of gradually increasing sizes that span an increasing number of words and ultimately the entire sentence. Each word in a sentence was represented using word embedding and part-of-speech of that word. The details are as follows -</p><p>• Word Embeddings -We employ the publicly available word2vec vectors that were trained on 100 billion words from Google News. The vectors have dimensionality 300 trained using the continuous bag-of-words architecture <ref type="bibr" target="#b87">[87]</ref>. Words not present in the set of pre-trained words are initialized randomly.</p><p>• Part of Speech -The part of speech of each word was also appended to the word's vector representation. As there are a total of 6 part of speech, so the length of part of speech vector was 6.</p><p>So, in the end a word was represented by a 306 dimensional vector. Each sentence was wrapped to a window of 50 words to reduce the number of parameters and hence over-fitting the model. The CNN we developed in our experiment had two convolution layers, a kernel size of 3 and 50 feature maps was used in the first convolution layer and a kernel size 2 and 100 feature maps in the second. It should be noted that the output of each convolution hidden layer is computed using a non-linear function (in our case we use tanh). Each convolution layer was followed by a max-pool layer. The max-pool size of the first and second max-pool layer was 2. The penultimate max-pool layer is followed by a fully connected layer with softmax output. We used 500 neurons in the full connected layer. The output layer corresponded to two neurons for each class of sentiments.</p><p>We used the output of the fully connected layer (layer 6) of the network as our feature vector. This feature vector was used in the final fusion process. So, in the fusion the 500 dimensional textual vector was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Other Sentence-Level Textual Features</head><p>We have ultimately fed the features extracted by CNN to the SVM and MKL. Motivated by the state of the art <ref type="bibr" target="#b88">[88]</ref>, we have decided to use other sentence-level features with the CNN extracted features. Below, we explain these features -</p><p>• Commonsense Knowledge Features -Commonsense knowledge features consist of concepts are represented by means of AffectiveSpace <ref type="bibr" target="#b89">[89]</ref>. In particular, concepts extracted from text through the semantic parser are encoded as 100-dimensional real-valued vectors and then aggregated into a single vector representing the sentence </p><formula xml:id="formula_7">A C C E P T E D M A N U S C R I P T</formula><formula xml:id="formula_8">x i = N ∑ j=1 x i j ,</formula><p>where x i is the i-th coordinate of the sentence's feature vector, i = 1, ..., 100; x i j is the i-th coordinate of its j-th concept's vector, and N is the number of concepts in the sentence (extracted by means of our concept parser <ref type="bibr" target="#b90">[90]</ref>).</p><p>• Sentic Feature -The polarity scores of each concept extracted from the sentence were obtained from SenticNet and summed up to produce a single scalar feature.</p><p>• Part-of-Speech Feature -This feature is defined by the number of adjectives, adverbs and nouns in the sentence, which give three distinct features.</p><p>• Modification Feature -This is a single binary feature. For each sentence, we obtained its dependency tree from the dependency parser. This tree was analyzed to determine whether there is any word modified by a noun, adjective, or adverb. The modification feature is set to 1 in case of any modification relation in the sentence; 0 otherwise.</p><p>• Negation Feature -Similarly, the negation feature is a single binary feature determined by the presence of any negation in the sentence. It is important because the negation can invert the polarity of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Experimental Results</head><p>For the experiment, we removed all neutral classes resulting in the final dataset of 448 utterances. Of these, 247 were negative and 201 were positive. In this section, we describe the experimental results of the unimodal and multimodal frameworks. For each experiment, we carried out 10-fold cross validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.">Extracting sentiment from Visual modality</head><p>To extract sentiment from only visual modality we used SVM classifier with a polykernel. Features were extracted using the method explained in Section 5. Table <ref type="table" target="#tab_5">6</ref> shows the results for each class -{positive and negative}.</p><p>Clearly, the recall is lower for positive samples. This means many negative instances were labelled as positive. Below, we show some features which took major role to confuse the classifier.</p><p>• The large change in distance of FCPs on eyelid from lower eyebrow.</p><p>• Small change between the two corners of the mouth (F 49 and F 55 as shown in Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>We compared the performance of SVM with other classifiers like Multilayer Perceptron (MLP) and Extreme Learning Machine (ELM) <ref type="bibr" target="#b91">[91]</ref>. SVM was found to produce best performance results. On visual modality, the best state-of-the-art result on this dataset was obtained by <ref type="bibr" target="#b4">[5]</ref> where they got 67.31% accuracy. In terms of accuracy our method has outperformed their result by achieving 75.22% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.">Extracting sentiment from Audio Modality</head><p>For each utterance, we extracted the features as stated in Section 6 and formed a feature vector which was then fed to SVM. Table <ref type="table" target="#tab_6">7</ref> shows that for the positive class, the classifier obtained relatively lower recall than for the visual modality obtained. Rosas et al. <ref type="bibr" target="#b4">[5]</ref> obtained 64.85% accuracy on audio modality. Conversely, a 74.49% accuracy was obtained using the proposed method, outperforming the accuracy of the state-of-the-art-model <ref type="bibr" target="#b4">[5]</ref>. For 1 utterance in the dataset, there is no audio data. This resulted in 447 utterances in the final dataset for this experiment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.">Extracting sentiment from Textual Modality</head><p>As we described in Section 7, deep Convolutional Network (CNN) was used to extract features from textual modality and a SVM classifier was then employed on those features to identify sentiment. We call this hybrid classifier CNN-SVM. Comparing the performance of CNN-SVM with other supervised classifiers, we found it to offer the best classification results (Table <ref type="table" target="#tab_7">8</ref>). In this experiment, our method also outperformed the state-of-the-art accuracy achieved by <ref type="bibr" target="#b4">[5]</ref>. For 2 utterances, no text data was available in the dataset. So, the final dataset for this experiment consists of 446 utterances out of which 246 are negative and 200 are positive.</p><p>The results shown in Table <ref type="table" target="#tab_7">8</ref> were obtained when the utterances in the dataset were translated from Spanish to English. Without this translation process we obtained a much lower accuracy of 68.56%. Another experimental study showed that while using CNN-SVM produced a 79.14% accuracy, an accuracy of only 75.50% was achieved using CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Feature-Level Fusion of Audio, Visual and Textual Modalities</head><p>After extracting features from all modalities, we merged them to form a long feature vector. That feature vector was then fed to MKL for the classification task. We tested several polynomial kernels of different degree and RBF kernels having different gamma values as base kernels in MKL. We compared the performance of SPG-GMKL (Spectral Projected Gradient-Generalized Multiple Kernel Learning ) <ref type="bibr" target="#b92">[92]</ref> and Simple-MKL in the classification task and found that SPG-GMKL outperformed Simple-MKL with a 1.3% relative error reduction rate. Based on the cross validation performance, the best set of kernels and their corresponding parameters were chosen. Finally, we chose a configuration with 8 kernels: 5 RBF with gamma from 0.01 to 0.05 and 3 polynomial with powers 2, 3, 4.</p><p>Table <ref type="table" target="#tab_8">9</ref> shows the results of the audio-visual feature-level fusion. Clearly, the performance in terms of both precision and recall increased when these two modalities are fused.</p><p>Among the unimodal classifiers, textual modality was found to provide the most accurate classification result. We observed the same fact when textual features were fused with audio and visual modalities. Both the audio-textual (Table <ref type="table" target="#tab_9">10</ref>) and visual-textual (Table <ref type="table" target="#tab_10">11</ref>) framework outperformed the audio-visual framework. According to the experimental results, visual-textual modality performed best.</p><p>Table <ref type="table" target="#tab_12">13</ref> shows the results when all three modalities were fused producing a 87.89% accuracy. Clearly, this accuracy is higher than the best state-of-the-art framework, which obtained a 74.09% accuracy. The fundamental reason for our method outperforming the state-of-the-art method is the extraction of salient features from each modality before fusing those features using MKL.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.">Feature Selection</head><p>In order to see whether a reduced optimal feature subset can produce a better result than using all features, we conducted a cyclic Correlation-based Feature Subset Selection (CFS) using the training set of each fold. The main idea of CFS is that useful feature subsets should contain features that are highly correlated with the target class while being uncorrelated with each other. However, superior results were obtained when we used all features. This signifies that some relevant features were excluded by CFS. We then employed Principal Component Analysis (PCA) for feature selection to rank all features according to their importance in classification. To measure whether top K features selected PCA can produce better accuracy, we fed the top K features to the classifier. However, even worse accuracy was obtained than when using CFS based feature selection. When we took the combination of top K features from that ranking and CFS-based selected features and employed the classifier on them, we observed the best accuracy. To set the value of K, an exhaustive search was made and finally we found that K=300 gave the best result. This evaluation was carried out for each experiment stated in Section 8.1, 8.2 and 8.4.</p><p>For our audio, visual and textual fusion experiment using CFS and PCA, a total 437 features were selected out of which 305 features were textual, 74 were visual and 58 were from audio modality. This proves the fact that textual features were the most important for trimodal sentiment analysis thanks to CNN feature extractor. Table <ref type="table" target="#tab_13">14</ref> shows the comparative evaluation using feature selection method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.">Feature-Level Fusion for Multimodal Emotion Recognition</head><p>Besides doing the experiment on multimodal sentiment analysis dataset, we also carried out an extensive experiment on multimodal emotion analysis dataset as described in 4.2. We followed the same method as applied for the sentiment analysis dataset. However, instead of taking it as a binary classification task, we considered it as a 4-way classification. This dataset already provides the facial points detected by the markers and we only used those facial points in our study. CLM-Z was not able to detect faces in most of the facial images as the images in this dataset are small and of low resolution. Using a similar feature selection algorithm as described in Section 8.5, a total of 693 features were selected, of which 85 features were textual, 239 were audio and 369 were from visual modality.</p><p>In Table <ref type="table" target="#tab_14">15</ref> we see that both precision and recall of the Happy class is higher. However, Angry and Sad classes are very tough to distinguish from the textual clues. One of the possible reasons is both of these classes are negative emotions and many words are commonly used to express both of the emotions. On the other hand, the classifier was confused and often classified Neutral with Happy and Anger. Interestingly, it classifies Sad and Neutral classes well.   In the case of Audio modality (Table <ref type="table" target="#tab_15">16</ref>) we observe better accuracy than textual modality for Sad and Neutral classes. However, for Happy and Angry, the performance decreased. The confusion matrix shows the classifier performed poorly when distinguishing Angry from Happy. Clearly, audio features are unable to effectively classify these based on extracted features. However, the classifier performs very well to discriminate between the classes of Sad and Anger. Overall identification accuracy of the Neutral emotion has also increased. But Happy and Neutral emotions are still very hard to classify effectively by Audio classifier alone.</p><p>Visual modality produced the best accuracy (Table <ref type="table" target="#tab_16">17</ref>) when compared to other two modalities. The similar trend has been observed as textual modality. Angry and Sad faces are hard to classify using visual clues. However, Angry and Happy, Happy and Sad faces can be effectively classified. Neutral classes were also separated accurately in respect to other classes.</p><p>When we fuse the modalities using the feature-level fusion strategy (Table <ref type="table" target="#tab_17">18</ref>) as stated in Section 8.4, as expected higher accuracy was obtained than with unimodal classifiers. Although the identification accuracy has been improved for every emotion, the confusion between a Sad and Angry face is still higher. Neutral and Sad emotions are also more difficult to classify.</p><p>The comparison with the state-of-the-art model in terms of weighted accuracy shows that the proposed method performs significantly better. Comparing the weighted accuracy (WA) with the state of the art, the proposed method ob-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.">Decision-Level Fusion</head><p>In this section, we describe different frameworks that we developed for the decision-level fusion. Clearly, the motivation for developing these frameworks is to perform the fusion process in less time. The fusion frameworks were developed according to the architecture as shown in Figure <ref type="figure" target="#fig_5">3</ref>. Each of the experiments stated below were processed through the feature selection algorithm stated in Section 8.5.</p><p>Each block M i denotes a modality. As the architecture shows, modality M 1 and M 2 are fused using feature-level fusion and then at last stage are fused with another modality M 3 using decision-level fusion. For feature-level fusion of M 1 and M 2 , we used SPG-GMKL. The decision-level algorithm is described below -In decision-level fusion, we obtained the feature vectors from the above-mentioned methods but used separate classifier for each modality instead of concatenating feature vectors as in feature-level fusion. The output of each classifier was treated as a classification score. In particular, from each classifier we obtained a probability score for each sentiment class. In our case, as there are two sentiment classes, we obtained 2 probability scores from each modality. Let, q 12  1 and q 12 2 are the class probabilities resulted from the feature-level fusion of M 1 and M 2 . On the other hand let, q 3  1 and q 3 2 are the class probabilities of modality M 3 . We then form a feature vector by concatenating these class probabilities.</p><p>We also used sentic patterns <ref type="bibr" target="#b95">[94]</ref> to obtain the sentiment label for each text. If the result by sentic patterns for a sentence is "positive" then we included 1 in the feature vector, otherwise 0 was included in the feature vector. So, the final feature vector looks like this -[q 12  1 , q 12 2 , q 3 1 , q 3 2 , sentic] where sentic = 1 if the output of sentic patterns is positive otherwise we set sentic = 0. We then employed SVM on this feature vector in order to obtain the final polarity label.</p><p>The best accuracy was obtained when we early fused visual and audio modalities. However, when we fuse all the modalities without carrying out the early fusion, the obtained accuracy was lower. Table <ref type="table" target="#tab_1">20</ref> shows the decision-level accuracy in detail.   </p><p>It is convenient to define an 'activation matrix', H, such that the entry {h i j ∈ H; i = 1, ..., N; j = 1, ..., N h } is the activation value of the j-th hidden neuron for the i-th input pattern. The H matrix is:</p><formula xml:id="formula_10">H ≡    ϕ( ŵ1 • x 1 + b1 ) • • • ϕ( ŵN h • x 1 + bN h ) . . . . . . . . . ϕ( ŵ1 • x N + b1 ) • • • ϕ( ŵN h • x N + bN h )   <label>(5)</label></formula><p>In the ELM model, the quantities { ŵ j , b j } in (3) are set randomly and are not subject to any adjustment, and the quantities { w j , b} in ( <ref type="formula" target="#formula_9">4</ref> A matrix pseudo-inversion yields the unique L 2 solution, as proven in <ref type="bibr" target="#b96">[95]</ref>:</p><formula xml:id="formula_11">w = H + y<label>(7)</label></formula><p>The simple and efficient procedure to train an ELM therefore involves the following steps:</p><p>1. Randomly set the input weights ŵi and bias bi for each hidden neuron;</p><p>2. Compute the activation matrix, H, as per (5);  3. Compute the output weights by solving a pseudo-inverse problem as per <ref type="bibr" target="#b6">(7)</ref>.</p><p>Despite the apparent simplicity of the ELM approach, the crucial result is that even random weights in the hidden layer endow a network with a notable representation ability <ref type="bibr" target="#b96">[95]</ref>. Moreover, the theory derived in <ref type="bibr" target="#b100">[99]</ref> proves that regularization strategies can further improve its generalization performance. As a result, the cost function ( <ref type="formula">6</ref>) is augmented by an L 2 regularization factor as follows: min w { H wy 2 + λ w 2 } (8)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.">Experiment and Comparison with SVM</head><p>The experimental results in Table <ref type="table" target="#tab_18">22</ref> shows ELM and SVM offering equivalent performance in terms of accuracy. While for multimodal sentiment analysis SVM outperformed ELM with a sharp 1.23% accuracy margin, on the emotion analysis dataset their performance difference is not significant. On the IEMOCAP dataset, ELM showed better accuracy for text based emotion detection. Importantly, for the purposes of feature-level fusion, we used a multiple kernel variant of the ELM algorithm namely Multiple Kernel Extreme Learning Machine. The details of the Multiple Kernel ELM can be found here <ref type="bibr" target="#b101">[100]</ref>. As MK-ELM <ref type="bibr" target="#b101">[100]</ref> is not in the scope of this paper, we encourage readers to refer to that paper. As with SPG-GMKL for feature-level fusion (Section 8.4), the same set of kernels was used for MK-ELM.</p><p>However, ELM edges SVM out by a big margin when it comes to computational time, i.e., training time of feature-level fusion (see Table <ref type="table" target="#tab_19">23</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>In this work, a novel multimodal affective data analysis framework is proposed. It includes the extraction of salient features, development of unimodal classifiers, building feature-and decision-level fusion frameworks. The deep CNN-SVM -based textual sentiment analysis component is found to be the key element for outperforming the state-of-the-art model's accuracy. MKL has played a significant role in the fusion experiment. The novel decisionlevel fusion architecture is also an important contribution of this paper. In the case of the decision-level fusion experiment, the coupling of sentic patterns to determine the weight of textual modality has enriched the performance of the multimodal sentiment analysis framework considerably.</p><p>Interestingly, a lower accuracy was obtained for the emotion recognition task, which may indicate that extracting emotions from video may be more difficult than inferring polarity. While text is the most important factor for determining polarity, the visual modality shows the best performance for emotion analysis. The most interesting part of this paper is that a common multimodal affect data analysis framework is well capable of extracting emotion and sentiment from different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head><p>Future work will focus on extracting more relevant features via visual modality. Specifically, deep 3D CNNs will be employed for automatic feature extraction from videos. A feature selection method will be used to select only the best features in order to ensure both scalability and stability of the framework. Consequently, we will strive to improve the decision-level fusion process using a cognitive inspired fusion engine. In order to realize our ambitious goal of developing a novel real-time system for multimodal sentiment analysis, the time complexities of the methods need to be consistently reduced. Hence, another aspect of our future work will be to effectively analyze and appropriately address the system's time complexity requirements in order to create a better, more time efficient and reliable multimodal sentiment analysis engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M</head><p>A N U S C R I P T</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Hourglass of Emotion</figDesc><graphic coords="5,157.96,105.14,255.16,397.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A Sample of Facial Characteristic Points extracted by CLM-Z</figDesc><graphic coords="9,157.96,105.13,255.15,205.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Features</head><label></label><figDesc>Distance between right eye and left eye Distance between the inner and outer corner of the left eye Distance between the upper and lower line of the left eye Distance between the left iris corner and right iris corner of the left eye Distance between the inner and outer corner of the right eye Distance between the upper and lower line of the right eye Distance between the left eyebrow inner and outer corner Distance between the right eyebrow inner and outer corner Distance between top of the mouth and bottom of the mouth Distance between left and right mouth corner Distance between the middle point of left and right mouth side. Distance between Lower nose point and upper mouth point.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Decision-Level Fusion Framework</figDesc><graphic coords="18,72.91,105.13,425.28,277.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>) are the only degrees of freedom. The training problem reduces to the minimization of the convex cost</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance of various learning algorithms for detecting emotions from facial images.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Utterances per emotion class</figDesc><table><row><cell cols="2">Angry Happy</cell><cell cols="2">Sad Neutral Total</cell></row><row><cell>1083</cell><cell cols="2">1630 1083</cell><cell>1683 5479</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Some relevant facial characteristic points (out of the 68 facial characteristic points detected by CLM-Z)</figDesc><table><row><cell cols="2">Features Description</cell></row><row><cell>48</cell><cell>Left eye</cell></row><row><cell>41</cell><cell>Right eye</cell></row><row><cell>43</cell><cell>Left eye inner corner</cell></row><row><cell>46</cell><cell>Left eye outer corner</cell></row><row><cell>47</cell><cell>Left eye lower line</cell></row><row><cell>44</cell><cell>Left eye upper line</cell></row><row><cell>40</cell><cell>Right eye inner corner</cell></row><row><cell>37</cell><cell>Right eye outer corner</cell></row><row><cell>42</cell><cell>Right eye lower line</cell></row><row><cell>38</cell><cell>Right eye upper line</cell></row><row><cell>23</cell><cell>Left eyebrow inner corner</cell></row><row><cell>25</cell><cell>Left eyebrow middle</cell></row><row><cell>27</cell><cell>Left eyebrow outer corner</cell></row><row><cell>22</cell><cell>Right eyebrow inner corner</cell></row><row><cell>20</cell><cell>Right eyebrow middle</cell></row><row><cell>18</cell><cell>Right eyebrow outer corner</cell></row><row><cell>52</cell><cell>Mouth top</cell></row><row><cell>58</cell><cell>Mouth bottom</cell></row><row><cell>55</cell><cell>Mouth Left corner</cell></row><row><cell>49</cell><cell>Mouth Right Corner</cell></row><row><cell>14</cell><cell>Middle of the Left Mouth Side</cell></row><row><cell>4</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Some important facial features used for the experiment</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Features extracted using GAVAM from the facial featuresFeaturesThe time of occurrence of the particular frame in milliseconds. The displacement of the face w.r.t X-axis. It is measured by the displacement of the normal to the frontal view of the face in the X-direction.</figDesc><table /><note><p>The displacement of the face w.r.t Y-axis. The displacement of the face w.r.t Z-axis. The angular displacement of the face w.r.t X-axis. It is measured by the angular displacement of the normal to the frontal view of the face with the X-axis. The angular displacement of the face w.r.t Y-axis. The angular displacement of the face w.r.t Z-axis.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Confusion matrix for the Visual Modality (SVM Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>197</cell><cell>50</cell><cell>76.40%</cell><cell>79.80%</cell></row><row><cell>Positive</cell><cell>61</cell><cell>140</cell><cell>73.70%</cell><cell>69.70%</cell></row><row><cell>by coordinate-wise summation:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Confusion matrix for the Audio Modality (SVM Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>208</cell><cell>38</cell><cell>73.20%</cell><cell>84.60%</cell></row><row><cell>Positive</cell><cell>76</cell><cell>125</cell><cell>76.70%</cell><cell>62.20%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Confusion matrix for the Textual Modality (CNN Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>210</cell><cell>36</cell><cell>78.65%</cell><cell>85.36%</cell></row><row><cell>Positive</cell><cell>57</cell><cell>143</cell><cell>79.88%</cell><cell>71.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Confusion matrix for the Audio-Visual Modality (MKL Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>214</cell><cell>32</cell><cell>82.90%</cell><cell>87.00%</cell></row><row><cell>Positive</cell><cell>44</cell><cell>156</cell><cell>83.00%</cell><cell>78.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Confusion matrix for the Audio-Textual Modality (SPG-GMKL Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>217</cell><cell>29</cell><cell>83.46%</cell><cell>88.21%</cell></row><row><cell>Positive</cell><cell>43</cell><cell>157</cell><cell>84.40%</cell><cell>78.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Confusion matrix for the Visual-Textual Modality (MKL Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>221</cell><cell>25</cell><cell>84.03%</cell><cell>89.83%</cell></row><row><cell>Positive</cell><cell>42</cell><cell>158</cell><cell>86.33%</cell><cell>79.00%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Confusion matrix for the Audio-Visual-Textual Modality (SPG-GMKL Classifier)</figDesc><table><row><cell>Actual</cell><cell cols="2">Predicted classification</cell><cell></cell><cell></cell></row><row><cell>classification</cell><cell>Negative</cell><cell cols="3">Positive Precision Recall</cell></row><row><cell>Negative</cell><cell>227</cell><cell>19</cell><cell>86.64%</cell><cell>92.27%</cell></row><row><cell>Positive</cell><cell>35</cell><cell>165</cell><cell>89.67%</cell><cell>82.50%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Results and Comparison of Unimodal experiment and Multimodal Feature-Level Fusion (Accuracy)</figDesc><table><row><cell></cell><cell cols="2">Perez-Rosa et al. [5] Our Method</cell></row><row><cell>Audio Modality</cell><cell>64.85%</cell><cell>74.49%</cell></row><row><cell>Visual Modality</cell><cell>67.31%</cell><cell>75.22%</cell></row><row><cell>Textual Modality</cell><cell>70.94%</cell><cell>79.14%</cell></row><row><cell>Visual and text-based features</cell><cell>72.39%</cell><cell>84.97%</cell></row><row><cell>Visual and audio-based features</cell><cell>68.86%</cell><cell>82.95%</cell></row><row><cell>Audio and text-based features</cell><cell>72.88%</cell><cell>83.85%</cell></row><row><cell>Fusing all three modalities</cell><cell>74.09%</cell><cell>87.89%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Results and Comparison of Unimodal experiment and Multimodal Feature-Level Fusion (Accuracy):Feature Selection was carried out</figDesc><table><row><cell></cell><cell cols="2">Perez-Rosa et al. [5] Our Method</cell></row><row><cell>Audio Modality</cell><cell>64.85%</cell><cell>74.22%</cell></row><row><cell>Visual Modality</cell><cell>67.31%</cell><cell>76.38%</cell></row><row><cell>Textual Modality</cell><cell>70.94%</cell><cell>79.77%</cell></row><row><cell>Visual and text-based features</cell><cell>72.39%</cell><cell>85.46%</cell></row><row><cell>Visual and audio-based features</cell><cell>68.86%</cell><cell>83.69%</cell></row><row><cell>Audio and text-based features</cell><cell>72.88%</cell><cell>84.12%</cell></row><row><cell>Fusing all three modalities</cell><cell>74.09%</cell><cell>88.60%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Confusion matrix for the Textual Modality (SVM Classifier, Feature selection carried out)</figDesc><table><row><cell>Actual</cell><cell></cell><cell cols="3">Predicted classification</cell><cell></cell></row><row><cell>classification</cell><cell cols="5">Angry Happy Sad Neutral Precision Recall</cell></row><row><cell>Angry</cell><cell>650</cell><cell>82 165</cell><cell>186</cell><cell>55.13%</cell><cell>60.01%</cell></row><row><cell>Happy</cell><cell>193</cell><cell>957 149</cell><cell>331</cell><cell>68.40%</cell><cell>58.71%</cell></row><row><cell>Sad</cell><cell>139</cell><cell>87 619</cell><cell>238</cell><cell>55.51%</cell><cell>57.15%</cell></row><row><cell>Neutral</cell><cell>197</cell><cell>273 182</cell><cell>1031</cell><cell>57.72%</cell><cell>61.25%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>Confusion matrix for the Audio Modality (SVM Classifier, Feature selection carried out)</figDesc><table><row><cell>Actual</cell><cell></cell><cell></cell><cell cols="3">Predicted classification</cell><cell></cell></row><row><cell>classification</cell><cell cols="6">Angry Happy Sad Neutral Precision Recall</cell></row><row><cell>Angry</cell><cell>648</cell><cell>137</cell><cell>89</cell><cell>209</cell><cell>61.53%</cell><cell>59.83%</cell></row><row><cell>Happy</cell><cell>159</cell><cell cols="2">926 123</cell><cell>422</cell><cell>65.21%</cell><cell>56.81%</cell></row><row><cell>Sad</cell><cell>84</cell><cell cols="2">152 658</cell><cell>189</cell><cell>63.08%</cell><cell>60.75%</cell></row><row><cell>Neutral</cell><cell>162</cell><cell cols="2">205 173</cell><cell>1143</cell><cell>58.22%</cell><cell>67.91%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 17 :</head><label>17</label><figDesc>Confusion matrix for the Visual Modality (SVM Classifier, Feature selection carried out)</figDesc><table><row><cell>Actual</cell><cell></cell><cell cols="3">Predicted classification</cell><cell></cell></row><row><cell>classification</cell><cell cols="5">Angry Happy Sad Neutral Precision Recall</cell></row><row><cell>Angry</cell><cell>710</cell><cell>83 116</cell><cell>174</cell><cell>66.17%</cell><cell>65.55%</cell></row><row><cell>Happy</cell><cell>102</cell><cell>1034 148</cell><cell>346</cell><cell>72.76%</cell><cell>63.43%</cell></row><row><cell>Sad</cell><cell>123</cell><cell>83 726</cell><cell>151</cell><cell>63.18%</cell><cell>67.03%</cell></row><row><cell>Neutral</cell><cell>138</cell><cell>221 159</cell><cell>1165</cell><cell>63.45%</cell><cell>69.22%</cell></row><row><cell cols="6">tained 3.75% higher accuracy. However, for Anger emotion class, an approximately 3% lower accuracy was achieved.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>Confusion matrix for the Audio-Visual-Textual Modality (SPG-GMKL Classifier, Feature selection carried out)</figDesc><table><row><cell>Actual</cell><cell></cell><cell></cell><cell cols="3">Predicted classification</cell><cell></cell></row><row><cell>classification</cell><cell cols="6">Angry Happy Sad Neutral Precision Recall</cell></row><row><cell>Angry</cell><cell>821</cell><cell>79</cell><cell>93</cell><cell>90</cell><cell>69.16%</cell><cell>75.80%</cell></row><row><cell>Happy</cell><cell>119</cell><cell>1217</cell><cell>92</cell><cell>202</cell><cell>80.11%</cell><cell>74.67%</cell></row><row><cell>Sad</cell><cell>93</cell><cell cols="2">82 782</cell><cell>126</cell><cell>67.24%</cell><cell>72.20%</cell></row><row><cell>Neutral</cell><cell>154</cell><cell cols="2">141 196</cell><cell>1192</cell><cell>73.99%</cell><cell>70.82%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 22 :</head><label>22</label><figDesc>Accuracy Comparison between SVM and ELM (A=Audio, V=Video, T=Textual,UWA=Un-weighted Average)</figDesc><table><row><cell>Dataset</cell><cell>SVM</cell><cell>A</cell><cell>ELM</cell><cell>SVM</cell><cell>V</cell><cell>ELM</cell><cell>SVM</cell><cell>T</cell><cell>ELM</cell><cell cols="2">A+V+T (UWA) SPG-GMKL MK-ELM</cell></row><row><cell>YouTube</cell><cell cols="10">74.22% 73.81% 76.38% 76.24% 79.77% 78.36% 88.60%</cell><cell>87.33%</cell></row><row><cell cols="11">IEMOCAP 61.32% 60.85% 66.30% 64.74% 59.28% 59.87% 73.37</cell><cell>72.68%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 23 :</head><label>23</label><figDesc>Computational Time comparison between SVM and ELM GMKL outperformed SVM for the feature-level fusion task by 2.7%.</figDesc><table><row><cell></cell><cell cols="2">YouTube Dataset IEMOCAP dataset</cell></row><row><cell>SPG-GMKL</cell><cell>1926 seconds</cell><cell>4389 seconds</cell></row><row><cell>MK-ELM</cell><cell>584 seconds</cell><cell>2791 seconds</cell></row><row><cell>SPG-</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D M A N U S C R I P T</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.7.1.">Decision-Level fusion for Multimodal Emotion Detection</head><p>Like decision-level fusion for multimodal sentiment analysis, similar method was applied for multimodal emotion analysis as well (Table <ref type="table">21</ref>).</p><p>Similarly as we saw in the sentiment analysis experiment, the configuration yielding best accuracy was obtained using M 1 , M 2 and M 3 as Visual, Audio and Textual respectively.</p><p>Table <ref type="table">19</ref> shows the detail result of decision-level fusion experiment on IEMOCAP dataset. It should be noted that sentic patterns cannot be used in this experiment as it is specific to sentiment analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Speeding up the computational time: The role of ELM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1.">Extreme learning machine</head><p>The ELM approach <ref type="bibr" target="#b96">[95]</ref> was introduced to overcome some issues in back-propagation network <ref type="bibr" target="#b97">[96]</ref> training, specifically; potentially slow convergence rates, the critical tuning of optimization parameters, and the presence of local minima that call for multi-start and re-training strategies. The ELM learning problem settings require a training set, X, of N labeled pairs, where (x i , y i ), where x i ∈ R m is the i-th input vector and y i ∈ R is the associate expected 'target' value; using a scalar output implies that the network has one output unit, without loss of generality.</p><p>The input layer has m neurons and connects to the 'hidden' layer (having N h neurons) through a set of weights { ŵ j ∈ R m ; j = 1, ..., N h }. The j-th hidden neuron embeds a bias term, b j ,and a nonlinear 'activation' function, ϕ(•); thus the neuron's response to an input stimulus, x, is:</p><p>Note that (3) can be further generalized as a wider class of functions <ref type="bibr" target="#b98">[97]</ref> but for the subsequent analysis this aspect is not relevant. A vector of weighted links, w j ∈ R N h , connects hidden neurons to the output neuron without any bias <ref type="bibr" target="#b99">[98]</ref>. The overall output function, f (x), of the network is:  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Affective computing and sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="107" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards multimodal sentiment analysis: Harvesting opinions from the web</title>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces, ACM</title>
		<meeting>the 13th international conference on multimodal interfaces, ACM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Sentic blending: Scalable multimodal fusion for continuous interpretation of semantics and sentics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<editor>IEEE SSCI</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="108" to="117" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context, Intelligent Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis of spanish online videos</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rosas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3d constrained local model for rigid and non-rigid facial tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2610" to="2617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multisensor data fusion in distributed sensor networks using mobile agents</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 5th International Conference on Information Fusion</title>
		<meeting>5th International Conference on Information Fusion</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="11" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional MKL based multimodal emotion recognition and sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A review of affective computing: From unimodal analysis to multimodal fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Information Fusion</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guest editorial: Big social data analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Thumbs up?: Sentiment classification using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaithyanathan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<publisher>ACL</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1631</biblScope>
			<biblScope unit="page" from="1642" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hatzivassiloglou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2003, ACL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiment analysis of blogs by combining lexical knowledge with text classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Melville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gryc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1275" to="1284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Thumbs up or thumbs down?: semantic orientation applied to unsupervised classification of reviews</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting on association for computational linguistics</title>
		<meeting>the 40th annual meeting on association for computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Unsupervised sentiment analysis with emotional signals</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="607" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Frame-based detection of opinion holders and topics: A model and a tool</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gangemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Presutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Reforgiato</forename><surname>Recupero</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCI.2013.2291688</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer</publisher>
			<pubPlace>Cham, Switzerland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajpai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>COLING</publisher>
			<biblScope unit="page" from="2666" to="2677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Expanding domain sentiment lexicon through double propagation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1199" to="1204" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully automatic lexicon expansion for domain-oriented sentiment analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kanayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nasukawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2006, ACL</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="355" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="440" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cross-domain sentiment classification via spectral feature alignment</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cross-domain sentiment classification using a sentiment sensitive thesaurus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bollegala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carroll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1719" to="1731" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wordnet affect: an affective extension of wordnet</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Valitutti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LREC</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1083" to="1086" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotions from text: machine learning for text-based emotion prediction</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Alm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sproat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing</title>
		<meeting>the conference on Human Language Technology and Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="579" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The hourglass of emotions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive behavioural systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="144" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Experiments with mood classification in blog posts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mishne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access</title>
		<meeting>ACM SIGIR 2005 Workshop on Stylistic Analysis of Text for Information Access</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Statistical learning theory and ELM for big social data analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Building emotion lexicon from weblog corpora</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>-Y. Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</title>
		<meeting>the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Upar7: A knowledge-based system for headline sentiment tagging</title>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Chaumartin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th International Workshop on Semantic Evaluations</title>
		<meeting>the 4th International Workshop on Semantic Evaluations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="422" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aspect extraction for opinion mining with a deep convolutional neural network</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">News impact on stock price return via sentiment analysis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modelling public sentiment in twitter: Using linguistic patterns to enhance supervised learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chikersal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Siong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Linguistics and Intelligent Text Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="49" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Common sense knowledge based personality recognition from text</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Soft Computing and Its Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="484" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Universal facial expressions of emotion, Culture and Personality</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<imprint>
			<publisher>Contemporary Readings/Chicago</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">More evidence for the universality of a contempt expression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Motivation and Emotion</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="368" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A unified approach to coding and interpreting face images, in: Computer Vision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lanitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings., Fifth International Conference on</title>
		<meeting>Fifth International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995">1995. 1995</date>
			<biblScope unit="page" from="368" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic audio-visual data fusion for automatic emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euromedia</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recognition of facial expression from optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kenji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEICE TRANSACTIONS on Information and Systems</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3474" to="3483" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Expression analysis/synthesis system based on emotion space constructed by multilayered neural network</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ueki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harashima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems and Computers in Japan</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="95" to="107" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<pubPlace>Citeseer</pubPlace>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image re-emotionalizing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Era of Interactive Media</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Facial expression recognition from video sequences: temporal and static modeling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="160" to="187" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal information fusion application to human emotion recognition from face and speech</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mansoorizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Charkari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="277" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human expression recognition from motion using a radial basis function network architecture, Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rosenblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1121" to="1138" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A study of transformation of facial expressions based on expression recognition from temproal image sequences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>Institute of Electronic, Information, and Communications Engineers (IEICE</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep., Technical report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Recognizing human facial expressions from long image sequences using optical flow, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Coding, analysis, interpretation, and recognition of facial expressions, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="757" to="763" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward the simulation of emotion in synthetic speech: A review of the literature on human vocal emotion</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Arnott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1097" to="1108" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Automatic statistical analysis of the signal and prosodic signs of emotion in speech</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1989" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Recognizing emotion in speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Polzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1970" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Emotional speech elicited using computer games</title>
		<author>
			<persName><forename type="first">T</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1985" to="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An objective and subjective study of the role of semantics and prosodic features in building corpora for emotional tts, Audio, Speech, and Language Processing</title>
		<author>
			<persName><forename type="first">E</forename><surname>Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1117" to="1127" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Facial emotion recognition using multi-modal information</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information, Communications and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997. 1997</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="397" to="401" />
		</imprint>
	</monogr>
	<note>Proceedings of 1997 International Conference on</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Automatic Face and Gesture Recognition, 1998. Proceedings</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyasato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="366" to="371" />
		</imprint>
	</monogr>
	<note>Multimodal human emotion/expression recognition</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Recognizing human emotional state from audiovisual signals*, Multimedia</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="936" to="946" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Emotion recognition using bimodal data fusion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Rothkrantz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Computer Systems and Technologies, ACM</title>
		<meeting>the 12th International Conference on Computer Systems and Technologies, ACM</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="122" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kessous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Caridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="33" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Recognizing affect from linguistic information in 3d continuous space, Affective Computing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="192" to="205" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Human emotion recognition from videos using spatio-temporal and audio features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-Bakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mokji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Visual Computer</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1269" to="1275" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Kalman filter based classifier fusion for affective state recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Glodek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dietmayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schwenker ; A C C E P T E D M A N U S C R I</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">P T Multiple Classifier Systems</title>
		<imprint>
			<biblScope unit="page" from="85" to="94" />
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Attention and emotion based adaption of dialog systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hommel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Handmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Systems: Models and Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="215" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Rozgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<title level="m">Speech language &amp; multimedia technol., raytheon bbn technol., cambridge, ma, usa, in: Signal &amp; Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Audio-visual emotion recognition using gaussian mixture models for face and voice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth IEEE International Symposium on</title>
		<editor>
			<persName><surname>Multimedia</surname></persName>
		</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008. 2008</date>
			<biblScope unit="page" from="250" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Multimodal User Interfaces</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="7" to="19" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Emotion recognition of affective speech based on multiple classifiers using acoustic-prosodic information and semantic labels, Affective Computing</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-B</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="21" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Multiple kernel learning for visual object recognition: A review, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bucak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1354" to="1369" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Mkl for robust multi-modality ad classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hinrichs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med Image Comput Comput Assist Interv</title>
		<imprint>
			<biblScope unit="volume">5762</biblScope>
			<biblScope unit="page" from="786" to="794" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Adamkl: A novel biconvex multiple kernel learning approach</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2126" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multiple kernel learning with high order kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition, International Conference on 0</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2138" to="2141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sparse multiple kernel learning for signal processing applications, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Mining opinion attributes from texts using multiple kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wawer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops (ICDMW)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="123" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Group-sensitive multiple kernel learning for object recognition, Image Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2838" to="2852" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Object detection with dog scale-space: A multiple kernel learning approach, Image Processing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nilufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3744" to="3756" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Compositional models for video event detection: A multiple kernel learning latent variable approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fusion methods for multi-modal indexing of web data</title>
		<author>
			<persName><forename type="first">U</forename><surname>Niaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis for Multimedia Interactive Services (WIAMIS), 2013 14th International Workshop on</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Soft margin multiple kernel learning, Neural Networks and Learning Systems</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="749" to="761" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Beta process multiple kernel learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="963" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Multiple kernel learning in the primal for multimodal alzheimers disease classification, Biomedical and Health Informatics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="984" to="990" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Online multiple kernel similarity learning for visual search, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="536" to="549" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Face alignment through subspace constrained mean-shifts, in: Computer Vision</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1034" to="1041" />
			<date type="published" when="2009">2009. 2009</date>
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Opensmile: the munich versatile and fast open-source audio feature extractor</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Multimedia</title>
		<meeting>the international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1459" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW), 2014 IEEE Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno>CoRR abs/1404.2188</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2539" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">AffectiveSpace 2: Enabling affective intuition for concept-level sentiment analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<editor>AAAI, Austin</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="508" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<title level="m" type="main">A graph-based approach to commonsense concept extraction and semantic similarity detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olsher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwok</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="565" to="570" />
			<pubPlace>Rio De Janeiro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">New trends of learning in computational intelligence</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Toh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Widrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="16" to="17" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Spf-gmkl: generalized multiple kernel learning with a million kernels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="750" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Ensemble of svm trees for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">V</forename><surname>Rozgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal &amp; Information Processing Association Annual Summit and Conference</title>
		<imprint>
			<biblScope unit="page">2012</biblScope>
		</imprint>
		<respStmt>
			<orgName>APSIPA ASC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Asia-Pacific</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Sentiment data flow analysis by means of dynamic linguistic patterns</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computational Intelligence Magazine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="26" to="36" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Extreme learning machines: a survey</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Circular backpropagation networks for classification, Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ridella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rovetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zunino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="97" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes, Neural Networks</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">An insight into extreme learning machines: Random neurons, random features and kernels</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-014-9255-2</idno>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Extreme learning machine for regression and multiclass classification, Systems, Man, and Cybernetics, Part B: Cybernetics</title>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="529" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Multiple kernel extreme learning machine</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="page" from="253" to="264" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>Part A</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
