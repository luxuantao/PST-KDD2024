<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers</title>
				<funder>
					<orgName type="full">Cockrell Foundation</orgName>
				</funder>
				<funder ref="#_ArTJvg8">
					<orgName type="full">Intel Corporation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Santhosh</forename><surname>Srinath</surname></persName>
							<email>santhosh@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">?Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">?Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
							<email>hyesoon@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">?Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
							<email>patt@ece.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="department">?Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Feedback Directed Prefetching: Improving the Performance and Bandwidth-Efficiency of Hardware Prefetchers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High performance processors employ hardware data prefetching to reduce the negative performance impact of large main memory latencies. While prefetching improves performance substantially on many programs, it can significantly reduce performance on others. Also, prefetching can significantly increase memory bandwidth requirements. This paper proposes a mechanism that incorporates dynamic feedback into the design of the prefetcher to increase the performance improvement provided by prefetching as well as to reduce the negative performance and bandwidth impact of prefetching. Our mechanism estimates prefetcher accuracy, prefetcher timeliness, and prefetcher-caused cache pollution to adjust the aggressiveness of the data prefetcher dynamically. We introduce a new method to track cache pollution caused by the prefetcher at run-time. We also introduce a mechanism that dynamically decides where in the LRU stack to insert the prefetched blocks in the cache based on the cache pollution caused by the prefetcher.</p><p>Using the proposed dynamic mechanism improves average performance by 6.5% on 17 memory-intensive benchmarks in the SPEC CPU2000 suite compared to the best-performing conventional stream-based data prefetcher configuration, while it consumes 18.7% less memory bandwidth. Compared to a conventional stream-based data prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6% higher performance. Our results show that feedback-directed prefetching eliminates the large negative performance impact incurred on some benchmarks due to prefetching, and it is applicable to streambased prefetchers, global-history-buffer based delta correlation prefetchers, and PC-based stride prefetchers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hardware data prefetching works by predicting the memory access pattern of the program and speculatively issuing prefetch requests to the predicted memory addresses before the program accesses those addresses. Prefetching has the potential to improve performance if the memory access pattern is correctly predicted and the prefetch requests are initiated early enough before the program accesses the predicted memory addresses. Since the memory latencies faced by today's processors are on the order of hundreds of processor clock cycles, accurate and timely prefetching of data from main memory to the processor caches can lead to significant performance gains by hiding the latency of memory accesses. On the other hand, prefetching can negatively impact the performance and energy consumption of a processor due to two major reasons, especially if the predicted memory addresses are not accurate:</p><p>? First, prefetching can increase the contention for the available memory bandwidth. Additional bandwidth con-tention caused by prefetches can lead to increased DRAM bank conflicts, DRAM page conflicts, memory bus contention, and queueing delays. This can significantly reduce performance if it results in delaying demand (i.e. load/store) requests. Moreover, inaccurate prefetches increase the energy consumption of the processor because they result in unnecessary memory accesses (i.e. waste memory/bus bandwidth). Bandwidth contention due to prefetching will become more significant as more and more processing cores are integrated onto the same die in chip multiprocessors, effectively reducing the memory bandwidth available to each core. Therefore, techniques that reduce the memory bandwidth consumption of hardware prefetchers while maintaining their performance improvement will become more desirable and valuable in future processors <ref type="bibr" target="#b21">[22]</ref>. ? Second, prefetching can cause cache pollution if the prefetched data displaces cache blocks that will later be needed by load/store instructions in the program. 1 Cache pollution due to prefetching might not only reduce performance but also waste memory bandwidth by resulting in additional cache misses. Furthermore, prefetcher-caused cache pollution generates new cache misses and those generated cache misses can in turn generate new prefetch requests. Hence, the prefetcher itself is a positive feedback system that can be unstable in terms of both performance and bandwidth consumption. Therefore, we would like to augment the prefetcher with a negative feedback system to make it stable.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> compares the performance of varying the aggressiveness of a stream-based hardware data prefetcher from No prefetching to Very Aggressive prefetching on 17 memoryintensive benchmarks in the SPEC CPU2000 benchmark suite. 2  Aggressive prefetching improves IPC performance by 84% on average 3 and by over 800% for some benchmarks (e.g. mgrid) compared to no prefetching. Furthermore, aggressive prefetch- 1 Note that this is a problem only in designs where prefetch requests bring data into processor caches rather than into separate prefetch buffers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref>. In many current processors (e.g. Intel Pentium 4 <ref type="bibr" target="#b5">[6]</ref> or IBM POWER4 <ref type="bibr" target="#b23">[24]</ref>), prefetch requests bring data into the processor caches. This reduces the complexity of the memory system by eliminating the need to design a separate prefetch buffer. It also makes the large L2 cache space available to prefetch requests, enabling the prefetched blocks and demand-fetched blocks to share the available cache memory dynamically rather than statically partitioning the storage space for demand-fetched and prefetched data. 2 Aggressiveness of the prefetcher is determined by how far the prefetcher stays ahead of the demand access stream of the program as well as how many prefetch requests are generated, as shown in Table <ref type="table" target="#tab_1">1</ref> and Section 2.1.</p><p>3 Similar results were reported by <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b17">[18]</ref>. All average IPC results in this paper are computed as geometric mean of the IPC's of the benchmarks.</p><p>ing on average performs better than conservative and middleof-the-road prefetching. Unfortunately, aggressive prefetching significantly reduces performance on some benchmarks. For example, an aggressive prefetcher reduces the IPC performance of ammp by 48% and applu by 29% compared to no prefetching. Hence, blindly increasing the aggressiveness of the hardware prefetcher can drastically reduce performance on several applications even though it improves the average performance of a processor. Since aggressive prefetching significantly degrades performance on some benchmarks, many modern processors employ relatively conservative prefetching mechanisms where the prefetcher does not stay far ahead of the demand access stream of the program <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. The goal of this paper is to reduce the negative performance and bandwidth impact of aggressive prefetching while preserving the large performance benefits provided by it. To achieve this goal, we propose simple and implementable mechanisms that dynamically adjust the aggressiveness of the hardware prefetcher as well as the location in the processor cache where prefetched data is inserted.</p><p>The proposed mechanisms estimate the effectiveness of the prefetcher by monitoring the accuracy and timeliness of the prefetch requests as well as the cache pollution caused by the prefetch requests. We describe simple hardware implementations to estimate accuracy, timeliness, and cache pollution. Based on the run-time estimation of these three metrics, the aggressiveness of the hardware prefetcher is decreased or increased dynamically. Also, based on the run-time estimation of the cache pollution caused by the prefetcher, the proposed mechanism dynamically decides where to insert the prefetched blocks in the processor cache's LRU stack.</p><p>Our results show that using the proposed dynamic feedback mechanisms improve the average performance of 17 memory-intensive benchmarks in the SPEC CPU2000 suite by 6.5% compared to the best-performing conventional streambased prefetcher configuration. With the proposed mechanism, the negative performance impact incurred on some benchmarks due to stream-based prefetching is completely eliminated. Furthermore, the proposed mechanism consumes 18.7% less memory bandwidth than the best-performing stream-based prefetcher configuration. Compared to a conventional streambased prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 13.6% higher performance. We also show that the dynamic feedback mechanism works similarly well when implemented to dynamically adjust the aggressiveness of a global-historybuffer (GHB) based delta correlation prefetcher <ref type="bibr" target="#b9">[10]</ref> or a PC-based stride prefetcher <ref type="bibr" target="#b0">[1]</ref>. Compared to a conventional GHB-based delta correlation prefetcher configuration that consumes similar amount of memory bandwidth, feedback directed prefetching provides 9.9% higher performance. The proposed mechanism provides these benefits with a modest hardware storage cost of 2.54 KB and without significantly increasing hardware complexity. On the remaining 9 SPEC CPU2000 benchmarks, the proposed dynamic feedback mechanism performs as well as the best-performing conventional stream prefetcher configuration for those 9 benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Motivation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Stream Prefetcher Design</head><p>The stream prefetcher we model is based on the stream prefetcher in the IBM POWER4 processor <ref type="bibr" target="#b23">[24]</ref> and more details on the implementation of stream-based prefetching can be found in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>. The modeled prefetcher brings cache blocks from the main memory to the last-level cache, which is the second-level (L2) cache in our baseline processor.</p><p>The stream prefetcher is able to keep track of multiple different access streams. For each tracked access stream, a stream tracking entry is created in the stream prefetcher. Each tracking entry can be in one of four different states:</p><p>1. Invalid: The tracking entry is not allocated a stream to keep track of. Initially, all tracking entries are in this state. 2. Allocated: A demand (i.e. load/store) L2 miss allocates a tracking entry if the demand miss does not find any existing tracking entry for its cache-block address.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Training:</head><p>The prefetcher trains the direction (ascending or descending) of the stream based on the next two L2 misses that occur +/-16 cache blocks from the first miss. <ref type="foot" target="#foot_0">4</ref> If the next two accesses in the stream are to ascending (descending) addresses, the direction of the tracking entry is set to 1 (0) and the entry transitions to Monitor and Request state. 4. Monitor and Request: The tracking entry monitors the accesses to a memory region from a start pointer (address A) to an end pointer (address P). The maximum distance between the start pointer and the end pointer is determined by Prefetch Distance, which indicates how far ahead of the demand access stream the prefetcher can send requests. If there is a demand L2 cache access to a cache block in the monitored memory region, the prefetcher requests cache blocks [P+1, ..., P+N] as prefetch requests (assuming the direction of the tracking entry is set to 1). N is called the Prefetch Degree. After sending the prefetch requests, the tracking entry starts monitoring the memory region between addresses A+N to P+N (i.e. effectively it moves the tracked memory region by N cache blocks). <ref type="foot" target="#foot_1">5</ref>Prefetch Distance and Prefetch Degree determine the aggressiveness of the prefetcher. In a traditional prefetcher configuration, the values of Prefetch Distance and Prefetch Degree are fixed at the design time of the processor. In the feedback directed mechanism we propose, the processor dynamically changes Prefetch Distance and Prefetch Degree to adjust the aggressiveness of the prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Metrics of Prefetcher Effectiveness</head><p>We use three metrics (Prefetch Accuracy, Prefetch Lateness, and Prefetcher-Generated Cache Pollution) as feedback inputs to feedback directed prefetchers. In this section, we define the metrics and describe the relationship between the metrics and the performance provided by a conventional prefetcher. We evaluate four configurations: No prefetching, Very Conservative prefetching (distance=4, degree=1), Middle-of-the-Road prefetching (distance=16, degree=2), and Very Aggressive prefetching (distance=64, degree=4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Prefetch Accuracy:</head><p>Prefetch accuracy is a measure of how accurately the prefetcher can predict the memory addresses that will be accessed by the program. It is defined as where Number of Useful Prefetches is the number of prefetched cache blocks that are used by demand requests while they are resident in the L2 cache.</p><p>Figure <ref type="figure">2</ref> shows the IPC of the four configurations along with prefetch accuracy measured over the entire run of each benchmark. The results show that in benchmarks where prefetch accuracy is less than 40% (applu, galgel, and ammp), employing the stream prefetcher always degrades performance compared to no prefetching. In all benchmarks where prefetch accuracy exceeds 40% (except mcf), using the stream prefetcher significantly improves performance over no prefetching. For benchmarks with high prefetch accuracy, performance increases as the aggressiveness of the prefetcher is increased. Hence, the performance improvement provided by increasing the aggressiveness of the prefetcher is correlated with prefetch accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Prefetch Lateness:</head><p>Prefetch lateness is a measure of how timely the prefetch requests generated by the prefetcher are with respect to the demand accesses that need the prefetched data. A prefetch is defined to be late if the prefetched data has not yet returned from main memory by the time a load or store instruction requests the prefetched data. Even though the prefetch requests are accurate, a prefetcher might not be able to improve performance if the prefetch requests are very late. We define prefetch lateness as: Figure <ref type="figure">3</ref> shows the IPC of the four configurations along with prefetch lateness measured over the entire run of each program. These results explain why prefetching does not provide significant performance benefit on mcf, even though the prefetch accuracy is close to 100%. More than 90% of the useful prefetch requests are late in mcf. In general, prefetch lateness decreases as the prefetcher becomes more aggressive. For example, in vortex, prefetch lateness decreases from 70% to 22% when a very aggresive prefetcher is used instead of a very conservative one. Aggressive prefetching reduces the lateness of prefetches because an aggressive prefetcher generates prefetch requests earlier than a conservative one would. A demand miss is defined to be caused by the prefetcher if it would not have occurred had the prefetcher not been present. If the prefetcher-generated cache pollution is high, the performance of the processor can degrade because useful data in the cache could be evicted by prefetched data. Furthermore, high cache pollution can also result in higher memory bandwidth consumption by requiring the re-fetch of the displaced data from main memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Prefetcher-Generated</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feedback Directed Prefetching (FDP)</head><p>FDP dynamically adapts the aggressiveness of the prefetcher based on the accuracy, lateness, and pollution metrics defined in the previous section. This section describes hardware mechanisms that track these metrics and the FDP mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Collecting Feedback Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Prefetch Accuracy:</head><p>To track the usefulness of prefetch requests, we add a bit (pref-bit), to each tag-store entry in the L2 cache. <ref type="foot" target="#foot_2">6</ref> When a prefetched block is inserted into the cache, the pref-bit associated with that block is set. Prefetcher accuracy is tracked using two hardware counters. The first counter, preftotal, tracks the number of prefetches sent to memory. The second counter, used-total, tracks the number of useful prefetches. When a prefetch request is sent to memory, pref-total is incremented. When an L2 cache block that has the pref-bit set is accessed by a demand request, the pref-bit is reset and used-total is incremented. The accuracy of the prefetcher is computed by taking the ratio of used-total to pref-total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Prefetch Lateness:</head><p>Miss Status Holding Register (MSHR) <ref type="bibr" target="#b11">[12]</ref> is a hardware structure that keeps track of all in-flight memory requests. Before allocating an MSHR entry for a request, the MSHR checks if the requested cache block is being serviced by an earlier memory request. Each entry in the L2 cache MSHR has a bit, called the pref-bit, which indicates that the memory request was generated by the prefetcher. A prefetch request is late if a demand request for the prefetched address is generated while the prefetch request is in the MSHR waiting for main memory. We use a hardware counter, latetotal, to keep track of such late prefetches. If a demand request hits an MSHR entry that has its pref-bit set, the late-total counter is incremented, and the pref-bit associated with that MSHR entry is reset. The lateness metric is computed by taking the ratio of late-total to used-total. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Prefetcher-Generated Cache Pollution:</head><p>To track the number of demand misses caused by the prefetcher, the processor needs to store information about all demand-fetched L2 cache blocks dislodged by the prefetcher. However, such a mechanism is impractical as it incurs a heavy overhead in terms of both hardware and complexity. We use the Bloom filter concept <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b19">20]</ref> to provide a simple cost-effective hardware mechanism that can approximate the number of demand misses caused by the prefetcher.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> shows the filter that is used to approximate the number of L2 demand misses caused by the prefetcher. The filter consists of a bit-vector, which is indexed with the output of the exclusive-or operation of the lower and higher order bits of the cache block address. When a block that was brought into the cache due to a demand miss is evicted from the cache due to a prefetch request, the filter is accessed with the address of the evicted cache block and the corresponding bit in the filter is set (indicating that the evicted cache block was evicted due to a prefetch request). When a prefetch request is serviced from memory, the pollution filter is accessed with the cache-block address of the prefetch request and the corresponding bit in the filter is reset, indicating that the block was inserted into the cache. When a demand access misses in the cache, the filter is accessed using the cache-block address of the demand request. If the corresponding bit in the filter is set, it is an indication that the demand miss was caused by the prefetcher. In such cases, the hardware counter, pollution-total, that keeps track of the total number of demand misses caused by the prefetcher is incremented. Another counter, demand-total, keeps track of the total number of demand misses generated by the processor and is incremented for each demand miss. Cache pollution caused by the prefetcher can be computed by taking the ratio of pollution-total to demand-total. We use a 4096-entry bit vector in our experiments.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sampling-based Feedback Collection</head><p>To adapt to the time-varying memory phase behavior of a program, we use interval-based sampling for all counters described in Section 3.1. Program execution time is divided into intervals and the value of each counter is computed as:</p><formula xml:id="formula_0">CounterV alue = 1 2 CounterV alueAtT heBeginningOf T heInterval + 1 2 CounterV alueDuringInterval<label>(1)</label></formula><p>The CounterValueDuringInterval is reset at the end of each sampling interval. The above equation used to update the counters (Equation <ref type="formula" target="#formula_0">1</ref>) gives more weight to the behavior of the program in the most recent interval while taking into account the behavior in all previous intervals. Our mechanism defines the length of an interval based on the number of useful cache blocks evicted from the L2 cache. <ref type="foot" target="#foot_3">7</ref> A hardware counter, eviction-count, keeps track of the number of blocks evicted from the L2 cache. When the value of the counter exceeds a statically-set threshold T interval , the interval ends. At the end of an interval, all counters described in Section 3.1 are updated according to Equation 1. The updated counter values are then used to compute the three metrics: accuracy, lateness, and pollution. These metrics are used to adjust the prefetcher behavior for the next interval. The eviction-count register is reset and a new interval begins. In our experiments, we use a value of 8192 (half the number of blocks in the L2 cache) for T interval .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dynamically Adjusting Prefetcher Behavior</head><p>At the end of each sampling interval, the computed values of the accuracy, lateness, and pollution metrics are used to dynamically adjust prefetcher behavior. Prefetcher behavior is adjusted in two ways: (1) by adjusting the aggressiveness of the prefetching mechanism, (2) by adjusting the location in the L2 cache's LRU stack where prefetched blocks are inserted.<ref type="foot" target="#foot_4">8</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Adjusting Prefetcher Aggressiveness:</head><p>The aggressiveness of the prefetcher directly determines the potential for benefit as well as harm that is caused by the prefetcher. By dynamically adapting this parameter based on the collected feedback information, the processor can not only achieve the performance benefits of aggressive prefetching during program phases where aggressive prefetching performs well but also eliminate the negative performance and bandwidth impact of aggressive prefetching during phases where aggressive prefetching performs poorly.</p><p>As shown in At the end of each sampling interval, the value of the Dynamic Configuration Counter is updated based on the computed values of the accuracy, lateness, and pollution metrics. The computed accuracy is compared to two thresholds (A high and A low ) and is classified as high, medium or low. Similarly, the computed lateness is compared to a single threshold (T lateness ) and is classified as either late or not-late. Finally, the computed pollution is compared to a single threshold (T pollution ) and is classified as high (polluting) or low (notpolluting). We use static thresholds in our mechanisms. The effectiveness of our mechanism can be improved by dynamically tuning the values of these thresholds and/or using more thresholds, but such optimization is out of the scope of this paper. In Section 5, we show that even with untuned threshold values, FDP can significantly improve performance and reduce memory bandwidth consumption on different data prefetchers.</p><p>Table <ref type="table" target="#tab_3">2</ref> shows in detail how the estimated values of the three metrics are used to adjust the dynamic configuration of the prefetcher. We determined the counter update choice for each case empirically. If the prefetches are causing pollution (all even-numbered cases), the prefetcher is adjusted to be less aggressive to reduce cache pollution and to save memory bandwidth (except in Case 2 when the accuracy is high and prefetches are late -we do increase aggressiveness in this case to gain more benefit from highly-accurate prefetches). If the prefetches are late but not polluting (Cases 1, 5, 9), the aggressiveness is increased to increase timeliness unless the prefetch accuracy is low (Case 9 -we reduce aggressiveness in this case because a large fraction of inaccurate prefetches will waste memory bandwidth). If the prefetches are neither late nor polluting (Cases 3, 7, 11), the aggressiveness is left unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Adjusting Cache Insertion Policy of Prefetched</head><p>Blocks: FDP also adjusts the location in which a prefetched block is inserted in the LRU-stack of the corresponding cache set based on the observed behavior of the prefetcher. In many cache implementations, prefetched cache blocks are simply inserted into the Most-Recently-Used (MRU) position in the LRU-stack, since such an insertion policy does not require any changes to the cache implementation. Inserting the prefetched blocks into the MRU position can allow the prefetcher to be more aggressive and request data long before its use because this insertion policy allows the useful prefetched blocks to stay longer in the cache. However, if the prefetched cache blocks create cache pollution, having a different cache insertion policy for prefetched cache blocks can help reduce the cache pollution caused by the prefetcher. A prefetched block that is not useful creates more pollution in the cache if it is inserted into the MRU position rather than a less recently used position because it stays in the cache for a longer time period, occupying cache space that could otherwise be allocated to a useful demandfetched cache block. Therefore, if the prefetch requests are causing cache pollution, it would be desirable to reduce this pollution by changing the location in the LRU stack in which prefetched blocks are inserted.</p><p>We propose a simple heuristic that decides where in the LRU stack of the L2 cache set a prefetched cache block is inserted based on the estimated prefetcher-generated cache pollution. At the end of a sampling interval, the estimated cache pollution metric is compared to two thresholds (P low and P high ) to determine whether the pollution caused by the prefetcher was low, medium, or high. If the pollution caused by the prefetcher was low, the prefetched cache blocks are inserted into the middle (MID) position in the LRU stack during the next sampling interval (for an n-way set-associative cache, we define the MID position in the LRU stack as the floor(n/2)th least-recently-used position). <ref type="foot" target="#foot_5">9</ref> On the other hand, if the pollution caused by the prefetcher was medium, prefetched cache blocks are inserted into the LRU-4 position in the LRU stack (for an n-way set-associative cache, we define the LRU-4 position in the LRU stack as the floor(n/4)th least-recently-used position). Finally, if the pollution caused by the prefetcher was high, prefetched cache blocks are inserted into the LRU position during the next sampling interval. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation Methodology</head><p>We evaluate the performance impact of FDP on an in-house execution-driven Alpha ISA simulator that models an aggressive superscalar, out-of-order execution processor. The parameters of the processor we model are shown in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Memory Model</head><p>We evaluate our mechanisms using a detailed memory model which mimics the behavior and the bandwidth/port limitations of all the hardware structures in the memory system faithfully. All the mentioned effects are modeled correctly and bandwidth limitations are enforced in our model as described in <ref type="bibr" target="#b15">[16]</ref>. The memory bus has a bandwidth of 4.5 GB/s.</p><p>The baseline hardware data prefetcher we model is a stream prefetcher that can track 64 different streams. Prefetch requests generated by the stream prefetcher are inserted into the Prefetch Request Queue which has 128 entries in our model. Requests are drained from this queue and inserted into the L2 Request Queue and are given the lowest priority so that they do not delay demand load/store requests. Requests that miss in the L2 cache access DRAM memory by going through the Bus Request Queue. L2 Request Queue, Bus Request Queue, and L2 Fill Queue have 128 entries each. Only when a prefetch request goes out on the bus does it count towards the number of prefetches sent to memory. A prefetched cache block is placed into the MRU position in the L2 cache in the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Benchmarks</head><p>We focus our evaluation on those benchmarks from the SPEC CPU2000 suite where the most aggressive prefetcher configuration sends out to memory at least 200K prefetch requests over the 250 million instruction run. On the remaining nine programs of the SPEC CPU2000 suite, the potential for improving either performance or bandwidth-efficiency of the prefetcher is limited because the prefetcher is not active (even if it is configured very aggressively). 10 For reference, the number of prefetches generated for each benchmark in the SPEC CPU2000 suite is shown in Table <ref type="table" target="#tab_6">4</ref>. The benchmarks were compiled using the Compaq C/Fortran compilers with the -fast optimizations and profile-driven feedback enabled. All benchmarks are fast forwarded to skip the initialization portion and then simulated for 250 million instructions. 10 We also evaluated the remaining benchmarks that have less potential. Results for these benchmarks are shown in Section 5.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Thresholds Used in FDP Implementation</head><p>The thresholds used in the implementation of our mechanism are provided below. We determined the parameters of our mechanism empirically using a limited number of simulation runs. However, we did not tune the parameters to our application set since this requires an exponential number of simulations in terms of the different parameter combinations. We estimate that optimizing these thresholds can further improve the performance and bandwidth-efficiency of our mechanism. In systems where bandwidth contention is estimated to be higher (e.g. systems where many threads share the memory bandwidth), A high and A low thresholds can be increased to restrict the prefetcher from being too aggressive. In systems where the lateness of prefetches is estimated to be higher due to higher contention in the memory system, reducing the T lateness threshold can increase performance by increasing the timeliness of the prefetcher. Reducing T pollution , P high or P low thresholds results in reducing the prefetcher-generated cache pollution. In systems with higher contention for the L2 cache space (e.g. systems with a smaller L2 cache or with many threads sharing the same L2 cache), reducing the values of T pollution , P high or P low may be desirable to reduce the cache pollution due to prefetching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results and Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Adjusting Prefetcher Aggressiveness</head><p>We first evaluate the performance of FDP to adjust the aggressiveness of the stream prefetcher (as described in Section 3.3.1) in comparison to four traditional configurations that do not incorporate dynamic feedback: No prefetching, Very Conservative prefetching, Middle-of-the-Road prefetching, and Very Aggressive prefetching. Figure <ref type="figure">5</ref> shows the IPC performance of each configuration. Adjusting the prefetcher aggressiveness dynamically (i.e. Dynamic Aggressiveness) provides the best average performance across all configurations. Dynamically adapting the aggressiveness of the prefetcher using the proposed feedback mechanism provides 4.7% higher average IPC over the Very Aggressive configuration and 11.9% higher IPC over the Middle-of-the-Road configuration.</p><p>On almost all benchmarks, Dynamic Aggressiveness provides performance that is very close to the performance achieved by the best-performing traditional prefetcher configuration for each benchmark. Hence, the dynamic mechanism is  able to detect and employ the best-performing aggressiveness level for the stream prefetcher on a per-benchmark basis.</p><p>Figure <ref type="figure">5</ref> shows that Dynamic Aggressiveness almost completely eliminates the large performance degradation incurred on some benchmarks due to Very Aggressive prefetching. While the most aggressive traditional prefetcher configuration provides the best average performance, it results in a 28.9% performance loss on applu and a 48.2% performance loss on ammp compared to no prefetching. In contrast, Dynamic Aggressiveness results in a 1.8% performance improvement on applu and only a 5.9% performance loss on ammp compared to no prefetching, similar to the best-performing traditional prefetcher configuration for the two benchmarks. For benchmarks where aggressive prefetching hurts performance (e.g. applu, galgel, ammp), the feedback mechanism chooses and employs the least aggressive dynamic configuration (counter value of 1) for most of the sampling intervals. For example, the prefetcher is configured to be Very Conservative in more than 98% of the intervals for both applu and ammp. On the other hand, for benchmarks where aggressive prefetching significantly increases performance (e.g. wupwise, mgrid, equake), FDP employs the most aggressive configuration for most of the sampling intervals. For example, the prefetcher is configured to be Very Aggressive in more than 98% of the intervals for wupwise, mgrid, and equake. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Adjusting Cache Insertion Policy of Prefetches</head><p>Figure <ref type="figure" target="#fig_8">7</ref> shows the performance of dynamically adjusting the cache insertion policy (i.e. Dynamic Insertion) using FDP as described in Section 3.3.2. The performance of Dynamic Insertion is compared to four different static insertion policies that always insert a prefetched block into the (1) MRU position, (2) MID (floor(n/2)th) position where n is the set-associativity, (3) LRU-4 (f loor(n/4)th least-recently-used) position, and (4) LRU position in the LRU stack. The dynamic cache insertion policy is evaluated using the Very Aggressive prefetcher configuration. The data in Figure <ref type="figure" target="#fig_8">7</ref> shows that statically inserting prefetches in the LRU position can result in significant average performance loss compared to statically inserting prefetches in the MRU position. This is because inserting prefetched blocks in the LRU position causes an aggressive prefetcher to evict prefetched blocks before they get used by demand loads/stores. However, inserting in the LRU position eliminates the performance loss due to aggressive prefetching in benchmarks where aggressive prefetching hurts performance (e.g. applu and ammp). Among the static cache insertion policies, inserting the prefetched blocks into the LRU-4 position provides the best average performance, improving performance by 3.2% over inserting prefetched blocks in the MRU position.</p><p>Adjusting the cache insertion policy dynamically provides higher performance than any of the static insertion policies. Dynamic Insertion achieves 5.1% better performance than inserting prefetched blocks into the MRU position and 1.9% better performance than inserting them into the LRU-4 position. Furthermore, Dynamic Insertion almost always provides the performance of the best static insertion policy for each benchmark. Hence, dynamically adapting the prefetch insertion policy using run-time estimates of prefetcher-generated cache pollution is able to detect and employ the best-performing cache insertion policy for the stream prefetcher on a per-benchmark basis.</p><p>Figure <ref type="figure" target="#fig_9">8</ref> shows the distribution of the insertion position of the prefetched blocks when Dynamic Insertion is used. For benchmarks where a static policy of inserting prefetched blocks into the LRU position provides the best performance across all static configurations (applu, galgel, ammp), Dynamic Insertion places most (more than 50%) of the prefetched blocks into the LRU position. Therefore, Dynamic Insertion improves the performance of these benchmarks by dynamically employing the best-performing insertion policy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Putting It All Together: Dynamically Adjusting Both Aggressiveness and Insertion Policy</head><p>This section examines the use of FDP for dynamically adjusting both the prefetcher aggressiveness (Dynamic Aggressiveness) and the cache insertion policy of prefetched blocks (Dynamic Insertion). Using Dynamic Aggressiveness and Dynamic Insertion together provides the best performance across all configurations, improving the IPC by 6.5% over the best-performing traditional prefetcher configuration (i.e. Very Aggressive configuration). This performance improvement is greater than the performance With the use of FDP to dynamically adjust both aspects of prefetcher behavior, the performance loss incurred on some benchmarks due to aggressive prefetching is completely eliminated. No benchmark loses performance compared to no prefetching if both Dynamic Aggressiveness and Dynamic Insertion are used. In fact, FDP improves the performance of applu by 13.4% and ammp by 11.4% over no prefetching -two benchmarks that otherwise incur very significant performance losses with an aggressive traditional prefetcher configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Impact of FDP on Bandwidth Consumption</head><p>Aggressive prefetching can adversely affect the bandwidth consumption in the memory system when prefetches are not used or when they cause cache pollution. Figure <ref type="figure" target="#fig_12">10</ref> shows the bandwidth impact of prefetching in terms of Memory Bus Accesses per thousand retired Instructions (BPKI). <ref type="foot" target="#foot_6">11</ref> Increasing the aggressiveness of the traditional stream prefetcher significantly increases the memory bandwidth consumption, especially for benchmarks where the prefetcher degrades performance. FDP reduces the aggressiveness of the prefetcher in these benchmarks. For example, in applu and ammp our feedback mechanism usually chooses the least aggressive prefetcher configuration and the least aggressive cache insertion policy as shown in Figures <ref type="figure" target="#fig_7">6</ref> and<ref type="figure" target="#fig_9">8</ref>. This results in the large reduction in BPKI shown in Figure <ref type="figure" target="#fig_12">10</ref>. FDP (Dynamic Aggressiveness and Dynamic Insertion) consumes 18.7% less memory bandwidth than the Very Aggressive traditional prefetcher configuration, while it provides 6.5% higher performance.</p><p>Table <ref type="table" target="#tab_7">5</ref> shows the average performance and average bandwidth consumption of different traditional prefetcher configurations and FDP. Compared to the traditional prefetcher configuration that consumes similar amount of memory bandwidth as FDP,<ref type="foot" target="#foot_7">12</ref> FDP provides 13.6% higher performance. Hence, incorporating our dynamic feedback mechanism into the stream prefetcher significantly increases the bandwidth-efficiency of the baseline stream prefetcher.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Hardware Cost and Complexity of FDP</head><p>Table <ref type="table" target="#tab_8">6</ref> shows the hardware cost of the proposed mechanism in terms of the required state. FDP does not add significant combinational logic complexity to the processor. Combinational logic is required for the update of counters, update of the pref-bits in the L2 cache, update of the entries in the pollution filter, calculation of feedback metrics at the end of each sampling interval, determination of when a sampling interval ends, and insertion of prefetched blocks into appropriate locations in the LRU stack of an L2 cache set. None of the required logic is on the critical path of the processor. The storage overhead of our mechanism is less than 0.25% of the data-store size of the baseline 1MB L2 cache.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Using only Prefetch Accuracy for Feedback</head><p>We use a comprehensive set of metrics -prefetch accuracy, timeliness, and pollution-in order to provide feedback to adjust the prefetcher aggressiveness. In order to assess the benefit of using timeliness as well as cache pollution, we evaluated a mechanism where we adapted the prefetcher aggressiveness based only on accuracy. In such a scheme, we increment the Dynamic Configuration Counter if the accuracy is high and decrement it if the accuracy is low. We found that, compared to this scheme that only uses accuracy to throttle the aggressiveness of a stream prefetcher, our comprehensive mechanism that also takes into account timeliness and cache pollution provides 3.4% higher performance and consumes 2.5% less bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">FDP vs. Using a Prefetch Cache</head><p>Cache pollution caused by prefetches can be eliminated by bringing prefetched data into separate prefetch buffers <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b10">11]</ref> rather than inserting prefetched data into the L2 cache. Figures 11 and 12 respectively show the performance and bandwidth consumption of the Very Aggressive prefetcher with different prefetch cache sizes -ranging from a 2KB fully-associate prefetch cache to a 1MB 16-way prefetch cache. <ref type="foot" target="#foot_8">13</ref> The per-formance of the Very Aggressive prefetcher and FDP when prefetched data is inserted into the L2 cache is also shown. The results show that using small (2KB and 8KB) prefetch caches do not provide as high performance as inserting the prefetched data into the L2 cache. With an aggressive prefetcher and a small prefetch cache, the prefetched blocks are displaced by later prefetches before being used by the program -which results in performance degradation with a small prefetch cache. However, larger prefetch caches (32KB and larger) improve performance compared to inserting prefetched data into the L2 cache because a larger prefetch cache reduces the pollution caused by prefetched data in the L2 cache while providing enough space for prefetched blocks.</p><p>Using FDP (both Dynamic Aggressiveness and Dynamic Insertion) that prefetches into the L2 cache provides 5.3% higher performance than that provided by augmenting the Very Aggressive traditional prefetcher configuration with a 32KB prefetch cache. The performance of FDP is also within 2% of the performance of the Very Aggressive configuration with a 64KB prefetch cache. Furthermore, the memory bandwidth consumption of FDP is 16% and 9% less than the Very Aggressive prefetcher configurations with respectively a 32KB and 64KB prefetch cache. Hence, FDP achieves the performance provided by a relatively large prefetch cache bandwidthefficiently and without requiring as large hardware cost and complexity as that introduced by the addition of a prefetch cache that is larger than 32KB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8.">Effect on a Global History Buffer Prefetcher</head><p>We have also implemented FDP on the C/DC (C-Zone Delta Correlation) variant of the Global History Buffer (GHB) prefetcher <ref type="bibr" target="#b9">[10]</ref>. In order to vary the aggressiveness of this prefetcher dynamically, we vary the Prefetch Degree. <ref type="foot" target="#foot_9">14</ref> Below, we show the aggressiveness configurations used for the GHB prefetcher. FDP adjusts the configuration of the GHB prefetcher as described in Section 3.3. Figure <ref type="figure" target="#fig_14">13</ref> shows the performance and bandwidth consumption of different GHB prefetcher configurations and the feedback directed GHB prefetcher using both Dynamic Aggressiveness and Dynamic Insertion. The feedback directed GHB prefetcher performs similarly to the best-performing traditional configuration (Very Aggressive configuration), while it consumes 20.8% less memory bandwidth. Compared to the traditional GHB prefetcher configuration that consumes similar amount of memory bandwidth as FDP (i.e. Middle-of-the-Road configuration), FDP provides 9.9% higher performance. Hence, FDP significantly increases the bandwidth-efficiency of GHB-based delta correlation prefetching. Note that it is possible to improve the performance and bandwidth benefits of the proposed mechanism by tuning the thresholds used in feedback mechanisms to the behavior of the GHB-based prefetcher, but we did not pursue this option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dyn</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9.">Effect of FDP on a PC-Based Stride Prefetcher</head><p>We also evaluated FDP on a PC-based stride prefetcher <ref type="bibr" target="#b0">[1]</ref> and found that the results are similar to those achieved on both stream and GHB-based prefetchers. On average, using the feedback directed approach results in a 4% performance gain and a 24% reduction in memory bandwidth compared to the best-performing conventional configuration for a PC-based stride prefetcher. Due to space constraints, we do not present detailed graphs for these results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10.">Sensitivity to L2 Size and Memory Latency</head><p>We evaluate the sensitivity of FDP to different cache sizes and memory latencies. In these experiments, we varied the L2 cache size keeping the memory latency at 500 cycles (baseline) and varied the memory latency keeping the cache size at 1MB (baseline). Table <ref type="table" target="#tab_11">7</ref> shows the change in average IPC and BPKI provided by FDP over the best performing conventional prefetcher configuration. FDP provides better performance and consumes significantly less bandwidth than the best-performing conventional prefetcher configuration for all evaluated cache sizes and memory latencies. As memory latency increases, the IPC improvement of FDP also increases be-cause the effectiveness of the prefetcher becomes more important when memory becomes a larger performance bottleneck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.11.">Effect on Other SPEC CPU2000 Benchmarks</head><p>Figure <ref type="figure" target="#fig_15">14</ref> shows the IPC and BPKI impact of FDP on the remaining 9 SPEC CPU2000 benchmarks that have less potential. We find that our feedback directed scheme provides 0.4% performance improvement over the best performing conventional prefetcher configuration (i.e. Middle-of-the-Road configuration) while reducing the bandwidth consumption by 0.2%. None of the benchmarks lose performance with FDP. Note that the best-performing conventional configuration for these 9 benchmarks is not the same as the best-performing conventional configuration for the 17 memory-intensive benchmarks (i.e. Very-Aggressive configuration). Also note that the remaining 9 benchmarks are not bandwidth-intensive except for fma3d and gcc. In gcc, the performance improvement of FDP is 3.0% over the Middle-of-the-Road configuration. The prefetcher pollutes the L2 cache and evicts many useful instruction blocks in gcc, resulting in very long-latency instruction cache misses that leave the processor idle. Using FDP reduces this negative effect by detecting the pollution caused by prefetch references and dynamically reducing the aggressiveness of the prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Even though mechanisms for prefetching have been studied for a long time, dynamic mechanisms to adapt the aggressiveness of the prefetcher have not been studied as extensively as algorithms that decide what to prefetch. We briefly describe previous work in dynamic adaptation of prefetching policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Dynamic Adaptation of Data Prefetching Policies</head><p>The work most related to ours in adapting the prefetcher's aggressiveness is Dahlgren et al.'s paper that proposed adaptive sequential (next-line) prefetching <ref type="bibr" target="#b3">[4]</ref> for multiprocessors. This mechanism implemented two counters to count the number of sent prefetches (counter-sent) and the number of useful prefetches (counter-used). When counter-sent saturates, counter-used is compared to a static threshold to decide whether to increase or decrease the aggressiveness (i.e. Prefetch Distance) of the prefetcher. While Dahlgren et al.'s mechanism to calculate prefetcher accuracy is conceptually similar to ours, their approach considered only prefetch accuracy to dynamically adapt prefetch distance. Also, their mechanism is designed for a simple sequential prefetching mechanism which prefetches up to 8 cache blocks following each cache miss. In this paper, we provide a generalized feedbackdirected approach for dynamically adjusting the aggressiveness of a wide range of state-of-the-art hardware data prefetchers by taking into account not only accuracy but also timeliness and pollution.    When the program enters a new phase of execution, the prefetcher is tuned based on the characteristics of the phase in Nesbit et al. <ref type="bibr" target="#b9">[10]</ref>. In order to perform phase detection/prediction and identification of the best prefetcher configuration for a given phase, significant amount of extra hardware is needed. In comparison, our mechanism is simpler because it does not require phase detection or prediction mechanisms.</p><p>Recently, Hur and Lin <ref type="bibr" target="#b6">[7]</ref> proposed a probabilistic technique that adjusts the aggressiveness of a stream prefetcher based on the estimated spatial locality of the program. Their approach is applicable only to stream prefetchers as it tries to estimate the a histogram of the stream length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Cache Pollution Filtering</head><p>Charney and Puzak <ref type="bibr" target="#b2">[3]</ref> proposed filtering L1 cache pollution caused by next-sequential prefetching and shadow directory prefetching from the L2 cache into the L1 cache. Their scheme associates a confirmation bit with each block in the L2 cache which indicates if the block was used by a demand access when it was prefetched into the L1 cache the last time. If the confirmation bit is not set when a prefetch request accesses the L2, the prefetch request is discarded. Extending this scheme to prefetching from main memory to the L2 cache requires a separate structure that maintains information about the blocks evicted from the L2 cache. This significantly increases the hardware cost of their mechanism. Our mechanism does not need to keep history information for evicted L2 cache blocks.</p><p>Zhuang and Lee <ref type="bibr" target="#b24">[25]</ref> proposed to filter prefetcher-generated cache pollution by using schemes similar to two-level branch predictors. Their mechanism tries to identify whether or not a prefetch will be useful based on past information about the usefulness of the prefetches generated to the same memory address or triggered by the same load instruction. In contrast, our mechanism does not require the collection of fine-grain information on each prefetch address or load address in order to vary the aggressiveness of the prefetcher.</p><p>Other approaches for cache pollution filtering include using a profiling mechanism to mark load instructions that can trigger hardware prefetches <ref type="bibr" target="#b22">[23]</ref>, and using compile-time techniques to mark dead cache locations so that prefetches can be inserted in dead locations <ref type="bibr" target="#b8">[9]</ref>. In comparison to these two mechanisms, our mechanism does not require any software or ISA support and can adjust to dynamic program behavior even if it differs from the behavior of the compile-time profile. Lin et al. <ref type="bibr" target="#b14">[15]</ref> proposed using density vectors to determine what to prefetch inside a region. This was especially useful in their model as they used very bandwidth-intensive scheduled region prefetching, which prefetches all the cache blocks in a memory region on a cache miss. This approach can be modified and combined with our proposal to further remove the pollution caused by blocks that are not used in a prefetch stream.</p><p>Mutlu et al. <ref type="bibr" target="#b16">[17]</ref> used the L1 caches as filters to reduce L2 cache pollution caused by useless prefetches. In their scheme, all prefetched blocks are placed into only the L1 cache. A prefetched block is placed into the L2 when it is evicted from the L1 cache only if it was needed by a demand request while it was in L1. In addition to useless prefetches, this approach also filters out some useful but early prefetches that are not used while residing in the L1 cache (such prefetches are common in very aggressive prefetchers). To obtain performance benefit from such prefetches, their scheme can be combined with our cache insertion policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Cache Insertion Policy for Prefetches</head><p>Lin et al. <ref type="bibr" target="#b13">[14]</ref> evaluated static policies to determine the placement in cache of prefetches generated by a scheduled region prefetcher. Their scheme placed prefetches in the LRU position of the LRU stack. We found that, even though inserting prefetches in the LRU position reduces the cache pollution effects of prefetches on some benchmarks, it also reduces the positive benefits of aggressive stream prefetching on other benchmarks because useful prefetches-if placed in the LRU positioncan be easily evicted from the cache in an aggressive prefetching scheme without providing any benefit. Dynamically adjusting the insertion policy of prefetched blocks based on the estimated pollution increases performance by 1.9% over the best static policy (LRU-4) and by 18.8% over inserting prefetches in the LRU position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion and Future Work</head><p>This paper proposed a feedback directed mechanism that dynamically adjusts the behavior of a hardware data prefetcher to improve performance and reduce memory bandwidth consumption. Over previous research in adaptive prefetching, our contributions are:</p><p>? We propose a comprehensive and low-cost feedback mechanism that takes into account prefetch accuracy, timeliness, and cache pollution caused by prefetch requests together to both throttle the aggressiveness of the prefetcher and to decide where in the cache to place the prefetched blocks. Previous approaches considered using only prefetch accuracy to determine the aggressiveness of simple sequential (next-line) prefetchers.</p><p>? We develop a low-cost mechanism to estimate at run-time the cache pollution caused by hardware prefetching.</p><p>? We propose and evaluate using comprehensive feedback mechanisms for state-of-the-art stream prefetchers that are commonly employed by today's high-performance processors. Our feedback-directed mechanism is applicable to any kind of hardware data prefetcher. We show that it works well with stream-based prefetchers, global-historybuffer based prefetchers and PC-based stride prefetchers. Previous adaptive mechanisms were applicable to only simple sequential prefetchers <ref type="bibr" target="#b3">[4]</ref>.</p><p>Future work can incorporate other important metrics, such as available memory bandwidth, estimates of the contention in the memory system, and prefetch coverage, into the dynamic feedback mechanism to provide further improvement in performance and further reduction in memory bandwidth consumption. The metrics defined and used in this paper could also be used as part of the selection mechanism in a hybrid prefetcher. Finally, the mechanisms proposed in this paper can be easily extended to instruction prefetchers.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Performance vs. aggressiveness of the prefetcher</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>P</head><label></label><figDesc>ref etch Accuracy = N umber of U sef ul P ref etches N umber of P ref etches Sent T o M emory</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>ref etch Lateness = Number of Late P ref etches N umber of U sef ul P ref etches</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .Figure 3 .</head><label>23</label><figDesc>Figure 2. IPC performance (left) and prefetch accuracy (right) with different aggressiveness configurations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Filter to estimate prefetcher-generated cache pollution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 . Dynamic adjustment of prefetcher aggressiveness 5 . 1 . 1 .</head><label>5511</label><figDesc>Figure 5. Dynamic adjustment of prefetcher aggressiveness 5.1.1. Adapting to the Program Figure6shows the distribution of the value of the Dynamic Configuration Counter over all sampling intervals in the Dynamic Aggressiveness mechanism. For benchmarks where aggressive prefetching hurts performance (e.g. applu, galgel, ammp), the feedback mechanism chooses and employs the least aggressive dynamic configuration (counter value of 1) for most of the sampling intervals. For example, the prefetcher is configured to be Very Conservative in more than 98% of the intervals for both applu and ammp.On the other hand, for benchmarks where aggressive prefetching significantly increases performance (e.g. wupwise, mgrid, equake), FDP employs the most aggressive configuration for most of the sampling intervals. For example, the prefetcher is configured to be Very Aggressive in more than 98% of the intervals for wupwise, mgrid, and equake.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Distribution of the dynamic aggressiveness level</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Dynamic adjustment of prefetch insertion policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Distribution of the insertion position of prefetched blocks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9</head><label>9</label><figDesc>compares the performance of five different mechanisms from left to right: (1) No prefetching, (2) Very Aggressive prefetching, (3) Very Aggressive prefetching with Dynamic Insertion, (4) Dynamic Aggressiveness , and (5) Dynamic Aggressiveness and Dynamic Insertion together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Overall performance of FDP improvement provided by Dynamic Aggressiveness or Dynamic Insertion alone. Hence, dynamically adjusting both aspects of prefetcher behavior (aggressiveness and insertion policy) provides complementary performance benefits.With the use of FDP to dynamically adjust both aspects of prefetcher behavior, the performance loss incurred on some benchmarks due to aggressive prefetching is completely eliminated. No benchmark loses performance compared to no prefetching if both Dynamic Aggressiveness and Dynamic Insertion are used. In fact, FDP improves the performance of applu by 13.4% and ammp by 11.4% over no prefetching -two benchmarks that otherwise incur very significant performance losses with an aggressive traditional prefetcher configuration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Effect of FDP on memory bandwidth consumption No pref. Very Cons. Middle Very Aggr. FDP IPC 0.85 1.21 1.47 1.57 1.67 BPKI 8.56 9.34 10.60 13.38 10.88</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Figure 11. Performance of prefetch cache vs. FDP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Effect of FDP on the IPC performance (left) BPKI memory bandwidth consumption (right) of GHB-based C/DC prefetchers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. IPC performance (left) and memory bandwidth consumption in BPKI (right) impact of FDP on the remaining SPEC benchmarks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Cache Pollution: Prefetcher</head><label></label><figDesc>generated cache pollution is a measure of the disturbance caused by prefetched data in the L2 cache. It is defined as:</figDesc><table><row><cell>P ref etcher Generated Cache P ollution =</cell></row><row><cell>N umber of Demand M isses Caused By the P ref etcher</cell></row><row><cell>N umber of Demand M isses</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc>our baseline stream prefetcher has five different configurations ranging from Very Conservative to Very Aggressive. The aggressiveness of the stream prefetcher is determined by the Dynamic Configuration Counter, a 3-bit saturating counter that saturates at values 1 and 5. The initial value of the Dynamic Configuration Counter is set to 3, indicating Middle-of-the-Road aggressiveness.</figDesc><table><row><cell>Dyn. Config. Counter</cell><cell>Aggressiveness</cell><cell>Pref. Distance</cell><cell>Pref. Degree</cell></row><row><cell>1 2 3 4 5</cell><cell>Very Conservative Conservative Middle-of-the-Road Aggressive Very Aggressive</cell><cell>4 8 16 32 64</cell><cell>1 1 2 4 4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 . Stream prefetcher configurations</head><label>1</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 . How to adapt? Use of the three metrics to adjust the aggressiveness of the prefetcher</head><label>2</label><figDesc></figDesc><table><row><cell>Case</cell><cell>Prefetch Accuracy</cell><cell>Prefetch Lateness</cell><cell>Cache Pollution</cell><cell>Dynamic Configuration Counter Update (reason)</cell></row><row><cell>1 2 3 4</cell><cell>High High High High</cell><cell>Late Late Not-Late Not-Late</cell><cell>Not-Polluting Polluting Not-Polluting Polluting</cell><cell>Increment (to increase timeliness) Increment (to increase timeliness) No Change (best case configuration) Decrement (to reduce pollution)</cell></row><row><cell>5 6 7 8</cell><cell>Medium Medium Medium Medium</cell><cell>Late Late Not-Late Not-Late</cell><cell>Not-Polluting Polluting Not-Polluting Polluting</cell><cell>Increment (to increase timeliness) Decrement (to reduce pollution) No Change (to keep the benefits of timely prefetches) Decrement (to reduce pollution)</cell></row><row><cell>9 10 11 12</cell><cell>Low Low Low Low</cell><cell>Late Late Not-Late Not-Late</cell><cell>Not-Polluting Polluting Not-Polluting Polluting</cell><cell>Decrement (to save bandwidth) Decrement (to reduce pollution) No Change (to keep the benefits of timely prefetches) Decrement (to reduce pollution and save bandwidth)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 . Number of prefetches sent by a very aggressive stream prefetcher for each benchmark in the SPEC CPU2000 suite</head><label>4</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 . Average IPC and BPKI for FDP vs conventional prefetchers</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="4">Very Cons. Middle Very Aggr. FDP</cell></row><row><cell>IPC</cell><cell>0.85</cell><cell>1.21</cell><cell>1.47</cell><cell>1.57</cell><cell>1.67</cell></row><row><cell>BPKI</cell><cell>8.56</cell><cell>9.34</cell><cell>10.60</cell><cell>13.38</cell><cell>10.88</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 . Hardware cost of feedback directed prefetching</head><label>6</label><figDesc>L2 cache without any adverse latency impact on L2 cache access time.</figDesc><table><row><cell>pref-bit for each tag-store entry in the L2 cache</cell><cell>16384 blocks * 1 bit/block = 16384 bits</cell></row><row><cell>Pollution Filter</cell><cell>4096 entries * 1 bit/entry = 4096 bits</cell></row><row><cell>16-bit counters used to estimate feedback metrics</cell><cell>11 counters * 16 bits/counter = 176 bits</cell></row><row><cell>pref-bit for each MSHR entry</cell><cell>128 entries * 1 bit/entry = 128 bits</cell></row><row><cell>Total hardware cost Percentage area overhead compared to baseline 1MB L2 cache</cell><cell>20784 bits = 2.54 KB 2.5KB/1024KB = 0.24%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>BPKI ? IPC ? BPKI ? IPC ? BPKI ? IPC ? BPKI ? IPC ? BPKI ? IPC ?BPKI 0% -13.9% 6.5% -18.7% 6.3% -29.6% 4.5% -23.0% 6.5% -18.7% 8.4% -16.9%</figDesc><table><row><cell cols="3">L2 Cache Size (memory latency = 500 cycles)</cell><cell cols="3">Memory Latency (L2 cache size = 1 MB)</cell></row><row><cell>512 KB</cell><cell>1 MB</cell><cell>2 MB</cell><cell>250 cycles</cell><cell>500 cycles</cell><cell>1000 cycles</cell></row><row><cell>? IPC ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 . Change in IPC and BPKI with FDP when L2 size and memory latency are varied</head><label>7</label><figDesc></figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Note that all addresses tracked by the prefetcher are cache-block addresses.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Right after a tracking entry is trained, the prefetcher sets the start pointer to the the first L2 miss address that allocated the tracking entry and the end pointer to the last L2 miss address that determined the direction of the entry plus an initial start-up distance. Until the monitored memory region's size becomes the same as the Prefetch Distance (in terms of cache blocks), the tracking entry increments only the end pointer by the Prefetch Degree when prefetches are issued (i.e. the end pointer points to the last address requested as a prefetch and the start pointer points to the L2 miss address that allocated the tracking entry). After the monitored memory region's size becomes the same as Prefetch Distance, both the start pointer and the end pointer are incremented by Prefetch Degree (N) when prefetches are issued. This way, the prefetcher is able to send prefetch requests that are Prefetch Distance ahead of the demand access stream.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Note that several proposed prefetching implementations, such as tagged next-sequential prefetching<ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref> already employ pref-bits in the cache.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>There are other ways to define the length of an interval, e.g. based on the number of instructions executed. We use the number of useful cache blocks evicted to define an interval because this metric provides a more accurate view of the memory behavior of a program than the number of instructions executed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>Note that we adjust prefetcher behavior on a global (across-streams) basis rather than on a per-stream basis as we did not find much benefit in adjusting on a per-stream basis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>We found inserting prefetched blocks to the MRU position doesn't provide significant benefits over inserting them to the MID position. Thus, our dynamic mechanism doesn't insert prefetched blocks to the MRU position. For a detailed analysis of the cache insertion policy, see Section 5.2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6"><p>We use Bus Accesses (rather than the number of prefetches sent) as our bandwidth metric, because this metric includes the effect of L2 misses caused due to demand accesses as well as prefetches. If the prefetcher is polluting the cache, then the number of L2 misses due to demand accesses also increases. Hence, counting the number of bus accesses provides a more accurate measure of the memory bandwidth consumed by the prefetcher.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7"><p>Middle-of-the-Road configuration consumes only 2.5% less memory bandwidth than FDP.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8"><p>In the configurations with a prefetch cache, a prefetched cache block is moved from the prefetch cache into the L2 cache if it is accessed by a demand load/store request. The block size of the prefetch cache and the L2 cache are the same and the prefetch cache is assumed to be accessed in parallel with the</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_9"><p>In the GHB-based prefetching mechanism, Prefetch Distance and Prefetch Degree are the same.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank <rs type="person">Matthew Merten</rs>, <rs type="person">Moinuddin Qureshi</rs>, members of the <rs type="institution">HPS Research Group</rs>, and the anonymous reviewers for their comments and suggestions. We gratefully acknowledge the support of the <rs type="funder">Cockrell Foundation</rs>, <rs type="funder">Intel Corporation</rs> and the <rs type="programName">Advanced Technology Program</rs> of the <rs type="institution">Texas Higher Education Coordinating Board</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ArTJvg8">
					<orgName type="program" subtype="full">Advanced Technology Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An effective on-chip preloading scheme to reduce data access penalty</title>
		<author>
			<persName><forename type="first">J</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Space/time trade-offs in hash coding with allowable errors</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Bloom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="422" to="426" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Prefetching and memory system behavior of the SPEC95 benchmark suite</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IBM Journal of Reseach and Development</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="286" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sequential hardware prefetching in shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="733" to="746" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Buffer block prefetching method. IBM Technical Disclosure</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Gindele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="696" to="697" />
			<date type="published" when="1977-07">July 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The microarchitecture of the Pentium 4 processor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Upton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Carmean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Roussel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intel Technology Journal</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2001-02">Feb. 2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICRO-39</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Effective stream-based and execution-based data prefetching</title>
		<author>
			<persName><forename type="first">S</forename><surname>Iacobovici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kadambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Controlling cache pollution in prefetching with software-assisted cache replacement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Devadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
		<idno>CSG-462</idno>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AC/DC: An adaptive data cache prefetcher</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-17</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lockup-free instruction fetch/prefetch cache organization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-8</title>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data prefetching in shared memory multiprocessors</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lawrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reducing DRAM latencies with an integrated memory hierarchy design</title>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Filtering superfluous prefetches using density vectors</title>
		<author>
			<persName><forename type="first">W.-F</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCD</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An analysis of the performance impact of wrong-path memory references on out-of-order and runahead execution processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1556" to="1571" />
			<date type="published" when="2005-12">Dec. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using the first-level caches as filters to reduce the pollution caused by speculative memory references</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="529" to="559" />
			<date type="published" when="2005-10">October 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Techniques for efficient processing in runahead execution engines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-32</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Evaluating stream buffers as a secondary cache replacement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA-21</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bloom filtering cache misses for accurate data speculation and prefetching</title>
		<author>
			<persName><forename type="first">J.-K</forename><surname>Peir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cache memories</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="530" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chip multithreading: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spracklen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Abraham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA-11</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A static filter for reducing prefetch traffic</title>
		<author>
			<persName><forename type="first">V</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Tyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Davidson</surname></persName>
		</author>
		<idno>CSE-TR-400-99</idno>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
		<respStmt>
			<orgName>University of Michigan Technical</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">POWER4 system microarchitecture. IBM Technical White Paper</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dodson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sinharoy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001-10">Oct. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A hardware-based cache pollution filtering mechanism for aggressive prefetches</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPP-32</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
