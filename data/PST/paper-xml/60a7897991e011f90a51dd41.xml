<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-20">20 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Pan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liwei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">ByteDance AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrastive Learning for Many-to-many Multilingual Neural Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-20">20 May 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2105.09501v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing multilingual machine translation approaches mainly focus on English-centric directions, while the non-English directions still lag behind. In this work, we aim to build a many-to-many translation system with an emphasis on the quality of non-English language directions. Our intuition is based on the hypothesis that a universal cross-language representation leads to better multilingual translation performance. To this end, we propose mCOLT, a training method to obtain a single unified multilingual translation model. mCOLT is empowered by two techniques: a) a contrastive learning scheme to close the gap among representations of different languages, and b) data augmentation on both multiple parallel and monolingual data to further align token representations. For English-centric directions, mCOLT achieves competitive or even better performance than a strong pre-trained model mBART on tens of WMT benchmarks. For non-English directions, mCOLT achieves an improvement of average 10+ BLEU compared with the multilingual baseline 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer has achieved decent performance for machine translation with rich bilingual parallel corpus <ref type="bibr" target="#b32">(Vaswani et al., 2017a)</ref>. Recent work on multilingual machine translation aims to create a single unified model to translate many languages <ref type="bibr" target="#b20">(Johnson et al., 2017;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019;</ref><ref type="bibr" target="#b37">Zhang et al., 2020;</ref><ref type="bibr" target="#b14">Fan et al., 2020;</ref><ref type="bibr" target="#b28">Siddhant et al., 2020)</ref>. Multilingual translation models are appealing for two reasons. First, they are model efficient, enabling easier deployment <ref type="bibr" target="#b20">(Johnson et al., 2017)</ref>. Further, parameter sharing across different languages encourages knowledge transfer, which benefits low-resource translation directions and Figure <ref type="figure">1</ref>: The proposed mCOLT. It takes a pair of sentences (or augmented pseudo-pair) and computes normal cross entropy loss with a multi-lingual encoderdecoder. In addition, it computes contrastive loss on the representations of the aligned pair (positive example) and randomly selected non-aligned pair (negative example).</p><p>potentially enables zero-shot translation (i.e. direct translation between a language pair not seen during training) <ref type="bibr" target="#b17">(Ha et al., 2017;</ref><ref type="bibr" target="#b16">Gu et al., 2019;</ref><ref type="bibr" target="#b19">Ji et al., 2020)</ref>. Despite these benefits, challenges still remain in multilingual NMT. First, previous work on multilingual NMT does not always perform well as their corresponding bilingual baseline especially on rich resource language pairs <ref type="bibr" target="#b29">(Tan et al., 2019;</ref><ref type="bibr" target="#b37">Zhang et al., 2020;</ref><ref type="bibr" target="#b14">Fan et al., 2020)</ref>. Such performance gap becomes larger with the increasing number of accommodated languages for multilingual NMT, as model capacity necessarily must be split between many languages <ref type="bibr" target="#b2">(Arivazhagan et al., 2019)</ref>. In addition, an optimal setting for multilingual NMT should be effective for any language pairs, while most previous work focus on improving English-centric directions <ref type="bibr" target="#b20">(Johnson et al., 2017;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019;</ref><ref type="bibr" target="#b37">Zhang et al., 2020)</ref>. A few recent exceptions are <ref type="bibr" target="#b37">Zhang et al. (2020)</ref> and <ref type="bibr" target="#b14">Fan et al. (2020)</ref>, who trained many-to-many sys-tems with introducing more non-English corpus, through data mining or back translation.</p><p>In this work, we take a step towards a unified many-to-many multilingual NMT with only English-centric parallel corpus and additional monolingual corpus.</p><p>Our key insight is to close the representation gap between different languages to encourage transfer learning as much as possible. As such, many-to-many translations can make the most of the knowledge from all supervised directions and the model can perform well for both English-centric and non-English settings. In this paper, we propose a multilingual COntrastive Learning framework for Translation (mCOLT) to reduce the representation gap of different languages, as shown in Figure <ref type="figure">1</ref>.</p><p>The objective of mCOLT ensures the model to represent similar sentences across languages in a shared space by training the encoder to minimize the representation distance of similar sentences. In addition, we also boost mCOLT by leveraging monolingual data to further improve multilingual translation quality. We introduce an effective aligned augmentation technique by extending RAS (Lin et al., 2020) -on both parallel and monolingual corpus to create pseudo-pairs. These pseudo-pairs are combined with multilingual parallel corpus in a unified training framework.</p><p>Simple yet effective, mCOLT achieves consistent translation performance improvements for both English-centric and non-English directions on a wide range of benchmarks. For Englishcentric directions, mCOLT outperforms a strong multilingual baseline in 20 translation directions on WMT testsets. On 10 WMT translation benchmarks, mCOLT even obtains better results than the strong bilingual mBART model. For zero-shot and unsupervised directions, mCOLT obtains surprisingly strong results on 36 translation directions 2 , with 10+ BLEU improvements on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>mCOLT unifies both parallel corpus and monolingual corpus with contrastive learning. This section will explain our proposed mCOLT. The overall framework is illustrated in Figure <ref type="figure">1</ref> 2.1 Multilingual NMT A multilingual neural machine translation model learns a many-to-many mapping function f to 2 6 unsupervised directions + 30 zero-shot directions translate from one language to another. To distinguish different languages, we add an artificial token in front of each sentence, for both source side and target side. The base architecture of mCOLT is the state-of-the-art Transformer <ref type="bibr" target="#b33">(Vaswani et al. (2017b)</ref>). A little different from previous work, we choose a larger setting with a 12-layer encoder and a 12-layer decoder to increase the model capacity. The model dimension is 1024 on 16 heads. To ease the training of the deep model, we apply Layer Normalization for word embedding and pre-norm residual connection following <ref type="bibr" target="#b34">Wang et al. (2019a)</ref> for both encoder and decoder. Therefore, our multilingual NMT baseline is much stronger than that of Transformer big model.</p><p>More formally, we define L = {L 1 , . . . , L M } where L is a collection of M languages involving in the training phase. D i,j denotes a parallel dataset of (L i , L j ), and D denotes all parallel datasets. The multilingual training loss is then defined as:</p><formula xml:id="formula_0">L mt = x i ,x j ∈D − log P θ (x i |x j )<label>(1)</label></formula><p>where x i represents a sentence in language L i , and θ is the parameter of multilingual NMT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilingual Contrastive Learning</head><p>Multilingual NMT enables implicitly learning shared representation of different languages. mCOLT introduces contrastive loss to explicitly project different languages to a shared semantic space.</p><p>The key idea of contrastive learning is to minimize the representation gap of similar sentences and maximize that of irrelevant sentences. Formally, given a bilingual translation pairs (x i , x j ) ∈ D, (x i , x j ) is the positive example and we randomly choose a sentence y j from language L j to form a negative example 3 (x i , y j ). The objective of contrastive learning is to minimize:</p><formula xml:id="formula_1">L ctl = − x i ,x j ∈D log e sim + (R(x i ),R(x j ))/τ</formula><p>y j e sim − (R(x i ),R(y j ))/τ</p><p>(2) where sim(•) calculates the similarity of different sentences. + and − denotes positive and negative respectively. R(s) denotes the averagepooled encoded output of an arbitrary sentence s. τ is the temperature, which controls the difficulty of distinguishing between positive and negative examples 4 . In our experiments, it is set to 0.1. The similarity of two sentences is calculated with the cosine similarity of the averagepooled encoded output. To simplify implementation, the negative samples are sampled from the same training batch. Intuitively, by maximizing the softmax term sim + (R(x i ), R(x j )), the contrastive loss forces their semantic representations projected close to each other. In the meantime, the softmax function also minimizes the non-matched pairs sim − (R(x i ), R(y j )).</p><p>During the training of mCOLT, the model can be optimized by jointly minimizing the contrastive training loss and multilingual NMT training loss:</p><formula xml:id="formula_2">L mCOLT = L mt + λ|s|L ctl (3)</formula><p>where λ is the coefficient to balance the two training losses. Since L ctl is calculated on the sentencelevel and L mt is calculated on the token-level, therefore L ctl should be multiplied by the averaged sequence length |s|. We then will introduce how to improve mCOLT with data augmentation methods, including the introduction of noised bilingual and noised monolingual data for multilingual NMT.  <ref type="formula">2020</ref>) propose Random Aligned Substitution technique (or RAS 5 ) that builds code-4 Higher temperature increases the difficulty to distinguish positive sample from negative ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Aligned Augmentation</head><p>5 They apply RAS only on parallel data switched sentence pairs (C(x i ), x j ) for multilingual pre-training. In this paper, we extend it to Aligned Augmentation (AA), which can also be applied to monolingual data. For a bilingual or monolingual sentence pair (x i , x j )<ref type="foot" target="#foot_1">6</ref> , AA creates a perturbed sentence C(x i ) by replacing aligned words from a synonym dictionary<ref type="foot" target="#foot_2">7</ref> . For every word contained in the synonym dictionary, we randomly replace it to one of its synonym with a probability of 90%.</p><p>For a bilingual sentence pair (x i , x j ), AA creates a pseudo-parallel training example (C(x i ), x j ). For monolingual data, AA takes a sentence x i and generates its perturbed C(x i ) to form a pseudo self-parallel example (C(x i ), x i ). (C(x i ), x j ) and (C(x i ), x i ) is then used in the training by calculating both the translation loss and contrastive loss. For a pseudo self-parallel example (C(x i ), x i ), the contrastive loss is basically the reconstruction loss from the perturbed sentence to the original one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>This section shows that mCOLT can achieve substantial improvements over previous many-tomany multilingual translation on a wide range of benchmarks. Especially, it obtains substantial gains on zero-shot directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Settings and Datasets</head><p>Parallel Dataset PC32 We use the parallel dataset PC32 provided by Lin et al. ( <ref type="formula">2020</ref>). It contains a large public parallel corpora of 32 Englishcentric language pairs. The total number of sentence pairs is 97.6 million.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monolingual Dataset MC24</head><p>We create a dataset MC24 with monolingual text in 24 languages<ref type="foot" target="#foot_3">8</ref> . It is a subset of the Newscrawl<ref type="foot" target="#foot_4">9</ref> dataset by retaining only those languages in PC32, plus three additional languages that are not in PC32 (Nl, Pl, Pt). In order to balance the volume across different languages, we apply temperature sampling ñi = n i / j n j 1/T with T =5 over the dataset, where n i is the number of sentences in ith language. Then we apply M-RAS on monolin-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Fr wmt14</head><p>En-Tr wmt17</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Es wmt13</head><p>En-Ro wmt16 For zero-shot directions, we follow <ref type="bibr" target="#b37">(Zhang et al., 2020)</ref> and use their proposed OPUS-100 zero-shot testset. The testset is comprised of 6 languages (Ru, De, Fr, Nl, Ar, Zh), resulting in 15 language pairs and 30 translation directions.</p><formula xml:id="formula_3">En-Fi wmt17 Avg ∆ → ← → ← → ← →(*) ← → ← Transformer(*</formula><p>We report de-tokenized BLEU with Sacre-BLEU <ref type="bibr" target="#b26">(Post (2018)</ref>). For tokenized BLEU, we tokenize both reference and hypothesis using Sacremoses 10 toolkit then report BLEU using the multi-bleu.pl script 11 . For Chinese (Zh), BLEU score is calculated on character-level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Details</head><p>We use the Transformer model in our experiments, with 12 encoder layers and 12 decoder layers. The embedding size and FFN dimension are set to 1024. We use dropout = 0.1, as well as a learning rate of 3e-4 with polynomial decay scheduling and a warm-up step of 10000. For optimization, we use Adam optimizer (Kingma and Ba ( <ref type="formula">2014</ref>)) with = 1e-6 and β 2 = 0.98. To stabilize training, we set the threshold of gradient norm to be 5.0 and clip all gradients with a larger norm. We set the hyper-parameter λ = 1.0 in Eq.3 during training. For multilingual vocabulary, we follow the shared BPE (Sennrich et al. ( <ref type="formula">2016</ref>)) vocabulary of Lin et al. ( <ref type="formula">2020</ref>), which includes 59 languages. The vocabulary con-10 https://github.com/alvations/sacremoses 11 https://github.com/moses-smt/mosesdecoder/ tains 64808 tokens. After adding 59 language tokens, the total size of vocabulary is 64867.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head><p>This section shows that mCOLT provides consistent performance gains for supervised and unsupervised English-centric translation directions as well as for non-English directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">English-Centric Directions</head><p>Supervised Directions As shown in Table <ref type="table" target="#tab_0">1</ref>, mCOLT clearly improves multilingual baselines by a large margin in 10 translation directions. Previously, multilingual machine translation underperforms bilingual translation in rich-resource scenarios. It is worth noting that our multilingual machine translation baseline is already very competitive. It is even on par with the strong mBART bilingual model, which is fine-tuned on a large scale unlabeled monolingual dataset. mCOLT further improves the performance.</p><p>We summarize the key factors for the success training of our baseline<ref type="foot" target="#foot_5">12</ref> m-Transformer: a) The batch size plays a crucial role in the success of training multilingual NMT. We use 8×200 NVIDIA V100 to train the models and each batch contains about 3 million tokens. b) We enlarge the number of layers from 6 to 12 and observe significant improvements for multilingual NMT. By contrast, the gains from increasing the bilingual model size is not that large. mBART also uses 12 encoder and decoder layers. c) We use gradient norm to stable the training. Without this regularization, the large scale training will be collapsed</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>En-Nl iwslt2014</head><p>En-Pt opus-100 sometimes.</p><formula xml:id="formula_4">En-Pl wmt20 Avg ∆ → ← → ← → ← m-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Directions</head><p>In Table <ref type="table" target="#tab_1">2</ref>, we observe that mCOLT achieves reasonable results on unsupervised translation directions. The language pairs of En-Nl, En-Pt, and En-Pl are never observed by m-Transformer. m-Transformer sometimes achieves reasonable BLEU for X→En, e.g. 10.7 for Pt→En, since there are many similar languages in PC32, such as Es and Fr. Not surprisingly, it totally fails on En→X directions. By contrast, mCOLT obtains +14.13 BLEU score on an average without explicitly introducing supervision signals for these directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Zero-shot Translation for non-English Directions</head><p>Zero-shot Translation has been an intriguing topic in multilingual neural machine translation. Previous work shows that the multilingual NMT model can do zero-shot translation directly. However, the translation quality is quite poor compared with pivot-based model. We evaluate mCOLT on the OPUS-100 (Zhang et al. ( <ref type="formula">2020</ref>)) zero-shot test set, which contains 6 languages 13 and 30 translation directions in total.</p><p>13 Arabic, Chinese, Dutch, French, German, Russian To make the comparison clear, we also report the results of several different baselines. mCOLT w/o AA only adopt contrastive learning on the basis of m-Transformer. mCOLT w/o MC24 excludes monolingual data from mCOLT.</p><p>The evaluation results are listed in Appendix and we summarize them in Table <ref type="table" target="#tab_2">3</ref>. We find that our mCOLT significantly outperforms m-Transformer and substantially narrows the gap with pivot-based model. This is in line with our intuition that bridging the representation gap of different languages can improve the zero-shot translation.</p><p>The main reason is that contrastive loss, aligned augmentation and additional monolingual data enable a better language-agnostic sentence representation. It is worth noting that, <ref type="bibr" target="#b37">Zhang et al. (2020)</ref> achieves BLEU score improvements on zero-shot translations at sacrifice of about 0.5 BLEU score loss on English-centric directions. By contrast, mCOLT improves zero-shot translation by a large margin without losing performance on English-Centric directions. Therefore, mCOLT has a great potential to serve many-to-many translations, including both English-centric and non-English directions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>To understand what contributes to the performance gain, we conduct analytical experiments in this section. First we summarize and analysis the performance of mCOLT in different scenarios.</p><p>Second we adopt the sentence representation of mCOLT to retrieve similar sentences across languages. This is to verify our argument that the improvements come from the universal language representation learned by mCOLT. Finally we visualize the sentence representations, mCOLT indeed draws the representations closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>To make a better understand of the effectiveness of mCOLT, we evaluate models of different settings.</p><p>We summarize the experiment results in Table <ref type="table" target="#tab_3">4</ref>:</p><p>• 1 v.s. 2 : 2 performs comparably with m-Transformer in supervised and unsupervised scenarios, whereas achieves a substantial BLEU improvement for zero-shot translation. This indicates that by introducing contrastive loss, we can improve zero-shot translation quality without harming other directions.</p><p>• 3 v.s. 4 : 3 performs poorly for zero-shot directions. This means contrastive loss is crucial for the performance in zero-shot directions.</p><p>• 5 : mCOLT further improves BLEU in all of the three scenarios, especially in unsupervised directions. Therefore it is safe to conjecture that by accomplishing with monolingual data, mCOLT learns a better representation space. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Similarity Search</head><p>In order to verify whether mCOLT learns a better representation space, we conduct a set of similarity search experiments. Similarity search is a task to find the nearest neighbor of each sentence in another language according to cosine similarity. We argue that mCOLT benefits this task in the sense that it bridges the representation gap across languages. Therefore we use the accuracy of similarity search tasks as a quantitative indicator of cross-lingual representation alignment. We conducted comprehensive experiments to support our argument and experiment on mCOLT and mCOLT w/o AA .We divide the experiments into two scenarios: First we evaluate our method on Tatoeba dataset <ref type="bibr" target="#b3">(Artetxe and Schwenk (2019)</ref>), which is English-centric. Then we conduct similar similarity search task on non-English language pairs. Following <ref type="bibr" target="#b31">Tran et al. (2020)</ref>, we construct a multi-way parallel testset (Ted-M) of 2284 samples by filtering the test split of ted <ref type="foot" target="#foot_6">14</ref>  translations for all 15 languages 15 . Under both settings, we follow the same strategy: We use the average-pooled encoded output as the sentence representation. For each sentence from the source language, we search the closest sentence in the target set according to cosine similarity.</p><p>English-Centric: Tatoeba We display the evaluation results in Table <ref type="table" target="#tab_4">5</ref>. We detect two trends: (i) The overall accuracy follows the rule: m-Transformer &lt; mCOLT w/o AA &lt; mCOLT. (ii) mCOLT brings more significant improvements for languages with less data volume in PC32. The two trends mean that mCOLT increases translation BLEU score in a sense that it bridges the representation gap across languages.</p><p>Non-English: Ted-M It will be more convincing to argue that mCOLT indeed bridges the representation gap if similarity search accuracy increases on zero-shot directions. We list the averaged top-1 accuracy of 210 non-English directions 16 in Table <ref type="table" target="#tab_5">6</ref>. The results show that mCOLT increases the similarity search accuracy in zeroshot scenario. The results support our argument that our method generally narrows the representation gap across languages.</p><p>To better understanding the specifics beyond 15 Arabic, Czech, German, English, Spanish, French, Italian, Japanese, Korean, Dutch, Romanian, Russian, Turkish, Vietnamese, Chinese 16 15 languages, resulting in 210 directions </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Visualization</head><p>In order to visualize the sentence representations across languages, we retrieve the sentence representation R(s) for each sentence in Ted-M, resulting in 34260 samples in the high-dimensional space.</p><p>To facilitate visualization, we apply T-SNE dimension reduction to reduce the 1024-dim representations to 2-dim. Then we select 3 representative languages: English, German, Japanese and depict the bivariate kernel density estimation based on the 2-dim representations. It is clear in Figure <ref type="figure" target="#fig_3">4</ref> that m-Transformer cannot align the 3 languages. By contrast, mCOLT draws the representations across 3 languages much closer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multilingual Neural Machine Translation</head><p>While initial research on NMT starts with building translation systems between two languages, <ref type="bibr" target="#b12">Dong et al. (2015)</ref> extends the bilingual NMT to one-to-many translation with sharing encoders across 4 language pairs. Hence, there has been a massive increase in work on MT systems that involve more than two languages <ref type="bibr" target="#b7">(Chen et al., 2018;</ref><ref type="bibr" target="#b8">Choi et al., 2017;</ref><ref type="bibr" target="#b9">Chu and Dabre, 2018;</ref><ref type="bibr" target="#b11">Dabre et al., 2017)</ref>. Recent efforts mainly focuses on designing language specific components for multilingual NMT to enhance the model performance on rich-resource languages <ref type="bibr" target="#b4">(Bapna et al., 2019;</ref><ref type="bibr" target="#b21">Kim et al., 2019;</ref><ref type="bibr" target="#b35">Wang et al., 2019b;</ref><ref type="bibr" target="#b13">Escolano et al., 2020)</ref>. Another promising thread line is to enlarge the model size with extensive training data to improve the model capability <ref type="bibr" target="#b2">(Arivazhagan et al., 2019;</ref><ref type="bibr" target="#b0">Aharoni et al., 2019;</ref><ref type="bibr" target="#b14">Fan et al., 2020)</ref>. Different from these approaches, mCOLT proposes to explicitly close the semantic representation of different languages and make the most of cross lingual transfer.</p><p>Zero-shot Machine Translation Typical zeroshot machine translation models rely on a pivot language (e.g. English) to combine the sourcepivot and pivot-target translation models <ref type="bibr" target="#b6">(Chen et al., 2017;</ref><ref type="bibr" target="#b17">Ha et al., 2017;</ref><ref type="bibr" target="#b16">Gu et al., 2019;</ref><ref type="bibr" target="#b10">Currey and Heafield, 2019)</ref>. <ref type="bibr" target="#b20">Johnson et al. (2017)</ref> shows that a multilingual NMT system enables zero-shot translation without explicitly introducing pivot methods. Promising, but the performance still lags behind the pivot competitors. Most following up studies focused on data augmentation methods. <ref type="bibr" target="#b37">Zhang et al. (2020)</ref> improved the zero-shot translation with online back translation. <ref type="bibr" target="#b19">Ji et al. (2020)</ref>; <ref type="bibr" target="#b23">Liu et al. (2020)</ref> shows that large scale monolingual data can improve the zero-shot translation with unsupervised pre-training. <ref type="bibr" target="#b14">Fan et al. (2020)</ref> proposes a simple and effective data mining method to enlarge the training corpus of zero-shot directions. Some work also attempted to explicitly learn shared semantic representation of different languages to improve the zero-shot translation. <ref type="bibr" target="#b24">Lu et al. (2018)</ref> suggests that by learning an explicit "interlingual" across languages, multilingual NMT model can significantly improve zero-shot translation quality. <ref type="bibr" target="#b1">Al-Shedivat and Parikh (2019)</ref> introduces a consistent agreement-based training method that en-courages the model to produce equivalent translations of parallel sentences in auxiliary languages. Different from these efforts, mCOLT attempts to learn a universal many-to-many model, and bridge the cross-lingual representation with contrastive learning and m-RAS. The performance is very competitive both on zero-shot and supervised directions on large scale experiments.</p><p>Contrastive Learning Contrastive Learning has become a rising domain and achieved significant success in various computer vision tasks <ref type="bibr" target="#b38">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b30">Tian et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2020;</ref><ref type="bibr" target="#b5">Chen et al., 2020;</ref><ref type="bibr" target="#b25">Misra and Maaten, 2020)</ref>. Researchers in the NLP domain have also explored contrastive Learning for sentence representation. <ref type="bibr" target="#b36">Wu et al. (2020)</ref> employed multiple sentence-level augmentation strategies to learn a noise-invariant sentence representation. <ref type="bibr" target="#b15">Fang and Xie (2020)</ref> applies the back-translation to create augmentations of original sentences. Inspired by these studies, we apply contrastive learning for multilingual NMT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We demonstrate that contrastive learning can significantly improve zero-shot machine translation directions. Combined with additional unsupervised monolingual data, we achieve substantial improvements over on all translation directions of multilingual NMT. We analyze and visualize our method, and find that contrastive learning as well as m-RAS trends to close the representation gap of different languages. Our results also show the possibilities of training a true many-to-many Multilingual NMT that works well on any translation direction.</p><p>In future work, we will scale up the current training to more languages, e.g. PC100 and MC500. As such, a single model can handle more than 100 languages and outperforms the corresponding bilingual baseline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Aligned augmentation on both parallel and monolingual data by replacing words with the same meaning in synonym dictionaries. It either creates a pseudo-parallel example (C(x i ), x j ) or a pseudo selfparallel example (C(x i ), x i ).</figDesc><graphic url="image-2.png" coords="3,72.00,412.07,218.25,125.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The above two types of training samples are illustrated in Figure 2. Lin et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Accuracy Improvements of m-Transformer → mCOLT w/o AA → mCOLT for Ted-M. Darker red means larger improvements. mCOLT w/o AA generally improves accuracy over m-Transformer and mCOLT especially improves the accuracy X ↔ Nl over mCOLT w/o AA .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Bivariate kernel density estimation plots of representations after using T-SNE dimensionality reduction to 2 dimension. The blue line is English, the orange line is Japanese and the green line is German. This figure illustrates that the sentence representations are drawn closer after applying mCOLT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>mCOLT outperforms m-Transformer in supervised translation directions. Consistent BLEU gains are observed in 20 directions (See Appendix) and in this table we pick the representative ones. We report tokenized BLEU above. Different from our work, their final BLEU scores are obtained by fine-tuning on a single direction. (*) Note that for En→Ro direction, we follow the previous setting to calculate BLEU score after removing Romanian dialects. (**) BLEU scores for Transformer and mBART are cited from<ref type="bibr" target="#b23">(Liu et al., 2020)</ref> gual data. The total number of sentences in MC24 is 1.01 billion. The detail of data volume is listed in the Appendix.</figDesc><table><row><cell cols="2">*) 41.4 -</cell><cell>9.5</cell><cell>12.2 35.0 -</cell><cell>34.3</cell><cell>36.8 20.2 21.8</cell><cell>-</cell></row><row><cell>mBART25(**)</cell><cell>41.1 -</cell><cell cols="2">17.8 22.5 33.3 -</cell><cell>37.7</cell><cell>38.8 22.4 28.5</cell><cell>-</cell></row><row><cell>m-Transformer</cell><cell cols="4">42.0 38.1 18.8 23.1 32.8 33.7 35.9</cell><cell cols="2">37.7 20.0 28.2 31.11</cell></row><row><cell>mCOLT</cell><cell cols="4">43.7 39.4 21.9 23.9 34.1 34.3 38.0</cell><cell cols="2">38.8 23.0 28.6 32.57 +1.46</cell></row></table><note>Evaluation Datasets For supervised directions, most of our evaluation datasets are from WMT and IWSLT benchmarks, for pairs that are not available in WMT or IWSLT, we use OPUS-100 instead.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>mCOLT outperforms m-Transformer in unsupervised translation directions by a large margin. We report tokenized BLEU above.</figDesc><table><row><cell cols="2">Transformer 1.3</cell><cell>7.0</cell><cell>3.7</cell><cell cols="2">10.7 0.6 3.2</cell><cell>4.42</cell><cell></cell></row><row><cell>mCOLT</cell><cell cols="7">10.1 28.5 18.4 30.5 6.7 17.1 18.55 +14.13</cell></row><row><cell></cell><cell>Ar</cell><cell></cell><cell></cell><cell>Zh</cell><cell cols="2">Nl(*)</cell><cell></cell></row><row><cell></cell><cell cols="6">X→Ar Ar→X X→Zh Zh→X X→Nl Nl→X</cell><cell></cell></row><row><cell>Pivot</cell><cell>5.5</cell><cell>17.0</cell><cell>28.5</cell><cell>16.4</cell><cell>2.2</cell><cell>6.0</cell><cell></cell></row><row><cell cols="2">m-Transformer 3.7</cell><cell>5.6</cell><cell>6.7</cell><cell>4.1</cell><cell>2.3</cell><cell>6.3</cell><cell></cell></row><row><cell>mCOLT</cell><cell>5.8</cell><cell>17.2</cell><cell>28.6</cell><cell>14.2</cell><cell>5.5</cell><cell>7.1</cell><cell></cell></row><row><cell></cell><cell>Fr</cell><cell></cell><cell></cell><cell>De</cell><cell cols="2">Ru</cell><cell>Avg of all</cell></row><row><cell></cell><cell cols="6">X→Fr Fr→X X→De De→X X→Ru Ru→X</cell><cell></cell></row><row><cell>Pivot</cell><cell>26.1</cell><cell>22.3</cell><cell>14.4</cell><cell>14.2</cell><cell>16.6</cell><cell>19.9</cell><cell>15.56</cell></row><row><cell cols="2">m-Transformer 7.7</cell><cell>4.8</cell><cell>4.2</cell><cell>4.8</cell><cell>5.7</cell><cell>4.8</cell><cell>5.05</cell></row><row><cell>mCOLT</cell><cell>24.5</cell><cell>20.2</cell><cell>11.9</cell><cell>14.5</cell><cell>14.8</cell><cell>18.8</cell><cell>15.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Zero</figDesc><table /><note>-Shot: We report de-tokenized BLEU using sacreBLEU in OPUS-100. We observe consistent BLEU gains in zero-shot directions on different evaluation sets, see Appendix for more details. mCOLT further improves the quality. We also list BLEU of pivot-based model (X→En then En→Y using m-Transformer) as a reference, mCOLT only lags behind Pivot by -0.39 BLEU. (*) Note that Dutch(Nl) is not included in PC32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Summary of average BLEU of mCOLT w/o AA and mCOLT in different scenarios. We report averaged tokenized BLEU. For supervised translation, we report the average of 20 directions; for zero-shot translation, we report the average of 30 directions of OPUS-100. mCOLT w/o AA only adopts contrastive learning on the basis of m-Transformer. mCOLT w/o MC24&amp;CTL excludes MC24 and contrastive loss from mCOLT. mCOLT w/o MC24 excludes MC24 from mCOLT.</figDesc><table><row><cell>model</cell><cell cols="3">CTL AA MC24 Supervised Unsupervised Zero-shot</cell></row><row><cell>1 m-Transformer</cell><cell>28.65</cell><cell>4.42</cell><cell>5.05</cell></row><row><cell>2 mCOLT w/o AA</cell><cell>28.79</cell><cell>4.75</cell><cell>13.55</cell></row><row><cell>3 mCOLT w/o MC24&amp;CTL</cell><cell>29.82</cell><cell>5.40</cell><cell>4.91</cell></row><row><cell>4 mCOLT w/o MC24</cell><cell>29.96</cell><cell>5.80</cell><cell>14.60</cell></row><row><cell>5 mCOLT</cell><cell>30.02</cell><cell>18.55</cell><cell>15.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>English-Centric: Sentence retrieval top-1 accuracy on Tatoeba evaluation set. The reported accuracy is the average of En→X and X→En accuracy.</figDesc><table><row><cell>that have</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Non-English: The averaged sentence similarity search top-1 accuracy on Ted-M testset. m-Transformer &lt; mCOLT w/o AA &lt; mCOLT, which is consistent with the results in English-centric scenario.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">All codes, data and models are released on https://github.com/PANXiao1994/mCOLT</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_1">x i is in language Li and x j is in language Lj, where i, j ∈ {L1, . . . , LM }</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_2">We will release our synonym dictionary</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_3"> Bg, Cs, De, El, En, Es, Et, Fi, Fr, Gu, Hi, It, Ja, Kk, Lt,  Lv, Ro, Ru, Sr, Tr, Zh, Nl, Pl, Pt   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_4">http://data.statmt.org/news-crawl/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_5">many-to-many Transformer trained on PC32 as in Johnson et al. (2017) except that we apply language indicator the same way as /method</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_6">http://phontron.com/data/ted talks.tar.gz</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.00089</idno>
		<title level="m">Massively multilingual neural machine translation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Maruan</forename><surname>Al</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Shedivat</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02338</idno>
		<title level="m">Consistency by agreement in zero-shot neural machine translation</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Simple, scalable adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08478</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A teacher-student framework for zeroresource neural machine translation</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.00753</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zeroresource neural machine translation with multiagent communication game</title>
		<author>
			<persName><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving a multi-source neural machine translation model with corpus extension for low-resource languages</title>
		<author>
			<persName><forename type="first">Gyu-Hyeon</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Kil</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08898</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multilingual and multi-domain adaptation for neural machine translation</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24st Annual Meeting of the Association for Natural Language Processing</title>
				<meeting>the 24st Annual Meeting of the Association for Natural Language Processing<address><addrLine>Okayama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="909" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Zeroresource neural machine translation with monolingual pivot data</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Currey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Neural Generation and Translation</title>
				<meeting>the 3rd Workshop on Neural Generation and Translation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="99" to="107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Enabling multi-source neural machine translation by concatenating source sentences in multiple languages</title>
		<author>
			<persName><forename type="first">Raj</forename><surname>Dabre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Cromieres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadao</forename><surname>Kurohashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06135</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-task learning for multiple language translation</title>
		<author>
			<persName><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dianhai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd ACL</title>
		<title level="s">Long Papers</title>
		<meeting>the 53rd ACL</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1723" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Training multilingual machine translation by alternately freezing language-specific encoders-decoders</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Escolano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Jussà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikel</forename><surname>José Ar Fonollosa</surname></persName>
		</author>
		<author>
			<persName><surname>Artetxe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01594</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Beyond english-centric multilingual machine translation</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11125</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Cert: Contrastive self-supervised learning for language understanding</title>
		<author>
			<persName><forename type="first">Hongchao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12766</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improved zero-shot neural machine translation via ignoring spurious correlations</title>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01181</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Effective strategies in zero-shot neural machine translation</title>
		<author>
			<persName><forename type="first">Thanh-Le</forename><surname>Ha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07893</idno>
		<imprint>
			<date type="published" when="2017-01">Jan Niehues, and Alexander Waibel. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-lingual pre-training based transfer for zero-shot neural machine translation</title>
		<author>
			<persName><forename type="first">Baijun</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00065</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Effective cross-lingual transfer of neural machine translation models without shared vocabularies</title>
		<author>
			<persName><forename type="first">Yunsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05475</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pretraining multilingual neural machine translation by leveraging alignment information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ba ; Zehui Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.210</idno>
		<idno>arxiv:1412.6980Comment</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
				<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014. 2015. 2020</date>
			<biblScope unit="page" from="2649" to="2663" />
		</imprint>
	</monogr>
	<note>the 3rd International Conference for Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A neural interlingua for multilingual machine translation</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Keung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-6309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers</title>
				<meeting>the Third Conference on Machine Translation: Research Papers<address><addrLine>Brussels; Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="92" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A call for clarity in reporting BLEU scores</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w18-6319</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Conference on Machine Translation: Research Papers, WMT 2018</title>
				<meeting>the Third Conference on Machine Translation: Research Papers, WMT 2018<address><addrLine>Belgium, Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 1, 2018</date>
			<biblScope unit="page" from="186" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<title level="s">Long Papers</title>
		<meeting>the 54th ACL<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Leveraging monolingual data with self-supervision for multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04816</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Multilingual neural machine translation with knowledge distillation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cross-lingual retrieval for iterative self-supervised training</title>
		<author>
			<persName><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1176</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACL</title>
				<meeting>the 57th ACL<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019a</date>
			<biblScope unit="page" from="1810" to="1822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A compact and language-sensitive multilingual translation method</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqing</forename><surname>Zong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th ACL</title>
				<meeting>the 57th ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019b</date>
			<biblScope unit="page" from="1213" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Clear: Contrastive learning for sentence representation</title>
		<author>
			<persName><forename type="first">Zhuofeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15466</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.148</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th ACL</title>
				<meeting>the 58th ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
				<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
