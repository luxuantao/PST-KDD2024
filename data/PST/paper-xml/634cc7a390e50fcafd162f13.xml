<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-14">14 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Keyu</forename><surname>Duan</surname></persName>
							<email>k.duan@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
							<email>peihaowang@utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
							<email>w.zheng@utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
							<email>kaixiong.zhou@rice.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
							<email>tianlong.chen@utexas.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xia.hu@rice.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-14">14 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.07494v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efficiency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-ofthe-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Graph Neural Networks (GNNs) have shown great prosperity in recent years <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref>, and have dominated a variety of applications, including recommender systems <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref>, social network analysis <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>, scientific topological structure prediction (e.g. cellular function prediction <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, molecular structure prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, and chemical compound retrieval <ref type="bibr" target="#b14">[15]</ref>), and scalable point cloud segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, etc. Although the message passing (MP) strategy provides GNNs' superior performance, the nature of evolving massive topological structures prevents MP-based GNNs <ref type="bibr">[18-20, 1, 2, 4, 21, 22]</ref> from scaling to industrial-grade graph applications. Specifically, as MP requires nodes aggregating information from their neighbors, the integral graph structures are inevitably preserved during forward and backward propagation, thus occupying considerable running memory and time. For example <ref type="bibr" target="#b5">[6]</ref>, training a GNN-based recommendation system over 7.5 billion items requires three days on a 16-GPU cluster (384 GB memory in total).</p><p>To facilitate understanding, a unified formulation of MP with K layers is presented as follows:</p><formula xml:id="formula_0">X (K) = A (K-1) ? A (K-2) ? ? ? ? ?(A (0) X (0) W (0) ) ? ? ? W (K-2) W (K-1) ,<label>(1)</label></formula><p>where ? is an activation function (e.g. ReLU) and A (l) is the weighted adjacency matrix at the l-th layer. As in Equ. <ref type="bibr" target="#b0">(1)</ref>, the key bottleneck of vanilla MP lies on A (l) X (l) . For the memory usage, the entire sparse adjacency matrix is supposed to be stored in one GPU. As the number of nodes grows, it is quite challenging for a single GPU to afford the message passing over the full graph.</p><p>36th Conference on Neural Information Processing Systems (NeurIPS 2022).</p><p>Up to now, massive efforts have been made to mitigate the aforementioned issue and scale up GNNs <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref>. Most of them focus on approximating the iterative full-batch MP to reduce the memory consumption for training within one single GPU. It is worth noting that we target at the algorithmic scope and do not extend to scalable infrastructure topics like distributed training with multiple GPUs <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and quantization <ref type="bibr" target="#b32">[33]</ref>. Briefly, the previous works encompass two branches: Sampling-based and Decoupling-based. Namely, the former methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> perform batch training that utilizes sampled subgraphs as a small batch to approximate the full-batch MP so that the memory consumption is considerably reduced. The latter follows the principle of performing propagation (A (l) X (l) ) and prediction (X (l) W (l) ) separately, either precomputing the propagation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31]</ref> or post-processing with label propagation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>. Despite the prosperity of scalable GNNs, there are still plights under-explored: we lack a systematic study of the reservoir from the perspective of effectiveness and efficiency, without which it is unachievable to tell the rationale of the designing philosophy for large-scale graph learning in practice.</p><p>To this end, we first establish a consistent benchmark and provide a systematic study for large-scale graph training for both Sampling-based methods (Sec. 2.1) and Decoupling-based methods (Sec. 2.2).</p><p>For each branch, we conduct a thorough investigation of the design strategy and implementation details of typical methods. Then, we carefully examine the sensitive hyperparameters and unify them in one "sweet spot" set by a linear greedy hyperparameter (HP) search (Sec. 3), i.e., iteratively searching the optimal value for an HP while fixing the others. For all concerned methods, the performance comparison is conducted on representative datasets of different scales, varying from about 80, 000 nodes to 2, 400, 000, including Flickr <ref type="bibr" target="#b23">[24]</ref>, Reddit <ref type="bibr" target="#b2">[3]</ref>, and ogbn-products <ref type="bibr" target="#b10">[11]</ref>. This step is a crucial precondition on our way to the ultimate as the configuration inconsistency significantly prohibits a fair comparison as well as the following analysis. Besides, regarding efficiency, we theoretically present the time and space complexities for the various branches, and empirically evaluate them on GPU memory usage, throughput, and convergence (Sec. 4). In addition to the benchmark, we further present a new ensembling training manner EnGCN (Sec. 5) to address the existing issues mentioned in our benchmark analysis (Sec. 5.1). Notably, via organically integrating with self-label-enhancement (SLE) <ref type="bibr" target="#b28">[29]</ref>, EnGCN achieves the new state-of-the-art (SOTA) on multiple large-scale datasets.</p><p>2 Formulations For Large-scale Graph Training Paradigms</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sampling-based Methods</head><p>Given the formulation of Equ. (1), sampling-based paradigm seeks the optimal way to perform batch training, such that each batch will meet the memory constraint of a single GPU for message passing.</p><p>For completeness, we restate the unified formulation of sampling-based methods as follows:</p><formula xml:id="formula_1">X (k) B0 = A (k-1) B1 ? A (k-2) B2 ? ? ? ? ?( A (0) B K X (0) B K W (0) ) ? ? ? W (K-2) W (K-1) ,<label>(2)</label></formula><p>where B l is the set of sampled nodes for the l-th layer, and A (l) is the adjacency matrix for the l-th layer sampled from the full graph. Given the local view of GNN -one node's representation is only related to its neighbors -a straightforward way for unbiased batch training would be B l+1 = N (B l ), where N denotes the set of neighbors. B 0 is randomly sampled according to the uniform distribution. Notably, this batch training style could achieve SOTA performance but also suffers from the "neighbor explosion" problem, where the time consumption and memory usage grow exponentially with the GNN depth, causing significant memory and time overhead. To mitigate this, a number of sampling-based methods were proposed. The key difference among them is how {B 0 , . . . , B K-1 , B K } are sampled. Given a large-scale graph G = (V, E), there are three categories of widely-used sampling strategies:</p><formula xml:id="formula_2">Node-wise Sampling [3] B l+1 = v?B l {u | u ? Q ? P N (v) }</formula><p>, where P is a sampling distribution; N (v) is the sampling space, i.e., the 1-hop neighbors of v; and Q denotes the number of samples. The representative node-wise sampling method is:</p><formula xml:id="formula_3">GraphSAGE [3]: In GraphSAGE, P is the uniform distribution.</formula><p>Compared with the aforementioned naive batch training, the node-wise sampling <ref type="bibr" target="#b2">[3]</ref> alleviates the "node explosion" problem by fixing the number of sampled neighbors Q for each node. It thus reduces the space complexity from D K to Q K , where D is the averaged node degree. However, as Q is not far less than D in order of magnitude, such mitigation is moderate, which is empirically validated by our empirical results in Sec. 3 and Sec. 4.</p><p>Layer-wise Sampling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>.</p><formula xml:id="formula_4">B l+1 = {u | u ? Q ? P N (B l ) }, where N (B l ) = v?B l N (v)</formula><p>denotes the union of 1-hop neighbors of all nodes in B l . We introduce a couple of layer-wise sampling methods as follows.</p><p>FastGCN <ref type="bibr" target="#b24">[25]</ref>: The sampling distribution P is designed regarding the node degree, where the probability for node u of being sampled is p(u) ? || ?(u, :)|| 2 .</p><p>LADIES <ref type="bibr" target="#b25">[26]</ref>: More recently, based on FastGCN, Zou et.al. <ref type="bibr" target="#b25">[26]</ref> propose LADIES that extends the sampling space from N (B l ) to N (B l ) ? B l by adding the self-loops.</p><p>Notably, Compared with the node-wise sampling, the layer-wise sampling essentially solves the "neighbor explosion" problem by fixing the number of overall sampled nodes in a layer to Q. However, the layer-wisely induced adjacency matrix is usually sparser than the others, which accounts for its sub-optimal performance in practice.</p><p>Subgraph-wise Sampling <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>.</p><formula xml:id="formula_5">B K = B K-1 = ? ? ? = B 0 = {u | u ? Q ? P G }.</formula><p>In the subgraph-wise sampling, all layers share the same subgraph induced from the entire graph G based on a specific sampling strategy P G , such that the sampled nodes are confined in the subgraph. Typically, this sampling strategy has two representative works:</p><p>ClusterGCN <ref type="bibr" target="#b22">[23]</ref>: ClusterGCN first partitions the entire graph into clusters based on some graph partition algorithms, e.g. METIS <ref type="bibr" target="#b38">[39]</ref>, and then select several clusters to form a batch.</p><p>GraphSAINT <ref type="bibr" target="#b23">[24]</ref>: GraphSAINT samples a subset of nodes based on sampling strategy P G and then induces the corresponding subgraph as a batch. The commonly-used sampling strategies include: (i) node sampler:</p><formula xml:id="formula_6">P(u) = || A :,u || 2 , (ii) edge sampler: P(u, v) = 1 deg(u) + 1 deg(v)</formula><p>, and (iii) random walk sampler. They are illustrated in Appendix A1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Decoupling-based Methods</head><p>Training GNNs with full-batch message passing at each epoch is not plausible. In this section, we summarize another line of scalable GNNs which decouple the message passing from GPU training to CPUs. Specifically, the message passing is conducted only once at CPUs accompanied by large accessible memory. Depending on the processing order, there are two typical ways to decouple these two operations: (i) pre-processing and (ii) post-processing.</p><p>Pre-processing: MP precomputating <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref>. Recalling Equ. <ref type="bibr" target="#b0">(1)</ref>, without loss of generalization, we assume that A (k-1) = A (k-2) = ? ? ? A (0) = A, i.e. the topological structure for the entire graph remains the same during forward propagation, meeting most of the cases. To decouple the two operations, message passing (AX) and feature transformation (XW), we can first pre-compute the propagated node representations and then train a neural network for the downstream task based on these fused representations:</p><formula xml:id="formula_7">X l = A l X precomputing , X = ?(X, X 1 , ? ? ? , X K ), Y = f ? ( X) end-to-end training on a GPU ,<label>(3)</label></formula><p>where X l can be regarded as the node representation aggregating l-hop neighborhood information; K is the largest propagation hop; ?(?) is a function that combines the aggregated features from different hops; and f ? (?) is a feature mapping function parameterized by ?. We summarize three existing pre-computing schemes as follows.</p><p>SGC <ref type="bibr" target="#b26">[27]</ref>: SGC leverages the node representations aggregated with k hops and feeds the resultant features to a full-connected layer. We can formulate this scheme by letting ?(?) select the last element X K and f ? (?) be a linear layer with readout activation: Y = ?(X K ?). SIGN <ref type="bibr" target="#b27">[28]</ref>: SIGN concatenates features from different hops and then fuse them as the final node representation via a linear layer. To be more specific, ?(?) is defined as</p><formula xml:id="formula_8">X = X X 1 ? ? ? X K ?,</formula><p>where ? is a transformation matrix, and f ? (?) is defined as a linear readout layer Y = ?( X?).</p><p>SAGN <ref type="bibr" target="#b28">[29]</ref>: SAGN adopts attention mechanism to combine feature representations from K hops: X = K l=1 T l X l , where T l is a diagonal matrix whose diagonal corresponds to the attention weight for each node of k-hop information. The attention weight for the i-th node is calculated by</p><formula xml:id="formula_9">T k i = softmax K (LeakyReLU(u T X i + v T X k j ))</formula><p>, where the subscripts slices the data matrices along the row. The feature mapping function is implemented by an MLP block with a skip connection to initial features: Y = MLP ? ( X + X? r ).</p><p>Post-processing: Label Propagation. The label propagation algorithms <ref type="bibr">[40-44, 38, 45]</ref> diffuse labels in the graph and make predictions based on the diffused labels. It is a classical family of graph algorithms for transductive learning, where the nodes for testing are used in the training procedure. The label propagation can be written in a unified form as follows:</p><formula xml:id="formula_10">Y (l) = ?AY (l-1) + (1 -?)G.<label>(4)</label></formula><p>The diffusion procedure iterates the formula above with l for multiple times to guarantee convergence. It requires two sets of inputs: (i) the stack of the label embeddings of all nodes, denoted as Y (0) ? R N ?c , where c is the number of classes. In our implementation, the Y (0) is the output of a trained MLP model <ref type="bibr" target="#b37">[38]</ref>. (ii) the diffusion embedding, denoted as G ? R N ?c that propagate themselves across the edges in the graph. Depending on how the diffusion embeddings of unlabeled nodes are computed, two types of G are summarized as follows:</p><formula xml:id="formula_11">Zeros [40]: G i,: = ?i,: -?AY (k) i,: , i ? T train 0,</formula><p>otherwise , where T train denotes the training set and ? is the stack of true labels. For zeros, Y (0) = G.</p><p>Residual <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_12">G i,: = ?i , v i ? T train ?i , otherwise</formula><p>, where ? = Z + ?. Z is the predictions of a trained simple neural network, e.g. MLP, and ? is an residual error matrix, which is optimized iteratively for multiple times by t) , where E = Z -? and E (0) = E.</p><formula xml:id="formula_13">E t+1 = (1 -?)E + ?AE (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">More Related Works</head><p>Model-agnostic Tricks. Besides the training methods as introduced above, there are some modelagnostic tricks that have been empirically confirmed to be effective for boosting large-scale graph training. Although those add-ons cannot be included into our benchmarking analysis, it is of equal importance to introduce them for completeness. Here we briefly introduce two representative ones:</p><p>Self-Label-Enhanced (SLE) <ref type="bibr" target="#b28">[29]</ref>: SLE includes two individual tricks, self training and label augmentation. Here we use T denoting the training set. For self training, the unlabeled nodes with high confidence (larger than a pre-defined threshold) are added to T after a certain number of training epochs. For label augmentation, it trains an additional model ?(?). The forward propagation can be formulated as out = ?( ?k Y T ). out is added to the main model to make the final prediction.</p><p>GIANT <ref type="bibr" target="#b45">[46]</ref>: In general, node features are usually pre-embed with graph-agnostic language models, such as word2vec <ref type="bibr" target="#b46">[47]</ref> and BERT <ref type="bibr" target="#b47">[48]</ref>. Recently, Chien et.al. propose a graph-related node feature extraction framework (GIANT), which embeds the raw texts to numerical features by taking advantage of graph structures, to help boost the performance of GNNs for the downstream tasks.</p><p>Memory-based GNN Training. Focus on mitigating the "Neighbor Explosion" problem of fullbatch training as introduced, memory-based GNNs <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b49">50]</ref> try to save the GPU memory with different techniques while including all neighbor nodes into computing during the message passing. GAS <ref type="bibr" target="#b48">[49]</ref> incorporates historical embeddings <ref type="bibr" target="#b33">[34]</ref> to provably maintain the expressive power of full-batch GNN. VQ-GNN <ref type="bibr" target="#b49">[50]</ref> utilizes vector quantization to scale convolutional-based GNN and resemble the performance of full-batch message passing by learning an additional quantized feature matrix and a corresponding low-rank adjacent matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Benchmarking Over Effectiveness</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation Details</head><p>We test numerous large-scale graph training methods with a greedy hyperparameter (HP) search to find their sweet spot and the best performance for a fair comparison. The search space is defined in Table <ref type="table" target="#tab_0">1</ref>. The access and statistics of all used datasets are introduced in Appendix A3.1. Particularly, for label propagation, we select two representative algorithms: Huang et.al. <ref type="bibr" target="#b37">[38]</ref>, the residual diffusion type, and Zhu et.al. <ref type="bibr" target="#b39">[40]</ref>, the zeros type. The number of propagation is the maximum iteration k. The aggregation ratio is ? as in Equ. ( <ref type="formula" target="#formula_10">4</ref>), and the number of MLP layers is the number of MLP layers that precedes the label propagation module following Huang et.al. <ref type="bibr" target="#b37">[38]</ref>. Limited by space, we select five representative approaches that covers all branches as we introduced, including GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, LADIES <ref type="bibr" target="#b25">[26]</ref>, Clus-terGCN <ref type="bibr" target="#b22">[23]</ref>, SAGN <ref type="bibr" target="#b28">[29]</ref>, and C&amp;S <ref type="bibr" target="#b37">[38]</ref>. We illustrate the selected results in Fig. <ref type="figure">1</ref> and the results of other methods in Fig. <ref type="figure">A5</ref>. For each subplot, from left to right, each column denotes the search results for one HP. Once one HP was searched, its value will be fixed to the best results for the rest HP searching. Iteratively, we obtain the best performance in the last column. For convenience and clarity, we list the searched optimal hyperparameter settings of all test methods in Table <ref type="table">A5</ref>. Regarding the space complexity, we need to store the activations of each layer in memory, which has a O(bLD) space complexity. Note that we ignore the memory usage of model weights and the optimizer here since they are negligible compared to the activations. For decoupling-based methods, the training paradigm is simplified as MLPs, and thus the complexity is the same as the traditional mini-batch training. We do not include label propagation in our analysis since it can be trained totally on CPUs. Here "Throughput" measures how many times can we complete the training steps within a second. Note that we omit the label propagation methods since it is not trained by backward propagation. We provide our implementation details for computing the throughput and memory usage in section A3.2. We report the hardware throughput and activation usage in Table <ref type="table">3</ref>. We summarize three main observations. Table <ref type="table">3</ref>: The memory usage of activations and the hardware throughput (higher is better). The hardware here is an RTX 3090 GPU. Obs. 4. GraphSAGE is significantly slower and occupies more memory compared to other baselines. This is partially because of the large neighbor sampling threshold we set and inherently owing to its neighborhood explosion. Namely, to compute the loss for a single node, it requires the neighbors' embeddings at the down-streaming layer recursively. Please refer to Sec. 2.1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Observations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Throughput And Memory Usage</head><p>Obs. 5. SGC does not occupy any activation memory. As shown in Table <ref type="table">3</ref>, SGC only occupies about 0.01 MB actual memory during training. This is because SGC only has one linear layer and the activation is exactly the input feature matrix, which has been stored in memory. Thus, it is not accounted towards the activation memory.</p><p>Obs. 6. In general, the speed of decoupling-based methods is comparable to sampling-based methods. Besides the scale of sparse adjacency matrix, the feature set size is also crucial for occupying memory. Although precomputing-based methods avoid storing the graph structures in a GPU, they may take advantage of multi-hop features, where the corresponding memory is multiplied many times. For convergence analysis, we test all benchmarked methods on Flickr, Reddit, and ogbn-products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Convergence Analysis</head><p>The training loss and validation accuracy (val_acc) are shown in Fig. <ref type="figure" target="#fig_0">2</ref>. Based on the empirical results, we summarize the main observation as follows:</p><p>Obs. 7. In general, precomputing-based methods have faster and more stable convergence than sampling-based methods. This is because sampling-based methods usually incur a variance among batches that poses unstable and slow convergence <ref type="bibr" target="#b34">[35]</ref>. However, precomputing-based methods mitigate this by moving message passing from backward propagation to the precomputing stage.</p><p>5 EnGCN: Rethinking Graph Convolutional Networks With Ensembling</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">An Empirical Summary: Pros And Cons</head><p>Based on our benchmark results in section 3 and 4, we summarize the advantages (marked as Pros) and constraints (marked as Cons) for different branches as follows. Besides the summary, we also provide a joint comparison of effectiveness and efficiency for various methods in Appendix A2.2.</p><p>Sampling-based: (Pros) Sampling subgraphs into GPU training allows them taking advantage of numerous graph convolution layers, such as GCN <ref type="bibr" target="#b0">[1]</ref>, GraphSAGE <ref type="bibr" target="#b2">[3]</ref>, and GIN <ref type="bibr" target="#b3">[4]</ref>, and this is flexible for them to design specific architecture for different downstream tasks, e.g. node classification and graph classification. (Cons) As aforementioned, sampling-based methods suffers from link sparsity <ref type="bibr" target="#b23">[24]</ref> (section 3) and unstable and low convergence (section 4) problems, both of which prevent sampling-based methods from achieving SOTA performance.</p><p>Precomputing-based: (Pros) Decoupling message passing from GPU training to CPU precomputing allows precomputing-based methods utilize a mixture of feature transformation units (e.g. attention mechanism and MLP) to train in a well-studied manner. This guarantees stable and fast convergence (section 4). Particularly, integrating with the add-ons like SLE <ref type="bibr" target="#b28">[29]</ref> and GIANT <ref type="bibr" target="#b45">[46]</ref>, precomputingbased methods achieve SOTA performance on large-scale open graph benchmark (ogb) <ref type="bibr" target="#b10">[11]</ref> datasets. (Cons) In general, precomputing-based methods at least occupy a CPU memory space of O(LN d), where L is the number of layers; N is the number of nodes; and d is the dimension of input features. In comparison, it is L times as large as the others, which is not affordable for extremely large-scale graphs. For example, containing about 111 million nodes, the largest ogb dataset, ogbn-papers100M, requires approximately 57 Gigabytes (GB) to store the initial feature matrix, given the data type is float and the dimension of features is 128. As the number of layers increases, the required CPU memory space will grow proportionally to an unaffordable number.</p><p>Label Propagation: (Pros) As a traditional branch of graph learning algorithm, label propagation is a simple but effective add-on as a post-processing trick nowadays. Because of its mode-agnostic nature, it can be simply attached to the end of any graph representation learning algorithm to boost the final prediction. (Cons) Label propagation has many additional sensitive hyperparameters as we introduced in Table <ref type="table" target="#tab_0">1</ref> and is specifically designed for the node classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Motivation and Related Works</head><p>To address the above constraints of sampling-based methods and precomputing-based methods, let us first recap the full-batch message passing in Equ. <ref type="bibr" target="#b0">(1)</ref>. We reformulate it into a more general form:</p><formula xml:id="formula_14">X (k) = ? (k-1) A? (k-2) ? ? ? A? (0) (AX (0) ) ,</formula><p>where ? (i) denotes the feature mapping model for the i-th layer. To make the message passing scalable, following the rationale of decoupling, we propose a different training scheme from precomputing: Instead of end-to-end training, we sequentially train the ?s in a layer-wise manner. In this way, no precomputing is required and thus the corresponding constraint of CPU memory occupation is essentially mitigated. To elaborate on this, we present the layer-wise training manner:</p><formula xml:id="formula_15">X (l) = AX (l-1) Message passing on CPUs , Z (l) = ? (l) (X (l) ) forword propagation , ?? (l) = ?L(Z (l) , Y) backward propagtion .<label>(5)</label></formula><p>From layer 0 to k, we do message passing once and then train ? (l) in batches for epochs. Finally, one can simply use the output of model ? as the prediction. Besides, from the perspective of ensembling, the models ? can be naturally viewed as a set of weak learners trained on multiple views of the input X. As a result, it is compatible to use ensembling to boost the final predictions, such as majority voting. In addition, Based on our empirical results, this training manner is capable of achieving SOTA methods on relatively small datasets without exhaustive finetuning. To further boost the performance, we organically integrate SLE, which has achieved new SOTA performance on several representative datasets. We name this model EnGCN (Ensembling GCN).</p><p>Related Works. Interestingly, the layer-wise training manner and majority voting are naturally consistent with the boosting algorithms, where we sequentially train weak learners with instance reweighting and make the final prediction by majority voting. In the scope of graph representation learning, AdaGCN <ref type="bibr" target="#b55">[56]</ref> first applies adaboosting <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58]</ref> to address the oversmoothing problem of deep GCNs. Though focusing on different topics, AdaGCN has a similar training scheme as ours. Therefore, we implement a scalable version for it, which is included as a SOTA baseline in our experiment. In addition, AdaClusterGCN <ref type="bibr" target="#b58">[59]</ref> proposed an adaboosting application that ensembles weak learners trained on different clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Methodology</head><p>Considering a large-scale graph G = (A, X, y), where A is the adjacent matrix, X is the node features, and y is the true labels. Respectively, T train , T val and T test denotes the training, validation, and test set. Let X (l) and Y (l) denotes the embeddings of node features and labels at the l-th layer, respectively. We use ?(l) and T (l)</p><p>train denoting the pseudo labels, pseudo training set for self training at layer l.</p><p>Initialization. we initialise several important matrices and vectors: Pre-processing. For pre-processing, we precompute X (l) and Y (l) in CPUs as follows:</p><formula xml:id="formula_16">X (0) = X, Y (0) i,: = one_hot(y i ), i ? T train 0, otherwise , T<label>(0)</label></formula><formula xml:id="formula_17">train = T train , ?<label>(0)</label></formula><formula xml:id="formula_18">X (l) = ?X (l-1) , Y (l) = ?Y (l-1) ,<label>(6)</label></formula><p>where ? is symmetrically normalized <ref type="bibr" target="#b0">[1]</ref>. Note that pre-processing is skipped when l = 0.</p><p>Training. We solely train two simple models till convergence, which empirically takes dozens of epochs on real-world datasets. The forward propagation is:</p><formula xml:id="formula_19">out (l) = ?(X (l) , Y (l) ) = ?(X (l) ) + ?(Y (l) ),<label>(7)</label></formula><p>where ? and ? are two MLP models that are shared through all layers. Specifically, when l = 0, the forward propagation is reduced to out (0) = ?(X (0) ) where ? is not evolved. This is because the initialized Y (0) contains many zero vectors and will pose the overfitting problem. For backward propagation, we compute the training loss using the pseudo labels ?(l) instead of y (l) .</p><p>post-processing. After obtaining the trained models, we save the state of them as ? (l) = (? (l) , ? (l) ) for ensembling. Furthermore, self training is used to enhance the training set. Following Sun et.al. <ref type="bibr" target="#b28">[29]</ref>, the pseudo labels and pseudo training masks are updated as follows.</p><formula xml:id="formula_20">T (l+1) train = T (l) train ?{i | max c (? (out (l) i )) ? ?}, ?<label>(l+1)</label></formula><formula xml:id="formula_21">i = y i , i ? T train c, else if max c (? (out (l) i )) ? ? ,<label>(8)</label></formula><p>where ? is the softmax function.</p><p>Inference With Majority Voting. After k layers, we have obtained a series of weak learners {? (l) | 0 ? l ? k}. The final prediction of node n is made by weighted majority voting <ref type="bibr" target="#b57">[58]</ref>:</p><formula xml:id="formula_22">?n = argmax c k l=0 z (l) n - 1 d d i=1 (z (l) n,i ) ,<label>(9)</label></formula><p>where z (l) = log_sof tmax(out</p><formula xml:id="formula_23">(l) n ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Empirical Analysis</head><p>Experiment Settings. Consistent with our effectiveness benchmark, we test our proposed EnGCN on Flickr, Reddit, and ogbn-products. A similar hyperparameter (HP) search was conducted to find its suitable HP setting. The search space is provided in Appendix A2.3. For the baselines, we directly use all benchmark results from section 3, where the SOTA performance has been achieved. The Training Efficiency and Convergence Landscape of EnGCN. For EnGCN, since all we need to train is two simple shallow MLPs (Section 5.3), the GPU throughput and memory consumption are expected to be sufficiently efficient. The remained concern is solely about the convergence of EnGCN. Due to the nature of layer-wise training, the convergence of EnGCN is more complicated than other end-to-end training methods. In Figure <ref type="figure">3</ref>, we show the convergence landscape of EnGCN and provide several interesting observations as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Experiment. As shown in</head><p>As shown in Figure <ref type="figure">3</ref>, the train accuracy and validation accuracy generally increase layer-wisely till convergence. Noticeably, though the training accuracy occasionally drops, the validation accuracy still relatively remains positive. At the beginning of each layer, the accuracy changes rapidly, indicating the remarkable distribution difference for various hops. Different datasets are sensitive to different hops. For example, the 2-nd hop is crucial to boost the training and validation accuracy on Flickr, while for Reddit and ogbn-products, 1-hop neighbors are more important. The CPU memory consumption of EnGCN. To confirm the low CPU memory consumption of En-GCN, we provide a comparison experiment and illustrate the results in Figure <ref type="figure">4</ref>. The x-axis denotes the models' number of layers while the y-axis records their allocated memories that are reported by "aten::empty" of PyTorch. As shown in Figure <ref type="figure">4</ref>, the precomputing-based methods, SIGN and SAGN, suffer from expensive CPU memory consumption as the model depth increases. For sampling-based methods, since there is no need to pre-store a large number of feature matrices, the memory consumption increases much more smoothly. For EnGCN, as no precomputing is required, the CPU memory consumption is considerably reduced in comparison with SIGN and SAGN, which intuitively validates the CPU memory efficiency of EnGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The scalability issue of graph convolutional networks has been a notoriously challenging research problem. In this work, we establish a fair and consistent benchmark for large-scale graph training w.r.t effectiveness and efficiency. We provide a unified formulation for dozens of works and further assess them on the basis of accuracy, memory usage, throughput, and convergence. Furthermore, provided with the comprehensive benchmark results, we rethink the scalability issue of GCNs from the perspective of ensembling and then present an ensembling-based trainer scheme (EnGCN) that solely needs to train a couple of simple MLPs to achieve new SOTA on multi-scale large datasets. We hope our study on benchmarking and rethinking to help lay a solid, practical, and systematic foundation for the scalable GCN community and provide researchers with broader and deeper insights into large-scale graph training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 More details of Formulations A1.1 Representative Subgraph Sampling Schemes</head><p>Node Sampler <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>: P(u) = || A :,u || 2 , where all nodes are sampled independently based on the normalized distribution of P. This sampling strategy is logically equivalent to layer-wise sampling <ref type="bibr" target="#b24">[25]</ref>.</p><p>Edge Sampler <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_24">P(u, v) = 1 deg(u) + 1 deg(v)</formula><p>, where all edges are sampled independently based the edge distribution above. In our implementation, we utilize the sampled nodes (once contained in the sampled edges) to induce the subgraph as input, which should include more edges to help boost the performance.</p><p>Random Walk Sampler <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b23">24]</ref>: Here, we first sample a subset of root nodes uniformly, based on which we perform a random walk at a certain length to obtain the subgraph as a batch.</p><p>Graph Partitioner <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>: We first partition the entire graph into clusters with graph clustering algorithms and then select multiple clusters to form a batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 A Joint Comparison of Effectiveness and Efficiency</head><p>To further facilitate a comprehensive understanding of the benchmark results, we provide an illustration in Figure <ref type="figure" target="#fig_3">A6</ref> to jointly compare the effectiveness and efficiency of the methods. An empirical summary could be found in Section 5.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3 The Hyperparameter Settings for EnGCN</head><p>The searched HPs for EnGCN includes learning rate (0.01, 0.001, 0.0001), weight decay (0, 1e-5, 1e-4), dropout (0.2, 0.5, 0.7), epochs <ref type="bibr" target="#b29">(30,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr">70)</ref>, hidden dimension (128, 256, 512), batch size (5000, 10000), batch norm (True, False), self learning threshold (?=0.8, 0.9, 0.95), and number of layers <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8)</ref>. The searching results are shown in Figure <ref type="figure" target="#fig_4">A7</ref>. The searched HPs that produce the reported results on Flickr, Reddit, and ogbn-products are shown in Table <ref type="table">A6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4 Ablation Study for EnGCN</head><p>Ablating Ensembling. Here we provide an ablation study to confirm the effectiveness of ensembling (inference with majority voting). For ablated models, we directly use the ones after l-hop training, 0 ? l ? 3. The experiment results are shown in Table <ref type="table" target="#tab_7">A7</ref>. Notably, with majority voting, the performance is boosted by a large margin on Flickr and also has noticeable improvement on Reddit and ogbn-products. Besides, we find that the test accuracy of EnGCN after l-hop training keeps increasing as l grows. This phenomenon is consistent with the empirical results in AdaGCN <ref type="bibr" target="#b55">[56]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The empirical results of convergence for sampling-based methods (real line) and precomputing-based methods (dash line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Training. From 0 to k, we follow a layer-wise training manner, where each training stage contains three phases: pre-processing, training, and post-processing. For layer l, the three phases are introduced as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: The convergence landscape of EnGCN. All models are trained with 4 layers' features. For each layer-wise phase, we train the model with 70 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure A6 :</head><label>A6</label><figDesc>Figure A6: The joint comparison of effectiveness (accuracy) and efficiency (throughput) for samplingbased and precomputing-based methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure A7 :</head><label>A7</label><figDesc>Figure A7: The hyperparameter searching results of EnGCN.Table A6: The searched optimal hyperparameters for EnGCN on Flickr, Reddit, and ogbn-products Datasets Searched HPs Flickr lr 0.0001 weight decay 0.0001 dropout 0.2 epoch 70 hidden dimension 256 number of layers 4 batch size 10000 ? 0.9 Reddit lr 0.001 weight decay 0 dropout 0.2 epochs 70 hidden dimension 512 number of layers 4 batch size 5000 ? 0.95 ogbn-products lr 0.01 weight decay 0 dropout 0.2 epochs 70 hidden dimension 512 number of layers 8 batch size 10000 ? 0.8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The search space of hyperparameters for benchmarked methods. marks the default value a we do not search batch size for precomputing based methods since they do not follow a sample-training style.</figDesc><table><row><cell>Category</cell><cell>Hyperparameter (Abbr.)</cell><cell>Candidates</cell></row><row><cell></cell><cell>Learning rate (LR)</cell><cell>{1e -2  *  , 1e -3, 1e -4}</cell></row><row><cell></cell><cell>Weight Decay (WD)</cell><cell>{1e -4  *  , 2e -4, 4e -4}</cell></row><row><cell>Sampling &amp; Precomputing</cell><cell>Dropout Rate (DP) Training Epochs (#E) Hidden Dimension (HD)</cell><cell>{0.1, 0.2  *  , 0.5, 0.7} {20, 30, 40, 50  *  } 128  *  , 256, 512</cell></row><row><cell></cell><cell># layers (#L)</cell><cell>{2  *  , 4, 6}</cell></row><row><cell></cell><cell>Batch size a (BS)</cell><cell>{1000  *  , 2000, 5000}</cell></row><row><cell></cell><cell>Diffusion Type (DT)</cell><cell>{ residual  *  , zeros }</cell></row><row><cell></cell><cell># Propagations (#Prop)</cell><cell>{ 2, 20  *  , 50 }</cell></row><row><cell>LP</cell><cell>Aggregation Ratio (AR) Adj. Norm (Adj.)</cell><cell>{ 0.5, 0.75  *  , 0.9, 0.99 } { D -1 A, AD -1 , D -1/2 AD -1/2 *  }</cell></row><row><cell></cell><cell>Auto Scale (AS)</cell><cell>{ True  *  , False }</cell></row><row><cell></cell><cell># MLP Layers (#ML)</cell><cell>{ 2  *  , 3, 4 }</cell></row><row><cell></cell><cell></cell><cell></cell></row></table><note><p>*  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>According to the results of the last column of all sampling-based methods, the performance of the layer-wise and subgraph-wise sampling methods is roughly proportional to the batch size. Expectedly, the model performance could further increase as the batch size grows till the upper bound of full-batch training because more links can be preserved. Particularly, in our experiment, we set the number of sampled neighbors of node-wise sampling to a large threshold such that the performance of GraphSAGE can be regarded as full-batch training's. It can be easily found that the performance of sampling-based methods is inferior to full-batching training (GraphSAGE), further proving our conjecture that the missing links by sampling are non-trivial.Obs. 3. Precomputing-based methods generally perform better on larger datasets. As show in Fig.1and Fig.A5, C&amp;S (label propagation) outperforms the full-batch training (GraphSAGE as introduced in Obs. 2) on the largest dataset ogb-products by a large margin of 4.5%, although both two branches have on-par performance on smaller datasets. Remarkably, our searched results for GraphSAGE and LP on ogbn-products also reached better performance, compared with the ones on the OGB leaderboard1 . Noticing that GraphSAGE encounters the out-of-memory (OOM) 2 runtime error with increasing depth, the observation partially indicates that, limited by model depth and neighbor explosion problem, it is possibly not powerful for extremely large-scale graphs to learn expressive representations.In this section, we present another benchmark regarding the efficiency of scalable graph training methods. Firstly, we briefly summarize a general complexity analysis in Table2. For sampling-based methods, we note that the time complexity is for training GNNs by iterating over the whole graph. The time complexity O(L||A|| 0 D + LN D 2 ) consists of two parts. The first part L||A|| 0 D is from the Sparse-Dense Matrix Multiplication,i.e., AX. The second part LN D 2 is from the normal Dense-Dense Matrix Multiplication, i.e., (AX)W.</figDesc><table><row><cell cols="6">4 Benchmarking Over Efficiency</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">4.1 Time And Space Complexity</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>GraphSAGE</cell><cell></cell><cell>LADIES</cell><cell></cell><cell>ClusterGCN</cell><cell></cell><cell>SAGN</cell><cell></cell><cell></cell><cell>LP</cell></row><row><cell></cell><cell>48.13 51.73 51.58 51.02 51.79 52.26</cell><cell></cell><cell>48.63 48.85 49.75 49.16 49.75 49.87 49.87</cell><cell></cell><cell>50.74 50.99 50.81 50.98 50.79 50.77 50.81</cell><cell></cell><cell cols="2">48.43 48.37 48.44 48.28 48.55 49.11</cell><cell></cell><cell>47.63 45.63 45.12 49.82 51.24 47.40</cell></row><row><cell>Flickr</cell><cell>51.51 51.68 51.73 51.37 52.11 53.63 51.69 51.73 51.82 51.71 52.31 53.39</cell><cell>Flickr</cell><cell>48.81 49.23 49.23 49.16 49.66 48.03 50.09 46.42 49.04 49.54 49.26 49.87 44.46 50.51</cell><cell>Flickr</cell><cell>51.00 51.07 50.94 50.97 51.01 50.91 51.00 50.79 50.75 50.76 50.93 50.84 50.66 51.20</cell><cell>Flickr</cell><cell cols="2">45.76 48.34 48.44 48.37 49.75 49.75 42.68 48.22 48.44 47.50 49.63 49.96</cell><cell>Flickr</cell><cell>51.12 44.24 44.53 46.26 51.24 51.24 51.12 49.90 47.11 50.68</cell></row><row><cell></cell><cell>51.63 51.77</cell><cell></cell><cell>48.48 49.75</cell><cell></cell><cell>50.90 50.84</cell><cell></cell><cell>48.44</cell><cell>50.07</cell><cell></cell><cell>51.24</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell></cell><cell>LADIES</cell><cell></cell><cell>ClusterGCN</cell><cell></cell><cell>SAGN</cell><cell></cell><cell></cell><cell>LP</cell></row><row><cell></cell><cell>95.31 96.19 96.17 95.67 96.21 96.34</cell><cell></cell><cell>83.37 83.64 83.55 83.09 83.64 84.12 84.12</cell><cell></cell><cell>92.58 95.12 95.01 94.68 95.53 95.54 95.62</cell><cell></cell><cell cols="2">23.81 77.86 96.28 96.00 96.31 96.09</cell><cell></cell><cell>93.06 93.77 93.62 94.26 89.92 95.30</cell></row><row><cell>Reddit</cell><cell>96.20 96.18 96.21 95.98 96.28 96.50 96.21 96.15 96.20 96.16 96.34 96.40</cell><cell>Reddit</cell><cell>82.91 83.24 83.64 83.22 83.18 51.02 85.15 51.45 83.20 83.31 83.48 84.12 10.93 86.96</cell><cell>Reddit</cell><cell>95.00 95.11 95.12 95.30 95.55 95.63 95.68 95.12 95.10 95.30 95.46 95.50 94.99 95.37</cell><cell>Reddit</cell><cell cols="2">93.63 96.08 96.15 96.31 95.79 96.31 80.68 95.89 96.25 95.85 96.24 96.48</cell><cell>Reddit</cell><cell>93.62 93.62 93.62 93.06 94.24 92.91 93.06 94.09 95.30 95.33</cell></row><row><cell></cell><cell>95.99 96.21</cell><cell></cell><cell>83.25 83.64</cell><cell></cell><cell>95.17 95.53</cell><cell></cell><cell>96.28</cell><cell>96.41</cell><cell></cell><cell>94.06</cell></row><row><cell></cell><cell>GraphSAGE</cell><cell></cell><cell>LADIES</cell><cell></cell><cell>ClusterGCN</cell><cell></cell><cell>SAGN</cell><cell></cell><cell></cell></row><row><cell></cell><cell>75.01 77.26 77.02 76.54 77.41 78.20</cell><cell></cell><cell>72.42 72.04 72.34 71.57 72.69 73.31 73.31</cell><cell></cell><cell>65.99 75.47 75.04 75.71 76.19 76.44 76.41</cell><cell></cell><cell cols="2">80.68 81.18 81.18 80.96 80.00 81.02</cell><cell></cell><cell>71.06 73.25 78.90 84.30 84.30 84.33</cell></row><row><cell>Products</cell><cell>77.18 76.99 77.27 77.11 78.05 80.61 73.95 76.24 77.31 77.34 78.06 OOM</cell><cell>Products</cell><cell>70.89 72.32 72.42 72.69 73.31 68.80 74.34 62.14 72.06 70.47 72.69 72.59 61.86 75.31</cell><cell>Products</cell><cell>75.40 75.41 75.62 75.70 75.49 78.67 78.62 74.28 74.83 75.39 76.43 76.18 78.20 78.26</cell><cell>Products</cell><cell cols="2">81.11 79.05 81.17 81.06 81.19 81.03 76.52 78.22 81.14 81.22 81.17 81.21</cell><cell>Products</cell><cell>73.06 75.12 81.06 82.92 81.11 85.11 69.24 83.62 83.62 84.31</cell></row><row><cell></cell><cell>76.68 77.32</cell><cell></cell><cell>68.22 72.42</cell><cell></cell><cell>75.16 76.11</cell><cell></cell><cell></cell><cell>81.11</cell><cell></cell><cell>79.10</cell></row><row><cell></cell><cell>LR WD DP #E HD #L</cell><cell></cell><cell>LR WD DP #E HD #L BS</cell><cell></cell><cell>LR WD DP #E HD #L BS</cell><cell></cell><cell cols="2">LR WD #E HD #L DP</cell><cell></cell><cell>DT #Prop AR Adj AS #ML</cell></row></table><note><p><p><p><p><p><p><p><p><p><p><p><p>LP</p>Figure</p>1</p>: The greedy hyperparameter searching results for selected representative methods. The x-axis denotes the searched HPs, where the abbreviations are consistent with Table</p>1</p>.</p>Obs. 1. Sampling-based methods are more sensitive to the hyperparameters related to MP. According to Fig.</p>1</p>, in comparison with precomputing, all sampling-based methods are non-sensitive to hyperparameters (HPs) that are related to the feature transformation matrices, including weight decay, dropout, and hidden dimension; but particularly sensitive to the MP-related HPs, including the number of layers and batch size. For model depth, sampling-based methods generally achieve the sweet spots when the number of layers is confined to shallow and suffers from the oversmoothing problem</p><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref><ref type="bibr" target="#b52">[53]</ref><ref type="bibr" target="#b53">[54]</ref><ref type="bibr" target="#b54">[55]</ref> </p>as the GNN models go deeper. However, this issue is moderately mitigated in decoupling-based methods as the model depth does not align with the number of MP hop.</p>Obs. 2. Sampling-based methods' performance is nearly positive-correlated with the training batch size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The time and space complexity for training GNNs with sampling-based and decoupling-based methods, where b is the averaged number of nodes in the sampled subgraph and r is the averaged number of neighbors of each node. Here we do not consider the complexity of pre-processing sice it can be done in CPUs.</figDesc><table><row><cell>Implementation Details To fairly</cell><cell></cell><cell></cell><cell></cell></row><row><cell>benchmark the training speed and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>memory usage for large-scale graph</cell><cell></cell><cell></cell><cell></cell></row><row><cell>training methods, we empirically eval-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>uate the throughputs and actual mem-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ory for various methods during the</cell><cell></cell><cell></cell><cell></cell></row><row><cell>training procedure.</cell><cell>Category</cell><cell>Time Complexity</cell><cell>Space Complexity</cell></row><row><cell></cell><cell>Node-wise Sampling [3]</cell><cell>O(r L N D 2 )</cell><cell>O(br L D)</cell></row><row><cell></cell><cell>Layer-wise Sampling [28, 26]</cell><cell>O(rLN D 2 )</cell><cell>O(brLD)</cell></row><row><cell></cell><cell cols="2">Subgraph-wise Sampling [23, 24] O(L||A|| 0 D + LN D 2 )</cell><cell>O(bLD)</cell></row><row><cell></cell><cell>Precomputing [27-29]</cell><cell>O(LN D 2 )</cell><cell>O(bLD)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 ,</head><label>4</label><figDesc>EnGCN outperforms the sampling-based and decouplingbased methods on multi-scale datasets. For Flickr and Reddit, EnGCN outperforms the baselines by a large margin. Remarkably, EnGCN has achieved new SOTA performance on ogbn-products, outperforming C&amp;S by 2.88% and the SOTA model (GIANT-XRT+SAGN+MCR+C&amp;S) in the ogb leaderboard by 1.26%. In addition to the comparison experiment, we also conduct a couple of ablation studies to provide more insights into EnGCN in Appendix A2.4.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The comparison experiment results on Flickr, Reddit, and ogbn-products</figDesc><table><row><cell>Category</cell><cell>Baselines</cell><cell>Flickr</cell><cell>Reddit</cell><cell>ogbn-products</cell></row><row><cell></cell><cell>GraphSAGE [3]</cell><cell cols="3">53.63 ? 0.13% 96.50 ? 0.03% 80.61 ? 0.16%</cell></row><row><cell></cell><cell>FastGCN [25]</cell><cell cols="3">50.51 ? 0.13% 79.50 ? 1.22% 73.46 ? 0.20%</cell></row><row><cell>Sampling-based</cell><cell>LADIES [26]</cell><cell cols="3">50.51 ? 0.13% 86.96 ? 0.37% 75.31 ? 0.56%</cell></row><row><cell></cell><cell>ClusterGCN [23]</cell><cell cols="3">51.20 ? 0.13% 95.68 ? 0.03% 78.62 ? 0.61%</cell></row><row><cell></cell><cell>GraphSAINT [24]</cell><cell cols="3">51.81 ? 0.17% 95.62 ? 0.05% 75.36 ? 0.34%</cell></row><row><cell></cell><cell>SGC [27]</cell><cell cols="3">50.35 ? 0.05% 93.51 ? 0.04% 67.48 ? 0.11%</cell></row><row><cell></cell><cell>SIGN [28]</cell><cell cols="3">51.60 ? 0.11% 95.95 ? 0.02% 76.85 ? 0.56%</cell></row><row><cell>Decoupling-based</cell><cell>SAGN [29]</cell><cell cols="3">50.07 ? 0.11% 96.48 ? 0.03% 81.21 ? 0.07%</cell></row><row><cell></cell><cell>GAMLP [30]</cell><cell cols="3">52.58 ? 0.12% 96.73 ? 0.03% 83,76 ? 0.19%</cell></row><row><cell></cell><cell>C&amp;S [38]</cell><cell cols="3">51.24 ? 0.17% 95.33 ? 0.08% 85.11 ? 0.07%</cell></row><row><cell></cell><cell>AdaGCN [56]</cell><cell cols="3">52.97 ? 0.01% 96.05 ? 0.00% 76.41 ? 0.00%</cell></row><row><cell>Other SOTA Methods</cell><cell>SAGN+SLE [29]  *</cell><cell cols="3">54.60 ? 0.40% 97.10 ? 0.00% 84.28 ? 0.14%</cell></row><row><cell></cell><cell>GIANT-XRT+ SAGN+MCR+C&amp;S [60]  *</cell><cell>-</cell><cell>-</cell><cell>86.73 ? 0.08%</cell></row><row><cell>Ours</cell><cell>EnGCN</cell><cell cols="3">56.43 ? 0.21% 97.14 ? 0.03% 87.99 ? 0.04%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>* : the results are from the original papers</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>LR WD DP EP HD BN BS TH #L55.<ref type="bibr" target="#b19">20</ref> 56.25 56.36 55.87 56.63 56.58 56.63 56.63 56.50 54.84 56.25 56.38 56.11 56.19 56.63 56.70 56.92 56.92 56.25 56.27 56.27 56.38 56.63 56.70 57.40 Flickr LR WD DP EP HD BN BS TH #L 96.78 96.83 96.87 96.79 96.92 97.27 97.27 97.22 97.13 96.83 96.78 96.89 96.75 96.53 97.27 97.27 97.39 97.39 96.24 96.72 96.83 96.89 96.92 97.27 97.23 Reddit LR WD DP EP HD BN BS TH #L 85.18 85.18 85.35 84.61 80.32 85.58 85.99 86.91 86.91 84.43 85.02 85.18 85.35 83.61 85.58 85.58 85.99 87.52 78.62 83.10 84.97 85.58 85.58 85.08 87.96</figDesc><table /><note><p>ogbn-products</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A7 :</head><label>A7</label><figDesc>The test accuracy (%) for ablated EnGCNs.</figDesc><table><row><cell>Category</cell><cell>Flickr</cell><cell>Reddit</cell><cell>ogbn-products</cell></row><row><cell>EnGCN after 0-hop training</cell><cell cols="2">46.11?0.14 74.51?0.09</cell><cell>61.97?0.08</cell></row><row><cell>EnGCN after 1-hop training</cell><cell cols="2">46.26?0.17 94.26?0.05</cell><cell>83.48?0.13</cell></row><row><cell>EnGCN after 2-hop training</cell><cell cols="2">50.00?0.49 95.23?0.03</cell><cell>87.69?0.06</cell></row><row><cell>EnGCN after 3-hop training</cell><cell cols="2">50.56?0.80 95.28?0.04</cell><cell>87.80?0.33</cell></row><row><cell cols="3">EnGCN with majority voting 56.43?0.21 97.14?0.03</cell><cell>87.99?0.04</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://ogb.stanford.edu/docs/leader_nodeprop/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We rerun it on a GPU with larger memory and the accuracy is 80.56%</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/pyg-team/pytorch_geometric</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ablating SLE. Here we provide another simple ablation study to confirm the contribution of SLE to EnGCN. From another perspective, the results further demonstrate the importance of label propagation in graph representation learning, especially for large-scale graphs. All datasets we used could be accessed through the APIs provided py PyTorch Geometric 3 <ref type="bibr" target="#b61">[62]</ref>. The statistics of Flickr, Reddit, and ogbn-products are provided as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.2 Implementation details of testing GPU memory and throughput</head><p>Here we provide the details of implementation and hyperparameters for the throughput and memory usage experiments. Regarding the implementation, we evaluate the hardware throughput based on Chen et.al. <ref type="bibr" target="#b62">[63]</ref>. For the activation memory, we measure it based on torch.cuda.memory_allocated.</p><p>Regarding the hyperparameter setting in the throughput and memory usage measurement, we set the hidden dimension to 128 across different models and datasets. We control the number of nodes whose embedding requires gradients roughly equal to 5,000 across different models and datasets. Thus, our method is fair in the sense that we control the number of active nodes per batch in the same for different methods. We note that for graph-wise sampling-based methods (e.g., ClusterGCN, GraphSAINT), the number of nodes whose embedding requires gradients equals the number of nodes retained in the GPU memory. However, for other sampling-based methods (e.g., GraphSAGE, FastGCN), they need to gather the neighbor embeddings to update the node embedding in the current batch. These embeddings of nodes that are outside the current batch do not require gradients. We also want to clarify that the hyperparameter "batch_size" in our script has a different meaning for different methods. For example, for precomputing methods, a 5,000 "batch_size" means each mini-batch contains 5,000 input samples (i.e., nodes). For GraphSAINT, "batch_size" means the number of roots in the random walk sampler. Thus, the number of nodes in each mini-batch roughly contains "batch_size" ? "walk_length".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Intended Use</head><p>The license of our repository is MIT license. For more information, please refer to https://github. com/VITA-Group/Large_Scale_GCN_Benchmarking/blob/main/LICENSE. Our benchmark is for researchers and scientists in graph mining and data science community to propose innovative methods, especially for large-scale graph training. We implement a number of representative scalable GNN models, provide several abstract classes for further inheriting, and define a unified training process for a fair comparison. In our code base, we implement two abstract classes for sampling-based and precomputing-based methods based on our unified formulations in Section 2 , respectively. One could build up his/her new sampling-based or precomputing-based GNN models upon the code base by solely overwriting a few specific functions. For detailed usage including installation, reproduction, etc., please refer to our documentation in the repository.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeuIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Subbian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.04840</idno>
		<title level="m">Cold brew: Distilling graph node representations with incomplete or missing neighborhoods</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relational learning via latent social dimensions</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="817" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph recurrent networks with attributed random walks</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="732" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparison of descriptor spaces for chemical compound retrieval and classification</title>
		<author>
			<persName><forename type="first">Nikil</forename><surname>Wale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="375" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions On Graphics (tog)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepergcn: All you need to train deeper gcns</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07739</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingquan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08306</idno>
		<title level="m">Multi-channel graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.04931</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Fastgcn: fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.10247</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Layerdependent importance sampling for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Fabrizio Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with self-labelenhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoshi</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10097</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scaling graph neural networks with approximate pagerank</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedek</forename><surname>R?zemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2464" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Vasimuddin</forename><surname>Md</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanchit</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guixiang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramanarayan</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Heinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesreen K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06700</idno>
		<title level="m">Distgnn: Scalable distributed training for large-scale graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exact: Scalable graph neural networks training via extreme activation compression</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Stochastic training of graph convolutional networks with variance reduction</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10568</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Minimal variance sampling with provable guarantees for fast training of graph neural networks</title>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rana</forename><surname>Forsati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmut</forename><surname>Kandemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1393" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive sampling towards fast graph representation learning</title>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neighbor2seq: Deep learning on massive graphs by transforming neighbors to sequences</title>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03341</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A fast and high quality multilevel scheme for partitioning irregular graphs</title>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on scientific Computing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="359" to="392" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Semi-supervised learning with graphs</title>
		<author>
			<persName><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="67" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1547" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Label propagation via teachingto-learn and learning-to-teach</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1452" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Yanbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseop</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunho</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10002</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unifying graph convolutional neural networks and label propagation</title>
		<author>
			<persName><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06755</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Node feature extraction by self-supervised multi-scale neighborhood prediction</title>
		<author>
			<persName><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00064</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3294" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vq-gnn: A universal framework to scale up graph neural networks using vector quantization</title>
		<author>
			<persName><forename type="first">Mucong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6733" to="6746" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bag of tricks for training deeper graph neural networks: A comprehensive benchmark study</title>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards deeper graph neural networks with differentiable group normalization</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuening</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dirichlet energy constrained learning for deep graph neural networks</title>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="21834" to="21846" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Adagcn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05081</idno>
		<title level="m">Adaboosting graph convolutional networks into deep models</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoki</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-class adaboost</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saharon</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and its Interface</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="349" to="360" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaboosting clusters on graph neural networks</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1523" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Improving the training of graph neural networks with consistency regularization</title>
		<author>
			<persName><forename type="first">Chenhui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenyu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04319</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sampling from large graphs</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Actnn: Reducing training memory footprint via 2-bit activation compressed training</title>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianmin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<idno>49.10 49.10 49.28 50.22 50.30 47.72 49.10 49.83 50.19 50.32 42.79 49.09 50.06 50.15 50.35</idno>
		<title level="m">GraphSAINT Flickr</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<idno>94.92 94.93 93.70 95.59 95.63 95.70 75.39 94.93 94.93 95.61 95.74 95.78 42.46 94.91 95.32 95.63 95.69 95.88 95.59 95.95 SIGN Reddit 96.51 96.62 96.62 96.62 96.62 96.61 96.67 96.58 96.62 96.58 96.63 96.66 96.25 96.51 96.62 96.36 96.66 96.73</idno>
		<title level="m">SGC Reddit</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<idno>64.46 66.02 66.02 66.02 67.48 66.32 65.74 65.35 67.37 67.48 56.28 65.30 64.81 67.48 67.48</idno>
		<title level="m">L DP Products</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wd #e Hd #l</forename><surname>Sgc Lr</surname></persName>
		</author>
		<author>
			<persName><surname>Dp</surname></persName>
		</author>
		<idno>76.85 76.55 76.55 76.55 76.55 76.83 76.69 75.83 76.53 76.37 76.85 76.85 70.99 74.56 76.52 76.07 76.49 76.40</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The searched optimal hyperparameters for all tested methods Category Methods Datasets Flickr Reddit ogbn-products Sampling GraphSAGE</title>
		<idno>WD: 0.0001</idno>
	</analytic>
	<monogr>
		<title level="j">Table A</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1000</biblScope>
		</imprint>
	</monogr>
	<note>83.76 GAMLP Figure A5: The greedy hyperparameter searching results for other methods. DP: 0.5, EP: 50, HD: 512, #L: 4, BS: 1000 LR: 0.0001, WD: 0.0 DP: 0.2, EP: 50, HD: 512, #L: 4, BS: 1000 LR: 0.001, WD: 0.0 DP: 0.5, EP: 50, HD: 512, #L: 4, BS</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">EP: 30, HD: 256, #L: 2</title>
		<idno>WD: 0.0 DP: 0.5</idno>
		<imprint/>
	</monogr>
	<note>EP: 50, HD: 256, #L: 4, BS: 2000 LR: 0.001, WD: 0.0001 DP: 0.2, EP: 40, HD: 128, #L: 4, BS: 2000</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<idno>WD: 0.0002 DP: 0.7</idno>
		<title level="m">DP: 0.2, EP: 50, HD: 512, #L: 4</title>
		<imprint>
			<biblScope unit="page">5000</biblScope>
		</imprint>
	</monogr>
	<note>EP: 30, HD: 128, #L: 2, BS: 5000 LR: 0.01, WD: 0.0 DP: 0.2, EP: 40, HD: 128, #L: 2, BS</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><surname>Decoupling</surname></persName>
		</author>
		<idno>WD: 0.0001</idno>
		<title level="m">EP: 100, #L:2</title>
		<imprint/>
	</monogr>
	<note>EP: 50, #L:2, DP: 0.1 LR: 0.001, WD: 0.0001, EP: 500, #L:8, DP: 0.1</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><surname>Lp</surname></persName>
		</author>
		<title level="m">DT: residual, #Prop: 20, AR: 0.9, Adj: D -1/2 AD -1/2 , AS: True, #ML:2 DT: residual, #Prop: 50, AR: 0.9, Adj: D -1 A, AS: True, #ML:2 DT: residual, #Prop: 20, AR: 0.9</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>38, 40. Adj: D -1 A, AS: True, #ML</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
