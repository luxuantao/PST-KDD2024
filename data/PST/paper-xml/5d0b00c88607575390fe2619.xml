<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adaptive gait detection based on foot-mounted inertial sensors and multi-sensor fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-03-04">4 March 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hongyu</forename><surname>Zhao</surname></persName>
							<email>zhaohy@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhelong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sen</forename><surname>Qiu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiaxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Shenyang Fire Research Institute</orgName>
								<address>
									<postCode>110034</postCode>
									<settlement>Shenyang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fang Xu b</roleName><forename type="first">Zhengyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Control Science and Engineering</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Neurology Department</orgName>
								<orgName type="institution">Dalian Locomotive Hospital</orgName>
								<address>
									<postCode>116021</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanming</forename><surname>Shen</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116024</postCode>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Adaptive gait detection based on foot-mounted inertial sensors and multi-sensor fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-03-04">4 March 2019</date>
						</imprint>
					</monogr>
					<idno type="MD5">52C18D905D4EDD50228262D0D870BD9C</idno>
					<idno type="DOI">10.1016/j.inffus.2019.03.002</idno>
					<note type="submission">Received 2 November 2018; Received in revised form 12 January 2019; Accepted 3 March 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Gait analysis Body-worn sensors Machine learning Hidden Markov model (HMM) Neural network (NN)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gait detection plays an important role in areas where spatial-temporal gait parameters are needed. Inertial sensors are now sufficiently small in size and light in weight for collection of human gait data with body sensor networks (BSNs). However, gait detection methods usually rely on careful sensor alignment and a set of rule-based thresholds, which are brittle or difficult to implement. This paper presents an adaptive method for gait detection, which models human gait with a hidden Markov model (HMM), and employs a neural network (NN) to deal with the raw measurements and feed the HMM with classifications. Six gait events are involved for a detailed analysis, i.e., heel strike, foot flat, mid-stance, heel off, toe off, and mid-swing. In order to obtain enough gait data for training a gait model, the gait events are labeled by a rule-based detection method, in which the predefined rules are verified with an optical motion capture system. Experiments were conducted by nine subjects, based on a dual-sensor configuration with one sensor on each foot. Detection performance is quantified using metrics of accuracy, sensitivity and specificity, and the averaged performance values are 98.11%, 94.32% and 98.86% respectively with a timing error less than 2.5 ms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Gait analysis has potential use in many applications, such as health care, clinical rehabilitation, sport training, and pedestrian navigation <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref> . Clinical gait analysis is usually carried out in laboratories that are equipped with sophisticated measurement and analysis devices, such as ground reaction force plates and 3D motion analysis systems, some of which can achieve an accuracy of a few centimeters when tracking human motion. However, in some scenarios, accuracy is not the only or primary concern for gait analysis, and other relevant concerns include mobility, accessibility, portability, etc. For example, it might be more meaningful to monitor gait parameters for patients or elders in their daily lives than just a brief examination in a lab or a clinic.</p><p>There is an increasing demand for a portable and accurate gait analysis system that can be used in both clinical trials and daily lives <ref type="bibr" target="#b3">[4]</ref> . Body sensor networks (BSNs) are deployed on a person and can collect his motion data wherever he goes <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> . Therefore, gait analysis systems constructed of inertial sensors are gaining increasing popularity. However, the main challenge in developing inertial gait analysis systems is to ensure a sufficient accuracy, as low-cost microelectromechanical sys-tems (MEMS)-based inertial measurement unit (IMU) are prone to errors <ref type="bibr" target="#b9">[10]</ref> .</p><p>In general, gait detection is a prerequisite for gait analysis. Different methods and sensor configurations have been used for gait detection in the literature <ref type="bibr" target="#b10">[11]</ref> . For most types of pedal locomotion that are achieved by legged motion of human and animals, the intuitive experience is to achieve gait detection by placing sensors on feet. As most inertial sensors include 3-axis gyroscopes and accelerometers, the gait events can be detected by threshold-based methods using the measurements of gyroscopes and accelerometers separately or by fusing them together <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> . Sensors can also be placed on other parts of human body for gait detection <ref type="bibr" target="#b13">[14]</ref> , e.g., the use of a gyroscope attached to thigh is demonstrated in <ref type="bibr" target="#b14">[15]</ref> , and the use of gyroscopes attached to both shanks and thigh is demonstrated in <ref type="bibr" target="#b15">[16]</ref> .</p><p>The above-mentioned detection methods involve a set of rule-based thresholds, which are empirically predetermined and relatively brittle.</p><p>As each individual has a unique gait pattern, the thresholds may work well for the measurements that they are derived from, but not adapt to each person's individual data. Besides, the thresholds are mainly hand tuned and usually fixed in the whole walking process in spite of gait Fig. <ref type="figure">1</ref>. Key events and phases of a gait cycle. changes, and the process of rule designing and threshold tuning itself is frustrating and time-consuming. We have tried to tune one of the thresholds automatically (i.e., the duration threshold of gait phases, as detailed in <ref type="bibr" target="#b16">[17]</ref> ), but careful setting is stilled needed for other thresholds. Furthermore, if a new sensor is added or the sensor is attached to a new location, new detection rules and associated thresholds are required. Therefore, there is a need for an adaptive detection method that can be quickly adapted to new subjects, new motions, new sensors, and new sensor locations.</p><p>Recently, statistical methods have been developed for gait detection. Originally developed for speech recognition, hidden Markov models (HMMs) have been widely used in other pattern recognition applications <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> . The potential of HMM for gait segmentation has been previously demonstrated in <ref type="bibr" target="#b19">[20]</ref> for galloping Thoroughbred racehorses with a trunk-mounted six-DOF inertial sensor. An HMM-based algorithm was developed in <ref type="bibr" target="#b20">[21]</ref> for gait detection in children with and without hemiplegia by attaching single-axis gyroscopes one on each foot, and the four typical events (i.e., heel-strike, foot-flat, heel-off, and toe-off) were specified as hidden states of the HMM. A classifier based on HMM is applied in <ref type="bibr" target="#b21">[22]</ref> for gait phase detection and discrimination between walking-jogging activities by using a uniaxial foot-mounted gyroscope, and also four typical events are involved. An HMM was applied in <ref type="bibr" target="#b22">[23]</ref> to detect the gait phases of children with cerebral palsy by placing two inertial sensors (only the sagittal components of gyroscope measurements are used) on shank and foot of the ipsilateral leg, and three gait models with two, four and six phases are investigated.</p><p>Generally, the pure HMMs work well for low dimensional data, but are less suitable for high dimensional data. An HMM was adopted for extracting temporal gait parameters in <ref type="bibr" target="#b23">[24]</ref> by using eight five-DOF inertial sensors placed on human body, with a feature selection and model parametrization system based on genetic algorithms (GAs), and four events are identified. An HMM was presented in <ref type="bibr" target="#b24">[25]</ref> for detecting gait phases by using seven six-DOF inertial sensors attached to lower body, with the HMM observations provided by a five-layer feed-forward neural network (FNN), and five events are involved. These hybrid methods typically have better performance compared with the pure HMMs. An HMM was applied in <ref type="bibr" target="#b25">[26]</ref> to identify six gait phases from foot pressure data, with the ground contact forces (GRFs) measured by shoe-embedded airbladder sensors.</p><p>This paper aims to accurately and robustly detect the gait events for a close examination of human gait. Inspired by the above-mentioned methods, this paper presents an adaptive hybrid method by modeling human gait with a six-state left-to-right HMM and employing a threelayer neural network (NN) to deal with the raw measurements, with the train data labeled by a rule-based detection method. Different from the gait events described in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b25">[26]</ref> , the six-state model in this paper inherits the four typical events and includes two additional events of interest (i.e., mid-stance and mid-swing).</p><p>The main contributions of this paper are as follows:</p><p>(1) the six-event gait model not only covers more details of human gait, but also complies well with the well-established two-event and fourevent gait models;</p><p>(2) placing one sensor on each foot presents less complexity than on various locations of human body, and allows more precise results than on ipsilateral limb by using the coupling relationship between limbs;</p><p>(3) the gait events are labeled by a rule-based detection method, which can work anytime and anywhere to yield enough walking data for training a gait model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gait division and delimitation</head><p>Evaluation of gait parameters during walking is considered of the utmost importance in gait analysis, as walking is the most common form of human activity in daily life and also one of the major aims of rehabilitation in clinical practice. Terminologically, the term gait describes the way of human walking, which exhibits periodic patterns termed as gait cycle. Each gait cycle has a sequence of ordered gait events, which occur at specific temporal locations and thereby referred to as temporal gait parameters. The four typical events in one normalized gait cycle, i.e., heel-strike (HS), foot-flat (FF), heel-off (HO), and toe-off (TO), are shown in Fig. <ref type="figure">1</ref> when identified relative to right foot. It can be seen that these key events can divide a gait cycle into two to four consecutive gait phases, or even more when taking the two events in the middle of stance and swing phases into consideration, i.e., mid-stance (MSt) and mid-swing (MSw).</p><p>Different researches concentrate on different events according to the specific application. For instance, mid-stance is a critical temporal event in the applications related to spatial gait parameters, e.g., pedestrian localization and navigation. At mid-stance, all of the body's weight rests solely on one foot, and the foot velocity is closer to zero than any other moment. This information is of great use, especially that the zero velocity update (ZUPT) technique can effectively use this information to reset the accumulated velocity errors <ref type="bibr" target="#b26">[27]</ref> .</p><p>For a close examination of human gait, the mentioned six events are modeled in our study. As the motions of subject's feet are strongly coupled with each other, gait events are detected using the data from both feet, which is expected to obtain more accurate and precise results than using that of the ipsilateral limb. When the gait events of each foot are correctly detected, the gait cycles will be divided and delimited accordingly, and therefore the interested gait parameters will be derived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data acquisition and labeling</head><p>The gait events can be identified by examining patterns in the sensed data. There are two sources of motion data in our study, i.e., a 3D inertial sensing system and a 3D optical motion capture system. The inertial sensors and reflective markers are placed on the subject's feet, as shown in Fig. <ref type="figure" target="#fig_0">2</ref> . For training a gait model, the gait events are labeled by the data from inertial sensors, using a rule-based detection method, in which the predefined rules are validated against the ground truth provided by the optical system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data acquisition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Inertial data</head><p>The subjects were equipped with two foot-mounted inertial sensors, one on each heel, as shown in Fig. <ref type="figure" target="#fig_0">2 (a)</ref>. The inertial sensor used in this   system is the ADIS16448 iSensor® device from Analog Devices Ltd., USA <ref type="bibr" target="#b27">[28]</ref> . Each sensor includes a triaxial gyroscope and a triaxial accelerometer. The bandwidth and sampling rate of the IMU is 330 Hz and 400 Hz, respectively. Fig. <ref type="figure" target="#fig_1">3</ref> shows the main hardware components of the inertial sensing system. The dimensions of the entire system assembly are 4.5 cm × 3.5 cm × 2.25 cm. Data collected during the experiments were stored in an internal memory and transferred to an external computer for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Vicon data</head><p>The subjects were instructed to walk in a laboratory equipped with a Vicon® optical motion capture system from Oxford Metrics Ltd., UK <ref type="bibr" target="#b28">[29]</ref> . The Vicon system consists of a Nexus TM software and six Vantage TM V5 cameras with three on each side of the ceiling walls, as shown in Fig. <ref type="figure" target="#fig_2">4</ref> . Three pairs of reflective markers were attached to the subject's feet, as shown in Fig. <ref type="figure" target="#fig_0">2 (b</ref>), which are referred as "heel ", "ankle ", and "toe " markers respectively. The sampling rate of Vicon system is 100 Hz, and the foot kinematics are calculated with reference to the laboratory floor. The size of laboratory is 7 m × 5.5 m, and the coverage area of Vicon system is approximately 5 m × 4 m.</p><p>In this experiment, the height limitation is not an issue; however, the relatively small coverage area means that the subjects cannot travel any great distance in one direction, and only three to four complete gait cycles can be captured. Generally, the subjects only show normal gait when they become accustomed to the walking over a path of enough length. Therefore, only the middle gait cycles are examined, as the periodicity and regularity are ideal for obtaining high detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data synchronization</head><p>The Vicon system was used as a reference system to establish a rulebased detection method. The rule-based method works anytime and anywhere regardless of the area restriction, and therefore has the ability to yield enough gait data. However, the inertial and Vicon data are out of phase from each other and need to be time-synchronized. As the Vicon system and inertial system have their respective hardware and software, synchronization between these data sources can be a problem. Besides, as the inertial sensor measures the specific force and angular rate in the body frame ( b -frame for short) while the Vicon system measures the marker locations in the reference frame ( r -frame for short), the two data sources cannot be synchronized by direct comparison. To overcome this problem, shared information can be derived in both systems through two ways:</p><p>(1) differentiating the marker locations once and twice to yield velocity and acceleration information respectively as</p><formula xml:id="formula_0">𝒗 𝑟 𝑉 𝑖𝑐𝑜𝑛 = ṗ 𝑟 𝑉 𝑖𝑐𝑜𝑛 𝒂 𝑟 𝑉 𝑖𝑐𝑜𝑛 = v 𝑟 𝑉 𝑖𝑐𝑜𝑛 (1)</formula><p>where r denotes the r -frame, Vicon denotes the information from Vicon system; 𝒑 𝑟 𝑉 𝑖𝑐𝑜𝑛 is the position, 𝒗 𝑟 𝑉 𝑖𝑐𝑜𝑛 is the velocity, and 𝒂 𝑟 𝑉 𝑖𝑐𝑜𝑛 is the acceleration.</p><p>(2) integrating the gravity-corrected acceleration once and twice to yield velocity and position information respectively, by solving the equations of strapdown inertial navigation system (INS) as</p><formula xml:id="formula_1">Ċ 𝑟 𝑏 = 𝑪 𝑟 𝑏 𝛀 𝑏 𝑖𝑏 v 𝑟 𝐼 𝑁 𝑆 = 𝒂 𝑟 𝐼 𝑁 𝑆 ṗ 𝑟 𝐼 𝑁 𝑆 = 𝒗 𝑟 𝐼 𝑁 𝑆 (2)</formula><p>where b denotes the b -frame, i denotes the inertial coordinate frame, INS denotes the information from inertial system; 𝑪 𝑟 𝑏 is the rotation matrix transforming from b -frame to r -frame, 𝒗 𝑟  𝐼 𝑁 𝑆 is the velocity, and 𝒑 𝑟 𝐼 𝑁 𝑆 is the position; 𝛀 𝑏 𝑖𝑏 is the skew-symmetric matrix of the angular rate 𝝎 𝑏 𝑖𝑏 measured by gyroscope; 𝒂 𝑟 𝐼 𝑁 𝑆 is the gravity-corrected acceleration and 𝒂 𝑟  𝐼 𝑁 𝑆 = 𝑪 𝑟 𝑏 𝒇 𝑏 -𝒈 𝑟 , where f b is the specific force measured by accelerometer and g r is the gravity. Synchronized data are generated by compensating for the time-shift between the two data. The time-shift can be found by a cross-correlation analysis <ref type="bibr" target="#b29">[30]</ref> . It should be noted that cross-correlation in discrete domain requires the data streams to have the same sampling rate, which can be achieved by interpolating the Vicon data sampled at 100 Hz with the inertial data sampled at 400 Hz. The synchronized raw and derived information in r -frame are shown in Fig. <ref type="figure" target="#fig_3">5</ref> , where Z denotes the vertical Z -axis. It can be seen that for inertial information, positon exhibits everincreasing error drifts due to the integrative nature of INS algorithm, while acceleration features more rapid fluctuations due to the specific nature of foot motion. Therefore, the velocity information is selected to synchronize the IMU and Vicon data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rule-based gait detection</head><p>The Vicon system captured the foot motion with relatively high resolution but limited coverage area. When the two data are matched exactly in temporal domain, the Vicon data and its derived motion infor-  As the inertial sensors and Vicon markers are placed on subject's feet, the sensed data feature periodic patterns according to the gait cycles. A segment of sensed data is shown in Fig. <ref type="figure" target="#fig_4">6</ref> , together with the key events identified relative to right foot. For each foot, the TO and FF events can be marked by the toe marker (see subplot 4 ○), while the HS and HO events can be marked by the heel marker (see subplot 5 ○). As for the MSw and MSt events, they are defined as the moments that ipsilateral and contralateral heels reach the highest clearance respectively, and therefore can also be marked by the heel or ankle markers (see subplot <ref type="bibr" target="#b4">5</ref> ○ and 6 ○ for heel markers of both feet). In this paper, the rules for detecting key gait events are established as:</p><p>(1) Rules for MSw and MSt : The highest heel clearance refers to the peak vertical position of heel (or ankle) marker during swing phase, which coincides with the minimum magnitude of specific force. By this rules, MSw and MSt events can be detected through dual-gait analysis by a peak detection rule (see subplot 2 ○ and 3 ○).</p><p>(2) Rules for FF to HO : As seen in subplot 1</p><p>○ and 4 ○, the stance phase lasts from FF to HO, during which the angular rate is near to zero. These patterns facilitate the identification of HO and FF events by a flat-zone detection rule.</p><p>(3) Rules for HS to TO : As seen in subplot 1</p><p>○ and 5 ○, the ankle reaches the maximum dorsiflexion angle at HS, which can be detected by a zero-crossing detection rule; the ankle reaches the maximum rate of plantar flexion at TO, which can be detected by a peak detection rule.</p><p>The rule-based method can identify all the six events from a long inertial data sequence. For a 20 m straight-line walking, the detection results are shown in Fig. <ref type="figure" target="#fig_5">7</ref> . To avoid the effects of walking initiation and termination, the first and last gait cycles are excluded, especially for the right foot that steps first but stops last in this walking. As discussed above, three kinds of rules are involved in the detection process, i.e., peak detection, zero-crossing detection, and flat-zone detection. Due to the natural variability of human gait, the involved detection thresholds vary considerably and need to be tuned independently for each individual's gait. Therefore, there is a need for an adaptive detection method that can be quickly adapted to new gait of new subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model definition and training</head><p>A six-state HMM is defined in this paper to cover more details of human gait. However, HMMs are less suitable for high dimensional data, and require an explicit model of the dependencies between the sensed data and the hidden states. Therefore, a hybrid discriminative/generative method is presented by embedding an NN into the HMM for gait detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Left-to-right HMM model</head><p>HMM models a Markov process with discrete and stochastic hidden states, which is in just one state at each time instant. During each sampling interval, the process generates an observation, and then transits to the next state or stays in the current state. This process generates a sequence of hidden states and a sequence of observations. The HMM parameter set 𝝀= { 𝝅, 𝑨 , 𝐵 } gives a complete statistical description of the time evolution of Markov processes, which consists of three sets of probabilities, i.e., initial state probabilities 𝝅, state transition probabilities A , and emission probabilities B . Define the Markov process has N states 𝑺 = { 𝑠 1 , 𝑠 2 , ⋯ , 𝑠 𝑁 } , and can emit M observations 𝒀 = { 𝑦 1 , 𝑦 2 , ⋯ , 𝑦 𝑀 } . For a given sequence of L observations 𝑶 = ( 𝑜 1 , 𝑜 2 , ⋯ , 𝑜 𝐿 ) , there is a corresponding sequence of L hidden states 𝑸 = ( 𝑞 1 , 𝑞 2 , ⋯ , 𝑞 𝐿 ) .</p><p>For gait detection, the gait phases are the hidden states of the HMM. Each hidden state s i represents a gait phase that begins with the present event and continues until the next successive event, as shown in Fig. <ref type="figure">1</ref> with N = 6. Due to the periodic nature of foot motion with ordered gait events, each state can only transit to itself or the "right " state. Thus, each gait phase can be represented by a unique state in HMM using a left-to-right model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hybrid NN/HMM model</head><p>Given a sequence of time-ordered observations 𝑶 = ( 𝑜 1 , 𝑜 2 , ⋯ , 𝑜 𝐿 ) and a trained HMM with all the parameters 𝝀= { 𝝅, 𝑨 , 𝐵 } learned, the Viterbi algorithm can estimate the most likely sequence of hidden states that have generated the observations. However, HMMs have their own drawbacks <ref type="bibr" target="#b30">[31]</ref> : (1) HMMs assume that the probability of each observation depends only on the current state, which makes contextual effects difficult to model; (2) HMMs are generative models, whereas discriminative models generally achieve better classification performance. Discriminative models utilizing machine learning techniques seem to be promising alternatives to HMMs.</p><p>Generally, any machine learning method can be used for pattern recognition, such as support vector machine (SVM) and k-nearest neighbor (k-NN). Preliminary studies showed that NN allows the best tradeoff between computational complexity, efficiency, and accuracy. The power of NN lies in its ability of learning non-linear combinations of input data automatically, and a three-layer network can approximate any multivariate polynomial function <ref type="bibr" target="#b31">[32]</ref> . However, the pure NNs have been limited to process each input data in isolation, independently of its surrounding context.</p><p>To combine the strengths of both techniques, a practical use of NN for gait detection is to combine it with an HMM in the so-called hybrid manner <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33]</ref> . The NN has a temporal window in the input layer, which provides observations for the HMM by using the past and future data of the process to be modeled. The HMM models the sequential property of human gait, which complements the NN by providing contextual information. The block diagram in Fig. <ref type="figure" target="#fig_6">8</ref> shows the training and testing phases of the hybrid NN/HMM detection method. As labeled datasets are available, the training phase is conducted in supervised conditions to facilitate the parameter estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment and discussion</head><p>To evaluate the hybrid NN/HMM algorithm, the experiments were conducted in a corridor of a typical office building. In this section, the experimental setup and performance metrics are first described, and then the results of the experiments are presented and analyzed. Finally, some discussions on the experimental results are made. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental setup</head><p>The subjects were instructed to walk at self-selected speeds along a 20 m straight and level path. Nine healthy subjects were chosen randomly for the experiments, and each subject repeated the experiment four times. Prior to experiment, the system kept stationary for a brief period to the initial alignment and calibration. rulebased detection method was used to label the data that used to train and test the NN/HMM method.</p><p>For the NN shown in Fig. <ref type="figure" target="#fig_6">8</ref> , each input vector is formed from a moving window of fifty-one samples with a step-size of one, the hidden layer consists of fifteen nodes, and the output layer consists of six nodes corresponding to the six gait phases. The NN assigns a label to each input vector, which indicates the gait phase at the central point of the temporal window. Based on these parameters, the network can be well trained with the cross-entropy cost function and scaled conjugate gradient backpropagation algorithm.</p><p>To train the HMM, the NN classifications represent the HMM observations, while the gait labels indicate the true HMM states. As only labeled gait events are provided and there are a number of unlabeled samples between successive events, each state is annotated with the label of the event that it begins with. The observations and the states can be used to estimate the HMM transition and emission probabilities. Examples of transition and emission probability matrices of a trained HMM are shown in Table <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">Table 2</ref> , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance metrics</head><p>Three performance metrics are employed in this paper, i.e., accuracy, sensitivity and specificity of gait detection. To assess the overall performance, the performance values are averaged over all phases and all tests. The performance values can be calculated with a timing tol-Fig. <ref type="figure">9</ref>. detections for a random trial. erance of ± n samples between labeled and estimated gait events. The timing tolerance acts as a tolerated timing error, where a tolerance of ± 1 samples is equivalent to a timing error within 5 ms for the sampling rate of 400 Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">Detection for a random trial</head><p>With the settings specified in the experimental setup, the NN/HMM method is applied to the sensed inertial data of a random walking trial. The raw measurements and detection results are illustrated in Fig. <ref type="figure">9</ref> , and the ground truth from the well-tuned rule-based detection method is provided for visual inspection.</p><p>The HMM transition probabilities ensure that the outputs follow the left-to-right topology with self-loops and no transitions skip over states. The HMM emission probabilities encode the error rates of the base NN classifier by providing contextual information. If the NN incorrectly classifies a small number of samples but correctly classifies their adjacent samples, the Viterbi algorithm could correct these misclassifications by recovering of the most likely state sequence, especially seen in Fig. <ref type="figure">9 (b)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">Overall detection performance</head><p>In order to evaluate the generalization capabilities of the NN/HMM method, leave-one-subject-out cross-validation is adopted in the evaluation process. The training is performed using the data from all subjects but one, and the data from the remaining subject is used as test data. During the validation process, the confounding effects of inter-subject variability are maximized, when compared to N -fold cross-validation for example. This validation procedure is repeated for each trial of each subject in turn, so that each dataset is eventually used for testing and training.</p><p>The individual and overall performance values are listed in Table <ref type="table" target="#tab_2">3</ref> , where the values for each subject is obtained by averaging over both feet of all trials that the subject have conducted. The performance values are calculated with a stringent tolerance value of ± 0 samples, which may be further improved if a refinement strategy is applied to eliminate the remaining false detections and/or a larger timing tolerance is allowed, as discussed below.</p><p>The detection results in this paper (referred as Results Our ) compare favorably with that reported by Mannini and Sabatini <ref type="bibr" target="#b21">[22]</ref> (referred as Results MS ) Evans and <ref type="bibr" target="#b24">[25]</ref> (referred as Results EA as listed in Table <ref type="table" target="#tab_3">4</ref> . It can be that our method can cover more details of human gait by building a six-state gait model, with a smaller timing error less than 2.5 ms based on a dual-sensor configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3.">Temporal gait parameters</head><p>Gait detection helps to delimit the gait phases and furthermore to derive spatial-temporal gait parameters. Close examination and comparison of gait parameters can be done between two feet for each subject, e.g., to measure the gait symmetry and evaluate the effect of gait symmetry on gait patterns. The percentages of each foot's gait phases are displayed in a radar chart with two regular hexagons (one for each foot, respectively), as illustrated in Fig. <ref type="figure" target="#fig_7">10</ref> . The higher the coincidence of the two hexagons is, the more symmetrical the dual-gait would be. The radar charts show that there is no perfectly symmetrical gait, even for the healthy subjects. The gait asymmetry is much more pronounced in Subject 1, Subject 5, and Subject 7.</p><p>If we divide a gait cycle into two phases, i.e., stance and swing, the percentages of the gait cycle spent in each phase are shown in Fig. <ref type="figure" target="#fig_8">11</ref> for both feet. In this division, the stance phase is defined to be the same as that of the four-phase model in Fig. <ref type="figure">1</ref> while the foot is entirely on the ground, whereas the swing phase is defined to last from HO to FF while any part of the foot is in the air. According to dual-gait analysis, the foot that is in stance phase helps the contralateral foot generate momentum needed its heel-off, toe-off and swing. The averaged percentages of stance phase is around the expected value of 30%, while the averaged percentages of swing phase is around 70%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion</head><p>As indicated above, good detection performance can be achieved by using the hybrid NN/HMM method. However, further improvement can be made to enhance the detection performance.</p><p>As shown in Fig. <ref type="figure">9</ref> , the Viterbi algorithm itself features erroneous outputs, which give rise to deletions and insertions of gait cycles. Actually, miss detections rarely occur, whereas insertions are one of the main error sources. Once a gait event is incorrectly detected, a complete false gait cycle appears to implement the self-loop in HMM. Two partial enlarged views of the HMM output in Fig. <ref type="figure">9</ref> (b) are shown in Fig. <ref type="figure" target="#fig_9">12</ref> . If the insertions of gait cycles can be effectively deleted, the detection performance will be further improved.</p><p>Except for the false gait cycles, the timing tolerance also affects the performance values. If a larger timing tolerance of ± n ( n &gt; 0) samples is allowed, better performance is expected, as shown in Fig. <ref type="figure" target="#fig_10">13</ref> . Each performance value increases steadily at the beginning, and ultimately reaches a high plateau level at the tolerance value of approximately ± 28 samples, i.e., a timing error within 70 ms. This timing error accounts for approximately 6% of a gait cycle, which is less than the proportion of any target gait phase. Users can specify the acceptable timing error according to the specific use.  Besides, the HMM model is prone to errors at both ends of the analysis, due to the need for contextual information. The average detection performance of the start and end gait cycles is compared to that of the other cycles, and the results are given in Fig. <ref type="figure" target="#fig_11">14</ref> . It is shown that there are more errors on the edges of the trials, which can also be seen in Fig. <ref type="figure">9</ref> . Thus, if the first and last gait cycles (removed in the training phase to avoid unnecessary gait variation) are included in the testing phase, which provides an opportunity for the HMM to establish a cycle, the final performance will be also further improved.</p><p>In practical applications, human motion recognition methods would be helpful to distinguish typical walking movements at different speeds from other atypical or non-walking locomotion, as well as the walking initiation and termination mentioned previously. Besides, different models are desired for multiple gait patterns, such as sidestepping, running, ascending or descending stairs as well as pathological gaits, where   the gait might has different event sequences or measurement profiles, which are not addressed in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and future work</head><p>This paper embeds a three-layer NN into a six-state HMM to build a hybrid NN/HMM model for gait detection. The NN achieves better performance in classification, and can handle high dimensional data. The HMM models the sequential property of human gait, and complements the NN with contextual information. The hybrid method is designed to be computationally complex for training, but computationally efficient at runtime. It does not require a careful sensor alignment or parameter adjustment for each person's individual data, and generalizes well to new subjects. Furthermore, if training data are available, this method can be quickly adapted to new motions, new sensors, and new sensor locations without modification.</p><p>In future work, the presented detection method will be evaluated with more gait patterns for practical use, more spatial-temporal gait parameters will be derived based on the detection results, and more applications will be explored to demonstrate the versatility of the entire system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Gait data acquisition systems.</figDesc><graphic coords="3,53.30,179.12,223.48,82.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hardware setup of the inertial sensors.</figDesc><graphic coords="3,55.22,305.12,125.08,129.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Vicon optical motion capture system.</figDesc><graphic coords="3,179.84,305.12,93.99,193.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Synchronized information from inertial and Vicon systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Rules for gait detection relative to right foot.</figDesc><graphic coords="4,37.94,191.75,290.40,347.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Results of gait division and gait detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Block diagram of the hybrid NN/HMM method.</figDesc><graphic coords="6,61.64,266.48,169.75,105.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Temporal gait parameters of each foot per subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Averaged percentages of swing and stance in gait cycle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Insertions of gait cycles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Impact of timing tolerance on performance values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Detection of gait cycles in the middle and at both ends.</figDesc><graphic coords="9,42.42,233.78,242.40,202.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Transition probability matrix of a trained HMM.</figDesc><table><row><cell></cell><cell>Heel off</cell><cell>Toe off</cell><cell>Mid-swing</cell><cell>Heel strike</cell><cell>Foot flat</cell><cell>Mid-stance</cell></row><row><cell>Heel off</cell><cell>0.9855</cell><cell>0.0145</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Toe off</cell><cell>0</cell><cell>0.9847</cell><cell>0.0153</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell>Mid-swing</cell><cell>0</cell><cell>0</cell><cell>0.9915</cell><cell>0.0085</cell><cell>0</cell><cell>0</cell></row><row><cell>Heel strike</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.9850</cell><cell>0.0150</cell><cell>0</cell></row><row><cell>Foot flat</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.9741</cell><cell>0.0259</cell></row><row><cell>Mid-stance</cell><cell>0.0097</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0.9903</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Emission probability matrix of a trained HMM.</figDesc><table><row><cell></cell><cell>Heel off</cell><cell>Toe off</cell><cell>Mid-swing</cell><cell>Heel strike</cell><cell>Foot flat</cell><cell>Mid-stance</cell></row><row><cell>Heel off</cell><cell>0.9756</cell><cell>0.0045</cell><cell>0.0009</cell><cell>0.0009</cell><cell>0.0009</cell><cell>0.0172</cell></row><row><cell>Toe off</cell><cell>0.0067</cell><cell>0.9800</cell><cell>0.0105</cell><cell>0.0010</cell><cell>0.0010</cell><cell>0.0010</cell></row><row><cell>Mid-swing</cell><cell>0.0005</cell><cell>0.0032</cell><cell>0.9910</cell><cell>0.0027</cell><cell>0.0005</cell><cell>0.0021</cell></row><row><cell>Heel strike</cell><cell>0.0047</cell><cell>0.0009</cell><cell>0.0028</cell><cell>0.9823</cell><cell>0.0065</cell><cell>0.0028</cell></row><row><cell>Foot flat</cell><cell>0.0016</cell><cell>0.0016</cell><cell>0.0016</cell><cell>0.0578</cell><cell>0.8973</cell><cell>0.0401</cell></row><row><cell>Mid-stance</cell><cell>0.0048</cell><cell>0.0006</cell><cell>0.0006</cell><cell>0.0006</cell><cell>0.0030</cell><cell>0.9903</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Individual and overall performance values (%).</figDesc><table><row><cell>Subject</cell><cell>Accuracy</cell><cell>Sensitivity</cell><cell>Specificity</cell></row><row><cell>1</cell><cell>97.00</cell><cell>91.01</cell><cell>98.20</cell></row><row><cell>2</cell><cell>98.02</cell><cell>94.05</cell><cell>98.81</cell></row><row><cell>3</cell><cell>97.59</cell><cell>92.77</cell><cell>98.55</cell></row><row><cell>4</cell><cell>98.14</cell><cell>94.43</cell><cell>98.89</cell></row><row><cell>5</cell><cell>98.38</cell><cell>95.15</cell><cell>99.03</cell></row><row><cell>6</cell><cell>98.53</cell><cell>95.59</cell><cell>99.12</cell></row><row><cell>7</cell><cell>97.49</cell><cell>92.46</cell><cell>98.49</cell></row><row><cell>8</cell><cell>98.95</cell><cell>96.86</cell><cell>99.37</cell></row><row><cell>9</cell><cell>98.84</cell><cell>96.53</cell><cell>99.31</cell></row><row><cell>Mean</cell><cell>98.11</cell><cell>94.32</cell><cell>98.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparison of detection performance.</figDesc><table><row><cell></cell><cell cols="5">Tolerance Sensitivity Specificity Phases Sensors</cell></row><row><cell>Results Our</cell><cell>2.5 ms</cell><cell>94.32%</cell><cell>98.86%</cell><cell>Six</cell><cell>One on each heel</cell></row><row><cell>Results MS</cell><cell>20 ms</cell><cell>94.90%</cell><cell>98.30%</cell><cell>Four</cell><cell>One on the instep</cell></row><row><cell>Results EA</cell><cell>23 ms</cell><cell>88.49%</cell><cell>97.12%</cell><cell>Five</cell><cell>Seven on lower body</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was jointly supported by China Postdoctoral Science Foundation no. 2017M621131 , National Natural Science Foundation of China no. 61873044 , Dalian Science and Technology Innovation Fund no. 2018J12SN077 , and Fundamental Research Funds for the Central Universities no. DUT18RC(4)036 and DUT16RC(3)015 .</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gait analysis of pregnant patients with lumbopelvic pain using inertial sensor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tanigawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Morino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aoyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gait Posture</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="176" to="181" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inertial/magnetic sensors based pedestrian dead reckoning by means of multi-sensor fusion</title>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="108" to="119" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Heading drift reduction for foot-mounted inertial navigation system via multi-sensor fusion and dual-gait analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSEN.2018.2866802</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Sens. J</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">IDNet: smartphone-based gait recognition with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gadaleta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rossi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="25" to="37" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-sensor fusion in body sensor networks: state-of-the-art and research challenges</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Alinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="68" to="80" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enabling effective programming and flexible management of efficient body sensor network applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giannantonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kuryloski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human Mach. Sys</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="133" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Power-aware activity monitoring using distributed wearable sensors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Panuccio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trovato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Human Mach. Sys</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="537" to="544" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cloud-assisted body area networks: state-of-the-art and future challenges</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Fatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Vasilakos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wirel. Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1925" to="1938" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From modeling to implementation of virtual sensors in body sensor networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Raveendranathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galzarano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Loseu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giannantonio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sgroi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sens. J</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="593" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A time-controllable Allan variance method for MEMS IMU</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ind. Robot</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="120" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gait partitioning methods: a systematic review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taborri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-velocity detection -an algorithm evaluation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Skog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Händel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rantakokko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2657" to="2666" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A framework for collaborative computing and multi-sensor data fusion in body sensor networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galzarano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="50" to="70" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kernel fusion based extreme learning machine for cross-location activity recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gravina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fortino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long-term unrestrained measurement of stride length and walking velocity utilizing a piezoelectric gyroscope</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miyazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Biomed. Eng</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="753" to="759" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatio-temporal parameters of gait measured by an ambulatory system using miniature gyroscopes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Aminian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Büla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Leyvraz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="689" to="699" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stance-phase detection for ZUPT-aided footmounted pedestrian navigation system</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ASME Trans. Mechatron</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3170" to="3181" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activity recognition for incomplete spinal cord injury subjects using hidden Markov models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Azeze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Albert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sens. J</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6369" to="6374" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automatic recognition of gait phases using a multiple regression hidden Markov model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Attal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Amirat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chibani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ASME Trans. Mechatron</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1597" to="1607" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A hidden Markov model-based stride segmentation technique applied to equine inertial sensor trunk movement data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Biomech</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="216" to="220" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gait detection in children with and without hemiplegia using single-axis wearable gyroscopes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Petrarca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Porfiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">73152</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gait phase detection and discrimination between walking-jogging activities using hidden Markov models applied to foot motion data from a gyroscope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mannini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Sabatini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gait Posture</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="657" to="661" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Validation of inter-subject training for hidden Markov models applied to gait phase detection in children with cerebral palsy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Taborri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Scalona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="24514" to="24529" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A method for extracting temporal parameters based on hidden Markov models in body sensor networks with inertial sensors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guenterberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ghasemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Technol. Biomed</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1019" to="1030" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detection of gait phases using orient specks for mobile clinical gait analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Arvind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 11th International Conference on Wearable and Implantable Body Sensor Networks</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gait analysis based on a hidden Markov model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 12th International Conference on Control, Automation and Systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Smooth estimation of human foot motion for zero-velocity-update-aided inertial pedestrian navigation system</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alelaiwi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sens. Rev</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="389" to="400" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Analog</forename><surname>Devices</surname></persName>
		</author>
		<ptr target="http://www.analog.com/en/products/sensors-mems/inertial-measurement-units/adis16448.html" />
		<title level="m">ADIS16448</title>
		<imprint>
			<date type="published" when="2018-10-15">15 October 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Oxford</forename><surname>Metrics</surname></persName>
		</author>
		<ptr target="https://www.vicon.com/products/camera-systems/vantage" />
		<title level="m">Vicon Motion Systems</title>
		<imprint>
			<date type="published" when="2018-10">October 2018</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A benchmarking tool for mav visual pose estimation</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Achtelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 11th International Conference on Control Automation Robotics &amp; Vision</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Computational Intelligence Paradigms in Advanced Pattern Classification</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Ogiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Theory of the backpropagation neural network, in: Neural Networks for Perception</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hecht-Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="65" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">pocket or bag: practical activity tracking with mobile phones</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Antos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<author>
			<persName><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><surname>Belt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci. Methods</title>
		<imprint>
			<biblScope unit="volume">231</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
