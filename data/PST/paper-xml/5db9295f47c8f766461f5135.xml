<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Haichao</forename><surname>Zhang</surname></persName>
							<email>hczhang1@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianyu</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Horizon Robotics Baidu Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">33rd Conference on Neural Information Processing Systems</orgName>
								<address>
									<postCode>2019)</postCode>
									<settlement>Vancouver</settlement>
									<region>NeurIPS</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks. Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works. Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach through extensively experiments on different datasets compared with state-of-the-art approaches. Code is available: https://github.com/Haichao-Zhang/FeatureScatter.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While breakthroughs have been made in many fields such as image classification leveraging deep neural networks, these models could be easily fooled by the so call adversarial examples <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b3">4]</ref>. In terms of the image classification, an adversarial example for a natural image is a modified version which is visually indistinguishable from the original but causes the classifier to produce a different label prediction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b23">24]</ref>. Adversarial examples have been shown to be ubiquitous beyond classification, ranging from object detection <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b17">18]</ref> to speech recognition <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. Many encouraging progresses been made towards improving model robustness against adversarial examples under different scenarios <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b70">71]</ref>. Among them, adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref> is one of the most popular technique <ref type="bibr" target="#b1">[2]</ref>, which conducts model training using the adversarially perturbed images in place of the original ones. However, several challenges remain to be addressed. Firstly, some adverse effects such as label leaking is still an issue hindering adversarial training <ref type="bibr" target="#b31">[32]</ref>. Currently available remedies either increase the number of iterations for generating the attacks <ref type="bibr" target="#b35">[36]</ref> or use classes other than the ground-truth for attack generation <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b60">61]</ref>. Increasing the attack iterations will increase the training time proportionally while using non-ground-truth targeted approach cannot fully eliminate label leaking. Secondly, previous approaches for both standard and adversarial training treat each training sample individually and in isolation w.r.t.other samples. Manipulating each sample individually this way neglects the inter-sample relationships and does not fully leverage the potential for attacking and defending, thus limiting the performance.</p><p>Manifold and neighborhood structure have been proven to be effective in capturing the inter-sample relationships <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b21">22]</ref>. Natural images live on a low-dimensional manifold, with the training and testing images as samples from it <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. Modern classifiers are over-complete in terms of parameterizations and different local minima have been shown to be equally effective under the clean image setting <ref type="bibr" target="#b13">[14]</ref>. However, different solution points might leverage different set of features for prediction. For learning a well-performing classifier on natural images, it suffices to simply adjust the classification boundary to intersect with this manifold at locations with good separation between classes on training data, as the test data will largely reside on the same manifold <ref type="bibr" target="#b27">[28]</ref>. However, the classification boundary that extends beyond the manifold is less constrained, contributing to the existence of adversarial examples <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b58">59]</ref>. For examples, it has been pointed out that some clean trained models focus on some discriminative but less robust features, thus are vulnerable to adversarial attacks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Therefore, the conventional supervised attack that tries to move feature points towards this decision boundary is likely to disregard the original data manifold structure. When the decision boundary lies close to the manifold for its out of manifold part, adversarial perturbations lead to a tilting effect on the data manifold <ref type="bibr" target="#b55">[56]</ref>; at places where the classification boundary is far from the manifold for its out of manifold part, the adversarial perturbations will move the points towards the decision boundary, effectively shrinking the data manifold. As the adversarial examples reside in a large, contiguous region and a significant portion of the adversarial subspaces is shared <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b39">40]</ref>, pure label-guided adversarial examples will clutter as least in the shared adversarial subspace. In summary, while these effects encourage the model to focus more around the current decision boundary, they also make the effective data manifold for training deviate from the original one, potentially hindering the performance.</p><p>Motived by these observations, we propose to shift the previous focus on the decision boundary to the inter-sample structure. The proposed approach can be intuitively understood as generating adversarial examples by perturbing the local neighborhood structure in an unsupervised fashion and then performing model training with the generated adversarial images. The overall framework is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The contributions of this work are summarized as follows:</p><p>• we propose a novel feature-scattering approach for generating adversarial images for adversarial training in a collaborative and unsupervised fashion; • we present an adversarial training formulation which deviates from the conventional minimax formulation and falls into a broader category of bilevel optimization; • we analyze the proposed approach and compare it with several state-of-the-art techniques, with extensive experiments on a number of standard benchmarks, verifying its effectiveness.  <ref type="bibr" target="#b54">[55]</ref>. A fast gradient sign method (FGSM) for adversarial attack generation is developed and used in adversarial training in <ref type="bibr" target="#b23">[24]</ref>. Many variants of attacks have been developed later <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref>. In the mean time, many efforts have been devoted to defending against adversarial examples <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Recently, <ref type="bibr" target="#b1">[2]</ref> showed that many existing defence methods suffer from a false sense of robustness against adversarial attacks due to gradient masking, and adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref> is one of the effective defense method against adversarial attacks. It improves model robustness by solving a minimax problem as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>:</p><formula xml:id="formula_0">min θ max x ∈Sx L(x , y; θ)<label>(1)</label></formula><p>where the inner maximization essentially generates attacks while the outer minimization corresponds to minimizing the "adversarial loss" induced by the inner attacks <ref type="bibr" target="#b35">[36]</ref>. The inner maximization can be solved approximately, using for example a one-step approach such as FGSM <ref type="bibr" target="#b23">[24]</ref>, or a multi-step projected gradient descent (PGD) method <ref type="bibr" target="#b35">[36]</ref> x t+1 = P Sx x t + α • sign ∇ x L(x t , y; θ) ,</p><p>where P Sx (•) is a projection operator projecting the input into the feasible region S x . In the PGD approach, the original image x is randomly perturbed to some point x 0 within B(x, ), the -cube around x, and then goes through several PGD steps with a step size of α as shown in Eqn. <ref type="bibr" target="#b1">(2)</ref>.</p><p>Label leaking <ref type="bibr" target="#b31">[32]</ref> and gradient masking <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b1">2]</ref> are some well-known issues that hinder the adversarial training <ref type="bibr" target="#b31">[32]</ref>. Label leaking occurs when the additive perturbation is highly correlated with the ground-truth label. Therefore, when it is added to the image, the network can directly tell the class label by decoding the additive perturbation without relying on the real content of the image, leading to higher adversarial accuracy than the clean image during training. Gradient masking <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b1">2]</ref> refers to the effect that the adversarially trained model learns to "improve" robustness by generating less useful gradients for adversarial attacks, which could be by-passed with a substitute model for generating attacks, thus giving a false sense of robustness <ref type="bibr" target="#b1">[2]</ref>.</p><formula xml:id="formula_2">C T {x i } f ✓ (x i ) {x 0 j } f ✓ (x 0 j ) C ij</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Different Distances for Feature and Distribution Matching</head><p>Euclidean distance is arguably one of the most commonly used metric for measuring the distance between a pair of points. When it comes to two sets of points, it is natural to accumulate the individual pairwise distance as a measure of distance between the two sets, given the proper correspondence. Alternatively, we can view each set as an empirical distribution and measure the distance between them using Kullback-Leibler (KL) or Jensen-Shannon (JS) divergence. The challenge for learning with KL or JS divergence is that no useful gradient is provided when the two empirical distributions have disjoint supports or have a non-empty intersection contained in a set of measure zero <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>The optimal transport (OT) distance is an alternative measure of the distance between distributions with advantages over KL and JS in the scenarios mentioned earlier. The OT distance between two probability measures µ and ν is defined as:</p><formula xml:id="formula_3">D(µ, ν) = inf γ∈Π(µ,ν) E (x,y)∼γ c(x, y) ,<label>(3)</label></formula><p>where Π(µ, ν) denotes the set of all joint distributions γ(x, y) with marginals µ(x) and ν(y), and c(x, y) is the cost function (Euclidean or cosine distance). Intuitively, D(µ, ν) is the minimum cost that γ has to transport from µ to ν. It provides a weaker topology than many other measures, which is important for applications where the data typically resides on a low dimensional manifold of the input embedding space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>, which is the case for natural images. It has been widely applied to many tasks, such as generative modeling <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>, auto-encoding <ref type="bibr" target="#b56">[57]</ref> and dictionary learning <ref type="bibr" target="#b46">[47]</ref>. For comprehensive historical and computational perspective of OT, we refer to <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>3 Feature Scattering-based Adversarial Training While effective, it disregards the inter-relationship between different feature points, as the adversarial perturbation is computed individually for each sample, neglecting any collective distributional property. Furthermore, the supervised generation of the attacks makes the generated perturbations highly biases towards the decision boundary, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. This is less desirable as it might neglect other directions that are crucial for learning robust models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17]</ref> and leads to label leaking due to high correlation between the perturbation and the decision boundary. The idea of leveraging inter-sample relationship for learning dates back to the seminal work of <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>. This type of local structure is also exploited in this work, but for adversarial perturbation. The quest of local structure utilization and seamless integration with the end-to-end-training framework naturally motivates an OT-based soft matching scheme, using the OT-distance as in Eqn. <ref type="bibr" target="#b2">(3)</ref>. We consider OT between discrete distributions hereafter as we mainly focus on applying the OT distance on image features. Specifically, consider two discrete distributions µ, ν ∈ P(X), which can be written as µ = n i=1 u i δ xi and ν = n i=1 v i δ x i , with δ x the Dirac function centered on x. <ref type="foot" target="#foot_0">2</ref> The weight vectors µ={u i } n i=1 ∈∆ n and ν ={v i } n i=1 ∈∆ n belong to the n-dimensional simplex, i.e., i u i = i v i =1, as both µ and ν are probability distributions. Under such a setting, computing the OT distance as defined in Eqn.( <ref type="formula" target="#formula_3">3</ref>) is equivalent to solving the following network-flow problem</p><formula xml:id="formula_4">D(µ, ν) = min T∈Π(u,v) n i=1 n j=1 Tij • c(xi, x j ) = min T∈Π(u,v) T, C<label>(4)</label></formula><p>where</p><formula xml:id="formula_5">Π(u, v) = {T ∈ R n×n + |T1 n = u, T 1 n = v}. 1 n is an n-dimensional all-one vector. •, • represents the Frobenius dot-product. C is the transport cost matrix such that C ij = c(x i , x j ).</formula><p>In this work, the transport cost is defined as the cosine distance between image features:</p><formula xml:id="formula_6">c(xi, x j ) = 1 − f θ (xi) f θ (x j ) f θ (xi) 2 f θ (x j ) 2 = 1 − f i f j fi 2 f j 2<label>(5)</label></formula><p>where f θ (•) denotes the feature extractor with parameter θ. We implement f θ (•) as the deep neural network upto the softmax layer. We can now formally define the feature matching distance as follows. Definition 1. (Feature Matching Distance) The feature matching distance between two set of images is defined as D(µ, ν), the OT distance between empirical distributions µ and ν for the two sets.</p><p>Note that the feature-matching distance is also a function of θ (i.e. D θ ) when f θ (•) is used for extracting the features in the computation of the ground distance as in Eqn. <ref type="bibr" target="#b4">(5)</ref>. We will simply use the notation D in the following when there is no danger of confusion to minimize notional clutter .</p><p>Feature Scattering. Based on the feature matching distance defined above, we can formulate proposed feature scattering method as follows:</p><formula xml:id="formula_7">ν = arg max ν∈Sµ D(µ, ν), µ = n i=1 u i δ xi , ν = n i=1 v i δ x i .<label>(6)</label></formula><p>This can be intuitively interpreted as maximizing the feature matching distance between the original and perturbed empirical distributions with respect to the inputs subject to domain constraints S µ</p><formula xml:id="formula_8">S µ = { i v i δ zi , | z i ∈ B(x i , ) ∩ [0, 255] d },</formula><p>where B(x, ) = {z | z − x ∞ ≤ } denotes the ∞ -cube with center x and radius . Formally, we present the notion of feature scattering as follows. Definition 2. (Feature Scattering) Given a set of clean data {x i }, which can be represented as an empirical distribution as µ = i u i δ xi with i u i = 1, the feature scattering procedure is defined as producing a perturbed empirical distribution ν = i v i δ x i with i v i = 1 by maximizing D(µ, ν), the feature matching distance between µ and ν, subject to domain and budget constraints.</p><p>Remark 1. As the feature scattering is performed on a batch of samples leveraging inter-sample structure, it is more effective as adversarial attacks compared to structure-agnostic random perturbation while is less constrained than supervisedly generated perturbation which is decision boundary oriented and suffers from label leaking. Empirical comparisons will be provided in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Training with Feature Scattering</head><p>We leverage feature scattering for adversarial training, with the mathmatical formulation as follows</p><formula xml:id="formula_9">min θ 1 n n i=1 L θ (x i , y i ) s.t. ν * n i=1 v i δ x i = max ν∈Sµ D(µ, ν).<label>(7)</label></formula><p>The proposed formulation deviates from the conventional minimax formulation for adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. More specifically, it can be regarded as an instance of the more general bilevel optimization problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref>. Feature scattering is effective for adversarial training scenario as there is a requirements of more data <ref type="bibr" target="#b51">[52]</ref>. Feature scattering promotes data diversity without drastically altering the structure of the data manifold as in the conventional supervised approach, with label leaking as one manifesting phenomenon. Secondly, the feature matching distance couples the samples within the batch together, therefore the generated adversarial attacks are produced collaboratively by taking the inter-sample relationship into consideration. Thirdly, feature scattering implicitly induces a coupled regularization (detailed below) on model training, leveraging the inter-sample structure for joint regularization.</p><p>The proposed approach is equivalent to the minimization of a loss,</p><formula xml:id="formula_10">1 n n i=1 L θ (x i , y i ) + λR θ (x 1 , • • • , x n ),</formula><p>consisting of the conventional loss L θ (x i , y i ) on the original data, and a regularization term R θ coupled over the inputs. It first highlights the unique property of the proposed feature scattering approach that it induces an effective regularization term that is coupled over all inputs, i.e., R θ</p><formula xml:id="formula_11">(x 1 , • • • , x n ) = i R θ (x i ).</formula><p>This implies that the model leverages information from all inputs in a joint fashion for learning, offering the opportunity of collaborative regularization leveraging inter-sample relationships. Second, the usage of a function (D θ ) different from L θ for inducing R θ offers more flexibilities in the effective regularization; moreover, no label information is incorporated in D θ , thus avoiding potential label leaking as in the conventional case when ∂L θ (xi,yi) ∂xi is highly correlated with y i . Finally, in the case when D θ is separable over inputs and takes the form of a supervised loss, e.g., D θ ≡ i L θ (x i , y i ), the proposed approach reduces to the conventional adversarial training setup <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b35">36]</ref>. The overall procedure for the proposed approach is in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Feature Scattering-based Adversarial Training</head><p>Input: dataset S, training epochs K, batch size n, learning rate γ, budget , attack iterations T for k = 1 to K do for random batch {x i , y i } n i=1 ∼S do initialization:</p><formula xml:id="formula_12">µ = i u i δ xi , ν = i v i δ x i , x i ∼ B(x i , ) feature scattering (maximizing feature matching distance D w.r.t. ν): for t = 1 to T do • x i ← P Sx x i + • sign ∇ x i D(µ, ν) ∀i = 1, • • • , n, ν = i v i δ x i end for adversarial training (updating model parameters): • θ ← θ − γ • 1 n n i=1 ∇ θ L(x i , y i ; θ)</formula><p>end for end for Output: model parameter θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussions</head><p>Manifold-based Defense <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27]</ref> proposed to defend by projecting the perturbed image onto a proper manifold. <ref type="bibr" target="#b14">[15]</ref> used a similar idea of manifold projection but approximated this step with a nearest neighbor search against a web-scale database. Differently, we leverage the manifold in the form of inter-sample relationship for the generation of the perturbations, which induces an implicit regularization of the model when used in the adversarial training framework. While defense in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref> is achieved by shrinking the perturbed inputs towards the manifold, we expand the manifold using feature scattering to generate perturbed inputs for adversarial training.</p><p>Inter-sample Regularization <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref>. Mixup <ref type="bibr" target="#b69">[70]</ref> generates training examples by linear interpolation between pairs of natural examples, thus introducing an linear inductive bias in the vicinity of training samples. Therefore, the model is expected to reduce the amount of undesirable oscillations for off-manifold samples. Logit pairing <ref type="bibr" target="#b29">[30]</ref> augments the original training loss with a "pairing" loss, which measures the difference between the logits of clean and adversarial images. The idea is to suppress spurious logits responses using the natural logits as a reference. Similarly, virtual adversarial training <ref type="bibr" target="#b38">[39]</ref> proposed a regularization term based on the KL divergence of the prediction probability of original and adversarially perturbed images. In our model, the inter-sample relationship is leveraged for generating the adversarial perturbations, which induces an implicit regularization term in the objective function that is coupled over all input samples. Wasserstein GAN and OT-GAN <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b9">10]</ref>. Generative Adversarial Networks (GAN) is a family of techniques that learn to capture the data distribution implicitly by generating samples directly <ref type="bibr" target="#b22">[23]</ref>. It originally suffers from the issues of instability of training and mode collapsing <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>. OTrelated distances <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref> have been used for overcoming the difficulties encountered in the original GAN training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49]</ref>. This technique has been further extended to generating discrete data such as texts <ref type="bibr" target="#b9">[10]</ref>. Different from GANs, which maximizes a discrimination criteria w.r.t.the parameters of the discriminator for better capturing the data distribution, we maximize a feature matching distance w.r.t.the perturbed inputs for generating proper training data to improve model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Baselines and Implementation Details. Our implementation is based on PyTorch and the code as well as other related resources are available on the project page. <ref type="foot" target="#foot_1">3</ref> We conduct extensive experiments across several benchmark datasets including CIFAR10 <ref type="bibr" target="#b30">[31]</ref>, CIFAR100 <ref type="bibr" target="#b30">[31]</ref> and SVHN <ref type="bibr" target="#b41">[42]</ref>. We use Wide ResNet (WRN-28-10) <ref type="bibr" target="#b67">[68]</ref> as the network structure following <ref type="bibr" target="#b35">[36]</ref>. We compare the performance of the proposed method with a number of baseline methods, including: i) the model trained with standard approach using clean images (Standard) <ref type="bibr" target="#b30">[31]</ref>, ii) PGD-based approach from Madry et al. <ref type="bibr">(Madry)</ref>  <ref type="bibr" target="#b35">[36]</ref>, which is one of the most effective defense method <ref type="bibr" target="#b1">[2]</ref>, iii) another recent method performs adversarial training with both image and label adversarial perturbations (Bilateral) <ref type="bibr" target="#b60">[61]</ref>. For training, the initial learning rate γ is 0.1 for CIFAR and 0.01 for SVHN. We set the number of epochs the Standard and Madry methods as 100 with transition epochs as {60, 90} as we empirically observed the performance of the trained model stabilized before 100 epochs. The training scheduling of 200 epochs similar to <ref type="bibr" target="#b60">[61]</ref> with the same transition epochs used as we empirically observed it helps with the model performance, possibly due to the increased variations of data via feature scattering. We performed standard data augmentation including random crops with 4 pixels of padding and random horizontal flips <ref type="bibr" target="#b30">[31]</ref> during training. The perturbation budget of = 8 is used in training following literature <ref type="bibr" target="#b35">[36]</ref>. Label smoothing of 0.5, attack iteration T=1 and Sinkhorn algorithm <ref type="bibr" target="#b11">[12]</ref> with regularization of 0.01 is used. For testing, model robustness is evaluated by approximately computing an upper bound of robustness on the test set, by measuring the accuracy of the model under different adversarial attacks, including white-box FGSM <ref type="bibr" target="#b23">[24]</ref>, PGD <ref type="bibr" target="#b35">[36]</ref>, CW <ref type="bibr" target="#b7">[8]</ref> (CW-loss <ref type="bibr" target="#b7">[8]</ref> within the PGD framework) attacks and variants of black-box attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Visual Classification Performance Under White-box Attacks</head><p>CIFAR10. We conduct experiments on CIFAR10 <ref type="bibr" target="#b30">[31]</ref>, which is a popular dataset that is widely use in adversarial training literature <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b60">61]</ref> with 10 classes, 5K training images per class and 10K test images. We report the accuracy on the original test images (Clean) and under PGD and CW attack with T iterations (PGDT and CWT ) <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8]</ref>. The evaluation results are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>It is observed Standard model fails drastically under different white-box attacks. Madry method improves the model robustness significantly over the Standard model. Under the standard PGD20 attack, it achieves 44.9% accuracy. The Bilateral approach further boosts the performance to 57.5%. The proposed approach outperforms both methods by a large margin, improving over Madry by 25.6%, and is 13.0% better than Bilateral, achieving 70.5% accuracy under the standard 20 steps PGD attack. Similar patten has been observed for CW metric. We further evaluate model robustness against PGD attacker under different attack budgets with a fixed attack step of 20, with the results shown in Figure <ref type="figure" target="#fig_3">3</ref>    <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> methods on CIFAR10 under different threat models.</p><p>further boosts the performance over the Madry model <ref type="bibr" target="#b35">[36]</ref> by a large margin under different attack budgets. We also conduct experiments using PGD attacker with different attack iterations with a fixed attack budget of 8, with the results shown in Figure <ref type="figure" target="#fig_3">3 (b-c</ref>) and also Table <ref type="table" target="#tab_1">1</ref>. It is observed that both Madry <ref type="bibr" target="#b35">[36]</ref> and Proposed can maintain a fairly stable performance when the number of attack iterations is increased. Notably, the proposed approach consistently outperforms the Madry <ref type="bibr" target="#b35">[36]</ref> model across a wide range of attack iterations. From Table <ref type="table" target="#tab_1">1</ref>, it is also observed that the Proposed approach also outperforms Bilateral <ref type="bibr" target="#b60">[61]</ref> under all variants of PGD and CW attacks. We will use a PGD/CW attackers with =8 and attack step 20 and 100 in the sequel as part of the threat models.  SVHN. We further report results on the SVHN dataset <ref type="bibr" target="#b41">[42]</ref>. SVHN is a 10-way house number classification dataset, with 73257 training images and 26032 test images. The additional training images are not used in experiment. The results are summarized in Table <ref type="table" target="#tab_3">2</ref>(a). Experimental results show that the proposed method achieves the best clean accuracy among all three robust models and outperforms other method with a clear margin under both PGD and CW attacks with different number of attack iterations, demonstrating the effectiveness of the proposed approach. CIFAR100. We also conduct experiments on CIFAR100 dataset, with 100 classes, 50K training and 10K test images <ref type="bibr" target="#b30">[31]</ref>. Note that this dataset is more challenging than CIFAR10 as the number of training images per class is ten times smaller than that of CIFAR10. As shown by the results in Table <ref type="table" target="#tab_3">2</ref>(b), the proposed approach outperforms all baseline methods significantly, which is about 20% better than Madry <ref type="bibr" target="#b35">[36]</ref> and Bilateral <ref type="bibr" target="#b60">[61]</ref> under PGD attack and about 10% better under CW attack. The superior performance of the proposed approach on this data set further demonstrates the importance of leveraging inter-sample structure for learning <ref type="bibr" target="#b68">[69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Studies</head><p>We investigate the impacts of algorithmic components and more results are in the supplementary file. The Importance of Feature Scattering. We empirically verify the effectiveness of feature scattering, by comparing the performances of models trained using different perturbation schemes: i) Random: a natural baseline approach that randomly perturb each sample within the epsilon neighborhood; ii) Supervised: perturbation generated using ground-truth label in a supervised fashion; iii) FeaScatter: perturbation generated using the proposed feature scattering method. All other hyper-parameters are kept exactly the same other than the perturbation scheme used. The results are summarized in Table <ref type="table">3</ref>(a). It is evident that the proposed feature scattering (FeaScatter) approach The Role of Matching. We further investigate the role of matching schemes within the feature scattering component by comparing several different schemes: i) Uniform matching, which matches each clean sample uniformly with all perturbed samples in the batch; ii) Identity matching, which matches each clean sample to its perturbed sample only; iii) OT-matching: the proposed approach that assigns soft matches between the clean samples and perturbed samples according to the optimization criteria. The results are summarized in Table <ref type="table">3</ref>(b). It is observed all variants of matching schemes lead to performances that are on par or better than state-of-the-art methods, implying that the proposed framework is effective in general. Notably, OT-matching leads to the best results, suggesting the importance of the proper matching for feature scattering.</p><p>The Impact of OT-Solvers. Exact minimization of Eqn.(4) over T is intractable in general <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12]</ref>. Here we compare two practical solvers, the Sinkhorn algorithm <ref type="bibr" target="#b11">[12]</ref> and the Inexact Proximal point method for Optimal Transport (IPOT) algorithm <ref type="bibr" target="#b65">[66]</ref>. More details on them can be found in the supplementary file and <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b44">45]</ref>. The results are summarized in Table <ref type="table" target="#tab_4">4</ref>. It is shown that different instantiations of the proposed approach with different OT-solvers lead to comparable performances, implying that the proposed approach is effective in general regardless of the choice of OT-solvers.</p><p>OT-solver CIFAR10 SVHN CIFAR100</p><p>Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100 Clean FGSM PGD20 PGD100 CW20 CW100 Sinkhorn 90.0 78. <ref type="bibr" target="#b3">4</ref>  To further verify if a degenerate minimum is obtained, we evaluate the robustness of the model trained with the proposed approach w.r.t.black-box attacks (B-Attack) following <ref type="bibr" target="#b57">[58]</ref>. Two different models are used for generating test time attacks: i) Undefended: undefended model trained using Standard approach, ii) Siamese: a robust model from another training session using the proposed approach. As demonstrated by the results in the table on the right, the model trained with the proposed approach is robust against different types of black-box attacks, verifying that a non-degenerate solution is learned <ref type="bibr" target="#b57">[58]</ref>. Finally, we visualize in Figure <ref type="figure" target="#fig_4">4</ref> the loss surfaces of different models as another level of comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We present a feature scattering-based adversarial training method in this paper. The proposed approach distinguish itself from others by using an unsupervised feature-scattering approach for generating adversarial training images, which leverages the inter-sample relationship for collaborative perturbation generation. We show that a coupled regularization term is induced from feature scattering for adversarial training and empirically demonstrate the effectiveness of the proposed approach through extensive experiments on benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Feature Scattering-based Adversarial Training Pipeline. The adversarial perturbations are generated collectively by feature scattering, i.e., maximizing the feature matching distance between the clean samples {x i } and the perturbed samples {x j }. The model parameters are updated by minimizing the cross-entropy loss using the perturbed images {x j } as the training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>3. 1 Feature</head><label>1</label><figDesc>Matching and Feature Scattering Feature Matching. Conventional training treats training data as i.i.d samples from a data distribution, overlooking the connections between samples. The same assumption is used when generating adversarial examples for training, with the direction for perturbing a sample purely based on the direction from the current data point to the decision boundary, regardless of other samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration Example of Different Perturbation Schemes. (a) Original data. Perturbed data using (b) supervised adversarial generation method and (c) the proposed feature scattering, which is an unsupervised method. The overlaid boundary is from the model trained on clean data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Model performance under PGD attack with different (a) attack budgets (b) attack iterations. Madry and Proposed models are trained with the attack iteration of 7 and 1 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Loss surface visualization in the vicinity of a natural image along adversarial direction (d a ) and direction of a Rademacher vector (d r ) for (a) Standard (b) Madry (c) Proposed models. outperforms both Random and Supervised methods, demonstrating its effectiveness. Furthermore, as it is the major component that is difference from the conventional adversarial training pipeline, this result suggests that feature scattering is the main contributor to the improved adversarial robustness. Perturb Clean White-box Attack ( = 8) FGSM PGD20 PGD100 CW20 CW100 Random 95.3 75.7 29.9 18.3 34.7 26.2 Supervised 86.9 64.4 56.0 54.5 51.2 50.3 FeaScatter 90.0 78.4 70.5 68.6 62.4 60.6</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Accuracy comparison of the Proposed approach with Standard, Madry</figDesc><table><row><cell>Models</cell><cell>Clean</cell><cell cols="9">Accuracy under White-box Attack ( = 8) FGSM PGD10 PGD20 PGD40 PGD100 CW10 CW20 CW40 CW100</cell></row><row><cell>Standard</cell><cell>95.6</cell><cell>36.9</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell></row><row><cell>Madry</cell><cell>85.7</cell><cell>54.9</cell><cell>45.1</cell><cell>44.9</cell><cell>44.8</cell><cell>44.8</cell><cell>45.9</cell><cell>45.7</cell><cell>45.6</cell><cell>45.4</cell></row><row><cell>Bilateral</cell><cell>91.2</cell><cell>70.7</cell><cell>-</cell><cell>57.5</cell><cell>-</cell><cell>55.2</cell><cell>-</cell><cell>56.2</cell><cell>-</cell><cell>53.8</cell></row><row><cell>Proposed</cell><cell>90.0</cell><cell>78.4</cell><cell>70.9</cell><cell>70.5</cell><cell>70.3</cell><cell>68.6</cell><cell>62.6</cell><cell>62.4</cell><cell>62.1</cell><cell>60.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparison on (a) SVHN and (b) CIFAR100.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>70.5 68.6 62.4 60.6 96.2 83.5 62.9 52.0 61.3 50.8 73.9 61.0 47.2 46.2 34.6 30.6 IPOT 89.9 77.9 69.9 67.3 59.6 56.9 96.0 82.6 60.0 49.3 57.8 48.4 74.2 67.3 47.5 46.3 32.0 29.3 Impacts of OT-solvers. The proposed approach performs well with different OT-solvers. 5.3 Performance under Black-box Attack</figDesc><table><row><cell cols="3">B-Attack PGD20 PGD100 CW20 CW100</cell></row><row><cell cols="2">Undefended 89.0</cell><cell>88.7 88.9 88.8</cell></row><row><cell>Siamese</cell><cell>81.6</cell><cell>81.0 80.3 79.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">The two discrete distributions could be of different dimensions; here we present the exposition assuming the same dimensionality to avoid notion clutter.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://sites.google.com/site/hczhang1/projects/feature_scattering</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Practical Bilevel Optimization: Algorithms and Applications</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evasion attacks against machine learning at test time</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Corona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maiorca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srndic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giacinto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wild patterns: Ten years after the rise of adversarial machine learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</author>
		<idno>CoRR, abs/1712.03141</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Decision-based adversarial attacks: Reliable attacks against black-box machine learning models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<idno>CoRR, abs/1712.09665</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Audio adversarial examples: Targeted attacks on speech-to-text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy Workshops</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial text generation via feature-mover&apos;s distance</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Houdini: Fooling deep structured prediction models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Keshet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bilevel Programming Problems: Theory, Algorithms and Applications to Energy Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dempe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Prez-Valds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalashnykova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Essentially no barriers in neural network energy landscape</title>
		<author>
			<persName><forename type="first">F</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veschgini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salmhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Defense against adversarial images using web-scale nearest-neighbor search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yalniz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<idno>CoRR, abs/1903.01612</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the connection between adversarial robustness and saliency map interpretability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Etmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lunz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-B</forename><surname>Schönlieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Physical adversarial examples for object detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eykholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Evtimov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahmati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno>CoRR, abs/1807.07769</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical study of the topology and geometry of deep networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">GAN and VAE from an optimal transport point of view</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01807</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Countering adversarial images using input transformations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cissé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The robust manifold defense: Adversarial training using generative models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Asteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno>CoRR, abs/1712.09196</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial examples are not bugs, they are features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Excessive invariance causes adversarial vulnerability</title>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial logit pairing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno>CoRR, abs/1803.06373</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Defense against adversarial attacks using high-level representation guided denoiser</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">AutoGAN: Robust classifier against adversarial attacks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lindqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sugrim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Izmailov</surname></persName>
		</author>
		<idno>CoRR, abs/1812.03405</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards robust neural networks via random self-ensemble</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">MagNet: a two-pronged defense against adversarial examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security</title>
				<meeting>the 2017 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On detecting adversarial perturbations</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno>CoRR, abs/1704.03976</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
				<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The limitations of deep learning in adversarial settings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fredrikson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<idno>CoRR, abs/1511.07528</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representing and learning high dimensional data with the optimal transport map from a probabilistic viewpoint</title>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Thorpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<title level="m">Computational optimal transport. to appear in Foundations and Trends in Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast dictionary learning with a smoothed Wasserstein loss</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Improving GANs using optimal transport</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Defense-GAN: Protecting classifiers against adversarial attacks using generative models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Samangouei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kabkab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Think globally, fit locally: Unsupervised learning of low dimensional manifolds</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="119" to="155" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11285</idno>
		<title level="m">Adversarially robust generalization requires more data</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Pixeldefend: Leveraging generative models to understand and defend against adversarial examples</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kushman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10766</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakurai</surname></persName>
		</author>
		<idno>CoRR, abs/1710.08864</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">A boundary tilting persepective on the phenomenon of adversarial examples</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Griffin</surname></persName>
		</author>
		<idno>CoRR, abs/1608.07690</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Ensemble adversarial training: Attacks and defenses</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">The space of transferable adversarial examples</title>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno>CoRR, abs/1704.03453</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Optimal transport, old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Bilateral adversarial training: Towards fast training of more robust models against adversarial attacks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Generating adversarial examples with adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Mitigating adversarial effects through randomization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03411</idno>
		<title level="m">Feature denoising for improving adversarial robustness</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">A fast proximal point method for Wasserstein distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04307</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep defense: Training dnns with improved adversarial robustness</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">The limitations of adversarial training and the blind-spot attack</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Joint adversarial training: Incorporating both spatial and pixel attacks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1907.10737</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Towards adversarially robust object detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
