<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuchao</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences https://github.com/TencentARC</orgName>
								<address>
									<country>VQFR</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences https://github.com/TencentARC</orgName>
								<address>
									<country>VQFR</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Platform Technologies</orgName>
								<orgName type="laboratory">Laboratory</orgName>
								<address>
									<addrLine>Tencent Online Video 4 Shanghai AI</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<country>Tencent PCG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">TMCC</orgName>
								<orgName type="institution" key="instit2">Nankai University</orgName>
								<address>
									<region>CS</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Blind Face Restoration</term>
					<term>Vector Quantization</term>
					<term>Parallel Decoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although generative facial prior and geometric prior have recently demonstrated high-quality results for blind face restoration, producing fine-grained facial details faithful to inputs remains a challenging problem. Motivated by the classical dictionary-based methods and the recent vector quantization (VQ) technique, we propose a VQ-based face restoration method -VQFR. VQFR takes advantage of high-quality low-level feature banks extracted from high-quality faces and can thus help recover realistic facial details. However, the simple application of the VQ codebook cannot achieve good results with faithful details and identity preservation. Therefore, we further introduce two special network designs. 1). We first investigate the compression patch size in the VQ codebook and find that the VQ codebook designed with a proper compression patch size is crucial to balance the quality and fidelity. 2). To further fuse low-level features from inputs while not "contaminating" the realistic details generated from the VQ codebook, we proposed a parallel decoder consisting of a texture decoder and a main decoder. Those two decoders then interact with a texture warping module with deformable convolution. Equipped with the VQ codebook as a facial detail dictionary and the parallel decoder design, the proposed VQFR can largely enhance the restored quality of facial details while keeping the fidelity to previous methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Blind face restoration aims at recovering low-quality (LQ) faces with unknown degradations, such as noise <ref type="bibr" target="#b39">[40]</ref>, blur <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29]</ref>, down-sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref>, etc. This task becomes more challenging in real-world scenarios, where there are more complicated degradations, diverse face poses and expressions. Previous works typically exploit face-specific priors, including geometric priors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>, generative priors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> and reference priors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref>. Specifically, geometric priors usually consist of facial landmarks <ref type="bibr" target="#b5">[6]</ref>, face parsing maps <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> and facial component heatmaps <ref type="bibr" target="#b38">[39]</ref>. They could provide global guidance for restoring accurate face shapes, but do not help generate realistic details. Fig. <ref type="figure">1</ref>: Comparisons of restoration quality between GFP-GAN <ref type="bibr" target="#b32">[33]</ref> and VQFR. Our VQFR can restore high-quality facial details on various facial regions and keep the fidelity as well, while GFP-GAN lacks realistic fine details. (Zoom in for best view)</p><p>Besides, geometric priors are estimated from degraded images and thus become inaccurate for inputs with severe degradations. These properties motivate researchers to find better priors.</p><p>Recent works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> begin to investigate generative priors in face restoration and achieve superior performance. They usually leverage the powerful generation ability of a pre-trained face generative adversarial network (e.g., StyleGAN <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>) to generate realistic textures. These methods typically project the degraded images back into the GAN latent space, and then decode high-quality (HQ) faces with the pre-trained generator. Although GAN-prior-based methods achieve decent overall restoration quality at first glance, they still fail to produce fine-grained facial details, especially the fine hair and delicate facial components (see examples in Fig. <ref type="figure">1</ref>). This can be partially attributed to the imperfect latent space of the well-trained GAN model. Reference-based methods explore the high-quality guided faces <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> or facial component dictionary <ref type="bibr" target="#b22">[23]</ref> to solve face restoration problems. DFDNet <ref type="bibr" target="#b22">[23]</ref> is a representative method, which does not need to access the faces of the same identity. It explicitly establishes a high-quality "texture bank" for several facial components and then replaces degraded facial components with the nearest HQ facial components in the dictionary. Such a discrete replacement operation directly bridges the gap between the low-quality facial components and high-quality ones, thus having the potential to provide decent facial details. However, the facial component dictionary in DFDNet still has two weaknesses. 1) It offline generates the facial component dictionary with a pre-trained VGGFace <ref type="bibr" target="#b2">[3]</ref> network, which is optimized for the recognition task but is sub-optimal for restoration. 2) It only focuses on several facial components (i.e., eyes, nose, and mouth), but does not include other important areas, such as hair and skin.</p><p>The limitations of the facial component dictionary motivate us to explore Vector Quantized (VQ) codebook, a dictionary constructed for all facial areas. The proposed face restoration method -VQFR, takes advantage of both dictionary-based methods and GAN training, yet does not require any geometric or GAN prior. Compared to the facial component dictionary <ref type="bibr" target="#b22">[23]</ref>, the VQ codebook could provide a more comprehensive lowlevel feature bank that is not restricted to limited facial components. It is also learned in an end-to-end manner by the face reconstruction task. Besides, the mechanism of vector quantization makes it more robust for diverse degradations. Nevertheless, it is not easy to achieve good results simply by applying the VQ codebook. We further introduce two special network designs, which allow VQFR to surpass previous methods in both detail generation and identity preserving.</p><p>First, to generate realistic details, we find that it is crucial to select a proper compression patch size f , which indicates "how large a patch is represented" by an atom of the codebook. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, a larger f could lead to better visual quality but worse fidelity. After a comprehensive investigation, we suggest using f = 32 for the input image size 512 × 512. However, such a selection is only a trade-off between quality and fidelity. The expression and identity could also change a lot even with a proper compression patch size (see Fig. <ref type="figure" target="#fig_1">2</ref>). A straightforward solution is to fuse low-level features from input into different decoder layers, just like in GFP-GAN <ref type="bibr" target="#b32">[33]</ref>. Although the input features could bring more fidelity information, they will also "contaminate" the realistic details generated from the VQ codebook. This problem leads to our second network design -a parallel decoder. Specifically, the parallel decoder structure consists of a texture decoder and a main decoder. The texture decoder only receives information from the latent representations from the VQ codebook, while the main decoder warps the features from the texture decoder to match the information from degraded input. In order to eliminate the loss of high-quality details and better match the degraded faces, we further adopt a texture warping module with deformable convolution <ref type="bibr" target="#b42">[43]</ref> in the main decoder. Equipped with the VQ codebook as a facial dictionary and the parallel decoder design, we can achieve more high-quality facial details while reserving the fidelity for face restoration.</p><p>Our contributions are summarized as follows:</p><p>1. We propose the VQ dictionary of HQ facial details for face restoration. Our analysis of the VQ codebook shows the potential and limitations of the VQ codebook, together with the importance of the compression patch sizes in face restoration. 2. A parallel decoder is proposed to gradually fuse input features and texture features from VQ codebooks, which keeps the fidelity without sacrificing HQ facial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Extensive experiments with quantitative and qualitative comparisons show VQFR</head><p>largely surpasses previous works in restoration quality while keeping high fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Blind Face Restoration Early works explore different facial priors in face restoration. Those priors can be categorized into three types: geometric priors <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b4">5]</ref>, generative priors <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref> and reference priors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref>. The geometric priors include facial landmark <ref type="bibr" target="#b5">[6]</ref>, face parsing maps <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> and facial component heatmaps <ref type="bibr" target="#b38">[39]</ref>. Those priors are estimated from degraded images and thus become inaccurate for inputs with severe degradations. Besides, the geometric structures cannot provide sufficient information to recover facial details.</p><p>In this work, we do not explicitly integrate geometric priors, but we use landmark distance to estimate restoration fidelity.</p><p>Recent works investigate the generative priors to provide facial details and achieve decent performance. In the early arts, GAN inversion methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14]</ref> aim to find the closest latent vector in the GAN space given an input image. Within this category, PULSE <ref type="bibr" target="#b25">[26]</ref> iteratively optimizes the latent code of a pre-trained StyleGAN <ref type="bibr" target="#b17">[18]</ref>. mGANprior <ref type="bibr" target="#b13">[14]</ref> simultaneously optimize several codes to promote its reconstruction. Recent works GFP-GAN <ref type="bibr" target="#b32">[33]</ref> and GPEN <ref type="bibr" target="#b36">[37]</ref> extract fidelity information from inputs and then leverage the pre-trained GAN as a decoder, which achieves a good balance between visual quality and fidelity. Those methods still fail to produce fine-grained facial details. We conjecture that StyleGAN constructs a continuous latent space, and thus GAN-prior methods easily project degraded faces into a suboptimal latent code.</p><p>Reference priors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b7">8]</ref> typically rely on reference images of the same identity. To address this issue, DFDNet <ref type="bibr" target="#b22">[23]</ref> constructs an offline facial component dictionary <ref type="bibr" target="#b22">[23]</ref> with VGGFace <ref type="bibr" target="#b2">[3]</ref> for face recognition. It then conducts discrete replacement, i.e., replacing the degraded facial components with high-quality ones in the dictionary by the nearest search. With the facial component dictionary, DFDNet <ref type="bibr" target="#b22">[23]</ref> restores better facial components. However, it still has two limitations: 1) The dictionary is offline generated with a recognition model, which is sub-optimal for face restoration. 2) It only builds on several facial components (i.e., eyes, nose and mouth), leaving other facial regions like skin and hair untouched. In this work, we explore the VQ codebook as a facial dictionary, which can be end-to-end trained by a reconstruction objective and provide realistic facial details. Recent RestoreFormer <ref type="bibr" target="#b33">[34]</ref> also exploits the VQ codebook, but their work mainly discusses the diverse cross-attention mechanism for LQ latent and HQ code interaction. It does not explore deeply in VQ codebook while we show the dilemma between fidelity and realness when using the VQ codebook of different scales. Moreover, restoreformer does not exploit the input feature of larger resolution and thus results in limited restoration fidelity. Instead, we explore the parallel decoder to achieve realness and fidelity simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vector-Quantized Codebook</head><p>The vector-quantized codebook is first introduced in VQ-VAE <ref type="bibr" target="#b30">[31]</ref>. With this codebook, the encoder network outputs are discrete rather than continuous, and the prior encapsulated in the codebook is learned rather than static. The following works propose different improvements to codebook learning. VQVAE2 <ref type="bibr" target="#b27">[28]</ref> introduces a multiscale codebook for better image generation. VQGAN <ref type="bibr" target="#b10">[11]</ref> trains the codebook with the adversarial objective and thus the codebook can achieve high perceptual quality. To improve the codebook usage, some works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38]</ref> explore training techniques like L2normalization or periodically re-initialization. Such a VQ codebook is a patch tokenizer and can be adopted in multiple tasks, like image generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b10">11]</ref>, multi-modal generation <ref type="bibr" target="#b34">[35]</ref> and large-scale pretraining <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b0">1]</ref>. Different from previous works that use the VQ codebook to get token features, we explore the potential of the VQ codebook as an HQ facial details dictionary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We describe the proposed VQFR framework in this section. Our goal is to restore highquality faces with realistic facial details while reserving the fidelity of degraded faces.</p><p>To achieve this goal, VQFR explores two key ingredients: Vector-Quantized (VQ) dictionary and parallel decoder. The overview of VQFR framework is illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>. VQFR contains an encoder, a parallel decoder and a pretrained HQ codebook. We first learn a VQ codebook from only HQ faces with an encoder-decoder structure by the vector-quantization technique <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b10">11]</ref>. Then, a degraded face is encoded to a compact latent representation with a downsampling factor f . At each spatial location of this latent representation, we replace the latent vector with its nearest code in the HQ codebook. After that, the substituted latent representation is then decoded back to the image space, i.e., restored image with high-quality facial details.</p><p>Vector Quantization</p><formula xml:id="formula_0">0 1 N-1 N-2 Codebook 0 1 … N-1 N-2 0.4 0.1 … 0.6 0.9</formula><p>Code Distance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vector-Quantized Codebook</head><p>Preliminary. The Vector-Quantized (VQ) codebook is first introduced in VQVAE <ref type="bibr" target="#b30">[31]</ref>, which aims to learn discrete priors to encode images. The following work VQGAN <ref type="bibr" target="#b10">[11]</ref> proposes a perceptual codebook by further using perceptual loss <ref type="bibr" target="#b16">[17]</ref> and adversarial training objectives <ref type="bibr" target="#b15">[16]</ref>. We briefly describe the VQGAN model with its codebook in this section, and more details can be found in <ref type="bibr" target="#b10">[11]</ref>. VQGAN is comprised of an encoder E, a decoder G and a codebook Z = {z k } K k=1 with K discrete codes. For an input image x ∈ R H×W ×3 , the encoder E maps the image x to its spatial latent representation ẑ = E(x) ∈ R h×w×nz , where n z is the dimension of latent vectors. The vector-quantized representation z q is then obtained by applying element-wise quantization q(•) of each spatial code ẑij ∈ R nz onto its closest codebook entry z k :</p><formula xml:id="formula_1">z q = q(ẑ) := arg min z k ∈Z ∥ẑ ij − z k ∥ ∈ R h×w×nz .<label>(1)</label></formula><p>The decoder G maps the quantized representation z q back to the image space, and the overall reconstruction x ≈ x can be formulated as:</p><formula xml:id="formula_2">x = G(z q ) = G (q(E(x))) .<label>(2)</label></formula><p>The encoder E maps images of size H ×W into discrete codes of size H/f ×W/f , where f denotes the downsampling factor. It can be regarded as compressing each f ×f patch in the image x into one code. In other words, for each code in z q , f also denotes the corresponding spatial size in the original image x. We name this downsampling factor f as the compression patch size in our paper.</p><p>Since the quantization operation in Equ. 2 is discrete and non-differentiable, VQ-GAN adopts straight-through gradient estimator <ref type="bibr" target="#b1">[2]</ref>, which simply copies the gradients from the decoder to the encoder. Thus, the model and codebook can be trained end-toend via the loss function L vq . VQGAN also employs perceptual loss and adversarial loss to encourage reconstructions with better perceptual quality. The full training objective of VQGAN and its codebook is: </p><formula xml:id="formula_3">L(E, G, Z) = ∥x − x∥ 1 + ∥sg[E(x)] − z q ∥ 2 2 + β∥sg[z q ] − E(x)∥ 2 2 Lvq +L per + L adv ,<label>(3)</label></formula><formula xml:id="formula_4">[E(x)] − z q ∥ 2 2 . β∥sg[z q ] − E(x)∥ 2 2</formula><p>is the commitment loss <ref type="bibr" target="#b30">[31]</ref> to reduce the discrepancy between the encoded latent vectors and codes, where β is the commitment weight and set to 0.25 in all experiments.</p><p>Analysis In order to better understand the potential and limitations of VQ codebooks for face restoration, we conduct several preliminary experiments and draw the following observations.</p><p>Observation 1: Degradations in LQ faces can be removed by VQ codebooks trained only with HQ faces, when we adopt a proper compression patch size f . Following <ref type="bibr" target="#b10">[11]</ref> and Equ. 3, we first train VQGAN with perceptual codebooks on HQ faces using different compression patch sizes f = {8, 16, 32, 64}. All our experiments are conducted on 512 × 512 input faces. The illustration of training architectures is shown in Fig. <ref type="figure" target="#fig_4">4</ref>. We then examine the reconstruction quality for different compression patch sizes f . Note that the reconstruction output is expected to be the same as the input. As shown in row 1 of Fig. <ref type="figure" target="#fig_1">2</ref>, the reconstruction quality of HQ faces is as expected. The reconstruction quality of HQ faces exhibits a reasonable trend: a smaller compression patch size f will lead to more faithful outputs. After that, we are curious whether the codebook trained only on HQ faces can also reconstruct LQ faces. We use LQ faces as inputs and examine the outputs. Interestingly, we can observe that under the compression patch sizes f = 32, the degradation in LQ faces can be removed (see row 2 in Fig. <ref type="figure" target="#fig_1">2</ref>). This is because the codebook trained only on HQ faces has almost no degradation-related codes. During reconstruction, the vector quantization operation can replace the "degraded" vectors of inputs with the "clean" codes in codebooks.</p><p>Such a phenomenon shows the potential of the VQ codebook. However, it only happens with a large compression patch size f , as the codebook with a small compression patch size cannot well distinguish the degradations and detailed textures. In the extreme, for f = 1 with each pixel having a quantized code, both the degradation and detailed textures can be well recovered by a combination of codebook entries. On the other hand, a too large compression patch size (e.g., f = 64) will result in a significant change in identity, even it could also reconstruct "clean" face images.</p><p>Observation 2: When training for the restoration task, there is also a trade-off between improved detailed textures and fidelity changes. We then investigate the VQ codebooks in face restoration, i.e., training with LQ-HQ pairs as most face restoration works do <ref type="bibr" target="#b32">[33]</ref>. Based on the trained model for reconstruction, we then fix the VQ codebooks, and finetune the encoder and decoder with LQ-HQ pairs (the training details are the same as that in Sec. 4.1). We denote this simple VQ model for face restoration as SimVQFR. We can observe from Fig. <ref type="figure" target="#fig_1">2</ref> that there is still a trade-off between improved detailed textures and fidelity changes. The key influential factor is the compression patch size f . With a small f (i.e., {8, 16}), the SimVQFR model fails to remove degradations and cannot recover sufficient detailed textures. While with a large f (i.e., {32, 64}), the textures are largely improved but the fidelity (i.e., expression and identity) also changes a lot by the codebook. Our choice. Based on the above analysis, we can conclude that the VQ codebook, as a texture dictionary, has its value in generating high-quality facial textures and removing input degradations. However, there is a trade-off between the improved detailed textures and fidelity changes. The key influential factor is the compression patch size f . In order to better leverage the strength of generating high-quality facial textures, we choose the compression patch size f = 32 for 512 × 512 input faces. The left problem is how to preserve the fidelity with the VQ codebook of f = 32. We will present our parallel decoder solution to address this problem in Sec. 3.2.</p><formula xml:id="formula_5">𝒒 𝒒 𝒒 𝒒 … … … … … … … … … … … 𝒒 Fixed Params VQ Codebook Encoder Texture Decoder Main Decoder (a) VQGAN (b) SimVQFR (c) Single Branch (d) VQFR x x x d x r x d x r x d x m x t f = 1 f = 2 f = 4 • • • f = 32</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parallel Decoder</head><p>The VQ codebook with the compression patch size of f = 32 can be used as a texture bank to provide realistic facial details, but it also brings the problem of fidelity changes. From the results in Fig. <ref type="figure" target="#fig_1">2</ref>, we can find that the position of facial components and the facial lines are changed, making the expression and identity largely deviate from the inputs. A straightforward solution is to integrate the feature information from degraded inputs to help improve the fidelity. However, simply fusing input features into the decoder with a single branch (as shown in Fig. <ref type="figure" target="#fig_4">4</ref>) will lead to inferior details (see Fig. <ref type="figure" target="#fig_9">8</ref>). In other words, such a single branch fusion strategy tends to corrupt the generated highquality details. Though input features can bring more fidelity information, these features also contain input degradations. During feature propagation, i.e., the upsampling process from small spatial size to large spatial size, the intermediate features will largely be influenced by input features, and gradually contain fewer high-quality facial details from the VQ codebook.</p><p>To overcome the dilemma of keeping the realistic facial details and promoting fidelity, we propose a parallel decoder structure with a texture warping module. The core idea of the parallel decoder (Fig. <ref type="figure" target="#fig_4">4</ref>) is to decouple the two goals of face restoration, i.e., generating high-quality facial details and keeping the fidelity. As shown in Fig. <ref type="figure" target="#fig_2">3</ref>, given a degraded face x d ∈ R H×W ×3 , we first encode it to the latent vector z d by z d = E(x d ). The encoder consists of several residual blocks and downsampling operations. Then we replace the code z d with the HQ codebooks by Equ. 1 to get the quantized code z d q . Since the quantized code z d q is from the HQ codebook, it contains HQ facial details without degradations. In order to keep its realistic textures, we use a texture decoder G t to decode it back to image space x t = G t (z d q ). We denote the multi-level feature of the texture branch as F t = {F t i }. Since the texture branch only decodes from the HQ code, F t can keep realistic facial details.</p><p>The main branch decoder G m aims to generate faces x m with high fidelity while having the realistic facial details from the texture decoder G t . As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, the main branch decoder G m warps the texture feature F t based on the input feature extracted from degraded inputs at multiple spatial levels. We use the input feature with the largest spatial resolution as it retains the richest fidelity information of degraded faces. We then directly downsample the input feature to different resolution levels to get the multi-level features of degraded faces F d = {F d i }. For the i-th resolution level, we first warp F t i with high-quality facial details towards F d i by a texture warping module, which will be described later. After that, we fuse the warped feature F w i and the upsampled feature from F i−1 , to get the F i feature in the main decoder. The process can be formulated as:</p><formula xml:id="formula_6">F w i = T W M (F d i , F t i ), F i = Conv(Concat(U psample(F i−1 ), F w i )),<label>(4)</label></formula><p>where T W M is the texture warping module. Our parallel decoder shares the same spirits as reference-based restoration <ref type="bibr" target="#b23">[24]</ref>. In our case, the features from the texture decoder serve as the reference features containing high-quality details. Unlike reference-based restoration, our method does not require extra high-quality images with rich textures. Instead, the parallel decoder learns the main feature and "reference" feature jointly.</p><p>Texture Warping Module (TWM) The texture warping module aims to warp realistic facial details to match the fidelity of degraded inputs, especially the position of facial components and expressions. There are two inputs of TWM, one is the input feature F d and the other is the texture feature F t . As analyzed above, the texture features are decoded from the HQ codebook and have high-quality facial details. But their fidelity probably deviates from inputs. Therefore, we adopt a deformable convolution <ref type="bibr" target="#b42">[43]</ref> to better warp the realistic facial details F t towards the input feature F d . Specifically, we first concatenate those two features to generate offsets. Then the offset is used in the deformable convolution to warp the texture features to match the fidelity of input, which can be formulated as:</p><formula xml:id="formula_7">of f set = Conv(Concat(F d i , F t i )), F w i = Dconv(F t i , of f set),<label>(5)</label></formula><p>where Dconv denotes the deformable convolution. We also adopt a separable convolution with a large kernel size to model large position offsets between texture features and input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Objective</head><p>The training objective of VQFR consists of 1) pixel reconstruction loss that constraints the restored outputs close to the corresponding HQ faces; 2) code alignment loss that forces the codes of LQ inputs to match the codes of the corresponding HQ inputs; 3) perceptual loss to improve the perceptual quality in feature space; and 4) adversarial loss for restoring realistic textures. We denote the degraded face as x d , decoder restored results as x r = {x t , x m } and the ground truth HQ image as x h . The loss definitions are as follows.</p><p>Pixel Reconstruction Loss. We use the widely-used L1 loss in the pixel space as the reconstruction loss, which is denoted as: L pix = ∥x r − x h ∥ 1 . Empirically, we find that with the input feature of degraded faces, the pixel reconstruction loss has a negative impact on facial details. Therefore, we discard pixel reconstruction loss in the main decoder.</p><p>Code Alignment Loss. The code alignment loss aims to improve the performance of matching the codes of LQ images with codes of HQ images. We adopt the L2 loss to measure the distance, which can be formulated as:</p><formula xml:id="formula_8">L code = ∥z d − z h q ∥ 2 2</formula><p>, where z h q is the ground truth code obtained from encoding HQ face to the quantized code by pretrained VQGAN. Perceptual Loss. We use the widely used perceptual loss <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> for both two decoder outputs: L per = ∥ϕ(x r ) − ϕ(x h )∥ 1 + λ style ∥Gram(ϕ(x r )) − Gram(ϕ(x h ))∥ 1 , where ϕ is the pretrained VGG-16 <ref type="bibr" target="#b29">[30]</ref> network and Gram means the Gram matrix <ref type="bibr" target="#b12">[13]</ref>. The former term measures the content distance and the latter term measures the style difference between the restoration results and the corresponding HQ images. Adversarial Loss. We employ the global discriminator D g in SWAGAN <ref type="bibr" target="#b11">[12]</ref> to encourage VQFR to favor the solutions in the natural image manifold and generate realistic textures. In order to increase the local quality of facial details, we further adopt the local discriminator D l of PatchGAN <ref type="bibr" target="#b15">[16]</ref> in our training. The objectives of the global and local discriminators are defined as</p><formula xml:id="formula_9">L global adv = −E x r [softplus(D g (x r ))], L local adv = E x r [log D l (x h ) + log(1 − D l (x r ))].<label>(6</label></formula><p>) Total objective. The total training objective is the combination of above losses:  channels in all experiments. In order to increase the codebook usage, we follow <ref type="bibr" target="#b21">[22]</ref> and periodically re-initialize the codebook with k-means clustering. More implementation details are provided in the supplementary.</p><formula xml:id="formula_10">L total = λ pix L pix + λ code L code + λ per L per + λ global L global adv + λ local L local adv ,<label>(7)</label></formula><p>In the first stage of codebook training, we adopt ADAM <ref type="bibr" target="#b19">[20]</ref> optimizer with a learning rate of 1e − 4. The training iteration is set to 800K with a batch size of 16. For the second stage training for restoration, the loss weights are set to λ pix =λ code =λ per =1, λ style =2000 and the adversarial loss weights are set to λ local =λ global = 0.5. We train VQFR for 200K iterations by ADAM optimizer with the learning rate of 1e − 5. Training Datasets. The VQFR is trained on the FFHQ dataset <ref type="bibr" target="#b17">[18]</ref> including 70,000 high-quality faces. The images are resized to 512 2 during training. Following the common practice in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref>, we use the following degradation model to synthesize training pairs: x = [(y ⊛ k σ ) ↓ r +n δ ] JPEGq , where σ, r, δ and q are randomly sampled from {0.2 : 10}, {1 : 8}, {0 : 15} and {60 : 100}, respectively. Testing Datasets. Following the practice in GFP-GAN <ref type="bibr" target="#b32">[33]</ref>, we conduct experiments on the synthetic dataset CelebA-Test and three real-world datasets -LFW-Test, CelebChild-Test and WebPhoto-Test. These datasets have diverse and complicated degradations. All these datasets have no overlap with the training dataset. Evaluation Metrics. Our evaluation metrics contain two widely-used non-reference perceptual metrics: FID <ref type="bibr" target="#b14">[15]</ref> and NIQE <ref type="bibr" target="#b26">[27]</ref>. We also measure the pixel-wise metrics (PSNR and SSIM) and perceptual metric (LPIPS <ref type="bibr" target="#b41">[42]</ref>) for benchmarking CelebA-Test with Ground-Truth (GT). We follow previous work <ref type="bibr" target="#b32">[33]</ref> to use the embedding angle of ArcFace <ref type="bibr" target="#b6">[7]</ref> as the identity metric, which is denoted by 'Deg.'. In order to better measure the fidelity with accurate facial positions and expressions, we further adopt landmark distance (LMD) as the fidelity metric. More details are provided in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with State-of-the-art Methods</head><p>We compare our VQFR with several state-of-the-art face restoration methods: Wan et al. <ref type="bibr" target="#b31">[32]</ref>, HiFaceGAN <ref type="bibr" target="#b35">[36]</ref>, DFDNet <ref type="bibr" target="#b22">[23]</ref>, PSFRGAN <ref type="bibr" target="#b4">[5]</ref>, mGANprior <ref type="bibr" target="#b13">[14]</ref>, PULSE <ref type="bibr" target="#b25">[26]</ref> and GFP-GAN <ref type="bibr" target="#b32">[33]</ref>. Synthetic CelebA-Test. From the quantitative results in Table <ref type="table">.</ref> 1, VQFR achieves the lowest LPIPS, implying that VQFR generates the restored faces with the closest perceptual quality to ground-truth. VQFR also achieves the best FID and NIQE, with a large improvement over GFP-GAN, demonstrating that results of VQFR are closer to real faces and have more realistic details. For fidelity, VQFR can achieve comparable landmark distance and identity degree to GFP-GAN, showing that it can recover accurate facial expressions and detail positions.</p><p>Qualitative results are presented in Fig. <ref type="figure" target="#fig_5">5</ref>. Thanks to the VQ codebook design, VQFR generates high-quality facial components like eyes and mouth as well as other facial regions.VQFR also maintains the fidelity with the help of the parallel decoder. Real-World LFW, CelebChild, and WedPhoto-Test. We evaluate VQFR on three real-world test datasets to test the generalization ability. Table <ref type="table">.</ref> 2 shows the quantitative results. It is observed that VQFR largely improves the realness and perceptual quality of all three real-world datasets. PULSE <ref type="bibr" target="#b25">[26]</ref> achieves a higher perceptual quality on CelebChild, but its fidelity is severely affected. From the qualitative results in Fig. <ref type="figure" target="#fig_6">6</ref>, the face restored by VQFR is of the most high quality on different facial regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Importance of input features from degraded faces. The SimVQFR model directly uses the VQ codebook without exploiting input features. As shown in Fig. <ref type="figure" target="#fig_8">7</ref> it can achieve high perceptual quality (low FID and NIQE), its landmark distance is large, indicating the low fidelity. After incorporating the input features of degraded faces, the fidelity improves significantly. From the visualization of Fig. <ref type="figure" target="#fig_9">8</ref>(a), we can observe that the facial lines of SimVQFR are deviated from LQ faces, resulting in an expression change. Such a phenomenon can also be observed in the examples in Fig. <ref type="figure" target="#fig_2">3</ref>, where the texture decoder output changes the woman's expression apparently.</p><p>Importance of parallel decoder. When comparing Variant 1 and Variant 2, the NIQE (which favors high-quality details) of Variants 1 is higher and clearly inferior to Variant 2 (Fig. <ref type="figure" target="#fig_8">7 (a)</ref>). From the visualization in Fig. <ref type="figure" target="#fig_9">8</ref>(b), we observe that, in Variant 1 without the parallel decoder, the eyes and hair lose high-frequency details and are biased to degraded ones. While with the parallel decoder, the details are clearer and more realistic.</p><p>Texture Warping Module. As shown in Fig. <ref type="figure" target="#fig_8">7</ref> (a), Variant 2 does not have a TWM module and directly utilizes concatenation fusion of texture features and input features of degraded features. It lacks the ability to dynamically adjust the features with warping. Therefore, Variant 2 without TWM keeps the high-quality textures but changes the fidelity, as it cannot well adjust the fine details and expression. From the visualization    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the vector-quantized (VQ) dictionary of high-quality facial details for face restoration. Our analysis of the VQ codebook shows the potential and limitations of the VQ codebook. In order to keep the fidelity without the loss of highquality facial details, a parallel decoder is further proposed to gradually fuse input features and texture features from VQ codebooks. Equipped with the VQ codebook as a dictionary and the parallel decoder, our proposed vector-quantized face restoration (VQFR) can produce high-quality facial details while preserving fidelity. Extensive experiments show that our methods surpass previous works on both synthetic and realworld datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Reconstruction and restoration results based on codebook with compression patch size f = {8, 16, 32, 64}. In the reconstruction experiment, we analyze the HQ reconstruction (row 1) and LQ reconstruction (row 2) based on pretrained HQ codebook. In the restoration experiment, we visualized the restoration result of faces of large degradation (row 3) and small degradation (row 4), respectively.</figDesc><graphic url="image-46.png" coords="5,216.34,345.98,59.74,59.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Overview of VQFR framework. It consists of an encoder to map degraded face into latent and a parallel decoder to exploit the HQ code and input feature. The encoder and decoder are bridged by vector quantization model and a pretrained HQ codebook to replace the encoded latent to HQ code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where ∥x − x∥ 1 is the reconstruction loss and sg[•] denotes stop gradient operation. The codebook is updated by ∥sg</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Illustration of the architecture variants. (a) The VQGAN structure is used in codebook training. (b) The SimVQFR structure. (c) Single branch decoder. (d) The proposed parallel decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Qualitative comparisons on the CelebA-Test. VQFR is able to restore highquality facial details in various facial components, e.g., eyes and mouth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Qualitative comparisons on three real-world datasets. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Ablation results on CelebA-Test. Inp Feat.: input feature of degraded faces; Para Dec.: parallel decoder; TWM: texture warping module. better better (b) Visualization of the balance between realness of fidelity of different configurations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Quantitative ablation studies of key designs in VQFR.</figDesc><graphic url="image-174.png" coords="14,137.13,214.51,169.31,112.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Qualitative ablation studies of key designs on VQFR.</figDesc><graphic url="image-175.png" coords="14,426.80,278.04,50.82,50.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>where the λ pix , λ code , λ per , λ global and λ local are scale factors of corresponding loss. We implement the VQGAN and VQFR with six resolution levels, i.e., {1, 2, 4, 8, 16, 32}. For the VQ codebook, we use 1024 codebook entries with 256 Quantitative comparison on the CelebA-Test dataset for blind face restoration. Red and blue indicates the best and second best performance.</figDesc><table><row><cell>4 Experiments</cell></row><row><cell>4.1 Implementation and Evaluation Settings</cell></row><row><cell>Implementation.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison on the real-world LFW, CelebChild, WebPhoto. Red and blue indicates the best and second best performance.</figDesc><table><row><cell>Dataset</cell><cell>LFW-Test</cell><cell>CelebChild</cell><cell>WebPhoto</cell></row><row><cell>Methods</cell><cell cols="3">FID↓ NIQE ↓ FID↓ NIQE ↓ FID↓ NIQE ↓</cell></row><row><cell>Input</cell><cell cols="3">137.56 11.214 144.42 9.170 170.11 12.755</cell></row><row><cell cols="4">Wan et al. [32] 73.19 5.034 115.70 4.849 100.40 5.705</cell></row><row><cell cols="4">HiFaceGAN [36] 64.50 4.510 113.00 4.855 116.12 4.885</cell></row><row><cell>DFDNet [23]</cell><cell cols="3">62.57 4.026 111.55 4.414 100.68 5.293</cell></row><row><cell cols="4">PSFRGAN [5] 51.89 5.096 107.40 4.804 88.45 5.582</cell></row><row><cell cols="4">mGANprior [14] 73.00 6.051 126.54 6.841 120.75 7.226</cell></row><row><cell>PULSE [26]</cell><cell cols="3">64.86 5.097 102.74 5.225 86.45 5.146</cell></row><row><cell cols="4">GFP-GAN [33] 49.96 3.882 111.78 4.349 87.35 4.144</cell></row><row><cell>VQFR (ours)</cell><cell cols="3">50.64 3.589 105.18 3.936 75.38 3.607</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>ModelsConfigurations CelebA-Test Inp Feat. Para Dec. TWM FID↓ NIQE↓ LMD ↓</figDesc><table><row><cell>SimVQFR</cell><cell></cell><cell></cell><cell>40.51 3.844 2.96</cell></row><row><cell>Variant 1</cell><cell>✓</cell><cell></cell><cell>39.64 4.019 2.36</cell></row><row><cell>Variant 2</cell><cell>✓</cell><cell>✓</cell><cell>42.61 3.714 2.48</cell></row><row><cell>VQFR</cell><cell>✓</cell><cell>✓</cell><cell>✓ 41.28 3.693 2.43</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is funded by the National Key Research and Development Program of China Grant No.2018AAA0100400 and NSFC (NO. 62176130). And this work is partially supported by National Natural Science Foundation of China (61906184, U1913210), and the Shanghai Committee of Science and Technology, China (Grant No. 21DZ1100100).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Léonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.3432</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IEEE international conference on automatic face &amp; gesture recognition</title>
				<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04200</idno>
		<title level="m">Maskgit: Masked generative image transformer</title>
				<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressive semantic-aware style transformation for blind face restoration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>3, 4, 11</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face superresolution with facial priors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exemplar guided face image super-resolution without facial landmarks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Peco: Perceptual codebook for bert pre-training of vision transformers</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12710</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Swagan: A style-based wavelet-driven generative model</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bermano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image processing using multi-code gan prior</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Robust training of vector quantized bottleneck models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Łańcucki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Marxer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Dolfing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Alumäe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laurent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Blind face restoration via deep multiscale component dictionaries</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2020) 1, 2, 3, 4, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>2, 3, 4, 9</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
				<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pulse: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee/cvf conference on computer vision and pattern recognition</title>
				<meeting>the ieee/cvf conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep semantic face deblurring</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Neural discrete representation learning</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bringing old photos back to life</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
				<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards real-world blind face restoration with generative facial prior</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2021) 1, 2, 3, 4, 8, 11</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Restoreformer: High-quality blind face restoration from undegraded key-value pairs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.06374</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">N\&quot; uwa: Visual synthesis pre-training for neural visual world creation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12417</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hifacegan: Face renovation via collaborative suppression and replenishment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
				<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gan prior embedded network for blind face restoration in the wild</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Vector-quantized image modeling with improved vqgan</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04627</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
				<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
