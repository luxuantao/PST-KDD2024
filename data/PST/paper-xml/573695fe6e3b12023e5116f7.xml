<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pixel Recurrent Neural Networks Aäron van den Oord</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
							<email>nalk@google.com</email>
						</author>
						<author>
							<persName><forename type="first">Google</forename><surname>Deepmind</surname></persName>
						</author>
						<title level="a" type="main">Pixel Recurrent Neural Networks Aäron van den Oord</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast twodimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative image modeling is a central problem in unsupervised learning. Probabilistic density models can be used for a wide variety of tasks that range from image compression and forms of reconstruction such as image inpainting (e.g., see Figure <ref type="figure" target="#fig_1">1</ref>) and deblurring, to generation of new images. When the model is conditioned on external information, possible applications also include creating images based on text descriptions or simulating future frames in a planning task. One of the great advantages in generative modeling is that there are practically endless amounts of image data available to learn from. However, because images are high dimensional and highly structured, estimating the distribution of natural images is extremely challenging.</p><p>One of the most important obstacles in generative modeling is building complex and expressive models that are  also tractable and scalable. This trade-off has resulted in a large variety of generative models, each having their advantages. Most work focuses on stochastic latent variable models such as VAE's <ref type="bibr" target="#b16">(Rezende et al., 2014;</ref><ref type="bibr" target="#b10">Kingma &amp; Welling, 2013</ref>) that aim to extract meaningful representations, but often come with an intractable inference step that can hinder their performance.</p><p>One effective approach to tractably model a joint distribution of the pixels in the image is to cast it as a product of conditional distributions; this approach has been adopted in autoregressive models such as NADE <ref type="bibr" target="#b11">(Larochelle &amp; Murray, 2011)</ref> and fully visible sigmoid belief networks <ref type="bibr" target="#b14">(Neal, 1992)</ref>. The factorization turns the joint modeling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels. But to model the highly nonlinear and long-range correlations between pixels and the complex conditional distributions that result, a highly expressive sequence model is necessary.</p><p>Recurrent Neural Networks (RNN) are powerful models that offer a compact, shared parametrization of a series of conditional distributions. RNNs have been shown to excel at hard sequence problems ranging from handwriting generation <ref type="bibr" target="#b2">(Graves, 2013)</ref>, to character prediction <ref type="bibr">(Sutskever et al., 2011)</ref> and to machine translation <ref type="bibr" target="#b8">(Kalchbrenner &amp; Blunsom, 2013)</ref>. A two-dimensional RNN has produced very promising results in modeling grayscale images and textures <ref type="bibr">(Theis &amp; Bethge, 2015)</ref>.</p><p>In this paper we advance two-dimensional RNNs and apply them to large-scale modeling of natural images. The resulting PixelRNNs are composed of up to twelve, fast two-dimensional Long Short-Term Memory (LSTM) lay-</p><p>x 1</p><p>x i</p><p>x n</p><p>x n 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head><p>x n 2</p><p>Multi-scale context</p><formula xml:id="formula_0">x 1 x i x n x n 2 R G B R G B R G B Mask A Mask B Context Figure 2. Left:</formula><p>To generate pixel xi one conditions on all the previously generated pixels left and above of xi. Center: To generate a pixel in the multi-scale case we can also condition on the subsampled image pixels (in light blue). Right: Diagram of the connectivity inside a masked convolution. In the first layer, each of the RGB channels is connected to previous channels and to the context, but is not connected to itself. In subsequent layers, the channels are also connected to themselves.</p><p>ers. These layers use LSTM units in their state <ref type="bibr" target="#b7">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b3">Graves &amp; Schmidhuber, 2009)</ref> and adopt a convolution to compute at once all the states along one of the spatial dimensions of the data. We design two types of these layers. The first type is the Row LSTM layer where the convolution is applied along each row; a similar technique is described in <ref type="bibr" target="#b22">(Stollenga et al., 2015)</ref>. The second type is the Diagonal BiLSTM layer where the convolution is applied in a novel fashion along the diagonals of the image. The networks also incorporate residual connections <ref type="bibr" target="#b6">(He et al., 2015)</ref> around LSTM layers; we observe that this helps with training of the PixelRNN for up to twelve layers of depth.</p><p>We also consider a second, simplified architecture which shares the same core components as the PixelRNN. We observe that Convolutional Neural Networks (CNN) can also be used as sequence model with a fixed dependency range, by using Masked convolutions. The PixelCNN architecture is a fully convolutional network of fifteen layers that preserves the spatial resolution of its input throughout the layers and outputs a conditional distribution at each location.</p><p>Both PixelRNN and PixelCNN capture the full generality of pixel inter-dependencies without introducing independence assumptions as in e.g., latent variable models. The dependencies are also maintained between the RGB color values within each individual pixel. Furthermore, in contrast to previous approaches that model the pixels as continuous values (e.g., Theis &amp; Bethge (2015); Gregor et al. ( <ref type="formula">2014</ref>)), we model the pixels as discrete values using a multinomial distribution implemented with a simple softmax layer. We observe that this approach gives both representational and training advantages for our models.</p><p>The contributions of the paper are as follows. In Section 3 we design two types of PixelRNNs corresponding to the two types of LSTM layers; we describe the purely convolutional PixelCNN that is our fastest architecture; and we design a Multi-Scale version of the PixelRNN. In Section 5 we show the relative benefits of using the discrete softmax distribution in our models and of adopting residual connections for the LSTM layers. Next we test the models on MNIST and on CIFAR-10 and show that they obtain loglikelihood scores that are considerably better than previous results. We also provide results for the large-scale Ima-geNet dataset resized to both 32 × 32 and 64 × 64 pixels; to our knowledge likelihood values from generative models have not previously been reported on this dataset. Finally, we give a qualitative evaluation of the samples generated from the PixelRNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head><p>Our aim is to estimate a distribution over natural images that can be used to tractably compute the likelihood of images and to generate new ones. The network scans the image one row at a time and one pixel at a time within each row. For each pixel it predicts the conditional distribution over the possible pixel values given the scanned context. Figure <ref type="figure">2</ref> illustrates this process. The joint distribution over the image pixels is factorized into a product of conditional distributions. The parameters used in the predictions are shared across all pixel positions in the image.</p><p>To capture the generation process, <ref type="bibr">Theis &amp; Bethge (2015)</ref> propose to use a two-dimensional LSTM network <ref type="bibr" target="#b3">(Graves &amp; Schmidhuber, 2009</ref>) that starts at the top left pixel and proceeds towards the bottom right pixel. The advantage of the LSTM network is that it effectively handles long-range dependencies that are central to object and scene understanding. The two-dimensional structure ensures that the signals are well propagated both in the left-to-right and topto-bottom directions.</p><p>In this section we first focus on the form of the distribution, whereas the next section will be devoted to describing the architectural innovations inside PixelRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Generating an Image Pixel by Pixel</head><p>The goal is to assign a probability p(x) to each image x formed of n × n pixels. We can write the image x as a onedimensional sequence x 1 , ..., x n 2 where pixels are taken from the image row by row. To estimate the joint distribution p(x) we write it as the product of the conditional distributions over the pixels:</p><formula xml:id="formula_1">p(x) = n 2 i=1 p(x i |x 1 , ..., x i−1 )<label>(1)</label></formula><p>The value p(x i |x 1 , ..., x i−1 ) is the probability of the i-th pixel x i given all the previous pixels x 1 , ..., x i−1 . The generation proceeds row by row and pixel by pixel. Figure <ref type="figure">2</ref> (Left) illustrates the conditioning scheme.</p><p>Each pixel x i is in turn jointly determined by three values, one for each of the color channels Red, Green and Blue (RGB). We rewrite the distribution p(x i |x &lt;i ) as the following product:</p><formula xml:id="formula_2">p(x i,R |x &lt;i )p(x i,G |x &lt;i , x i,R )p(x i,B |x &lt;i , x i,R , x i,G ) (2)</formula><p>Each of the colors is thus conditioned on the other channels as well as on all the previously generated pixels.</p><p>Note that during training and evaluation the distributions over the pixel values are computed in parallel, while the generation of an image is sequential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Pixels as Discrete Variables</head><p>Previous approaches use a continuous distribution for the values of the pixels in the image (e.g. Theis &amp; Bethge (2015); Uria et al. ( <ref type="formula">2014</ref>)). By contrast we model p(x) as a discrete distribution, with every conditional distribution in Equation 2 being a multinomial that is modeled with a softmax layer. Each channel variable x i, * simply takes one of 256 distinct values. The discrete distribution is representationally simple and has the advantage of being arbitrarily multimodal without prior on the shape (see Fig. <ref type="figure" target="#fig_5">6</ref>). Experimentally we also find the discrete distribution to be easy to learn and to produce better performance compared to a continuous distribution (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pixel Recurrent Neural Networks</head><p>In this section we describe the architectural components that compose the PixelRNN. In Sections 3.1 and 3.2, we describe the two types of LSTM layers that use convolutions to compute at once the states along one of the spatial dimensions. In Section 3.3 we describe how to incorporate residual connections to improve the training of a PixelRNN with many LSTM layers. In Section 3.4 we describe the softmax layer that computes the discrete joint distribution of the colors and the masking technique that ensures the proper conditioning scheme. In Section 3.5 we describe the PixelCNN architecture. Finally in Section 3.6 we describe the multi-scale architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Row LSTM</head><p>The Row LSTM is a unidirectional layer that processes the image row by row from top to bottom computing features for a whole row at once; the computation is performed with a one-dimensional convolution. For a pixel x i the layer captures a roughly triangular context above the pixel as shown in Figure <ref type="figure" target="#fig_3">4</ref> (center). The kernel of the onedimensional convolution has size k × 1 where k ≥ 3; the larger the value of k the broader the context that is captured. The weight sharing in the convolution ensures translation invariance of the computed features along each row.</p><p>The computation proceeds as follows. An LSTM layer has an input-to-state component and a recurrent state-to-state component that together determine the four gates inside the LSTM core. To enhance parallelization in the Row LSTM the input-to-state component is first computed for the entire two-dimensional input map; for this a k × 1 convolution is used to follow the row-wise orientation of the LSTM itself.</p><p>The convolution is masked to include only the valid context (see Section 3.4) and produces a tensor of size 4h × n × n, representing the four gate vectors for each position in the input map, where h is the number of output feature maps.</p><p>To compute one step of the state-to-state component of the LSTM layer, one is given the previous hidden and cell states h i−1 and c i−1 , each of size h × n × 1. The new hidden and cell states h i , c i are obtained as follows:</p><formula xml:id="formula_3">[o i , f i , i i , g i ] = σ(K ss h i−1 + K is x i ) c i = f i c i−1 + i i g i h i = o i tanh(c i )<label>(3)</label></formula><p>where x i of size h × n × 1 is row i of the input map, and represents the convolution operation and the elementwise multiplication. The weights K ss and K is are the kernel weights for the state-to-state and the input-to-state components, where the latter is precomputed as described above. In the case of the output, forget and input gates o i , f i and i i , the activation σ is the logistic sigmoid function, whereas for the content gate g i , σ is the tanh function.</p><p>Each step computes at once the new state for an entire row of the input map. Because the Row LSTM has a triangular receptive field (Figure <ref type="figure" target="#fig_3">4</ref>), it is unable to capture the entire available context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Diagonal BiLSTM</head><p>The Diagonal BiLSTM is designed to both parallelize the computation and to capture the entire available context for any image size. Each of the two directions of the layer scans the image in a diagonal fashion starting from a corner at the top and reaching the opposite corner at the bottom. Each step in the computation computes at once the LSTM state along a diagonal in the image. Figure <ref type="figure" target="#fig_3">4</ref> (right)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PixelCNN</head><p>Row LSTM Diagonal BiLSTM illustrates the computation and the resulting receptive field.</p><p>The diagonal computation proceeds as follows. We first skew the input map into a space that makes it easy to apply convolutions along diagonals. The skewing operation offsets each row of the input map by one position with respect to the previous row, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>; this results in a map of size n × (2n − 1). At this point we can compute the input-to-state and state-to-state components of the Diagonal BiLSTM. For each of the two directions, the input-to-state component is simply a 1 × 1 convolution K is that contributes to the four gates in the LSTM core; the operation generates a 4h × n × n tensor. The state-to-state recurrent component is then computed with a column-wise convolution K ss that has a kernel of size 2 × 1. The step takes the previous hidden and cell states, combines the contribution of the input-to-state component and produces the next hidden and cell states, as defined in Equation <ref type="formula" target="#formula_3">3</ref>. The output feature map is then skewed back into an n × n map by removing the offset positions. This computation is repeated for each of the two directions. Given the two output maps, to prevent the layer from seeing future pixels, the right output map is then shifted down by one row and added to the left output map.</p><p>Besides reaching the full dependency field, the Diagonal BiLSTM has the additional advantage that it uses a convolutional kernel of size 2 × 1 that processes a minimal amount of information at each step yielding a highly nonlinear computation. Kernel sizes larger than 2 × 1 are not particularly useful as they do not broaden the already global receptive field of the Diagonal BiLSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Residual Connections</head><p>We train PixelRNNs of up to twelve layers of depth. As a means to both increase convergence speed and propagate signals more directly through the network, we deploy residual connections <ref type="bibr" target="#b6">(He et al., 2015)</ref> from one LSTM layer to the next. Figure <ref type="figure" target="#fig_4">5</ref> shows a diagram of the residual blocks.</p><p>The input map to the PixelRNN LSTM layer has 2h features. The input-to-state component reduces the number of features by producing h features per gate. After applying the recurrent layer, the output map is upsampled back to 2h features per position via a 1 × 1 convolution and the input map is added to the output map. This method is related to previous approaches that use gating along the depth of the recurrent network <ref type="bibr" target="#b9">(Kalchbrenner et al., 2015;</ref><ref type="bibr">Zhang et al., 2016)</ref>, but has the advantage of not requiring additional gates. Apart from residual connections, one can also use learnable skip connections from each layer to the output.</p><p>In the experiments we evaluate the relative effectiveness of residual and layer-to-output skip connections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Masked Convolution</head><p>The h features for each input position at every layer in the network are split into three parts, each corresponding to one of the RGB channels. When predicting the R channel for the current pixel x i , only the generated pixels left and above of x i can be used as context. When predicting the G channel, the value of the R channel can also be used as context in addition to the previously generated pixels. Likewise, for the B channel, the values of both the R and G channels can be used. To restrict connections in the network to these dependencies, we apply a mask to the inputto-state convolutions and to other purely convolutional layers in a PixelRNN.</p><p>We use two types of masks that we indicate with mask A and mask B, as shown in Figure <ref type="figure">2</ref> (Right). Mask A is applied only to the first convolutional layer in a PixelRNN and restricts the connections to those neighboring pixels and to those colors in the current pixels that have already been predicted. On the other hand, mask B is applied to all the subsequent input-to-state convolutional transitions and relaxes the restrictions of mask A by also allowing the connection from a color to itself. The masks can be easily implemented by zeroing out the corresponding weights in the input-to-state convolutions after each update. Similar masks have also been used in variational autoencoders <ref type="bibr" target="#b4">(Gregor et al., 2014;</ref><ref type="bibr" target="#b1">Germain et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">PixelCNN</head><p>The Row and Diagonal LSTM layers have a potentially unbounded dependency range within their receptive field. This comes with a computational cost as each state needs to be computed sequentially. One simple workaround is to make the receptive field large, but not unbounded. We can use standard convolutional layers to capture a bounded receptive field and compute features for all pixel positions at once. The PixelCNN uses multiple convolutional layers that preserve the spatial resolution; pooling layers are not used. Masks are adopted in the convolutions to avoid seeing the future context; masks have previously also been used in non-convolutional models such as MADE <ref type="bibr" target="#b1">(Germain et al., 2015)</ref>. Note that the advantage of parallelization of the PixelCNN over the PixelRNN is only available during training or during evaluating of test images. The image generation process is sequential for both kinds of networks, as each sampled pixel needs to be given as input back into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Multi-Scale PixelRNN</head><p>The Multi-Scale PixelRNN is composed of an unconditional PixelRNN and one or more conditional PixelRNNs.</p><p>The unconditional network first generates in the standard way a smaller s×s image that is subsampled from the original image. The conditional network then takes the s × s image as an additional input and generates a larger n × n image, as shown in Figure <ref type="figure">2</ref> (Middle).</p><p>The conditional network is similar to a standard PixelRNN, but each of its layers is biased with an upsampled version of the small s × s image. The upsampling and biasing processes are defined as follows. In the upsampling process, one uses a convolutional network with deconvolutional layers to construct an enlarged feature map of size c × n × n, where c is the number of features in the output map of the upsampling network. Then, in the biasing process, for each layer in the conditional PixelRNN, one simply maps the c × n × n conditioning map into a 4h × n × n map that is added to the input-to-state map of the corresponding layer; this is performed using a 1 × 1 unmasked convolution. The larger n × n image is then generated as usual.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Specifications of Models</head><p>In this section we give the specifications of the PixelRNNs used in the experiments. We have four types of networks:</p><p>the PixelRNN based on Row LSTM, the one based on Diagonal BiLSTM, the fully convolutional one and the Multi-Scale one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section we describe our experiments and results. We begin by describing the way we evaluate and compare our results. In Section 5.2 we give details about the training.</p><p>Then we give results on the relative effectiveness of architectural components and our best results on the MNIST, CIFAR-10 and ImageNet datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation</head><p>All our models are trained and evaluated on the loglikelihood loss function coming from a discrete distribution. Although natural image data is usually modeled with continuous distributions using density functions, we can compare our results with previous art in the following way.</p><p>In the literature it is currently best practice to add realvalued noise to the pixel values to dequantize the data when using density functions <ref type="bibr">(Uria et al., 2013)</ref>. When uniform noise is added (with values in the interval [0, 1]), then the log-likelihoods of continuous and discrete models are directly comparable <ref type="bibr">(Theis et al., 2015)</ref>. In our case, we can use the values from the discrete distribution as a piecewiseuniform continuous function that has a constant value for every interval [i, i + 1], i = 1, 2, . . . 256. This corresponding distribution will have the same log-likelihood (on data with added noise) as the original discrete distribution (on discrete data).</p><p>For MNIST we report the negative log-likelihood in nats as it is common practice in literature. For CIFAR-10 and ImageNet we report negative log-likelihoods in bits per dimension. The total discrete log-likelihood is normalized by the dimensionality of the images (e.g., 32 × 32 × 3 = 3072 for CIFAR-10). These numbers are interpretable as the number of bits that a compression scheme based on this model would need to compress every RGB color value (van den <ref type="bibr">Oord &amp; Schrauwen, 2014b;</ref><ref type="bibr">Theis et al., 2015)</ref>; in practice there is also a small overhead due to arithmetic coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training Details</head><p>Our models are trained on GPUs using the Torch toolbox.</p><p>From the different parameter update rules tried, RMSProp gives best convergence performance and is used for all experiments. The learning rate schedules were manually set for every dataset to the highest values that allowed fast convergence. The batch sizes also vary for different datasets.</p><p>For smaller datasets such as MNIST and CIFAR-10 we use smaller batch sizes of 16 images as this seems to regularize the models. For ImageNet we use as large a batch size as allowed by the GPU memory; this corresponds to 64 images/batch for 32 × 32 ImageNet, and 32 images/batch for 64 × 64 ImageNet. Apart from scaling and centering the images at the input of the network, we don't use any other preprocessing or augmentation. For the multinomial loss function we use the raw pixel color values as categories. For all the PixelRNN models, we learn the initial recurrent state of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Discrete Softmax Distribution</head><p>Apart from being intuitive and easy to implement, we find that using a softmax on discrete pixel values instead of a mixture density approach on continuous pixel values gives better results. For the Row LSTM model with a softmax output distribution we obtain 3.06 bits/dim on the CIFAR-10 validation set. For the same model with a Mixture of Conditional Gaussian Scale Mixtures (MCGSM) (Theis &amp; Bethge, 2015) we obtain 3.22 bits/dim.</p><p>In Figure <ref type="figure" target="#fig_5">6</ref> we show a few softmax activations from the model. Although we don't embed prior information about the meaning or relations of the 256 color categories, e.g. that pixel values 51 and 52 are neighbors, the distributions predicted by the model are meaningful and can be multimodal, skewed, peaked or long tailed. Also note that values 0 and 255 often get a much higher probability as they are more frequent. Another advantage of the discrete distribution is that we do not worry about parts of the distribution mass lying outside the interval [0, 255], which is something that typically happens with continuous distributions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Residual Connections</head><p>Another core component of the networks is residual connections. In Table <ref type="table">2</ref> we show the results of having residual connections, having standard skip connections or having both, in the 12-layer CIFAR-10 Row LSTM model. We see that using residual connections is as effective as using skip connections; using both is also effective and preserves the advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No skip Skip</head><p>No residual: 3.22 3.09 Residual:</p><p>3.07 3.06</p><p>Table <ref type="table">2</ref>. Effect of residual and skip connections in the Row LSTM network evaluated on the Cifar-10 validation set in bits/dim.</p><p>When using both the residual and skip connections, we see in Table <ref type="table">3</ref> that performance of the Row LSTM improves with increased depth. This holds for up to the 12 LSTM layers that we tried.</p><p># layers: 1 2 3 6 9 12 NLL: 3.30 3.20 3.17 3.09 3.08 3.06</p><p>Table <ref type="table">3</ref>. Effect of the number of layers on the negative log likelihood evaluated on the CIFAR-10 validation set (bits/dim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">MNIST</head><p>Although the goal of our work was to model natural images on a large scale, we also tried our model on the binary version <ref type="bibr" target="#b19">(Salakhutdinov &amp; Murray, 2008)</ref> of <ref type="bibr">MNIST (LeCun et al., 1998)</ref> as it is a good sanity check and there is a lot of previous art on this dataset to compare with. In Table <ref type="table" target="#tab_2">4</ref> we report the performance of the Diagonal BiLSTM model and that of previous published results. To our knowledge this is the best reported result on MNIST so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">CIFAR-10</head><p>Next we test our models on the CIFAR-10 dataset <ref type="bibr" target="#b11">(Krizhevsky, 2009)</ref>. Table <ref type="table" target="#tab_3">5</ref> lists the results of our models and that of previously published approaches. For the proposed networks, the Diagonal BiLSTM has the best performance, followed by the Row LSTM and the Pixel-    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">ImageNet</head><p>Although to our knowledge the are no published results on the ILSVRC ImageNet dataset <ref type="bibr" target="#b17">(Russakovsky et al., 2015)</ref> that we can compare our models with, we give our Ima-geNet log-likelihood performance in Table <ref type="table" target="#tab_4">6</ref>. On ImageNet the current PixelRNNs do not appear to overfit, as we saw </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>occluded completions original</head><p>Figure <ref type="figure">9</ref>. Image completions sampled from a model that was trained on 32x32 ImageNet images. Note that diversity of the completions is high, which can be attributed to the log-likelihood loss function used in this generative model, as it encourages models with high entropy. As these are sampled from the model, we can easily generate millions of different completions. It is also interesting to see that textures such as water, wood and shrubbery are also inputed relative well (see Figure <ref type="figure" target="#fig_1">1</ref>).</p><p>that their validation performance improved with size and depth. The main constraint on model size are currently computation time and GPU memory.</p><p>Note that the ImageNet models are in general less compressible than the CIFAR-10 images. ImageNet has greater variety of images, and the CIFAR-10 images were most likely resized with a different algorithm than the one we used for ImageNet images. The ImageNet images are less blurry, which means neighboring pixels are less correlated to each other and thus less predictable. Because the downsampling method can influence the compression performance, we will release the used downsampled images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we significantly improve and build upon deep recurrent neural networks as generative models for natural images. We have described novel two-dimensional LSTM layers: the Row LSTM and the Diagonal BiLSTM, that scale more easily to larger datasets. The models were trained to model the raw RGB pixel values. We treated the pixel values as discrete random variables by using a softmax layer in the conditional distributions. We employed masked convolutions to allow PixelRNNs to model full dependencies between the color channels. We proposed and evaluated architectural improvements in these models resulting in PixelRNNs with up to 12 LSTM layers.</p><p>We have shown that the PixelRNNs significantly improve the state of the art on the MNIST and CIFAR-10 datasets. We also provide new benchmarks for generative image modeling on the ImageNet dataset. Based on the samples and completions drawn from the models we can conclude that the PixelRNNs are able to model both spatially local and long-range correlations and are able to produce images that are sharp and coherent. Given that these models improve as we make them larger and that there is practically unlimited data available to train on, more computation and larger models are likely to further improve the results. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&amp;CP volume 48. Copyright 2016 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Image completions sampled from a PixelRNN.</figDesc><graphic url="image-1.png" coords="1,308.66,229.48,229.32,85.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. In the Diagonal BiLSTM, to allow for parallelization along the diagonals, the input map is skewed by offseting each row by one position with respect to the previous row. When the spatial layer is computed left to right and column by column, the output map is shifted back into the original size. The convolution uses a kernel of size 2 × 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of the input-to-state and state-to-state mappings for the three proposed architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Residual blocks for a PixelCNN (left) and PixelRNNs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Example softmax activations from the model. The leftmost shows the distribution of the first pixel red value (first value to sample).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Samples from models trained on CIFAR-10 (left) and ImageNet 32x32 (right) images. In general we can see that the models capture local spatial dependencies relatively well. The ImageNet model seems to be better at capturing more global structures than the CIFAR-10 model. The ImageNet model was larger and trained on much more data, which explains the qualitative difference in samples.</figDesc><graphic url="image-3.png" coords="7,304.52,67.06,230.85,208.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Samples from models trained on ImageNet 64x64 images. Left: normal model, right: multi-scale model. The single-scale model trained on 64x64 images is less able to capture global structure than the 32x32 model. The multi-scale model seems to resolve this problem. Although these models get similar performance in log-likelihood, the samples on the right do seem globally more coherent.</figDesc><graphic url="image-4.png" coords="8,61.52,67.06,230.85,173.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 (</head><label>7</label><figDesc>Figure 7 (right) shows 32 × 32 samples drawn from our model trained on ImageNet. Figure 8 shows 64 × 64 samples from the same model with and without multi-scale conditioning. Finally, we also show image completions sampled from the model in Figure 9.</figDesc><graphic url="image-6.png" coords="8,55.44,311.00,234.00,141.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Details of the architectures. In the LSTM architectures i-s and s-s stand for input-state and state-state convolutions.</figDesc><table><row><cell>PixelCNN</cell><cell>Row LSTM</cell><cell>Diagonal BiLSTM</cell></row><row><cell></cell><cell cols="2">7 × 7 conv mask A</cell></row><row><cell cols="3">Multiple residual blocks: (see fig 5)</cell></row><row><cell>Conv</cell><cell>Row LSTM</cell><cell>Diagonal BiLSTM</cell></row><row><cell cols="2">3 × 3 mask B i-s: 3 × 1 mask B</cell><cell>i-s: 1 × 1 mask B</cell></row><row><cell></cell><cell cols="2">s-s: 3 × 1 no mask s-s: 1 × 2 no mask</cell></row><row><cell cols="3">ReLU followed by 1 × 1 conv, mask B (2 layers)</cell></row><row><cell cols="3">256-way Softmax for each RGB color (Natural images)</cell></row><row><cell></cell><cell cols="2">or Sigmoid (MNIST)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>specifies each layer in the single-scale networks. The first layer is a 7 × 7 convolution that uses the mask of type A. The two types of LSTM networks then use a variable number of recurrent layers. The input-to-state convolution in this layer uses a mask of type B, whereas the state-to-state convolution is not masked. The PixelCNN uses convolutions of size 3 × 3 with a mask of type B. The top feature map is then passed through a couple of layers consisting of a Rectified Linear Unit (ReLU) and a 1×1 convolution. For the CIFAR-10 and ImageNet experiments, these layers have 1024 feature maps; for the MNIST experiment, the layers have 32 feature maps. Residual and layer-to-output connections are used across the layers of all three networks.The networks used in the experiments have the following hyperparameters. For MNIST we use a Diagonal BiLSTM with 7 layers and a value of h = 16 (Section 3.3 and Figure5right). For CIFAR-10 the Row and Diagonal BiLSTMs have 12 layers and a number of h = 128 units. The Pixel-</figDesc><table /><note>CNN has 15 layers and h = 128. For 32 × 32 ImageNet we adopt a 12 layer Row LSTM with h = 384 units and for 64 × 64 ImageNet we use a 4 layer Row LSTM with h = 512 units; the latter model does not use residual connections.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Test set performance of different models on MNIST in nats (negative log-likelihood).</figDesc><table><row><cell>Model</cell><cell>NLL Test</cell><cell>Model</cell><cell>NLL Test (Train)</cell></row><row><cell>DBM 2hl [1]:</cell><cell>≈ 84.62</cell><cell>Uniform Distribution:</cell><cell>8.00</cell></row><row><cell>DBN 2hl [2]:</cell><cell>≈ 84.55</cell><cell>Multivariate Gaussian:</cell><cell>4.70</cell></row><row><cell>NADE [3]:</cell><cell>88.33</cell><cell>NICE [1]:</cell><cell>4.48</cell></row><row><cell>EoNADE 2hl (128 orderings) [3]:</cell><cell>85.10</cell><cell>Deep Diffusion [2]:</cell><cell>4.20</cell></row><row><cell>EoNADE-5 2hl (128 orderings) [4]:</cell><cell>84.68</cell><cell>Deep GMMs [3]:</cell><cell>4.00</cell></row><row><cell>DLGM [5]:</cell><cell>≈ 86.60</cell><cell>RIDE [4]:</cell><cell>3.47</cell></row><row><cell>DLGM 8 leapfrog steps [6]: DARN 1hl [7]: MADE 2hl (32 masks) [8]: DRAW [9]:</cell><cell>≈ 85.51 ≈ 84.13 86.64 ≤ 80.97</cell><cell>PixelCNN: Row LSTM: Diagonal BiLSTM:</cell><cell>3.14 (3.08) 3.07 (3.00) 3.00 (2.93)</cell></row><row><cell>PixelCNN:</cell><cell>81.30</cell><cell></cell><cell></cell></row><row><cell>Row LSTM:</cell><cell>80.54</cell><cell></cell><cell></cell></row><row><cell>Diagonal BiLSTM (1 layer, h = 32):</cell><cell>80.75</cell><cell></cell><cell></cell></row><row><cell>Diagonal BiLSTM (7 layers, h = 16):</cell><cell>79.20</cell><cell></cell><cell></cell></row><row><cell cols="2">Prior results taken from [1]</cell><cell></cell><cell></cell></row><row><cell cols="2">(Salakhutdinov &amp; Hinton, 2009), [2] (Murray &amp; Salakhutdinov,</cell><cell></cell><cell></cell></row><row><cell cols="2">2009), [3] (Uria et al., 2014), [4] (Raiko et al., 2014), [5] (Rezende</cell><cell></cell><cell></cell></row><row><cell cols="2">et al., 2014), [6] (Salimans et al., 2015), [7] (Gregor et al., 2014),</cell><cell></cell><cell></cell></row><row><cell>[8] (Germain et al., 2015), [9] (Gregor et al., 2015).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CNN. This coincides with the size of the respective recep-</cell><cell></cell><cell></cell></row><row><cell cols="2">tive fields: the Diagonal BiLSTM has a global view, the</cell><cell></cell><cell></cell></row><row><cell cols="2">Row LSTM has a partially occluded view and the Pixel-</cell><cell></cell><cell></cell></row><row><cell cols="2">CNN sees the fewest pixels in the context. This suggests</cell><cell></cell><cell></cell></row><row><cell cols="2">that effectively capturing a large receptive field is impor-</cell><cell></cell><cell></cell></row><row><cell cols="2">tant. Figure 7 (left) shows CIFAR-10 samples generated</cell><cell></cell><cell></cell></row><row><cell>from the Diagonal BiLSTM.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Test set performance of different models on CIFAR-10 in bits/dim. For our models we give training performance in brack-</figDesc><table><row><cell cols="2">ets. [1] (Dinh et al., 2014), [2] (Sohl-Dickstein et al., 2015), [3]</cell></row><row><cell cols="2">(van den Oord &amp; Schrauwen, 2014a), [4] personal communication</cell></row><row><cell>(Theis &amp; Bethge, 2015).</cell><cell></cell></row><row><cell cols="2">Image size NLL Validation (Train)</cell></row><row><cell>32x32:</cell><cell>3.86 (3.83)</cell></row><row><cell>64x64:</cell><cell>3.63 (3.57)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Negative log-likelihood performance on 32×32 and 64× 64 ImageNet in bits/dim.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Sutskever, Ilya, Martens, James, and Hinton, Geoffrey E.Generating text with recurrent neural networks. In Proceedings of the 28th International Conference on Machine Learning, 2011. Theis, Lucas and Bethge, Matthias. Generative image modeling using spatial LSTMs. In Advances in Neural Information Processing Systems, 2015. Theis, Lucas, van den Oord, Aäron, and Bethge, Matthias. A note on the evaluation of generative models. arXiv preprint arXiv:1511.01844, 2015. Uria, Benigno, Murray, Iain, and Larochelle, Hugo. RNADE: The real-valued neural autoregressive densityestimator. In Advances in Neural Information Processing Systems, 2013. Uria, Benigno, Murray, Iain, and Larochelle, Hugo. A deep and tractable density estimator. In Proceedings of the 31st International Conference on Machine Learning, 2014. van den Oord, Aäron and Schrauwen, Benjamin. Factoring variations in natural images with deep gaussian mixture models. In Advances in Neural Information Processing Systems, 2014a. van den Oord, Aäron and Schrauwen, Benjamin. The student-t mixture as a natural image patch prior with application to image compression. The Journal of Machine Learning Research, 2014b. Zhang, Yu, Chen, Guoguo, Yu, Dong, Yao, Kaisheng, Khudanpur, Sanjeev, and Glass, James. Highway long shortterm memory RNNs for distant speech recognition. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, 2016.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Shakir Mohamed and Guillaume Desjardins for helpful input on this paper and Lucas Theis, Alex Graves, Karen Simonyan, Lasse Espeholt, Danilo Rezende, Karol Gregor and Ivo Danihelka for insightful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">NICE: Non-linear independent components estimation</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">MADE: Masked autoencoder for distribution estimation</title>
		<author>
			<persName><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><surname>Karol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03509</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.0850</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Andriy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DRAW: A recurrent neural network for image generation</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.01526</idno>
		<title level="m">Grid long short-term memory</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">;</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009">2009. 2011</date>
		</imprint>
	</monogr>
	<note>The neural autoregressive distribution estimator</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><surname>Léon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
				<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evaluating probabilities under high-dimensional latent variable models</title>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connectionist learning of belief networks</title>
		<author>
			<persName><forename type="first">Radford</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Iterative neural autoregressive distribution estimator NADE-k</title>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Shakir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
				<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><surname>Su</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhiheng</surname></persName>
		</author>
		<author>
			<persName><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><surname>Andrej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
				<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
				<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><surname>Jascha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName><surname>Niru</surname></persName>
		</author>
		<author>
			<persName><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><surname>Surya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
				<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Parallel multi-dimensional lstm, with application to fast biomedical volumetric image segmentation</title>
		<author>
			<persName><forename type="first">Marijn</forename><forename type="middle">F</forename><surname>Stollenga</surname></persName>
		</author>
		<author>
			<persName><surname>Byeon</surname></persName>
		</author>
		<author>
			<persName><surname>Wonmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
