<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Iterative Pruning Algorithm for Feedforward Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giovanna</forename><surname>Castellano</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Anna</forename><forename type="middle">Maria</forename><surname>Fanelli</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Istituto di Elaborazione dei Segnali e delle Immagini</orgName>
								<orgName type="institution">Consiglio Nazionale delle Ricerche</orgName>
								<address>
									<postCode>70126</postCode>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Informatica</orgName>
								<orgName type="institution">Universita&apos; di Bari</orgName>
								<address>
									<postCode>70126</postCode>
									<settlement>Bari</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dipartimento di Matematica Applicata e Informatica</orgName>
								<orgName type="institution">Universita&apos; &quot;Ca&apos; Foscari&quot; di Venezia</orgName>
								<address>
									<postCode>30173</postCode>
									<settlement>Venezia Mestre</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Iterative Pruning Algorithm for Feedforward Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AB4F9D4DFBA05CE4D9E930669AB121B3</idno>
					<note type="submission">received May 16, 1994; revised February 13, 1995, January 16, 1996, and October 8, 1996.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Feedforward neural networks</term>
					<term>generalization</term>
					<term>hidden neurons</term>
					<term>iterative methods</term>
					<term>least-squares methods</term>
					<term>network pruning</term>
					<term>pattern recognition</term>
					<term>structure simplification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of determining the proper size of an artificial neural network is recognized to be crucial, especially for its practical implications in such important issues as learning and generalization. One popular approach tackling this problem is commonly known as pruning and consists of training a larger than necessary network and then removing unnecessary weights/nodes. In this paper, a new pruning method is developed, based on the idea of iteratively eliminating units and adjusting the remaining weights in such a way that the network performance does not worsen over the entire training set. The pruning problem is formulated in terms of solving a system of linear equations, and a very efficient conjugate gradient algorithm is used for solving it, in the least-squares sense. The algorithm also provides a simple criterion for choosing the units to be removed, which has proved to work well in practice. The results obtained over various test problems demonstrate the effectiveness of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D ESPITE many advances, for neural networks to find gen- eral applicability in real-world problems, several questions must still be answered. One such open question involves determining the most appropriate network size for solving a specific task. The network designer's dilemma stems from the fact that both large and small networks exhibit a number of advantages. When a network has too many free parameters (i.e., weights and/or units) not only is learning fast <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b4">[5]</ref>, but local minima are more easily avoided <ref type="bibr" target="#b5">[6]</ref>. In particular, a theoretical study <ref type="bibr" target="#b6">[7]</ref> has shown that when the number of hidden units equals the number of training examples (minus one), the backpropagation error surface is guaranteed to have no local minima. Large networks can also form as complex decision regions as the problem requires <ref type="bibr" target="#b7">[8]</ref> and should exhibit a certain degree of fault tolerance under damage conditions (however, this appears not to be as obvious as might intuitively have been expected <ref type="bibr" target="#b8">[9]</ref>). On the other hand, both theory <ref type="bibr" target="#b9">[10]</ref> and experience <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b12">[13]</ref> show that networks with few free parameters exhibit a better generalization performance, and this is explained by recalling the analogy between neural network learning and curve fitting. Moreover, knowledge embedded in small trained networks is presumably easier to interpret and thus the extraction of simple rules can hopefully be facilitated <ref type="bibr" target="#b13">[14]</ref>. Lastly, from an implementation standpoint, small networks only require limited resources in any physical computational environment.</p><p>To solve the problem of choosing the right size network, two different incremental approaches are often pursued (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> and references therein). The first starts with a small initial network and gradually adds new hidden units or layers until learning takes place. Well-known examples of such growing algorithms are cascade correlation <ref type="bibr" target="#b16">[17]</ref> and others <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b19">[20]</ref>. The second, referred to as pruning, starts with a large network and excises unnecessary weights and/or units. This approach combines the advantages of training large networks (i.e., learning speed and avoidance of local minima) and those of running small ones (i.e., improved generalization) <ref type="bibr" target="#b20">[21]</ref>. However it requires advance knowledge of what size is "large" for the problem at hand, but this is not a serious concern as upper bounds on the number of hidden units have been established <ref type="bibr" target="#b21">[22]</ref>. Among pruning algorithms there are methods that reduce the excess weights/nodes during the training process, such as penalty term methods <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref> and the gain competition technique <ref type="bibr" target="#b25">[26]</ref>, and methods in which the training and pruning processes are carried out in completely separate phases. <ref type="foot" target="#foot_0">1</ref> The latter approach is exemplified by Sietsma and Dow's two-stage procedure <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, Mozer and Smolensky's skeletonization technique <ref type="bibr" target="#b31">[32]</ref>, the optimal brain damage (OBD) algorithm <ref type="bibr" target="#b29">[30]</ref> and the optimal brain surgeon (OBS) <ref type="bibr" target="#b30">[31]</ref>. These post-training pruning procedures do not interfere with the learning process <ref type="bibr" target="#b2">[3]</ref>, but they usually require some retraining to mantain the performance of the original network.</p><p>In this paper, a novel posttraining pruning method for arbitrary feedforward networks is proposed, which aims to select the optimal size by gradually reducing a large trained network. The method is based on the simple idea of iteratively removing hidden units and then adjusting the remaining weights with a view to maintaining the original input-output behavior. This is accomplished by imposing that, at each step, the net input of the units fed by the unit being removed be approximately the same as the previous one, across the entire training set. This amounts to defining a system of linear equations that we solve in the least-squares sense using an efficient preconditioned conjugate gradient procedure. Although the approach does not itself provide a criterion for choosing the units to be removed, a computationally simple rule has been derived directly from the particular class of least-squares procedures employed, and has proved to work well in practice. Besides sharing the advantages of posttraining pruning procedures, the one proposed here exhibits a number of additional features. First, in contrast with many existing algorithms (e.g., <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>), ours does not make use of any working parameter and this frees the designer from a lengthy, problemdependent tuning phase. Second, the proposed algorithm does not require any retraining phase after pruning, like the OBS procedure <ref type="bibr" target="#b30">[31]</ref>, but it requires far less computational effort (see Section III-D for details). Finally, although we shall focus primarily on hidden unit removal in feedforward networks, the approach presented here is quite general and can be applied to networks of arbitrary topology <ref type="bibr" target="#b35">[36]</ref> as well as to the elimination of connections.</p><p>Some algorithms bear similarities with the proposed one. First, the basic idea of removing redundant hidden units and properly adjusting remaining weights was proposed by Sietsma and Dow <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> who heuristically derived a two-stage pruning procedure for layered networks. While their primary goal was to devise specific rules for locating redundant units, we focus mainly on directly solving a linear system without explicitly taking into account the redundancy of individual units. Furthermore, as described in Section IV, their stage-one pruning rules are but special consistency conditions for the system we solve, and the remaining weights update corresponds to a particular solution of that system. Second, the Frobenius approximation reduction method (FARM) <ref type="bibr" target="#b36">[37]</ref> selects the set of hidden units in a layered network so as to preserve, like in our approach, the original training-set behavior. Our algorithm, however, differ significantly as regards how both the units to be removed are selected and the weights of the reduced networks are derived. Further, for FARM to be applicable, the optimal number of hidden units must be determined in advance. This can be carried out by a computationally expensive singular value decomposition (SVD) procedure <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, but it may fail to detect linear dependencies among hidden neurons even in very redundant networks <ref type="bibr" target="#b8">[9]</ref>. Finally, system formulations similar to ours were used in <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b39">[40]</ref>, but for different goals.</p><p>The definitions and notations used in this paper are introduced in Section II. In Section III we formulate the pruning problem in terms of solving a system of linear equations and derive the pruning algorithm. In Section IV the relations between the proposed pruning approach and that of Sietsma and Dow are pointed out. In Section V experimental results on different test problems are presented. Finally, Section VI gives the summary and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DEFINITIONS AND NOTATIONS</head><p>Since the pruning algorithm presented in this paper can be applied to arbitrary feedforward networks, not necessarily layered or fully connected, some definitions and notations must be introduced. A feedforward artificial neural network can be represented by an acyclic weighted directed graph where is a set of units (or neurons), is a set of connections, and is a function that assigns a real-valued weight to each connection positive weights correspond to excitatory connections and negative weights to inhibitory connections. In the following, the more familiar notation will be used instead of Each unit is associated with its own projective field</p><formula xml:id="formula_0">(1)</formula><p>which represents the set of units that are fed by unit and its own receptive field</p><formula xml:id="formula_1">(2)</formula><p>which is the set of units that feed unit In the special case of layered fully connected networks, the receptive and projective fields of a given unit are simply its preceding and succeeding layers, if any, respectively. In the following, we will denote the cardinality of by and the cardinality of by The set of units is divided into three subsets: the set of input units having an empty receptive field, the set of output units having an empty projective field, and the set of hidden units As usual, it is assumed that a particular input unit (here labeled by zero) works as a bias unit which is permanently clamped at 1 and is connected to any noninput unit. Fig. <ref type="figure" target="#fig_0">1</ref> shows an example of feedforward network architecture and illustrates the notations introduced above.</p><p>The network operates as follows. Input units receive from the external environment an activity pattern which is propagated to all units in the corresponding projective fields. Every noninput unit in turn, receives from its own receptive field a net input given by <ref type="bibr" target="#b2">(3)</ref> where represents the output value of unit and sends to its projective field an output signal equal to <ref type="bibr" target="#b3">(4)</ref> being an arbitrary differentiable activation function. The process continues until the output units are reached and their outgoing signals are thus taken to be the actual response of the network. A common choice for the function is the logistic function but no restriction is placed on the type of activation function used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE ALGORITHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>Our approach to network minimization consists of successively removing hidden units after the network has been trained for satisfactory performance. It is assumed that learning is carried out over a sample of training patterns by means of an arbitrary learning procedure (note that the pruning algorithm developed in the following is completely independent of the particular training procedure).</p><p>Assume that hidden unit has somehow been identified as a candidate for removal (later on, we shall address Here, the set of input units is V I = f1; 2; 3; 4g; the set of hidden units is V H = f5; 6; 7g; and the set of output units is V O = f8;9;10g: As an example, the receptive field of node 7 is R 7 = f1;3;5g (light-gray units) whereas its projective field is P 7 = f8;10g (dark-gray units).</p><p>the question of how to choose the units to be excised). First, the elimination of unit involves removing all its incoming and outgoing connections. More precisely, this amounts to stating that the new pruned network will have the following set of connections: where is the connection set of the preceding unpruned network.</p><p>Removing the in/out 's connections is not the whole story. Our approach to network pruning consists first of removing unit and then appropriately adjusting the weights incoming into 's projective field so as to preserve the overall network input/output behavior of the training set. To be more precise, let be a unit of 's projective field Its net input upon presentation of pattern is given by where denotes the output of unit corresponding to pattern After removal of unit will take its own input from <ref type="foot" target="#foot_1">2</ref> In order to maintain the original network behavior, we attempt to adjust the remaining weights incoming into node i.e., the 's for all so that its new net input remains as close as possible to the old one, for all the training patterns (see Fig. <ref type="figure">2</ref>). This amounts to requiring that the following relation holds: <ref type="bibr" target="#b4">(5)</ref> for all and where the 's are appropriate adjusting factors to be determined. Simple algebraic manipulations yield <ref type="bibr" target="#b5">(6)</ref> which is a (typically overdetermined) system of linear equations in the unknowns Observe that represents the total number of connections incoming into 's projective field after unit has been removed.</p><p>It is convenient to represent system (6) in a more compact matrix notation. To do so, consider for each unit the -vector composed of the output values of unit upon presentation of the training patterns Also, let denote the matrix, whose columns are the output vectors of 's new receptive field that is, <ref type="bibr" target="#b6">(7)</ref> where the indexes for all vary in Now, we have to solve the disjoint systems <ref type="bibr" target="#b7">(8)</ref> for every where is the unknown vector, and</p><p>Finally, putting these systems together, we obtain <ref type="bibr" target="#b9">(10)</ref> where</p><formula xml:id="formula_3">(11)<label>(12)</label></formula><p>and ( <ref type="formula">13</ref>)</p><p>Here, the indexes 's vary in Fig. <ref type="figure">2</ref>. Illustration of the pruning process. Unit 7 has been selected to be removed. After pruning, all its incoming and outgoing connections (dashed lines) will be excised and all the weights incoming into its projective field (thick lines) will be properly adjusted.</p><p>To summarize, the core of the proposed pruning approach consists of solving the linear system defined in <ref type="bibr" target="#b9">(10)</ref>. 3 This will be accomplished in the least-squares sense, which amounts to solving the following problem:</p><formula xml:id="formula_4">minimize (<label>14</label></formula><formula xml:id="formula_5">)</formula><p>Since we start with larger than necessary networks, linear dependencies should exist among the output vectors of the hidden units, and the matrix is therefore expected to be rank deficient, or nearly so. This implies that an infinite number of solutions typically exist for problem <ref type="bibr" target="#b13">(14)</ref> and in such cases the one having the lowest norm is generally sought. Because of the particular meaning of our unknown vector however, 3 Instead of directly solving system <ref type="bibr" target="#b9">(10)</ref>, one can independently solve the p h disjoint systems (8). Here, the former approach was preferred because of the moderate dimensions of the networks considered. When large networks are involved, the latter approach can well be pursued.</p><p>we deviate from this practice and will be satisfied with any solution vector, regardless of its length.</p><p>We emphasize that, although it was developed to prune off hidden units, the approach presented here can be applied to remove single connections as well. Supposing that connection is to be removed, the idea is to distribute the weight over the connections incoming into unit in such a way that 's net input remains unchanged across the training set. It is easy to see that this condition is equivalent to solving a single system of equations and unknowns that is identical to <ref type="bibr" target="#b7">(8)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Weight Adjustment by a Conjugate Gradient Least-Squares Method</head><p>To solve the least-squares problem ( <ref type="formula" target="#formula_4">14</ref>), standard QR factorization methods with column pivoting or the SVD technique can be employed <ref type="bibr" target="#b40">[41]</ref>. These methods, however, turn out to be impractical for large and sparse coefficient matrices, and iterative conjugate gradient (CG) methods, especially when used in conjunction with appropriate preconditioning techniques <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, are advantageous in this case. Moreover, CG-methods are particularly attractive because they do not depend upon parameters that are difficult to choose. In this work we made use of a general preconditioned CG-method derived by Björck and Elfving in <ref type="bibr" target="#b43">[44]</ref>, which proved to be extremely efficient thanks not only to its modest computational cost per step, but also to its fast rate of convergence. This section is devoted to a brief description of Björck and Elfving's so-called CGPCNE algorithm, <ref type="foot" target="#foot_2">4</ref> as applied to problem <ref type="bibr" target="#b13">(14)</ref>. Consider our original system as defined in (10)- <ref type="bibr" target="#b12">(13)</ref>. For notational convenience the index which denotes the unit being removed, will be suppressed. Write the matrix as <ref type="bibr" target="#b14">(15)</ref> and recall from the previous section that denotes the number of columns of Here, is a diagonal matrix whose nonzero elements are defined to be <ref type="bibr" target="#b15">(16)</ref> (the notation indicates the th column of matrix and is a strictly lower triangular matrix constructed as <ref type="bibr" target="#b16">(17)</ref> Now, consider the normal system and let denote the following preconditioning matrix: <ref type="bibr" target="#b17">(18)</ref> where represents a relaxation parameter in the interval (0, 2) used to control the convergence rate of the algorithm. Making the change of variables we finally arrive at the preconditioned normal system <ref type="bibr" target="#b18">(19)</ref> Applying the CG-method to <ref type="bibr" target="#b18">(19)</ref>, the CGPCNE algorithm for solving problem ( <ref type="formula" target="#formula_4">14</ref>) results. It begins with an initial tentative solution range and iteratively produces a sequence of points in such a way that the residuals <ref type="bibr" target="#b19">(20)</ref> are monotonically decreased, i.e., for all .</p><formula xml:id="formula_6">Algorithm 1: CGPCNE Least-Squares Algorithm 1) 2) 3) 4) 5) repeat 6) 7) 8) 9) 10) 11) 12) 13) 14) until</formula><p>/* where is a small predetermined constant *.</p><p>Björck and Elfving showed that the vectors and can be computed efficiently according to the following iterative procedure (where .</p><p>Algorithm 2 1)</p><p>2) for downto 1 do 3) 4) 5) endfor 6)</p><p>Analogously, the following algorithm calculates .</p><formula xml:id="formula_7">Algorithm 3 1)</formula><p>2) for to do 3) 4) 5) endfor. Therefore, the above vectors can be computed with only two sweeps through the columns of the matrix so that the overall computational complexity of each CGPCNE cycle turns out to be in the order of where denotes the number of nonzero elements of Analyzing the structure of from <ref type="bibr" target="#b10">(11)</ref>, it is readily seen that and, in turn, is smaller than the total number of connections in the network. Consequently, each CGPCNE cycle requires a number of operations which is roughly proportional to and this is exactly the computational complexity of one epoch of the feedforward backpropagation algorithm <ref type="bibr" target="#b14">[15]</ref> (observe that one CGPCNE cycle "sees" all the training patterns at once, and therefore it should be compared with one backpropagation epoch). However, unlike backpropagation, the CGPCNE algorithm contains dependencies which make it less suitable for parallel implementation. Fortunately, this does not appear to be a serious problem because, as seen in the experimental section, the number of CGPCNE cycles needed to find a solution is typically low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Choosing the To-Be-Removed Units</head><p>One of the fundamental concerns in any pruning algorithm is how best to select the units to be eliminated. One possible strategy is to make use of some relevance or sensitivity measure to quantify the contribution that individual nodes make in solving the network task, and then to select the less relevant units as those to be removed (see, e.g., <ref type="bibr" target="#b31">[32]</ref>). This choice, however, has nothing to do with the basic idea underlying the proposed method, as there is in general no guarantee that selecting the less relevant units will actually make system (10) consistent, or nearly so, which is clearly our ultimate goal.</p><p>Ideally, the most appropriate choice would be to eliminate among all the hidden units the one that, after solving the corresponding problem <ref type="bibr" target="#b13">(14)</ref>, results in the smallest final residual. This guarantees that the removal of that unit and the corresponding updating of the weights will have a minimal effect on the network's input-output behavior. This brute-force approach, however, involves solving as many least-squares problems as there are hidden units and would become impractical even for moderate problem dimensions. Fortunately, the specific method we use to solve problem (14) exhibits a nice property that naturally suggests a suboptimal selection criterion. Namely, recalling that CG-methods monotonically decrease the residuals at each time step, a reasonable approximation to the globally optimal approach mentioned above is to choose the unit to be eliminated in such a way that the initial residual <ref type="bibr" target="#b20">(21)</ref> be the smallest among all the hidden units or, more formally, <ref type="bibr" target="#b21">(22)</ref> Since the initial solution is typically chosen to be the null vector, and recalling the meaning of from ( <ref type="formula" target="#formula_2">9</ref>) and ( <ref type="formula">13</ref>), rule ( <ref type="formula">22</ref>) becomes <ref type="bibr" target="#b22">(23)</ref> Along the same lines, in the case of weight elimination, the rule for detecting the to-be-removed connections would become <ref type="bibr" target="#b23">(24)</ref> It should be clear that the proposed criterion is by no means intended to be the globally optimal choice because there is no guarantee that starting from the smallest initial residual will result in the smallest final residual. Moreover, these selection rules suffer from the same disadvantage as the sensitivity methods <ref type="bibr" target="#b20">[21]</ref>. Namely, since the decision as to which unit/weight to remove is based mainly on the output of individual nodes, we could fail to identify possible correlations among units. To avoid this problem, combined selection criteria that take into account some correlation measure could well be employed, without altering the nature of the proposed pruning algorithm. Despite these caveats, in the present work we prefer this criterion not only because of the encouraging results it yielded in practice but also for its operational simplicity.</p><p>As a final remark, we note that the preceding selection rules have an interesting interpretation which comes from rather different considerations. In fact, as suggested in <ref type="bibr" target="#b44">[45]</ref>, the quantity can be regarded as a measure of the synaptic activity of the connection between units and upon presentation of pattern</p><p>By averaging across all training patterns we obtain Accordingly, the total synaptic activity of unit can be defined by summing the above quantities across all the units in 's projective field and this is proportional to (a nearly identical measure was independently proposed in <ref type="bibr" target="#b45">[46]</ref>, in an attempt to quantify the "goodness" of individual hidden units). Therefore, rules ( <ref type="formula">23</ref>) and ( <ref type="formula">24</ref>) can also be interpreted as criteria that select, respectively, the units and connections having the smallest synaptic activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Definition of the Pruning Algorithm</head><p>We now define precisely the pruning algorithm. Starting with an initial trained network the algorithm iteratively produces a sequence of smaller and smaller networks by first identifying the unit to be removed, and then solving the corresponding system <ref type="bibr" target="#b9">(10)</ref> to properly adjust the remaining weights. The process is iterated until the performance of the reduced networks falls below the designer's requirements. More explicitly, the algorithm can be written as follows.</p><p>Algorithm 4: Proposed Pruning Algorithm 1)</p><p>2) repeat 3) identify excess unit in network according to rule (23) 4) apply the CGPCNE algorithm to find a that solves problem (14) 5) construct as follows: if if 6) 7) until the "performance" of deteriorates excessively.</p><p>Note that the iterative nature of the above procedure allows the network designer to define appropriate performance measures depending on his own requirements (e.g., if separate training and validation sets are available, the network's performance can be measured as the error rate over validation data, in an attempt to improve generalization). Moreover, after pruning, the final network need not be retrained because the updating of the weights is embedded in the pruning algorithm itself.</p><p>Finally, we point out that the overall computational cost of each iteration of our pruning algorithm depends mainly on Step 4) which, as seen in Section III-B, requires a number of operations per step roughly proportional to being the total number of connections in the network. Also, the number of cycles performed by the CGPCNE procedure, like any CGbased least-squares method <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, turns out to be very low and typically far less than either or so that the overall computational complexity of each pruning step can be approximately</p><p>It may be interesting to compare this computational cost with that of the OBS procedure <ref type="bibr" target="#b30">[31]</ref> which, like our algorithm, does not demand further retraining sessions after pruning. Whereas OBS takes time to remove one single weight and update the remaining ones, we are able to remove a processing unit with all its incoming and outgoig connections and adjust the remaining weights in time (a major difference between the two procedures is that, unlike our method, OBS updates all the weights in the network). Furthermore, if our procedure were employed to remove a single connection, say its computational complexity would scale to where is the number of connections incoming into unit since our pruning algorithm is far less computationally expensive than OBS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RELATIONS WITH SIETSMA AND DOW'S PRUNING APPROACH</head><p>In this section we show how the linear system formulation developed in this paper can lead to an alternative manual approach to network pruning, of which Sietsma and Dow's (S&amp;D) algorithm <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref> is the best-known representative (see also <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b46">[47]</ref>). Specifically, rather than trying to directly solve system <ref type="bibr" target="#b9">(10)</ref>, one can study specific consistency conditions for it, thereby deriving rules for locating and removing redundant units. This is basically the approach made by Sietsma and Dow, although they were apparently unaware of the linear system formulation underlying their idea. Starting from simple heuristic considerations, they developed the following simple rules for pruning layered networks.</p><p>1) If the output of hidden unit is constant over the whole training set, i.e., then remove and, for each unit in the succeeding layer (which is 's projective field), add to its bias value the quantity .</p><p>2) If unit gives the same output as a second unit across the entire training set, i.e., for all then remove and adjust 's outgoing weights as . 3) If unit gives opposite output values as a second unit i.e., then remove adjust 's outgoing weights as and, for each in the next layer, add to its bias value the quantity</p><p>We now demonstrate how S&amp;D's rules can easily be derived within our framework. In particular, if any of the previous conditions are fulfilled then all the subsystems defined in ( <ref type="formula">8</ref>) are consistent, and so therefore will be system <ref type="bibr" target="#b9">(10)</ref>. Moreover, the way in which rules 1)-3) update the remaining weights after pruning corresponds to a specific solution of that system. In fact, all the matrices 's defined in <ref type="bibr" target="#b6">(7)</ref> will always contain a column consisting of all "1's", which corresponds to the output of the bias unit. When the output of unit is constant (rule 1), the vector (for all becomes by simply setting and all the other components of at zero, can therefore be written as a linear combination of the columns of which means that is consistent, for any Similar arguments apply to rule 2) where the vector now becomes proportional to the column in corresponding to unit in this case, too, system ( <ref type="formula">8</ref>) is consistent and a solution can be obtained by setting and all other components at zero. Finally, rule 3) can be derived analogously by observing that the vectors 's can be obtained as a linear combination of the columns of the 's corresponding to the "0" (bias) and units.</p><p>It is clear that the major drawback of S&amp;D's approach is its simplicity: in real-world applications, in fact, hidden units are unlikely to be exactly correlated and the algorithm may thus fail to detect redundant elements. Additionally, although rules 1)-3) are particularly simple to implement in networks of threshold units, in the practical case of nonlinear activation functions, they must be translated into more precise criteria. This involves determining a number of problemdependent threshold parameters whose choice is problematic: small threshold values, in fact, typically lead to removing very few redundant units, while large values result in too many excised nodes, thereby seriously worsening the performance of the network. Due to these approximations, a further slow retraining stage is generally required after pruning and we found experimentally that sometimes the retraining process may even fail to converge. The approach to network pruning pursued in this paper offers a general and systematic method for reducing the size of a trained network which contrasts sharply with the heuristic, ad hoc procedure developed by Sietsma and Dow: ours, in fact, is not based on any simplifying assumption, does not depend on any working parameter, and does not demand additional retraining sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>To test the effectiveness of our pruning algorithm, several simulations were carried out over different problems. In all the experiments presented, fully connected neural networks with one hidden layer were considered. For each test problem, ten independent networks (denoted with A to J), with weights randomly generated from a standard Gaussian distribution, were trained by the backpropagation algorithm <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref> and Each trained network was reduced by applying our algorithm, where the CGPCNE procedure was iterated, with relaxation parameter fixed at 1, until the distance between two successive solutions became smaller than For comparison, the S&amp;D pruning algorithm was implemented with a fine-tuned choice of parameters. Specifically, a hidden unit was regarded as having constant output (rule 1) when the variance of its output vector is lower than a threshold two hidden units and had the same output values (rule 2) when the normalized distance is less than a threshold and the same condition was applied to and for locating anti-parallel units (rule 3). The choice of and appeared to be near-optimal for the considered problems. Also, following S&amp;D's practice <ref type="bibr" target="#b28">[29]</ref>, before detecting the units to be removed, output values less than 0.35 and greater 0.65 were approximated to zero and one, respectively. Units were removed in the following order, shown to be the best: first constant-output units, then parallel units, and finally antiparallel units.</p><p>To evaluate the behavior of both methods, three different measures were adopted: 1) the number of hidden nodes in the reduced networks; 2) the recognition rate, measured as the proportion of examples for which all network output values differed from the corresponding target by less than 0.5; and 3) the usual mean-squared error (MSE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parity and Symmetry Problems</head><p>To assess the performance of the proposed method, the well-known parity and symmetry tasks <ref type="bibr" target="#b1">[2]</ref> were chosen, as the near-optimal number of hidden units required to achieve their solution is known. In the case of the parity task, this is equal to the number of input units (but, see <ref type="bibr" target="#b47">[48]</ref> for smaller solution networks), while the symmetry problem can be solved with only two hidden nodes, whatever the length of the input string. In both series of experiments, ten randomly initialized 4-10-1 networks were trained until, for each pattern in the training set, all the network output values differed from the corresponding targets by less than 0.05.</p><p>Then, to evaluate the behavior of our pruning procedure under "stressed" conditions, each trained network was pruned (regardless of its performance) until the hidden layer contained exactly one unit. The average results are shown in Fig. <ref type="figure" target="#fig_1">3</ref> which plots, for both problems, the evolution of the recognition rate and the MSE during the pruning process. As can be seen, our algorithm exhibits very steady behavior in the first stages of pruning, whereas, as may be expected, it performs poorly when near-optimal size is approached. Also, as shown in Fig. <ref type="figure" target="#fig_2">4</ref>, the number of CGPCNE iterations required to find a solution for each pruning step is extremely small and decreases linearly with the number of hidden units. This is in agreement with theoretical and experimental results reported in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b42">[43]</ref>.</p><p>However, for the proposed procedure to produce useful small networks, a stopping criterion should be specified, which takes into account the performance of the reduced networks. For the two tasks at hand, the following stopping rule was adopted, aiming to obtain networks with the same trainingset performance as the original ones. At each pruning step, the performance of the reduced network, measured as the recognition rate over the training set, was compared with that of the original network. If a deterioration of 1% or more was observed, then pruning was stopped and the previous reduced network was retained as the final one. The results obtained by our pruning procedure with the above stopping criterion, and the S&amp;D pruning algorithm with no retraining phase, are summarized in Table <ref type="table" target="#tab_0">I</ref> for parity and Table II for symmetry. As can be observed, our procedure appears extremely robust since all trials resulted in a near-optimal solution network with perfect 100% recognition rate, irrespective of the initial conditions. The S&amp;D method, by contrast, exhibits much more unstable behavior with poorer recognition performance of the pruned networks, that were therefore later retrained. We found that in the case of the parity problem five out of ten trials failed to converge within 3000 epochs. The remaining five required a median number of 24 (ranging from zero to 119). In the symmetry case, instead, all the retraining trials converged to a solution in a median number of 48 epochs.</p><p>Finally, to compare the time required to train and prune a large solution network with that for directly training a small network, ten independent 4-5-1 and 4-4-1 networks were trained for parity and symmetry respectively; the size of these networks corresponds to the "median" sizes found by our procedure. Tables <ref type="table" target="#tab_1">III</ref> and<ref type="table" target="#tab_2">IV</ref> show the median number of epochs required to train such small networks, for parity and symmetry, respectively, and the median number of backpropagation as well as cycles (which, as seen in Section III-B, have comparable computational complexity) needed to train the original 4-10-1 networks and then reduce them to the median size. As can be observed, in any case the overall time required to train a large network and then prune it to a small size compares very favorably with that of simply training a small network. In addition, for initially small networks, convergence is not always guaranteed; in fact, seven out of ten parity trials did not converge within 3000 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Simulated Pattern Recognition Task</head><p>A classification task nearer to real-world problems, suggested in <ref type="bibr" target="#b48">[49]</ref> and used as a test problem by many authors   [50]- <ref type="bibr" target="#b51">[52]</ref>, was chosen to test the generalization performance of the networks reduced by our procedure. This is a two-class simulated pattern recognition problem with a two-dimensional continuous feature space, and known probability distributions. As shown in Fig. <ref type="figure" target="#fig_3">5</ref>, class 1 is a single Gaussian distribution, while class 2 is an equal likelihood mixture of two Gaussians.</p><p>In the experiments reported here the values and were used, which correspond to "mixture 2" data used by others <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>. A training set of 200 samples, and two separate 1000-sample data sets, one for validation and the other for testing, were randomly generated from the Gaussian distributions for both classes with equal probability. Due to overlapping between classes, perfect learning cannot be achieved. Accordingly, ten randomly initialized 2-10-1 networks were trained for 1000 epochs. After training, the pruning procedure was applied. Fig. <ref type="figure" target="#fig_4">6</ref> shows the results of running the algorithm until all but one unit remained in the hidden layer; the performance over both the training and the validation set, averaged over the ten trials, is reported. It can be observed how the algorithm exhibits a very stable behavior, both in terms of recognition rate and MSE and even with just one hidden unit performance deterioration is negligible. In Fig. <ref type="figure" target="#fig_5">7</ref>, the (average) number of CGPCNE cycles is plotted as a function of the size of the hidden layer. As in both the previous logical tasks, linear behavior can be observed.</p><p>In practical real-world applications, a better generalization performance is more important than optimal behavior over the training data. As suggested by many authors <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, one way to improve generalization and thereby avoid overfitting consists of stopping the learning stage as soon as a deterioration of the network performance over a separate validation set is observed. Recently, this approach has proved to be effective when combined with some pruning process <ref type="bibr" target="#b54">[55]</ref>. Accordingly, the following rule was employed to stop pruning. Whenever a hidden unit was removed, the recognition rate of the new smaller network was evaluated over the validation set, and then compared with the performance of the previous network. If a deterioration of 1% or more was observed, then pruning was stopped and the previous reduced network was taken as the final one.</p><p>Table <ref type="table" target="#tab_3">V</ref> summarizes the results obtained by applying our pruning with the above stopping condition as well as the S&amp;D results. It is clear that our procedure yields a better performance than S&amp;D procedure, in terms of network size reduction. Comparable results, instead, were obtained in terms of recognition rate over both the training and validation set. The original networks performed better than the pruned ones over the training set, due to the particular rule employed to stop pruning, which does not take into account the performance of the reduced networks over the training data as pruning proceeds. Besides, it is well known that good generalization results are typically achieved when performance over the learning set is nonoptimal <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>. On the other hand, when the performance on the training set is important, alternative criteria can be employed to stop pruning, in order to avoid loss of accuracy on training data. Next, we evaluated the generalization ability of the reduced networks with respect to the original ones. The statistics of the performance measures, number of hidden units and recognition rate computed over the test set, are listed in Table <ref type="table" target="#tab_4">VI</ref>. As can be seen, the networks reduced by our procedure generalize slightly better than the original ones as well as the ones reduced by S&amp;D.</p><p>Finally, we point out that the generalization results of the networks reduced by our pruning algorithm are not only superior to those found by Holt <ref type="bibr" target="#b50">[51]</ref> under experimental conditions similar to ours (i.e., same classification problem, equal training and test set sizes, network architecture, and learning stopping criterion), but are comparable with the best results he obtained using an alternative cost function aiming to improve the generalization performance. Moreover, the median size of our reduced networks (2-3-1) coincides with the minimal network size required for achieving the best   generalization results for this problem, as claimed in <ref type="bibr" target="#b50">[51]</ref> and <ref type="bibr" target="#b48">[49]</ref>. This demonstrates once again that the proposed pruning procedure is able to find the most appropriate network architecture to solve a given problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>This paper describes a method for reducing the size of trained feedforward neural networks, in which the key idea consists of iteratively removing hidden units and then adjusting the remaining weights in such a way as to preserve the overall network behavior. This leads to formulating the pruning problem in terms of defining a system of linear equations that we solve with a very efficient conjugate gradient least-squares procedure. A simple and effective rule is also derived which proves to work well in practice, and turns out to be closely related with a selection rule derived elsewhere in a different context. However, alternative selection rules can be adopted as well, without altering the method as a whole. A nice feature is that no parameter needs to be set, and this contrasts with most existing pruning procedures. Furthermore, the iterative nature of the algorithm permits the network designer to monitor the behavior of the reduced networks at each stage of the pruning process, so as to define his own stopping criterion. The experimental results obtained prove that the method does a very good job of reducing the network size while preserving excellent performance, without requiring the additional cost of a retraining phase. In addition, the time required to train a small network is typically much longer than that needed to train a large network and then reduce its size by means of our pruning procedure. The results of the experimental comparison with a well-known pruning procedure show that the proposed one yields better results in terms of network size, performance, and robustness.</p><p>Although we have focused primarily on the problem of removing units, our approach is naturally applicable to the elimination of single connections as well. Our choice was motivated by the observation that computational nodes represent the "bottleneck" through which information in a neural network is conveyed, and are therefore more important than individual connections <ref type="bibr" target="#b25">[26]</ref>. Moreover, node pruning algorithms are more efficient, although less accurate, than weight elimination methods. However a "coarse-to-fine" approach could be pursued by removing first units and then, when no unit can be further excised, single connections. Finally, we emphasize that the proposed pruning approach is far more general than we have presented here and can be applied to networks of arbitrary topology.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example of feedforward network architecture to illustrate notations (the bias unit zero has not been included to make representation easier).</figDesc><graphic coords="3,40.98,59.58,255.12,432.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Behavior of the recognition rate and the MSE during the pruning process. (a) Parity problem; (b) symmetry problem.</figDesc><graphic coords="9,41.52,440.58,254.16,138.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Number of CGPCNE cycles at each pruning step. (a) Parity problem; (b) symmetry problem.</figDesc><graphic coords="9,305.52,438.90,252.24,138.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Gaussian densities used in the pattern recognition example.</figDesc><graphic coords="10,307.20,285.18,248.88,232.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Behavior of the recognition rate and the MSE during the pruning process for the pattern recognition problem. Both the behavior over the training and validation sets are displayed.</figDesc><graphic coords="11,83.04,180.66,434.16,239.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Number of CGPCNE cycles at each pruning step for the pattern recognition problem.</figDesc><graphic coords="11,42.48,468.18,252.24,137.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,40.98,59.58,255.12,431.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,115.80,80.88,368.64,200.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,116.16,314.70,367.92,201.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I NUMERICAL</head><label>I</label><figDesc>RESULTS FOR THE PARITY PROBLEM TABLE II NUMERICAL RESULTS FOR THE SYMMETRY PROBLEM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III MEDIAN</head><label>III</label><figDesc>NUMBER OF BACKPROP/CGPCNE CYCLES REQUIRED BOTH TO TRAIN A SMALL NETWORK AND TO TRAIN-AND-PRUNE A LARGE NETWORK, FOR THE PARITY PROBLEM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV MEDIAN</head><label>IV</label><figDesc>NUMBER OF BACKPROP/CGPCNE CYCLES REQUIRED BOTH TO</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V NUMERICAL</head><label>V</label><figDesc>RESULTS ON TRAINING AND VALIDATION SETS FOR THE PATTERN RECOGNITION PROBLEM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI GENERALIZATION</head><label>VI</label><figDesc>RESULTS FOR THE PATTERN RECOGNITION PROBLEM</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This idea has been proved to be quite effective in the analogous problem of determining the proper dimension of a classification tree, thereby improving its generalization performance<ref type="bibr" target="#b26">[27]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For convenience, it is assumed here that the following condition is always satisfied 8i 2 P h : R i 0 f0;hg 6 = ;, which means that, after removing node h; all the units of its projective field will receive at least one input other than the bias signal.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The acronym CGPCNE is not explicitly defined in<ref type="bibr" target="#b43">[44]</ref>; however, it should stand for "conjugate gradient preconditioned normal equation."</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Giovanna Castellano was born in Bari, Italy, on <ref type="bibr">June 27, 1969</ref>. She received the "Laurea" degree with honors in computer science from the University of Bari, Italy in 1993, with a thesis on structural optimization of artificial neural networks.</p><p>From 1993 to 1995 she worked at the Computer Science Department of the University of Bari for research in the field of artificial neural networks. Since March 1995, she has been a Researcher at the Institute for Signal and Image Processing with a scholarship under a grant from the "Consiglio Nazionale delle Ricerche." Her current research interests include artificial neural networks, fuzzy systems, neuro-fuzzy modeling, intelligent control using fuzzy logic, robotics, and autonomous systems.</p><p>Anna Maria Fanelli (M'89) was born in Bari, Italy, on June 29, 1949. She received the "Laurea" degree in physics from the University of Bari, Italy, in 1974.</p><p>From 1975 to 1979, she was full-time Researcher at the Physics Department of the University of Bari, Italy, where she became Assistant Professor in 1980. In 1985 she joined the Department of Computer Science at the University of Bari, Italy, as Professor of Computer Science. Currently, she is responsible for the courses "computer systems architectures" and "neural networks" at the degree course in computer science. Her research activity has involved issues related to pattern recognition, image processing, and computer vision. Her work in these areas has been published in several journals and conference proceedings. Her current research interests include artificial neural networks, genetic algorithms, fuzzy systems, neuro-fuzzy modeling, and hybrid systems.</p><p>Prof. Fanelli is a Member of the System, Man, and Cybernetics Soceity and the International Neural Network Society. She is also on the editorial board of the journal Neural Processing Letters.</p><p>Marcello Pelillo (M'92) was born in Taranto, Italy, on June 1, 1966. He received the "Laurea" degree with honors in Computer Science from the University of Bari, Italy, in 1989.</p><p>From 1988 to 1989 he was at the IBM Scientific Center in Rome, where he was involved in studies on natural language and speech processing. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Progress in supervised neural networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Hush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="8" to="39" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Parallel Distributed Processing-</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Foundations</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Rumelhart</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mcclelland</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
			<date type="published" when="1986">1986</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An algebraic projection analysis for optimal hidden units size and learning rates in backpropagation learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Networks</title>
		<meeting>IEEE Int. Conf. Neural Networks<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning sets of filters using backpropagation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Speech Language</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="61" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Experiments on neural net recognition of spoken and written text</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Burr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1162" to="1168" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Can backpropagation error surface not have local minima</title>
		<author>
			<persName><forename type="first">X.-H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1019" to="1021" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An introduction to computing with neural nets</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acoust., Speech, Signal Processing Mag</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="4" to="22" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Determining and improving the fault tolerance of multilayer perceptrons in a pattern-recognition application</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Emmerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">I</forename><surname>Damper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="788" to="793" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What size net gives valid generalization?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large automatic learning, rule extraction, and generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Solla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="877" to="922" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalization and network design strategies</title>
		<author>
			<persName><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionism in</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Perspective</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Pfeifer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Schreter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Fogelman-Soulie</surname></persName>
		</editor>
		<editor>
			<persName><surname>Steels</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="143" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalization performance of overtrained backpropagation networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks-Proc. EURASIP Wkshp</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Almeida</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Wellekens</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1990">1990. 1990</date>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constructive induction in knowledge-based neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Towell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Craven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Shavlik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Wkshp. Machine Learning</title>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Birnbaum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Collins</surname></persName>
		</editor>
		<meeting>8th Int. Wkshp. Machine Learning<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="213" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Introduction to the Theory of Neural Computation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Redwood City, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Configuration of feedforward multilayer perceptron network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
	<note>unpublished manuscript</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The cascade-correlation learning architecture</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Fahlman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lebiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="524" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Optimal linear discriminants</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Conf. Pattern Recognition</title>
		<meeting>8th Int. Conf. Pattern Recognition<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="849" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic node creation in backpropagation networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="375" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning in feedforward layered networks: The Tiling algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mézard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Nadal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Phys. A</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2191" to="2204" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pruning algorithms-A review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bounds on the number of hidden neurons in multilayer perceptrons</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="47" to="55" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Connectionist learning procedures</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A backpropagation algorithm with optimal use of hidden units</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chauvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 1</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalization by weight-elimination with application to forecasting</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="875" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Creating local and distributed bottlenecks in hidden layers of backpropagation networks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kruschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1988 Connectionist Models Summer School</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</editor>
		<meeting>1988 Connectionist Models Summer School<address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="120" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<title level="m">Classification and Regression Trees</title>
		<meeting><address><addrLine>Belmont, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Wadsworth</publisher>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Neural net pruning: Why and how</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sietsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J F</forename><surname>Dow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Neural Networks</title>
		<meeting>IEEE Int. Conf. Neural Networks<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Creating artificial neural networks that generalize</title>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="67" to="79" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2, D. S. Touretsky</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Second-order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Using relevance to reduce network size automatically</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Sci</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A simple procedure for pruning backpropagation trained neural networks</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Karnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="239" to="242" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimization of the architecture of feedforward neural networks with hidden layers by unit elimination</title>
		<author>
			<persName><forename type="first">A</forename><surname>Burkitt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Complex Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="371" to="380" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A node pruning algorithm for backpropagation networks</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="301" to="314" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pruning in recurrent neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Artificial Neural Networks (ICANN&apos;94)</title>
		<meeting>Int. Conf. Artificial Neural Networks (ICANN&apos;94)<address><addrLine>Sorrento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="451" to="454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Frobenius approximation reduction method (FARM) for determining optimal number of hidden units</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. J. Conf. Neural Networks</title>
		<meeting>Int. J. Conf. Neural Networks<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="163" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzes of the hidden units of backpropagation model by singular value decomposition (SVD)</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Tompkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. J. Conf. Neural Networks</title>
		<meeting>Int. J. Conf. Neural Networks<address><addrLine>Washington, D.C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="739" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Structural simplification of a feedforward multilayer perceptron artificial neural network</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Tompkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>Int. Conf. Acoust., Speech, Signal essing<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="1061" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple method to derive bounds on the size and to train multilayer neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Sartori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Antsaklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="467" to="471" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Johns Hopkins Univ. Press</publisher>
			<pubPlace>Baltimore, MD</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Methods for sparse linear least-squares problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Björck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Sparse Matrix Computations</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Bunch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Rose</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="177" to="199" />
			<date type="published" when="1976">1976</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey of sparse matrix research</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Duff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="500" to="535" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accelerated projection methods for computing pseudoinverse solutions of systems of linear equations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Björck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Elfving</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BIT</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="145" to="163" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic pruning in artificial neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Uncini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Luminari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ascone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architectures and Neural Networks</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Caianello</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="199" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A backpropagation algorithm which automatically determines the number of association units</title>
		<author>
			<persName><forename type="first">K</forename><surname>Murase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. J. Conf. Neural Networks</title>
		<meeting>Int. J. Conf. Neural Networks<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="783" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Structure Level Adaptation for Artificial Neural Networks</title>
		<author>
			<persName><forename type="first">T.-C</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Kluwer</publisher>
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Speed up learning and network optimization with extended backpropagation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="365" to="383" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">How limited training data can allow a neural network to outperform an optimal statistical classifier</title>
		<author>
			<persName><forename type="first">L</forename><surname>Niles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Silverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tajchman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Acoust., Speech, Signal Processing</title>
		<meeting>Int. Conf. Acoust., Speech, Signal essing<address><addrLine>Glasgow, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning vector quantization for the probabilistic neural network</title>
		<author>
			<persName><forename type="first">P</forename><surname>Burrascano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="458" to="461" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Comparison of generalization in multilayer perceptrons with the log-likelihood and least-squares cost functions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J J</forename><surname>Holt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Int. Conf. Pattern Recognition</title>
		<meeting>11th Int. Conf. Pattern Recognition<address><addrLine>The Hague, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On an asymptotically optimal adaptive classifier design criterion</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tenorio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="312" to="318" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generalization and parameter estimation in feedforward nets: Some experiments</title>
		<author>
			<persName><forename type="first">N</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Touretzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 2</title>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="630" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Predicting the future: A connectionist approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weigend</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="209" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving model selection by nonconvergent methods</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zinnoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hergert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
