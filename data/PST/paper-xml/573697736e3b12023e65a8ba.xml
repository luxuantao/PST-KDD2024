<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brendan</forename><surname>Jou</surname></persName>
							<email>bjou@ee.columbia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Chen</surname></persName>
							<email>taochen@ee.columbia.edu</email>
						</author>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
							<email>npappas@idiap.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Yahoo Labs London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">JW Player</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miriam</forename><surname>Redi</surname></persName>
							<email>redi@yahoo-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Mercan</forename><surname>Topkara</surname></persName>
							<email>mercan@jwplayer.com</email>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Idiap Research Institute/EPFL Martigny</orgName>
								<address>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Affect Around the World: A Large-scale Multilingual Visual Sentiment Ontology</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3C53BF56B37A0F96D3AED5364D212537</idno>
					<idno type="DOI">10.1145/2733373.2806246</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T05:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>H.5.4 [Information Interfaces and Presentation]: Hypertext/Hypermedia</term>
					<term>I.2.10 [Artificial Intelligence]: Vision and Scene Understanding Multilingual</term>
					<term>Language</term>
					<term>Cultures</term>
					<term>Cross-cultural</term>
					<term>Emotion</term>
					<term>Sentiment</term>
					<term>Ontology</term>
					<term>Concept Detection</term>
					<term>Social Multimedia * Denotes equal contribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Every culture and language is unique. Our work expressly focuses on the uniqueness of culture and language in relation to human affect, specifically sentiment and emotion semantics, and how they manifest in social multimedia. We develop sets of sentiment-and emotion-polarized visual concepts by adapting semantic structures called adjective-noun pairs, originally introduced by Borth et al. <ref type="bibr" target="#b5">[5]</ref>, but in a multilingual context. We propose a new language-dependent method for automatic discovery of these adjective-noun constructs. We show how this pipeline can be applied on a social multimedia platform for the creation of a large-scale multilingual visual sentiment concept ontology (MVSO). Unlike the flat structure in [5], our unified ontology is organized hierarchically by multilingual clusters of visually detectable nouns and subclusters of emotionally biased versions of these nouns. In addition, we present an image-based prediction task to show how generalizable language-specific models are in a multilingual context. A new, publicly available dataset of &gt;15.6K sentiment-biased visual concepts across 12 languages with language-specific detector banks, &gt;7.36M images and their metadata is also released.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>If you scoured the world and took several people at random from major countries and asked them to fill in the blank " love" in their native tongue, how many unique adjectives would you expect to find? Would people from some cultures tend to fill it with twisted, while others pure or unconditional or false? All over the world, we daily express our thoughts and feelings in culturally isolated contexts; and when we travel abroad, we know that to cross a physical border also means to cross into the unique behaviors and interactions of that people group -its cultural border. How similar or different are our sentiments and feelings from this other culture? Or the thoughts and objects we tend to talk about most? Motivated by questions like this, our work explores the computational understanding of human affect along cultural lines, with focus on visual content. In particular, we seek to answer the following important questions: <ref type="bibr" target="#b1">(1)</ref> how are images in various languages used to express affective visual concepts, e.g. beautiful place or delicious food ? And (2) how are such affective visual concepts used to convey different emotions and sentiment across languages?</p><p>Figure <ref type="figure">2</ref>: The construction process of our multilingual visual sentiment ontology (MVSO) begins with crawling images and metadata based on emotion keywords. Image tags (t1, . . . , t5) are labeled with part-of-speech tags, and adjectives and nouns are used to form candidate adjective-noun pair (ANP) combinations <ref type="bibr" target="#b5">[5]</ref>, while others are ignored (in red). Finally, these candidate ANPs are filtered based on various criteria (Sec. 3.2) which help remove incorrect pairs (in red), forming a final MVSO with diversity and coverage.</p><p>In Psychology, there are two major schools-of-thought on the connection between cultural context and human affect, i.e. our experiential feelings via our sentiments and emotions. Some believe emotion to be culture-specific <ref type="bibr" target="#b29">[29]</ref>, that is, emotion is dependent on one's cultural context, while others believe emotion to be universal <ref type="bibr" target="#b16">[16]</ref>, that is, emotion and culture are independent mechanisms. For example, while this paper is written in English, there are emotion words/phrases in other languages for which there is no exact translation in English, e.g., Schadenfreude in German refers to the pleasure at someone else's expense. Do Englishspeakers not feel those same emotions or do they simply refer to them in a different way? Or even if the reference is the same, perhaps the underlying emotion is different?</p><p>In Affective Computing <ref type="bibr" target="#b33">[33]</ref> and Multimedia, we often refer to the affective gap as the conceptual divide between the low-level visual stimuli, like images and features, and the high-level, abstracted semantics of human affect, e.g. happy or sad. In one attempt to bridge sentiment and visual media, Borth et al. <ref type="bibr" target="#b5">[5]</ref> developed a visual sentiment ontology (VSO), a set of 1,200 mid-level concepts using structured semantics called adjective-noun pairs (ANPs). The noun portion of the ANP allows for computer vision detectability and the adjective serves to polarize the noun toward a positive or negative sentiment, or emotion, e.g. so instead of having visual concepts like sky or dog, we have beautiful sky or scary dog. Many works like this that have built algorithms, models and datasets on the assumption of the psychology theory that emotions are universal. However, while such works provide great research contributions in that native language, their applicability and generalization to other languages and cultures remains largely unexplored.</p><p>We present a large-scale multilingual visual sentiment ontology (MVSO) and dataset including adjective-noun pairs from 12 languages of diverse origins: Arabic, Chinese, Dutch, English, French, German, Italian, Persian, Polish, Russian, Spanish, and Turkish. We make the following contributions:</p><p>(1) a principled, context-aware pipeline for designing a multilingual visual sentiment ontology, (2) a Multilingual Visual Sentiment Ontology mined from social multimedia data end-to-end, (3) a MVSO organized hierarchically into nounbased clusters and sentiment-biased adjective-noun pair subclusters, (4) a multilingual, sentiment-driven visual concept detector bank, and (5) the release of a dataset containing MVSO and large-scale image collection with benchmark cross-lingual sentiment prediction<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>We address the general challenge of affective image understanding, aiming at both recognition and analysis of sentiment and emotions in visual data, but from a multilingual and cross-cultural perspective. Our work is closely related to Multimedia and Vision research that focus on visual aesthetics <ref type="bibr" target="#b22">[22]</ref>, interestingness <ref type="bibr" target="#b14">[14]</ref>, popularity <ref type="bibr" target="#b23">[23]</ref>, and creativity <ref type="bibr" target="#b35">[35]</ref>. Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b34">34]</ref>, but also neuroaesthetics <ref type="bibr" target="#b41">[41]</ref>, visual preference <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b44">44]</ref>, and social interaction <ref type="bibr" target="#b28">[28]</ref>.</p><p>Progressive research in "visual affect" recognition was done in <ref type="bibr" target="#b42">[42]</ref> and <ref type="bibr" target="#b27">[27]</ref> where image features were designed based on art and psychology principles for emotion prediction. And such works were later improved in <ref type="bibr" target="#b18">[18]</ref> by adding social media data in semi-supervised frameworks. From this research effort in visual affect understanding, several affective image datasets were released to the public. The International Affective Picture System (IAPS) dataset <ref type="bibr" target="#b25">[25]</ref> is a seminal dataset of ∼1,000 images, focused on induced emotions in humans for biometric measurement. The Geneva Affective PicturE Database (GAPED) <ref type="bibr" target="#b8">[8]</ref> consists of 730 pictures meant to supplement IAPS and tries to narrow the themes across images. And recently, in <ref type="bibr" target="#b5">[5]</ref>, a visual sentiment ontology (VSO) and dataset was created from Flickr image data, resulting in a collection of adjective-noun pairs along with corresponding images, tags and sentiment. One major issue with these datasets and existing methods is that they do not consider the context in which emotions are felt and perceived. Instead, they assume that visual affect is universal, and do not account for the influence of culture or language. We explicitly tackle visual affect understanding from a multi-cultural, multilingual perspective. In addition, while existing works often use handpicked data, we gather our data "in the wild" on a popular, multilingual social multimedia platform. The study of emotions across language and culture has long been a topic of research in Psychology. A main contention in the area concerns whether emotions are culturespecific <ref type="bibr" target="#b29">[29]</ref>, i.e. their perception and elicitation varies with the context, or universal <ref type="bibr" target="#b16">[16]</ref>. In <ref type="bibr" target="#b36">[36]</ref>, a survey of crosscultural work on semantics surrounding emotion elicitation and perception is given, showing that there are still competing views as to whether emotion is pan-cultural, culturespecific, or some hybrid of both. Inspired by research in this domain, we are the first to investigate the relationship between visual affect and culture<ref type="foot" target="#foot_1">2</ref> from a multimedia-driven and computational perspective, as far as we know.</p><p>Other work in cross-lingual research comes from text sentiment analysis and music information retrieval. In <ref type="bibr" target="#b3">[3]</ref> and <ref type="bibr" target="#b31">[31]</ref>, they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively. In <ref type="bibr" target="#b26">[26]</ref> and <ref type="bibr" target="#b17">[17]</ref>, they presented approaches to indexing digital music libraries with music from multiple languages. Specific to emotion, <ref type="bibr" target="#b17">[17]</ref> tried to highlight differences between languages by building models for predicting the musical mood and then cross-predicting in other languages. Unlike these works, we propose a multimediadriven approach for cross-cultural visual sentiment analysis in the context of online image collections.</p><p>It is important to distinguish our work from that of Borth et al. on VSO <ref type="bibr" target="#b5">[5]</ref> and its associated detector bank, SentiBank <ref type="bibr" target="#b4">[4]</ref>. Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b21">21]</ref>, social media commenting <ref type="bibr" target="#b7">[7]</ref>, etc. However, in addition to lack of multilingual support, there are several technical challenges with VSO <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5]</ref> that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to "constructed" pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.. Our proposed MVSO discovery method can be easily extended to any language, while achieving greater coverage and diversity than VSO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ONTOLOGY CONSTRUCTION</head><p>An overview of the proposed method for multilingual visual sentiment concept ontology construction is shown in Figure <ref type="figure">2</ref>. In the first stage, we obtain a set of images and their tags using seed emotion keyword queries, selected according to emotion ontologies from psychology such as <ref type="bibr" target="#b34">[34]</ref> or <ref type="bibr" target="#b11">[11]</ref>. Next, each image tag is labeled automatically by a language-specific part-of-speech tagger and adjective-noun combinations are discovered from words in the tags. Then, the combinations are filtered based on language, semantics, sentiment, frequency and diversity filters to ensure that the final set of ANPs have the following properties: (a) are written in the target language, (b) they do not refer to named entities or technical terms, (c) reflect a non-neutral sentiment, (d) are frequently used, and (e) are used by a non-trivial number of users of the target language.</p><p>The discovery of affective visual concepts for these languages using adjective-noun pairs poses several challenges in lexical, structural and semantic ambiguities, which are well-known problems in natural language processing. Lexical ambiguity is when a word has multiple meanings which depend on the context, e.g. sport jaguar or forest jaguar. Structural ambiguity is when a word might have different grammatical interpretation depending on the position in the context, e.g. ambient light or light room. Semantic ambiguity is when a combination of words with the same syntactic structure have different semantic interpretation, e.g. big apple. We selected languages in our MVSO according to the availability of public natural language processing tools and sentiment ontologies per language so that automatic processing was feasible. In addition, we sought to cover a wide range of geographic regions from the Americas to Europe and to Asia. We settled on 12 languages: Arabic, Chinese, Dutch, English, French, German, Italian, Persian, Polish, Russian, Spanish, and Turkish.</p><p>We applied our proposed data collection pipeline to a popular social multimedia sharing platform, Yahoo! Flickr<ref type="foot" target="#foot_2">3</ref> , and collected public data from November 2014 to February 2015 using the Flickr API. We selected Flickr because there is an existing body of multimedia research using it in the past, and in particular, <ref type="bibr" target="#b20">[20]</ref> describes how Flickr satisfies two conditions for making use of the "wisdom of the social multimedia": popularity and availability. We do not repeat the argument in <ref type="bibr" target="#b20">[20]</ref>, but note that in addition to those benefits, Flickr has multilingual support and the use of Flickr facilitates a natural comparison to the seminal VSO <ref type="bibr" target="#b5">[5]</ref> work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Adjective-Noun Pair Discovery</head><p>As our seed emotion ontology, we selected the Plutchik's Wheel of Emotions <ref type="bibr" target="#b34">[34]</ref>. This psychology ontology was selected because it consists of graded intensities for multiple basic emotions providing a richer set of emotional valences compared to alternatives like <ref type="bibr" target="#b11">[11]</ref>; it has also been shown to be useful for VSO <ref type="bibr" target="#b5">[5]</ref>. The Plutchik emotions are organized by eight basic emotions, each with three valences: ecstasy &gt; joy &gt; serenity; admiration &gt; trust &gt; acceptance; terror &gt; fear &gt; apprehension; amazement &gt; surprise &gt; distraction; grief &gt; sadness &gt; pensiveness; loathing &gt; disgust  Multilingual Query Construction: To obtain seeds for each language, we recruited 12 native and proficient language speakers to provide a set of translated or synonymous keywords to those of the 24 Plutchik emotions. Speakers were allowed to use any number of keywords per emotion since the possible synonyms per emotion and language can vary, but they were asked to rank their chosen keywords along each emotion seed. They were also allowed to use tools like Google Translate 4 or other resources to enrich their emotion keywords. Table <ref type="table" target="#tab_0">1</ref> lists top ranked keywords according to speakers for 7 out of 12 languages in each emotion.</p><p>Given the set of keywords</p><formula xml:id="formula_0">E (l) = {e (l) ij | i = 1 .</formula><p>. . 24, j = 1 . . . ni} describing each emotion i per language l, where ni is the number of keywords per emotion i, we performed tagbased queries on tags with the Flickr API to retrieve images and their related tags. Like <ref type="bibr" target="#b5">[5]</ref>, for each emotion, we chose to sample only the top 50K images ranked by Flickr relevance to simply limit the size of our results, but if an emotion had less than 50K images, we extended the search to additional metadata, i.e. title and description.</p><p>Part-of-speech Labeling: To identify the type of each word in a Flickr tag, we performed automatic part-of-speech labeling using pre-trained language-specific taggers which achieve high accuracy (&gt;95% for most languages), namely TreeTagger <ref type="bibr" target="#b37">[37]</ref>, Stanford tagger <ref type="bibr" target="#b39">[39]</ref>, HunPos tagger <ref type="bibr" target="#b15">[15]</ref> and a morphological analyzer for Turkish <ref type="bibr" target="#b13">[13]</ref>. Though not all the tags contained multiple words, the average number of words was always greater than the average number of tags for all languages, so word context was almost always taken into account. From the full set of part-of-speech labels, we retained identified nouns, adjectives and other part-ofspeech types which can be used as adjectives, such as simple or past participle (e.g. smiling face) in English.</p><p>Discovery Strategy: We based our discovery strategy for ANPs on co-occurrence in image tags, that is, if an adjective-noun pair is relevant to the specific emotion it should appear at least once as that exact pair phrase in the crawled images for that emotion. To validate the com-4 translate.google.com pleteness of our strategy we compared with VSO and found that ∼86% of ANPs discovered by VSO <ref type="bibr" target="#b5">[5]</ref> overlap with the English ANPs discovered by our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Filtering Candidate Adjective-Noun Pairs</head><p>From these discovered ANPs, we applied several filters to ensure they satisfied the following criteria: (a) written in the target language, (b) do not refer to named entities, (c) reflect a non-neutral sentiment, (d) frequently used and (e) used by multiple speakers of the language.</p><p>Language &amp; Semantics: We used a combination of language dictionaries<ref type="foot" target="#foot_3">5</ref> instead of language classifiers to verify the correctness of the ANP as the performance of using the latter was low for short-length text, especially for Romance languages which share characters. All of the English ANPs were classified as indeed English by the dictionary, while for other languages, ANPs were removed if they passed the English dictionary filter but not the target language dictionary. The intuition for this was that most candidate ANPs in other languages were mixed mostly with English. We removed candidate pairs which referred to named entities or technical terms, where named entities were detected using several public knowledge bases such as Wikipedia and dictionaries for names <ref type="foot" target="#foot_4">6</ref> , cities, regions and countries <ref type="foot" target="#foot_5">7</ref> , and technical terms were removed via a manually created list of words specific to our source domain, Flickr, containing photography-related (e.g. macro, exposure) and camera-related words (e.g. DSLR).</p><p>Non-neutral Sentiment: To filter out neutral candidate adjective-noun pairs, each ANP was scored in sentiment using two publicly available sentiment ontologies: Sen-tiStrength <ref type="bibr" target="#b38">[38]</ref> and SentiWordnet <ref type="bibr" target="#b12">[12]</ref>. SentiStrength ontology supported all the languages we considered, but since SentiWordnet could only be used directly for English, we passed in automatic translations in English from all other languages to it, following previous research on multilingual sentiment analysis in machine translation <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref> <ref type="foot" target="#foot_6">8</ref> . We computed the ANP sentiment score S(anp) ∈ [-2, +2] as:</p><formula xml:id="formula_1">S(anp) = S(a) : sgn {S(a)} = sgn {S(n)} S(a) + S(n) : otherwise (1)</formula><p>where S(a) ∈ [-1, +1] and S(n) ∈ [-1, +1] are the sentiment scores of the individual adjective and noun words, respectively, each of which are given by the arithmetic mean of SentiStrength and SentiWordnet scores on the word, and sgn is the sign of the scores. The piecewise condition essentially says that if the signs of the sentiment scores of the adjective and noun differ, then we ignore the noun. This highlights our belief that adjectives are the dominant sentiment modifiers in an adjective-noun pair, so for example, even if a noun is positive, like wedding, an adjective such as horrible would completely change the sentiment of the combined pair. And so, for these sign mismatch cases, we chose the adjective's sentiment alone. In the other case, when the sign of the adjective and noun were the same, whether both positive (e.g. happy wedding) or both negative (e.g. scary spider ), we simply allowed the ANP sentiment score to be the unweighted sum of its parts. ANP candidates with zero sentiment score were filtered out.</p><p>Frequency: Good ANPs are those which are actually used together. Here, we loosely defined an ANP's "frequency" of usage as its number of occurrences as an image tag on Flickr. When computing counts for each pair, we accounted for language-specific syntax like the ordering of adjectives and nouns. Following anthropology research <ref type="bibr" target="#b10">[10]</ref>, we followed two dominant orderings (91.5% of the languages worldwide): adj-noun and noun-adj. We also "merged" simplified and traditional forms in Chinese by considering them to be from the same language pool but distinct characters sets. In addition, we considered the possible intermediate Chinese character 的 during our frequency counting. For all non-English languages, we retained all ANPs that occurred at least once as an image tag; but for English, since Flickr's most dominant number of users are English-speaking, we set a higher frequency threshold of 40.</p><p>Diversity: The shear frequency of an adjective-noun pair occurrence alone was not sufficient to ensure a pair's pervasive use in a language. We also checked if the ANP was used by a non-trivial number of distinct Flickr users for a given language. We identified the number of users contributing to uploads of images for each ANP and found a power law distribution in every language. To avoid this uploader bias, we removed all ANPs with less than three uploaders. Many removed candidate pairs came from companies and merchants for advertising and branding.</p><p>To further ensure diversity in our MVSO, we subsampled nouns in every language by limiting to the 100 most frequent ANPs per adjective so that we do not have, for example, the adjective surprising modifying every possible noun in our corpus. In addition, we performed stem unification by checking and including only the inflected form (e.g. singular/plural) of an ANP that was most popular in usage as a tag on Flickr. This unification did also filter some candidate ANPs as some "duplicates" were present but simply in different inflected forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Crowdsourcing Validation</head><p>A further inspection of the corpus after the automatic filtering process showed that some issues could not completely be solved in an automatic fashion. Common errors included many fundamental natural language processing challenges like confusions in named entity recognition (e.g. big apple), language mixing (e.g. adjective in English + noun in Turkish), grammar inconsistency (e.g. adj-adj, or verb-noun) and semantic incongruity (e.g. happy happiness). So to refine our multilingual visual sentiment ontology, we crowdsourced a validation task. For each language, we asked native speaking workers to evaluate the correctness of ANPs post automatic filtering. We collected judgements using Crowd-Flower 9 , a crowdsourcing platform that distributes small tasks to a large number of workers, where we limited workers by their language expertise. We note that while we elected to perform this additional stage of crowdsourcing, other researchers may find a fully automatic pipeline more desirable, so in our public release, we also release the pre-crowdsourced version of our MVSO. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Crowdsourcing Setup</head><p>We required that each ANP was evaluated by at least three independent workers. To ensure high quality results, we also required workers to be (1) native speakers of the language, for which CrowdFlower had its own language competency and expertise test for workers, and (2) have a good reputation according to the crowdsourcing platform, measured by workers' performance on other annotation jobs. For whatever reasons, for three languages (Persian, Polish and Dutch), the CrowdFlower platform does not evaluate workers based on their language expertise, so we filtered them by provenience, selecting the countries according to the official language spoken (e.g. Netherlands, Belgium, Aruba and Suriname for Dutch).</p><p>Task Interface: The verification task for workers consisted of simply evaluating the correctness of adjective-noun pairs. At the top of each page, we gave a short summary of the job and tasked workers: "Verify that a word pair in &lt;Language&gt; is a valid adjective-noun pair." Workers were provided with a detailed definition of what an adjectivenoun pair is and a summary of the criteria for evaluating ANPs, i.e. it (1) is grammatically correct (adjective + noun), <ref type="bibr" target="#b2">(2)</ref> shows language consistency, (3) shows generality, that is, commonly used and does not refer to a named entity, and ( <ref type="formula">4</ref>) is semantically logical. To guide workers, examples of correct and incorrect ANPs were provided for each criteria, where these ground truths were carefully judged and selected by four independent expert annotators. In the interface, aside from instructions, workers were shown five ANPs and simply chose between "yes" or "no" to validate ANPs.</p><p>Quality Control: Like some other crowdsourcing platforms, CrowdFlower provides a quality control mechanism called test questions to evaluate and track the performance of workers. These test questions come from pre-annotated ground truth, which in our case, correspond to ANPs with binary validation decisions for correctness. To access our task at all, workers were first required to correctly answer at least seven out of ten such test questions. In addition though, worker performance was tracked throughout the course of the task where these test questions were randomly inserted at certain points, disguised as normal units. For each language, we asked language experts to select ten correct and ten incorrect adjective-noun pairs from each language corpus to serve as the test questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Crowdsourcing Results</head><p>To measure the quality of our crowdsourcing, we looked at the annotator agreement along each validation task. For all languages, the agreement was very strong with an average annotator agreement of 87%, where workers agreed on either the correctness or incorrectness of ANPs. We found that workers tended to agree more that ANPs were correct than that they were incorrect. This was likely due to the wide range of possible criteria for rejecting an ANP where some criteria are easy to evaluate (e.g. language consistency), while others, such as general usage versus named entity, may cause disagreement among users due to the cultural background of the worker. For example, not all workers may agree that an ANP like big eyes or big apple refers to a named entity. However, for languages where the agreement on the incorrect ANPs was high, namely Arabic, German, and Polish, the average annotator agreement as a percentage of all ANP for that language were greater than 90%.</p><p>On average, our crowdsourcing validated that a vast number of the input candidate ANPs from our automatic ANP discovery and filtering process were indeed correct ANPs. English, Spanish and Russian were the top three for which the automatic pipeline performed the best, where every three in five ANPs were approved by the crowd judgements. However, for certain languages, including German, Dutch, Persian the number of ANPs rejected by the crowd was actually greater than accepted ANPs due to a higher occurrence of mixed language pairs, e.g. witzig humor. In Table <ref type="table" target="#tab_3">3</ref>, we summarize statistics from our crowdsourcing experiments according to the number of ANPs, percentage of correct/incorrect ANPs by worker majority vote, and average agreement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ANALYSIS &amp; STATISTICS</head><p>Having acquired a final set of adjective-noun pairs for each of the 12 languages, we downloaded images by querying the Flickr API with ANPs using a mix of tag and metadata search. To limit the size of our dataset, we downloaded no more than 1,000 images per ANP query and also enforced a limit of no more than 20 images from any given uploader on Flickr for increased visual diversity. The selected 1,000 images were selected from the pool of retrieved image tag search results, but in the event that this pool is less than 1,000, we also enlarged the pool to include searches on the image title and description, or metadata. Selections from the pool of results were always randomized and a small number of images which Flickr or uploaders removed or changed privacy settings midway were removed. In total, we downloaded 7,368,364 images across 15,630 ANPs for the 12 languages, where English (4,049,507), Spanish (1,417,781) and Italian (845,664) contributed the most images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Comparison with VSO [5]</head><p>To verify and test the efficacy of our MVSO, we provide a comparison of our extracted English visual sentiment ontology with that of VSO <ref type="bibr" target="#b5">[5]</ref> along dimensions of size (number of ANPs) and diversity of nouns and adjectives (Figure <ref type="figure" target="#fig_0">3</ref>). In Figure <ref type="figure" target="#fig_0">3a</ref>, the overlap of English MVSO with VSO is compared with VSO alone after applying all filtering criteria except subsampling which might exclude ANPs belonging to VSO. As mentioned previously, about 86% overlaps between them. As we vary a frequency threshold t (as described in  <ref type="figure">d)</ref>, (e) and (f ), in terms of the no. of ANPs, adjectives and nouns when varying the frequency threshold t from 0 to 10,000 (on log-scale), respectively. Sec. 3.2) over image tag counts, the overlap converges to 100%. This confirms that the popular ANPs covered by VSO are also covered by MVSO, an interesting finding given the difference in the crawling time periods and approaches. In Figure <ref type="figure" target="#fig_0">3b</ref>, we show that there are far greater number of ANPs in our English MVSO compared to VSO ANPs throughout all the possible values of frequency threshold, after applying all filtering criteria. Similarly, as shown in Figure <ref type="figure" target="#fig_0">3c</ref>, given there are more adjectives and nouns in our English MVSO, we also achieve greater diversity than VSO.</p><p>In Figure <ref type="figure" target="#fig_0">3d</ref>, we compare the number of ANPs for the remaining languages in MVSO with VSO after applying all filtering criteria. The curves show that VSO has more ANPs than all the languages for most of the languages over all values of t, except from Spanish, Italian and French in the low values of t. Our intuition is that this is due to the popularity of English on Flickr compared to other languages. In Figures <ref type="figure" target="#fig_0">3e</ref> and<ref type="figure" target="#fig_0">3d</ref>, we observe that these three languages have greater diversity of adjectives and nouns than VSO for t ≤ 10 3 , German and Dutch have greater diversity than VSO for smaller values of threshold t, while the rest of the languages have smaller diversity over most values of t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment Distributions</head><p>Returning to our research motivation from the Introduction, an interesting question to ask is which languages tend to be more positive or negative in their visual content. To answer this, we computed the median sentiment value across all ANPs of each language and ranked languages as in Fig. <ref type="figure">4</ref>.</p><p>Here, to take into account the popularity difference among ANPs, we replicated each ANP k times, with k equal to the number of images tagged with the ANP, up to an upper limit L = α × Avgi, where Avgi is the average image count per ANP in the ith language. Varying α value will result in different medians and distributions, but the trend in differentiating positive languages from negative ones was quite stable. We show the case when α = 3 in Fig. <ref type="figure">4</ref>, indicating that there is an overall tendency toward positive sentiment across all languages, where Spanish demonstrates the highest positive sentiment, followed by Italian. This surprising observation is in fact compatible with previous research showing that there is a universal positivity bias over languages with Spanish being the most relatively positive language <ref type="bibr">[9]</ref>. The languages with the lowest sentiment were Persian and Arabic, followed by English.</p><p>The sentiment distributions (Fig. <ref type="figure">4</ref>: right) also showed interesting phenomena: Spanish being the most positive language also has the highest variation in sentiment, while German has the most concentrated sentiment distribution. Even for languages that have the lowest median sentiment values, the range of sentiment was concentrated in a small range near zero (between 0 and -0.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Median sentiment computed over all</head><p>ANPs per language is shown on left, and the sentiment distribution using box plots on the right (zoomed at 90% of the distributions). On right, languages are sorted by median sentiment in ascending order (from the left).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Emotion Distributions</head><p>Another interesting question arises when considering cooccurrence of ANPs with the emotions in different languages. While our adjective-noun pair concepts were selected to be sentiment-biased, emotions still represent the root of our framework since we built MVSO out from seed emotion terms. So aside from sentiment, which focuses on only positivity/negativity, what are probable mappings of ANPs to emotions for each language? What emotions are most frequently occurring across languages? Given the set of keywords E (l) = {e (l) ij | i = 1 . . . 24, j = 1 . . . ni} describing each emotion i per language l, where ni is the number of keywords per emotion i, the set of ANPs belonging to language l, noted as x ∈ X (l) , and the number of images tagged with both ANP x and emotion keyword eij, C (x) = {c (x) ij | i = 1 . . . 24, j = 1 . . . ni}, we define the probabilities of emotion for each ANP x in language l as:</p><formula xml:id="formula_2">emo i (x) = 1 n i n i j=1 c (x) ij 24 i=1 1 n i n i j=1 c (x) ij ∈ [0, 1]<label>(2)</label></formula><p>Note the model in <ref type="bibr" target="#b2">(2)</ref> does not take into account correlation among emotions, where for example, by an image tagged with "ecstasy," users may also imply "joy" even though the latter is not explicitly tagged. These correlations can be easily accounted for by smoothing co-occurrence counts cij over correlated emotions, e.g. the co-occurrence counts of an ANP tagged with "ecstasy" can be included partially in the co-occurrence count of "joy." Regardless, still based on (2), we compute a normalized emotion score per language l and emotion i as:</p><formula xml:id="formula_3">score i (l) = |X (l) | x=1 emo i (x) • count(x) 24 i=1 |X (l) | x=1 emo i (x) • count(x) ∈ [0, 1]<label>(3)</label></formula><p>Figure <ref type="figure" target="#fig_1">5</ref> shows these scores per language and Plutchik emotion <ref type="bibr" target="#b34">[34]</ref> on a heatmap diagram. Scores in each row sum to 1 (over 24 emotions). The emotions are ordered by the sum of their scores across languages. The top-5 emotions across all languages are joy, serenity, interest, grief and fear. And the highest ranked emotion is joy in Russian, Chinese and Arabic. Two other emotions in the top-5 were also positive: serenity, being high ranked emotion for Dutch, Italian, Chinese and Persian, and interest for English, Turkish and Dutch. The remaining two emotions in the top-5 were negative: grief for Persian and Turkish, and fear, which was high ranked in German and Polish. We also observed that pensiveness was top ranked for Persian and Polish, vigilance for French, rage for German, while apprehension and distraction for Spanish. We note that these results are more concrete for languages with many ANPs (&gt;1000) and less conclusive for those with few ANPs like Arabic and Persian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CROSS-LINGUAL MATCHING</head><p>To get a gauge on the topics commonly mentioned across different cultures and languages, we analyzed alignments of translations for each ANP to English as a basis. Two approaches were taken to study this: exact and approximate alignment. We ensured that translations of ANPs also passed all our validation filters described in Sec. 3.2 for this analysis.</p><p>Exact Alignment: We grouped ANPs from each language that have the exact same translation. For example, old books was the translation for one or more ANPs from seven languages, including 老書 (Chinese), livres anciens (French), vecchi libri (Italian), Старые книги (Russian), libros antiguos (Spanish), eski kitaplar (Turkish). The translation covered by the greatest number of languages was beautiful girl with ANPs from ten languages. Figure <ref type="figure" target="#fig_2">6</ref> (left) shows a correlation matrix of the number of times ANPs from pairs of languages appeared together in a set with the exact same translation, e.g. out of all the translations that German ANPs were translated to (782), more of them were translated to the same phrase with the ANPs used by Dutch speakers <ref type="bibr" target="#b39">(39)</ref> than with the ANPs used by Chinese speakers <ref type="bibr" target="#b23">(23)</ref>. This was striking given that there were less (340) translation phrases from Dutch than from Chinese (473).</p><p>Approximate Alignment: Translations can be inaccurate, especially when capturing underlying semantics where context is not provided. And so, we relaxed the strict condition for exact matches by approximately matching using a hierarchical two-stage clustering approach instead. First, we extracted nouns using TreeTagger <ref type="bibr" target="#b37">[37]</ref> from the list of translated phrases and discovered 3,099 total nouns. We then extracted word2vec <ref type="bibr" target="#b32">[32]</ref> features, a word representation trained on a Google News 10 corpus, for these translated nouns (188 nouns were out-of-vocabulary), and performed kmeans clustering (k=200) to get groups of nouns with similar meaning. The number of clusters was picked based on the coherence of clusters; and we picked the number where the inertia value of the clustering started saturating while gradually increasing k. In the second stage of our hierarchical clustering, we split phrases from the translations into different groups based on the clusters their nouns belonged to. We extracted word2vec <ref type="bibr" target="#b32">[32]</ref> features from the full translated phrase in each cluster and ran another round of k-means clustering (adjusting k based on the number of phrases in each cluster, where phrases in each noun-cluster ranged from 3 to 253). This two-stage clustering enables us to create a hierarchical organization of our ANPs across languages and form a multilingual ontology over visual sentiment concepts (MVSO), unlike the flat structure in VSO <ref type="bibr" target="#b5">[5]</ref>. We discovered 3,329 sub-clusters of ANP concepts, e.g. resulting in clusters containing little pony and little horse as in Figure <ref type="figure" target="#fig_3">7</ref>. This approach also yielded a larger intersection between languages, where German and Dutch share 118 clusters, and German and Chinese intersect over 101 ANP clusters.</p><p>The correlation matrix from this approximate matching is shown in Figure <ref type="figure" target="#fig_2">6</ref>, along with one subtree from our ontology by hierarchical clustering in Figure <ref type="figure" target="#fig_3">7</ref>. For Figure <ref type="figure" target="#fig_3">7</ref>, we projected data to R 2 using t-SNE dimensionality reduction <ref type="bibr" target="#b40">[40]</ref>. On the left, six clusters composed of different sets of nouns are shown with clusters of sunlight-rays-glow and dog-catpony. On the right, we show the sub-clustering of ANPs for the dog-cat-pony cluster in A, giving us noun groupings modified by sentiment-biasing adjectives to get ANPs like funny dog-funny cats and adopted dog-abondoned puppy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">VISUAL SENTIMENT PREDICTION</head><p>To test the effectiveness of a vision-based approach for visual affect understanding when crossing languages, we de-10 news.google.com For visualization, word2vec <ref type="bibr" target="#b32">[32]</ref> vectors were projected to R 2 using t-SNE <ref type="bibr" target="#b40">[40]</ref>.</p><p>signed and built language-specific sentiment predictors using the data collected with MVSO. Inspired by work in <ref type="bibr" target="#b17">[17]</ref>, we studied the extent to which the visual sentiments of a given language can be predicted by sentiment models of other languages. We chose to focus on a sentiment prediction task, i.e. predicting whether an image is of positive or negative sentiment, because there is a large body of work expressly focused on sentiment (e.g. <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b43">43]</ref>) for its simplicity, compared to emotion prediction. More importantly, we wanted to reduce the number of variables to be analyzed since our primary goal was to uncover cross-lingual differences.</p><p>We first constructed a bank of visual concept detectors like in <ref type="bibr" target="#b4">[4]</ref> for our final MVSO adjective-noun pairs. For simplicity, we focused on the six languages with the most ANPs and associated images in our dataset: in decreasing order, English, Spanish, Italian, French, German and Chinese. Combined these six languages account for 94.7% of the ANPs in MVSO and 98.4% of the images in our dataset. However, to ensure that there were enough training images for each ANP, only the ANPs with no less than 125 images were selected for model training and prediction. This reduced the combined ANP coverage to 63.5% but still ensured 92.0% coverage for images. For each ANP, the images were split randomly 80/20% train/test, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visual Sentiment Concept Detectors</head><p>To construct our bank of visual concept detectors of ANPs, we used convolutional neural networks (CNNs), in particular, adopting an AlexNet-styled architecture <ref type="bibr" target="#b24">[24]</ref> for its good performance on large-scale vision recognition and detection tasks. To train our detector bank, we fined-tuned six models, one for each language, where network weights were initialized with DeepSentiBank <ref type="bibr" target="#b6">[6]</ref>, an AlexNet model trained on the VSO <ref type="bibr" target="#b5">[5]</ref> dataset. This fine-tuning approach ensures that each network begins with weights that are already somewhat "affectively" biased. The base learning rates were set to 0.001 and the number of output neurons in the last fully connected layer were set to the number of training ANPs of each language. Step sizes for reducing the learning rate in the second stage were set proportional to the number of training images per language. For a single language, finetuning took between 12 and 40 hours for convergence on a single NVIDIA GTX 980 GPU implemented with Caffe <ref type="bibr" target="#b19">[19]</ref>. From Table <ref type="table" target="#tab_5">4</ref>, as expected we achieve higher top-1 and top-5 accuracies than DeepSentiBank <ref type="bibr" target="#b6">[6]</ref>, even when the numbers of output neurons in English and Spanish are higher than those in <ref type="bibr" target="#b6">[6]</ref>. Top-k accuracy refers to the percentage of classifications that the true class is in the top k predicted ranks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Sentiment Prediction on Flickr</head><p>We used the CNN-based visual concept models trained for each language to extract image features and use the sentiment scores of ANPs as supervised labels to learn sentiment prediction models. We compared different layers of the CNN models as image features. To simplify the process, we binarized the ANP sentiment scores computed with Eq. ( <ref type="formula">1</ref>), i.e. into positive and negative classes, and learned a binary classifier using linear SVMs, one for each language. The training images are those associated with ANPs of strong sentiment scores (absolute values higher than 0.05). Splits of training and test sets were stratified across all languages so that the amount of training and testing for positive and negative sentiment classes was the same for fair cross-lingual experiment comparison.</p><p>We found that the softmax output features from the penultimate layer outputs of each language's CNN model performed the best for all languages, and we show resulting sentiment prediction results in Figure <ref type="figure" target="#fig_5">9</ref>. Each language expectedly did better in predicting test samples from its own language, but in addition, Chinese generally was the most difficult to predict by models trained from other languages; and using a sentiment model trained over Chinese images to predict the sentiment in other languages was also the worst   in average. We speculate that this is due to the difference in the visual sentiment portrayal from Eastern and Western cultures. Interestingly, the classification of French and Italian sentiments was the most consistent using models from all languages. We also observed good performance in crosslingual prediction for Latin languages, i.e. Spanish, Italian and French, where Italian was the best cross-lingual classifier for Spanish and French sentiment, and Spanish was best for Italian sentiment, followed by French. Despite not performing as well as others in average, the English-specific sentiment model had the least variance in its accuracy across all languages, likely from the pervasiveness of English worldwide and across cultures. In Figure <ref type="figure" target="#fig_6">10</ref>, we show three classification example results from our cross-lingual sentiment prediction. On the left, an image from the Italian test set representing the costumi tradizionali concept was labeled as positive via sentiment scoring, but was predicted by the German model to be negative; this may be due to differences in cultural perceptions of traditional clothing. In the center, the Chinese model wrongly predicted an image from the English test set of foggy morning as positive, possibly for its resemblance to a Chinese painting. And on the right, an image of a beau village from the French test set was successfully classified as positive with the Spanish sentiment predictor. These examples and preliminary experiments highlight some similarities and differences in how visual sentiment is expressed and perceived by various cultures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION &amp; FUTURE WORK</head><p>We proposed a new multilingual discovery method for visual sentiment concepts and showed its efficacy on a social multimedia platform for 12 languages. We based our approach on the psychology theory that emotions are culturespecific and carry inherent linguistic context, and so we showed how to use language-specific part-of-speech labeling along with progressive filtering to achieve coverage and diversity of visual affect concepts in multiple languages. In addition, we presented a two-stage hierarchical clustering approach to unify our ontology across languages. And we make our Multilingual Visual Sentiment Ontology (MVSO), pre-crowdsourcing as well as post, and image dataset, available to the public. A cross-lingual analysis of our large-scale MVSO and image dataset using semantic matching and visual sentiment prediction hint that emotions are not necessarily culturally universal. Our preliminary results show that there are indeed commonalities, but also distinct separations, in how visual affect is expressed and perceived, where other works assumed only commonalities. We believe these point to the colorful diversity of our world, rather than our inability to understand one another.</p><p>In the future, we plan to explore differences along other factors which can be collected from self-reported user metadata like age group, gender, profession, etc. We will also adopt our approach to other language-specific social multimedia platforms to counter the insufficient data for some languages like Arabic, Persian and Chinese. In addition, while we discussed culture and languages in this work, we have not yet performed an in-depth study on geo-location data in MVSO, often provided along with uploaded images on Flickr. While such information could be useful to distinguish between sub-cultures speaking the same languages (e.g. Spanish vs. South-Americans), we omitted such a study here because of the noise that geo-location data can add. For example, an American traveling in China uploading pictures is still more likely to use their native tongue to tag and sentimentally describe their content. The trade-off is that while their semantics are culturally American, the uploaded visual content is now from another culture, so there is still much to be explored from geo-location and user metadata.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of our English MVSO and VSO [5] in Figures (a), (b) and (c), in terms of ANP overlap, no. of ANPs, adjectives and nouns; and with all other languages in Figures (d), (e) and (f ), in terms of the no. of ANPs, adjectives and nouns when varying the frequency threshold t from 0 to 10,000 (on log-scale), respectively.</figDesc><graphic coords="6,324.43,53.80,220.80,272.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Probabilities of emotions per language with respect to their visual sentiment content. Emotions are ordered by the sum of their probabilities across languages (left to right) and clipped for better visualization. Each row sums to 1.</figDesc><graphic coords="7,317.21,53.80,238.31,103.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Percentage of times ANPs from one language (columns) were translated to [Left] the same phrase, or to [Right] the phrase in the same cluster as in another language (rows).</figDesc><graphic coords="8,53.80,53.80,243.91,89.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of noun clusters (left) and ANP sub-clusters (right) from our two-stage clustering for cross-lingual matching.For visualization, word2vec<ref type="bibr" target="#b32">[32]</ref> vectors were projected to R 2 using t-SNE<ref type="bibr" target="#b40">[40]</ref>.</figDesc><graphic coords="8,317.56,53.80,237.59,98.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example top-5 classification results from our multilingual visual sentiment detector bank. Translations to English provided for convenience.</figDesc><graphic coords="9,53.81,53.81,239.07,194.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Image-based, cross-lingual domain transfer sentiment prediction results with languagespecific models applied on cross-lingual examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Classification examples from crosslingual sentiment prediction. The model from a source language is used to predict the sentiment of a target language image where the true label comes from the sentiment of the associated ANP.</figDesc><graphic coords="9,317.56,398.90,237.60,80.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,77.03,53.80,452.60,144.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Most representative keywords according to native/proficient speakers for eight basic emotions and for 7 of our 12 languages, chosen and shown top-to-bottom in decreasing no. of discovered visual affect concepts, or adjective-noun pairs.</figDesc><table><row><cell>English</cell><cell>joy</cell><cell>trust</cell><cell>fear</cell><cell>surprise</cell><cell>sadness</cell><cell>disgust</cell><cell>anger</cell><cell>anticipation</cell></row><row><cell>Spanish</cell><cell>alegría</cell><cell>confianza</cell><cell>miedo</cell><cell>sorpresa</cell><cell>tristeza</cell><cell>asco</cell><cell>ira</cell><cell>previsión</cell></row><row><cell>Italian</cell><cell>gioia</cell><cell>fiducia</cell><cell>paura</cell><cell>sorpresa</cell><cell>tristezza</cell><cell>disgusto</cell><cell>rabbia</cell><cell>anticipazione</cell></row><row><cell>French</cell><cell>bonheur</cell><cell>confiance</cell><cell>peur</cell><cell>surprise</cell><cell>tristesse</cell><cell>dégoût</cell><cell>colère</cell><cell>prévision</cell></row><row><cell>German</cell><cell>Freude</cell><cell>Vertrauen</cell><cell>Angst</cell><cell>Überraschung</cell><cell>Traurigkeit</cell><cell>Empörung</cell><cell>Ärger</cell><cell>Vorfreude</cell></row><row><cell>Chinese</cell><cell>歡樂</cell><cell>信任</cell><cell>害怕</cell><cell>震驚</cell><cell>悲</cell><cell>討厭</cell><cell>憤怒</cell><cell>預期</cell></row><row><cell>Dutch</cell><cell>vreugde</cell><cell>vertrouwen</cell><cell>angst</cell><cell>verrassing</cell><cell>verdriet</cell><cell>walging</cell><cell>woede</cell><cell>anticipatie</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ontology refinement statistics over 12 languages. Beginning with many images from seed emotion keywords denoted by #images, we extracted tags from these images #tags, and performed adjective-noun pair (ANP) discovery for candidate combinations #cand. Through a series of filters -frequency, language, semantics filter, sentiment filter and diversity -and after crowdsourcing, we got our final visual sentiment concepts #anps.</figDesc><table /><note><p>&gt; boredom; rage &gt; anger &gt; annoyance; and, vigilance &gt; anticipation &gt; interest.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Crowdsourcing results via no. of input candidate ANPs #cand, #users, countries #coun, and perc. of ANPs accepted %correct and annotator agreement %agree.</figDesc><table><row><cell>9 www.crowdflower.com</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Chinese target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>English target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>French target</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>German target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Italian target</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Spanish target</cell></row><row><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>C hi ne se</cell><cell>E ng lis h</cell><cell>F re nc h</cell><cell>G er m an</cell><cell>Ita lia n</cell><cell>S pa ni sh</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Language-specific Sentiment Model</cell><cell></cell></row></table><note><p><p><p>Adjective-noun pair (ANP) classification performance on Flickr images for six major languages in MVSO and compared to DeepSentiBank (DSB)</p><ref type="bibr" target="#b6">[6]</ref></p>. No. of visual sentiment concepts #ANPs, #train and #test images along with learning rate step size (lrs, in thousands) are shown with training times (in hours), top-1 and top-5 accuracies.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>mvso.cs.columbia.edu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that we use language and culture interchangeably often. We define language as the "lens" through which we can observe culture. So while the two can be distinguished, for simplicity, we use them interchangeably.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>www.flickr.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>www.winedt.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>www.wikipedia.org and www.ssa.gov, respectively</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>www.geobytes.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>For four non-English languages with the highest ANP counts, we have verified only a small percentage of nonneutral ANPs (less than 2%) reverse sentiment polarity after translation, confirming similar observations in the previous work.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>Research was sponsored by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF-12-C-0028. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the of official policies, either expressed or implied, of the U.S. Defense Advanced Research Projects Agency or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here-on. B. Jou was supported by the Department of Defense (DoD) through the National Defense Science &amp; Engineering Graduate Fellowship (NDSEG) Program. N. Pappas was supported by the InEvent (FP7-ICT n. 287872) and MODERN Sinergia (CRSII2 147653) projects.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual sentiment analysis using machine translation?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balahur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WASSA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multilingual subjectivity analysis using machine translation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hassan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">International sentiment analysis for news and blogs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bautin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vijayarenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWSM</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SentiBank: Large-scale ontology and classifiers for detecting sentiment and emotions in visual content</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale visual sentiment ontology and detectors using adjective noun pairs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8586</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Predicting viewer affective comments based on image content in social media</title>
		<author>
			<persName><forename type="first">Y.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><forename type="middle">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Geneva affective picture database: A new 730-picture database focusing on valence and normative significance</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Dan-Glauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behav. Res. Meth</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human language reveals a universal positivity bias</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dodds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PNAS</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Dryer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haspelmath</surname></persName>
		</author>
		<ptr target="http://wals.info/chapter/87" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>WALS Online. Max Planck Institute for Evolutionary Anthropology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial expression and emotion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SENTIWORDNET: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing Turkish using the lexical functional grammar formalism</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Güngördü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oflazer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The interestingness of images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HunPos: An open source trigram tagger</title>
		<author>
			<persName><forename type="first">P</forename><surname>Halácsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kornai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oravecz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Irrational emotions or emotional wisdom? The evolutionary psychology of affect and social behavior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Haselton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ketelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Affect in Soc. Think. and Behav</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">21</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-cultural mood regression for music digital libraries</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JCDL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Can we understand van Gogh&apos;s mood?: Learning to infer affects from images in social networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The wisdom of social multimedia: Using Flickr for prediction and forecast</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Predicting viewer perceived emotions in animated GIFs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The design of high-level features for photo quality assessment</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What makes an image popular</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">International Affective Picture System (IAPS): Technical manual and affective ratings</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cuthbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>NIMH CSEA</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Challenges in cross-cultural/multilingual music information seeking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Culture and the self: Implications for cognition, emotion, and motivation</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Markus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kitayama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The social construction of emotions: New directions from culture theory</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Mccarthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Perspectives on Emotion</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Culture and emotion</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Frijda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Cross-cultural Psychology</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Berry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Dasen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Saraswathi</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multilingual subjective language via cross-lingual projections</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Banea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wiebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Picard</surname></persName>
		</author>
		<title level="m">Affective Computing</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Emotion: A Psychoevolutionary Synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Plutchik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980">1980</date>
			<publisher>Harper &amp; Row</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">6 Seconds of sound and vision: Creativity in micro-videos</title>
		<author>
			<persName><forename type="first">M</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>O'hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schifanella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trevisiol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Culture and the categorization of emotions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic part-of-speech tagging using decision trees</title>
		<author>
			<persName><forename type="first">H</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl Conf. on New Methods in Language Proc</title>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Sentiment strength detection in short informal text</title>
		<author>
			<persName><forename type="first">M</forename><surname>Thelwall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Buckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Paltoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jour. Ameri. Soci. for Info. Sci. &amp; Tech</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feature-rich part-of-speech tagging with a cyclic dependency network</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Visualizing high-dimensional data using t-SNE</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Personalized visual aesthetics</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Vessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">G</forename><surname>Starr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE-IS&amp;T Electronic Imaging</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Emotional valence categorization using holistic image features</title>
		<author>
			<persName><forename type="first">V</forename><surname>Yanulevskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Herbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Geusebroek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Robust image sentiment analysis using progressively trained and domain transferred deep networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feeling and thinking: Preferences need no inferences</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Zajonc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
