<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tri-Training: Exploiting Unlabeled Data Using Three Classifiers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
							<email>zhouzh@lamda.nju.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210093</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tri-Training: Exploiting Unlabeled Data Using Three Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9305BC6A5907A31029EC3A030449A3C3</idno>
					<note type="submission">received 6 Aug. 2004; revised 30 Dec. 2004; accepted 21 Apr. 2005; published online 19 Sept. 2005.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data mining</term>
					<term>machine learning</term>
					<term>learning from unlabeled data</term>
					<term>semi-supervised learning</term>
					<term>co-training</term>
					<term>tri-training</term>
					<term>Web page classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain. Therefore, semi-supervised learning algorithms such as co-training have attracted much attention. In this paper, a new co-training style semi-supervised learning algorithm, named tri-training, is proposed. This algorithm generates three classifiers from the original labeled example set. These classifiers are then refined using unlabeled examples in the tri-training process. In detail, in each round of tri-training, an unlabeled example is labeled for a classifier if the other two classifiers agree on the labeling, under certain conditions. Since tri-training neither requires the instance space to be described with sufficient and redundant views nor does it put any constraints on the supervised learning algorithm, its applicability is broader than that of previous co-training style algorithms. Experiments on UCI data sets and application to the Web page classification task indicate that tri-training can effectively exploit unlabeled data to enhance the learning performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I N many practical data mining applications, such as Web page classification, unlabeled training examples are readily available, but labeled ones are fairly expensive to obtain because they require human effort. Therefore, semisupervised learning that exploits unlabeled examples, in addition to labeled ones, has become a hot topic.</p><p>Many current semi-supervised learning algorithms use a generative model for the classifier and employ Expectation-Maximization (EM) <ref type="bibr" target="#b10">[11]</ref> to model the label estimation or parameter estimation process. For example, mixture of Gaussians <ref type="bibr" target="#b25">[26]</ref>, mixture of experts <ref type="bibr" target="#b16">[17]</ref>, and naive Bayes <ref type="bibr" target="#b19">[20]</ref> have been respectively used as the generative model, while EM is used to combine labeled and unlabeled data for classification. There are also many other algorithms such as using transductive inference for support vector machines to optimize performance on a specific test set <ref type="bibr" target="#b15">[16]</ref>, constructing a graph on the examples such that the minimum cut on the graph yields an optimal labeling of the unlabeled examples according to certain optimization functions <ref type="bibr" target="#b3">[4]</ref>, etc.</p><p>A prominent achievement in this area is the co-training paradigm proposed by Blum and Mitchell <ref type="bibr" target="#b4">[5]</ref>, which trains two classifiers separately on two different views, i.e., two independent sets of attributes, and uses the predictions of each classifier on unlabeled examples to augment the training set of the other. Such an idea of utilizing the natural redundancy in the attributes has been employed in some other works. For example, Yarowsky <ref type="bibr" target="#b27">[28]</ref> performed word sense disambiguation by constructing a sense classifier using the local context of the word and a classifier based on the senses of other occurrences of that word in the same document; Riloff and Jones <ref type="bibr" target="#b22">[23]</ref> classified a noun phrase for geographic locations by considering both the noun phrase itself and the linguistic context in which the noun phrase appears; Collins and Singer <ref type="bibr" target="#b7">[8]</ref> performed named entity classification using both the spelling of the entity itself and the context in which the entity occurs. It is noteworthy that the co-training paradigm has already been used in many domains such as statistical parsing and noun phrase identification <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>.</p><p>The standard co-training algorithm <ref type="bibr" target="#b4">[5]</ref> requires two sufficient and redundant views, that is, the attributes be naturally partitioned into two sets, each of which is sufficient for learning and conditionally independent to the other given the class label. Dasgupta et al. <ref type="bibr" target="#b9">[10]</ref> have shown that when the requirement is met, the cotrained classifiers could make fewer generalization errors by maximizing their agreement over the unlabeled data. Unfortunately, such a requirement can hardly be met in most scenarios. Goldman and Zhou <ref type="bibr" target="#b13">[14]</ref> proposed an algorithm which does not exploit attribute partition. However, it requires using two different supervised learning algorithms that partition the instance space into a set of equivalence classes, and employing the time-consuming cross-validation technique to determine how to label the unlabeled examples and how to produce the final hypothesis.</p><p>In this paper, a new co-training style algorithm, named tri-training, is proposed. Tri-training does not require sufficient and redundant views, nor does it require the use of different supervised learning algorithms whose hypothesis partitions the instance space into a set of equivalence classes. Therefore, it can be easily applied to common data mining scenarios. In contrast to previous algorithms that utilize two classifiers, tri-training uses three classifiers. This setting tackles the problem of determining how to label the unlabeled examples and how to produce the final hypothesis, which contributes much to the efficiency of the algorithm. Moreover, better generalization ability can be achieved through combining these three classifiers. Experiments on UCI data sets <ref type="bibr" target="#b2">[3]</ref> and application to the Web page classification task show that tri-training can effectively exploit unlabeled data, and the generalization ability of its final hypothesis is quite good, sometimes even outperforming that of the ensemble of three classifiers being provided with labels of all the unlabeled examples.</p><p>The rest of this paper is organized as follows: Section 2 presents the tri-training algorithm. Section 3 reports on the experiments on UCI data sets. Section 4 describes the application to the task of Web page classification. Finally, Section 5 concludes and raises several issues for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">TRI-TRAINING</head><p>Let L denote the labeled example set with size jLj and U denote the unlabeled example set with size jUj. In previous co-training style algorithms, two classifiers are initially trained from L, each of which is then retrained with the help of unlabeled examples that are labeled by the latest version of the other classifier. In order to determine which example in U should be labeled and which classifier should be biased in prediction, the confidence of the labeling of each classifier must be explicitly measured. Sometimes, such a measuring process is quite time-consuming <ref type="bibr" target="#b13">[14]</ref>.</p><p>Assume that besides these two classifiers, i.e., h 1 and h 2 , a classifier h 3 is initially trained from L. Then, for any classifier, an unlabeled example can be labeled for it as long as the other two classifiers agree on the labeling of this example, while the confidence of the labeling of the classifiers are not needed to be explicitly measured. For instance, if h 2 and h 3 agree on the labeling of an example x in U, then x can be labeled for h 1 . It is obvious that in such a scheme if the prediction of h 2 and h 3 on x is correct, then h 1 will receive a valid new example for further training; otherwise, h 1 will get an example with noisy label. However, even in the worse case, the increase in the classification noise rate can be compensated if the amount of newly labeled examples is sufficient, under certain conditions, as shown below.</p><p>Inspired by Goldman and Zhou <ref type="bibr" target="#b13">[14]</ref>, the finding of Angluin and Laird <ref type="bibr" target="#b0">[1]</ref> is used in the following analysis. That is, if a sequence of m samples is drawn, where the sample size m satisfies (1):</p><formula xml:id="formula_0">m ! 2 2 1 À 2 ð Þ 2 ln 2N ;<label>ð1Þ</label></formula><p>where is the hypothesis worst-case classification error rate, (&lt; 0:5) is an upper bound on the classification noise rate, N is the number of hypothesis, and is the confidence, then a hypothesis H i that minimizes disagreement with will have the PAC property:</p><formula xml:id="formula_1">Pr ½dðH i ; H Ã Þ ! ;<label>ð2Þ</label></formula><p>where dð; Þ is the sum over the probability of elements from the symmetric difference between the two hypothesis sets H i and H Ã (the ground-truth). Let c ¼ 2 ln ð 2N Þ, where makes (1) hold equality, then (1) becomes (3):</p><formula xml:id="formula_2">m ¼ c 2 1 À 2 ð Þ 2 :<label>ð3Þ</label></formula><p>To simplify the computation, it is helpful to compute the quotient of the constant c divided by the square of the error:</p><formula xml:id="formula_3">u ¼ c 2 ¼ m 1 À 2 ð Þ 2 :<label>ð4Þ</label></formula><p>In each round of tri-training, the classifiers h 2 and h 3 choose some examples in U to label for h 1 . Since the classifiers are refined in the tri-training process, the amount as well as the concrete unlabeled examples chosen to label may be different in different rounds. Let L t and L tÀ1 denote the set of examples that are labeled for h 1 in the tth round and the ðt À 1Þth round, respectively. Then, the training set for h 1 in the tth round and ðt À 1Þth round are respectively L [ L t and L [ L tÀ1 , whose sample size m t and m tÀ1 are jL [ L t j and jL [ L tÀ1 j, respectively. Note that the unlabeled examples labeled in the ðt À 1Þth round, i.e., L tÀ1 , will not be put into the original labeled example set, i.e., L. Instead, in the tth round all the examples in L tÀ1 will be regarded as unlabeled and put into U again.</p><p>Let </p><formula xml:id="formula_4">t ¼ L L j j þ e e t 1 L t j j L [ L t j j :<label>ð5Þ</label></formula><p>Then, according to (4), u t can be computed as:</p><formula xml:id="formula_5">u t ¼ m t 1 À 2 t À Á 2 ¼ L [ L t 1 À 2 L L j j þ e e t 1 L t j j L [ L t j j 2 :<label>ð6Þ</label></formula><p>Similarly, u tÀ1 can be computed as:</p><formula xml:id="formula_6">u tÀ1 ¼ m tÀ1 1 À 2 tÀ1 À Á 2 ¼ L [ L tÀ1 1 À 2 L L j j þ e e tÀ1 1 L tÀ1 L [ L tÀ1 j j 2 :<label>ð7Þ</label></formula><p>As shown in (4), since u is in proportion to 1= 2 , it can be derived that if u t &gt; u tÀ1 , then t &lt; tÀ1 , which implies that h 1 can be improved through utilizing L t in its training. This condition can be expressed as ( <ref type="formula" target="#formula_7">8</ref>) by comparing ( <ref type="formula" target="#formula_5">6</ref>) and ( <ref type="formula" target="#formula_6">7</ref>):</p><formula xml:id="formula_7">L [ L t 1 À 2 L L j j þ e e t 1 L t j j L [ L t j j 2 &gt; L [ L tÀ1 1 À 2 L L j j þ e e tÀ1 1 L tÀ1 L [ L tÀ1 j j 2 :<label>ð8Þ</label></formula><p>Considering that L can be very small and assuming 0 e e t 1 ; e e tÀ1 1 &lt; 0:5, then the first term on the left hand of ( <ref type="formula" target="#formula_7">8</ref>) is bigger than its correspondence on the right hand if jL tÀ1 j &lt; jL t j, while the second term on the left hand is bigger than its correspondence on the right hand if e e t 1 jL t j &lt; e e tÀ1 1 jL tÀ1 j. These restrictions can be summarized into the condition shown in <ref type="bibr" target="#b8">(9)</ref>, which is used in tri-training to determine when an unlabeled example could be labeled for a classifier:</p><formula xml:id="formula_8">0 &lt; e e t 1 e e tÀ1 1 &lt; jL tÀ1 j jL t j &lt; 1:<label>ð9Þ</label></formula><p>Note that, when e e t 1 &lt; e e tÀ1 1 and jL tÀ1 j &lt; jL t j, e e t 1 jL t j may not be less than e e tÀ1 1 jL tÀ1 j due to the fact that jL t j may be far bigger than jL tÀ1 j. When this happens, in some cases L t could be randomly subsampled such that e e t 1 jL t j &lt; e e tÀ1 1 jL tÀ1 j. Given e e t 1 , e e tÀ1 1 , and jL tÀ1 j, let integer s denote the size of L t after subsampling, then if <ref type="bibr" target="#b9">(10)</ref> holds, e e t 1 jL t j &lt; e e tÀ1 1 jL tÀ1 j is obviously satisfied.</p><formula xml:id="formula_9">s ¼ e e tÀ1 1 jL tÀ1 j e e t 1 À 1 $ % ;<label>ð10Þ</label></formula><p>where L tÀ1 should satisfy <ref type="bibr" target="#b10">(11)</ref> such that the size of L t after subsampling, i.e., s, is still bigger than jL tÀ1 j.</p><formula xml:id="formula_10">jL tÀ1 j &gt; e e t 1 e e tÀ1 1 À e e t 1 :<label>ð11Þ</label></formula><p>The pseudocode of tri-training is presented in Table <ref type="table">1</ref>. The function MeasureErrorðh j &amp;h k Þ attempts to estimate the classification error rate of the hypothesis derived from the combination of h j and h k . Since it is difficult to estimate the classification error on the unlabeled examples, here only the original labeled examples are used, heuristically based on the assumption that the unlabeled examples hold the same distribution as that held by the labeled ones. In detail, the classification error of the hypothesis is approximated through dividing the number of labeled examples on which both h j and h k make incorrect classification by the number of labeled examples on which the classification made by h j is the same as that made by h k . The function SubsampleðL t ; sÞ randomly removes jL t j À s number of examples from L t , where s is computed according to <ref type="bibr" target="#b9">(10)</ref>.</p><p>It is noteworthy that the initial classifiers in tri-training should be diverse because if all the classifiers are identical, then for any of these classifiers, the unlabeled examples labeled by the other two classifiers will be the same as these labeled by the classifier for itself. Thus, tri-training degenerates to self-training <ref type="bibr" target="#b18">[19]</ref> with a single classifier. In the standard co-training algorithm, the use of sufficient and redundant views enables the classifiers be different. In fact, previous research has shown that even when there is no natural attribute partitions, if there are sufficient redundancy among the attributes, then a fairly reasonable attribute partition will enable co-training exhibit advantages <ref type="bibr" target="#b18">[19]</ref>. While in the extended co-training algorithm which does not require sufficient and redundant views, the diversity among the classifiers is achieved through using different supervised learning algorithms <ref type="bibr" target="#b13">[14]</ref>. Since the tri-training algorithm does not assume sufficient and redundant views and different supervised learning algorithms, the diversity of the classifiers have to be sought from other channels. Indeed, here the diversity is obtained through manipulating the original labeled example set. In detail, the initial classifiers are trained from data sets generated via bootstrap sampling <ref type="bibr" target="#b12">[13]</ref> from the original labeled example set. These classifiers are then refined in the tri-training process, and the final hypothesis is produced via majority voting. The generation of the initial classifiers looks like training an ensemble from the labeled example set with a popular ensemble learning <ref type="bibr" target="#b11">[12]</ref> algorithm, that is, Bagging <ref type="bibr" target="#b5">[6]</ref>.</p><p>Tri-training can be regarded as a new extension to the cotraining algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>. As mentioned before, Blum and Mitchell's algorithm requires the instance space be described by two sufficient and redundant views, which can hardly be satisfied in common data mining scenarios. Since tri-training does not rely on different views, its applicability is broader. Goldman and Zhou's algorithm does not rely on different views either. However, their algorithm requires two different supervised learning algorithms that partition the instance space into a set of equivalence classes. Moreover, their algorithm frequently uses 10-fold cross validation on the original labeled example set to determine how to label the unlabeled examples and how to produce the final hypothesis. If the original labeled example set is rather small, cross validation will exhibit high variance and is not helpful for model selection. Also, the frequently used cross validation makes the learning process time-consuming. Since tri-training does not put any constraint on the supervised learning algorithm nor does it employ time-consuming cross validation process, both its applicability and efficiency are better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS ON UCI DATA SETS</head><p>Twelve UCI data sets <ref type="bibr" target="#b2">[3]</ref> are used in the experiments. Information on these data sets are tabulated in Table <ref type="table" target="#tab_1">2</ref>, where pos/neg presents the percentage of the number of positive examples against that of negative examples. Note that these data sets do not hold sufficient and redundant views. Here, the pos/neg ratio of L, U, and the test set are similar to that of the original data set. Note that on some big data sets, such as hypothyroid, kr-vs-kp, and sick, the number of labeled examples might be sufficient to train a good classifier even under 80 percent unlabel rate. However, even in these cases, semi-supervised learning algorithms such as tri-training can still be helpful, which will be shown in the experiments reported in this section.</p><p>J4.8 decision trees <ref type="bibr" target="#b28">[29]</ref>, BP neural networks, and Naive Bayes classifiers are used in the experiments. Under each unlabel rate, three independent runs with different random partitions of L and U are performed. The averaged results are summarized in Tables 3, 4, 5, and 6, which present the classification error rates of the hypothesis at round 0, i.e., the combination of the three initial classifiers trained from L, the final hypothesis generated by tri-training, and the improvement of the latter over the former. The performance of tri-training is compared with three semi-supervised learning algorithms, i.e., co-training, self-training1, and self-training2. The co-training algorithm is almost the same as the standard one <ref type="bibr" target="#b4">[5]</ref> except that since the experimental data sets are without natural sufficient and redundant views, the original attribute sets are randomly partitioned into two subsets with similar sizes and then each subset is regarded as a view.  <ref type="table" target="#tab_2">3,</ref><ref type="table">4</ref>, 5, and 6, the biggest improvements achieved by the semi-supervised learning algorithms have been boldfaced. Note that some values in the tables may look inconsistent due to truncation. For example, the initial and final performance of tri-training with J4.8 decision tree on hypothyroid appear identical in Table <ref type="table">5</ref> but the improvement is not zero.</p><p>Tables <ref type="table" target="#tab_2">3,</ref><ref type="table">4</ref>, 5, and 6, show that tri-training can effectively improve the hypotheses with all the classifiers under all the unlabel rates. In fact, if the improvements are averaged across all the data sets, classifiers, and  are apparently better than these generated by co-training under all the unlabel rates, apparently better than these generated by self-training1 under 20 percent unlabel rate, and apparently better than these generated by self-training2 on all but 80 percent unlabel rate. When Naive Bayes classifiers are used, the hypotheses generated by tri-training are comparable to these generated by co-training and self-training1, while apparently better than these generated by self-training2 under all the unlabel rates.</p><p>For further studying the performance of the compared semi-supervised learning algorithms, the number of test examples misclassified by the algorithms are depicted in Figs. <ref type="figure" target="#fig_10">4, 5,</ref> and<ref type="figure" target="#fig_11">6</ref>, which belongs to the one of the three runs of each algorithm that has the median performance. Note that here only a small number of figures are depicted since it may be too tedious to present all the figures (12 data sets Â 3 classifiers Â 4 unlabel rates = 144 figures). The figures presented are chosen according to the following two criteria. First, for each classifier and under each unlabel rate, the data sets where tri-training achieves median improvements can   Figs. <ref type="figure" target="#fig_8">4,</ref><ref type="figure" target="#fig_10">5</ref>, and 6 also show that sometimes the performance of ens-all is worse than that of single, especially when Naive Bayes classifiers are used. This is not strange because previous research on ensemble learning has disclosed that Bagging does not always improve the performance and especially it does not work well with stable learners such as Naive Bayes classifiers <ref type="bibr" target="#b5">[6]</ref>. However, Figs. <ref type="figure" target="#fig_8">4</ref> and<ref type="figure" target="#fig_10">5</ref> reveal that when ens-all is effective and     Here, the experimental configuration is the same as that used in <ref type="bibr" target="#b4">[5]</ref>. In each experiment, 263 <ref type="bibr">(</ref> Note that this data set is with two sufficient and redundant views since a Web page can be classified based on the words occurring on that page as well as these occurring in hyperlinks that point to that page <ref type="bibr" target="#b4">[5]</ref>. Therefore, Blum and Mitchell <ref type="bibr" target="#b4">[5]</ref> trained a page-based classifier and a hyperlink-based classifier for co-training. In addition, they defined a combined classifier to merge the outputs of these two classifiers.</p><p>Here, the algorithm described in <ref type="bibr" target="#b4">[5]</ref> is reimplemented and the parameters are set to the same values as in <ref type="bibr" target="#b4">[5]</ref>. J4.8 decision trees, BP neural networks, and Naive Bayes classifiers are used as alternatives to train the classifiers. The average predictive error rates and corresponding standard deviations are shown in Table <ref type="table">7</ref>, where initial denotes the performance achieved before semi-supervised learning, i.e., the performance obtained using only the labeled training examples, final denotes the performance achieved after semi-supervised learning, improv denotes the corresponding improvements, and Hypothesis denotes the performance of the hypotheses, i.e., the learned models. Note that after stemming and feature selection, 66 and five features are used to train the page-based and hyperlinkbased classifiers, respectively. Table <ref type="table">7</ref> also presents the results obtained by tri-training, self-training1, and self-training2 on this data set. Note that since these algorithms do not require different views, the page-based and hyperlink-based features are put together in training the individual classifiers. In the table, the best final hypothesis and the biggest improvement with each base classifier have been boldfaced.</p><p>Table <ref type="table">7</ref> shows that on the Web page classification task, tri-training can effectively utilize unlabeled data to enhance the learning performance. Actually, when Naive Bayes classifiers are used, the improvement of the final hypothesis generated by tri-training is about 13.4 percent, while when J4.8 decision trees and BP neural networks are used, the improvements are more impressive, i.e., 39.0 percent and 25.3 percent, respectively.</p><p>Table <ref type="table">7</ref>   Actually, through observing Table <ref type="table">7</ref> it can be found that when J4.8 decision trees and BP neural networks are used, the page-based classifiers degenerate in the co-training process, but the final hypotheses of co-training are still not bad. This suggests that the combination scheme used by the co-training algorithm may play an important role. Moreover, the co-training algorithm evidently utilizes the advantages offered by the two sufficient and redundant views because even when one component classifier has degenerated, the improvement of the other component classifier can still enable the improving of the final hypothesis. This confirms the claim raised by Blum and Mitchell <ref type="bibr" target="#b4">[5]</ref>, that is, when there exist sufficient and redundant views, appropriately utilizing them will benefit the learning performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, the tri-training algorithm is proposed. Through employing three classifiers, tri-training is facilitated with good efficiency and generalization ability because it could gracefully choose examples to label and use multiple classifiers to compose the final hypothesis. Moreover, its applicability is wide because it neither requires sufficient and redundant views nor does it put any constraint on the employed supervised learning algorithm. Experiments on UCI data sets and application to Web page classification indicate that although the algorithm is simple, it could exploit unlabeled examples effectively.</p><p>Note that the performance of semi-supervised learning algorithms are usually not stable because the unlabeled examples may often be wrongly labeled during the learning process <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b19">[20]</ref>. A promising solution to this problem may be using data editing mechanisms, such as the one described in <ref type="bibr" target="#b17">[18]</ref>, to help identify the wrongly labeled examples. Incorporating data editing mechanisms into tri-training and other semi-supervised learning algorithms is an interesting issue to be investigated in future work.</p><p>Ensemble learning techniques <ref type="bibr" target="#b11">[12]</ref>, in particular, Boosting, have already been introduced into semi-supervised learning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b8">[9]</ref>. It is evident that the working style of tritraining exhibits a new way to exploit ensemble techniques in this area. However, in its current form, such an exploitation is very limited because there are only three classifiers. Although previous research has shown that using three classifiers to make an ensemble could already improve the generalization ability <ref type="bibr" target="#b21">[22]</ref>, better performance can be anticipated with more classifiers, which is another interesting future issue.</p><p>Besides semi-supervised learning, unlabeled examples can be exploited by active learning <ref type="bibr" target="#b6">[7]</ref>, where the labels of some selected unlabeled examples are asked from the user. The employment of ensemble techniques in tri-training enables the introduction of a classic active learning method, i.e., query-by-committee <ref type="bibr" target="#b24">[25]</ref>. Roughly, the most disagreed unlabeled example by the classifiers can be selected to query. Designing an effective algorithm to combine tritraining with query-by-committee is an issue well worth studying.</p><p>Moreover, in the present implementation of tri-training, the classifiers are retrained in each round. If the base learners are incremental algorithms, it might be feasible for the classifiers to learn only the newly labeled examples, which could help improve the efficiency. This is also an interesting issue to be explored in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>TABLE 1 Pseudocode Describing the Tri-Training AlgorithmFor each data set, about 25 percent of the data are kept as test examples while the rest are used as the pool of training examples, i.e., L [ U. In each pool, L and U are partitioned under different unlabel rates including 80 percent, 60 percent, 40 percent, and 20 percent. For instance, assuming a pool contains 1,000 examples, when the unlabel rate is 80 percent, 200 examples are put into L with their labels while the remaining 800 examples are put into U without their labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The self-training1 algorithm uses the same three initial classifiers as these used by tri-training. In each round, instead of using the other two classifiers to label examples, each classifier labels unlabeled examples for itself, while, in predicting new examples, all the three classifiers are used. In other words, three single classifiers refined by self-training [19] is combined via majority voting. In contrast to tri-training, this algorithm does not utilize any co-training process while the voting scheme is used to improve generalization. The self-training2 algorithm also uses the same three initial classifiers as these used by tri-training. In each round, the unlabeled examples are labeled via majority voting the classifiers, and each classifier is refined by the same copy of the newly labeled data. Finally, the newly labeled examples and the original labeled examples are used together to train a single classifier which is used in prediction. It is worth noting that although self-training2 uses the same initial classifiers as these used in tri-training and self-training1, its initial hypotheses are different because it uses a single classifier instead of an ensemble in prediction. For fair comparison, the termination criteria used by these semisupervised learning algorithms are similar to that used by tri-training. In Tables</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Rates of the Initial and Final Hypotheses and the Corresponding Improvements of Tri-Training, Co-Training, Self-Training1, and Self-Training2, under 60 Percent Unlabel Rate unlabel rates, it can be found that the average improvement of tri-training is about 11.9 percent. It is impressive that with all the classifiers and under all the unlabel rates, tri-training has achieved the biggest average improvement. Moreover, Tables 3, 4, 5, and 6 also show that if the algorithms are compared through counting the number of winning data sets, i.e., the number of data sets on which an algorithm has achieved the biggest improvement among the compared algorithms, tri-training is almost always the winner. In detail, under 80 percent unlabel rate, when J4.8 decision trees are used, tri-training has 11 wining data sets while the other algorithms have at most one winning data set; when Naive Bayes classifiers are used, tri-training and self-training1 have seven winning data sets, respectively, while the remaining algorithms do not have winning data sets. Under 60 percent unlabel rate, when J4.8 decision trees and BP neural networks are used, tri-training has eight and nine winning data sets, respectively, while the other algorithms have at most two and three winning data sets, respectively; when Naive Bayes classifiers are used, tri-training has 10 winning data sets, while the other algorithms have at most five winning data sets. Under 40 percent unlabel rate, tri-training has seven, six, and nine winning data sets when J4.8 decision trees, BP neural networks, and Naive Bayes classifiers are used, respectively, while the other algorithms have at most five, four, and six winning data sets, respectively. Under 20 percent unlabel rate, when J4.8 decision trees and BP neural networks are used, tri-training has eight and nine winning data sets, respectively, while the remaining algorithms have at most three and two winning data sets, respectively; when Naive Bayes classifiers are used, tri-training has seven winning data sets while the other algorithms have at most six winning data sets. Only when BP neural networks are used under 80 percent unlabel rate, tritraining has fewer winning data sets than self-training1. The error rates of the compared algorithms are depicted in Figs. 1, 2, and 3. Besides the semi-supervised learning algorithms, on each data set three single classifiers are trained from only the labeled training examples, i.e., L. The average error rate of the single classifiers is shown as a horizontal line in each figure, which is denoted by single.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Rates of the Initial and Final Hypotheses and the Corresponding Improvements of Tri-Training, Co-Training, Self-Training1, and Self-Training2, under 40 Percent Unlabel Rate Moreover, three ensembles each comprising three classifiers are trained by Bagging from the pool of training examples, i.e., ðL [ UÞ while labels of all the examples are provided. The average error rate of the ensembles is also shown as a horizontal line in each figure, which is denoted by ens-all.Note that in Figs.1, 2, and 3, the error rates have been averaged across all the experimental data sets, and since the semi-supervised learning algorithms may terminate in different rounds, the error rates at termination are used as the error rates of the rounds after termination. Figs. 1, 2, and 3 reveal that on all the subfigures, the final hypotheses generated by tri-training are better than the initial hypotheses, which confirms that tri-training can effectively exploit unlabeled examples to enhance the learning performance. When J4.8 decision trees are used, the hypotheses generated by tri-training are apparently better than these generated by the other semi-supervised learning algorithms in the same rounds, except that under 40 percent unlabel rate the hypotheses generated by tritraining and self-training1 are comparable. When BP neural networks are used, the hypotheses generated by tri-training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Rates of the Initial and Final Hypotheses and the Corresponding Improvements of Tri-Training, Co-Training, Self-Training1, and Self-Training2, under 20 Percent Unlabel Rate be chosen. For instance, according to Table 6, when BP neural networks are used under 20 percent unlabel rate, tritraining achieves its sixth and seventh biggest improvements on sick and wdbc, respectively, among all of the 12 data sets. Therefore, sick and wdbc can be chosen. Second, attempts are made to choose as more diverse data sets as possible. For instance, since sick has already been chosen when J4.8 decision trees are used under 80 percent unlabel rate, wdbc instead of sick is chosen for BP neural networks under 20 percent unlabel rate. Figs. 4, 5, and 6 reveal that on all the subfigures, the final hypotheses generated by tri-training are better than the initial hypotheses. Compared with co-training, the final hypotheses of tri-training are almost always better except on Fig. 5a where the final hypothesis of co-training is slightly better. Comparing with self-training1, the final</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Error rates averaged across all the data sets when J4.8 decision trees are used. (a) 80 percent unlabel rate, (b) 60 percent unlabel rate, (c) 40 percent unlabel rate, and (d) 20 percent unlabel rate.</figDesc><graphic coords="8,112.71,69.17,341.27,260.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Error rates averaged across all the data sets when BP neural networks are used. (a) 80 percent unlabel rate, (b) 60 percent unlabel rate, (c) 40 percent unlabel rate, and (d) 20 percent unlabel rate.</figDesc><graphic coords="8,112.59,362.04,341.50,263.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Error rates averaged across all the data sets when Naive Bayes classifiers are used. (a) 80 percent unlabel rate, (b) 60 percent unlabel rate, (c) 40 percent unlabel rate, and (d) 20 percent unlabel rate.</figDesc><graphic coords="9,109.70,69.17,347.09,266.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Results on data sets where tri-training achieves median improvement (J4.8 decision trees are used). (a) sick, 80 percent unbale rate, (b) diabetes, 60 percent unlabel rate, (c) tic-tac-toe, 40 percent unlabel rate, and (d) hypothyroid, 20 percent unlabel rate.</figDesc><graphic coords="9,112.08,366.18,342.47,263.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4</head><label></label><figDesc>APPLICATION TO WEB PAGE CLASSIFICATION The Web page classification data set 1 consists of 1,051 Web pages collected from Web sites of Computer Science departments of four universities: Cornell University,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Results on data sets where tri-training achieves median improvement (BP neural networks are used). (a) ionosphere, 80 percent unlabel rate, (b) bupa, 60 percent unlabel rate, (c) australian, 40 percent unlabel rate, and (d) wdbc, 20 percent unlabel rate.</figDesc><graphic coords="10,112.59,368.45,341.50,263.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Results on data sets where tri-training achieves median improvement (Naive Bayes classifiers are used). (a) vote, 80 percent unlabel rate, (b) colic, 60 percnet unlabel rate, (c) ionosphere, 40 percent unlabel rate, and (d) german, 20 percent unlabel rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>25 percent) of the 1,051 Web pages are first selected at random as test examples. The remaining data is used to generate a labeled training example set L containing three positive and nine negative examples drawn at random. The remaining examples that are not drawn for L are used as the unlabeled pool U. Five runs with different training/test splits are performed, and the average result and standard deviation are recorded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>also shows that the improvement of tri-training is bigger than that of the co-training algorithm and the selftraining algorithms when J4.8 decision trees and BP neural networks are used. While when Naive Bayes classifiers are used, the improvement of tri-training is smaller than that of co-training and the self-training algorithms. In particular, when J4.8 decision trees and BP neural networks are used, the improvements brought by tri-training are much bigger than that brought by co-training; while when Naive Bayes classifiers are used, the improvement of tri-training is much smaller than that of co-training. However, Table 7 shows that the component Naive Bayes classifiers refined by tritraining are better than these refined by co-training. This may imply that although the majority voting scheme used by tri-training is effective in combining the component J4.8 decision trees and BP neural networks, it may be far less effective than the combination scheme used by the cotraining algorithm in combining the component Naive Bayes classifiers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>TABLE 7 The</head><label>7</label><figDesc>Performances of Tri-Training, Co-Training, Self-Training1, and Self-Training2 on the Web Page Classification Problem</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,104.43,345.43,357.78,412.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>L denote the classification noise rate of L, that is, the number of examples in L that are mislabeled is L jLj. Let e e t 1 denote the upper bound of the classification error rate of h 2 &amp;h 3 in the tth round, i.e., the error rate of the hypothesis derived from the combination of h 2 and h 3 . Assuming there are z number of examples on which the classification made by h 2 agrees with that made by h 3 , and among these examples both h 2 and h 3 make correct classification on z 0</figDesc><table><row><cell>examples, then number of examples in L t that are mislabeled is e e t 1 can be estimated as ðzÀz 0 Þ z . Thus, the e e t 1 jL t j.</cell></row><row><cell>Therefore, the classification noise rate in the tth round is:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Experimental</head><label>2</label><figDesc>Data Sets</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 The</head><label>3</label><figDesc>Classification Error Rates of the Initial and Final Hypotheses and the Corresponding Improvements of Tri-Training, Co-Training, Self-Training1, and Self-Training2, under 80 Percent Unlabel Rate</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 17,NO. 11, NOVEMBER 2005   </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank the anonymous reviewers for their constructive comments, especially for the suggestions on the comparison with self-training algorithms. This work was supported by the the National Science Fund for Distinguished Young Scholars of China under the Grant No. 60325207, the Foundation for the Author of National Excellent Doctoral Dissertation of China under the Grant No. 200343, the Jiangsu Science Foundation Key Project under the Grant No. BK2004001, and the National Science Foundation of China under the Grant No. 60496320.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning from Noisy Examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="343" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting Unlabeled Data in Ensemble Methods</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Demiriz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Maclin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eighth ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining</title>
		<meeting>Eighth ACM SIGKDD Int&apos;l Conf. Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<ptr target="http://www.ics.uci.edu/~mlearn/MLRepository.html" />
		<title level="m">UCI Repository of Machine Learning Databases</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Information and Computer Science, Univ. of California, Irvine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning from Labeled and Unlabeled Data Using Graph Mincuts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Int&apos;l Conf. Machine Learning</title>
		<meeting>18th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Combining Labeled and Unlabeled Data with Co-Training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Ann. Conf. Computational Learning Theory</title>
		<meeting>11th Ann. Conf. Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagging Predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved Generalization with Active Learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Atlas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ladner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="201" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised Models for Named Entity Classifications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint SIGDAT Conf. Empirical Methods in Natural Language Processing and Very Large Corpora</title>
		<meeting>Joint SIGDAT Conf. Empirical Methods in Natural Language essing and Very Large Corpora</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="100" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-Supervised Marginboost</title>
		<author>
			<persName><forename type="first">F</forename><surname>D'alche ´-Buc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ambroise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="553" to="560" />
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PAC Generalization Bounds for Co-Training</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Becker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="375" to="382" />
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum Likelihood from Incomplete Data via the EM Algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Royal Statistical Soc., Series B</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble Methods in Machine Learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Kittler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Roli</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">1867</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">An Introduction to the Bootstrap</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing Supervised Learning with Unlabeled Data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th Int&apos;l Conf. Machine Learning</title>
		<meeting>17th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="327" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Corrected Co-Training for Statistical Parsers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working Notes of the ICML&apos;03 Workshop Continuum from Labeled to Unlabeled Data in Machine Learning and Data Mining</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transductive Inference for Text Classification Using Support Vector Machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Int&apos;l Conf. Machine Learning</title>
		<meeting>16th Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Uyar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Jordan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Petsche</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, Mass</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="571" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying and Handling Mislabelled Instances</title>
		<author>
			<persName><forename type="first">F</forename><surname>Muhlenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lallich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Zighed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="109" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analyzing the Effectiveness and Applicability of Co-Training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ghani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Ninth ACM Int&apos;l Conf. Information and Knowledge Management</title>
		<imprint>
			<biblScope unit="page" from="86" to="93" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text Classification from Labeled and Unlabeled Documents Using EM</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="103" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Limitations of Co-Training for Natural Language Learning from Large Data Sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2001 Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>2001 Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">MiniBoosting Decision Trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<ptr target="http://www.cse.unsw.edu.au/~quinlan/miniboost.ps" />
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping</title>
		<author>
			<persName><forename type="first">E</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 16th Nat&apos;l Conf. Artificial Intelligence</title>
		<meeting>16th Nat&apos;l Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="474" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Applying Co-Training Methods to Statistical Parsing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Second Ann. Meeting of the North Am. Chapter of the Assoc. for Computational Linguistics</title>
		<meeting>Second Ann. Meeting of the North Am. Chapter of the Assoc. for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query by Committee</title>
		<author>
			<persName><forename type="first">H</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Opper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fifth Ann. ACM Conf. Computational Learning Theory</title>
		<meeting>Fifth Ann. ACM Conf. Computational Learning Theory</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="287" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The Effect of Unlabeled Samples in Reducing the Small Sample Size Problem and Mitigating the Hughes Phenomenon</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shahshahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Landgrebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1087" to="1095" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bootstrapping Statistical Parsers from Small Data Sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Steedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruhlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 11th Conf. European Chapter</title>
		<title level="s">the Assoc. Computational Linguistics</title>
		<meeting>11th Conf. European Chapter</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="331" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised Word Sense Disambiguation Rivaling Supervised Methods</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Ann. Meeting of the Assoc. Computational Linguistics</title>
		<meeting>33rd Ann. Meeting of the Assoc. Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<title level="m">Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations</title>
		<meeting><address><addrLine>San Francisco</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
