<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One Embedder, Any Task: Instruction-Finetuned Text Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-12-19">19 Dec 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
							<email>hjsu@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
							<email>jkasai@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
							<email>yizhongw@cs.washington.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yushi</forename><surname>Hu</surname></persName>
							<email>yushihu@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
							<email>ostendor@uw.edu</email>
						</author>
						<author>
							<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Allen Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">?</forename><forename type="middle">?</forename><surname>Luke</surname></persName>
						</author>
						<author>
							<persName><forename type="first">?</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">The University of Hong Kong</orgName>
								<orgName type="institution" key="instit2">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<title level="a" type="main">One Embedder, Any Task: Instruction-Finetuned Text Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-12-19">19 Dec 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2212.09741v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce INSTRUCTOR, a new method for computing text embeddings given task instructions: every text input is embedded together with instructions explaining the use case (e.g., task and domain descriptions). Unlike encoders from prior work that are more specialized, INSTRUCTOR is a single embedder that can generate text embeddings tailored to different downstream tasks and domains, without any further training. We first annotate instructions for 330 diverse tasks and train INSTRUCTOR on this multitask mixture with a contrastive loss. We evaluate INSTRUC-TOR on 70 embedding evaluation tasks (66 of which are unseen during training), ranging from classification and information retrieval to semantic textual similarity and text generation evaluation. INSTRUCTOR, while having an order of magnitude fewer parameters than the previous best model, achieves stateof-the-art performance, with an average improvement of 3.4% compared to the previous best results on the 70 diverse datasets. Our analysis suggests that INSTRUCTOR is robust to changes in instructions, and that instruction finetuning mitigates the challenge of training a single model on diverse datasets. Our model, code, and data are available at https: //instructor-embedding.github.io.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text embeddings represent discrete text inputs (e.g., sentences, documents, and code) as fixed-sized vectors that can be used in many downstream tasks. These tasks include semantic textual similarity <ref type="bibr" target="#b0">(Agirre et al., 2012;</ref><ref type="bibr" target="#b44">Marelli et al., 2014;</ref><ref type="bibr" target="#b5">Cer et al., 2017;</ref><ref type="bibr" target="#b38">Lin et al., 2018)</ref>, information retrieval <ref type="bibr" target="#b46">(Mitra et al., 2017;</ref><ref type="bibr" target="#b27">Karpukhin et al., 2020;</ref><ref type="bibr" target="#b24">Izacard et al., 2022)</ref>, automatic text evaluation <ref type="bibr">(Zhang et al., 2020;</ref><ref type="bibr" target="#b59">Sellam et al., 2020;</ref><ref type="bibr" target="#b21">Hessel et al., 2021)</ref>, prompt retrieval for in-context learning <ref type="bibr">(Liu et al.</ref>, 2022; <ref type="bibr" target="#b57">Rubin et al., 2022;</ref><ref type="bibr" target="#b62">Su et al., 2022)</ref>, and beyond. Recently, we have seen dramatic advances in learning text embeddings <ref type="bibr" target="#b32">(Kiros et al., 2015;</ref><ref type="bibr" target="#b8">Conneau et al., 2017;</ref><ref type="bibr" target="#b43">Logeswaran and Lee, 2018;</ref><ref type="bibr" target="#b55">Reimers and Gurevych, 2019;</ref><ref type="bibr" target="#b17">Gao et al., 2021;</ref><ref type="bibr" target="#b50">Ni et al., 2021</ref><ref type="bibr" target="#b49">Ni et al., , 2022) )</ref> that perform well on their intended tasks or datasets.</p><p>However, most existing embeddings can have significantly degraded performance when applied to new tasks or domains <ref type="bibr" target="#b63">(Thakur et al., 2021;</ref><ref type="bibr" target="#b47">Muennighoff et al., 2022)</ref>. For example, DPR <ref type="bibr" target="#b27">(Karpukhin et al., 2020)</ref> is stronger for retrieval than text similarity tasks, and vice versa for SimCSE <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>. Moreover, existing embeddings usually perform poorly when applied to the same type of task but in different domains such as medicine and finance. A common method to address this issue is to further finetune the embed-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Similiarty</head><p>Why do rockets look white?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>Retrieve documents that can help answer the question:</p><p>Find documents that can help verify the fact:</p><p>The Ten Commandments is an epic film.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fact Checking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Analysis</head><p>Classify the sentiment of the sentence:</p><p>You should see their decadent dessert menu ... dings on datasets in downstream tasks and domains, which often requires a lot of annotated data <ref type="bibr" target="#b19">(Gururangan et al., 2020)</ref>. In this paper, we hypothesize that text embeddings (even for the same text input) can be adjusted to different downstream applications using task and domain descriptions, without further task-or domain-specific finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We introduce INSTRUCTOR (Instruction-based</head><p>Omnifarious Representations), a single multitask model that generates task-and domain-aware embeddings given a text input and its task instructions. It achieves state-of-the-art performance on massively many downstream embedding tasks without any training. At the core of our approach is instruction-based finetuning <ref type="bibr">(Zhong et al., 2021;</ref><ref type="bibr" target="#b45">Min et al., 2022;</ref><ref type="bibr" target="#b58">Sanh et al., 2022;</ref><ref type="bibr" target="#b67">Wei et al., 2022)</ref>: we embed every input together with its end task and domain instruction, departing from prior approaches to embeddings that only take text input. INSTRUCTOR embeds the same input into different vectors for different end goals (e.g., Who sings the song "Love Story"? is embedded into three different vectors for different tasks in Fig. <ref type="figure" target="#fig_0">1</ref>). As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, INSTRUCTOR is trained on MEDI, our new collection of 330 text embedding datasets newly annotated with human-written task instructions ( ?2.3). We train INSTRUCTOR with a contrastive loss over all datasets that maximizes the similarity between semantically related text pairs while minimizing unrelated pairs. We extensively evaluate INSTRUCTOR on diverse domains (e.g., finance, medicine, and news) and a variety of downstream applications (a total of 70 embedding evaluation datasets, including 66 not seen during training), spanning classification, semantic textual similarity, information retrieval, text generation evaluation, and prompt retrieval for incontext learning. INSTRUCTOR significantly outperforms prior state-of-the-art embedding models by an average of 3.4% over the 70 diverse datasets. INSTRUCTOR also outperforms a variant that is trained without task instructions ( ?4), demonstrating the importance of instructions to create taskaware embeddings. Our analysis shows that instruction finetuning addresses the challenge of training a single model on diverse datasets ( ?4.1). Further, we demonstrate that the task diversity of MEDI makes the performance of INSTRUCTOR particularly robust to paraphrases in instructions ( ?4.2). Overall, these results strongly suggest that instruction finetuning should be adopted broadly for text embeddings, which we support by sharing all of our models and code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">INSTRUCTOR</head><p>INSTRUCTOR encodes inputs together with task instructions, thereby providing task-specific representations that can be used for many downstream language tasks, without any additional training.</p><p>Here we introduce the architecture of INSTRUC-TOR ( ?2.1), present how we perform multitask instruction-based finetuning ( ?2.2), and describe how we collect and annotate the MEDI training data ( ?2.3). By default, we refer "task" to a dataset, and use them interchangeably throughout the paper, while a "task category", such as Retrieval, includes many tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding Architecture</head><p>We build INSTRUCTOR, based on the single encoder architecture <ref type="bibr" target="#b25">(Izacard and Grave, 2021;</ref><ref type="bibr" target="#b50">Ni et al., 2021</ref><ref type="bibr" target="#b49">Ni et al., , 2022))</ref>. Following prior work <ref type="bibr" target="#b50">(Ni et al., 2021</ref><ref type="bibr" target="#b49">(Ni et al., , 2022))</ref>, we use GTR models as the backbone encoder (GTR-Base for INSTRUCTOR-Base, GTR-Large for INSTRUCTOR, GTR-XL for INSTRUC-TOR-XL). The GTR models are initialized from T5 models, pretrained on a web corpus, and finetuned on information search datasets. The availability of different sizes in the GTR model family allows us to explore the scaling behaviors of instructionfinetuned embedding models. Given an input text x and a task instruction I x , INSTRUCTOR encodes their concatenation I x ? x. We then generate a fixed-sized, task-specific embedding E I (I x , x) by applying mean pooling to the last hidden representations over the tokens in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Objective</head><p>INSTRUCTOR is trained by formulating a wide variety of tasks as a text-to-text problem of distinguishing good/bad candidate outputs y ? {y + , y i } given an input x, where a training sample corresponds to the tuple (x, I x , y, I y ), with I x and I y being instructions associated with x and y, respectively. For example, in a retrieval task, x is a query, and good/bad y is a relevant/irrelevant document from some document collection. For a textual similarity task, the input and output have a similar form and typically come from the same source collection. For a classification task, training samples can be formed by choosing y as text sequences associated with the same vs. different classes for good vs. bad examples (Details about pair construction are in ?2.3). The input and output instructions depend on the symmetry of the task. For tasks such as textual similarity, where the input and output have the same form, the instructions are the same. For asymmetric tasks such as retrieval, where the input is a single sentence query and the output is a document, the instructions reflect that difference.</p><p>The goodness of candidate y for input x is given by similarity s(x, y) that is the cosine between their INSTRUCTOR embeddings: s(x, y) = cos(E I (I x ? x), E I (I y ? y)) Following <ref type="bibr" target="#b50">Ni et al. (2021)</ref>, we maximize the similarity between positive pairs (x, y + ) and minimize negative pairs {(x, y</p><formula xml:id="formula_0">- i )} k i=1 ,</formula><p>where k denotes the number of negative pairs per positive pair. Specifically, our training objective is:</p><formula xml:id="formula_1">L = e s(x,y + )/?</formula><p>? y?B e s(x,y)/? , where ? is the softmax temperature and B is a union of (x, y + ) and {(x,</p><formula xml:id="formula_2">y - i )} k i=1</formula><p>. Further following <ref type="bibr" target="#b50">Ni et al. (2021)</ref>, we compute the same loss with x and y swapped and add it to the previous loss (i.e., bidirectional in-batch sampled loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">MEDI: Multitask Embedding Data with Instructions</head><p>There are no existing datasets that consist of a variety of tasks for embedding training with instructions. We thus construct a collection of 330 datasets with instructions across diverse task categories and domains: Multitask Embeddings Data with Instructions (MEDI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Construction</head><p>We build MEDI by combining 300 datasets from Super-NaturalInstructions (super-NI; <ref type="bibr" target="#b66">Wang et al., 2022)</ref> with 30 datasets from existing collections designed for embedding training.</p><p>The super-NI datasets come with natural language instructions, but positive and negative pairs are not provided. We construct these pairs by using Sentence-T5 embeddings <ref type="bibr" target="#b49">(Ni et al., 2022)</ref>,<ref type="foot" target="#foot_0">1</ref> denoted with E(?). For the classification datasets, we calculate the pairwise cosine similarity between examples based on input text embeddings cos(E(x i ), E(x j )). An example x i with a high similarity to x j is used to create a positive pair if both examples have the same class label (y + j = y i ), and a negative pair if the labels differ (y</p><formula xml:id="formula_3">- j ? y i ).</formula><p>For the remaining tasks where the output labels are text sequences, the following scores are first computed:</p><p>s pos = cos(E(x i ), E(x j )) + cos(E(y i ), E(y j ))</p><formula xml:id="formula_4">s neg = cos(E(x i ), E(x j )) -cos(E(y i ), E(y j ))</formula><p>We select example pairs with the highest s pos as positive pairs and highest s neg as hard negative pairs. We use one hard negative together with in-batch sampled negatives in the training. Our later analysis shows that the training data from super-NI particularly improve the instruction robustness in evaluation due to the diverse task definitions ( ?4.2). The other 30 embedding training datasets come from the Sentence Transformers embedding data, 2 KILT <ref type="bibr" target="#b53">(Petroni et al., 2021)</ref>, and MedMCQA <ref type="bibr" target="#b52">(Pal et al., 2022)</ref>. These 30 datasets already contain positive pairs; a few of them, such as MS-MARCO <ref type="bibr" target="#b2">(Bajaj et al., 2016)</ref> and Natural Questions <ref type="bibr" target="#b34">(Kwiatkowski et al., 2019)</ref>, also contain hard negative pairs. Following <ref type="bibr" target="#b50">Ni et al. (2021)</ref>, we use four negative pairs (hard or in-batch negatives) during the model finetuning process. Since all of these datasets do not have instructions, we develop a unified instruction template and manually write a specific prompt for each dataset, as described next.</p><p><ref type="foot" target="#foot_2">3</ref> We release these instructions together with our MEDI data.</p><p>Instruction Annotation Each training instance from MEDI is a pair of text inputs {(x, y)}. For example, in open-domain QA, x is a query, and y is an evidence document. The training objective is to maximize the embedding similarity of positive pairs while minimizing that of negative pairs. We annotate each dataset with a pair of natural language instructions I x and I y that describes how the embeddings of x and y are used for the task. In open-domain QA (e.g., Natural Questions in Table <ref type="table" target="#tab_0">1</ref>), I x is "Represent the Wikipedia question for retrieving supporting documents; Input: ," and I y is "Represent the Wikipedia document for retrieval; Input: ." Therefore, the final dataset consists of a set of pair examples with their instructions {(x, I x , y, I y )}. In some cases, I x and I y are the same; for instance, STS tasks measure the similarity between two sentences x and y. Both x and y have the same text type and encoding objective. Therefore, we annotate them with the same instruction I x = I y and call these cases symmetric in contrast to asymmetric tasks such as open-domain QA.</p><p>To make instructions consistent across all datasets in MEDI, we design a unified instruction format that consists of the following parts (see Table 5 in the appendix for instances of each part):</p><p>? Text Type specifies the type of input text that we encode using the embedding model. For example, for an open-domain QA task, the input type of the query is a question, while the input type of the target is a document. ? Task Objective (Optional) describes the objective of how the input text is used in a task. For example, for a classification task, the task objective is to classify the sentence into some category, while the task objective of the retrieval is to retrieve a relevant document. Because not all sentences are associated with a specific task (e.g., STS targets general encoding), we make this part optional. ? Domain (Optional) describes the task domain. For example, for NewsIR, the domain of the task is news. Because not all tasks specify a domain (e.g., STS deals with general statements), we make this part optional as well. The final instruction takes the following format: "REPRESENT THE (DOMAIN) TEXT TYPE FOR TASK OBJECTIVE; INPUT:." Appendix 8 shows instructions for each dataset in MEDI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We train INSTRUCTOR on the MEDI data and evaluate it on a wide range of 70 downstream tasks. Specifically, we use the MTEB benchmark from recent work <ref type="bibr" target="#b47">(Muennighoff et al., 2022)</ref>, which consists of 56 datasets over 7 diverse task categories, such as classification, reranking, and information retrieval ( ?3.1.1). We then further apply INSTRUC-TOR to prompt retrieval for in-context learning ( ?3.1.2) and text generation evaluation ( ?3.1.3). In all three settings, INSTRUCTOR achieves the stateof-the-art performance ( ?3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Embedding Evaluations</head><p>Here we provide a high-level summary of the evaluation tasks (Table <ref type="table" target="#tab_0">1</ref>). Following MTEB <ref type="bibr" target="#b47">(Muennighoff et al., 2022)</ref>, Billboard <ref type="bibr">(Kasai et al., 2022a)</ref>, and prompt retrieval <ref type="bibr" target="#b62">(Su et al., 2022)</ref>, we split 70 evaluation datasets into 9 categories by task objectives. Out of the 70 evaluation tasks, 66 are unseen during training (See Table <ref type="table" target="#tab_9">6</ref> for datasets included during training). Table <ref type="table" target="#tab_0">1</ref> for examples and instructions for the evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Massive Text Embedding Benchmark</head><p>MTEB <ref type="bibr" target="#b47">(Muennighoff et al., 2022</ref>) is a comprehensive embedding evaluation benchmark that aims to provide a holistic view of current embedding  <ref type="table" target="#tab_6">7</ref> and<ref type="table">8</ref> in the appendix.</p><p>models' performance and to discover universal text embeddings applicable to a wide range of tasks. It combines several conventional benchmarks (e.g., BEIR, <ref type="bibr" target="#b63">Thakur et al., 2021, and</ref><ref type="bibr">STS, Cer et al., 2017)</ref> and spans a wide range of domain-specific datasets, including science, biology, and medicine. Following <ref type="bibr" target="#b47">Muennighoff et al. (2022)</ref>, we also report the average performance over 56 datasets. For each task family, we briefly describe the task objective, evaluation metric, and how embeddings are used.</p><p>Retrieval Given a query q and a corpus D = {p 1 , p 2 ...p n }, retrieval aims to find the most relevant documents p i in D for query q. The embedding model is used to embed q and p 1 ...p n into fixed-sized vectors, and then the similarity between q and p i is measured by their embedding cosine similarity. There are 14 diverse datasets (e.g., Natural Questions, Scifact, and NFCorpus) together with the community question-answering (CQA) benchmark <ref type="bibr" target="#b22">(Hoogeveen et al., 2015)</ref>. We use NDCG@10 (Normalized Discounted cumulative gain at rank position 10) to measure the performance.</p><p>Reranking Reranking ranks a list of documents based on their relevance to a query. Given a query q and a list of documents D = {p 1 , p 2 ...p n }, the embedding model computes embeddings of both the query and documents, which are then used to rank the documents based on their cosine similarities. We use MAP (mean average precision), a standard metric in reranking, to measure performance.</p><p>Clustering The goal of clustering is to group similar documents into meaningful clusters. Given a set of documents, the encoder maps each document into an embedding. The k-means clustering algorithm is then used to partition the embedded documents into clusters. The clustering performance is measured by the v-measure that is independent of the permutations of clustering labels <ref type="bibr" target="#b56">(Rosenberg and Hirschberg, 2007)</ref>.</p><p>Pair Classification Pair classification tasks aim to predict a binary label for a pair of texts. An example of this task is paraphrase identification, where the goal is to predict whether two sentences are paraphrases of each other. Given a sentence pair (t 1 , t 2 ), the embedding model encodes t 1 and t 2 separately. The cosine similarity between the two embeddings is then used to predict the label. The average precision score is measured for evaluation.</p><p>Classification Classification is a popular way to evaluate the quality of embeddings <ref type="bibr" target="#b7">(Conneau and Kiela, 2018)</ref>. For each example in the classification dataset, the embedding of the input text is used as features to a classifier. The classifier is trained on the training data while sentence embedings are kept frozen. We report the classification accuracy on the test set as the evaluation metric.</p><p>STS Semantic textual similarity (STS) tasks evaluate the similarity between two sentences. Given a sentence pair (t 1 , t 2 ), the embedding model maps t 1 and t 2 into embeddings separately, and then the similarity between t 1 and t 2 is measured by their embedding cosine similarity. The evaluation metric is Spearman's rank correlation, which measures the correlation between the similarity scores and human judgements.</p><p>Summarization Automatic summarization evaluation aims to evaluate the quality of a machinegenerated summary given a reference summary. While human evaluations are considered more accurate, automatic evaluations allow for fast, inexpensive development cycles <ref type="bibr" target="#b31">(Khashabi et al., 2022)</ref>. Given a reference summary r and a machinegenerated summary t, the embedding model maps them into embeddings separately, and we compute the cosine similarity between r and t. Spearman's rank correlation is reported between human judgements and automatic scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Prompt Retrieval</head><p>Large language models have demonstrated the ability of in-context learning, where the model can perform downstream tasks by conditioning generation on a few task demonstrations <ref type="bibr" target="#b41">(Liu et al., 2021)</ref>.  <ref type="formula">2022</ref>), we use the retrieved examples for in-context learning on GPT-J (Wang and Komatsuzaki, 2021) over 11 diverse downstream tasks (e.g., classification, multiple choice, and textto-SQL) that are not included in MEDI (thus zeroshot settings). We compare different embedding methods by measuring the average performance on these downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Automatic Evaluation for Generation</head><p>Similar to summarization evaluation in MTEB, we use the Billboard benchmark <ref type="bibr">(Kasai et al., 2022a)</ref> to apply INSTRUCTOR to automatic evaluations for three additional text generation tasks: MSCOCO image captioning <ref type="bibr" target="#b39">(Lin et al., 2014;</ref><ref type="bibr">Kasai et al., 2022b)</ref>, CNN/DailyMail news summarization <ref type="bibr" target="#b12">(Fabbri et al., 2021)</ref>, and WMT21 Chinese-to-English translation <ref type="bibr">(Barrault et al., 2020;</ref><ref type="bibr" target="#b15">Freitag et al., 2021)</ref>. Following <ref type="bibr">Kasai et al. (2022a)</ref>, we measure the cosine similarity between the generated text and each reference text and take the maximum similarity score over all references available <ref type="bibr">(Zhang et al., 2020)</ref>. We evaluate all embedding models by the Pearson correlation with the human judgments, again following <ref type="bibr">Kasai et al. (2022a)</ref>. We then report the average correlation scores over the three datasets. Note that we do not use the English-to-German dataset in Billboard because our models are trained only on English data.  <ref type="table" target="#tab_9">6</ref> in the appendix. At each step, we first randomly select a dataset and then construct a minibatch only using the examples from that dataset. In this way, we ensure that in-batch negatives are sampled from the same dataset, thereby preventing the model from using task differences to predict the negative label. We use the maximum batch size that fits the machine memory and run all our experiments on 40GB A100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Training</head><p>Training We initialize INSTRUCTOR with the GTR-Large model <ref type="bibr" target="#b50">(Ni et al., 2021</ref>, 335M parameters)<ref type="foot" target="#foot_3">4</ref> and finetune it on MEDI using the AdamW optimizer with learning rate 2 ? 10 -5 and warmup ratio 0.1. We use a softmax temperature of 0.01 and finetune INSTRUCTOR for 20K steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baselines</head><p>We use the official MTEB benchmark for comparisons, but here we highlight several strong baselines with the following two types. The first class of baselines is embedding models specializing in information retrieval: Contriever-MS (Izacard et al., 2022), GTR <ref type="bibr" target="#b50">(Ni et al., 2021)</ref>, and coCondenser-MS <ref type="bibr" target="#b16">(Gao and Callan, 2022)</ref>. They are all trained on open-domain QA datasets such as MS MARCO <ref type="bibr" target="#b2">(Bajaj et al., 2016)</ref>. The second class of baselines focuses on semantic textual similarity: SimCSE <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>, Sent-T5 <ref type="bibr" target="#b49">(Ni et al., 2022)</ref>, and SGPT-NLI <ref type="bibr" target="#b47">(Muennighoff, 2022)</ref>. They are mainly trained on symmetric paraphrase datasets such as NLI <ref type="bibr" target="#b68">(Williams et al., 2018)</ref> and the Quora question pairs. 5 All of these baselines are based on pretrained language models, achieving strong performance on the MTEB leaderboard.</p><p>In particular, Sent-T5-XXL and GTR-XXL (both with 4.8B parameters) achieve the first and second best average performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Main Results</head><p>Table <ref type="table" target="#tab_2">2</ref> presents the results from INSTRUCTOR and the baselines over the three benchmarks: MTEB, Billboard, and prompt retrieval. INSTRUCTOR achieves the best performance on all three benchmarks on average. Compared to GTR-Large (335M), from which INSTRUCTOR is initialized, 5 https://www.quora.com/q/quoradata/.</p><p>instruction finetuning enhances the performance by 4.6%, 24.0%, and 9.0% in MTEB, Billboard, and prompt retrieval respectively. Specifically, among all task categories, INSTRUCTOR-Large (335M) demonstrates large improvements over GTR-Large on the text evaluation (24.0%), prompt retrieval (9.0%), and classification tasks (7.3%). Particularly noteworthy is INSTRUCTOR-Large's performance compared to the previous state-of-the-art model, Sent-T5-XXL (58.4 vs. 56.5 on average), despite the fact that INSTRUCTOR has one order of magnitude fewer parameters (335M vs. 4.8B).</p><p>As expected, the retrieval-based models (e.g., GTR-XXL) show strong performance on retrieval and reranking but significantly lag behind on STS and classification. Conversely, similarity-based models (e.g., Sent-T5-XXL) perform well on STS, classification, and text evaluation, but not on retrieval. It suggests that these baselines tend to generate specialized embeddings that only excel at certain tasks, while INSTRUCTOR provides universal embeddings that perform well on diverse task categories.</p><p>We demonstrate INSTRUCTOR enables universal text embeddings for many diverse tasks. Here we analyze our results from various perspectives: the importance of instructions ( ?4.1), instruction robustness ( ?4.2) and complexity ( ?4.3), model sizes ( ?4.4), domain shifts ( ?4.5), and qualitative analysis ( ?4.6). By default, we report average performance across all categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Instructions Enable Diverse Training</head><p>Here we analyze the importance of instructions when training data are diverse. We first split MEDI into symmetric (e.g., text similarity) and asymmetric groups (e.g., open-domain QA), as defined in ?2.3 (see Table <ref type="table" target="#tab_9">?6</ref> in the appendix for details about the symmetric and asymmetric groups). We then train INSTRUCTOR with or without instructions on each group separately.</p><p>As shown in Fig. <ref type="figure" target="#fig_3">3</ref>, INSTRUCTOR-Large finetuned without instructions yields performance similar to or better than the original GTR model (dotted line), if the data are symmetric or asymmetric only. However, INSTRUCTOR suffers if finetuned without task instructions on the combination of both types of data (entire MEDI). In contrast, finetuning with instructions enables the model to benefit from the combination of symmetric and asymmetric data (see that the rightmost bar gets additive performance gains from the asymmetric and symmetric tasks). This result demonstrates the importance of instruction finetuning when diverse data are used for embedding training. Note that training on symmetric tasks only without instructions is similar to Sent-T5. Similarly, training on asymmetric tasks only without instructions is similar to GTR, which is also trained on asymmetric opendomain QA datasets. Departing from these prior methods, instruction-based finetuning enables diverse training on both types.  Previous work <ref type="bibr" target="#b58">(Sanh et al., 2022;</ref><ref type="bibr">Zhou et al., 2022)</ref> shows that instruction-finetuned language models are not robust to paraphrased instructions. Here we measure INSTRUCTOR's robustness to variances in human-written instructions. Specifically, we write five paraphrased instructions for all evaluation datasets (Table <ref type="table">4</ref> in Appendix) and measure INSTRUCTOR's performance gap between the best-performing and the worst-performing instructions. Fig. <ref type="figure" target="#fig_4">4</ref> shows that INSTRUCTOR (w/ super-NI) is robust to the instruction paraphrase, with a negligible performance difference between instruction variants across benchmarks. We also find that the inclusion of 300 super-NI datasets is critical to INSTRUCTOR's robustness. Removing these datasets from training (w/o super-NI) substantially increases the performance gap between the best-and worst-performing instructions, sug-gesting that super-NI's diverse instructions help the model handle different formats and styles.  Here we further analyze the role of instructions over varying degrees of their complexity. Specifically, we consider four levels of instruction complexity: N/A (no instructions), dataset tags, simple instructions, and detailed instructions (the original instruction format, ?2.3). In the dataset tag setup, each example is prepended with its dataset name. For instance, on the Natural Questions dataset, the query is formatted as "Natural Questions; Input: who sings the song Love Story"). In the simple instruction setup, we use one or two words to describe the domain (e.g., for Natural Questions, the input query is Wikipedia Questions; Input: who sings the song Love Story). Fig. <ref type="figure" target="#fig_6">5</ref> shows their average performances across all task categories. Even with trivial dataset tags, INSTRUCTOR outperforms the original GTR model, illustrating the effectiveness of instructions for diverse training. As more information is provided in the instruction (from tag to simple and from simple to detail), we observe consistent improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Instruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Complexity of Instructions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Sizes and Instruction Finetuning</head><p>Fig. <ref type="figure" target="#fig_7">6</ref> studies the influence of model sizes. Specifically, we use GTR-Base (0.1B), GTR-Large (0.3B), and GTR-XL (1.5B). They are pretrained on the same corpus and differ only in the encoder size (the embedding sizes are the same). We compare models of various sizes and report the average performance across all the categories. As the encoder transformer model scales up, the performance continues to increase for both GTR and INSTRUCTOR. Nonetheless, the improvement in INSTRUCTOR is more pronounced, perhaps because embeddings with instructions benefit from larger capacities. This implies that large models are more generalizable to compute texts in various domains and task types, providing embeddings for general purposes. Further scale-ups are left to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Instructions Mitigate Domain Shifts</head><p>One advantage of instruction-based finetuning is that it improves models' ability to generalize to unseen domains and tasks. To demonstrate this effectiveness, we found three unseen domains that INSTRUCTOR was not trained on: geography, biology, and civil comments. As shown in Table <ref type="table">3</ref>, INSTRUCTOR largely improves (above the average improvement) GTR-Large's performance on all three domains, indicating that instructions can help more when applying models to unseen or uncommon domains.  When embedded with the instruction, the distance between the green dot pair becomes farther.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualititive analysis</head><p>In this qualitative analysis, we use T-SNE (van der <ref type="bibr">Maaten and Hinton, 2008)</ref> to visualize two example of classification with and without instructions. The desired outcome is, for pairs with the same sentiment to be closer together, and pairs with different sentiment to be farther apart. As shown in Fig. <ref type="figure" target="#fig_8">7</ref>, without instructions, the green dot pairs (different sentiment) are closer together in the embedding space, while the red dot pairs (same sentiment) are farther apart. However, with instructions, our method (INSTRUCTOR) successfully encodes the red dot pairs into close embeddings and correctly classifies the pairs. The distance between the green dot pairs with different sentiment is also larger in the embedding space with instructions..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Text Embeddings Text embeddings are useful in many applications such as information retrieval <ref type="bibr" target="#b63">(Thakur et al., 2021</ref>), text similarity <ref type="bibr" target="#b17">(Gao et al., 2021)</ref>, prompt retrieval for in-context learning <ref type="bibr" target="#b62">(Su et al., 2022)</ref>, classification <ref type="bibr" target="#b55">(Reimers and Gurevych, 2019)</ref>, and beyond. Much prior work develops different embedding models for different applications. For example, SBERT <ref type="bibr" target="#b55">(Reimers and Gurevych, 2019)</ref> and SimCSE <ref type="bibr" target="#b17">(Gao et al., 2021)</ref> are applied solely to text similarity and classification tasks, while DPR <ref type="bibr" target="#b27">(Karpukhin et al., 2020)</ref> and Contriever <ref type="bibr" target="#b24">(Izacard et al., 2022)</ref> focus on information retrieval. Different from Sentence-T5 trained only on symmetric data or GTR trained only on asymmetric data, we combine both groups of datasets and build MEDI, which is then used to train INSTRUCTOR with instructions. <ref type="bibr" target="#b47">Muennighoff et al. (2022)</ref> introduced the massive text embedding benchmark, which can be used to evaluate embedding models on a variety of embedding tasks, spanning reranking, classification, information retrieval, bitext mining, pair classification, STS, and summarization. Their benchmark shows that models performing well on one task may not perform well on other tasks. The poor zero-shot transfer abilities of existing embedding models make it difficult to use them in applications where only few labeled data are available. This motivates us to develop a single embedding model that is applicable to a variety of tasks and has better generalization to unseen tasks.</p><p>Instruction Finetuning Recent work demonstrated that instruction-finetuned language models could perform new tasks given a natural language instruction <ref type="bibr">(Zhong et al., 2021;</ref><ref type="bibr" target="#b45">Min et al., 2022;</ref><ref type="bibr" target="#b58">Sanh et al., 2022;</ref><ref type="bibr" target="#b67">Wei et al., 2022;</ref><ref type="bibr" target="#b66">Wang et al., 2022;</ref><ref type="bibr" target="#b51">Ouyang et al., 2022)</ref>. Nonetheless, instruction finetuning has yet to be studied in the context of broadly-applicable embeddings. In this work, we explore finetuning embedding models to follow human instructions where the instruction specifies eventual use cases. Concurrent work demonstrated that instructions could facilitate information retrieval <ref type="bibr" target="#b1">(Asai et al., 2022)</ref>, which is related to our INSTRUCTOR design. They used instructions to build a task-aware retrieval system and conducted evaluations on the retrieval task; we build a generalpurpose embedding model with instructions that can be applied to 8 tasks categories (Fig. <ref type="figure" target="#fig_1">2</ref>), including retrieval, text similarity, clustering, and text evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced INSTRUCTOR, a single model that creates broadly-applicable text embeddings using natural language instructions. We constructed MEDI, a collection of diverse datasets, to finetune INSTRUCTOR with instructions. Our extensive experiments showed that INSTRUCTOR achieves state-of-the-art performance on text embedding benchmarks, as well as prompt retrieval for fewshot in-context learning. We hope that researchers and practitioners will benefit from our embeddings or our datasets for tasks of their interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Full instructions</head><p>We list all instructions for each dataset in MEDI in     <ref type="bibr" target="#b30">(Khashabi et al., 2021)</ref> Asymmetric 25,000 yahoo_answers_title_answer <ref type="bibr">(Zhang et al., 2015)</ref> Asymmetric 25,000 stackexchange <ref type="bibr" target="#b61">(Silva et al., 2018)</ref> Symmetric 25,000 eli5_question_answer <ref type="bibr" target="#b14">(Fan et al., 2019)</ref> Asymmetric 25,000 squad_pairs <ref type="bibr" target="#b54">(Rajpurkar et al., 2016)</ref> Asymmetric 25,000 NQ * <ref type="bibr" target="#b34">(Kwiatkowski et al., 2019)</ref> Asymmetric 50,000 amazon-qa <ref type="bibr" target="#b18">(Gupta et al., 2019)</ref> Asymmetric 100,000 WikiAnswers <ref type="bibr" target="#b13">(Fader et al., 2014)</ref> Symmetric 25,000 agnews <ref type="bibr">(Zhang et al., 2015)</ref> Asymmetric 45,000 AllNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref> Symmetric 50,000 npr <ref type="bibr" target="#b37">(Li, 2020)</ref> Asymmetric 25,000 specter_train_triples <ref type="bibr" target="#b6">(Cohan et al., 2020)</ref> Symmetric 50,000 ccnews_title_text <ref type="bibr" target="#b20">(Hamborg et al., 2017)</ref> Asymmetric 25,000 triviaqa <ref type="bibr" target="#b26">(Joshi et al., 2017)</ref> Asymmetric 50,000 zero_shot_re <ref type="bibr" target="#b35">(Levy et al., 2017)</ref> Asymmetric 15,000 flickr30k_captions <ref type="bibr">(Young et al., 2014)</ref> Symmetric 25,000 xsum <ref type="bibr" target="#b48">(Narayan et al., 2018)</ref> Asymmetric 10,000 code_search <ref type="bibr">(Husain et al.,</ref>  Asymmetric 100,000 S2ORC_title_abstract <ref type="bibr" target="#b19">(Lo et al., 2020)</ref> Asymmetric 100,000 PAQ_pairs <ref type="bibr" target="#b36">(Lewis et al., 2021)</ref> Asymmetric 25,000 wow <ref type="bibr" target="#b10">(Dinan et al., 2019)</ref> Asymmetric 30,000 trex <ref type="bibr" target="#b11">(Elsahar et al., 2018)</ref> Asymmetric 30,000 pubmed <ref type="bibr" target="#b60">(Sen et al., 2008)</ref> Asymmetric 30,000 medmcqa <ref type="bibr" target="#b52">(Pal et al., 2022)</ref> Asymmetric 30,000 wikihow <ref type="bibr" target="#b33">(Koupaee and Wang, 2018)</ref> Asymmetric 5,000 simple_wiki <ref type="bibr" target="#b9">(Coster and Kauchak, 2011)</ref> Asymmetric 5,000 Super-NI (300 datasets) <ref type="bibr" target="#b66">(Wang et al., 2022)</ref> Symmetric 180,000 </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: At execution time, INSTRUCTOR generates embeddings based on both the text input and the task instruction. The same input (e.g., Who sings the song "Love Story"?) will be encoded into different embeddings, depending on the end task (e.g., duplicate question detection, information retrieval, and topic classification).</figDesc><graphic url="image-1.png" coords="1,312.82,212.60,204.92,190.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: INSTRUCTOR training and evaluation pipeline. INSTRUCTOR is a single embedding model that takes not only text inputs but also task instructions, thereby creating task-and domain-aware embeddings. It is trained on a multitask mixture of 330 diverse datasets with human-written task instructions (MEDI dataset, ?2.3). After training on MEDI (left), INSTRUCTOR is evaluated on a variety of 70 embedding datasets (64 of which are not seen during training), spanning various downstream applications (right). INSTRUCTOR outperforms the prior best model by an average of 3.4% over the 70 diverse datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Setups Minibatch Sampling Training is performed on a combination of all training datasets in MEDI. Since the number of examples in each dataset is different in orders of magnitude, we downsample large ones. Details for the downsampled numbers of examples on each dataset are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average (by category) performance of IN-STRUCTOR-Large (with and without instructions) over three types of training data: symmetric data, asymmetric data, or both (entire MEDI). The model finetuned with instructions on both data is the original INSTRUC-TOR model. The diverse training data with both types hurt the performance when finetuned without instructions but improve when instructions are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison of the model performance across five paraphrased instructions. W/o super-NI (w/ super-NI) refers to the inclusion (exclusion) of the 300 datasets from Super-NaturalInstructions in MEDI.These diverse datasets with task instructions improve the robustness of INSTRUCTOR to instruction paraphrases (i.e., smaller performance gaps between bestand worst-performing instructions).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Average performance over varying degrees of instruction details. As the instructions become more detailed, the performance improves. N/A: no instructions are given; tag: dataset names are prepended; simple: one word or two for the task domain are given (e.g., Wikipedia); and detailed: our proposed instructions ( ?2.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average performance comparisons with varying sizes of models. INSTRUCTOR benefits more from scaling up, perhaps because instructions require additional computations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: T-SNE visualization of pair classification examples without (dot) and with instruction (dot with a solid border). The red dot pairs that have the same sentiment should be closer together, while the green dot pairs with different sentiment should be farther apart.When embedded with the instruction, the distance between the green dot pair becomes farther.</figDesc><graphic url="image-10.png" coords="10,88.02,98.65,183.96,133.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Performance of model tuned with instructions of various complexity degrees in Billboard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Performance of model tuned with instructions of various complexity degrees in Prompt retrieval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Query instruction: Represent the Wikipedia question for retrieving supporting documents; Input:, Doc instruction: Represent the Wikipedia document for retrieval; Input: Instruction examples for evaluation datasets. Our embedding evaluation includes 70 diverse datasets in 9 different downstream applications, ranging from classification and semantic textual similarity to information retrieval and text generation evaluation. The first two tasks are asymmetric and require two distinct instructions. Instructions for the MEDI training data can be found in Tables</figDesc><table><row><cell cols="2">Task type # of Datasets</cell><cell>Task</cell><cell>Instruction</cell></row><row><cell>Retrieval</cell><cell>15</cell><cell>Natural Question (BEIR)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Query instruction: Represent the News query for retrieving</cell></row><row><cell>Reranking</cell><cell>4</cell><cell>MindSmallReranking</cell><cell>articles; Input: Doc instruction: Represent the News article for</cell></row><row><cell></cell><cell></cell><cell></cell><cell>retrieval; Input:</cell></row><row><cell>Clustering</cell><cell>11</cell><cell>MedrxivClusteringS2S</cell><cell>Represent the Medicine statement for retrieval; Input:</cell></row><row><cell>Pair Clas-sification</cell><cell>3</cell><cell>TwitterSemEval2015</cell><cell>Represent the Tweet post for retrieving duplicate comments; Input:</cell></row><row><cell>Classification</cell><cell>12</cell><cell>ImdbClassification</cell><cell>Represent the Review sentence for classifying emotion as posi-tive or negative; Input:</cell></row><row><cell>STS</cell><cell>10</cell><cell>STS12</cell><cell>Represent the statement; Input:</cell></row><row><cell>Summarization</cell><cell>1</cell><cell>SummEval</cell><cell>Represent the Biomedical summary for retrieving duplicate sum-maries; Input:</cell></row><row><cell>Text Eval-uation</cell><cell>3</cell><cell>Mscoco</cell><cell>Represent the caption for retrieving duplicate captions; Input:</cell></row><row><cell>Prompt Retrieval</cell><cell>11</cell><cell>GeoQuery</cell><cell>Represent the Geography example for retrieving duplicate exam-ples; Input:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc><ref type="bibr" target="#b62">Su et al. (2022)</ref> introduce the prompt retrieval task, where the goal is to retrieve a few in-context learning (i.e., demonstration) examples from annotated examples given a test instance. The embedding model is used to encode all annotated examples and to find the few most similar examples to the test instance based on the cosine similarity. Following Su et al. (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the massive text embedding benchmark (MTEB;<ref type="bibr" target="#b47">Muennighoff et al., 2022</ref>), Billboard (Kasai et al., 2022a), and prompt retrieval<ref type="bibr" target="#b62">(Su et al., 2022)</ref>. The last column averages performance scores over 9 categories (7 from MTEB, 1 from Billboard, and 1 from prompt retrieval). Out of the 70 evaluation datasets, 64 (50 from MTEB, 3 from BillBoard, and 11 from prompt retrieval) are unseen tasks during finetuning. Retri.</figDesc><table><row><cell>Benchmark</cell><cell></cell><cell></cell><cell></cell><cell>MTEB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Billboard Prompt Avg.</cell></row><row><cell>Task category</cell><cell cols="9">Retri. Rerank Cluster Pair. Class. STS Sum. Avg. Text Eval.</cell><cell>Retri.</cell><cell></cell></row><row><cell># datasets</cell><cell>15</cell><cell>4</cell><cell>11</cell><cell>3</cell><cell>12</cell><cell>10</cell><cell>1</cell><cell>56</cell><cell>3</cell><cell>11</cell><cell>70</cell></row><row><cell>Small Models (&lt;500M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCSE (110M)</cell><cell>21.9</cell><cell>47.5</cell><cell>33.4</cell><cell>73.7</cell><cell>67.3</cell><cell cols="3">79.1 23.3 48.7</cell><cell>29.4</cell><cell>58.3</cell><cell>48.2</cell></row><row><cell>coCondenser (110M)</cell><cell>33.0</cell><cell>51.8</cell><cell>37.6</cell><cell>81.7</cell><cell>64.7</cell><cell cols="3">76.5 29.5 52.4</cell><cell>31.5</cell><cell>59.6</cell><cell>51.8</cell></row><row><cell>Contriever (110M)</cell><cell>41.9</cell><cell>53.1</cell><cell>41.1</cell><cell>82.5</cell><cell>66.7</cell><cell cols="3">76.5 30.4 56.0</cell><cell>29.0</cell><cell>57.3</cell><cell>53.2</cell></row><row><cell>GTR-Large (335M)</cell><cell>47.4</cell><cell>55.4</cell><cell>41.6</cell><cell>85.3</cell><cell>67.1</cell><cell cols="3">78.2 29.5 58.3</cell><cell>31.2</cell><cell>59.8</cell><cell>55.1</cell></row><row><cell>INSTRUCTOR (335M)</cell><cell>48.0</cell><cell>57.0</cell><cell>44.6</cell><cell>85.9</cell><cell>72.0</cell><cell cols="3">82.7 31.3 61.0</cell><cell>38.7</cell><cell>65.2</cell><cell>58.4</cell></row><row><cell>Relative gain (%)</cell><cell>+1.3</cell><cell>+2.9</cell><cell>+7.2</cell><cell cols="5">+0.7 +7.3 +5.8 +6.1 +4.6</cell><cell>+24.0</cell><cell>+9.0</cell><cell>+5.9</cell></row><row><cell>Large Models (?500M)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GTR-XL (1.5B)</cell><cell>48.0</cell><cell>56.0</cell><cell>41.5</cell><cell>86.1</cell><cell>67.1</cell><cell cols="3">77.8 30.2 58.4</cell><cell>32.0</cell><cell>60.4</cell><cell>55.5</cell></row><row><cell>Sent-T5-XXL (4.8B)</cell><cell>42.2</cell><cell>56.4</cell><cell>43.7</cell><cell>85.1</cell><cell>73.4</cell><cell cols="3">82.6 30.1 59.5</cell><cell>33.9</cell><cell>61.5</cell><cell>56.5</cell></row><row><cell>GTR-XXL (4.8B)</cell><cell>48.1</cell><cell>56.7</cell><cell>42.4</cell><cell>86.1</cell><cell>67.4</cell><cell cols="3">78.4 30.6 58.9</cell><cell>32.0</cell><cell>60.8</cell><cell>55.8</cell></row><row><cell>SGPT-NLI (5.8B)</cell><cell>32.3</cell><cell>52.3</cell><cell>37.0</cell><cell>77.0</cell><cell>70.1</cell><cell cols="3">80.5 30.4 53.7</cell><cell>29.6</cell><cell>57.9</cell><cell>51.9</cell></row><row><cell cols="2">INSTRUCTOR-XL (1.5B) 48.9</cell><cell>57.8</cell><cell>45.0</cell><cell>85.6</cell><cell>72.4</cell><cell cols="3">82.5 30.4 61.5</cell><cell>40.7</cell><cell>65.9</cell><cell>58.8</cell></row><row><cell>Relative gain (%)</cell><cell>+1.9</cell><cell>+3.2</cell><cell>+8.4</cell><cell>-0.6</cell><cell cols="4">+7.9 +6.0 +0.7 +5.3</cell><cell>+27.2</cell><cell>+9.1</cell><cell>+5.9</cell></row></table><note><p>, Pair., Class., Sum., Text Eval. refer to retrieval, pair classification, classification, summarization, and text evaluation, respectively. Compared to GTR(335M/1.5B), from which INSTRUCTOR (335M/1.5B) is initialized, instruction finetuninig enhances the performance by 5.9%. Compared to the state-of-the-art model (Sent-T5-XXL), INSTRUCTOR (335M/1.5B) achieves 3.4% and 4.1% performance gains respectively. The relative gain (%) shows INSTRUCTOR's relative gain over the original GTR model of the same size.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 and</head><label>7</label><figDesc>Table 8</figDesc><table><row><cell>B Full Results</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Examples of text types, objectives, and domains.</figDesc><table><row><cell>Dataset</cell><cell>Symmetric/Asymmetric Number</cell></row><row><cell>gooaq_pairs</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Number of training instances in each dataset. The dataset with * indicates that its test-split is included in the evaluation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We do not include instruction for Sentence-T5 as it is not fine-tuned with instructions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/datasets/ sentence-transformers/embedding-training-data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>All prompts are reviewed by multiple authors independently to make sure they consistently follow our template.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/sentence-transformers/ gtr-t5-large.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank <rs type="person">Akari Asai</rs>, <rs type="person">Jack Lin</rs>, <rs type="person">Minghan Li</rs>, and the <rs type="institution">ARK group at UW</rs> for their helpful feedback on this work.</p></div>
			</div>			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2012 task 6: A pilot on semantic textual similarity</title>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Gonzalez-Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval</title>
		<meeting>of Se-mEval</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<title level="m">Task-aware retrieval with instructions</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName><forename type="first">Payal</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhaskar</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoCo</title>
		<meeting>of CoCo</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toshiaki bnghvtcf Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20)</title>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Biesialska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ond?ej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Costa-Juss?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Joanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Kiu</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Ljube?i?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of WMT</title>
		<meeting>of WMT</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Se-mEval</title>
		<meeting>of Se-mEval</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SPECTER: Document-level representation learning using citation-informed transformers</title>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SentEval: An evaluation toolkit for universal sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lo?c</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simple english Wikipedia: a new text simplification task</title>
		<author>
			<persName><forename type="first">William</forename><surname>Coster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kauchak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Wizard of Wikipedia: Knowledge-powered conversational agents</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">T-rex: A large scale alignment of natural language with knowledge base triples</title>
		<author>
			<persName><forename type="first">Hady</forename><surname>Elsahar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Vougiouklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arslen</forename><surname>Remaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Gravier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederique</forename><surname>Laforest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SummEval: Re-evaluating summarization evaluation</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Alexander R Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Kry?ci?ski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Radev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open question answering over curated and extracted knowledge bases</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<idno type="DOI">10.1145/2623330.2623677</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD</title>
		<meeting>of KDD</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ELI5: long form question answering</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Experts, errors, and context: A large-scale study of human evaluation for machine translation</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Freitag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viresh</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qijun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised corpus aware language model pre-training for dense passage retrieval</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SimCSE: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">AmazonQA: A review-based question answering task</title>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghuveer</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudha</forename><surname>Rayasam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCAI</title>
		<meeting>of IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hamborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><surname>Meuschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Breitinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bela</forename><surname>Gipp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ISI. Ruining He and Julian McAuley</title>
		<meeting>of ISI. Ruining He and Julian McAuley</meeting>
		<imprint>
			<date type="published" when="2016">2017. 2016</date>
		</imprint>
	</monogr>
	<note>Proc. of WWW</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CLIPScore: A reference-free evaluation metric for image captioning</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CQADupStack: A benchmark data set for community question-answering research</title>
		<author>
			<persName><forename type="first">Doris</forename><surname>Hoogeveen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><forename type="middle">M</forename><surname>Verspoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2838931.2838934</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of ADCS</title>
		<meeting>of ADCS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Code-SearchNet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised dense information retrieval with contrastive learning</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>TMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging passage retrieval with generative models for open domain question answering</title>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EACL</title>
		<meeting>of EACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">2022a. Bidimensional leaderboards: Generate and evaluate language hand in hand</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lavinia</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Dunagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">R</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">2022b. Transparent human evaluation for image captioning</title>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lavinia</forename><surname>Dunagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GooAQ: Open question answering with diverse answer types</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the ACL: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GENIE: Toward reproducible and standardized human evaluation for text generation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Bragg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Russ R Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Mahnaz</forename><surname>Koupaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<title level="m">Wik-iHow: A large scale text summarization dataset</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Zero-shot relation extraction via reading comprehension</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CoNLL</title>
		<meeting>of CoNLL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">PAQ: 65 million probably-asked questions and what you can do with them</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>TACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">INTERVIEW: NPR media dialog transcripts</title>
		<author>
			<persName><forename type="first">Shuyang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">B</forename><surname>Miles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<title level="m">Semantic matching against a corpus: New methods and applications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lubomir</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECCV</title>
		<meeting>of ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">What makes good in-context examples for GPT-3?</title>
		<author>
			<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of DeeLIO</title>
		<meeting>of DeeLIO</meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An efficient framework for learning sentence representations</title>
		<author>
			<persName><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A SICK cure for the evaluation of compositional distributional semantic models</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Zamparelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LREC</title>
		<meeting>of LREC</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MetaICL: Learning to learn in context</title>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning to match using local and distributed representations of text for web search</title>
		<author>
			<persName><forename type="first">Mitra</forename><surname>Bhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<idno type="DOI">10.1145/3038912.3052579</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of WWW. Niklas Muennighoff</title>
		<meeting>of WWW. Niklas Muennighoff</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2022</date>
		</imprint>
	</monogr>
	<note>SGPT: GPT sentence embeddings for semantic search</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">MTEB: Massive text embedding benchmark</title>
		<author>
			<persName><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nouamane</forename><surname>Tazi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Loic Magne, and Nils Reimers</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Don&apos;t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization</title>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sentence-T5: Scalable sentence encoders from pre-trained text-to-text models</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the ACL: ACL 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Large dual encoders are generalizable retrievers</title>
		<author>
			<persName><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Hern?ndez ?brego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">B</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Training language models to follow instructions with human feedback</title>
		<author>
			<persName><forename type="first">Long</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diogo</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carroll</forename><forename type="middle">L</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katarina</forename><surname>Slama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MedMCQA: A large-scale multi-subject multi-choice dataset for medical domain question answering</title>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logesh</forename><surname>Kumar Umapathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malaikannan</forename><surname>Sankarasubbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of CHIL</title>
		<meeting>of CHIL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">KILT: a benchmark for knowledge intensive language tasks</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Majid</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using Siamese BERTnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vmeasure: A conditional entropy-based external cluster evaluation measure</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hirschberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning to retrieve prompts for in-context learning</title>
		<author>
			<persName><forename type="first">Ohad</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multitask prompted training enables zero-shot task generalization</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Webson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lintang</forename><surname>Sutawika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Alyafeai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnaud</forename><surname>Stiegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manan</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saiful Bari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urmish</forename><surname>Thakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanya</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Szczechla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taewoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunjan</forename><surname>Chhablani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nihal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debajyoti</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><surname>Tian-Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Manica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng Xin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<editor>
			<persName><forename type="first">Trishala</forename><surname>Neeraj</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jos</forename><surname>Rozen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abheesht</forename><surname>Sharma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Santilli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jason</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alan</forename><surname>Fries</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Teehan</surname></persName>
		</editor>
		<meeting>of ICLR<address><addrLine>Stella Rose Biderman, Leo Gao, Tali Bers, Thomas Wolf</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>AI magazine</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Duplicate question detection in stack overflow: A reproducibility study</title>
		<author>
			<persName><forename type="first">Rodrigo Fg</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kl?risson</forename><surname>Paix?o</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Almeida</forename><surname>Maia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SANER</title>
		<meeting>of SANER</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Selective annotation makes language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">BEIR: A heterogeneous benchmark for zero-shot evaluation of information retrieval models</title>
		<author>
			<persName><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NeurIPS</title>
		<meeting>of NeurIPS</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Abhishek Srivastava, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fever: a large-scale dataset for fact extraction and verification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE</title>
		<meeting>of NAACL. Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE</meeting>
		<imprint>
			<publisher>JMLR</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks</title>
		<author>
			<persName><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swaroop</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Alipoormolabashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amirreza</forename><surname>Mirzaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjana</forename><surname>Arunkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Ashok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arut</forename><surname>Selvan Dhanasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atharva</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Stap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshaan</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Karamanolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Haizhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishani</forename><surname>Purohit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Mondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kirby</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krima</forename><surname>Kuznia</surname></persName>
		</author>
		<author>
			<persName><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<editor>
			<persName><forename type="first">Maitreya</forename><surname>Patel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kuntal</forename><surname>Kumar Pal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mehrad</forename><surname>Moradshahi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mihir</forename><surname>Parmar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mirali</forename><surname>Purohit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Neeraj</forename><surname>Varshney</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rohitha</forename><surname>Phani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pulkit</forename><surname>Kaza</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ravsehaj</forename><surname>Verma</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rushang</forename><surname>Singh Puri</surname></persName>
		</editor>
		<editor>
			<persName><surname>Karia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Keyur</forename><surname>Shailaja</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Savan</forename><surname>Sampat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Siddhartha</forename><surname>Doshi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sujan</forename><surname>Mishra</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sumanta</forename><surname>Reddy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Tanay</forename><surname>Patro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xudong</forename><surname>Dixit</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chitta</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Yejin</forename><surname>Baral</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hannaneh</forename><surname>Choi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Hajishirzi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Smith</surname></persName>
		</editor>
		<editor>
			<persName><surname>Khashabi</surname></persName>
		</editor>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Finetuned language models are zero-shot learners</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR</title>
		<meeting>of ICLR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
