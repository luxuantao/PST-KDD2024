<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The ParaScope Parallel Programming Environment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Keith</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>7725</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MEMBER, IEEE</roleName><forename type="first">Mary</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Center for Integrated Systems</orgName>
								<address>
									<addrLine>Stanford Uni-versity</addrLine>
									<postCode>94305</postCode>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MEMBER, IEEE, KEN MEMBER, IEEE</roleName><forename type="first">Robert</forename><forename type="middle">T</forename><surname>Hood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Kubota Pacific Computer, Inc</orgName>
								<address>
									<addrLine>2630 Walsh Avenue</addrLine>
									<postCode>9505 1-0905</postCode>
									<settlement>Santa Clara</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>AND</roleName><forename type="first">Linda</forename><surname>Torczon</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>7725</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>MEMBER, IEEE</roleName><forename type="first">Scott</forename><forename type="middle">K</forename><surname>Warren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Rosetta, Inc</orgName>
								<address>
									<postCode>2502, 77005</postCode>
									<settlement>Robinhood, Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>SENIOR MEMBER, IEEE</roleName><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Kennedy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>7725</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">John</forename><forename type="middle">M</forename><surname>M~kinley</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Mellor-Crummey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>7725</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>7725</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">The ParaScope Parallel Programming Environment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D19084630F9A78F706A008940C771B00</idno>
					<note type="submission">received March 6, 1992; revised June 28, 1992.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ParaScope parallel programming environment* developed to support scientijic programming of shared-memop multiprocessors, includes a collection of tools that use global program analysis to help users develop and debug parallel programs. This paper focuses on ParaScope 's compilation system, its parallel program editor, and its parallel debugging system. The compilation system extends the traditional single-procedure compiler by providing a mechanism for managing the compilation of complete programs. Thus, ParaScope can support both traditional single-procedure optimization and optimization across procedure boundaries. The ParaScope editor brings both compiler analysis and user expertise to bear on program parallelization. It assists the knowledgeable user by displaying and managing analysis and by proiiding a variety of interactive program tran.formation.s that are effective in exposing parallelism. The debugging svstem detects and reports timing-dependent errors, called data races, in execution of parallel programs. The system combines static analysis. program instrumentation. and run-time reporting to provide a mechanical system for isolating errors in parallel program executions. Finally, we describe a new project to extend ParaScope to support programming in Fortran D, a machine-independent parallel programming language intended for use with both distributed-memory and shared-memo ry parallel computers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Computation is assuming a key role in the practice of science and engineering. Mathematical models running on today's most powerful computers are replacing physical experiments. Wind tunnels have yielded to aerodynamic simulation. Automotive design relies on computer graphics rather than on clay prototypes. Computer-based models are at the heart of current work in climate modeling, oil recovery, gene sequencing, and bioremediation.</p><p>In large part, this computational revolution is the result of the dramatic increases in available computer power over the last 15 years. Increased computing power helps in two ways. First, it makes new problems amenable to computer modeling. Second, it enables better solutions to existing problems. For example, two-dimensional models can be replaced with three-dimensional versions. Between these two trends, the demand for ever faster computers to support scientific applications will continue unabated into the next century.</p><p>The technology used to fabricate the processing elements used in high-performance machines is approaching physical device limits. To get significant improvements in computing speed, system designers are turning to parallelism. Already, the most powerful scientific computers, such as the Thinking Machines CM-5 and the Intel Paragon XP/S, rely on aggregating together many hundreds or thousands of processors. At the same time, parallelism is appearing on the desktop. Several companies have introduced multiprocessor UNIX workstations; more will appear. Within several years, we expect to see small-scale multiprocessors on a single chip, perhaps four processors and a shared cache.</p><p>Parallel machines have been around for more than two decades. In the last 10 years, commercial multiprocessors have become common. Yet, surprisingly few applications have been implemented on these machines. Why? Because parallel machines are difficult to program. As with a classical sequential computer, the programmer must specify precisely how to perform the task. Additionally, the programmer must consider the impact of concurrent execution on the program's correctness. Finally, the efficiency of the resulting program depends on many complex and machinespecific facts.</p><p>If parallel machines are to be successful, we must find better ways to program them. Ideally, no additional programming would be needed+ompilers would automatically translate programs written in a conventional language such as Fortran 77 so that they could execute efficiently on parallel machines. In pursuit of this goal, several research projects and prototypes have been built to investigate automatic parallelization. These include Parafrase at the University of Illinois [l], PFC at Rice University <ref type="bibr">[2]</ref>, <ref type="bibr">PTRAN at IBM Research [3]</ref>, and SUIF at Stanford University 141. In addition, several commercial systems, such as Vast by Pacific Sierra, KAP by Kuck and Associates, and compilers by Alliant, Convex, and Cray, perform automatic parallelization. While these systems achieve impressive results for some programs, they fail to find parallelism that exists in others.</p><p>Parallelizing compilers fail because it must take a conservative approach. If, for some loop nest, a compiler cannot prove that parallel execution produces the same results as sequential execution, it must generate the sequential version. Often, the compiler is unable to prove the safety of parallel execution because it cannot determine the values of variables used in subscript expressions. For example, the variables might be passed in as parameters or read in as data. Frequently, the programmer knows the values of these variables, either exactly or approximately. Thus, the compiler often fails to find parallelism in loops that the programmer knows to be parallelizable. For this reason, it seems unlikely that automatic techniques by themselves will provide a' comprehensive solution to the parallelization problem. It will be necessary for programmers to be involved in the specification of parallelism.</p><p>Given the need for programmer involvement, we need a language for explicit specification of parallelism. Usually, this takes the form of extensions to standard Fortran. For shared-memory parallel computers, a typical extension permits the programmer to specify one or more loops that should execute in parallel without synchronization between iterations. Parallel Computer Forum (PCF) Fortran is an example of this kind of extension [ 5 ] . It lets the programmer asserts that a loop is to run in parallel, even if the compiler's analysis does not support this decision.</p><p>Unfortunately, programmer-specified parallelism can be incorrect. The programmer can specify parallel execution for a loop that does not have independent iterations. For example, different iterations of the loop can share a variable, with at least one iteration writing to the variable. The result of executing this loop depends on the run-time scheduling of the iterations. With one schedule, it may get the same results as sequential execution; another schedule might produce different values. Such schedule-dependent problems, or data races, are extremely difficult to find. This phenomenon makes debugging parallel programs hard.</p><p>Because it is easy to make mistakes in parallel programs and hard to find them, programmers will need substantial help to write and maintain correct parallel programs. One way to provide this help is to make the deep program analysis of parallelizing compilers available to program-mers in an understandable form. With such information, the programmer can concentrate on those program areas that the compiler left sequential but the programmer knows are parallel. The tool can provide precise information about the facts that prevented it from generating parallel code. Automatic vectorization systems rely on this idea. They provide the user with comments about which loops cannot be vectorized and suggestions on how to overcome the problems. This frees users from analyzing every detail and allows them to focus on the specific problems beyond the compiler's abilities.</p><p>Taken together, these forces are creating a marketplace for parallel applications and the tools to create them. The ParaScope programming environment is designed to present the results of deep compiler analysis to the programmer. It provides three kinds of assistance. It helps determine whether a parallelization strategy preserves the meaning of a program. It helps correctly carry out a parallelization strategy. It helps find errors introduced during parallelization. These functions are embedded in the three major components of the ParaScope environment.</p><p>The ParaScope compilation system is designed to permit compiler analysis of whole programs rather than single modules. It provides more precise information about program variables and supports crossprocedural optimizations without sacrificing all of the benefits of separate compilation.</p><p>The ParaScope editor is an intelligent Fortran editor designed to help users interactively parallelize programs. It presents a user with the results of sophisticated compiler analysis and offers a variety of useful correctness-preserving transformations. Since it permits the user to consider alternative parallelization strategies and override the results of analysis, it can be thought of as a tool for exploratory parallel programming. The ParaScope debugging system supports a mechanical method for locating all data races in a program with respect to a given data set. It can be used for programs written in a useful subset of PCF Fortran. Although several research and production systems provide some features found in the ParaScope environment [6]-[8], ParaScope is unique for its integration of these functions into a unified system.</p><p>The next three sections of this paper provide an overview of the issues, problems, and methods that have arisen during the implementation of the three ParaScope components described above. The final section describes several directions for further work, much of it aimed at providing adequate support for Fortran D, a language for machine-independent parallel programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE PARASCOPE COMPILATION SYSTEM</head><p>The overall goal of the Parascope project is to support the development and compilation of parallel programs. The system provides an environment in which each tool has access to the results of sophisticated program analysis and transformation techniques. In particular, the tools have been designed for efficient analysis and optimization of whole programs.</p><p>Traditionally, compiler-based analyzers have limited their attention to single procedures. This kind of analysis, called global or intraprocedural analysis, yields substantial improvement on scalar machines. As the architecture of highperformance machines has shifted toward large-scale parallelism, the compiler's task has become more complex. To mitigate the overhead of startup and synchronization of processes on modem parallel machines, the compiler must find large program regions that can be run in parallel. Striving for large granularity parallelism has a natural consequence-the compiler must look for parallelism in regions of the program that span multiple procedures. This kind of analysis is called whole program or interprocedural analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dependence Analysis</head><p>From a compiler's perspective, the key to understanding the behavior of parallel programs lies in understanding the patterns of reference to data. For this reason, the compiler performs dependence analysis to locate the potential data and control dependences within a program [9]- <ref type="bibr">[12]</ref>. A data dependence exists between two memory references if they can access the same location. In the following code fragment, dependence analysis would show that the value stored into a on iteration i is used in both iteration i and iteration i + 2.</p><formula xml:id="formula_0">do i = lb to ub by 1 a[i] = . . . . . . . . . = a[i] + a[i-2]</formula><p>end For scalar variable references, the results of datadependence analysis are equivalent to those produced by classical data-flow analysis techniques. For subscripted variable references, dependence analysis provides much sharper information.</p><p>Control dependences provide a concrete representation for the relationship between control-flow tests in a program and the execution of individual statements. For example, in the following code fragment, statement S I is control dependent on the i f statement, while ~2 is control dependent only on the loop header.</p><p>do i = Ib to ub by</p><formula xml:id="formula_1">1 if b[i] # 0 then c[i] = c[i] / b[i] c[i] = 0.5 * c[i] + c[i+l] SI: ~2 ;</formula><p>end Control dependences are a compact and precise way of representing the relationship between the values of controlflow expressions and the execution of individual statements.</p><p>To provide a concrete representation, compilers build a dependence graph. Nodes in the graph represent individual references in the code. Edges correspond to individual dependences. Edges are directed. Thus, a value produced at the source is used at its sink.</p><p>Dependences play a critical role in the construction, compilation, and debugging of parallel programs. The dependences in a program define a partial order on the execution of its statements; a correct execution of the parallel program must preserve that order. If the endpoints of a dependence appear in concurrently executing threads, then the corresponding memory accesses may occur out of order, resulting in incorrect program execution. Compilers use control and data dependences to prove the safety of transformations that change the control flow in a program.</p><p>To perform dependence analysis, pairs of references are tested to determine if they can access the same memory location. Parascope's dependence analysis applies a hierarchy of tests on each reference [13]. It starts with inexpensive tests that can analyze the most commonly occurring subscript references. More expensive tests are applied when the simpler tests fail to prove or disprove dependence.</p><p>Automatic parallelizers try to transform the program so that the iterations of a loop execute in parallel. Looplevel parallelism is attractive for two reasons. First, in many loops, the amount of work per iteration is approximately fixed; this characteristic produces a roughly balanced workload inside a parallel region. Second, loopbased parallelism is often scalable; that is, as the number of processors increases we can increase the program's problem size proportionately and expect roughly the same processor utilization.</p><p>The compiler uses the computed dependence information to determine if a loop's iterations can safely execute in parallel. A dependence is l o o p c a r r i e d if its endpoints lie in different iterations of the loop. Loop-carried dependences inhibit parallelization of the loop. Dependences that are not loop-carried are termed loop-independent; such dependences would exist without the surrounding loop nest. In the first example, the dependence between a [ i I and a [ i -2 1 is loop carried, while the dependence between the two references to a [ i ] is loop independent.</p><p>Dependence analysis was originally developed to help compilers automatically discover opportunities for vectorization. These early dependence-based compilers were limited to intraprocedural analysis. For vectorization, intraprocedural techniques were usually sufficient; good vectorizing compilers produce substantial improvements by discovering a single parallel loop and making it the innermost loop in a loop nest.</p><p>Unfortunately, profitable MlMD parallelism is harder to find. To offset the increased startup and synchronization costs of parallel execution requires larger and deeper loop nests that contain more computation. This complicates program analysis in several ways.</p><p>I ) The compiler may need to combine the bodies of several procedures to get the critical size required to amortize overhead.</p><p>2) The likelihood that one or more loops in a loop nest contain a procedure call rises with the size of the region being considered. 3) As the region grows larger, so do the chances for discovering one or more dependences that would rule out parallel execution. The answer to each of these problems is the same-the compiler needs analytical techniques that enable it to reason about the behavior of regions larger than a single procedure. A combination of whole program analysis and transformation can supply additional information and context about the program that allows the dependence analyzer to compute sharper information. For example, loop bounds are often passed as parameters, particularly in libraries of numerical code. Knowing the constant value of the loop bound may lead to more precise dependence information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Program Compiler</head><p>The design of the Parascope compilation system has been shaped by the decision to perform whole program analysis and optimization. The compiler has been divided into two distinct components: the program compiler and the module compiler. The program compiler deals with issues that are interprocedural in their scope; in contrast, the module compiler handles the detailed job of tailoring individual procedures to the target machine. Through ParaScope' s central repository, the program compiler has access to a description of the entire program being compiled, to the abstract syntax tree (AST) for any of its constituent procedures, and to annotations associated with either the program or its procedures. Example annotations are the procedure's symbol table, the set of call sites that occur within a procedure, and a representation of the loop nesting structure and its relationship to call sites. More complex annotations include a data-dependence graph for each procedure and information about interprocedural constants.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the relationships between the various components of the parascope compilation system. Cooperation between the tools, coordinated through a central repository where the tools store derived information, leads to a more efficient implementation of interprocedural techniques [ 141. The Parascope editor derives information about individual procedures. The composition editor lets the user specify the individual procedures to be included in a program; it produces information that describes the set of possible procedure invocations. Using this information as input, the program compiler could be used to compute interprocedural data-flow information, decide where to perform interprocedural optimizations (see Section 11-D), and determine which procedures must be recompiled in response to an editing change.</p><p>Recompilation analysis requires two rules: 1) If some procedure p has been changed since its last compilation, it will be recompiled. 2) If the interprocedural environment in which p was last compiled has changed in a way that could invalidate an optimization performed, p will be recompiled. The module compiler need only be invoked to produce optimized object code for modules that must be recompiled.</p><p>The analysis performed by the program compiler produces information that relates the entire program to each of its component procedures. Thus, the code and some of the annotations are a function of the program that provided the context for the compilation. In the data base, programspecific objects must be associated with the program. Only objects and annotations that are independent of any context program are associated with the procedure.</p><p>Building on this framework, we plan to build a prototype source-to-source parallelizer for shared-memory multiprocessors [ 151. Using the information produced by the program compiler and a performance estimator, it will apply a combination of interprocedural transformations and parallelism-enhancing transformations to provide an initial parallel program [ 161-[ 181.</p><p>The next two subsections describe how the program compiler computes interprocedural data-flow information and how it applies interprocedural transformations. Recompilation analysis is handled using methods described by <ref type="bibr">Burke and Torczon [ 191.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Interprocedural Analysis</head><p>Interprocedural data-flow analysis addresses several distinct problems. These include discovering the program's dynamic structure (call graph analysis), summarizing sideeffects of executing a procedure call (summary analysis, regular section side-effect analysis, and kill analysis), understanding the interplay between call-by-reference parameter passing and the mapping of names to memory locations (alias analysis), and discovering when a variable has a value that can be derived at compile time (constant propagation and symbolic value analysis). This section presents a highlevel overview of the methods used in Parascope to address these problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ) Call Graph Construction:</head><p>The fundamental structure used in interprocedural data-flow analysis is the program's static call graph. Its nodes represent procedures; it has an edge from z to y for every call site in procedure z that invokes procedure y. In the absence of procedure-valued variables, the call graph can be built in a single pass over the program. However, most programming languages include some form of procedure-valued variables; this feature complicates the construction process.</p><p>The program compiler uses an algorithm due to Hall and Kennedy to construct an approximate call graph for languages with procedure-valued parameters <ref type="bibr">[20]</ref>. For such languages, the procedure variables may only receive their values through the parameter passing mechanism. Building the call graph involves locating the possible bindings for the procedure parameters, a problem similar to constant propagation (see below).</p><p>To provide information about loop nests that span procedure calls, it augments the call graph with nodes and edges that represent loops in a program. If x calls y from within some loop 1, there will be an edge from the node for z to a node for 1 and from there to the node for y.</p><p>2 ) Summary Analysis: The program compiler annotates each edge in the call graph with summary sets that describe the possible side effects of executing that call. In particular, each edge e has two associated sets:</p><p>1) MOD(e) contains those variables that might be mod-2) REF(e) contains those variables that might be refer-ParaScope computes Jlow-insensitive summary information; that is, the MOD set computed for a call contains all variables modified along some path from the call. (In contrast, a flow-sensitive version of MOD would contain only those variables modified along every path from the call.) The program compiler derives MOD and USE sets by solving a set of simultaneous equations posed over the call graph or one of its derivatives <ref type="bibr">[21]</ref>.</p><p>3) Regular Section Side-Effect Analysis: Classical summary analysis techniques treat array elements naively-they deal only with the entire array. For example, a E M O D ( p ) asserts that one or more elements of the array a may be modified by a call to p . It might be that every element of a is always assigned by p . Alternatively, p might assign only to a [ 1, 1 ] . Classical side-effect analysis does not distinguish between these cases. Because of this lack of precision, classical MOD and REF sets produce only limited improvement in the precision of dependence information for subscripted arrays.</p><p>To provide better information about side effects to arrays, the program compiler can compute a more complex form of summary information known as regular sections <ref type="bibr">[22]</ref>. The basic idea is to replace the single-bit representation of array side effects with a richer lattice that includes the representation of entire subregions of arrays. A regular section is simply a subregion that has an exact representation in the given lattice.</p><p>The following example illustrates this idea. Figure 2 displays a lattice of reference pattems for an array a. Here, the regular sections are single elements, whole rows, whole columns, and the entire array. Note that i, j, and k are arbitrary symbolic input parameters to the call. Thus, for a procedure that modified column i of a, the regular section MOD set would contain a descriptor 'a [ * , i ] .</p><p>We are implementing regular section side-effect analysis in the program compiler. It has been implemented in the PFC system since 1989; our experience with that implementation shows that the analysis is both efficient and effective 1231.</p><p>4 ) Kill Analysis: An important part of determining whether a loop can be run in parallel is the identification of variables that can be made private to the loop body. In ified if the call is taken, and enced if the call is taken. Fig. <ref type="figure">2</ref>. A simple regular section side-effect lattice the following example, the index variable i can be made private in the do loop, while a must be shared. do i=l,n end Modifications to private variables during one iteration of the loop do not affect other iterations of the loop. For this reason, dependences involving private variables do not inhibit parallelism.</p><p>To locate private variables, the compiler can perform KILL analysis. We say that a value is killed if it is redefined along every path that leads to a subsequent reference. For scalar variables, intraprocedural KILL analysis is well understood. The interprocedural problem is intractable in general <ref type="bibr">[24]</ref>; methods that compute approximate solutions have been proposed and implemented <ref type="bibr">[25]</ref>.</p><p>As with summary analysis, computing KILL information is more complex for arrays than for scalar variables [26]- <ref type="bibr">[28]</ref>. In practice, some simple cases arise in loops. These are easier to detect than the general case. For example, many initialization loops have the property that they define every element of some array a, and no element is referenced before its definition. In this case, the compiler can conclude that the loop kills a. If the loop contains a procedure call, interprocedural propagation of KILL information may be necessary.</p><p>Experience suggests that KILL information on arrays, both interprocedural and intraprocedural, is important in parallelizing existing applications [29], <ref type="bibr">[30]</ref>. Exploration of methods for approximating KILL sets for both arrays and scalars is underway in 5) Alias Analysis: To give the module compiler more information about the run-time environment in which a procedure will execute, the program compiler annotates each node in the call graph with an ALIAS set <ref type="bibr">[31]</ref>. For a procedure p , ALIAS(^) contains pairs of names. Each pair (2, y), where :I: and y are either formal parameters or global variables, is an assertion that :I: and y may occupy the same storage location on some invocation of y . As with the summary sets, the program compiler computes a conservative approximation to the actual set of aliases that might hold at run time. Because it uses a formulation of the problem that ignores control flow within a procedure, the analysis may include some alias pairs that cannot occur at run time.</p><formula xml:id="formula_2">a [ i ] = f ( a [ i ] -1 )</formula><p>The ALIAS computation is formulated as a set of simultaneous equations posed over the call graph and its derivatives Wl.</p><p>6) Constant Propagation: Interprocedural constant propagation tags each procedure p in the call graph with a set CONSTANTS that contains (name, value) pairs. Each pair asserts that name contains value on entry to the procedure, along all paths leading to an invocation of p. Additionally, it tags each edge e with a CONSTANTS set that contains (name, value) pairs. These pairs represent variables that are known to be set to a constant value as a result of executing the call.</p><p>Algorithms explored in Parascope to derive CONSTANTS work by iterating over the program's call graph. To model the way that values pass through procedures, it uses a set ofjumpfuncrions. These functions describe the relationship between the values of parameters that p passes out at a call site and the values that p itself inherits from the routines that call it. Similarly, reverse jumpfunctions are constructed for each variable visible in the caller whose value the callee sets. Using the jump functions for each procedure to model the transmission of values along edges in the call graph, the algorithm efficiently constructs a conservative CONSTANTS set for each node and each edge. The complexity of the jump function implementation affects the type of constants actually found by the algorithm [33].</p><p>7) Symbolic Value Analysis: Experience with dependence analyzers embedded in both research and commercial compilers has shown the value of symbolic analysis as a way of improving the precision of dependence analysis. In practice, the analyzer can often prove independence for two array references without knowing the value of some variables that arise in the dependence test. Cases that arise include expressions such as (y -y)/x where the system knows a value for z but not y. Here, the value of the expression is independent of ys specific value.</p><p>Parascope contains an analyzer that performs intraprocedural symbolic analysis. An analyzer that performs interprocedural symbolic analysis is nearing completion. The analysis extends traditional interprocedural constant propagation to include propagation of symbolic values around the call graph. It uses gated static single assignment graphs to find values that are provably equal-a notion similar to the classic value numbering technique [34], <ref type="bibr">[35]</ref>.</p><p>When interprocedural symbolic analysis is complete, the program compiler will discover variables that have consistent symbolic values. This happens during the interprocedural analysis phase; the results are recorded with the program in the repository. When the dependence analyzer is invoked to examine a procedure, it will check the data base for symbolic information and use that information as a natural part of its tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Interprocedural Transformations</head><p>Future versions of the program compiler will automatically employ both interprocedural code motion and procedure cloning to improve the program. Interprocedural code motion is a general term used to describe transformations that move code across a procedure boundary. Names are translated to model the effects of parameter binding and to merge the name spaces. Inline substitution is a simple form of interprocedural code motion [36]; it replaces a procedure call with a copy of the code for the called procedure. Loop extraction and loop embedding are two other forms of interprocedural code motion that the program compiler will use <ref type="bibr">[16]</ref>. Loop extraction pulls an outermost enclosing loop from a procedure body into a calling procedure; it is a form of partial inlining. Loop embedding is the inverse operation; it pushes a loop surrounding a call site into the called procedure. Procedure cloning lets the compiler produce multiple versions of a single procedure [37], <ref type="bibr">[38]</ref>. Each call site of the procedure is assigned to a specific version of the procedure; that version can be tailored to the calling environment. Call sites with similar calling environments can share a single version.</p><p>The program compiler will use these transformations to create additional opportunities for profitable parallel execution. In particular, it will target loop nests that either span procedures or are hidden in a called procedure. In keeping with Parascope's focus on transforming loop nests, it will try to create more loops with sufficient granularity and the right dependence structure for prpfitable parallel execution. This can be accomplished as follows:</p><p>1) increasing granularity-granularity can be increased by optimizations such as loop interchange and loop fusion [see Section III-A4)]. Unfortunately, it is problematic to apply these transformations when the loops lie in different procedures. Thus, the program compiler will useinterprocedural code motion to relocate these loops. a target intraprocedural optimization would be profitable, if only it could be applied, and supplying more context from other procedures would make application possible.</p><p>2) Use a combination of interprocedural data-flow information, interprocedural code motion, and procedure cloning to transform the focus point in a way that makes it possible to apply the target optimization. 3) Apply the target optimization. Thus, interprocedural transformations will be used exclusively to enable application of profitable target optimizations -in this case, parallelizing transformations. We call this strategy goal-directed interprocedural optimization <ref type="bibr">[45]</ref>. The effectiveness of this strategy in the context of parallelizing transformations was tested in an experiment with the PERFECT benchmark suite. A goal-directed approach to parallel code generation was employed to introduce parallelism and increase granularity of parallel loops [ 161. Because the approach proved to be effective, we plan to use this strategy in the planned prototype source-to-source parallelizer mentioned in Section 11-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Application of Interprocedural Techniques</head><p>Several of the techniques described are also implemented in the Convex Applications CompilerTM. It performs regular section analysis and constant propagation, as well as the classical summary and alias analysis. It clones procedures to improve the precision of its CONSTANTS sets, and performs limited inline substitution. Metzger and Smith report average improvements of twenty percent, with occasional speedups by a factor of four or more <ref type="bibr">[41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">PARASCOPE EDITOR</head><p>Upon completion, the compilation system described in Section I1 will be capable of automatically converting a sequential Fortran program into a parallel version. A substantial amount of work has been conducted on the viability of this approach by researchers at Rice and elsewhere [ 11-[3], <ref type="bibr">[ 15]</ref>, <ref type="bibr">[46]</ref>. Ideally, automatically parallelized programs will execute efficiently on the target architecture and users will not need to intervene. Although such systems can effectively parallelize many interesting programs, they have not established an acceptable level of success. Consequently, advanced programming tools such as the ParaScope editor are required to assist programmers in developing parallel programs.</p><p>At the core of automatic parallelization is dependence analysis (see Section 11-A), which identifies a conservative set of potential data race conditions in the code that make parallelization illegal. In general, the compiler cannot parallelize a loop or make transformations if doing so would violate a dependence and potentially change the sequential semantics of a program. Dependence analysis is necessarily conservative-it reports a dependence if there is a possibility that one exists, or is feasible. In some cases, it is quite obvious to a programmer that a reported dependence is infeasible and will not ever occur. In a completely automatic tool, the user is never given an opportunity to make this determination. Automatic parallelization must also make difficult decisions about how and where to introduce parallelism; evaluating performance tradeoffs can be highly dependent on run-time information such as symbolic loop upper bounds. Again, programmers might be able to provide such information if only there were a mechanism to communicate it to the compiler.</p><p>The ParaScope editor, PED, serves this purpose by enabling interaction between the compiler and the programmer. The compilation system, described in Section 11, computes a wealth of information about the program. The sheer volume of information dominates the program size: there is more information than the programmer can reasonably study. PED acts as a sophisticated filter; it provides a meaningful display of the program dependences calculated by the compiler. Furthermore, the tool assists the programmer with parallelization and the application of complex code transformations. The programmer can refine conservative assumptions by determining which dependences are valid and then selecting transformations to be applied. The editor updates dependence information and then source code to reflect any such programmer actions. The compiler may then use these modifications when generating code.</p><p>Research on PED began in the late 1980s. Its interface, design and implementation have been discussed in detail in other papers <ref type="bibr">[30]</ref>, <ref type="bibr" target="#b42">[47]</ref>- <ref type="bibr" target="#b44">[49]</ref>. This section describes a new version of the ParaScope editor from the user's point of view and discusses new research directions for PED. We also sketch also the design issues involved in constructing such a tool and relate how it integrates with the compilation system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ped Functionality</head><p>PED supports a paradigm called exploratory parallel programming, in which the user converts an existing sequential Fortran program into an efficient parallel one by repeatedly finding opportunities for parallelism and exploiting them via appropriate changes to the program <ref type="bibr" target="#b43">[48]</ref>, <ref type="bibr" target="#b44">[49]</ref>. This process involves a collaboration in which the system performs deep analysis of the program, the user interprets analysis results and the system helps the user carry out changes. Specifically, the user selects a particular loop to parallelize and PED provides a list of dependences that may prevent direct parallelization. The user may override PED'S conservative dependence analysis. Otherwise, the user may consider applying a program transformation to satisfy dependences or introduce partial parallelism of the loop. On request, PED gives the user advice and information on the application of transformations. If the user directs, PED performs transformations automatically and updates the dependence information.</p><p>PED uses three user interface techniques to manage and present the voluminous and detailed information computed by the program compiler: (1) a book metaphor with progressive disclosure of details, ( 2 ) user-controlled view $filtering, and ( 3 ) power steering for complex actions. The book metaphor portrays a Fortran program as an electronic book in which analysis results are treated as annotations of the main source text analogous to a book's marginal notes, footnotes, appendices, and indices <ref type="bibr" target="#b45">[50]</ref>. Progressive disclosure presents details incrementally as they become relevant rather than overwhelming the user with all details at once <ref type="bibr" target="#b46">[51]</ref>. View filtering acts as an electronic highlighter to emphasize or conceal parts of the book as specified by the user <ref type="bibr" target="#b47">[52]</ref>. The user can choose predefined filters or define new ones with boolean expressions over predefined predicates. Power steering automates repetitive or errorprone tasks, providing unobtrusive assistance while leaving the user in control.</p><p>The layout of a PED window is shown in Figure <ref type="figure" target="#fig_1">3</ref>. The large area at the top is the source pane displaying the main Fortran text. Beneath it are two footnotes displaying extra information, the dependence pane and the variable pane. All three panes operate in basically the same way. Scroll bars can be used to bring other portions of the display into view, predefined or user-defined view filters may be applied to customize the display, the mouse may be used to make a selection, and menu choices or keyboard input may be used to edit the displayed information. The Fortran text, the dependence information, and the variable information are each described below in terms of the information displayed, the view filtering, and the type of edits that may be performed. Section 111-A4) describes PED'S transformations, which provide power steering for complex source edits commonly encountered in parallelization. Section 111-AS) presents PED'S approach to updating dependence information.</p><p>I ) Fortran Source Code: Fortran source code is displayed in pretty-printed form by the source pane. At the left are marginal annotations showing the ordinal number of each source line, the start of each loop, and the extent of the current loop [see Section 111-A2)]. In the program text, placeholders used in structure editing are shown in italics and syntax errors are indicated with boldface messages. Manually controlled ellipses enable the user to conceal and later reveal any range of source lines.</p><p>Source view filtering can be based on either the text or the underlying semantics of each source line. For example, source view filter predicates can test whether a line contains certain text, whether the line contains a syntax or semantic error, whether the line is a specification statement, an executable statement, a subprogram header, or a loop header.</p><p>The source pane allows arbitrary editing of the Fortran program using mixed text and structure editing techniques <ref type="bibr" target="#b48">[53]</ref>. The user has the full freedom to edit character by character, or at any time to use the power steering afforded by template-based stiucture editing. During editing, PED directly constructs an abstract syntax tree (AST) representation of the procedure; this intemal form for procedures is used throughout the environment. The AST representation of source means that the code is always parsed. This feature has several important benefits. Analysis operates on the parsed trees, which maintain all appropriate semantic information needed by the analysis algorithms. Similarly, program transformations are straightforward manipulations of the tree. Incremental parsing occurs in response to editing changes, and the user is informed of any syntactic or semantic errors.</p><p>2 ) Dependence Information: The extensive compiler analysis described in Section I1 produces program dependence information that is revealed in the dependence pane. The display contains a list of dependences in tabular form, showing each dependence's source and sink variable references as well as some characterizations of the dependence useful for program transformations. Because Fortran programs may contain an unwieldy number of dependences, PED uses progressive disclosure of dependences based on a current loop. When the user expresses interest in a particular loop by clicking on its loop marker in the source pane, that loop's dependences immediately appear in the dependence pane. In addition to the dependence pane's tabular view, the source pane offers a graphical view of dependences. Each dependence is shown as a red arrow from its source variable reference to its sink variable reference, as illustrated in Fig. <ref type="figure" target="#fig_1">3</ref>. Showing arrows for all of a loop's dependences can produce an unintelligible jumble of red, so PED provides a second level of progressive disclosure. For example, the user can choose whether to see arrows for all listed dependences, for only the dependences affecting a selected range of source code, or for just one dependence selected in the dependence pane. In Figure <ref type="figure" target="#fig_1">3</ref>, the user has selected a single dependence which is reflected in the source and dependence panes.</p><p>Dependence view filter predicates can test the attributes of a dependence, such as its type, source and sink variable references, loop nesting level, and its source and sink line numbers in the Fortran text. In addition, predicates can test the user-controlled mark and reason attributes described in the next paragraph. By filtering the dependence list, the user may examine specific dependence relationships more clearly.</p><p>Naturally the dependence pane forbids modification of a dependence's computed attributes such as type, source, and sink, because these are implied by the Fortran source code. But the dependence pane does permit an important If PED proves a dependence exists with exact dependence tests [ 131, the dependence is marked as proven; otherwise it is marked pending. When users mark dependences as rejected, they are asserting that the dependences are not feasible and that the system should ignore them. Therefore, rejected dependences are disregardeG when PED considers the safety of a parallelizing transformation. However, rejected dependences are not actually deleted because the user may wish to reconsider these at a later time. Instead, the classification of each dependence is stored in a special mark attribute which the user can edit. If desired, the user can use the reason attribute to attach a comment explaining the classification decision. A mark dependences dialog box provides power steering for dependence marking by allowing the user to classify in one step an entire set of dependences that satisfy a chosen filter predicate.</p><p>3 ) Variable Information: In Section 11, we highlighted the importance of locating variables that can be made private to the loop body. Briefly, private variables do not inhibit parallelism. The program compiler locates many of these for scalar variables using local KILL analysis. However, this classification is conservative and may report a variable as shared even if it may legally be made private. The variable pane in PED enables users to peruse the results of this analysis and to correct overly conservative variable classifications for array and scalar variables.</p><p>The variable pane displays in tabular form a list of variables in the current loop, showing each variable's name, dimension, common block if any, and shared or private status. As with dependences, progressive disclosure is based on the current loop, filter predicates can test any of the attributes of a variable, and modification of a variable's computed attributes is forbidden. However, the variable pane also provides an important form of editing known as variable classijcation.  Variable classification enables the user to edit a variable's sharedprivate attribute and attach a comment in its reason attribute. A classify variables dialog box (analogous to Mark Dependences) provides power steering for variable classification. In this way, the user may sharpen variable analysis by asserting a variable is private when appropriate. This reclassification eliminates dependences from a loop and may enable parallelization.</p><p>4 ) Transformations: PED provides a number of interactive transformations that can be applied to programs to enhance or expose parallelism. Transformations are applied according to the power steering paradigm: the user specifies the transformations to be made, and the system provides advice and carries out the mechanical details. The system advises whether the transformation is applicable (i.e., makes semantic sense), safe (i.e., preserves the semantics of the program) and profitable (i.e., contributes to parallelization). The complexity of these transformations makes their correct application difficult and tedious. Thus, power steering provides safe, correct and profitable application of transformations. After each transformation, the dependence graph is directly updated to reflect the effects of the transformation.</p><p>PED supports a large set of transformations that have proven useful for introducing, discovering, and exploiting parallelism. PED also supports transformations for enhancing the use of the memory hierarchy. These transformations are described in detail in the literature <ref type="bibr">[36]</ref>, <ref type="bibr">[491,[541-[591.</ref> Figure <ref type="figure" target="#fig_2">4</ref> shows a taxonomy of the transformations supported in PED.</p><p>Reordering transformations change the order in which statements are executed, either within or across loop iterations. They are safe if all dependences in the original program are preserved. Reordering transformations are used to expose or enhance loop-level parallelism. They are often performed in concert with other transformations to introduce useful parallelism.</p><p>Dependence breaking transformations are used to satisfy specific dependences that inhibit parallelism. They may introduce new storage to eliminate storage-related dependences, or convert loop-carried dependences to loopindependent dependences, often enabling the safe application of other transformations. If all the dependences carried by a loop are eliminated, the loop may be executed in parallel.</p><p>Memory optimizing transformations adjust a loop's balance between computation and memory access to make better use of the memory hierarchy and functional pipelines. These transformations have proven to be extremely effective for both scalar and parallel machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">) Incremental Reanalysis:</head><p>Because PED is an interactive tool, efficient updates to the program representations are essential to the usability of the tool. We have taken a simple approach to reanalysis in PED that is based on the effect and scope of a change. After a change, PED performs reanalysis as follows.</p><p>1) If the effect of a change is known, PED directly updates the effected dependence and program representations.</p><p>2) If PED can determine the scope of a change, PED applies batch analysis to the affected region.</p><p>3) If PED cannot determine the scope of a change, PED applies batch analysis to the entire procedure.</p><p>For example, most of the structure program transformations in PED have the advantage that their effects are predictable and well understood. Thus, after a transformation, PED directly updates the dependence graph and AST. These updates are very efficient and fast in practice. However, they are specific to each transformation.</p><p>Updates to the dependence graph after some simple edits, such as addition and deletion of assignment statements, are performed incrementally. After a complicated edit or large number of edits, reanalysis is difficult because the effects can be extensive. As described in Section 11-A, the precision of dependence analysis relies on sophisticated data-flow information. These updates may be very expensive in an interactive setting and in some cases, incremental analysis may be more expensive than performing batch analysis for the effected procedures <ref type="bibr">[28]</ref>. Therefore, after arbitrary program edits, PED relies on efficient batch algorithms rather than on a general incremental algorithm.</p><p>Updates following edits must not only recalculate intraprocedural data-flow and dependence information, but must also recompute interprocedural data-flow information. Changes in interprocedural facts due to an edit in one procedure may cause the analysis of other procedures to be invalid. The program compiler discovers this type of inconsistency when it is invoked to analyze and compile a program (see Section 11-B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ongoing Research</head><p>The Parascope editor was originally inspired by a user workshop on FTOOL, a dependence and parallel program browser also developed at Rice <ref type="bibr" target="#b56">[60]</ref>- <ref type="bibr" target="#b58">[62]</ref>. PED has influenced and been influenced by other parallel programming tools and environments, such as MIMDizer and its predecessor Forge by Pacific Sierra [63], <ref type="bibr" target="#b59">[64]</ref>, SIGMACS in the FAUST programming environment [6], <ref type="bibr" target="#b60">[65]</ref> and SUPERB [81.</p><p>In addition, PED has been continually evaluated internally and externally by users, compiler writers, and user interface designers <ref type="bibr">[30]</ref>, <ref type="bibr" target="#b42">[47]</ref>, <ref type="bibr" target="#b61">[66]</ref>,[671.</p><p>In the summer of 1991, we held a workshop on Para-Scope to obtain more feedback from users and to further refine our tool <ref type="bibr">[30]</ref>. Participants from industry, government laboratories, and supercomputing centers used ParaScope to parallelize programs they brought with them. For the most part, these scientific programmers, tool designers, and compiler writers reported that the user interface and functionality were appropriate to the task of parallelization. Their suggestions and experiences led us to target several areas for future development.</p><p>In the following three sections, we describe revisions to the Parascope editor to further assist scientific programmers. We first present a method to assist users with the technology available from an automatic parallelizing system. Second, we discuss an alternative to the dependence deletion mechanism described above. This mechanism improves system response after editing changes. Finally, we describe improvements to system response when interprocedural information changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ) Combining Interactive and Automatic Parallelization:</head><p>The purpose of interactive tools like PED is to make up for the shortcomings of automatic parallelization. However, the proposed automatic parallelizer mentioned in Section 11-B will contribute knowledge about the effectiveness of transformation and parallelization choices that could be useful to the programmer. We plan to combine the machine-dependent knowledge of the parallelizer and deep analysis of the program compiler with the program-specific knowledge of the user.</p><p>We propose the following approach for Parascope [ 151. The program is first transformed using an automatic parallelizer. This source-to-source translation produces a machine-specific source version of the program. If the user is satisfied with the resultant program's performance, then the user need not intervene at all. Otherwise, the user attempts to improve parallelization with PED, incorporating information collected during parallelization. The parallelizer will mark the loops in the original version that it was unable to effectively parallelize. Using performance estimation information, it will also show the user the loops in the program where m3st af the time is spent, indicating to the user where intervention is most important. The user then makes assertions and applies transformations on these loops. In addition, the user may invoke the parallelizer's loop-based algorithms to find the best combinations of transformations to apply, providing interactive access to the parallelizer's decision process.</p><p>2 ) User Assertions: In PED, the dependence and variable displays assist users in perusing analysis and overriding conservative analysis. For example, when a user reclassifies a variable as private to a loop, subsequent transformations to that loop will ignore any loop-carried dependences. In addition, any dependences that the user classifies as rejected are ignored by the transformations. These mechanisms enable users to make assertions about specific dependences and variables. Unfortunately, the current interface is not rich enough. Consider the following example: r e a d * ,</p><formula xml:id="formula_3">k do i = 1 , m do j = 1, n a [ i , j ] = a [ i , k ] + 1 b [ i , j l = b [ k , j l * a [ k , j l enddo enddo</formula><p>The value of k is unknown at compile time. Therefore, PED conservatively reports dependences between a [ i , j ] and a [ k , j l and between b [ i , jl and b [ k , j l , reflecting the possibility that i could equal k on some iteration of the i loop. Similarly, dependences between a [ i , j ] and a [ i , k ] are reported, in case j could equal k on some iteration of the j loop. If the user knew that k was not in the range of I-m and also .not in the range I-n, then all dependences could be deleted. There is no convenient way to express this with the existing dependence filtering mechanism. The best approximation is to delete all dependences on a and b, but the user would still have to examine each dependence to justify that it was infeasible.</p><p>Another problem with explicit dependence deletion is that it fails to capture the reasons behind the user's belief that a dependence does not exist. While the user can add a comment accompanying the dependence for future reference, this information is unavailable to the system. After source code editing, it is very difficult to map dependence deletions based on the original source and program analysis to the newly modified and analyzed program. As a result, some dependences may reappear and programmers will be given an opportunity to reconsider them in their new program context.</p><p>For these two reasons, we intend to include a mechanism in ParaScope that will enable users to make assertions that can be incorporated into dependence testing. The assertion language will include information about variables, such as value ranges. All the dependences in the code above could be eliminated with the appropriate assertion about the range of the variable k. The programmer could determine the correctness and need for this assertion by examining at most two dependences rather than all of them. Furthermore, by making the assertions part of the program, the system can derive the necessary information to recover dependence deletions following edits. The user will be responsible for maintaining the validity of the assertions.</p><p>3 ) Reanalysis f o r Global Changes: PED'S current incremental analysis is designed to support updates in response to changes during a single editing session. This approach does not address the more difficult issue of updating after global program changes. More precisely, a programmer might parallelize a procedure p using interprocedural information, put 1' away, and edit other modules. Later changes to procedures in the program may affect the validity of information used to parallelize p .</p><p>Consider some ways in which the user might make decisions based on interprocedural information: the user may designate a loop as parallel, apply transformations to the program, or add assertions to be used by dependence testing. Below, we describe the consequences of these changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">) Parallel Loops:</head><p>When the user specifies a loop as parallel in PED, it is either because the system reports no dependences are carried by that loop, or because the user believes reported loop-carried dependences to be infeasible. Later, a change elsewhere in the program may cause the system to reanalyze the procedure containing this loop. If this analysis reports no loop-carried dependences for the same loop, the user's previous specification is still correct. However, if new loop-camed dependences have been introduced, the programmer will need to be warned that the change may have resulted in an erroneous parallel</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Structure Trunsfotmutions:</head><p>The correctness of transformations may also depend on interprocedural information. Safe transformations are guaranteed to preserve the semantics of the program. However, edits are not. To determine if an edit that follows a transformation preserves the information that was needed to safely perform the transformation is very difficult. It requires maintaining editing histories and a complex mapping between a procedure and its edited versions.</p><p>Rather than addressing this general problem, we will provide a compromise solution. When a transformation is applied, the system will record the interprocedural information used to prove the transformation safe. After reanalysis due to editing, the system could compare the new interprocedural information with that stored previously. The system will warn of any changes to interprocedural information that might have invalidated transformations. The programmer will be responsible for determining the effects of these changes on the meaning of their program.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6) Assertions and Arbitrary Edits:</head><p>The programmer may also rely on interprocedural facts in deriving assertions or in making arbitrary edits. To allow the system to track the information that the programmer used, we could include assumptions about interprocedural information in the assertion language. Then, the system could warn the user when this information changes. loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iv. PARALLEL DEBUGGING IN PARASCOPE</head><p>If during the development of a shared-memory parallel Fortran program, the programmer directs the compiler to ignore a feasible data dependence carried by a parallel loop or parallel section construct, a datu race will occur during execution of the program for some input dataset. A data race occurs when two or more logically concurrent threads access the same shared variable and at least one of the accesses is a write'. Data races are undesirable since they can cause transient errors in program executions.</p><p>Ignoring feasible dependences during program development is a real concern. Since dependence analysis is inherently conservative, often the dependence analyzer will report parallelism inhibiting dependences that can never be 'We use the term thread to refer to an indivisible unit of sequential work.</p><p>realized at run time. Using PED, programmers can examine dependences that limit opportunities for parallel execution and manually reclassify those thought to be infeasible. Since errors in this classification process are evident only at run time, programmers need run-time support for helping them determine when they have mistakenly classified a dependence as infeasible.</p><p>Traditional debugging tools designed to support statebased examination of executing programs are ill-equipped to help locate data races. Since transient behavior caused by data races is only evident in program executions on shared-memory multiprocessors, using tools for state-based examination to detect data races is only appropriate in this context. Unfortunately, inspecting live executions of programs on parallel machines can perturb the relative timing of operations and change the set of execution interleavings that occur. Thus, the act of trying to isolate a data race causing erroneous behavior by halting an executing program and examining its state can make evidence of the race vanish.</p><p>A promising approach for pinpointing data races is to instrument programs to monitor the logical concurrency of accesses during execution [68 ]- <ref type="bibr" target="#b69">[73]</ref>. With such instrumentation, a program can detect and report data races in its own execution. The Parascope debugging system uses such a strategy to detect data races at run time and supports automatic instrumentation of programs for this purpose.</p><p>Using the Parascope debugging system to detect data races requires little effort from a programmer. The programmer simply directs the compilation system to add data race instrumentation when it compiles the program. If a data race occurs during any execution of an instrumented program, diagnostic output from a run-time library reports the pair of references involved in the race and the parallel construct carrying the dependence that caused the race. If an instrumented program is run under control of the ParaScope source-level debugger, each race detected is treated as a breakpoint at which the programmer may inspect the state of the program to determine the conditions that caused the race to occur.</p><p>For a restricted (but common) class of programs, the Parascope debugging system guarantees that if any data race can arise during some execution with a particular input data set, then at least one race will be detected for any execution using that input. This is an important property since it makes parallel debugging tractable and frees programmers from needing to consider all possible execution interleavings for any particular input.</p><p>The support for parallel debugging in Parascope consists of several components: a run-time library of routines that can detect if a variable access is involved in a data race during program execution, an instrumentation system that adds calls to the runtime library routines where necessary so that data races will be detected during execution, and a graphical user interface that decodes data race reports from the run-time library and presents them in the  context of an interactive source-language debugger. In the following sections, we describe each of these components of in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Run-time Detection of Data Races</head><p>Run-time techniques for detecting data races fall into two classes: summary methods [72]- <ref type="bibr" target="#b71">[74]</ref> that report the presence of a data race with incomplete information about the references that caused it, and access history methods <ref type="bibr" target="#b64">[69]</ref>- <ref type="bibr" target="#b66">[71]</ref> that can precisely identify each of a pair of accesses involved in a data race. In ParaScope, we use an access history method for detecting data races at run time since precise race reports are more helpful to programmers.</p><p>To pinpoint accesses involved in data races, access history methods maintain a list of the threads that have accessed each shared variable and information that enables determination of whether any two threads are logically concurrent. When a thread t accesses a shared variable V , t determines if any thread in V s access history previously performed an access that conflicts with t s current access, reports a data race for each such thread that is logically concurrent with t , removes from the history list information about any threads that are no longer of interest, and adds a description of t s access to the access history. In the most general case, the length of a variable's access history may grow as large as the maximum logical concurrency in the program execution.</p><p>An open question in the development of run-time techniques for detecting data races is whether the time and space overhead they add to program executions may be too great for the techniques to be useful in practice. For programs with nested parallel constructs that use eventbased synchronization to coordinate Concurrent threads, the worst case time and space overhead is proportional to the product of the number of shared variables and the number of logically concurrent threads. Such high overhead would often be unacceptable to users. For our initial investigation into the feasibility of using run-time techniques for automatically detecting data races, we have chosen to focus our efforts on developing a run-time library to support a common but restricted class of programs for which data race detection can be accomplished particularly efficiently. Specifically, we limit our focus to Fortran programs that use nested parallel loops and sections to explicitly contain no synchronization other than that implied by Such programs are known as fork-join programs. In the next two sections, we describe a model of concurrency for such programs and the properties of an efficient protocol for detecting data races in their executions at run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I) Modeling Run-Time Concurrency in Fork-Join Programs:</head><p>Upon encountering a parallel loop or section construct, a thread terminates and spawns a set of logically concurrent threads (i.e., one for each loop iteration or section in the parallel construct). We call this operation a fork. Each fork operation has a corresponding join operation; when all of the threads descended from a fork terminate, the corresponding join succeeds and spawns a single thread. A thread participates in no synchronization operations other than the fork that spawned it and the join that will terminate it.</p><p>The structure of concurrency in an execution of a forkjoin program can be modeled by a concurrency graph that is directed and acyclic. Each vertex in a concurrency graph represents a unique thread executing a (possibly empty) sequence of instructions. Each graph edge is induced by synchronization implied by a fork or join construct. A directed path from vertex tl to vertex t 2 indicates that thread t terminated execution before thread f 2 began execution.</p><p>Figure <ref type="figure" target="#fig_3">5</ref> shows a fragment of parallel Fortran and the corresponding graph that models concurrency during its execution. Each vertex in the graph is labeled with the sequence of code blocks that are sequentially executed by the thread corresponding to the vertex. In Fig. <ref type="figure" target="#fig_3">5</ref>, the concurrency graph for each parallel loop is formed by linking in parallel the concurrency graph for each loop iteration. The concurrency graph for iteration I=2 of the outermost parallel loop is formed by linking in series of the concurrency graphs for the two loops nested inside that iteration.</p><p>denote parallelism, and the parallel loop and section constructs.</p><p>Two vertices in a concurrency graph represent logically concurrent threads in an execution of a program if and only if the vertices are distinct, and there is no directed path between them. To detect if a pair of conflicting accesses is involved in a data race, the debugging runtime system must determine if the threads that performed accesses are logically concurrent. Rather than explicitly building a concurrency graph representation at run time to make this determination, the run-time system assigns each thread a label that implicitly represents its position in the concurrency graph. For this purpose, the system uses offset- span labeling, an on-line scheme for assigning a label to each thread in an execution of a program with fork-join parallelism <ref type="bibr">[70]</ref>. By comparing the offset-span labels of two threads, their concurrency relationship can be deduced.</p><p>2 ) A Protocol for Detecting Data Races: In contrast to other access history protocols described in the literature <ref type="bibr" target="#b64">[69,</ref><ref type="bibr" target="#b66">71]</ref>, the run-time protocol used by ParaScope to detect data races bounds the length of each variable's access history by a small constant that is program independent <ref type="bibr">[70]</ref>. Bounding the length of access histories has two advantages. First, it reduces the asymptotic worst-case space requirements to O ( V N ) , where V is the number of monitored shared variables, and N is the maximum dynamic nesting depth of parallel constructs. Second, it reduces to O ( N ) the asymptotic worst-case number of operations necessary to determine whether a thread's access is logically concurrent with any prior conflicting accesses. However, since the protocol only maintains information about a bounded number of accesses to each shared variable, information about some accesses to shared variables will have to be discarded so as not to exceed the space bound. A concern is that discarding this information may cause a data race to escape detection.</p><p>To guarantee that the ParaScope debugging system always reports a data race if any is present in an execution, the race detection protocol used must be insensitive to the interleaving of accesses in an execution. In a previous paper <ref type="bibr">[70]</ref>, the following assertion is proven about the run-time protocol used by ParaScope. If an execution of a program with nested fork-join parallelism contains one or more data races, at least one will be reported.</p><p>Proof of this assertion establishes that a bounded access history is sufficient for detecting data races in programs with fork-join parallelism regardless of the actual temporal interleaving of accesses in an execution. With this condition, an execution will never be erroneously certified as race free. Furthermore, if no race is reported during a monitored execution for a given input, then the program is guaranteed to be deterministic for that input.</p><p>Because the ParaScope debugging system uses an access history protocol with this property, it can support the following effective debugging strategy for eliminating data races from a program execution for a given input:</p><p>1) Run the program on the given input using the monitoring protocol to check for data races involving accesses to shared variables.</p><p>2) Each time a data race is reported (the access history protocol precisely reports both endpoints of the race), fix the cause of the data race, and re-execute the program with the same input. Since the access history protocol used by Parascope reports data races (if any exist) regardless of the interleaving order, the protocol can be used to check for races in a program that is executed in a canonical serial order. Executing programs serially while debugging is convenient as it provides the user with deterministic behavior which simplifies the task of determining the origin of variable values that indirectly caused a data race to occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compile-Time Support f o r Data Race Detection</head><p>During an execution, the access history protocol used by the debugging run-time system determines if a data race exists between any two monitored accesses of a shared variable. The task of the compile-time instrumentation system is to guarantee that for each access potentially involved in a data race, there is a corresponding call to the proper run-time support routine to test for the presence of a race. Rather than instrumenting each access to a shared variable, the debugging instrumentation system is tightly integrated with the Parascope compilation system so that it can exploit compile-time analysis to significantly reduce the number of run-time checks. Here, we first describe the task of the debugging instrumentation system, then we describe how the instrumentation system is integrated into the ParaScope compilation system.</p><p>The instrumentation system has two main tasks: to allocate storage for access history variables; and to insert calls to run-time support routines that perform access checks or concurrency bookkeeping. To maximize the portability of instrumented code, we have chosen to perform data race instrumentation at the source code level. For any variable that may be involved in a data race at run time, the instrumentation system must allocate storage for an access history.</p><p>For references that need to be checked at run time, the instrumentation system inserts a call to the appropriate runtime support routine that will determine if the access is involved in a race. Locating the accesses that need to be monitored is conceptually easy. For variables local to a subprogram, only those accesses that are the endpoints of dependences carried by parallel loops in that subprogram need run-time checks. Using dependence information computed under control of the parascope compilation system (see Section 11-A) considerably reduces the amount of instrumentation necessary.</p><p>The instrumentation process becomes somewhat more complicated in the presence of procedure calls. If a procedure is called from within a parallel construct, the instrumentation system must ensure that any side-effects of the called procedure are appropriately checked for participation in data races. Adding this instrumentation requires two steps. First, the system expands the parameter list for each procedure to include a history variable for each formal COOPER er 01.: PARASCOPE PARALLEL PROGRAMMING ENVIRONMENT </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LOCRL(j)</head><p>Compute t h e r e f l e c t i o n p o i n t s and t h e i r f u n c t i o n v a l u e s .</p><p>p a r a l l e l loop 5888 7 i t e r a t i o n t u a l e I Fig. <ref type="figure">6</ref>. The ParaScope debugger: isolating a race condition parameter and adds a run-time check inside the procedure for each access to a variable that appears as one of its original formal parameters. Next, the system expands any common block declarations inside each procedure to include a history variable for each common variable, and adds a run-time check for each access to a common variable. The instrumentation process as described thus far for programs with procedure calls in parallel loops derives little or no benefit from compile-time analysis to prune the amount of instrumentation needed. To remedy this situation, we are in the process of incorporating interprocedural analysis into the instrumentation system. As currently envisioned, the interprocedural analysis to support data race instrumentation consists of three passes. The first pass determines which accesses inside each procedure require run-time checks. The second pass allocates access history storage for variables as needed. A third pass is necessary to ensure that the set of access history variables added to each declaration of a common block is globally consistent. The ParaScope program compiler directs the collection of this interprocedural information and coordinates instrumentation of the program.</p><formula xml:id="formula_4">= I, n R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R R The</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Reporting Data Races</head><p>There are several ways that an instrumented program can be debugged. Simply running the program will produce a collection of error messages that indicate all of the data races detected during execution. If the program is run under the control of a traditional interactive debugger such as d b x or gdb and a breakpoint is inserted at the appropriate place in the run-time support library, execution will halt as each race is detected enabling the user to investigate the program state.</p><p>One drawback of debugging this way is that the user is exposed to the instrumented code. The Parascope debugging system composes a map created by the instrumentation system with the standard compiler-provided symbol table to create a map between uninstrumented code and machine code. When debugging instrumented code using the Para-Scope source-level debugger (a window-based source level debugger with functionality similar to xgdb), a user sees the original code displayed on the screen. Debugging in terms of the original code uncluttered by instrumentation is usually preferable.</p><p>The Parascope debugger display shown in Fig. <ref type="figure">6</ref> illustrates several points. The debugger window shown is divided into three main panes. The top pane contains the source being debugged. The small pane below that has messages from the debugger, such as values of expressions. The pane at the bottom contains a list of all data races detected during execution.</p><p>In the display shown, the underlining in the bottom pane indicates that the user has selected the last data race reported during the debugging session. This selection is reflected in the source pane by underlining the (temporally) first access of the data race and emboldening the second access. The loop carrying the race is italicized. Examination of the source pane indicates that the first access is not currently being displayed. In fact, it is in the function value.</p><p>Clicking the mouse on the data race in the bottom pane will cause the display to shift so that the other endpoint of the selected race is visible.</p><p>The Parascope debugger supports view filtering (see Section 111-A), which is useful in the debugging process. For example, it provides a view filter predicate that matches source lines containing definitions (or uses) of a particular variable. It also defines a predicate that matches lines found to have been involved in a data race. Thus, it is possible to tailor the display so that only those lines and their surrounding parallel loops are shown. This can often simplify the process of viewing both ends of a race condition whose endpoints are widely separated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Preliminary Evaluation</head><p>Our experiences thus far with the data race detection support in Parascope have been positive. The user interface provides a convenient display of race reports that facilitates understanding how races have arisen. Results from preliminq experiments with the instrumentation system using interprocedural analysis have been encouraging. Using interprocedural information typically reduces the amount of information needed dramatically. For a pair of test programs, the run-time overhead of datarace instrumentation ranges from 100% to 300%. These measurements provide evidence that by using an efficient run-time protocol and aggressive compile-time analysis, the overhead of automatic data race instrumentation may well be tolerable in practice during a debugging and testing phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v. FUTURE DIRECTIONS FOR PARASCOPE</head><p>The current version of Parascope is designed to assist in the development of programs for shared-memory multiprocessors, the class of parallel architectures most widely used at the commencement of the project in the mid 1980's. The principal challenge faced by scientific users of such machines was finding enough loop-based parallelism to make effective use of the available computation power. Over the past few years, a paradigm shift has occurred and distributed-memory parallel computers, such as the Intel Paragon and the Thinking Machines CM-5, have become the dominant machines for large-scale parallel computation. In these machines, part of the memory is packaged with each of the individual processors and each processor can access directly only its own portion of the memory. To access a remote data location requires that the owning processor send the value to the processor that wishes to use it, an expensive operation. The most important optimization for these machines is minimizing the cost of communication. Data placement plays a central role in determining the extent to which communication can be optimized-good data placement is a prerequisite for optimum performance.</p><p>Unfortunately, the shared-memory programming model does not provide the programmer with a mechanism for specifying data placement. Unless the compiler can choose the right data placement automatically-a formidable task-this model will not be very useful on distributedmemory machines. To address this problem, we have developed an extended version of Fortran, called Fortran D (751, which enables the programmer to explicitly specify data distribution and alignment on a multiprocessor system. Furthermore, the specification can be expressed in a machine-independent form, making it possible for the programmer to write a single program image that can be compiled onto different parallel machines with high efficiency. Machine-independent parallel programming is particularly important if application software developed for the current generation of parallel machines is to be usable on the next generation of machines. We are currently developing compilers that translate Fortran D to run efficiently on two very different target architectures: the Intel iPSC/860 and the Thinking Machines CM-2.</p><p>Once the data distribution is known, parallelism can automatically be derived by the Fortran D compiler through the use of the "owner computes" rule, which specifies that the owner of a datum computes its value <ref type="bibr">[76]</ref>. Assignment to all locations in an array distributed across the processors can automatically be performed in parallel by having each processor assign values for its own portion of the array, as long as there are no dependences among the assignments. The compiler is also responsible for rewriting a sharedmemory Fortran D program, which is simply a Fortran 77 or Fortran 90 program with distribution specifications, as a distributed-memory program that can be run on the target machine. For the Intel iPSC1860, this means strip mining all of the loops to run on a single node of the processor array and inserting message passing calls for communication. The resulting message-passing Fortran program is quite different from the original source.</p><p>Since the appearance of the original technical report on Fortran D in December 1990, the language has gained widespread attention. There is currently underway an effort to standardize many of the features of Fortran D into an extended version of Fortran 90, called "High Performance Fortran" (HPF). However, with a language and compiler technology as complex as those required by Fortran D or HPF, the user will need a significant amount of program development assistance to use the machine effectively. To address this need, we have embarked on a project to build a programming environment for Fortran D. We will base it, in part, on Parascope. The paragraphs below describe the role that Parascope technology will play in the new environment.</p><p>I ) Compilation System: Fortran D semantics dictate that distribution specifications for a variable can be inherited from a calling procedure. In addition, the Fortran D distributions are executable, so a called procedure may redistribute a variable. To generate correct and efficient code, the Fortran D compiler must determine which distributions may hold at each point in the program and it must track these distributions interprocedurally <ref type="bibr" target="#b75">[77]</ref>, <ref type="bibr" target="#b76">[78]</ref>.</p><p>2) Intelligent Editor: Most of the functionality in the current Parascope editor will be useful in constructing Fortran D programs. For example, users will still want to find loop nests that can be run in parallel. However, an important additional challenge to the user is the selection of distribution specifications that will yield good performance. A new editor for Fortran D will permit the user to enter and modify distribution specifications. However, by itself, this will not be enough. Experience suggests that users will sometimes find the relationship between distributions and performance puzzling. The new editor will make this relationship clearer by statically predicting the performance of loop nests under specific collections of Fortran D distributions. To achieve sufficient precision while remaining machine-independent, we will employ a "training set" strategy to produce a performance predictor for each new target machine <ref type="bibr" target="#b77">[79]</ref>.</p><p>3) Debugger: Since Fortran D programs undergo radical transformations during compilation, source-level debugging tools for Fortran D will require significant support to relate the run-time behavior of a transformed program back to its original source, which is considerably more abstract. Also, the definition of Fortran D currently includes a parallel loop construct, the FORALL statement, that permits a limited form of nondeterminism. We expect that programmers using this construct will need some assistance of the kind provided by the shared-memory debugging system for pinpointing data races. 4 ) Automatic Data Distribution Tool: Once completed, the Fortran D compiler will provide a good platform for research on automatic data distribution. We intend to build an experimental system that automatically generates Fortran D distributions. To evaluate this system, we will compile and run the resulting programs, comparing their effectiveness to programs written by hand in Fortran D. If the automatic system produces reasonable results, it will be incorporated into the new editor and used to advise the programmer on initial distribution specifications for data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">) Performance Visualization:</head><p>The final step in parallel programming is performance tuning. It is imperative that the system provide effective tools to help the programmer understand program performance. We plan to adapt analysis tools for Fortran D to provide input to performance visualization systems under development elsewhere, such as the Pablo system at the University of Illinois <ref type="bibr">[SO]</ref>. An important component of performance visualization for Fortran D will be relating performance data for a transformed program back to the more abstract original program source.</p><p>The final result of this effort will be a collection of tools to support the development of machine-independent dataparallel programs. In addition, the new tools for Fortran D will be generalized to handle a subsiantial part of Fortran 90, particularly the array sub-language. At that point, it will be easy to adapt it for use with the emerging High Performance Fortran standard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The ParaScope system is the product of many years of labor. A large collection of students and staff have worked on its design and implementation. Researchers and users from outside the project have provided critical feedback and direction. We are deeply grateful to all these people.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The Parascope compilation system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The Parascope editor</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Transformation taxonomy in PED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. A fragment of parallel Fortran and its corresponding concurrency graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>F</head><label></label><figDesc>n,index,f ,dim,S),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>r e f l e c t i o n p o i n t s a r e computed a s : 2v -v . U R To s a v e s t o r a g e t h e " r e f l e c t e d f a c e " i s s t o r e d i n t h e R RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR R o r i g i n a l s i m p l e x . R do 481313 j = 1, n f ( i n d e x ( 1 ) ) = v a l u e ( i n d e x ( i ) , n, dim, s ) k ( i n d e x ( i ) , j) = 2.d0 * s ( i n d e x ( I 3 ) , J )s ( i n d e x ( i ) , J ) e r m i n e t h e " b e s t " o f t h e r e f l e c t e d v e r t i c e s . C b e s t = 1 i n d e x ( i ) : I3 s t o p p e d i n World/exnon-demo/AccessAnomaly/nelder.bug.f ( i n s u b p r o g r a m m a i n )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>code block D1 parallel do j=l,i [code block El enddo [code block F] enddo [code block G]</head><label></label><figDesc></figDesc><table><row><cell>[code block A]</cell></row><row><cell>parallel do i=2,4</cell></row><row><cell>[code block B]</cell></row><row><cell>if (i.eq.2) then</cell></row><row><cell>parallel do j = 1,2</cell></row><row><cell>[code block C]</cell></row><row><cell>enddo</cell></row><row><cell>endif</cell></row><row><cell>[</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="246" xml:id="foot_0"><p>PROCEEDINGS OF THk IEEk. VOL.. X I . NO 2. FEBRUARY 19Y1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>PROCEEDINGS OF THE IEEE, VOL. 81,NO. 2, FEBRUARY 1993   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>PROCEEDINGS OF THE IEEE, VOL. 81.NO. 2. FEBRUARY 1993   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>2.52PROCEEDINGS OF THE IEEE. VOL X I . NO. 2.FEBRUARY 1993   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>PROCEEDINGS Ob T H t IEtE. VOL 81. NO. 2. FEBRUARY 1YY3</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_5"><p>PROCtEDlNGS OF THE IEEE. VOL 81. NO. 2. FEBRUARY 1993</p></note>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was supported by the CRPC (Center for Research on Parallel Computation, a National Science Foundation Science and Technology Center), DARPA under ONR grant N0014-91-J-1989, the State of Texas, IBM Corporation, and a DARPA/NASA Research Assistantship in Parallel Processing, administered by the Institute for Advanced Computer Studies,</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, he is working as Associate Professor in the Department of Computer Science at Rice University. His research interests include interprocedural analysis and optimization, code optimization for advanced microprocessors, and advanced programming environments.  He is an Adjunct Assistant Professor of computer science at Rice University. He owns and operates Rosetta, Inc., a software development company in Houston, Texas. His research interests include man-machine interaction, objectoriented programming, and programming language implementation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis and transformation of programs for parallel computation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leasure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. COMPSAC 80, 4th Int. Computer Software and Applicurions ConK</title>
		<meeting>COMPSAC 80, 4th Int. Computer Software and Applicurions ConK<address><addrLine>Chicago, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1980-10">Oct. 1980</date>
			<biblScope unit="page" from="709" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PFC: A program to convert Fortran to parallel form</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputers: Design and Applications</title>
		<meeting><address><addrLine>Silver Spring, MD</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="186" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An overview of the PTRAN analysis system for multiprocessing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. First Inr. Con$ Supercomputing</title>
		<meeting>First Inr. Con$ Supercomputing<address><addrLine>Athens, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1987-06">June 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrating scalar optimization and parallelization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M E</forename><surname>Tjiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Pieper</surname></persName>
		</author>
		<author>
			<persName><surname>Hennessy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Workshop Languages und Compilers for Parallel Computing</title>
		<meeting>Fourth Workshop Languages und Compilers for Parallel Computing</meeting>
		<imprint>
			<date type="published" when="1991-08">Aug. 199 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">PCF Fortran: Language definition, version 3.1</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leasure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Parallel Computing Forum</title>
		<imprint>
			<date>Aug</date>
			<pubPlace>Champaign, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PAT-an interactive Fortran par-alleliLing assistant tool</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">W</forename><surname>Appelbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Workshop on Languages and Compilers,for Parallel Computing</title>
		<meeting>Third Workshop on Languages and Compilers,for Parallel Computing<address><addrLine>Irvine, CA; St. Charles, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1988-08">Aug. 1990. Aug. 1988</date>
		</imprint>
	</monogr>
	<note>Proc. 1988 Int. Con$ Parallel Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SUPERB: A tool for semiautomatic MIMD/SIMD parallelization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Bast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gerndt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dependence Anal~sis,forSupercomputing</title>
		<author>
			<persName><forename type="first">U</forename><surname>Banerjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The program dependence graph and its use in optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ferrante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ottenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Structure &lt;$ Computers and Computations</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Kuck</surname></persName>
		</editor>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1978">July 1987. 1978</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">0ptimi;ing Supercompilers for Supercomputers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical dependence testing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Goff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN &apos;91 Conj Programming Language Design urd Implementatiorz</title>
		<imprint>
			<date>June I99</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of interprocedural analysis and optimization in the R&quot; programming environment</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Programming Languages and Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="491" to="523" />
			<date type="published" when="1986-10">Oct. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Automatic and interactive parallelization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kinley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-04">Apr. 1992</date>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Interprocedural transformations for parallel code generation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91</meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Static performance estimation in a parallelizing compiler</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kinley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-12">Dec. 1991</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Comp. Sci., Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR91-174</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing for parallelism and memory hierarchy</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 ACM Int. Con5 Supercomputing</title>
		<meeting>1992 ACM Int. Con5 Supercomputing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interprocedural optimization: Eliminating unnecessary recompilation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Progmmming Languages and Sysr</title>
		<imprint>
			<date type="published" when="1991-04">Apr. 1991</date>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Managing interprocedural optimization</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Interprocedural side-effect analysis in linear time</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN &apos;88 Conj Pro-Rrumming Lunguuges Design und Implementation. SIGPLAN Notices</title>
		<meeting>ACM SIGPLAN &apos;88 Conj Pro-Rrumming Lunguuges Design und Implementation. SIGPLAN Notices</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Analysis of interprocedural side effects in a parallel programming environment</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="517" to="550" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An implementation of interprocedural bounded regular section analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Havlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel and Distributed Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="350" to="360" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A precise inter-procedural data flow algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Myers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Con$ Record of the Eighth Annual ACM Symposium on Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="1981-01">Jan. 198 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The program summary graph and flow-sensitive interprocedural data flow analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN &apos;88 Conf Programming Language Design and Implementation, SIGPLAN Notices</title>
		<meeting>ACM SIGPLAN &apos;88 Conf Programming Language Design and Implementation, SIGPLAN Notices</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting redundant accesses to array data</title>
		<author>
			<persName><forename type="first">E</forename><surname>Granston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veidenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Array privatization for parallel execution of loops</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 ACM Int. Conf Supercomputing</title>
		<meeting>1992 ACM Int. Conf Supercomputing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Incremental dependence analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rosene</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990-03">Mar. 1990</date>
			<pubPlace>Houston, TX</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rice University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An effectiveness study of parallelizing compiler techniques</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eigenmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Blume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 Int. Con$ Parallel Processing</title>
		<meeting>1991 Int. Con$ Parallel essing<address><addrLine>St. Charles, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-08">Aug. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Experiences using the Parascope editor</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Oldham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paleczny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ctr. for Res. on Parallel Computation</title>
		<imprint>
			<date type="published" when="1991-09">Sept. 1991</date>
		</imprint>
		<respStmt>
			<orgName>Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. CRPC-TR91-173</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An efficient way to find the side effects of procedure calls and the aliases of variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Banning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Sixth Annual ACM Symp. Principles of Programming Languages</title>
		<meeting>Sixth Annual ACM Symp. Principles of Programming Languages<address><addrLine>San Antonio, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1979-01">Jan. 1979</date>
			<biblScope unit="page" from="2" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast interprocedural alias analysis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf Record of the Sixteenth Annual ACM Symp. Principles of Programming Languages</title>
		<imprint>
			<date type="published" when="1989-01">Jan. 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Interprocedural constant propagation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN</title>
		<meeting>ACM SIGPLAN</meeting>
		<imprint>
			<biblScope unit="page">86</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m">Symp. Compiler Construction, SIGPLAN Notices</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Aho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Sethi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Compilers, Principles, Techniques and Tools</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting equality of variables in programs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Alpem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Wegman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Zadeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Con$ Record of the Fifleenth ACM Symp. Principles of Programming Languages</title>
		<meeting><address><addrLine>Reading, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1986">1986. 1986</date>
			<biblScope unit="page" from="152" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A catalogue of optimizing transformations</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cocke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Desinn and Optimization of Compilers</title>
		<meeting><address><addrLine>Ende-I</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Procedure cloning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE In?. Conf Comp. Languages</title>
		<meeting>IEEE In?. Conf Comp. Languages</meeting>
		<imprint>
			<date type="published" when="1992-04">Apr. 1992</date>
			<biblScope unit="page" from="96" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The impact of interprocedural analysis and optimization in the R&quot; programming environment</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cytron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN &apos;86 Symp. Compiler Construction, SIGPLAN Notices</title>
		<meeting>ACM SIGPLAN &apos;86 Symp. Compiler Construction, SIGPLAN Notices</meeting>
		<imprint>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
	<note>Interprocedural dependence analysis and parallelization</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient interprocedural analysis for program restructuring for parallel programs</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-C</forename><surname>Yew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIG-PLAN Symp. Parallel Programs: Experience with Applications, Languages and Syst</title>
		<meeting>SIG-PLAN Symp. Parallel Programs: Experience with Applications, Languages and Syst</meeting>
		<imprint>
			<date type="published" when="1988-07">July 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The CONVEX application compiler</title>
		<author>
			<persName><forename type="first">R</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fortran J</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="8" to="10" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Direct parallelization of call statements</title>
		<author>
			<persName><forename type="first">R</forename><surname>Triolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Irigoin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Feautrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN &apos;86 Symp. Compiler Construction, SIGPLAN Notices</title>
		<meeting>ACM SIGPLAN &apos;86 Symp. Compiler Construction, SIGPLAN Notices</meeting>
		<imprint>
			<date type="published" when="1986-07">July 1986</date>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="176" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An experiment with inline substitution</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Software-Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="581" to="601" />
			<date type="published" when="1991-06">June 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unexpected side effects of inline substitution: a case study</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Letters on Programming Languages and Systems</title>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Goaldirected interprocedural optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Briggs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torczon</surname></persName>
		</author>
		<idno>TR90- 148</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. of Comp. Sci</title>
		<imprint>
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
		<respStmt>
			<orgName>Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An empirical investigation of the effectiveness of and limitations of automatic parallelization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Singh</surname></persName>
			<affiliation>
				<orgName type="collaboration">S y ~t</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hennessy</surname></persName>
			<affiliation>
				<orgName type="collaboration">S y ~t</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Shared Memory Multiprocessors</title>
		<meeting>Int. Symp. Shared Memory Multiprocessors<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1986-07">Oct. 1986. 163-275, July 1986. Apr. 1991</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="491" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Parascope editor: User interface goals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Warren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dept. of Comp. Sci</title>
		<imprint>
			<date type="published" when="1990-05">May 1990</date>
		</imprint>
		<respStmt>
			<orgName>Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. &quot;-113</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Analysis and transformation in the Parascope Editor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1991 ACM Int. Con$ Supercomputing</title>
		<meeting>1991 ACM Int. Con$ Supercomputing<address><addrLine>Cologne, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-06">June 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interactive parallel programming using the Parascope editor</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel and Distributed Syst</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="329" to="341" />
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Reading and writing the electronic book</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yankelovitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meyrowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Dam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comp</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="15" to="29" />
			<date type="published" when="1985-10">Oct. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Designing the star user interface</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Irby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kimball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Verplank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Harslem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BYTE</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="242" to="282" />
			<date type="published" when="1982-04">Apr. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A research center for augmenting human intellect</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Engelbart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>English</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AFIPS 1968 Fall Joint Comp. Conf, pp. 3</title>
		<meeting>AFIPS 1968 Fall Joint Comp. Conf, pp. 3</meeting>
		<imprint>
			<date type="published" when="1968">9 5 4 1 0 , Dec. 1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Program editors should not abandon text oriented commands</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="1982-07">July 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Automatic translation of Fortran programs to vector form</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Programming Languages and Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="491" to="542" />
			<date type="published" when="1987-10">Oct. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving register allocation for subscripted variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGPLAN 90</title>
		<meeting>ACM SIGPLAN 90</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Con$ on Programming Language Design and Implementation, SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="53" to="65" />
			<date type="published" when="1990-06">June 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Loop distribution with arbitrary control flow</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Supercomputing &apos;90</title>
		<meeting>Supercomputing &apos;90<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990-11">Nov. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The structure of an advanced retargetable vectorizer</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Leasure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supercomputers: Design and Applications</title>
		<meeting><address><addrLine>Silver Spring, MD</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="163" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Program improvement by source-to-source transformations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Loveman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="121" to="145" />
			<date type="published" when="1977-01">Jan. 1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Loop skewing: The wavefront method revisited</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Parallel Programming</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="279" to="293" />
			<date type="published" when="1986-08">Aug. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">PTOOL: A semi-automatic parallel programming assistant</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porterfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1986 In?. Conf: Parallel Processing</title>
		<meeting>1986 In?. Conf: Parallel essing<address><addrLine>St. Charles, IL</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society Press</publisher>
			<date type="published" when="1986-08">Aug. 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">PTOOL: A system for static analysis of parallelism in programs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Callahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Subhlok</surname></persName>
		</author>
		<idno>TR88-71</idno>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
		<respStmt>
			<orgName>Dept. of Comp. Sci., Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the use of diagnostic dependency-analysis tools in parallel programming: Experiences using PTOOL</title>
		<author>
			<persName><forename type="first">L</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hiromoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lubeck</surname></persName>
		</author>
		<author>
			<persName><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Spang Robinson Report on Supercomputing and Parallel Processing</title>
		<imprint>
			<date type="published" when="1990-01">Jan. 1990</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
	<note>The MIMDizer: A new parallelization tool</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">An evaluation of automatic and interactive parallel programming tools</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pro. Supercomputing &apos;91</title>
		<meeting><address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Faust: An integrated environment for parallel programming</title>
		<author>
			<persName><forename type="first">V</forename><surname>Guama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jablonowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sofhyare</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2G27</biblScope>
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Private communication</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fletcher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Private communication</title>
		<author>
			<persName><forename type="first">J</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991-07">July 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An evaluation of monitoring algorithms for access anomaly detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dinning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schonberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989-07">July 1989</date>
		</imprint>
		<respStmt>
			<orgName>Courant Institute, New York University</orgName>
		</respStmt>
	</monogr>
	<note>Ultracomputer Note 163</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An empirical comparison of monitoring algorithms for access anomaly detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dinning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Schonberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Second ACM SIGPLAN Symp. Principles &amp; Practice of Parallel Programming (PPOPP)</title>
		<imprint>
			<date type="published" when="1990-03">Mar. 1990</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On-the-fly detection of data races for programs with nested fork-join parallelism</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Tools for efficient development of efficient parallel programs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Nudler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">First Israeli Con$ on Computer Systems Eng</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">On-the-fly detection of access anomalies</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schonberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="83" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<author>
			<persName><surname>Cooper</surname></persName>
		</author>
		<title level="m">PARASCOPE PARALLEL PROGRAMMING ENVIRONMENT Proc. ACM SIGPLAN &apos;89 Con$ Programming Language Design Robert T. Hood, (Member, IEEE) received the and Implementation, SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="1976">July 1989. 1976</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
	<note>A degree in mathematics form the University. graduating with highest</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Making asynchronous parallelism safe for distinction, and the Ph.D. degree in computer the world</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Steele</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf Record Seventeenth Annual ACM Symp. science from Cornell University</title>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Principles of Programming Languages</title>
	</analytic>
	<monogr>
		<title level="m">From 1982 to 1992 he was a Faculty Mem</title>
		<imprint>
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">An efficient cache-based access anomaly ber of the Department of Computer Science of detection scheme</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SICbeen employed by Kubota Pacific Computer in PLAN Notices</title>
		<imprint>
			<date type="published" when="1991-04">Apr 1991</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
	<note>Proc. 4th ACM Int. Con$ Architectural Rice University</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hiranandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koelbel</surname></persName>
		</author>
		<author>
			<persName><surname>Kremer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">In particular, he implemented the debugging system that runs under [761 s Hiranandani, K. Kennedy, c. K m l h l , U Kremer, and c . ParaScope. A collaboration with John Mellor-Crummey has enhanced Tseng</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TR90-141, Dept. of Comp. Sci</title>
		<imprint>
			<date type="published" when="1990-12">Dec 1990</date>
		</imprint>
		<respStmt>
			<orgName>Rice Univ.</orgName>
		</respStmt>
	</monogr>
	<note>Tech. programming environments and the debugging of scientific programs Rep. An overview of the Fortran D programming system,&quot; the debugger to provide support for isolating schedule-dependent errors</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m">PrOC. Fourth Workshop on Languages and ComPiiers for in parallel programs At Kubota Pacific Computer, he is using his Parallel Comp</title>
		<meeting><address><addrLine>Santa Clara, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-08">Aug. 1991</date>
		</imprint>
	</monogr>
	<note>Parascope experience in the design and implementation of a programming</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">ComPller OPtlmlzaenvironment for a shared-memory multiprocersor workstation tions for Fortran D on MIMD distributed-memory machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hiranandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Supercomputing &apos;91</title>
		<meeting>Supercomputing &apos;91<address><addrLine>Albuquerque, NM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-11">Nov 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Evaluation of compiler optimizations for Fortran D on MIMD distnbuted-memory machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hiranandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 1992 ACM In1 Con$ Supercomputing</title>
		<meeting>1992 ACM In1 Con$ Supercomputing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992-07">July 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">A static performance estimator to guide data partitioning decisions</title>
		<author>
			<persName><forename type="first">V</forename><surname>Balasundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><surname>Kremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Third ACM SICPLAN Svmp. Principles and Practice of Parallel Programming</title>
		<meeting>Third ACM SICPLAN Svmp. Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="1991-07">July 199 1</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Scalable performance environments for parallel sy\tems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Aydt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T M</forename><surname>Madhyastha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Birkett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Nazief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Totty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc Distributed Memory Comp</title>
		<meeting>Distributed Memory Comp<address><addrLine>Cant, Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991-04">Apr 1991</date>
			<biblScope unit="page" from="562" to="569" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
