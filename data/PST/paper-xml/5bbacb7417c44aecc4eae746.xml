<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A dynamic metaheuristic optimization model inspired by biological nervous systems: Neural network algorithm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Sadollah</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<postCode>11155-9567</postCode>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Hassan</forename><surname>Sayyaadi</surname></persName>
							<email>sayyaadi@sharif.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Sharif University of Technology</orgName>
								<address>
									<postCode>11155-9567</postCode>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anupam</forename><surname>Yadav</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Sciences and Humanities</orgName>
								<orgName type="institution">National Institute of Technology Uttarakhand Srinagar (Garhwal)</orgName>
								<address>
									<postCode>246174</postCode>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A dynamic metaheuristic optimization model inspired by biological nervous systems: Neural network algorithm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B83113F49BDCB2311F3C105F9E683FA</idno>
					<idno type="DOI">10.1016/j.asoc.2018.07.039</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Neural network algorithm</term>
					<term>Artificial neural networks</term>
					<term>Metaheuristics</term>
					<term>Global optimization</term>
					<term>Iterative convergence</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Highlights</head><p> A dynamic optimization model Neural Network Algorithm (NNA) is proposed.</p><p> NNA is inspired by the structure of ANNs and biological nervous systems.</p><p> NNA is a parallel associated memory-based sequential-batch learning optimizer.</p><p> Convergence proof has been carried out for a random initial population.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Among optimization approaches, metaheuristic optimization algorithms have shown their capabilities for finding near-optimal solutions to the numerical real-valued test problems. In contrast, analytical approaches may not detect the optimal solution within a reasonable computational time, especially when the global minimum is surrounded by many local minima.</p><p>Metaheuristic algorithms are usually inspired by observing phenomena and rules seen in nature such as the Genetic Algorithm (GA) <ref type="bibr" target="#b1">[1]</ref>, the Simulated Annealing (SA) <ref type="bibr" target="#b2">[2]</ref>, the Particle Swarm Optimization (PSO) <ref type="bibr" target="#b3">[3]</ref>, the Harmony Search (HS) <ref type="bibr" target="#b4">[4]</ref>, and so forth.</p><p>The GA is based on the genetic process of biological organisms <ref type="bibr" target="#b5">[5]</ref>. Over many generations, natural populations evolve according to the principles of natural selections, i.e. survival of the fittest. In the GA, a potential solution to a problem is represented as a set of parameters. Each independent variable During the reproduction phase, individuals are selected from the population and recombined. Having selected two parents, their chromosomes are recombined, typically using a crossover mechanism. Also, in order to satisfy the population diversity, a mutation operator is applied to some individuals <ref type="bibr" target="#b1">[1]</ref>. The GA has been utilized for solving various optimization problems in the literature and it is a well-known optimization method <ref type="bibr" target="#b6">[6]</ref><ref type="bibr" target="#b7">[7]</ref><ref type="bibr" target="#b8">[8]</ref><ref type="bibr" target="#b9">[9]</ref>.</p><p>The origins of SA lay in the analogy of optimization and a physical annealing process <ref type="bibr" target="#b2">[2]</ref>. Annealing refers to an analogy with thermodynamics, specifically with the way that metals cool and anneal. The SA is basically hill-climbing except instead of picking the best move, it picks a random move. If the selected move improves the solution, then it is always accepted. Otherwise, the algorithm makes the move anyway with some probability less than one. The probability decreases exponentially with the badness of the move, which is the amount of ∆ by which the solution is worsened. A parameter T is also used to determine this probability. At higher values of T, uphill moves are more likely to occur.</p><p>As T tends to zero, they become more and more unlikely. In a typical SA optimization, T starts with a high value and then, its value is gradually decreased according to an annealing schedule <ref type="bibr" target="#b2">[2]</ref>. The SA is useful in finding global optima in the presence of large numbers of local optima and it is applied for solving a wide range of optimization problems <ref type="bibr" target="#b10">[10]</ref><ref type="bibr" target="#b11">[11]</ref><ref type="bibr" target="#b12">[12]</ref>.</p><p>The PSO is an evolutionary optimization method, developed by Kennedy and Eberhart <ref type="bibr" target="#b3">[3]</ref>, for solving global optimization problems. In the PSO, simple individuals, called particles, move in the search space of an optimization problem. The position of a particle represents a candidate solution to the optimization problem at hand. Each particle searches for better positions in the search space by changing its velocity according to rules (i.e., population cooperation and competition) originally inspired by behavioral models of bird flocking. Researchers found that the synchrony of animal's behavior was through maintaining optimal distances between individual members and their neighbors <ref type="bibr" target="#b13">[13]</ref>. The PSO has proved its efficiency for handling real-life optimization problems and its variants have been developed considering design improvements for many engineering cases <ref type="bibr" target="#b14">[14]</ref><ref type="bibr" target="#b15">[15]</ref><ref type="bibr" target="#b16">[16]</ref>.</p><p>Geem et al. <ref type="bibr" target="#b4">[4]</ref> developed the HS that reproduces the musical process of searching for a perfect state of harmony. The harmony in music is analogous to the optimum design, and the musicians' improvisation is analogous to local/global search schemes. The engineering applications of HS shows its popularity and excellent performance <ref type="bibr" target="#b17">[17]</ref><ref type="bibr" target="#b18">[18]</ref><ref type="bibr" target="#b19">[19]</ref><ref type="bibr" target="#b20">[20]</ref>.</p><p>In spite of existing such metaheuristic methods, however, there are still gaps in developing efficient parameter free optimization methods having fast, matured convergence and capable of obtaining high solution quality. Being a user-parameter free optimization method is accounted as one of the most important aspects for every metaheuristic algorithm and it is a very significant area for future research, therefore, that deserves a lot more attention. In order to overcome those concerns which still This paper introduces a novel metaheuristic optimization algorithm for optimal problem solving. The proposed method is called neural network algorithm (NNA), which is inspired by the concepts of artificial neural networks (ANNs) and biological nervous systems. The NNA employs the structure and concept of ANNs to generate new candidate solutions and also other operators used in the ANNs.</p><p>The proposed method is examined using 21 unconstrained benchmarks and several constrained engineering design problems, and its efficiency and superiority are highlighted against other reported optimizers along with statistical tests.</p><p>The remaining of this paper is organized as follows: In Section 2, brief introduction of ANNs and concepts of the proposed NNA are presented in detail. Validation and efficiency of the proposed method in finding optimal solutions are given in Section 3. In this section, 21 unconstrained benchmarks and several constrained engineering design problems have been examined and the obtained optimization results have been compared with the state-of-the-art and recent algorithms in the literature. Furthermore, sensitivity analysis over user parameters of used optimizers and computational complexity of the proposed optimizer have been given in this section. Finally, this paper ends with the conclusions section along with some future research lines. In Appendix, the relationship between improvised exploitation and each parameter under asymmetric interval is proved mathematically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural network algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Basic idea</head><p>Artificial neural networks (ANNs) are computational models inspired by the structure and/or functional aspects of biological neural networks. The ANNs consist of dense interconnected computing units (i.e., artificial neurons) that are motivated by biological nervous systems <ref type="bibr">[21]</ref>. As in nature, the connections among units largely determine the network function.</p><p>Depending on their connectivity pattern (architecture), the ANNs can be grouped into the following two categories: a) feed-forward neural networks: these are networks whose architecture has no loops.</p><p>In general, the feed-forward networks are "static" because they produce only one set of output values rather than a sequence of values from a given input data set; b) recurrent networks: these are networks in which loops occur because of feedback connections, applying feedbacks means that a time parameter implicitly enters the model, and in this sense these neural networks are "dynamic" <ref type="bibr" target="#b22">[22]</ref>.</p><p>Recurrent networks have developed two kinds of feedback connections for neural networks: 1) local feedbacks: these are links that pass the output of a neuron to itself; 2) global feedbacks: these are links that pass the output of a neuron to other neurons in the same or lower layers in the multilayer network architecture <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b24">24]</ref>. Fig. <ref type="figure" target="#fig_3">1</ref> shows two typical architectures of ANNs for feed forward and recurrent  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2.</head><p>Proposed NNA Similar to other metaheuristic optimization algorithms, the NNA begins with an initial population so called population of pattern solutions. As the ANNs mostly used for prediction purposes, it receives input data and target data and predicts the relationship among input and target data (i.e., mapping input date to target data). Usually, inputs of the ANNs are input values obtained by experiments, calculations, and so forth.</p><p>The ANNs, simply speaking, tries to map input data to the target data. Therefore, the ANNs tries to reduce the error (e.g., mean square error) among predicted solutions and target solutions using iteratively changing the values of weight functions (wij) (see Fig. <ref type="figure" target="#fig_3">1b</ref>). However, in the optimization, the goal is to find the optimum solution, and a metaheuristic algorithm should search a feasible optimal solution using a defined strategy.</p><p>Therefore, inspired by the ANNs, in the NNA, the best obtained solution at each iteration (i.e., temporal optimal solution) is assumed as target data and the aim is to reduce the error among the target data and other predicated pattern solutions (i.e., moving other predicted pattern solutions towards the target solution). Based on the defined concept, the NNA is developed for minimization problems (i.e., minimizing the error among the target and pattern solutions). It is worth pointing out that this target solution has been updated at each iteration. In this paper, the structure of the ANNs, some parts of its mathematical formulations and concepts, all together are coming to develop a new </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Generating initial population</head><p>In order to solve an optimization problem, it may be necessary that the values of decision variables be represented as an array. Before explaining the NNA processes, the key terms used to describe this algorithm should be introduced. Each individual or agent, a set containing a value of each optimization variable, is called "pattern solution" (e.g., in the GA, this array is called "Chromosome"). In D dimensional optimization problem, a pattern solution is an array of 1×D, representing input data in the NNA. This array is defined as given follows:</p><formula xml:id="formula_0">Pattern Solution = [x1, x2, x3,…, xD].<label>(1)</label></formula><p>Indeed, population of pattern solutions corresponds to input data in the ANNs. To start the optimization algorithm, a candidate of pattern solution matrix with size Npop ×D is generated. Hence, matrix X which is randomly generated between lower and upper bounds of a problem (i.e., lower and upper bounds are assumed to be defined by a decision maker), is given as follows (rows and column are the population size (Npop) and dimension size (D), respectively): </p><formula xml:id="formula_1">1 1 1 1 1 2 3 2 2 2 2 1 2 3 1 2 3 pop pop pop pop D D N N N N D x x x x x x x x Population of Pattern Solutions X x x x x         . (<label>2</label></formula><formula xml:id="formula_2">i i i i D pop C f x x x i N   . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where f is the objective function. In the entire paper, notations having a vector sign are corresponded as vector values (array), otherwise the rest of notations and parameters are considered scalar values.</p><p>After calculating the cost function (fitness function) for all pattern solutions, then find the best pattern solution (in this paper, a candidate solution with the minimum objective function value) considered as the target solution.</p><p>The NNA resembles ANNs having Npop input data having D dimension(s) and only one target data (response) (see Fig. <ref type="figure" target="#fig_3">1a</ref>). After setting the target solution (X Target ) among the other pattern solutions, the target weight (W Target ), the weight corresponding to the target solution, has to be selected from the population of weight (weight matrix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_4">M A N U S C R I P T 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Weight matrix</head><p>In the ANNs, the artificial neurons or the processing units may have several input paths, corresponding to the dendrites. Using a simple summation, the unit combines the weighted values of these input paths. The result is an internal activity level for the unit <ref type="bibr" target="#b25">[25]</ref>.</p><p>The output path of a unit may be connected to the input path of other units through connection weights which correspond to the synaptic strength of the biological neural connections. Each connection has a corresponding weight (w) (see Fig. <ref type="figure" target="#fig_3">1b</ref>), where the signals on the input lines to a unit are modified or weighted prior to being summed <ref type="bibr" target="#b26">[26]</ref>.</p><p>Initial weights in ANNs are random numbers and when the iteration number is increasing, they will be updated considering the calculated error of the network. Back to the NNA, initial weights are defined as given in the following equation: </p><formula xml:id="formula_5">N i iN N i iN N N i N iN N N N N N w w w w w w w w w www W t W W W w w w w w w                  , (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where W is a square matrix (Npop×Npop) which generates random numbers uniformly between zero to one during iterations, and t is an iteration index. The first subscript of weight relates to its pattern solution (e.g., 2</p><p>w  relates to the second pattern solution) and the second subscript of weight is shared with the other pattern solutions (e.g., 23</p><p>w is shared with the third pattern solution). Every pattern solution has its corresponding weight value which has been involved it for generating a new candidate solution.</p><p>However, there is a constraint for the weight values. The imposed constraint is the summation of weights for a pattern solution should not exceed one, mathematically, it can be defined as given follows:</p><p>1</p><formula xml:id="formula_7">( ) 1, 1, 2,3,..., Npop ij pop j w t i N    . (<label>5</label></formula><formula xml:id="formula_8">) (0,1) , 1, 2,3,..., ij pop w U i j N  (6)</formula><p>Weight values are belonging to uniformly distributed random numbers between zero and one (Eq. ( <ref type="formula">6</ref>))</p><p>where their summation for a pattern solution should not exceed one (Eq. ( <ref type="formula" target="#formula_7">5</ref>)). Existence of such a constraint for weight values is due to control the bias of movement and generating new pattern solutions (new individuals). Without this constraint, the weight values tend to grow (i.e., values more than one) in a specific direction and therefore, the algorithm will be stuck in a local optimum point (For instance, act as pheromone parameter in the ant colony optimization (ACO) when a route has lots of pheromones for attracting other ants). Having this constraint gives the NNA's agents controlled</p><formula xml:id="formula_9">A C C E P T E D M A N U S C R I P T</formula><p>movement with mild bias (varying from zero to one). After forming the weight matrix (W), new pattern solutions (X New ) are calculated using the following equation inspired by the weight summation technique used in the ANNs:</p><formula xml:id="formula_10">1 ( 1) ( ) ( ), 1, 2,3,..., Npop New j ij i pop i X t w t X t j N       ,<label>(7)</label></formula><p>( 1) ( ) ( 1), 1, 2,3,...,</p><formula xml:id="formula_11">New i i i pop X t X t X t i N      , (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where t is an iteration index. Therefore, the new pattern solution has been updated for iteration t + 1 using Eqs. ( <ref type="formula" target="#formula_10">7</ref>) and ( <ref type="formula" target="#formula_11">8</ref>). In the following, an example has been provided. For instance, if we have six pattern solutions (i.e., six neurons, population size of 6), updating the first new pattern solution can be calculated as given follows: ( 1)</p><formula xml:id="formula_13">( ) ( ) ( ) ( ) ( ) ( ) New X t w X t w X t w X t w X t w X t w X t        . (<label>9</label></formula><formula xml:id="formula_14">)</formula><p>Also, for further clarification, Fig. <ref type="figure">2</ref> depicts how the NNA forms its new population of pattern solutions for D dimension(s).</p><p>Fig. <ref type="figure">2</ref>. Schematic view of generating new pattern solutions using Eqs. ( <ref type="formula" target="#formula_10">7</ref>) and <ref type="bibr" target="#b8">(8)</ref>.</p><p>After creating the new pattern solutions from the previous population of patterns, based on the best weight value so called "target weight", the weight matrix should be updated as well. The following equations suggest an updating equation for the weight matrix:</p><formula xml:id="formula_15">g ( 1) ( ) 2 ( ( ) ( )), 1, 2,3,..., Updated Tar et i i i pop W t W t rand W t W t i N        , (<label>10</label></formula><formula xml:id="formula_16">) A C C E P T E D M A N U S C R I P T</formula><p>It is noted that weight matrix should always satisfy the constraints ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula">6</ref>) during the optimization process</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Bias operator</head><p>The bias current plays a vital role in the dynamics of the neural networks model. By virtue of its role, the bias current is always tied to a surrounding condition (e.g., noise), so as to make the output of each neuron respect the surrounding condition <ref type="bibr" target="#b27">[27]</ref>. In the NNA, the bias operator modifies a certain percentage of the pattern solutions in the new population of pattern solutions (</p><p>New i</p><p>Xt  ) and updated weight matrix (</p><p>( 1)</p><formula xml:id="formula_18">updated i</formula><p>Wt  ) (acting as a noise). In other words, the bias operator in the NNA is another way to explore the search space (exploration process) and it acts similar to the mutation operator in the GA.</p><p>In general, the bias operator prevents the algorithm from premature convergence (especially at early iterations) and modifies a number of individuals in the population. In fact, the bias operator acts as a noise to the new pattern solutions (Eq. ( <ref type="formula" target="#formula_10">7</ref>)) and updated weight matrix (Eq. ( <ref type="formula" target="#formula_15">10</ref>)). For this purpose, the Pseudo code given in Table <ref type="table" target="#tab_2">1</ref> has been applied to new pattern solutions and updated weight matrix. Table <ref type="table" target="#tab_2">1</ref>. Suggested strategy for the bias operator applied to new input solutions and updated weight matrix.   <ref type="table" target="#tab_2">1</ref>, β is a modification factor, which determines the percentage of the pattern solutions that should be altered. The initial value of β is set to 1 (means 100 percentage chance to modify all individuals in population) and its value adaptively has been reduced at each iteration using any reduction formulation as suggested follows:</p><formula xml:id="formula_19">For i = 1 to Npop If rand ≤ β %% -------------Bias for New Pattern Solution ------------------------------------------------ Nb = Round (</formula><p>( 1)</p><formula xml:id="formula_20">( ) 0.99 1, 2,3,..., _ t t t Max Iteration     <label>(11)</label></formula><p>( 1) 1  <ref type="formula" target="#formula_20">11</ref>) and ( <ref type="formula">12</ref>), or any reduction equation can be used for this purpose. In this paper, Eq. ( <ref type="formula" target="#formula_20">11</ref>)</p><formula xml:id="formula_21">     (12) A C C E P T E D M A N U S C R I P T Both Eqs. (</formula><p>is used as reduction pattern for β. The bias operator adaptively is decreased to allow the algorithm searching for optimum solution near to the target solution and also avoid drastic changes in the pattern solutions at final iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Transfer function operator</head><p>In the NNA, unlike ANNs, transfer function operator transfers the new pattern solutions in the population from their current positions in the search space to new positions in order to update and generate better quality solutions toward the target solution. The improvement of the solutions is made by moving the current new pattern solutions closer to the best solution (target solution). Therefore, the following equation is defined as a transfer function operator (TF) for the proposed method given as follows:</p><p>* arg</p><p>( 1) ( ( <ref type="formula" target="#formula_0">1</ref>)</p><formula xml:id="formula_22">) ( 1) 2 ( ( )<label>( 1)</label></formula><p>), 1, 2,3,...,</p><formula xml:id="formula_23">T et i i i i pop X t TF X t X t rand X t X t i N            . (<label>13</label></formula><formula xml:id="formula_24">)</formula><p>The constant value of two in Eq. ( <ref type="formula" target="#formula_23">13</ref>) is logically chosen as for suggested in optimization updating equations (e.g., PSO and ICA). In order to transfer the updated pattern solution, value of two gives the chance of searching before and after target solution. For instance, if the value is chosen as one, the improved solution by transfer function operator moves toward the target solution from one side (between zero to one by rand operator), however it is not passing the target solution and exploring the other side of target solution. Therefore, the value of two is a constant value during optimization task.</p><p>Hence, using Eq. ( <ref type="formula" target="#formula_23">13</ref>), the new pattern solution i th ( ( 1)</p><p>i Xt  ) is transferred from its current position in the search space to its updated position ( * ( 1) i Xt  ). Table <ref type="table">2</ref> describes the process and collaboration of both bias and TF operators in the NNA in detail.</p><p>Table <ref type="table">2</ref>. Combination of Bias and TF operators in the NNA. <ref type="figure">----------------Transfer Function (TF) Operator ---------------------</ref>Apply Eq. ( <ref type="formula" target="#formula_23">13</ref>) End If End For As can be seen in Table <ref type="table">2</ref>, at early iterations, there exists more chances for the bias operator generating new pattern solutions (more opportunities for discovering unvisited pattern solutions) and also new weight values. However, when the iteration number is increasing, this chance decreases, and the TF operator plays more important roles in the NNA especially at final iterations.  Looking at Fig. <ref type="figure" target="#fig_8">3</ref>, "W", "Bias", and "TF" are the weight matrix, bias, and transfer function operators, respectively. Moreover, schematic view of the proposed NNA representing its functionality is shown in Fig. <ref type="figure" target="#fig_10">4</ref>. By observing Fig. <ref type="figure" target="#fig_10">4</ref>, the NNA has self-feedback (i.e., local feedback) in addition to having feedback to the other neurons (i.e., global feedback). By observing Fig. <ref type="figure" target="#fig_10">4</ref>, local and global feedbacks are shown in dashed and solid lines, respectively. Furthermore, the weight matrix (see Eq. <ref type="bibr" target="#b10">(10)</ref>) is modified at each iteration (i.e., the elements of the weight matrix change during the optimization process). In fact, the NNA is considered as a parallel sequential-batch learning method using global and local feedbacks.  </p><formula xml:id="formula_25">For i = 1 to Npop If rand ≤ β %% -----------------Bias Operator -------------------------------------------- Bias Operator (see subsection 2.2.3) Else (rand &gt; β) %% -</formula><formula xml:id="formula_26">i i pop X t t f X t P t i N     , (<label>14</label></formula><formula xml:id="formula_27">)</formula><p>where Xi(t+∆t) and Xi(t) are next and current locations of pattern solution i th , respectively. P(t) is population of pattern solutions with updated weights. Eq. ( <ref type="formula" target="#formula_26">14</ref>) shows the general trend of NNA as a dynamic optimization model. It is worth pointing out that using the concepts of ANNs and its strategies used in the NNA, it is considered as associated memory based algorithm by using global and local feedbacks in its structure. Furthermore, the steps of the proposed method are summarized as follows:</p><p>Step 1: Choose the number of pattern solutions (i.e., population size) and maximum number of iterations (i.e., NFEs).</p><p>Step 2: Randomly generate an initial population of pattern solution between LB and UB.</p><p>Step 3: Calculate the cost of initial pattern solutions. Step 4: Randomly generate the weight matrix (initialization phase) between zero and one considering the imposed constraint (see Eqs. ( <ref type="formula" target="#formula_7">5</ref>) and ( <ref type="formula">6</ref>)).</p><p>Step 5: Set target solution (X Target ) (the minimum value for minimization problems) and its corresponding target weight (W Target ).</p><p>Step 6: Generate new pattern solutions (X New ) and update the pattern solutions using Eqs. ( <ref type="formula" target="#formula_10">7</ref>) and (8).</p><p>Step 7: Update the weight matrix (W) using Eq. ( <ref type="formula" target="#formula_15">10</ref>) considering the applied constraints (see Eqs. <ref type="bibr" target="#b5">(5)</ref> and ( <ref type="formula">6</ref>)).</p><p>Step 8: Check the bias condition. If rand ≤ β, performs the bias operator for both new pattern solutions and updated weight matrix (see Table <ref type="table" target="#tab_2">1</ref>).</p><p>Step 8: Otherwise (rand &gt; β), apply the transfer function operator (TF) for updating new position of pattern solutions ( * i X ) using Eq. ( <ref type="formula" target="#formula_23">13</ref>) (see Table <ref type="table">2</ref>).</p><p>Step 10: Calculate the objective function value for all updated pattern solutions.</p><p>Step 11: Update the target solution (i.e., temporal best solution) and its corresponding target weight.</p><p>Step 12: Update the value of β using any reduction formulation (e.g., Eq. ( <ref type="formula" target="#formula_20">11</ref>))</p><p>Step 13: Check predefined stopping condition. If the stopping criterion is satisfied, the NNA stops.</p><p>Otherwise, return to the Step 6.</p><p>Interestingly, the NNA can be easily applied for solving combinatorial optimization problem (i.e., discrete design variables). For this purpose, by choosing binary values of zero and one for the weight matrix (W), the NNA can be utilized to solve discrete optimization problems such as scheduling and assignment optimization problems.</p><p>Due to complex computation among neurons and input solutions in the NNA, improper selection of target solution and target weight (i.e., local optima) can be compensated with the complex performance of the neuron (inspired by the ANNs) during optimization process in the NNA (i.e., graceful degradation). In fact, this behavior underlies in the feature of ANNs model. This definition is extracted from the ANNs, and since the NNA is inspired by the ANNs structure and model, the same behavior can be expected. In fact, graceful degradation is the ability of a computer, electronic system or network to maintain limited functionality even when a large portion of it has been destroyed or rendered inoperative. In graceful degradation, the operating efficiency or speed declines gradually as an increasing number of components fail. Parallel structure and having local and global feedbacks through complex performance of the neurons give the NNA such a flexibility to ensure about the quality of the generated solutions even with the presence of low quality solutions.</p><p>Based on the ANNs terminology, the NNA is an adaptive unsupervised method for solving optimization problems. Unsupervised in NNA means there is no clue and information of global optimum and the solutions have been updated by learning from the environment.</p><p>The NNA is a single-layer perceptron optimization method having self-feedback. In the proposed method, there are no sensitive initial parameters (except the common user parameters: population size</p><formula xml:id="formula_28">A C C E P T E D M A N U S C R I P T</formula><p>and maximum number of iteration or NFEs), therefore, a user does not require to fine tune or examine numerous set of initial parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.6.">NNA vs. ANNs</head><p>The NNA is an optimization method for finding an optimum solution for a given problem. However, the ANNs is an information processing paradigm that is inspired by the biological nervous systems such as the brain. In fact, as an inspiring idea, the NNA has borrowed the concept and computational model of feed forward ANNs for optimization task. Furthermore, the following highlights are the differences between the NNA and ANNs in an itemized way:</p><p> Feed forward ANNs is used for prediction, while the NNA (as a global optimizer) is utilized for finding optimum solution of an optimization problem.</p><p> Unlike feed forward NNA, the NNA has local and global feedback.</p><p> Transfer function used in the ANNs translates the input signals to output signals, while in the NNA, the transfer function operator is used for updating new positions of pattern solutions at each iteration.</p><p> A bias in ANNs is an extra neuron added to each hidden layer that stores the value of one, unlike NNA which the bias operator is applied for having more chances for exploration phase in search space.</p><p> Unlike ANNs, the NNA can be applied for optimal solving of constrained and multi-objective optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Validation of the NNA</head><p>The proposed NNA was implemented and coded in MATLAB programming software. In order to have fair and significant comparison, the most applied optimizers from state-of-the-art approaches to modern and recent optimization methods in the literature have been taken into account. In this paper, thirteen optimization methods including the proposed method have been considered in the comparison pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sensitivity analysis</head><p>In order to observe the effects of different values of common user parameters of the NNA (i.e., population size and maximum number of iteration), sensitivity analysis has been performed. Also, sensitivity analysis over user parameters of other reported optimizers has been given in this section. In this paper, the Taguchi method of design of experiment <ref type="bibr" target="#b28">[28]</ref> is employed to evaluate the influence of selected values on the performance of the proposed optimizer.</p><p>The Taguchi method is a fractional factorial experiment introduced by Taguchi as an efficient alternative for full factorial experiments. The Taguchi method focuses on the level combinations of the control parameters to minimize the effects of the noise factors. In the Taguchi's parameter design phase, an experimental design is used to arrange the control and noise factors in the inner and outer orthogonal arrays, respectively. Afterward, the signal-to-noise (S/N) ratio is computed for each</p><formula xml:id="formula_29">A C C E P T E D M A N U S C R I P T</formula><p>experimental combination. After the calculation of the S/N ratios, these ratios are analyzed to determine the optimal control factor for the level combination.</p><p>The Taguchi method categorizes an objective function into three groups: First, "smaller is better" for which the objective function is minimization. Second, "nominal is the best", for which the objective function has the modest variance around its target. Third, "bigger is better", where the objective function is maximization. Since, the objective functions used in this paper are minimization problems, the ''smaller is better'' option is appropriate, given as follows <ref type="bibr" target="#b28">[28]</ref>:</p><formula xml:id="formula_30">2 10 1 1 / 10log n e e S N C n       , (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>where Ce is the objective function value of a given experiment e, and n is the number of times the experiment is performed. Minitab software has been utilized for this purpose. Two common user parameters of NNA that are required for calibrations are the population size (Npop) and maximum number of iteration. Table <ref type="table">3</ref> shows the algorithms' parameters, each at three levels with nine observations represented by L9.</p><p>Benchmark F1 (given in Section 3.2) has been examined using the given levels for all reported optimizers under 30 independent runs. Fig. <ref type="figure" target="#fig_7">5</ref> depicts the average S/N ratio plot for different parameter levels for all applied algorithms. By observing Fig. <ref type="figure" target="#fig_7">5</ref>, the best parameter levels have the highest mean of S/N values. Therefore, based on the results seen from Fig. <ref type="figure" target="#fig_7">5</ref>, Table <ref type="table" target="#tab_5">4</ref> tabulates the optimal values of the initial parameters for all considered optimizers. Table <ref type="table">3</ref>. Considered parameters and their levels for fine tuning initial parameters of used optimizer.   As it can be seen from Fig. <ref type="figure" target="#fig_7">5</ref> and Table <ref type="table" target="#tab_5">4</ref>, all optimizers obtained better solutions when the maximum number of iteration is large and population size is increasing. In fact, algorithms having more individuals (population size) and time (in terms of function evaluations) explore search space efficiently which causes finding better solutions. Talking about the NNA, the same trend can be seen for the population size and the maximum number of iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters</head><formula xml:id="formula_32">A C C E P T E D M A N U S C R I P T A C C E P T E D M A N U S C R I P T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">NNA for solving unconstrained benchmark problems</head><p>In this research, 21 well-known benchmark minimization problems extracted from a special issue in the literature <ref type="bibr" target="#b29">[29]</ref> have been investigated to demonstrate the efficiency and robustness of the proposed algorithm. The obtained solutions are compared with the results obtained from other optimization methods in terms of statistical results and statistical tests which are shown in tables and figures.</p><p>The dimensions of benchmark functions were 50 to 200. The optimal solution results, f(x * ), were known for all benchmark functions. Properties of these functions are represented in Tables <ref type="table">5 to 7</ref>.</p><p>Required explanations regarding hybrid composition functions (F12-F21) have been fully described in the literature <ref type="bibr" target="#b29">[29]</ref>. It is worth pointing out that, in terms of complexity, functions F12 to F21 possess higher level of complication compared with their original shifted functions (F1 to F11).</p><formula xml:id="formula_33">A C C E P T E D M A N U S C R I P T Table 5. Benchmark functions F1 to F11.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Name Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F1</head><p>Shifted Hyper Sphere</p><formula xml:id="formula_34">21 1 _, D i i z f bias z x o      F2 Shifted Schwefel 2.21   max ,1 _ , ii z i D f bias z x o      F3 Shifted Rosenbrock   2 2 2 1 1 100( ) ( 1) _ , D i i i i z z z f bias z x o          F4 Shifted Rastrigin 2 1 (z 10cos(2 ) 10) _ , D ii i z f bias z x o         F5 Shifted Griewank 2 1 1 cos 1 _ , 4000 D D ii i i zz f bias z x o i             F6 Shifted Ackley 2 1 1 1 20exp( 0.2 ) exp( cos(2 )) 20 _ , D i D i i i z z e f bias z x o D D              F7 Schwefel 2.22 1 1 D D ii i i zz     F8 Schwefel 1.2 2 11 Di j ij z      F9 Extended f10         1 0.25 0.1 2 2 2 2 2 10 1 10 1 10 1 ( , )<label>( , ), sin 50 1 D</label></formula><formula xml:id="formula_35">i i m i f z z f z z f x y x y                F10 Bohachevsky   1 22 11 1 2</formula><p>0.3cos(3 ) 0.4cos(4 ) 0.7</p><formula xml:id="formula_36">D i i i i i z z z z          F11 Schaffer         1 0.25 0.1 2 2 2 2 2 11 1 sin 50 1 D i i i i i z z z z       </formula><p>1 Shifted point Table <ref type="table">6</ref>. Hybrid composition benchmark functions F12 to F21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function First Function Second Function Weight Factor</head><formula xml:id="formula_37">F12 F9 + F1 0.25 F13 F9 + F3 0.25 F14 F9 + F4 0.25 F15 F10 + F7 0.25 F16 F5 + F1 0.50 F17 F3 + F4 0.50 F18 F9 + F1 0.75 F19 F9 + F3 0.75 F20 F9 + F4 0.75 F21 F10 + F7 0.75 A C C E P T E D M A N U S C R I P T Table 7</formula><p>. Properties of F1 to F21. "U", "M", and "N/A" stand for unimodal and multimodal, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Function Range Optimum (f (x</head><formula xml:id="formula_38">* )) U/M Separable Shifted f_bias F1 [-100,100] D 0 U Yes Yes -450 F2 [-100,100] D 0 U No Yes -450 F3 [-100,100] D 0 M Yes Yes 390 F4 [-5,5] D 0 M Yes Yes -330 F5 [-600,600] D 0 M No Yes -180 F6 [-32,32] D 0 M Yes Yes -140 F7 [-10,10] D 0 U Yes No - F8 [-65.536,65.536] D 0 U No No - F9 [-100,100] D 0 U No No - F10 [-15,15] D 0 U Yes No - F11 [-100,100] D 0 U Yes No - F12 [-100,100] D 0 U No Yes - F13 [-100,100] D 0 M No Yes - F14 [-5,5] D 0 M No Yes - F15 [-10,10] D 0 U Yes No - F16 [-100,100] D 0 M No Yes - F17 [-10,10] D 0 M Yes Yes - F18 [-100,100] D 0 U No Yes - F19 [-100,100] D 0 M No Yes - F20 [-5,5] D 0 M No Yes - F21 [-10,10] D 0 U Yes No -</formula><p>Talking about maximum number of function evaluations (NFEs), considered as stopping condition in this paper, the predefined NFEs is 5000 multiples by dimension size (D) for each function <ref type="bibr" target="#b29">[29]</ref>. The task of optimizing for each algorithm was executed in 30 independent runs.</p><p>Therefore, in addition to NNA, the applied algorithms, from the state-of-the-art algorithms to modern optimizers, include the GA <ref type="bibr" target="#b1">[1]</ref>, SA <ref type="bibr" target="#b2">[2]</ref>, PSO <ref type="bibr" target="#b3">[3]</ref>, HS <ref type="bibr" target="#b4">[4]</ref>, Imperialist Competitive Algorithm (ICA) <ref type="bibr" target="#b30">[30]</ref>, Gravitational Search Algorithm (GSA) <ref type="bibr" target="#b31">[31]</ref>, Water Cycle Algorithm (WCA) <ref type="bibr" target="#b32">[32]</ref>, Cuckoo</p><p>Search <ref type="bibr" target="#b33">[33]</ref>, Teaching-Learning-Based Optimization (TLBO) <ref type="bibr" target="#b34">[34]</ref>, Differential Evolution <ref type="bibr" target="#b35">[35]</ref>, and Covariance Matrix Adaptation Evolution Strategy (CMA-ES) <ref type="bibr" target="#b36">[36]</ref>. All the optimizers in this paper have been coded and implemented in MATLAB programming software. For having fair comparison, for all applied algorithm, population size was set to 50.</p><p>Regarding the initial parameters of reported optimizers, suggested user parameters given in Section 3.1 have been utilized for optimization task. To appropriately evaluate the performance of the proposed NNA compared with the other state-of-art algorithms, four quality indicators are utilized.</p><p>The first one is the value-based method which is the solution quality in terms of four performance metrics including the best, average, median, worst, and standard deviation (SD).</p><p>The second metric is the rank-based method, which have been suggested by different authors in the literature. In this paper, the Friedman test <ref type="bibr" target="#b37">[37]</ref> which is a nonparametric statistical test is used to distinguish the differences among reported algorithms. Therefore, the average rankings of the algorithms according to the Friedman test are reported.</p><formula xml:id="formula_39">A C C E P T E D M A N U S C R I P T</formula><p>The third and fourth metrics are Kruskal-Wallis test <ref type="bibr" target="#b38">[38]</ref> and Multiple Comparison Test <ref type="bibr" target="#b39">[39]</ref>. Tables <ref type="table">8</ref> to 13 tabulate the obtained optimization results using different optimizers for the benchmarks given in Tables <ref type="table">5</ref> and<ref type="table">6</ref> with dimensions of 50 to 200. Looking at Tables <ref type="table" target="#tab_2">8 to 13</ref>, performance of well-used optimizers such as GA, PSO, HS has not surpassed the results of recent optimizers for the most reported functions. Therefore, the competition is mostly among recent developed optimization methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A C C E P T E D</head><formula xml:id="formula_40">M A N U S C R I P T Error SD D=50 F1 F2 F3 GA 1.71E-</formula><formula xml:id="formula_41">A C C E P T E D M A N U S C R I P</formula><p>Although, reporting the statistical results gives us a good sense of how an algorithm performs, the optimization results given in Tables 8 to 13 do not show the significance of an algorithm over the other. Therefore, in the following section, in addition to the optimizers given in Tables 8 to 13, four recent optimizers, i.e., Cuckoo Search (CS), Teaching-Learning-Based Optimization (TLBO), Differential Evolution (DE) <ref type="bibr" target="#b35">[35]</ref>, and Covariance Matrix Adaptation Evolution Strategy (CMA-ES) <ref type="bibr" target="#b36">[36]</ref>, are added to the comparison pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">NNA comparing with other optimizers</head><p>Recently, an optimizer has been proposed in the literature with no initial parameters <ref type="bibr" target="#b34">[34]</ref>. The idea of having a free-parameter optimizer is a challenging and interesting field in the optimization community, however, this important matter is hard to attain. Having user parameters gives an algorithm the maximum flexibly to adjust itself to a given problem having different level of complexity. However, working on user parameter free algorithms is highly appreciated in case of having design improvements.</p><p>Comparing the NNA with two different optimization method is focused on two aspects of NNA.</p><p>Firstly, The NNA does not require to fine tune user parameters, therefore, a recently developed optimizer with no user parameters named as Teaching-Learning-Based optimization algorithm (TLBO) <ref type="bibr" target="#b34">[34]</ref> is compared with the NNA. Regarding the second aspect of NNA, the exploration phase in NNA is based on random search between LB and UB. It is a simple exploration approach which one can use in any optimizer, therefore, based on exploration phases of NNA, a simple random search (RS) is also compared with the NNA. One may claim that the NNA is based on random search or random walk, this simple comparison shows the huge gap between the results obtained by RS and NNA. Hence, we can conclude that this superiority (solution improvements) is rely on the unique structure (inspired by ANNs) and strategy of NNA for solving optimization problems. More comparisons with other optimizers can be included, however, the similarity of having no initial parameter and random search exploration were very close to TLBO and RS methods.</p><p>The NNA is designed to have no parameters with the aid of its unique structure. The TLBO algorithm is also an optimizer with the same characteristic in this matter. Random search method is a simple searching strategy looking for candidate solution between lower and upper bounds. Tables <ref type="table" target="#tab_11">14 to 18</ref> tabulates the obtained optimization results using the NNA, TLBO, RS, and the other modern optimizers for all considered benchmarks.</p><p>The Friedman test with the confidence level of 0.05 has been used for evaluating the significance level of differences among the reported methods. The Friedman test <ref type="bibr" target="#b37">[37]</ref> is a nonparametric statistical test used to detect differences among algorithms. The Friedman test first ranks the j th of k algorithms on the i th of N datasets, and then calculates the average rank according to the F-distribution (i.e., distribution value) throughout all the datasets, and calculates the Friedman statistics.         By observing Tables <ref type="table" target="#tab_11">14 to 18</ref>, the DE and TLBO have tight competition with the optimization results of NNA. For some cases, NNA has surpassed the DE and TLBO and vice versa. Last columns of Tables 14 to 18 belongs to Friedman test with the confidence level of 0.05 (α). Table <ref type="table" target="#tab_12">19</ref> shows the summation of all average ranking obtained from Friedman test for each algorithm (the lower the rank the better the performance).  <ref type="bibr" target="#b6">(6)</ref> As can be seen from Table <ref type="table" target="#tab_12">19</ref>, the NNA has been placed at first rank and the DE and TLBO have been located at the second and third ranks, respectively. The RS, as expected, is the worst algorithm for solving the reported benchmarks and could not even find acceptable results for all cases (see Tables <ref type="table" target="#tab_11">14 to 18</ref>). Looking at huge gaps in the obtained optimization results by the NNA and RS, we can conclude that the NNA is not based on the simple random search even in its exploration phase.</p><p>With the aim of obtaining rigorous and fair conclusion, two other statistical tests have been carried out in this research including the Kruskal-Wallis H test <ref type="bibr" target="#b38">[38]</ref> and Multiple Comparison Test <ref type="bibr" target="#b39">[39]</ref>. These tests have been conducted for proving if there are significant differences in the results obtained by all methods and each method compared with the another. The Kruskal-Wallis H test (sometimes also called the "one-way ANOVA on ranks") is a rank-based nonparametric test that can be used to determine if there are statistically significant differences between two or more groups of an independent variable on a continuous or ordinal dependent variable. Also, in statistics, the multiple comparison test occurs when one considers a set of statistical Tables 20 to 24 show the multiple comparison test for comparing two by two methods for reported optimization methods for the results obtained in Tables <ref type="table" target="#tab_11">14 to 18</ref>.   NNA vs. RS -1.7679e+10 -1.6823e+10 -1.5966e+10 2.84e-07 NNA vs. TLBO -6.1005e+08 -6.1005e+08 6.1005e+08 1 NNA vs. ICA -6.1020e+08 -6.1020e+08 6.0991e+08 1 NNA vs. CS -9.9520e+09 -9.9520e+09 -8.7319e+09 2.24e-07 NNA vs. GSA -6.1005e+08 -6.1005e+08 6.1005e+08 1 NNA vs. WCA -6.1005e+08 -6.1005e+08 6.1005e+08 1 NNA vs. HS -6.1006e+08 -6.1006e+08 6.1005e+08 1 NNA vs. PSO -6.1006e+08 -6.1006e+08 6.1005e+08 1 NNA vs. GA -6.1005e+08 -6.1005e+08 6.1005e+08 1 NNA vs. SA -6.1006e+08 -6.1006e+08 6.1005e+08 1 NNA vs. DE -6.1005e+08 -6.1005e+08 6.1005e+08 1 NNA vs. CMA-ES -6.1006e+08 -6.1006e+08   The second column of Tables <ref type="table" target="#tab_13">20</ref> and<ref type="table" target="#tab_18">24</ref>           <ref type="table" target="#tab_19">25</ref>). Also, the steepest slope can be seen in the convergence of NNA in all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Computational complexity</head><p>Furthermore, NNA complexity on both 10 and 30 dimensions has been evaluated following the guidelines provided in CEC'15 <ref type="bibr" target="#b40">[40]</ref>. The value for T0 has been calculated using the test program provided in the guidelines. The calculated computing time for the test program is T0 = 0.274 seconds.</p><p>Next, the average complete computing time, T1, for all the benchmark functions is calculated. Finally, the algorithm complexity (T1/T0) for both dimensions has been measured and the NNA complexity has been tabulated in Table <ref type="table" target="#tab_21">26</ref>.  as well-known engineering benchmarks in the literature <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref>. Mathematical formulation and required descriptions for all reported constrained engineering benchmark problems can be found in detail in the literature <ref type="bibr" target="#b41">[41]</ref>. Using the feasible approach (i.e., direct approach) <ref type="bibr" target="#b43">[43]</ref>, the NNA has been equipped for solving constrained optimization problem.</p><p>For the fixed predefined NFEs (i.e., 50000), the aforementioned optimizers used in the previous sections have been considered and implemented for optimal solving of real world constrained engineering design problems. Statistical optimization results along with statistical tests (i.e., Friedman with α = 0.05) obtained by the proposed optimizer and the other optimizers are tabulated in Table <ref type="table" target="#tab_22">27</ref> for reported constrained engineering design problems. Table <ref type="table" target="#tab_23">28</ref> shows overall ranking of reported optimization methods for solving constrained engineering problems. Looking at Tables <ref type="table" target="#tab_22">27</ref> and<ref type="table" target="#tab_23">28</ref>, under pre-defined NFEs, in terms of statistical results and tests, the The reported simulation and optimization results should not be taken to mean that the NNA is "better" than other population-based and evolutionary optimization algorithms. Such a general statement would be an oversimplification, especially in view of the no free lunch theorem <ref type="bibr" target="#b44">[44]</ref>. However, the results presented in this paper show that the NNA provides superior performance than the utilized algorithms we tested for the particular unconstrained benchmarks and constrained engineering design problems. The proposed NNA in this paper is a simple search model inspired by ANNs. The purpose was to introduce the unique structure of ANNs as a new model for developing a metaheuristic optimization method and further developments in this area can be investigated. Furthermore, randomizing some of the control parameters <ref type="bibr" target="#b45">[45]</ref> may have positive effects on the performance of the NNA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and future works</head><p>This paper presented a dynamic optimization model inspired by structure and concept of artificial neural networks (ANNs) and biological nervous systems, so called neural network algorithm (NNA).</p><p>The NNA benefits from unique structure of ANNs along with search operators for solving complex optimization problems. The difficulty of tuning the initial parameters which is the most common and As future research, there are many works to do. To name a few, for updating the weight matrix, in this paper, the same concept as for transfer function operator was used. However, based on the concept of learning and updating weights in ANNs, other learning approaches existed in ANNs such as Hebbian learning, competitive learning, reinforcement learning, and gradient descent learning can be considered for deriving different versions of NNA. In this paper, a simple form of NNA with simple weight updating approach have been considered. Also, the NNA can be mapped into a multi-objective optimizer handling many objectives using existing methods in the literature such as non-dominated sorting approach. Hybridizing of NNA with other population-based methods and utilizing its unique Let rand = r and after applying transfer function as described in Eq. ( <ref type="formula" target="#formula_23">13</ref>), the set of Eq. (A.3) will becomes as: In order to solve the iterative system of equations shown in Eq. (A.4), the following system of equation can be established as given follows: </p><formula xml:id="formula_42">A C C E P T E D M A N U S C R I P T                              </formula><formula xml:id="formula_43">                                 </formula><formula xml:id="formula_44">                            </formula><formula xml:id="formula_45">                                                                     (A.6)</formula><p>The system of equations expressed in Eq. (A.6) is a stochastic in nature. The way the individual weights are defined (see Eqs. ( <ref type="formula" target="#formula_7">5</ref>), ( <ref type="formula">6</ref>) and ( <ref type="formula" target="#formula_15">10</ref>)) can be assumed as a uniform random variable defined  </p><formula xml:id="formula_46">                                                                       (A.8)</formula><p>Since the coefficient matrix A is stochastic as the values in the matrix are dynamically changing at each iteration, therefore to solve this system of equation (Eq. (A.6)), the Neumann Expansion method <ref type="bibr" target="#b46">[46]</ref> is applied. Hence, the matrix A is treated as the sum of deterministic and stochastic matrices, symbolically it may be represented as 0 A A A    , where A0 is a deterministic matrix and A  is stochastic matrix, which is defined as given follows: </p><formula xml:id="formula_47">                                                         (A.10)</formula><p>Therefore, the solution of 0 () A A X b    will be <ref type="bibr" target="#b46">[46]</ref>:   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is represented by a gene. Combining the genes, a chromosome is produced which represents a solution (individual).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>exists in many optimization methods (especially when the number of design variable increases), this paper is thus motivated to focus on these aims.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>neural networks. Other necessary information concerning ANNs are given at each related step of the proposed optimization method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic view of an ANNs with: (a) feed forward neural networks, (b) recurrent neural networks.</figDesc><graphic coords="6,72.00,109.90,383.75,269.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>optimization algorithm based on the neural network configuration. Detail descriptions and processes of the NNA are given in the following subsections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>)</head><label></label><figDesc>Each of the decision variable values (x1, x2, . . . , xD) can be represented as floating number (i.e., real values) or can be defined over a set of discrete variables. The cost (fitness for maximization problems) of a pattern solution is obtained by evaluating the cost function (fitness function) (C) at the corresponding pattern solution given as follows: 12 ( , , , ) 1, 2,3,...,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>D×β) % Nb: No. of biased variables in population of new pattern solution For j = 1: Nb X Input (i, Integer rand [0, D]) = LB+(UB-LB) ×rand. End For %% -------------Bias for Updated Weight Matrix ----------------------------------------------Nwb = Round (Npop×β) % Nwb: No. of biased variables in updated weight matrix For j = 1: Nwb W Updated (j, Integer rand [0, Npop]) = U (0,1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>5 .</head><label>5</label><figDesc>Steps of the NNATaking the advantages of ANNs into account, the NNA is inspired by the structure and concept of ANNs. The whole procedures of the NNA are illustrated in Fig.3including all processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Processes of the NNA.</figDesc><graphic coords="12,72.00,128.90,439.45,200.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Schematic view for the performance of the NNA. Since the current values of design variables affect their next values (i.e., local and global feedbacks), the NNA is categorized as a dynamic optimization model. Hence, general behavior of the NNA can be described in the following equation: ( ) ( ( ), ( )), 1, 2,3,...,</figDesc><graphic coords="13,72.00,72.00,450.95,383.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Mean S/N ratio plot for each level of the factors for the considered optimizers (Horizontal axis stands for different levels for each parameter).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>inferences simultaneously or infers a subset of parameters selected based on the observed values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. Box-plot of objective function using the reported optimizers (continues).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Box-plot of objective function using the reported optimizers (continues).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Box-plot of objective function using the reported optimizers (continues).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Box-plot of objective function using the reported optimizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>Looking at Figs. 6 to 9, the left side of the aforementioned figures are boxplots showing a standardized way of displaying the distribution of data based on the five number summary: the minimum value, first quartile, the median value, third quartile, and the maximum value. The right side of Figs. 6 to 9 demonstrates the multiple comparison test among reported optimizers.From Figs. 6 to 9, right side, the red color lines are the methods which significantly differ with the proposed NNA (The blue lines belong to the NNA). The gray color lines are the method which are not significantly different with the proposed NNA. The obtained p-values in last column of Tables 20 to 24 prove this comparison. For instance, looking at F17 in Fig.9, we can see the NNA statistically has outperformed six optimizers (i.e., TLBO, WCA, ICA, PSO, DE, and CMA-ES), while the optimization results obtained by the CS, GSA, HS, GA, and SA are the same with no significance difference. Lines right side of the NNA (i.e., blue lines) show the methods which are not significantly better than the NNA, while lines left side of the NNA show the methods which are statistically better than the NNA for a specific benchmark. Furthermore, Table25tabulates the obtained optimization results along with their statistical tests and ranking for dimension 200 (α = 0.05) for 1E+06 NFEs. Due to the limitation of execution time, only the TLBO and GSA have been selected in the comparison pool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Convergence history for considered functions with dimension 200 (Continues).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Convergence history for considered functions with dimension 200. Note that starting with larger objective function values means better potential for searching wider region in the search space. In other words, it shows the NNA ability for exploration phase searching farther regions and leads to escape from local optima. The proof is the obtained results reported in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>3 . 5 .</head><label>35</label><figDesc>NNA for solving real world constrained engineering design problemsRegarding the constrained benchmark problems, pressure vessel design, welded beam design, speed reducer design problem, three-bar truss design problem, and gear train design problem extensively used in the literature, have been investigated in this paper. Those constrained problems are recognized</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>NNA has obtained lower average ranking using the Friedman test. As the performance of NNA has been shown in Sections 3.2 and 3.3, here the efficiency of NNA for handling constrained optimization problems has been tested and validated over the other optimizers. It shows that the proposed NNA can be successfully applied for unconstrained and constrained optimization problems having many local optima. The NNA as for other optimizers can be used for solving optimization problems, where a problem is highly nonlinear and non-convex. Having mature convergence, being an algorithm without any effort for fine tuning initial parameters, and capability of finding quality solutions make the NNA attractive when one encounters NP-hard optimization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>important part in almost all metaheuristic algorithms has been eliminated in the proposed model. The optimization results show that the NNA is able to find the global minimum of multimodal functions with the minimum possibility of getting trapped in local minima.Sensitivity analysis over common user parameters of NNA which are population size and maximum number of iteration has been performed. Twenty-one unconstrained benchmarks have been considered and twelve well-known and recent optimizers have been compared for solving unconstrained and constrained benchmarks along with statistical results and tests. Computational optimization results obtained from several optimization problems clearly illustrate the attractiveness and competitiveness features of the proposed method for handling unconstrained and constrained real life problems with many design variables compared with recent and well-used optimizers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>on zero to one, however, the components of weight matrix have to follow one more condition which is restated in Eq. (A.7random variables defined in the range of zero and one, therefore the expected value of these variables can be evaluated as   these values in Eq. (A.6), the system will change into the following systemAX b , where</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>Therefore the random solution of the AX b  can now be expressed in terms of the following series,The aforementioned solution series will be convergent if the spectral radius of the matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,72.00,322.30,351.60,263.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,72.00,335.30,411.12,312.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="18,72.00,72.00,348.50,697.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="49,72.00,72.00,450.95,602.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 ,</head><label>1</label><figDesc>LB and UB are lower and upper bounds of a problem, respectively. As can be seen in Table</figDesc><table><row><cell>).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Optimal values of user parameters used in the reported optimizers.</figDesc><table><row><cell cols="2">Methods Parameters</cell><cell>Optimal Values</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>GA</cell><cell>Pc</cell><cell>0.8</cell></row><row><cell></cell><cell>Pm</cell><cell>0.3</cell></row><row><cell></cell><cell>HMS (Npop)</cell><cell>50</cell></row><row><cell>HS</cell><cell>HMCR</cell><cell>0.95</cell></row><row><cell></cell><cell>PAR</cell><cell>0.3</cell></row><row><cell>WCA</cell><cell>Npop Nsr</cell><cell>100 8</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>DE</cell><cell>Cross-over Rate</cell><cell>0.8</cell></row><row><cell></cell><cell>F</cell><cell>0.3</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>ICA</cell><cell>N_Empire</cell><cell>3</cell></row><row><cell></cell><cell>Revolution Rate</cell><cell>0.6</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>GSA</cell><cell>G0</cell><cell>100</cell></row><row><cell></cell><cell>α</cell><cell>10</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>PSO</cell><cell>C1,C2</cell><cell>2</cell></row><row><cell></cell><cell>w</cell><cell>0.9</cell></row><row><cell>TLBO</cell><cell>Npop</cell><cell>100</cell></row><row><cell>CS</cell><cell>Npop Discovery Rate</cell><cell>20 0.25</cell></row><row><cell></cell><cell>Npop</cell><cell>100</cell></row><row><cell>SA</cell><cell cols="2">Initial Temperature (T0) 100</cell></row><row><cell></cell><cell cols="2">Final Temperature (TF) 0.001</cell></row><row><cell>NNA</cell><cell>Npop</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 14 .</head><label>14</label><figDesc>Statistical test and optimization results obtained by reported optimizers (D=50) (Continued).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Rankings</cell></row><row><cell cols="3">Function Methods Best</cell><cell>Average</cell><cell>Median</cell><cell>Worst</cell><cell>SD</cell><cell>Friedman Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Ranking)</cell></row><row><cell></cell><cell>NNA</cell><cell>4.95E-12</cell><cell>2.25E-10</cell><cell>7.48E-11</cell><cell>1.39E-09</cell><cell>3.45E-10</cell><cell>6.80</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">7.07E+04 9.02E+04 9.10E+04 1.01E+05 6.45E+03 13</cell></row><row><cell></cell><cell>TLBO</cell><cell>6.25E-13</cell><cell>1.16E-09</cell><cell>3.33E-12</cell><cell>2.09E-08</cell><cell>4.17E-09</cell><cell>6.20</cell></row><row><cell></cell><cell>ICA</cell><cell>9.16E-03</cell><cell cols="2">3.44E+01 4.17E-02</cell><cell cols="3">9.50E+02 1.73E+02 10.19</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1.51</cell></row><row><cell></cell><cell>GSA</cell><cell>5.68E-14</cell><cell>5.68E-14</cell><cell>5.68E-14</cell><cell>5.68E-14</cell><cell cols="2">0.00E+00 3.46</cell></row><row><cell>1</cell><cell>WCA</cell><cell>1.71E-13</cell><cell>3.03E-13</cell><cell>2.84E-13</cell><cell>5.12E-13</cell><cell>8.22E-14</cell><cell>5</cell></row><row><cell></cell><cell>HS</cell><cell>1.42E-04</cell><cell>2.36E-04</cell><cell>2.42E-04</cell><cell>2.72E-04</cell><cell>2.61E-05</cell><cell>8</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">7.70E+00 2.36E+01 2.10E+01 5.51E+01 1.15E+01 11.93</cell></row><row><cell></cell><cell>GA</cell><cell>4.56E-04</cell><cell>2.71E-03</cell><cell>2.31E-03</cell><cell>1.09E-02</cell><cell>2.03E-03</cell><cell>9.00</cell></row><row><cell></cell><cell>SA</cell><cell>2.07E-01</cell><cell>3.21E-01</cell><cell>3.17E-01</cell><cell>4.57E-01</cell><cell>7.18E-02</cell><cell>10.86</cell></row><row><cell></cell><cell>DE</cell><cell cols="2">0.00E+00 6.06E-14</cell><cell>5.68E-14</cell><cell>1.14E-13</cell><cell>2.08E-14</cell><cell>3.50</cell></row><row><cell></cell><cell cols="7">CMA-ES 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1.51</cell></row><row><cell></cell><cell>NNA</cell><cell>4.52E-01</cell><cell cols="2">1.03E+00 9.83E-01</cell><cell cols="2">1.65E+00 3.10E-01</cell><cell>2.57</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">7.91E+01 8.53E+01 8.54E+01 9.16E+01 2.81E+00 12.97</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">2.84E+01 4.18E+01 4.31E+01 5.11E+01 4.98E+00 10.37</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">3.53E+01 5.54E+01 5.07E+01 9.52E+01 1.27E+01 11.57</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">2.52E+01 4.47E+01 4.53E+01 6.44E+01 9.21E+00 10.67</cell></row><row><cell></cell><cell>GSA</cell><cell>2.70E-01</cell><cell cols="5">9.84E+00 1.00E+01 2.18E+01 5.14E+00 6.63</cell></row><row><cell>`2</cell><cell>WCA</cell><cell cols="6">1.24E+00 6.27E+00 4.15E+00 2.44E+01 5.54E+00 5.97</cell></row><row><cell></cell><cell>HS</cell><cell cols="5">1.49E+00 2.21E+00 2.17E+00 2.98E+00 3.63E-01</cell><cell>5.10</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">1.32E+01 2.62E+01 2.79E+01 3.84E+01 6.79E+00 8.40</cell></row><row><cell></cell><cell>GA</cell><cell>8.50E-01</cell><cell cols="4">1.38E+00 1.27E+00 3.14E+00 4.51E-01</cell><cell>3.17</cell></row><row><cell></cell><cell>SA</cell><cell>9.70E-01</cell><cell cols="4">1.45E+00 1.46E+00 1.84E+00 1.95E-01</cell><cell>3.63</cell></row><row><cell></cell><cell>DE</cell><cell cols="6">1.30E+01 2.91E+01 2.84E+01 5.68E+01 1.01E+01 8.97</cell></row><row><cell></cell><cell cols="2">CMA-ES 6.92E-10</cell><cell>1.2E-09</cell><cell>1.21E-09</cell><cell>1.79E-09</cell><cell>3.35E-10</cell><cell>1</cell></row><row><cell></cell><cell>NNA</cell><cell>5.39E-01</cell><cell cols="5">9.60E+01 8.26E+01 3.35E+02 7.12E+01 4.40</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">1.76E+10 3.72E+10 3.78E+10 4.78E+10 6.67E+09 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">8.63E+00 4.54E+02 9.50E+01 1.04E+04 1.88E+03 5.36</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">2.89E+02 6.68E+04 6.19E+02 1.29E+06 2.55E+05 9.73</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">6.67E+02 7.87E+02 7.23E+02 8.89E+02 2.11E+02 12</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">4.08E+01 1.02E+02 4.13E+01 1.01E+03 1.88E+02 3.83</cell></row><row><cell>3</cell><cell>WCA</cell><cell cols="6">4.36E+00 9.80E+01 8.14E+01 4.22E+02 1.03E+02 4.03</cell></row><row><cell></cell><cell>HS</cell><cell cols="6">2.96E+00 5.94E+02 1.53E+02 7.69E+03 1.58E+03 6.80</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">3.31E+03 2.86E+04 1.88E+04 1.26E+05 3.07E+04 1.08</cell></row><row><cell></cell><cell>GA</cell><cell cols="6">2.54E+01 3.83E+02 1.47E+02 5.41E+03 1.00E+03 6.46</cell></row><row><cell></cell><cell>SA</cell><cell cols="6">9.51E+01 9.17E+02 1.69E+02 1.18E+04 2.57E+03 7.13</cell></row><row><cell></cell><cell>DE</cell><cell cols="6">3.85E+01 5.48E+01 4.11E+01 1.12E+02 2.43E+01 3.06</cell></row><row><cell></cell><cell cols="7">CMA-ES 2.04E+01 1.51E+03 2.17E+01 2.18E+04 4.51E+03 4.36</cell></row><row><cell></cell><cell>NNA</cell><cell cols="6">1.99E+00 6.60E+00 5.97E+00 1.19E+01 2.33E+00 2.63</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">5.87E+02 6.44E+02 6.48E+02 6.87E+02 2.56E+01 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">7.56E+01 1.08E+02 9.90E+01 1.50E+02 2.21E+01 7.30</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">2.06E+02 3.49E+02 3.57E+02 4.32E+02 5.42E+01 11.8</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">1.51E+01 3.07E+01 2.97E+01 6.47E+01 1.05E+01 5.26</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">2.39E+01 4.42E+01 4.48E+01 6.77E+01 1.07E+01 6</cell></row><row><cell>4</cell><cell>WCA</cell><cell cols="6">1.01E+02 1.91E+02 1.90E+02 3.64E+02 5.36E+01 8.96</cell></row><row><cell></cell><cell>HS</cell><cell>3.09E-02</cell><cell>4.36E-02</cell><cell>4.33E-02</cell><cell>5.44E-02</cell><cell>5.01E-03</cell><cell>1</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">1.14E+02 1.57E+02 1.52E+02 2.04E+02 2.42E+01 8.40</cell></row><row><cell></cell><cell>GA</cell><cell cols="6">1.99E+00 9.89E+00 9.46E+00 1.99E+01 4.64E+00 3.30</cell></row><row><cell></cell><cell>SA</cell><cell cols="6">3.21E+00 8.25E+00 7.23E+00 1.62E+01 3.86E+00 3.16</cell></row><row><cell></cell><cell>DE</cell><cell cols="6">1.62E+02 2.47E+02 2.58E+02 3.13E+02 4.07E+01 9.96</cell></row><row><cell></cell><cell cols="7">CMA-ES 6.96E+00 2.67E+02 3.06E+02 3.31E+02 1.03E+02 10.20</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 15 .</head><label>15</label><figDesc>Statistical test and optimization results obtained by reported optimizers (D=50) (Continued).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Rankings</cell></row><row><cell cols="3">Function Methods Best</cell><cell>Average</cell><cell>Median</cell><cell>Worst</cell><cell>SD</cell><cell>Friedman Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Ranking)</cell></row><row><cell></cell><cell>NNA</cell><cell>6.76E-12</cell><cell>6.20E-02</cell><cell>5.26E-02</cell><cell>1.61E-01</cell><cell>5.30E-02</cell><cell>6.96</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">6.96E+02 8.36E+02 8.45E+02 1.00E+03 6.18E+01 13</cell></row><row><cell></cell><cell>TLBO</cell><cell>9.09E-13</cell><cell>1.51E-01</cell><cell>7.36E-02</cell><cell cols="2">1.07E+00 2.15E-01</cell><cell>7.76</cell></row><row><cell></cell><cell>ICA</cell><cell>1.82E-02</cell><cell>5.34E-01</cell><cell>6.58E-02</cell><cell cols="3">8.41E+00 1.71E+00 8.13</cell></row><row><cell></cell><cell>CS</cell><cell cols="2">0.00E+00 5.49E-03</cell><cell cols="2">0.00E+00 4.16E-02</cell><cell>1.06E-02</cell><cell>2.80</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">3.10E+00 2.63E+01 2.03E+01 6.72E+01 1.74E+01 12</cell></row><row><cell></cell><cell>WCA</cell><cell>2.56E-13</cell><cell>3.01E-02</cell><cell>8.63E-03</cell><cell>5.24E-01</cell><cell>9.47E-02</cell><cell>5.13</cell></row><row><cell></cell><cell>HS</cell><cell>7.24E-06</cell><cell>1.11E-02</cell><cell>8.64E-03</cell><cell>5.91E-02</cell><cell>1.49E-02</cell><cell>5.16</cell></row><row><cell></cell><cell cols="7">PSO GA SA DE CMA-ES 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1.36 1.08E+00 1.22E+00 1.17E+00 1.64E+00 1.22E-01 10.33 2.50E-04 7.68E-03 5.31E-03 2.12E-02 5.80E-03 5.06 1.14E+00 1.21E+00 1.20E+00 1.28E+00 3.23E-02 10.53 0.00E+00 2.47E-04 2.84E-14 7.40E-03 1.35E-03 2.73 NNA 1.21E-07 1.12E-06 4.97E-07 5.27E-06 1.38E-06 4.03 RS 2.01E+01 2.04E+01 2.04E+01 2.05E+01 1.04E-01 12.97 TLBO 6.60E+00 9.82E+00 1.01E+01 1.25E+01 1.42E+00 9.87 ICA 4.09E+00 1.79E+01 1.89E+01 2.07E+01 3.72E+00 11.83 CS 1.16E+00 1.92E+00 1.84E+00 3.29E+00 4.99E-01 8.13 GSA 2.37E-09 3.18E-09 3.13E-09 3.91E-09 3.78E-10 3 WCA 9.05E-09 1.11E+01 1.21E+01 1.88E+01 6.44E+00 9.73 HS 7.35E-03 8.55E-03 8.68E-03 9.63E-03 4.81E-04 5.27 PSO GA 7.14E-03 1.07E-02 1.04E-02 1.64E-02 2.08E-03 5.93 SA 3.14E-01 4.26E-01 4.23E-01 5.99E-01 5.58E-02 7.10 DE 5.68E-14 9.76E-14 5.68E-14 4.26E-13 8.06E-14 CMA-ES 2.84E-14 2.84E-14 2.84E-14 2.84E-14 0.00E+00 1 NNA 1.78E-12 3.99E-11 3.01E-11 1.44E-10 3.71E-11 RS 1.06E+08 1.10E+12 1.65E+11 1.03E+13 2.26E+12 13 TLBO 5.42E-214 8.46E-213 6.04E-213 3.12E-212 0.00E+00 1 ICA 2.42E+01 2.15E+07 1.56E+02 6.45E+08 1.18E+08 12 M 5.83 A 2 N 7.04E+00 1.20E+01 1.21E+01 1.77E+01 3.50E+00 10.13 U S C R I P T</cell></row><row><cell cols="8">CS GSA WCA HS PSO GA SA DE CMA-ES 1.6E-13 3.68E-26 2.55E-08 1.39E-14 5.29E-02 3.07E-01 9.72E-03 4.48E-01 1.03E-22 NNA 6.80E-23 RS 6.72E+05 7.56E+05 7.54E+05 8.24E+05 3.71E+04 13 1.79E-03 1.92E-22 5.38E-02 9.83E-03 2.57 3.36E-08 3.40E-08 4.98E-08 5.02E-09 6.90 3.33E-01 7.00E-13 1.00E+01 1.83E+00 4.73 5.99E-02 6.07E-02 6.46E-02 3.18E-03 8.97 2.15E+00 1.82E+00 4.98E+00 1.21E+00 10.93 1.67E-02 1.64E-02 3.41E-02 4.90E-03 7.93 5.37E-01 5.39E-01 6.47E-01 5.45E-02 10 5.10E-12 7.93E-19 1.32E-10 2.42E-11 3.07 4.28E-13 3.74E-13 1.14E-12 2.15E-13 4.07 5.18E-20 1.34E-20 3.08E-19 9.07E-20 5.93 TLBO 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1 ICA 2.35E-01 2.01E+00 1.22E+00 1.54E+01 2.85E+00 10.90 CS 1.31E-44 1.50E-42 4.57E-43 1.17E-41 2.62E-42 2 GSA 1.12E-16 3.58E-16 3.29E-16 6.91E-16 1.21E-16 7 C C E P T E D</cell></row><row><cell>A</cell><cell>WCA HS PSO</cell><cell cols="6">3.58E-29 4.04E-03 6.36E+01 2.94E+02 2.53E+02 7.49E+02 1.72E+02 12 7.90E-23 3.07E-24 1.01E-21 2.21E-22 4.20 4.99E-03 4.91E-03 6.15E-03 5.40E-04 8.03</cell></row><row><cell></cell><cell>GA</cell><cell>3.48E-03</cell><cell>3.73E-02</cell><cell>3.30E-02</cell><cell>8.51E-02</cell><cell>1.93E-02</cell><cell>8.97</cell></row><row><cell></cell><cell>SA</cell><cell>2.54E-01</cell><cell>3.88E-01</cell><cell>3.89E-01</cell><cell>6.16E-01</cell><cell>8.83E-02</cell><cell>10.10</cell></row><row><cell></cell><cell>DE</cell><cell>3.64E-36</cell><cell>2.63E-20</cell><cell>2.26E-29</cell><cell>7.67E-19</cell><cell>1.40E-19</cell><cell>3.40</cell></row><row><cell></cell><cell cols="2">CMA-ES 6.54E-25</cell><cell>6.4E-24</cell><cell>5.57E-24</cell><cell>1.88E-23</cell><cell>4.41E-24</cell><cell>4.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 16 .</head><label>16</label><figDesc>Statistical test and optimization results obtained by reported optimizers (D=50) (Continued).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Rankings</cell></row><row><cell cols="2">Function Methods Best</cell><cell>Average</cell><cell>Median</cell><cell>Worst</cell><cell>SD</cell><cell>Friedman Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Ranking)</cell></row><row><cell>NNA</cell><cell>1.07E-01</cell><cell cols="5">3.21E+00 2.23E+00 1.10E+01 3.33E+00 4.77</cell></row><row><cell>RS</cell><cell cols="6">4.52E+02 4.83E+02 4.85E+02 4.97E+02 1.04E+01 13</cell></row><row><cell>TLBO</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1</cell></row><row><cell>ICA</cell><cell cols="6">3.30E+02 3.76E+02 3.78E+02 4.27E+02 2.31E+01 12</cell></row><row><cell>CS</cell><cell cols="6">1.01E+02 1.72E+02 1.78E+02 2.17E+02 2.87E+01 9.53</cell></row><row><cell>GSA</cell><cell>2.41E-03</cell><cell>9.06E-01</cell><cell>4.92E-02</cell><cell cols="3">8.99E+00 2.59E+00 3.60</cell></row><row><cell>WCA</cell><cell cols="6">2.08E+02 2.45E+02 2.40E+02 3.29E+02 2.61E+01 11</cell></row><row><cell>HS</cell><cell cols="6">2.09E+01 2.75E+01 2.78E+01 3.57E+01 3.16E+00 8</cell></row><row><cell>PSO</cell><cell cols="6">1.40E+02 1.68E+02 1.70E+02 2.02E+02 1.57E+01 9.47</cell></row><row><cell>GA</cell><cell cols="6">7.15E+00 1.08E+01 1.12E+01 1.44E+01 1.92E+00 6.67</cell></row><row><cell>SA</cell><cell cols="5">8.09E+00 8.76E+00 8.61E+00 1.00E+01 5.06E-01</cell><cell>6.10</cell></row><row><cell>DE</cell><cell>1.79E-03</cell><cell>6.19E-01</cell><cell>3.61E-01</cell><cell cols="2">3.71E+00 8.05E-01</cell><cell>3.87</cell></row><row><cell cols="2">CMA-ES 1.28E-04</cell><cell>5.82E-04</cell><cell>2.16E-04</cell><cell>1.11E-02</cell><cell>1.99E-03</cell><cell>2</cell></row><row><cell>NNA</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 2.42</cell></row><row><cell>RS</cell><cell cols="6">4.19E+03 5.18E+03 5.26E+03 5.71E+03 3.48E+02 13</cell></row><row><cell>TLBO</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 2.42</cell></row><row><cell>ICA</cell><cell cols="6">1.10E+01 1.64E+01 1.57E+01 2.21E+01 2.99E+00 10.90</cell></row><row><cell>CS</cell><cell cols="6">5.25E+00 1.07E+01 9.92E+00 1.76E+01 3.11E+00 10.10</cell></row><row><cell>GSA</cell><cell cols="2">0.00E+00 7.00E-02</cell><cell>1.67E-16</cell><cell cols="2">1.05E+00 2.66E-01</cell><cell>4.55</cell></row><row><cell>WCA</cell><cell>7.77E-16</cell><cell>5.64E-01</cell><cell>2.19E-01</cell><cell cols="2">2.57E+00 7.29E-01</cell><cell>7.32</cell></row><row><cell>HS</cell><cell>6.28E-03</cell><cell>1.04E-02</cell><cell>1.05E-02</cell><cell>1.24E-02</cell><cell>1.12E-03</cell><cell>7.17</cell></row><row><cell>PSO</cell><cell cols="6">2.92E+01 4.48E+01 4.52E+01 7.05E+01 1.05E+01 12</cell></row><row><cell>GA</cell><cell>1.07E-03</cell><cell>3.36E-03</cell><cell>2.84E-03</cell><cell>7.96E-03</cell><cell>1.73E-03</cell><cell>6.17</cell></row><row><cell>SA</cell><cell>1.77E-01</cell><cell>2.73E-01</cell><cell>2.65E-01</cell><cell>3.92E-01</cell><cell>5.92E-02</cell><cell>8.20</cell></row><row><cell>DE</cell><cell cols="2">0.00E+00 1.87E-01</cell><cell cols="3">0.00E+00 1.05E+00 3.72E-01</cell><cell>4.35</cell></row><row><cell cols="7">CMA-ES 0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 2.42</cell></row><row><cell>NNA</cell><cell>6.47E-02</cell><cell cols="5">5.80E+00 4.41E+00 2.04E+01 5.54E+00 5.20</cell></row><row><cell>RS</cell><cell cols="6">4.47E+02 4.69E+02 4.70E+02 4.89E+02 9.11E+00 13</cell></row><row><cell>TLBO</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1</cell></row><row><cell>ICA</cell><cell cols="6">3.31E+02 3.71E+02 3.68E+02 4.23E+02 2.38E+01 12</cell></row><row><cell>CS</cell><cell cols="6">1.12E+02 1.72E+02 1.73E+02 2.15E+02 2.70E+01 9.57</cell></row><row><cell>GSA</cell><cell>1.25E-02</cell><cell cols="2">1.43E+00 6.11E-02</cell><cell cols="3">1.45E+01 3.64E+00 3.53</cell></row><row><cell>WCA</cell><cell cols="6">1.85E+02 2.49E+02 2.45E+02 2.89E+02 2.17E+01 10.97</cell></row><row><cell>HS</cell><cell cols="6">2.17E+01 2.66E+01 2.67E+01 3.36E+01 2.82E+00 8</cell></row><row><cell>PSO</cell><cell cols="6">1.38E+02 1.64E+02 1.65E+02 1.88E+02 1.16E+01 9.47</cell></row><row><cell>GA</cell><cell cols="6">7.19E+00 1.07E+01 1.04E+01 1.69E+01 2.38E+00 6.60</cell></row><row><cell>SA</cell><cell cols="5">6.92E+00 8.45E+00 8.28E+00 1.05E+01 8.45E-01</cell><cell>5.87</cell></row><row><cell>DE</cell><cell>1.06E-05</cell><cell>4.29E-01</cell><cell>3.18E-01</cell><cell cols="2">1.78E+00 4.21E-01</cell><cell>3.77</cell></row><row><cell cols="2">CMA-ES 1.41E-04</cell><cell>1.86E-03</cell><cell>2.41E-04</cell><cell>3.23E-02</cell><cell>6.14E-03</cell><cell>2.03</cell></row><row><cell>NNA</cell><cell>1.75E-01</cell><cell cols="5">1.64E+01 1.59E+01 5.29E+01 1.23E+01 3.93</cell></row><row><cell>RS</cell><cell cols="6">4.36E+04 5.70E+04 5.79E+04 6.91E+04 5.11E+03 13</cell></row><row><cell>TLBO</cell><cell cols="6">3.41E+01 6.91E+01 6.80E+01 1.09E+02 1.64E+01 8.13</cell></row><row><cell>ICA</cell><cell cols="6">1.01E+02 1.40E+02 1.37E+02 2.21E+02 2.25E+01 11.60</cell></row><row><cell>CS</cell><cell>6.34E-02</cell><cell cols="5">1.90E+01 1.16E+01 5.80E+01 1.58E+01 3.97</cell></row><row><cell>GSA</cell><cell cols="6">1.17E+01 3.50E+01 3.49E+01 6.16E+01 1.33E+01 6.30</cell></row><row><cell>WCA</cell><cell cols="6">6.81E+01 1.03E+02 1.02E+02 1.37E+02 1.66E+01 9.70</cell></row><row><cell>HS</cell><cell cols="6">7.59E+00 1.69E+01 1.40E+01 3.58E+01 7.81E+00 4.33</cell></row><row><cell>PSO</cell><cell cols="6">8.20E+01 1.05E+02 1.02E+02 1.34E+02 1.31E+01 9.77</cell></row><row><cell>GA</cell><cell cols="6">1.78E+00 1.45E+01 1.47E+01 3.21E+01 7.23E+00 3.70</cell></row><row><cell>SA</cell><cell cols="6">2.48E+00 1.28E+01 1.25E+01 2.97E+01 6.51E+00 3.37</cell></row><row><cell>DE</cell><cell>1.84E-02</cell><cell cols="5">9.78E+00 7.91E+00 2.63E+01 6.64E+00 2.43</cell></row><row><cell cols="7">CMA-ES 6.31E+01 1.23E+02 1.28E+02 1.54E+02 2.17E+01 10.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 17 .</head><label>17</label><figDesc>Statistical test and optimization results obtained by reported optimizers (D=50) (Continued).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Rankings</cell></row><row><cell cols="3">Function Methods Best</cell><cell>Average</cell><cell>Median</cell><cell>Worst</cell><cell>SD</cell><cell>Friedman Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Ranking)</cell></row><row><cell></cell><cell>NNA</cell><cell cols="6">9.70E+00 1.46E+02 4.04E+01 9.42E+02 2.49E+02 3.07</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">1.13E+10 1.68E+10 1.71E+10 2.34E+10 2.60E+09 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">9.35E+01 2.15E+02 1.52E+02 1.11E+03 1.85E+02 6.17</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">3.18E+02 1.41E+05 5.86E+02 1.82E+06 4.39E+05 9.50</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">2.60E+02 9.34E+09 1.00E+10 1.00E+10 2.50E+09 11.83</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">6.09E+01 2.46E+02 7.60E+01 2.91E+03 5.47E+02 4.10</cell></row><row><cell>13</cell><cell>WCA</cell><cell cols="6">1.13E+02 2.27E+02 1.56E+02 5.68E+02 1.38E+02 6.47</cell></row><row><cell></cell><cell>HS</cell><cell cols="6">9.55E+00 1.01E+03 2.33E+02 7.94E+03 1.97E+03 6.63</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">1.06E+03 8.35E+03 6.07E+03 3.15E+04 7.05E+03 10.53</cell></row><row><cell></cell><cell>GA</cell><cell cols="6">3.35E+01 1.26E+02 1.23E+02 2.38E+02 5.92E+01 4.63</cell></row><row><cell></cell><cell>SA</cell><cell cols="6">5.38E+01 1.62E+03 1.64E+02 1.84E+04 4.64E+03 5.90</cell></row><row><cell></cell><cell>DE</cell><cell cols="6">1.40E+01 3.43E+01 2.28E+01 1.11E+02 2.70E+01 1.60</cell></row><row><cell></cell><cell cols="7">CMA-ES 1.44E+02 1.39E+03 1.85E+02 1.60E+04 3.70E+03 7.57</cell></row><row><cell></cell><cell>NNA</cell><cell cols="6">1.81E+00 6.83E+00 7.26E+00 1.15E+01 2.57E+00 2.37</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">4.12E+02 4.60E+02 4.63E+02 5.02E+02 2.26E+01 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">4.81E+01 8.58E+01 8.62E+01 1.27E+02 2.00E+01 7.30</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">1.46E+02 2.44E+02 2.51E+02 3.48E+02 4.61E+01 11.57</cell></row><row><cell></cell><cell>CS</cell><cell cols="6">1.52E+01 2.79E+01 2.86E+01 4.64E+01 6.11E+00 5.20</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">2.38E+01 3.55E+01 3.61E+01 4.97E+01 5.71E+00 5.90</cell></row><row><cell>14</cell><cell>WCA</cell><cell cols="6">8.85E+01 1.44E+02 1.47E+02 2.32E+02 3.67E+01 8.93</cell></row><row><cell></cell><cell>HS</cell><cell>3.76E-01</cell><cell cols="5">4.00E+00 3.78E+00 8.60E+00 1.96E+00 1.40</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">6.20E+01 1.23E+02 1.23E+02 1.94E+02 2.72E+01 8.40</cell></row><row><cell></cell><cell>GA</cell><cell cols="6">2.53E+00 1.15E+01 1.13E+01 2.22E+01 5.53E+00 3.33</cell></row><row><cell></cell><cell>SA</cell><cell cols="6">3.20E+00 9.12E+00 8.02E+00 2.08E+01 4.28E+00 2.97</cell></row><row><cell></cell><cell>DE</cell><cell cols="6">1.12E+02 1.79E+02 1.76E+02 2.36E+02 3.19E+01 10</cell></row><row><cell></cell><cell cols="7">CMA-ES 7.27E+00 2.06E+02 2.21E+02 2.41E+02 5.54E+01 10.63</cell></row><row><cell></cell><cell>NNA</cell><cell>1.48E-12</cell><cell>3.48E-11</cell><cell>1.00E-11</cell><cell>2.75E-10</cell><cell>6.33E-11</cell><cell>4.87</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">1.40E+03 4.48E+04 4.22E+04 1.25E+05 3.11E+04 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">3.85E+01 3.49E+02 1.35E+02 2.67E+03 5.68E+02 1</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">3.85E+01 3.49E+02 1.35E+02 2.67E+03 5.68E+02 12</cell></row><row><cell></cell><cell>CS</cell><cell>2.25E-37</cell><cell>5.20E-36</cell><cell>2.56E-36</cell><cell>3.58E-35</cell><cell>6.89E-36</cell><cell>2</cell></row><row><cell></cell><cell>GSA</cell><cell>1.97E-12</cell><cell>1.94E-11</cell><cell>8.55E-12</cell><cell>1.67E-10</cell><cell>3.21E-11</cell><cell>4.70</cell></row><row><cell>15</cell><cell>WCA</cell><cell>1.32E-12</cell><cell cols="2">4.00E+00 3.99E-11</cell><cell cols="3">6.00E+01 1.25E+01 6.07</cell></row><row><cell></cell><cell>HS</cell><cell>1.57E-01</cell><cell>2.42E-01</cell><cell>2.31E-01</cell><cell>4.03E-01</cell><cell>6.48E-02</cell><cell>8.77</cell></row><row><cell></cell><cell>PSO</cell><cell cols="6">2.35E+00 7.61E+00 5.73E+00 3.18E+01 6.39E+00 10.90</cell></row><row><cell></cell><cell>GA</cell><cell>4.12E-02</cell><cell>1.02E-01</cell><cell>9.51E-02</cell><cell>2.11E-01</cell><cell>3.80E-02</cell><cell>7.27</cell></row><row><cell></cell><cell>SA</cell><cell>5.71E-01</cell><cell>6.96E-01</cell><cell>6.89E-01</cell><cell>9.09E-01</cell><cell>8.09E-02</cell><cell>9.87</cell></row><row><cell></cell><cell>DE</cell><cell>6.83E-31</cell><cell>8.97E-30</cell><cell>2.02E-30</cell><cell>1.56E-28</cell><cell>2.81E-29</cell><cell>3.00</cell></row><row><cell></cell><cell cols="2">CMA-ES 6.99E-02</cell><cell>1.23E-01</cell><cell>1.26E-01</cell><cell>2.15E-01</cell><cell>3.52E-02</cell><cell>7.57</cell></row><row><cell></cell><cell>NNA</cell><cell>4.56E-18</cell><cell>7.35E-11</cell><cell>1.05E-14</cell><cell>2.01E-09</cell><cell>3.66E-10</cell><cell>4.53</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">1.75E+04 2.49E+04 2.49E+04 2.87E+04 2.56E+03 13</cell></row><row><cell></cell><cell>TLBO</cell><cell>1.26E-29</cell><cell>3.85E-01</cell><cell>2.41E-01</cell><cell cols="2">1.93E+00 5.71E-01</cell><cell>6.50</cell></row><row><cell></cell><cell>ICA</cell><cell>1.43E-08</cell><cell cols="2">1.10E+00 4.81E-01</cell><cell cols="3">3.50E+00 1.04E+00 9.47</cell></row><row><cell></cell><cell>CS</cell><cell cols="2">0.00E+00 1.12E-01</cell><cell cols="3">0.00E+00 1.93E+00 3.73E-01</cell><cell>2.82</cell></row><row><cell></cell><cell>GSA</cell><cell cols="2">0.00E+00 3.21E-02</cell><cell cols="2">0.00E+00 4.81E-01</cell><cell>1.22E-01</cell><cell>2.47</cell></row><row><cell>16</cell><cell>WCA</cell><cell cols="2">0.00E+00 9.15E-01</cell><cell>4.81E-01</cell><cell cols="2">1.93E+00 8.61E-01</cell><cell>7.73</cell></row><row><cell></cell><cell>HS</cell><cell>7.40E-06</cell><cell>1.34E-05</cell><cell>1.25E-05</cell><cell>2.50E-05</cell><cell>4.16E-06</cell><cell>5.77</cell></row><row><cell></cell><cell>PSO</cell><cell>4.45E-01</cell><cell cols="4">1.70E+00 1.65E+00 3.45E+00 8.11E-01</cell><cell>10.90</cell></row><row><cell></cell><cell>GA</cell><cell>6.34E-05</cell><cell>8.22E-04</cell><cell>5.74E-04</cell><cell>3.46E-03</cell><cell>8.10E-04</cell><cell>6.77</cell></row><row><cell></cell><cell>SA</cell><cell>7.43E-02</cell><cell>1.50E-01</cell><cell>1.46E-01</cell><cell>2.45E-01</cell><cell>4.34E-02</cell><cell>8</cell></row><row><cell></cell><cell>DE</cell><cell cols="2">0.00E+00 3.21E-02</cell><cell cols="2">0.00E+00 4.81E-01</cell><cell>1.22E-01</cell><cell>2.62</cell></row><row><cell></cell><cell cols="2">CMA-ES 2.89E-02</cell><cell cols="5">1.91E+00 1.96E+00 8.74E+00 2.16E+00 10.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 18 .</head><label>18</label><figDesc>Statistical test and optimization results obtained by reported optimizers (D=50).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Average Rankings</cell></row><row><cell cols="3">Function Methods Best</cell><cell>Average</cell><cell>Median</cell><cell>Worst</cell><cell>SD</cell><cell>Friedman Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Ranking)</cell></row><row><cell></cell><cell>NNA</cell><cell>4.63E-04</cell><cell cols="5">1.39E+00 1.07E+00 9.60E+00 1.79E+00 2.50</cell></row><row><cell></cell><cell>RS</cell><cell cols="6">5.05E+02 5.89E+02 5.93E+02 6.96E+02 4.87E+01 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">4.58E+01 9.46E+01 1.01E+02 1.45E+02 2.82E+01 8.93</cell></row><row><cell></cell><cell>ICA</cell><cell cols="6">1.17E+02 2.32E+02 2.33E+02 3.37E+02 5.05E+01 11.67</cell></row><row><cell></cell><cell>CS</cell><cell>9.95E-01</cell><cell cols="5">3.91E+00 2.89E+00 1.29E+01 3.08E+00 3.73</cell></row><row><cell></cell><cell>GSA</cell><cell cols="6">6.96E+00 1.88E+01 1.59E+01 6.27E+01 1.12E+01 5.83</cell></row><row><cell>17</cell><cell>WCA</cell><cell cols="6">1.69E+01 3.91E+01 3.58E+01 1.33E+02 2.12E+01 7.07</cell></row><row><cell></cell><cell>HS</cell><cell>3.34E-03</cell><cell>7.11E-01</cell><cell>2.55E-01</cell><cell cols="3">4.18E+00 1.05E+00 1.80</cell></row><row><cell>18</cell><cell cols="7">PSO GA SA DE CMA-ES 9.85E+01 1.20E+02 1.22E+02 1.38E+02 8.94E+00 9.83 1.23E+02 2.12E+02 2.08E+02 3.60E+02 6.29E+01 11.23 5.10E-04 2.83E+00 1.26E+00 1.30E+01 3.72E+00 2.67 2.91E-01 8.53E+00 5.82E+00 2.65E+01 6.77E+00 4.57 3.57E+01 7.15E+01 7.04E+01 1.18E+02 2.29E+01 8.17 NNA 1.90E+00 1.14E+01 1.07E+01 3.39E+01 7.23E+00 2.73 RS 3.26E+02 3.51E+02 3.52E+02 3.73E+02 9.60E+00 13 TLBO 1.28E+00 1.62E+01 1.62E+01 3.26E+01 8.29E+00 4.40 ICA 1.87E+02 2.61E+02 2.68E+02 3.48E+02 2.85E+01 11.87 CS 1.81E+01 5.58E+01 5.65E+01 9.87E+01 2.31E+01 8.57 GSA 4.05E-01 1.72E+01 1.30E+01 7.79E+01 1.61E+01 3.87 WCA 1.31E+02 2.14E+02 2.07E+02 3.12E+02 3.90E+01 11.13 HS 1.53E+01 2.46E+01 2.18E+01 5.91E+01 8.48E+00 6.23 PSO GA 4.82E+00 1.61E+01 1.36E+01 3.48E+01 7.57E+00 4.27 SA 7.36E+00 1.69E+01 1.53E+01 3.10E+01 6.60E+00 4.33 DE 2.39E-01 CMA-ES 2.27E+01 3.54E+01 3.45E+01 5.06E+01 7.25E+00 7.70 NNA 1.08E-01 RS 3.81E+02 4.08E+02 4.11E+02 4.24E+02 1.18E+01 11.80 TLBO 1.66E-08 1.58E+01 1.92E+00 1.69E+02 3.61E+01 3.07 ICA 2.78E+02 3.59E+02 3.54E+02 4.72E+02 5.35E+01 11 M 4.21E+00 3.21E+00 1.38E+01 3.78E+00 2.83 A 1.08E+01 1.02E+01 2.15E+01 6.22E+00 2.90 N 8.90E+01 1.30E+02 1.32E+02 1.71E+02 1.77E+01 10 U S C R I P T</cell></row><row><cell cols="8">19 C C E P T E D CS 1.16E+02 1.75E+02 1.76E+02 2.47E+02 2.93E+01 8.13 GSA 3.77E-01 6.95E+01 7.52E+01 1.58E+02 3.59E+01 6.20 WCA 2.03E+02 3.08E+02 3.09E+02 3.78E+02 3.89E+01 10.23 HS 1.80E+01 4.71E+01 4.02E+01 1.18E+02 2.80E+01 5.63 PSO 1.63E+02 2.06E+02 2.05E+02 2.50E+02 2.06E+01 8.83 GA 3.69E+00 1.31E+01 8.48E+00 7.91E+01 1.44E+01 3.83 SA 5.44E+00 4.93E+01 3.30E+01 1.37E+02 4.32E+01 5.37 DE 1.85E-07 1.62E-02 1.07E-02 6.28E-02 2.02E-02 1.10 CMA-ES 4.45E+02 5.18E+02 5.22E+02 5.85E+02 3.26E+01 12.97 NNA 1.68E-03 2.62E+00 2.86E+00 5.59E+00 1.53E+00 3.03 RS 7.53E+01 8.00E+01 8.04E+01 8.41E+01 2.22E+00 13 TLBO 0.00E+00 2.82E+00 2.65E+00 6.80E+00 1.72E+00 3.43 ICA 5.13E+01 6.14E+01 6.15E+01 6.93E+01 4.98E+00 11.97 CS 8.33E+00 1.57E+01 1.47E+01 2.62E+01 4.94E+00 8.83 GSA 2.59E-02 3.20E+00 2.71E+00 1.02E+01 2.31E+00 3.40</cell></row><row><cell>20 A</cell><cell>WCA HS PSO</cell><cell cols="6">3.25E+01 4.68E+01 4.63E+01 6.39E+01 8.29E+00 10.93 3.85E+00 9.98E+00 9.61E+00 1.47E+01 2.67E+00 7.77 2.55E+01 3.21E+01 3.19E+01 4.00E+01 3.52E+00 10.10</cell></row><row><cell></cell><cell>GA</cell><cell>9.35E-01</cell><cell cols="5">4.52E+00 3.88E+00 1.06E+01 2.31E+00 4.87</cell></row><row><cell></cell><cell>SA</cell><cell cols="6">2.03E+00 4.26E+00 4.12E+00 7.85E+00 1.56E+00 4.70</cell></row><row><cell></cell><cell>DE</cell><cell>6.28E-12</cell><cell cols="5">2.48E+00 2.07E+00 5.83E+00 1.74E+00 2.83</cell></row><row><cell></cell><cell cols="7">CMA-ES 3.25E+00 6.13E+00 5.92E+00 8.99E+00 1.61E+00 6.13</cell></row><row><cell></cell><cell>NNA</cell><cell>4.39E-72</cell><cell>8.85E-38</cell><cell>4.49E-49</cell><cell>2.66E-36</cell><cell>4.85E-37</cell><cell>3</cell></row><row><cell>21</cell><cell>RS</cell><cell cols="6">1.17E+03 1.43E+03 1.45E+03 1.62E+03 1.12E+02 13</cell></row><row><cell></cell><cell>TLBO</cell><cell cols="6">0.00E+00 0.00E+00 0.00E+00 0.00E+00 0.00E+00 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 19 .</head><label>19</label><figDesc>Sum of average ranking using Friedman test for applied optimization methods.</figDesc><table><row><cell cols="2">Methods Total Average Ranking by Friedman Test (Rank)</cell></row><row><cell>NNA</cell><cell>84.41 (1)</cell></row><row><cell>RS</cell><cell>271.74 (13)</cell></row><row><cell>TLBO</cell><cell>103.21 (3)</cell></row><row><cell>ICA</cell><cell>232.52 (12)</cell></row><row><cell>CS</cell><cell>139.35 (8)</cell></row><row><cell>GSA</cell><cell>107.27 (4)</cell></row><row><cell>WCA</cell><cell>163.17 (10)</cell></row><row><cell>HS</cell><cell>126.03 (7)</cell></row><row><cell>PSO</cell><cell>206.81 (11)</cell></row><row><cell>GA</cell><cell>116.06 (5)</cell></row><row><cell>SA</cell><cell>140.05 (9)</cell></row><row><cell>DE</cell><cell>85.33 (2)</cell></row><row><cell cols="2">CMA-ES 125.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 20 .</head><label>20</label><figDesc>Statistical test using multiple comparison test for considered optimizers (Continued).</figDesc><table><row><cell>Function Comparing</cell><cell>LL (95%)</cell><cell cols="2">Group Means UL (95%)</cell><cell>p-value (α=0.05)</cell></row><row><cell>NNA vs. TLBO</cell><cell cols="2">-1.5304e+03 -9.3549e-10</cell><cell cols="2">1.5304e+03 1</cell></row><row><cell>NNA vs. RS</cell><cell cols="2">-9.1758e+04 -9.0228e+04</cell><cell cols="2">-8.8697e+04 2.84e-07</cell></row><row><cell>NNA vs. ICA</cell><cell cols="2">-1.5648e+03 -3.4409e+01</cell><cell cols="2">1.4960e+03 1</cell></row><row><cell cols="5">NNA vs. CS NNA vs. GSA NNA vs. WCA NNA vs. HS NNA vs. PSO NNA vs. GA NNA vs. SA NNA vs. DE NNA vs. CMA-ES -1.5304e+03 2.2468e-10 -1.5304e+03 2.2468e-10 -1.5304e+03 2.2463e-10 -1.5304e+03 2.2438e-10 -1.5304e+03 -2.3587e-04 -1.5540e+03 -2.3575e+01 -1.5304e+03 -2.7148e-03 -1.5307e+03 -3.2074e-01 -1.5304e+03 2.2462e-10 NNA vs. TLBO -4.5988e+01 -4.0766e+01 NNA vs. RS -8.9498e+01 -8.4276e+01 NNA vs. ICA -5.9577e+01 -5.4356e+01 NNA vs. CS -4.8874e+01 -4.3653e+01 NNA vs. GSA -1.4031e+01 -8.8092e+00 NNA vs. WCA -1.0459e+01 -5.2377e+00 NNA vs. HS -6.3981e+00 -1.1766e+00 NNA vs. PSO -3.0427e+01 -2.5206e+01 NNA vs. GA -5.5694e+00 -3.4797e-01 NNA vs. DE -3.3341e+01 -2.8119e+01 NNA vs. CMA-ES -4.1931e+00 1.0284e+00 NNA vs. TLBO -1.5820e+09 -3.5838e+02 M NNA vs. SA -5.6457e+00 -4.2423e-01 A N U 1.5304e+03 1 1.5304e+03 1 1.5304e+03 1 1.5304e+03 1 1.5068e+03 1 1.5304e+03 1 1.5301e+03 1 1.5304e+03 1 1.5304e+03 1 -3.5545e+01 2.84e-07 -7.9055e+01 2.84e-07 -4.9134e+01 2.84e-07 -3.8432e+01 2.84e-07 -3.5878e+00 2.00e-06 4.0448e+00 9.99e-01 4.8735e+00 1 -2.2898e+01 2.84e-07 6.2499e+00 9.99e-01 1.5820e+09 1 4.7972e+00 1 -1.9984e+01 2.84e-07 -1.6191e-02 4.84e-02 S C R I P T</cell></row><row><cell cols="3">NNA vs. RS NNA vs. ICA NNA vs. CS NNA vs. GSA NNA vs. WCA NNA vs. HS NNA vs. PSO NNA vs. GA NNA vs. SA NNA vs. DE NNA vs. CMA-ES -1.5820e+09 -1.4167e+03 -3.8760e+10 -3.7178e+10 -1.5821e+09 -6.6734e+04 -1.1582e+10 -1.0000e+10 -1.5820e+09 -5.9479e+00 -1.5820e+09 -1.9716e+00 -1.5820e+09 -4.9764e+02 -1.5820e+09 -2.8502e+04 -1.5820e+09 -2.8681e+02 -1.5820e+09 -8.2124e+02 -1.5820e+09 4.1224e+01 NNA vs. TLBO -1.3502e+02 -1.0138e+02 NNA vs. RS -6.7095e+02 -6.3732e+02 NNA vs. ICA -3.7557e+02 -3.4193e+02 NNA vs. CS -5.7738e+01 -2.4104e+01 NNA vs. GSA -7.1208e+01 -3.7574e+01 NNA vs. WCA -2.1790e+02 -1.8426e+02 NNA vs. HS -2.7076e+01 6.5590e+00 C C E P T E D A</cell><cell cols="2">-3.5597e+10 2.84e-07 1.5819e+09 1 -8.4180e+09 2.84e-07 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 1.5820e+09 1 -6.7749e+01 2.84e-07 -6.0368e+02 2.84e-07 -3.0830e+02 2.84e-07 9.5306e+00 4.62e-01 -1.5063e+02 2.84e-07 4.0194e+01 9.99e-01 -3.9390e+00 1.34e-02</cell></row><row><cell>NNA vs. PSO</cell><cell cols="2">-1.8450e+02 -1.5086e+02</cell><cell cols="2">-1.1723e+02 2.84e-07</cell></row><row><cell>NNA vs. GA</cell><cell cols="2">-3.6918e+01 -3.2836e+00</cell><cell cols="2">3.0351e+01 1</cell></row><row><cell>NNA vs. SA</cell><cell cols="2">-3.5277e+01 -1.6425e+00</cell><cell cols="2">3.1992e+01 1</cell></row><row><cell>NNA vs. DE</cell><cell cols="2">-2.7360e+02 -2.3997e+02</cell><cell cols="2">-2.0633e+02 2.84e-07</cell></row><row><cell cols="3">NNA vs. CMA-ES -2.9429e+02 -2.6066e+02</cell><cell cols="2">-2.2702e+02 2.84e-07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 21 .</head><label>21</label><figDesc>Statistical test using multiple comparison test for considered optimizers (Continued).</figDesc><table><row><cell></cell><cell>Function Comparing</cell><cell>LL (95%)</cell><cell cols="2">Group Means UL (95%)</cell><cell>p-value (α=0.05)</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-1.5325e+01 -8.9279e-02</cell><cell>1.5147e+01 1</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-8.5093e+02 -8.3569e+02</cell><cell>-8.2046e+02 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-1.5708e+01 -4.7225e-01</cell><cell>1.4764e+01 1</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-1.5179e+01 5.6542e-02</cell><cell>1.5293e+01 1</cell></row><row><cell></cell><cell>NNA vs. GSA</cell><cell cols="2">-4.1461e+01 -2.6225e+01</cell><cell>-1.0989e+01 1.17e-06</cell></row><row><cell></cell><cell>NNA vs. WCA</cell><cell cols="2">-1.5204e+01 3.1940e-02</cell><cell>1.5268e+01 1</cell></row><row><cell></cell><cell>NNA vs. HS</cell><cell cols="2">-1.5185e+01 5.0946e-02</cell><cell>1.5287e+01 1</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-1.6390e+01 -1.1537e+00</cell><cell>1.4082e+01 1</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-1.5182e+01 5.4354e-02</cell><cell>1.5290e+01 1</cell></row><row><cell></cell><cell>NNA vs. SA</cell><cell cols="2">-1.6384e+01 -1.1477e+00</cell><cell>1.4088e+01 1</cell></row><row><cell></cell><cell cols="4">NNA vs. DE NNA vs. CMA-ES -1.5174e+01 6.2032e-02 -1.5174e+01 6.1786e-02 NNA vs. TLBO -1.1802e+01 -9.8179e+00 NNA vs. RS -2.2340e+01 -2.0356e+01 NNA vs. ICA -1.9888e+01 -1.7904e+01 NNA vs. CS -3.9009e+00 -1.9172e+00 NNA vs. GSA -1.9837e+00 1.1150e-06 NNA vs. WCA -1.3086e+01 -1.1102e+01 NNA vs. HS -1.9923e+00 -8.5488e-03 NNA vs. PSO -1.4005e+01 -1.2022e+01 NNA vs. GA -1.9945e+00 -1.0723e-02 NNA vs. SA -2.4097e+00 -4.2592e-01 NNA vs. DE -1.9837e+00 1.1182e-06 NNA vs. CMA-ES -1.9837e+00 1.1182e-06 NNA vs. TLBO -5.3682e+11 3.9874e-11 NNA vs. RS -1.6378e+12 -1.1010e+12 N U 1.5298e+01 1 1.5298e+01 1 -7.8341e+00 2.84e-07 -1.8372e+01 2.84e-07 -1.5920e+01 2.84e-07 6.6582e-02 7.00e-02 1.9837e+00 1 -9.1182e+00 2.84e-07 1.9752e+00 1 -1.0038e+01 2.84e-07 1.9730e+00 1 1.5578e+00 9.99e-01 1.9837e+00 1 5.3682e+11 1 -5.6417e+11 2.85e-07 1.9837e+00 1 S C R I P T</cell></row><row><cell></cell><cell>NNA vs. ICA NNA vs. CS NNA vs. GSA NNA vs. WCA NNA vs. HS NNA vs. PSO</cell><cell cols="2">-5.3684e+11 -2.1503e+07 -5.3682e+11 -1.7940e-03 -5.3682e+11 -3.3564e-08 -5.3682e+11 -3.3333e-01 -5.3682e+11 -5.9855e-02 -5.3682e+11 -2.1495e+00 M A</cell><cell>5.3679e+11 1 5.3682e+11 1 5.3682e+11 1 5.3682e+11 1 5.3682e+11 1 5.3682e+11 1</cell></row><row><cell cols="4">NNA vs. GA NNA vs. SA NNA vs. DE NNA vs. CMA-ES -5.3682e+11 3.9446e-11 -5.3682e+11 -1.6662e-02 -5.3682e+11 -5.3683e-01 -5.3682e+11 3.4775e-11 NNA vs. RS -7.6495e+05 -7.5615e+05 NNA vs. TLBO -4.1939e+01 5.1853e-20 NNA vs. ICA -4.3951e+01 -2.0121e+00 NNA vs. CS -4.1939e+01 5.1853e-20 NNA vs. GSA -4.1939e+01 -3.5835e-16 NNA vs. WCA -4.1939e+01 5.1774e-20 NNA vs. HS -4.1944e+01 -4.9857e-03 NNA vs. PSO -3.3598e+02 -2.9405e+02 NNA vs. GA -4.1976e+01 -3.7315e-02 NNA vs. SA -4.2327e+01 -3.8808e-01 NNA vs. DE -4.1939e+01 2.5548e-20 C C E P T E D</cell><cell>5.3682e+11 1 5.3682e+11 1 5.3682e+11 1 5.3682e+11 1 -7.4736e+05 2.84e-07 4.1939e+01 1 3.9927e+01 1 4.1939e+01 1 4.1939e+01 1 4.1939e+01 1 4.1934e+01 1 -2.5211e+02 2.24e-07 4.1902e+01 1 4.1551e+01 1 4.1939e+01 1</cell></row><row><cell>A</cell><cell cols="3">NNA vs. CMA-ES -4.1939e+01 5.1847e-20</cell><cell>4.1939e+01 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 23 .</head><label>23</label><figDesc>Statistical test using multiple comparison test for considered optimizers (Continued).</figDesc><table><row><cell>Function Comparing</cell><cell>LL (95%)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Group Means UL (95%) p-value (α=0.05)</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 24 .</head><label>24</label><figDesc>Statistical test using multiple comparison test for considered optimizers.</figDesc><table><row><cell cols="2">Function Comparing</cell><cell>LL (95%)</cell><cell cols="2">Group Means UL (95%)</cell><cell>p-value (α=0.05)</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-6.1238e+02 -5.8760e+02</cell><cell>-5.6281e+02 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-1.1569e+02 -9.3175e+01</cell><cell>-7.0656e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-2.5318e+02 -2.3066e+02</cell><cell>-2.0814e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-2.5044e+01 -2.5250e+00</cell><cell>1.9994e+01 1</cell></row><row><cell></cell><cell>NNA vs. GSA</cell><cell cols="2">-3.9903e+01 -1.7384e+01</cell><cell>5.1344e+00 3.25e-01</cell></row><row><cell>17</cell><cell>NNA vs. WCA NNA vs. HS</cell><cell cols="2">-6.0182e+01 -3.7663e+01 -2.1843e+01 6.7580e-01</cell><cell>-1.5145e+01 3.18e-06 2.3195e+01 1</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-2.3317e+02 -2.1065e+02</cell><cell>-1.8813e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-2.3961e+01 -1.4426e+00</cell><cell>2.1076e+01 1</cell></row><row><cell></cell><cell>NNA vs. SA</cell><cell cols="2">-2.9663e+01 -7.1444e+00</cell><cell>1.5374e+01 9.96e-01</cell></row><row><cell></cell><cell>NNA vs. DE</cell><cell cols="2">-9.2664e+01 -7.0145e+01</cell><cell>-4.7626e+01 2.24e-07</cell></row><row><cell></cell><cell cols="3">NNA vs. CMA-ES -1.4153e+02 -1.1901e+02</cell><cell>-9.6494e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-3.5426e+02 -3.3942e+02</cell><cell>-3.2457e+02 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-1.9874e+01 -4.8127e+00</cell><cell>1.0249e+01 9.96e-01</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-2.6433e+02 -2.4927e+02</cell><cell>-2.3421e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-5.9528e+01 -4.4466e+01</cell><cell>-2.9405e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GSA</cell><cell cols="2">-2.0876e+01 -5.8146e+00</cell><cell>9.2470e+00 9.83e-01</cell></row><row><cell>18</cell><cell>NNA vs. WCA NNA vs. HS</cell><cell cols="2">-2.1763e+02 -2.0257e+02 -2.8288e+01 -1.3226e+01</cell><cell>-1.8751e+02 2.24e-07 1.8353e+00 1.51e-01</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-1.3393e+02 -1.1887e+02</cell><cell>-1.0380e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-1.9780e+01 -4.7186e+00</cell><cell>1.0343e+01 9.97e-01</cell></row><row><cell></cell><cell>NNA vs. SA</cell><cell cols="2">-2.0567e+01 -5.5053e+00</cell><cell>9.5562e+00 9.89e-01</cell></row><row><cell></cell><cell>NNA vs. DE</cell><cell cols="2">-1.4451e+01 6.1102e-01</cell><cell>1.5673e+01 1</cell></row><row><cell></cell><cell cols="3">NNA vs. CMA-ES -3.9128e+01 -2.4067e+01</cell><cell>-9.0053e+00 1.16e-05</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-4.3049e+02 -4.0415e+02</cell><cell>-3.7781e+02 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-3.8497e+01 -1.1606e+01</cell><cell>1.5285e+01 9.61e-01</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-3.8215e+02 -3.5526e+02</cell><cell>-3.2837e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-1.9767e+02 -1.7078e+02</cell><cell>-1.4389e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GSA</cell><cell cols="2">-9.2211e+01 -6.5320e+01</cell><cell>-3.8429e+01 2.24e-07</cell></row><row><cell>19</cell><cell>NNA vs. WCA NNA vs. HS</cell><cell cols="2">-3.3106e+02 -3.0417e+02 -6.9754e+01 -4.2863e+01</cell><cell>-2.7728e+02 2.24e-07 -1.5973e+01 1.24e-05</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-2.2835e+02 -2.0146e+02</cell><cell>-1.7457e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-3.5814e+01 -8.9235e+00</cell><cell>1.7967e+01 9.95e-01</cell></row><row><cell></cell><cell>NNA vs. SA</cell><cell cols="2">-7.1955e+01 -4.5064e+01</cell><cell>-1.8173e+01 3.01e-06</cell></row><row><cell></cell><cell>NNA vs. DE</cell><cell cols="2">-2.2693e+01 4.1981e+00</cell><cell>3.1089e+01 1</cell></row><row><cell></cell><cell cols="3">NNA vs. CMA-ES -5.4063e+02 -5.1374e+02</cell><cell>-4.8685e+02 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-8.0428e+01 -7.7368e+01</cell><cell>-7.4308e+01 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-3.2941e+00 -1.9853e-01</cell><cell>2.8970e+00 1</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-6.1910e+01 -5.8815e+01</cell><cell>-5.5719e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-1.6149e+01 -1.3053e+01</cell><cell>-9.9578e+00 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GSA</cell><cell cols="2">-3.6799e+00 -5.8434e-01</cell><cell>2.5112e+00 9.99e-01</cell></row><row><cell>20</cell><cell>NNA vs. WCA NNA vs. HS</cell><cell cols="2">-4.7301e+01 -4.4206e+01 -1.0453e+01 -7.3577e+00</cell><cell>-4.1110e+01 2.24e-07 -4.2622e+00 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-3.2574e+01 -2.9478e+01</cell><cell>-2.6383e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-4.9925e+00 -1.8970e+00</cell><cell>1.1986e+00 6.91e-01</cell></row><row><cell></cell><cell>NNA vs. SA</cell><cell cols="2">-4.7391e+00 -1.6436e+00</cell><cell>1.4520e+00 8.52e-01</cell></row><row><cell></cell><cell>NNA vs. DE</cell><cell cols="2">-2.9531e+00 1.4244e-01</cell><cell>3.2380e+00 1</cell></row><row><cell></cell><cell cols="3">NNA vs. CMA-ES -6.6035e+00 -3.5079e+00</cell><cell>-4.1239e-01 1.14e-02</cell></row><row><cell></cell><cell>NNA vs. RS</cell><cell cols="2">-1.4598e+03 -1.4333e+03</cell><cell>-1.4068e+03 2.84e-07</cell></row><row><cell></cell><cell>NNA vs. TLBO</cell><cell cols="2">-1.3751e+00 8.8501e-38</cell><cell>1.3751e+00 1</cell></row><row><cell></cell><cell>NNA vs. ICA</cell><cell cols="2">-1.0209e+01 -8.8336e+00</cell><cell>-7.4585e+00 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. CS</cell><cell cols="2">-6.5445e+00 -5.1694e+00</cell><cell>-3.7943e+00 2.24e-07</cell></row><row><cell>21</cell><cell>NNA vs. GSA</cell><cell cols="2">-1.3751e+00 -8.7725e-33</cell><cell>1.3751e+00 1</cell></row><row><cell></cell><cell>NNA vs. WCA</cell><cell cols="2">-2.2983e+00 -9.2328e-01</cell><cell>4.5180e-01</cell><cell>5.53e-01</cell></row><row><cell></cell><cell>NNA vs. HS</cell><cell cols="2">-1.3788e+00 -3.7665e-03</cell><cell>1.3713e+00 1</cell></row><row><cell></cell><cell>NNA vs. PSO</cell><cell cols="2">-2.2369e+01 -2.0994e+01</cell><cell>-1.9619e+01 2.24e-07</cell></row><row><cell></cell><cell>NNA vs. GA</cell><cell cols="2">-1.3768e+00 -1.7022e-03</cell><cell>1.3734e+00 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 25 .</head><label>25</label><figDesc>Statistical test and optimization results obtained by recent optimizers for dimension 200.From Table25, the NNA obtained the lower total average ranking which shows its overall efficiency compared with the considered optimizers. For the most cases, the NNA has been placed in the first or second ranks. Now, it is interesting to see the performance of the NNA over few iterations. Therefore, Figs. 10 and 11 display the total cost reduction history of reported optimizers over only 200 iterations between selected optimizer given in Table25. It is worth mentioning that all reported optimizers start with a random initial population between the lower and upper bounds. By observing Figs.10 and 11, the NNA has the slowest convergence rate as iteration is increasing.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">U S C R I P T</cell></row><row><cell>Function 1</cell><cell cols="4">Methods Best GSA 3.41E-13 5.00E-13 4.97E-13 6.65E-13 Average Median Worst TLBO 7.00E-01 4.20E+02 4.09E+01 5.40E+03 NNA 6.04E-10 4.82E-07 4.9E-09 7.03E-06 M A N</cell><cell>SD 1.02E+03 3 Rankings Friedman Test 1.81E-06 1.93 9.81E-14 1.06 Average</cell></row><row><cell cols="5">2 3 4 5 6 7 A C C E P T E D GSA 4.16E+01 6.27E+01 6.12E+01 8.21E+01 TLBO 5.99E+01 6.88E+01 6.91E+01 7.71E+01 NNA 1.56E+01 2.40E+01 2.41E+01 2.83E+01 GSA 3.38E+02 4.91E+02 4.83E+02 6.73E+02 TLBO 1.71E+04 6.35E+06 3.27E+05 7.95E+07 NNA 3.28E+02 4.79E+02 4.32E+02 9.35E+02 GSA 2.78E+02 4.21E+02 4.26E+02 5.43E+02 TLBO 4.82E+02 6.19E+02 6.20E+02 8.14E+02 NNA 1.93E+02 2.37E+02 2.39E+02 2.95E+02 GSA 7.32E+01 1.08E+02 1.09E+02 1.41E+02 TLBO 1.74E-01 3.89E+00 1.69E+00 2.98E+01 NNA 3.95E-10 2.13E-02 1.48E-02 1.32E-01 GSA 1.63E+00 2.59E+00 2.64E+00 3.25E+00 TLBO 1.59E+01 1.66E+01 1.66E+01 1.71E+01 NNA 2.72E-06 1.81E-04 1.52E-05 1.56E-03 GSA 2.02E-07 2.83E-07 2.89E-07 3.81E-07 TLBO 0.00E+00 0.00E+00 0.00E+00 0.00E+00 NNA 4.48E-10 2.04E-09 1.69E-09 8.07E-09</cell><cell>1.38E+01 2.20 3.92E+00 2.80 3.33E+00 1 8.83E+01 1.6 1.77E+07 3 1.76E+02 1.40 7.83E+01 2 7.56E+01 2.93 2.91E+01 1.06 2.22E+01 3 6.58E+00 2 3.24E-02 1 4.21E-01 2 3.30E-01 3 4.27E-04 1 0.00E+00 1 1.98E-09 2 5.43E-08 3</cell></row><row><cell></cell><cell>GSA</cell><cell cols="3">1.17E-14 1.77E-14 1.79E-14 2.3E-14</cell><cell>3.45E-15 3</cell></row><row><cell>8</cell><cell>TLBO</cell><cell cols="3">0.00E+00 0.00E+00 0.00E+00 0.00E+00</cell><cell>0.00E+00 1</cell></row><row><cell></cell><cell>NNA</cell><cell>6.98E-19 9.9E-18</cell><cell>5.9E-18</cell><cell>3.98E-17</cell><cell>1.02E-17 2</cell></row><row><cell></cell><cell>GSA</cell><cell cols="3">1.05E+02 1.56E+02 1.58E+02 2.06E+02</cell><cell>3.40E+01 2.13</cell></row><row><cell>9</cell><cell>TLBO</cell><cell cols="3">0.00E+00 0.00E+00 0.00E+00 0.00E+00</cell><cell>0.00E+00 1</cell></row><row><cell></cell><cell>NNA</cell><cell cols="3">1.21E+02 2.09E+02 1.89E+02 3.13E+02</cell><cell>5.56E+01 2.86</cell></row><row><cell>10</cell><cell>GSA</cell><cell cols="3">2.90E+00 3.20E+00 3.14E+00 4.14E+00</cell><cell>6.33E-01 3</cell></row></table><note><p><p><p>value of objective functions at early iterations and although it proceeds with the slow pace, however, with the acceptable reduction in cost values, it reaches around the cost function values attained by other optimizers, and with more number of iterations, this reduction continues (see Table</p>25</p>).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 25 .</head><label>25</label><figDesc>To name a few cases, looking at F10, F12, and F14 shown in Figs.10 and 11, although, after 200 iterations (i.e., 10000 NFEs), the obtained values by the TLBO and GSA are better than the NNA, however, if iteration is increasing, the NNA can obtain better and statistically significance optimization results (see Table</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 26 .</head><label>26</label><figDesc>Computational complexity measured in the NNA.By observing Table26, value of TB/TA equal to one shows the zero complexity from dimension 10 to 30 for the reported benchmarks. Values greater than one represent the complexity of computational time using the NNA. F12 to F21 categorized as hybrid functions (see Table6) have shown higher TB/TA values due to their complication, especially for F19. Relative computationally expensive problems are highlighted in bold in Table26. For the rest of functions, the average TB/TA is almost equal to 3 which means the intricacy for dimension 10 to 30 is increased for almost 3 times.</figDesc><table><row><cell>Function</cell><cell>D = 10 T1</cell><cell cols="2">D = 30 TA=T1/T0 T1</cell><cell>TB=T1/T0</cell><cell>TB/TA</cell></row><row><cell>F1</cell><cell cols="2">5.707 20.775</cell><cell cols="2">16.649 60.607</cell><cell>2.917</cell></row><row><cell>F2</cell><cell cols="2">5.804 21.128</cell><cell cols="2">16.503 60.076</cell><cell>2.843</cell></row><row><cell>F3</cell><cell cols="2">6.080 22.133</cell><cell cols="2">16.618 60.495</cell><cell>2.733</cell></row><row><cell>F4</cell><cell cols="2">5.744 20.910</cell><cell cols="2">17.215 62.668</cell><cell>2.997</cell></row><row><cell>F5</cell><cell cols="2">6.181 22.500</cell><cell cols="2">18.470 67.236</cell><cell>2.988</cell></row><row><cell>F6</cell><cell cols="2">5.546 20.189</cell><cell cols="2">17.669 64.321</cell><cell>3.185</cell></row><row><cell>F7</cell><cell cols="2">5.061 18.423</cell><cell cols="2">15.895 57.863</cell><cell>3.140</cell></row><row><cell>F8</cell><cell cols="2">6.478 23.582</cell><cell cols="2">21.683 78.933</cell><cell>3.347</cell></row><row><cell>F9</cell><cell cols="2">5.611 20.425</cell><cell cols="2">22.298 81.172</cell><cell>3.974</cell></row><row><cell>F10</cell><cell cols="2">5.498 20.014</cell><cell cols="2">17.369 63.228</cell><cell>3.159</cell></row><row><cell>F11</cell><cell cols="2">5.394 19.635</cell><cell cols="2">22.391 81.510</cell><cell>4.151</cell></row><row><cell>F12</cell><cell cols="2">7.447 27.109</cell><cell cols="3">39.200 142.701 5.263</cell></row><row><cell>F13</cell><cell cols="2">5.666 20.626</cell><cell cols="2">22.358 81.390</cell><cell>3.945</cell></row><row><cell>F14</cell><cell cols="2">7.821 28.471</cell><cell cols="3">41.768 152.049 5.340</cell></row><row><cell>F15</cell><cell cols="2">8.004 29.137</cell><cell cols="3">38.654 140.713 4.829</cell></row><row><cell>F16</cell><cell cols="2">7.185 26.155</cell><cell cols="2">27.326 99.475</cell><cell>3.803</cell></row><row><cell>F17</cell><cell cols="2">6.366 23.174</cell><cell cols="2">21.320 77.611</cell><cell>3.349</cell></row><row><cell>F18</cell><cell cols="2">9.340 34.000</cell><cell cols="3">51.172 186.283 5.478</cell></row><row><cell>F19</cell><cell cols="2">8.652 31.496</cell><cell cols="3">51.719 188.274 5.977</cell></row><row><cell>F20</cell><cell cols="2">9.309 33.887</cell><cell cols="3">53.224 193.753 5.717</cell></row><row><cell>F21</cell><cell cols="2">8.294 30.192</cell><cell cols="3">43.839 159.588 5.285</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 27 .</head><label>27</label><figDesc>Comparison of statistical optimization results given by different methods for reported constrained benchmarks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">U S C R I P T</cell></row><row><cell></cell><cell cols="5">Methods PSO GA Pressure vessel design problem Worst Average 6946.41 6147.03 7187.02 6405.38 M Best 5906.30 6001.26 A N SD 2.08E+00 2.88E+00</cell><cell>(Ranking) 5.90 (6) 7.16 (8) Friedman Test</cell></row><row><cell cols="4">SA HS DE GSA ICA WCA TLBO NNA Welded beam design problem 5885.44 5885.37 7317.85 6794.55 5885.32 5885.32 170036 902451 6596.42 5998.37 7319.00 6302.32 5888.93 5885.53 7310.65 6501.62 PSO 1.72916 1.72524 GA 3.80378 2.86337 SA 1.76635 1.74870 HS 3.94678 2.81964 DE 1.72490 1.72490 C C E P T E D</cell><cell>5885.33 6333.48 5885.32 120366 5885.33 5885.33 5885.33 5885.33 1.72485 1.98404 1.73353 2.02476 1.72490</cell><cell>2.67E+00 3.06E+00 9.25E-13 5.39E+04 1.60E+01 5.36E+02 7.01E-01 5.18E+02 8.55E-04 4.65E-01 7.97E-03 4.80E-01 1.12E-15</cell><cell>3.10 (3) 8.30 (9) 1.03 (1) 10 (10) 4.63 (4) 5.50 (5) 2.53 (2) 6.83 (7) 3.80 (3) 9.26 (7) 6.56 (5) 9.26 (7) 3.80 (3)</cell></row><row><cell>A</cell><cell>GSA ICA WCA</cell><cell>2.81882 1.91091 1.75249</cell><cell>2.37691 1.76397 1.72596</cell><cell>2.05103 1.72606 1.72485</cell><cell>1.96E-01 4.80E-02 5.02E-03</cell><cell>8.46 (6) 6.36 (4) 3.80 (3)</cell></row><row><cell></cell><cell>TLBO</cell><cell>1.72485</cell><cell>1.72485</cell><cell>1.72485</cell><cell>1.12E-15</cell><cell>1.36 (1)</cell></row><row><cell></cell><cell>NNA</cell><cell>1.72613</cell><cell>1.72495</cell><cell>1.72485</cell><cell>2.71E-04</cell><cell>2.30 (2)</cell></row><row><cell></cell><cell cols="3">Speed reducer design problem</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PSO</cell><cell>3004.18</cell><cell>2994.47</cell><cell>2994.47</cell><cell>2.87E+00</cell><cell>6.10 (6)</cell></row><row><cell></cell><cell>GA</cell><cell>3853.02</cell><cell>3353.02</cell><cell>3019.19</cell><cell>3.12E+00</cell><cell>9.33 (9)</cell></row><row><cell></cell><cell>SA</cell><cell>2994.56</cell><cell>2994.50</cell><cell>2994.47</cell><cell>2.03E-02</cell><cell>6.83 (8)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 28 .</head><label>28</label><figDesc>Sum of average ranking using Friedman test for applied optimization methods.</figDesc><table><row><cell cols="2">Methods Total Average Ranking by Friedman Test (Rank)</cell></row><row><cell>PSO</cell><cell>20.63 (4)</cell></row><row><cell>GA</cell><cell>38.77 (8)</cell></row><row><cell>SA</cell><cell>33.41 (7)</cell></row><row><cell>HS</cell><cell>39.39 (9)</cell></row><row><cell>DE</cell><cell>24.50 (6)</cell></row><row><cell>GSA</cell><cell>42.76 (10)</cell></row><row><cell>ICA</cell><cell>22.23 (5)</cell></row><row><cell>WCA</cell><cell>20.59 (3)</cell></row><row><cell>TLBO</cell><cell>18.49 (2)</cell></row><row><cell>NNA</cell><cell>14.06 (1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>or in the same set of equation in matrix form can be expressed as given in the following formula:</figDesc><table><row><cell cols="2">X</cell><cell cols="2">1 New</cell><cell>t</cell><cell>1  </cell><cell cols="3">11 1 w X t w X t 21 2 </cell><cell cols="2"> </cell><cell cols="2">1 i w X t i</cell><cell> </cell><cell cols="3">1 w X pop N</cell><cell>N</cell><cell>pop</cell><cell>t</cell></row><row><cell cols="2">X</cell><cell cols="2">2 New</cell><cell>t</cell><cell>1  </cell><cell cols="3">12 1 w X t w X t 22 2 </cell><cell cols="2"> </cell><cell cols="2">2 w X t i i</cell><cell> </cell><cell cols="2">N w</cell><cell>pop</cell><cell>2</cell><cell>X</cell><cell>N</cell><cell>pop</cell><cell>t</cell><cell>(A.2)</cell></row><row><cell cols="2">X</cell><cell cols="2">pop New N</cell><cell>t</cell><cell>1  </cell><cell>1 w X t w 1 2 pop N </cell><cell>N</cell><cell>pop</cell><cell>2 X t</cell><cell cols="2"> </cell><cell cols="2">pop w X t iN i</cell><cell cols="3"> </cell><cell>pop pop N N w</cell><cell>X</cell><cell>N</cell><cell>pop</cell><cell>t</cell></row><row><cell>       </cell><cell cols="2">X X X</cell><cell cols="12">      11 1 11 21 12 22 2 22 1 2 1 1 1 pop pop pop pop pop pop pop pop       N New N New N N N N New NN w ww ww w t X t t X t w w w t X t                            </cell><cell>       </cell><cell>.</cell><cell>(A.3)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head></head><label></label><figDesc>Eq. (A.5) is a system of linear equations which can be represented in matrix form for better understanding as shown in Eq. (A.6):</figDesc><table><row><cell cols="3">1 X t</cell><cell>1</cell><cell cols="2"></cell><cell cols="3">11 w</cell><cell cols="6">1 2  </cell><cell cols="2">rand</cell><cell cols="5">11 r w X    2</cell><cell cols="2">Target</cell><cell></cell><cell cols="5">21 2 w X t</cell><cell> </cell><cell>NN 1 w X</cell><cell>t</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pop</cell><cell>pop</cell></row><row><cell cols="3">2 X t</cell><cell cols="2">1</cell><cell></cell><cell cols="3">22 w</cell><cell cols="7">1 2  </cell><cell>rand</cell><cell>2  </cell><cell cols="5">22 rand w X</cell><cell cols="3">Target</cell><cell cols="3"></cell><cell>12 2 w X t</cell><cell> </cell><cell>N w</cell><cell>pop</cell><cell>2</cell><cell>X</cell><cell>N</cell><cell>pop</cell><cell>t</cell><cell>(A.5)</cell></row><row><cell>X</cell><cell>N</cell><cell>pop</cell><cell>t</cell><cell></cell><cell cols="2">1</cell><cell></cell><cell cols="7">pop pop N N w</cell><cell></cell><cell>1 2  </cell><cell>rand</cell><cell cols="3">2  </cell><cell cols="6">pop pop N N r w</cell><cell cols="3">X</cell><cell>Target</cell><cell></cell><cell>1 w</cell><cell>N</cell><cell>pop</cell><cell>2 X t</cell><cell> </cell><cell>N w</cell><cell>pop</cell><cell>1 </cell><cell>N</cell><cell>pop</cell><cell>X</cell><cell>N</cell><cell>pop</cell><cell>1 </cell><cell>t</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell cols="4">11 w</cell><cell cols="3"> 1 2</cell><cell cols="2">r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">21 w</cell><cell cols="3">31 w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">w N</cell><cell>pop</cell><cell>1</cell><cell>  X t 1</cell><cell>11 w</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell cols="4">22 w</cell><cell cols="4"> 1 2</cell><cell>r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">12 w</cell><cell cols="3">32 w</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">N w</cell><cell>2</cell><cell>  X t 2</cell><cell>2</cell><cell>rX</cell><cell>Target</cell><cell>22 w</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>pop</cell></row><row><cell cols="2">1</cell><cell cols="6">w N N pop pop</cell><cell cols="5"> 1 2</cell><cell>r</cell><cell></cell><cell></cell><cell cols="3">12 w w pop N</cell><cell>N</cell><cell>pop</cell><cell></cell><cell></cell><cell></cell><cell cols="4">w N</cell><cell cols="2">pop</cell><cell>1 </cell><cell>N</cell><cell>pop</cell><cell>X</cell><cell>N</cell><cell>pop</cell><cell>  t</cell><cell>N N w pop pop</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Table9. Experimental optimization results for dimension 50 using reported optimizers (F12-F21).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported by a grant number (7000/1710) funded by Iran's National Elites Foundation.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Convergence analysis of NNA</head><p>In this section, a mathematical analysis of the underlying search mechanism of NNA is discussed. It is expected that such an analysis yields some important guidelines on the performance and validity of NNA. Convergence analysis shows the stability of the algorithm over the large number of iterations.</p><p>The presented analysis is done for the case when N→∞, the evaluation of exact number of N steps and the quality of the solution is experimental aspect and it will vary form problem to problem. The stability model is developed in terms of linear equations, the functions such as rand are assumed to be uniform random variable and with theoretical calculation, their expected value is used for executing the proof. As proposed algorithm is stochastic in nature, therefore it is one of the mathematical way to deal with the stochastic terms. In order to make sure and guarantee that a pattern solution in the NNA finally converges to the target solution, the following theorem is stated and proved.</p><p>Theorem: A typical pattern solution of the NNA population converges to a stable point if the spectral radius of weight matrix is less than 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof: Let</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> </head><p>i Xt be the any member of the population at a iteration t , where 1, 2, 3, pop iN  .</p><p>From Eq. ( <ref type="formula">7</ref>), new pattern solution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head> </head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>New i</head><p>Xt can be written as given follows:</p><p>In general, all new pattern solutions on corresponding previous pattern solutions can be expressed as the following system of equations: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ANNs) in generating new solutions may be resulted developing powerful hybrid optimizers for tackling large-scale optimization problems. Diverse optimization problems emerging in transportation, scheduling, energy saving, sizing optimization, and so forth can be accounted as further studies</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Genetic algorithms in search, optimization and machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Gelatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IJCNN</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1942" to="1948" />
			<date type="published" when="1995">1995</date>
			<pubPlace>Perth, Australia</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A new heuristic optimization algorithm: harmony search</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Geem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Loganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Simulation</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="60" to="68" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adaptation in natural and artificial systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>University of Michigan Press</publisher>
			<pubPlace>Ann Arbor, MI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extensive testing of a hybrid genetic algorithm for solving quadratic assignment problems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Omatu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="64" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A parameter-free self-adapting boundary genetic search for pipe network optimization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mariño</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="102" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimization of arches using genetic algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tayşi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Göğüş</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Özakça</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Optim. Appl</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="394" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Applications of network analysis and multiobjective genetic algorithm for selecting optimal water quality sensor locations in water distribution networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KSCE J. Civ. Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2333" to="2344" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated control of an actively compensated Langmuir probe system using simulated annealing</title>
		<author>
			<persName><forename type="first">L</forename><surname>Nolle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goodyear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Hopgood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Picton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Braithwaite</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">J</forename><surname>St</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="349" to="354" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Electromagnetism-like mechanism and simulated annealing algorithms for flowshop scheduling problems minimizing the total weighted tardiness and makespan</title>
		<author>
			<persName><forename type="first">B</forename><surname>Naderi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tavakkoli-Moghaddam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="77" to="85" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel version of simulated annealing based on linguistic patterns for solving facility layout problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Grobelny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl-Based Syst</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A discrete binary version of the particle swarm algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Eberhart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Sys. Man. Cybern</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4104" to="4108" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization of tile manufacturing process using particle swarm optimization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Navalertporn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Afzulpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using animal instincts to design efficient biomedical studies via particle swarm optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A cooperative particle swarm optimizer with stochastic movements for computationally expensive numerical optimization problems</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="68" to="82" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Jaya, harmony search, and water cycle algorithms for solving large-scale real-life urban traffic light scheduling problem</title>
		<author>
			<persName><forename type="first">K</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lentzakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Swarm Evol. Comput</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="58" to="72" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimizing Urban Traffic Light Scheduling Problem Using Harmony Search with Ensemble of Local Search</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Harmony search algorithm for image reconstruction from projections</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ouaddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boughaci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="924" to="935" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tournament-based harmony search algorithm for non-convex economic load dispatch problem</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Betar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Bolaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="449" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Hassoun</surname></persName>
		</author>
		<title level="m">Fundamentals of artificial neural networks</title>
		<meeting><address><addrLine>Campridge</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural networks for combinatorial optimization: A review on more than a decade of research</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informs J. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="15" to="34" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Introduction to Artificial Neural Systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Zurada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<pubPlace>West, New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Rojas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Springer-verlag</publisher>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
	<note>neural networks</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural Computing for Structural Mechanics</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H V</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahreininejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Saxe-Coburg Publication</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<pubPlace>Edinburgh, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel training of neural networks for finite element mesh decomposition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H V</forename><surname>Topping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahreininejad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Struct</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="693" to="707" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Enhancing Hopfield neural net capabilities in solving optimization problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cavalieri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 World Congress on Neural Networks</title>
		<meeting>the 1996 World Congress on Neural Networks<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="559" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Design and Analysis of Experiments</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Montgomery</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Test suite for the special issue of soft computing on scalability of evolutionary algorithms and other metaheuristics for large scale continuous optimization problems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009-12-23">December 23. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Imperialist competitive algorithm: An algorithm for optimization inspired by imperialistic competition. IEEE CEC</title>
		<author>
			<persName><forename type="first">E</forename><surname>Atashpaz-Gargari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="4661" to="4667" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">GSA: A Gravitational Search Algorithm</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rashedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nezamabadi-Pour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saryazdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Sciences</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2232" to="2248" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Water cycle algorithm -a novel metaheuristic optimization method for solving constrained engineering optimization problems</title>
		<author>
			<persName><forename type="first">H</forename><surname>Eskandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sadollah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bahreininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hamdi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Struct</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">111</biblScope>
			<biblScope unit="page" from="151" to="166" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cuckoo search via Lévy flights</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE World Congress on Nature &amp; Biologically Inspired Computing</title>
		<imprint>
			<biblScope unit="page" from="210" to="214" />
			<date type="published" when="2009">NaBIC 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Teaching-learning-based optimization: A novel method for constrained mechanical design optimization problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Savsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Vakharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Aided Design</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="315" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Differential evolution: a practical approach to global optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">V</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Storn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Lampinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer-Verlag</publisher>
			<biblScope unit="page" from="37" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adapting arbitrary normal mutation distributions in evolution strategies: The covariance matrix adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostermeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1996 IEEE CEC</title>
		<meeting>the 1996 IEEE CEC</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="312" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The use of ranks to avoid the assumption of normality implicit in the analysis of variance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="675" to="701" />
			<date type="published" when="1937">200. 1937</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Use of ranks in one-criterion variance analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Kruskal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="583" to="621" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiple Comparison Procedures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tamhane</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Problem definition and evaluation criteria for CEC 2015 special session and competition on bound constrained singleobjective computationally expensive numerical optimization</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Suganthan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Singapore</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computational Intelligence Laboratory, Zhengzhou University, Zhengzhou China and Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hybridizing particle swarm optimization with differential evolution for constrained numerical and engineering optimization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="629" to="640" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Artificial bee colony algorithm for large-scale problems and engineering design optimization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Akay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Karaboga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Manuf</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1001" to="1014" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An efficient constraint handling method for genetic algorithms</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Methods Appl. Mech. Engrg</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="311" to="338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simple explanation of the no free lunch theorem of optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Pepyne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40 th IEEE Decision and Control Conference</title>
		<meeting>the 40 th IEEE Decision and Control Conference<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A study on self-configuration in the differential evolution algorithm</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R R</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Guimaraes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Symposium on Differential Evolution (SDE)</title>
		<meeting><address><addrLine>Orlando, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Explicit solution to the stochastic system of linear algebraic equations (α1 A1+ α2 A2+⋯+ αm Am) x= b</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R J</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Method Appl. M</title>
		<imprint>
			<biblScope unit="volume">195</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="6560" to="6576" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
