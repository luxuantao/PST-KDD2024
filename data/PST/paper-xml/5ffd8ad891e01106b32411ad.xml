<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Real-World Blind Face Restoration with Generative Facial Prior</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-01-11">11 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
							<email>xintaowang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yu</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Honglun</forename><surname>Zhang</surname></persName>
							<email>honlanzhang@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<email>yingsshan@tencent.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Applied Research Center (ARC)</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Real-World Blind Face Restoration with Generative Facial Prior</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-11">11 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2101.04061v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Comparisons with state-of-the-art face restoration methods: HiFaceGAN [69], DFDNet [46], Wan et al. [63] and PULSE [54] on the real-world low-quality images. While previous methods struggle to restore faithful facial details or retain face identity, our proposed GFP-GAN achieves a good balance of realness and fidelity with much less artifacts. In addition, the powerful generative facial prior allows us to perform restoration and color enhancement jointly.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Blind face restoration aims at recovering high-quality faces from the low-quality counterparts suffering from unknown degradation, such as low-resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b8">9]</ref>, noise <ref type="bibr" target="#b71">[72]</ref>, blur <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b59">60]</ref>, compression artifacts <ref type="bibr" target="#b12">[13]</ref>, etc. When applied to real-world scenarios, it becomes more challenging, due to more complicated degradation, diverse poses and expressions. Previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b5">6]</ref> typically exploit face-specific priors in face restoration, such as facial landmarks <ref type="bibr" target="#b8">[9]</ref>, parsing maps <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, facial component heatmaps <ref type="bibr" target="#b69">[70]</ref>, and show that those geometry facial priors are pivotal to recover accurate face shape and details. However, those priors are usually estimated from input images and inevitably degrades with very low-quality inputs in the real world. In addition, despite their semantic guidance, the above priors contain limited texture information for restoring facial details (e.g., eye pupil).</p><p>Another category of approaches investigates reference priors, i.e., high-quality guided faces <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12]</ref> or facial component dictionaries <ref type="bibr" target="#b45">[46]</ref>, to generate realistic results and alleviate the dependency on degraded inputs. However, the inaccessibility of high-resolution references limits its practical applicability, while the fixed capacity of dictionaries restricts its diversity and richness of facial details.</p><p>In this study, we leverage Generative Facial Prior (GFP) for real-world blind face restoration, i.e., the prior implicitly encapsulated in pretrained face Generative Adversarial Network (GAN) <ref type="bibr" target="#b18">[19]</ref> models such as StyleGAN <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>. These face GANs are capable of generating faithful faces with a high degree of variability, and thereby providing rich and diverse priors such as geometry, facial textures and colors, making it possible to jointly restore facial details and enhance colors (ours in Fig. <ref type="figure" target="#fig_3">1</ref>). However, it is challenging to incorporate such generative priors into the restoration process. Previous attempts typically use GAN inversion <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b53">54]</ref>. They first 'invert' the degraded image back to a latent code of the pretrained GAN, and then conduct expensive image-specific optimization to reconstruct images. Despite visually realistic outputs, they usually produce images with low fidelity, as the low-dimension latent codes are insufficient to guide accurate restoration.</p><p>To address these challenges, we propose a novel GFP-GAN with delicate designs to achieve a good balance of realness and fidelity in a single forward pass. In specific, GFP-GAN consists of a degradation removal module and a pretrained face GAN as facial prior. They are connected by a direct latent code mapping, and several Channel-Split Spatial Feature Transform (CS-SFT) layers in a coarse-tofine manner. The proposed CS-SFT layers perform spatial modulation on a split of features and leave the left features to directly pass through for better information preservation, allowing our method to effectively incorporate generative prior while retraining high fidelity. Besides, we introduce facial component loss with local discriminators to further enhance perceptual facial details, while employing identity preserving loss to further improve fidelity.</p><p>We summarize the contributions as follows.</p><p>(1) We leverage rich and diverse generative facial priors for bind face restoration. Those priors contain sufficient facial textures and vivid color information, allowing us to jointly perform face restoration and color enhancement. <ref type="bibr" target="#b1">(2)</ref> We propose a novel GFP-GAN framework with delicate designs of architectures and losses to incorporate generative facial prior. Our GFP-GAN with CS-SFT layers achieves a good balance of fidelity and texture faithfulness in a single forward pass. <ref type="bibr" target="#b2">(3)</ref> Extensive experiments show that our method achieves superior performance to prior art on both synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image Restoration typically includes super-resolution <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b51">52]</ref>, denoising <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b26">27]</ref>, deblurring <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60]</ref> and compression removal <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. To achieve visually-pleasing results, generative adversarial network <ref type="bibr" target="#b18">[19]</ref> is usually employed as loss supervisions to push the solutions closer to the natural manifold <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>, while our work attempts to leverage the pretrained GANs as generative facial priors (GFP). Face Restoration. Based on general face hallucination <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b70">71]</ref>, two typical face-specific priors: geometry priors and reference priors, are incorporated to further improve the performance. The geometry priors include facial landmarks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b77">78]</ref>, face parsing maps <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> and facial component heatmaps <ref type="bibr" target="#b69">[70]</ref>. However, 1) those priors require estimations from low-quality inputs and inevitably degrades in real-world scenarios. 2) They mainly focus on geometry constraints and may not contain adequate details for restoration. Instead, our employed GFP does not involve an explicit geometry estimation from degraded images, and contains adequate textures inside its pretrained network.</p><p>Reference priors <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12]</ref> usually rely on reference images of the same identity. To overcome this issue, DFD-Net <ref type="bibr" target="#b45">[46]</ref> suggests to construct a face dictionary of each component (e.g., eyes, mouth) with CNN features to guide the restoration. However, DFDNet mainly focuses on components in the dictionary and thus degrades in the regions beyond its dictionary scope (e.g., hair, ears and face contour), instead, our GFP-GAN could treat faces as a whole to restore. Moreover, the limited size of dictionary restricts its diversity and richness, while the GFP could provide rich and diverse priors including geometry, textures and colors. Generative Priors of pretrained GANs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b2">3]</ref> is previously exploited by GAN inversion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b19">20]</ref>, whose primary aim is to find the closest latent codes given an input image. PULSE <ref type="bibr" target="#b53">[54]</ref> iteratively optimizes the latent code of StyleGAN <ref type="bibr" target="#b35">[36]</ref> until the distance between outputs and inputs is below a threshold. mGANprior <ref type="bibr" target="#b19">[20]</ref> attempts to optimize multiple codes to improve the reconstruction quality. However, these methods usually produce images with low fidelity, as the low-dimension latent codes are insufficient to guide the restoration. In contrast, our proposed CS-SFT modulation layers enable prior incorporation on multi-resolution spatial features to achieve high fidelity. Besides, expensive iterative optimization is not required in our GFP-GAN during inference. Channel Split Operation is usually explored to design compact models and improve model representation ability. MobileNet <ref type="bibr" target="#b28">[29]</ref> proposes depthwise convolutions and GhostNet <ref type="bibr" target="#b23">[24]</ref> splits the convolutional layer into two parts and uses fewer filters to generate intrinsic feature maps.  Dual path architecture in DPN <ref type="bibr" target="#b7">[8]</ref> enables feature re-usage and new feature exploration for each path, thus improving its representation ability. A similar idea is also employed in super-resolution <ref type="bibr" target="#b75">[76]</ref>. Our CS-SFT layers share the similar spirits, but with different operations and purposes. We adopt spatial feature transform <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b56">57]</ref> on one split and leave the left split as identity to achieve a good balance of realness and fidelity. Local Component Discriminators. Local discriminator is proposed to focus on local patch distributions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b63">64]</ref>. When applied to faces, those discriminative losses are imposed on separate semantic facial regions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b20">21]</ref>. Our introduced facial component loss also adopts such designs but with a further style supervision based on the learned discriminative features, which proves to be effective in restoring facial details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of GFP-GAN</head><p>In this section, we describe our proposed GFP-GAN framework that leverages the Generative Facial Prior (GFP) encapsulated in pretrained generative adversarial networks for blind face restoration. Given an input facial image x suffering from unknown degradation, the aim of blind face restoration is to estimate a high-quality image ŷ, which is as similar as possible to the ground-truth image y, in terms of realness and fidelity.</p><p>The overall framework of GFP-GAN is depicted in Fig. <ref type="figure" target="#fig_0">2</ref>. GFP-GAN is comprised of a degradation removal module and a pretrained face GAN (such as Style-GAN2 <ref type="bibr" target="#b36">[37]</ref>) as prior. They are bridged by a latent code mapping and several Channel-Split Spatial Feature Transform (CS-SFT) layers. In specific, the degradation re-moval module is designed to remove complicated degradation in the input x from the real word, and extract clean latent features F latent and multi-resolution spatial features F spatial for subsequent operations (see Sec. 3.2). After that, F latent is mapped to intermediate latent codes W, coarsely 'retrieving' features of a close face, denoted by F prior , in the leaned face GAN distributions (see Sec. 3.3). Multiresolution features F spatial are used to spatially modulate the face GAN features F prior with the proposed CS-SFT layers in a coarse-to-fine manner, achieving realistic results while preserving high fidelity (see Sec. 3.4).</p><p>During training, except for the global discriminative loss, we introduce facial component loss with discriminators to enhance the perceptually significant face components, i.e., eyes and mouth. In order to retrain identity, we also employ identity preserving guidance. More details are in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Degradation Removal Module</head><p>In real-world scenarios, blind face restoration becomes more challenging due to more complicated and severer degradation, which is typically a mixture of low-resolution, blur, noise and JPEG compression artifacts. Different from previous approaches <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b45">46]</ref> that usually learn to incorporate facial priors and eliminate degradation simultaneously, our degradation removal module is designed to explicitly remove troublesome corruptions and extract 'clean' features with intermediate guidance, thus alleviating the burden of subsequent modules.</p><p>The degradation removal module adopts a UNet [58] structure to 1) increase receptive field for large blur elimination, and 2) to generate multi-resolution features for subsequent operations. We also employ pyramid restoration guidance <ref type="bibr" target="#b41">[42]</ref> for intermediate supervision <ref type="bibr" target="#b61">[62]</ref>. The pyramid guidance at different levels could not only strengthen the restoration ability, but also provide 'clean' features for each resolution, which is required by subsequent coarse-tofine prior incorporation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generative Facial Prior and Latent Code Mapping</head><p>Current face GANs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> are able to generate realistic and vivid human faces with a high variability of identity, pose and expression. We leverage such pretrained face GANs that capture real-life face distributions to provide diverse and rich facial prior, namely generative facial prior, for our task. However, such generative priors could not be directly integrated into the restoration process since the priors are implicitly encapsulated in GANs by mapping random latent codes Z to real faces.</p><p>A typical way of deploying generative priors is to map the input image to its closest latent codes Z <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b19">20]</ref>. These methods require time-consuming iterative optimization for preserving fidelity. By contrast, we adopt such latent code mapping merely for a coarse mapping with one feed-forward pass, and leave the fidelity burden to subsequent channel-split spatial feature transform layers. We follow the practice in <ref type="bibr" target="#b76">[77]</ref> that inverts input images to intermediate latent codes W (i.e., the intermediate space transformed from Z with several multi-layer perceptron layers) for better preserving semantic property. The latent codes W are used for 'retrieving' features of a close face in the leaned face GAN distributions. After this operation, we could obtain GAN features F prior capturing generative facial priors. Discussion: Joint Restoration and Color Enhancement. Generative models capture diverse and rich priors beyond realistic details and vivid textures. For instance, they also encapsulate color priors, which could be employed in our task for joint face restoration and color enhancement. In specific, real-world face images, e.g., old photos, usually have black-and-white color, vintage yellow color, or dim color. Lively color prior in generative facial prior allows us to perform color enhancement including colorization <ref type="bibr" target="#b72">[73]</ref>. We believe the generative facial priors also incorporate conventional geometric priors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b69">70]</ref> and even 3D priors <ref type="bibr" target="#b16">[17]</ref> for restoration and manipulation, which will be left as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Channel-Split Spatial Feature Transform</head><p>Given the prior features F prior from the pretrained GAN and the extracted 'clean' features F spatial from the input image, our aim is to effectively incorporate these two features while preserving realness and fidelity.</p><p>One effective approach is spatial feature transform <ref type="bibr" target="#b64">[65]</ref>, which generates affine transformation parameters for spatial-wise feature modulation, and has shown its effectiveness on incorporating other conditions in image restoration <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b45">46]</ref> and image generation <ref type="bibr" target="#b56">[57]</ref>. In our task, a pair of affine transformation parameters (α, β) is first learned from input features F spatial by several convolutional layers. After that, the modulation is carried out by scaling and shifting the prior features F prior :</p><formula xml:id="formula_0">F output = SFT(F prior |α, β) = α F prior + β. (1)</formula><p>Despite its effectiveness in incorporating the input face information , it could not achieve a good balance of realness and fidelity, since all the prior features F prior (contributing to realness) are affected by input features F spatial (contributing to fidelity). In particular, with extreme low-quality images as inputs, the 'blurry' input features F spatial impose their influence on the modulations, and the outputs F output inevitably bias to 'blurry' results without realistic and rich facial details.</p><p>To address this problem, we propose channel-split spatial feature transform (CS-SFT) layers, which perform spatial modulation on part of the features and leave the left features to directly pass through for better information preservation, as shown in Fig. <ref type="figure" target="#fig_0">2</ref>. Mathematically, we have:</p><formula xml:id="formula_1">F output = CS-SFT(F prior |α, β) (2) = Concat[Identity(F split0 prior ), α F split1 prior + β],</formula><p>where F split0 prior and F split1 prior are split features from F prior in channel dimension, and Concat[•, •] denotes the concatenation operation.</p><p>As a result, CS-SFT enjoys the benefits of directly incorporating prior information and effective modulating by input images, thereby achieving a good balance between texture faithfulness and fidelity. CS-SFT shares similar spirits as DPN <ref type="bibr" target="#b7">[8]</ref>, whose dual path architecture enables feature re-usage and new feature exploration for each path, thus improving its representation ability.</p><p>Besides the performance, CS-SFT could also reduce complexity as it requires fewer channels for modulation, similar to GhostNet <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model Objectives</head><p>The learning objective of training our GFP-GAN consists of: 1) reconstruction loss that constraints the outputs ŷ close to the ground-truth y, 2) adversarial loss for restoring realistic textures, 3) proposed facial component loss to further enhance facial details, and 4) identity preserving loss. Reconstruction Loss. We adopt the widely-used L1 loss and perceptual loss <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b42">43]</ref> as our reconstruction loss L rec , defined as follows:</p><formula xml:id="formula_2">L rec = λ l1 ŷ − y 1 + λ per φ( ŷ) − φ(y) 1 , (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>where φ is the pretrained VGG-19 network <ref type="bibr" target="#b60">[61]</ref> and we use the {conv1, • • • , conv5} feature maps before activation <ref type="bibr" target="#b65">[66]</ref>. λ l1 and λ per denote the loss weights of L1 and perceptual loss, respectively. Adversarial Loss. We employ adversarial loss L adv to encourage the GFP-GAN to favor the solutions in the natural image manifold and generate realistic textures. Similar to StyleGAN2 <ref type="bibr" target="#b36">[37]</ref>, logistic loss <ref type="bibr" target="#b18">[19]</ref> is adopted:</p><formula xml:id="formula_4">L adv = −λ adv E ŷ softplus(D( ŷ))<label>(4)</label></formula><p>where D denotes the discriminator and λ adv represents the adversarial loss weight. Facial Component Loss. In order to further enhance the perceptually significant face components, we introduce facial component loss with local discriminators for left eye, right eye and mouth. As shown in Fig. <ref type="figure" target="#fig_0">2</ref>, we first crop interested regions with ROI align <ref type="bibr" target="#b24">[25]</ref>. For each region, we train separate and small local discriminators to distinguish whether the restore patches are real, pushing the patches close to the natural facial component distributions.</p><p>Inspired by <ref type="bibr" target="#b63">[64]</ref>, we further incorporate a feature style loss based on the learned discriminators. Different from previous feature matching loss with spatial-wise constraints <ref type="bibr" target="#b63">[64]</ref>, our feature style loss attempts to match the Gram matrix statistics <ref type="bibr" target="#b15">[16]</ref> of real and restored patches. Gram matrix calculates the feature correlations and usually effectively captures texture information <ref type="bibr" target="#b17">[18]</ref>. We extract features from multiple layers of the learned local discriminators and learn to match these Gram statistic of intermediate representations from the real and restored patches. Empirically, we found the feature style loss performs better than previous feature matching loss in terms of generating realistic facial details and reducing unpleasant artifacts.</p><p>The facial component loss is defined as follows. The first term is the discriminative loss <ref type="bibr" target="#b18">[19]</ref> and the second term is the feature style loss:</p><formula xml:id="formula_5">L comp = ROI λ local E ŷROI [log(1 − D ROI ( ŷROI ))]+ λ f s Gram(ψ( ŷROI )) − Gram(ψ(y ROI )) 1 (5)</formula><p>where ROI is region of interest from the component collection {left eye, right eye, mouth}. D ROI is the local discriminator for each region. ψ denotes the multi-resolution features from the learned discriminators. λ local and λ f s represent the loss weights of local discriminative loss and feature style loss, respectively. Identity Preserving Loss. We draw inspiration from <ref type="bibr" target="#b31">[32]</ref> and apply identity preserving loss in our model. Similar to perceptual loss <ref type="bibr" target="#b33">[34]</ref>, we define the loss based on the feature embedding of an input face. In specific, we adopt the pretrained face recognition ArcFace <ref type="bibr" target="#b10">[11]</ref> model, which captures the most prominent features for identity discrimination. The identity preserving loss enforces the restored result to have a small distance with the ground truth in the compact deep feature space:</p><formula xml:id="formula_6">L id = λ id η( ŷ) − η(y) 1 ,<label>(6)</label></formula><p>where η represents face feature extractor, i.e. ArcFace <ref type="bibr" target="#b10">[11]</ref> in our implementation. λ id denotes the weight of identity preserving loss.</p><p>The overall model objective is a combination of the above losses:</p><formula xml:id="formula_7">L total = L rec + L adv + L comp + L id .<label>(7)</label></formula><p>The loss hyper-parameters are set as follows: λ l1 = 0.1, λ per = 1, λ adv = 0.1, λ local = 1, λ f s = 200 and λ id = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Implementation</head><p>Training Datasets. We train our GFP-GAN on the FFHQ dataset <ref type="bibr" target="#b35">[36]</ref>, which consists of 70, 000 high-quality images.</p><p>We resize all the images to 512 2 during training.</p><p>Our GFP-GAN is trained on synthetic data that approximate to the real low-quality images and generalize to realworld images during inference. We follow the practice in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b45">46]</ref> and adopt the following degradation model to synthesize training data:</p><formula xml:id="formula_8">x = [(y k σ ) ↓ r +n δ ] JPEGq .<label>(8)</label></formula><p>The high quality image y is first convolved with Gaussian blur kernel k σ followed by a downsampling operation with a scale factor r. After that, additive white Gaussian noise n δ is added to the image and finally it is compressed by JPEG with quality factor q. Similar to <ref type="bibr" target="#b45">[46]</ref>, for each training pair, we randomly sample σ, r, δ and q from {0.2 : 10}, {1 : 8}, {0 : 15}, {60 : 100}, respectively. Testing Datasets. We construct one synthetic dataset and three different real datasets with distinct sources. All these datasets have no overlap with our training dataset. We provide a brief introduction here.</p><p>• CelebA-Test is the synthetic dataset with 3,000 CelebA-HQ images from its testing partition <ref type="bibr" target="#b52">[53]</ref>. The generation way is the same as that during training.</p><p>• LFW-Test. LFW <ref type="bibr" target="#b29">[30]</ref> contains low-quality images in the wild. We group all the first image for each identity in the validation partition, forming 1711 testing images.</p><p>• CelebChild-Test contains 180 child faces of celebrities collected from the Internet. They are low-quality and many of them are black-and-white old photos.</p><p>• WebPhoto-Test. We crawled 188 low-quality photos in real life from the Internet and extracted 407 faces to construct the WebPhoto testing dataset. These photos have diverse and complicated degradation. Some of them are old photos with very severe degradation on both details and color. Implementation. We adopt the pretrained StyleGAN2 <ref type="bibr" target="#b36">[37]</ref> with 512 2 outputs as our generative facial prior. The channel multiplier of StyleGAN2 is set to one for compact model size. The UNet for degradation removal consists of seven downsamples and seven upsamples, each with a residual block <ref type="bibr" target="#b25">[26]</ref>. For each CS-SFT layer, we use two convolutional layers to generate the affine parameters α and β respectively.</p><p>The training mini-batch size is set to 12. We augment the training data with horizontal flip and color jittering. We consider three components: left eye, right eye, mouth for face component loss as they are perceptually significant. Each component is cropped by ROI align <ref type="bibr" target="#b24">[25]</ref> with face landmarks provided in the origin training dataset. We train our model with Adam optimizer <ref type="bibr" target="#b38">[39]</ref> for a total of 800k iterations. The learning rate was set to 2 × 10 −3 and then decayed by a factor of 2 at the 700k-th, 750k-th iterations. We implement our models with the PyTorch framework and train them using four NVIDIA Tesla P40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-art Methods</head><p>We compare our GFP-GAN with several state-of-the-art face restoration methods: HiFaceGAN <ref type="bibr" target="#b68">[69]</ref>, DFDNet <ref type="bibr" target="#b45">[46]</ref>, PSFRGAN <ref type="bibr" target="#b5">[6]</ref>, Super-FAN <ref type="bibr" target="#b3">[4]</ref> and Wan et al. <ref type="bibr" target="#b62">[63]</ref>. GAN inversion methods for face restoration: PULSE <ref type="bibr" target="#b53">[54]</ref> and mGANprior <ref type="bibr" target="#b19">[20]</ref> are also included for comparison. We also compare our GFP-GAN with image restoration methods: RCAN <ref type="bibr" target="#b74">[75]</ref>, ESRGAN <ref type="bibr" target="#b65">[66]</ref> and DeblurGANv2 <ref type="bibr" target="#b40">[41]</ref>, and we finetune them on our face training set for fair comparisons. We adopt their official codes except for Super-FAN, for which we use a re-implementation. For the evaluation, we employ the widely-used nonreference perceptual metrics: FID <ref type="bibr" target="#b27">[28]</ref> and NIQE <ref type="bibr" target="#b54">[55]</ref>. We also adopt pixel-wise metrics (PSNR and SSIM) and the perceptual metric (LPIPS <ref type="bibr" target="#b73">[74]</ref>) for the CelebA-Test with Ground-Truth (GT). We measure the identity distance with angels in the ArcFace <ref type="bibr" target="#b10">[11]</ref> feature embedding, where smaller values indicate closer identity to the GT. Synthetic CelebA-Test. The comparisons are conducted under two settings: 1) blind face restoration whose inputs and outputs have the same resolution. 2) 4× face superresolution. Note that our method could take upsampled images as inputs for face super-resolution.</p><p>The quantitative results for each setting are shown in Table. 1 and Table <ref type="table">.</ref> 2. On both settings, GFP-GAN achieves the lowest LPIPS, indicating that our results is perceptually close to the ground-truth. GFP-GAN also obtain the lowest FID and NIQE, showing that the outputs have a close distance to the real face distribution and natural image distribution, respectively. Besides the perceptual performance, our method also retains better identity, indicated by the smallest degree in the face feature embedding. Note that the pixelwise metrics PSNR and SSIM are not correlation well with the subjective evaluation of human observers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43]</ref> and our model is not good at these two metrics.</p><p>Qualitative results are presented in Fig. <ref type="figure">3</ref> and Fig.    lashes), teeth, etc. 2) Our method treats faces as whole in restoration and could also generate realistic hair, while previous methods that rely on component dictionaries (DFD-Net) or parsing maps (PSFRGAN) fail to produce faithful hair textures (2nd row, Fig. <ref type="figure">3</ref>). 3) GFP-GAN is capable of retaining fidelity, e.g., it produces natural closed mouth without forced addition of teeth as PSFRGAN does (2nd row, Fig. <ref type="figure">3</ref>). And in Fig. <ref type="figure" target="#fig_2">4</ref>, GFP-GAN also restores reasonable eye gaze direction.</p><p>Real-World LFW, CelebChild and WedPhoto-Test. To test the generalization ability, we evaluate our model on three different real-world datasets. The quantitative results are show in Table <ref type="table">.</ref> 3. Our GFP-GAN achieves superior performance on all the three real-world datasets, showing its remarkable generalization capability. Although PULSE <ref type="bibr" target="#b53">[54]</ref> could also obtain high perceptual quality (lower FID scores), it could not retain the face identity as shown in Fig <ref type="figure" target="#fig_4">5</ref>.</p><p>The qualitative comparisons are shown in Fig. <ref type="figure" target="#fig_4">5</ref>. GFP-GAN could jointly conduct face restoration and color enhancement for real-life photos with the powerful generative prior. Our method could produce plausible and realistic faces on complicated real-world degradation while other methods fail to recover faithful facial details or produces artifacts (especially in WebPhoto-Test in <ref type="bibr">Fig 5)</ref>. Besides the common facial components like eyes and teeth, GFP-GAN also perform better in hair and ears, as the GFP prior takes the whole face into consideration rather than separate parts. With SC-SFT layers, our model is capable of achieving high fidelity. As shown in the last row of Fig. <ref type="figure" target="#fig_4">5</ref>, most previous methods fail to recover the closed eyes, while ours could successfully restore them with fewer artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>CS-SFT layers. As shown in Table <ref type="table">.</ref> 4 [configuration a)] and Fig. <ref type="figure" target="#fig_8">6</ref>, when we remove spatial modulation layers, i.e., only keep the latent code mapping without spatial information, the restored faces could not retain face identity even with identity-preserving loss (high LIPS score and large Deg.). Thus, the multi-resolution spatial features used in CS-SFT layers is vital to preserve fidelity. When we switch CS-SFT layers to simple SFT layers [configuration b) in Table <ref type="table">.</ref> 4], we observe that 1) the perceptual quality degrades on all metrics and 2) it preserves stronger identity (smaller Deg.), as the input image features impose influence on all the modulated features and the outputs bias to the degraded inputs, thus leading to lower perceptual quality. By contrast, CS-SFT layers provide a good balance of realness and fidelity by modulating a split of features. Pretrained GAN as GFP. Pretrained GAN provides rich and diverse features for restoration. A performance drop is observed if we do not use the generative facial prior, as shown in Table <ref type="table">.</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Limitations</head><p>Though our model trained on synthetic data demonstrates good generalization to real-world images, it still have     some limitations. As shown in Fig. <ref type="figure" target="#fig_11">8</ref>, when the degradation of real images is extremely severe, the restored facial details by GFP-GAN are twisted with artifacts. Our method also produces unnatural results for very large poses. This is because the synthetic degradation and training data distribution are different from those in real-world. One possible way is to learn those distributions from real data instead of merely using synthetic data, which is left as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed the GFP-GAN framework that leverages the rich and diverse generative facial prior for the challenging blind face restoration task. This prior is incorporated into the restoration process with novel channel-split spatial feature transform layers, allowing us to achieve a good balance of realness and fidelity. We also introduce delicate designs such as facial component loss, identity preserving loss and pyramid restoration guidance. Extensive comparisons demonstrate the superior capability of GFP-GAN in joint face restoration and color enhancement for real-world images, outperforming prior art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our GFP-GAN framework. It consists of a degradation removal module and a pretrained face GAN as facial prior. They are bridged by a latent code mapping and several Channel-Split Spatial Feature Transform (CS-SFT) layers. A good balance of realness and fidelity is realized by the proposed CS-SFT modulations. During training, we employ 1) Pyramid restoration guidance to remove complex degradation in the real world, 2) Facial component loss with discriminators to enhance facial details, and 3) identity preserving loss to retain face identity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Qualitative comparison on the CelebA-Test for blind face restoration. Our GFP-GAN produces faithful details in eyes, mouth and hair. Zoom in for best view.</figDesc><graphic url="image-31.png" coords="6,50.80,113.79,54.06,54.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4</head><label>4</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>. 1 )</head><label>1</label><figDesc>Thanks to the powerful generative facial prior, our GFP-GAN recovers faithful details in the eyes (pupils and eye-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Qualitative comparisons on three real-world datasets. Zoom in for best view.</figDesc><graphic url="image-167.png" coords="7,69.70,357.99,56.58,56.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>4 [configuration c)] and Fig. 6. Pyramid Restoration Loss. Pyramid restoration loss is employed in the degradation removal module and strengthens the restoration ability for complicated degradation in the real world. Without this intermediate supervision, the multi-resolution spatial features for subsequent modulations may still have degradation, resulting in inferior performance, as shown in Table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>4 [configuration d)] and Fig.6. Facial Component Loss. We compare the results of 1) removing all the facial component loss, 2) only keeping the component discriminators, 3) adding extra feature matching loss as in<ref type="bibr" target="#b63">[64]</ref>, and 4) adopting extra feature style loss based on Gram statistics<ref type="bibr" target="#b15">[16]</ref>. It is shown in Fig 7 that component discriminators with feature style loss could better capture the eye distribution and restore the plausible details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study results on CelebA-Test under blind face restoration. spatial modulation 0.550 (↑) 60.44 (↑) 4.183 (↑) 74.76 (↑) b) Use SFT 0.387 (↑) 47.65 (↑) 4.146(↑) 34.38 (↓) c) w/o GFP 0.379 (↑) 48.47 (↑) 4.153 (↑) 35.04 (↑) d) − Pyramid Restoration Loss 0.369 (↑) 45.17 (↑) 4.284 (↑) 35.50 (↑) GFP-GAN b) SFT c) No GFP d) No Pyramid a) No SC-SFT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Ablation studies on CS-SFT layers, GFP prior and pyramid restoration loss. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ablation studies on facial component loss. In the figure, D, fm, fs denotes component discriminator, feature matching loss and feature style loss, respectively. Zoom in for best view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Limitations of our model. The results of PSFR-GAN [6] are also presented.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison on CelebA-Test for blind face restoration. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance. Deg.↓ PSNR↑ SSIM↑ Input 0.4866 143.98 13.440 47.94 25.35 0.6848 DeblurGANv2* [41] 0.4001 52.69 4.917 39.64 25.91 0.6952 Wan et al. [63] 0.4826 67.58 5.356 43.00 24.71 0.6320 HiFaceGAN [69] 0.4770 66.09 4.916 42.18 24.92 0.6195 DFDNet [46] 0.4341 59.08 4.341 40.31 23.68 0.6622 PSFRGAN [6] 0.4240 47.59 5.123 39.69 24.71 0.6557 mGANprior [20] 0.4584 82.27 6.422 55.45 24.30 0.6758 PULSE [54] 0.4851 67.56 5.305 69.55 21.61 0.6200 GFP-GAN (ours) 0.3646 42.62 4.077 34.60 25.08 0.</figDesc><table><row><cell>6777</cell></row></table><note>MethodsLPIPS↓ FID↓ NIQE ↓</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison on CelebA-Test for 4× face super-resolution. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance.</figDesc><table><row><cell>Methods</cell><cell cols="5">LPIPS↓ FID↓ NIQE ↓ Deg.↓ PSNR↑ SSIM↑</cell></row><row><cell>Bicubic</cell><cell cols="5">0.4834 148.87 10.767 49.60 25.377 0.6985</cell></row><row><cell>RCAN* [75]</cell><cell cols="5">0.4159 93.66 9.907 38.45 27.24 0.7533</cell></row><row><cell cols="6">ESRGAN* [66] 0.4127 49.20 4.099 51.21 23.74 0.6319</cell></row><row><cell>Super-FAN [4]</cell><cell cols="5">0.4791 139.49 10.828 49.14 25.28 0.7033</cell></row><row><cell cols="6">GFP-GAN (ours) 0.3653 42.36 4.078 34.67 25.04 0.6744</cell></row><row><cell>GT</cell><cell>0</cell><cell>43.43 4.292</cell><cell>0</cell><cell>∞</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison on the real-world LFW, CelebChild, WebPhoto. Red and blue indicates the best and the second best performance. '*' denotes finetuning on our training set. Deg. represents the identity distance. FID↓ NIQE ↓ FID↓ NIQE ↓ Input 137.56 11.214 144.42 9.170 170.11 12.755 DeblurGANv2* [41] 57.28 4.309 110.51 4.453 100.58 4.666 Wan et al. [63] 73.19 5.034 115.70 4.849 100.40 5.705 HiFaceGAN [69] 64.50 4.510 113.00 4.855 116.12 4.885 DFDNet [46] 62.57 4.026 111.55 4.414 100.68 5.293 PSFRGAN [6] 51.89 5.096 107.40 4.804 88.45 5.582 mGANprior [20] 73.00 6.051 126.54 6.841 120.75 7.226 PULSE [54] 64.86 5.097 102.74 5.225 86.45 5.146 GFP-GAN (ours) 49.96 3.882 111.78 4.349 87.35 4.144</figDesc><table><row><cell>Dataset</cell><cell>LFW-Test</cell><cell>CelebChild</cell><cell>WebPhoto</cell></row><row><cell>Methods</cell><cell>FID↓ NIQE ↓</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space</title>
		<author>
			<persName><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on perceptual image super-resolution</title>
		<author>
			<persName><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Super-fan: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with gans</title>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Qingxing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Progressive semantic-aware style transformation for blind face restoration</title>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianhui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName><surname>Wong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.08709</idno>
		<imprint>
			<date type="published" when="2008">2020. 1, 2, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image blind denoising with generative adversarial network based noise modeling</title>
		<author>
			<persName><forename type="first">Jingwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual path networks</title>
		<author>
			<persName><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fsrnet: End-to-end learning face super-resolution with facial priors</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName><forename type="first">Jiankang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niannan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exemplar guided face image super-resolution without facial landmarks</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep generative adversarial compression artifact removal</title>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Galteri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Seidenari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction</title>
		<author>
			<persName><forename type="first">Baris</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stylianos</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irene</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of texture transfer for single image super-resolution</title>
		<author>
			<persName><forename type="first">Waleed</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image processing using multi-code gan prior</title>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2020. 2, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ladn: Local adversarial disentangling network for facial makeup and de-makeup</title>
		<author>
			<persName><forename type="first">Qiao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mang</forename><forename type="middle">Tik</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Closedloop matters: Dual regression networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Yong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeshuai</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic frequency masking to improve super-resolution and denoising networks</title>
		<author>
			<persName><forename type="first">Majed</forename><forename type="middle">El</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Andrew G Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamara</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2005">2020. 2, 3, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Progressive face super-resolution via attention to facial landmark</title>
		<author>
			<persName><forename type="first">Deokyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minseon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gihyun</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dae-Shik</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deblurgan: Blind motion deblurring using conditional adversarial networks</title>
		<author>
			<persName><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Budzan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Mykhailych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better</title>
		<author>
			<persName><forename type="first">Orest</forename><surname>Kupyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tetiana</forename><surname>Martyniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junru</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Photorealistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2017. 2, 4, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beautygan: Instance-level facial makeup transfer with deep generative adversarial network</title>
		<author>
			<persName><forename type="first">Tingting</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihe</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Blind face restoration via deep multi-scale component dictionaries</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shangchen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianhui</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 4, 5, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Enhanced blind face restoration with multi-exemplar images and adaptive spatial feature fusion</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning warped guidance for blind face restoration</title>
		<author>
			<persName><forename type="first">Xiaoming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoung</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Residual feature aggregation network for image superresolution</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pulse: Self-supervised photo upsampling via latent space exploration of generative models</title>
		<author>
			<persName><forename type="first">Sachit</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandru</forename><surname>Damian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2007">2020. 1, 2, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Making a &quot;completely blind&quot; image quality analyzer. IEEE Signal processing letters</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="209" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Exploiting deep generative prior for versatile image restoration and manipulation</title>
		<author>
			<persName><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep semantic face deblurring</title>
		<author>
			<persName><forename type="first">Ziyi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Methods and results</title>
		<author>
			<persName><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Bringing old photos back to life</title>
		<author>
			<persName><forename type="first">Ziyu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCVW</title>
				<imprint>
			<date type="published" when="2007">2018. 2, 4, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to superresolve blurry face and text images</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinshan</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanspeter</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Hifacegan: Face renovation via collaborative suppression and replenishment</title>
		<author>
			<persName><forename type="first">Lingbo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Face super-resolution guided by facial component heatmaps</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2004">2018. 1, 2, 3, 4</date>
			<biblScope unit="page" from="217" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Super-resolving very low-resolution face images with supplementary attributes</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Channel splitting network for single mr image super-resolution</title>
		<author>
			<persName><forename type="first">Xiaole</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueming</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5649" to="5662" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Indomain gan inversion for real image editing</title>
		<author>
			<persName><forename type="first">Jiapeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep cascaded bi-network for face hallucination</title>
		<author>
			<persName><forename type="first">Shizhan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
