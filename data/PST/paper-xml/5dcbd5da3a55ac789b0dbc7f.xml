<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-11-12">12 Nov 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Arash</forename><surname>Rahnama Booz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Allen</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andre</forename><forename type="middle">T</forename><surname>Nguyen Booz</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Edward</forename><surname>Raff Booz</surname></persName>
							<email>edward@bah.com</email>
						</author>
						<title level="a" type="main">Robust Design of Deep Neural Networks against Adversarial Attacks based on Lyapunov Theory</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-11-12">12 Nov 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1911.04636v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) are vulnerable to subtle adversarial perturbations applied to the input. These adversarial perturbations, though imperceptible, can easily mislead the DNN. In this work, we take a control theoretic approach to the problem of robustness in DNNs. We treat each individual layer of the DNN as a nonlinear dynamical system and use Lyapunov theory to prove stability and robustness locally. We then proceed to prove stability and robustness globally for the entire DNN. We develop empirically tight bounds on the response of the output layer, or any hidden layer, to adversarial perturbations added to the input, or the input of hidden layers. Recent works have proposed spectral norm regularization as a solution for improving robustness against 2 adversarial attacks. Our results give new insights into how spectral norm regularization can mitigate the adversarial effects. Finally, we evaluate the power of our approach on a variety of data sets and network architectures and against some of the well-known adversarial attacks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The objective of a supervised learning task for the input u ∈ R d , and its associated target value y for a given DNN denoted by H θ (u), where θ is a set of parameters to be learned during the training, is to classify the instance u correctly such that y = H θ (u). Recently, the research community has become interested in adversarial attacks, where the adversary's goal is to introduce a small amount of engineered perturbation ∆ ∈ R d to u, so that u = u + ∆, while still maintaining its similarity to u, can deceive the DNN into making a mistake, i.e., H θ (u + ∆) = y. The adversary is usually assumed to be constrained by an p -norm so that u − u p ≤ , where bounds the adversaries' freedom to alter the input. While this does not capture the full scope of potential adversaries <ref type="bibr" target="#b3">[4]</ref>, attacks of this form have proven difficult to prevent <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref>. While optimal defenses have been developed for simple linear models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, the over-parameterized nature of DNNs and the complexity of surfaces learned during training make the development of robust solutions against the adversary difficult <ref type="bibr" target="#b13">[14]</ref>.</p><p>In this work, we use Lyapunov theory of stability and robustness of nonlinear systems to develop a new understanding of how DNNs respond to changes in their inputs, and thus, to adversarial attacks under the 2 norm framework. By treating each layer l in the DNN as a nonlinear system h l , we develop a new framework which sets tight bounds on the response of the individual layer to adversarial perturbations (maximum changes in h l (u) − h l (u + ∆) 2  2 ) based on the spectral norm of the weights, ρ(W l ), of the individual layer. We characterize Lyapunov properties of an individual layer and their relationship to local and global stability and robustness of the network. Since our analysis is based on a sequence of nonlinear transformations h l=1,...,n , our method is the first to bound the response to input alterations by the attack parameter ∆ l for any layer l in the DNN. For simple forward networks (fully connected and convolutional), our results show that an attack's success is independent of the depth of the DNN and that the spectral norm of the weight matrix for the first and last layer of a network have the largest effect on robustness. Our Lyapunov analysis of Residual Blocks shows that the skip-connections of these blocks contribute to their lack of robustness against adversarial attacks <ref type="bibr" target="#b12">[13]</ref> and that these blocks require more restrictive Lyapunov conditions (a tighter spectral norm regularization) for maintaining the same level of robustness. This may compromise the DNN's accuracy on the clean data set <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b9">10]</ref>. Finally, our proposed robust training method, unlike the previous works in this arena <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>, regularizes the spectral norm of the weight matrix at each individual layer based on certain Lyapunov conditions, independently from other layers. Our layer-wise spectral regularization parameters may be selected based on our Lyapunov conditions and the level of robustness and accuracy required for a specific task. Our approach can help with the limitations associated with expressiveness of DNNs trained with the same level of spectral regularization across all layers <ref type="bibr" target="#b9">[10]</ref>. We show that this higher degree of freedom in selecting the hyper-parameters for each layer independently, leads to networks that are more accurate and robust against adversarial attacks in comparison to the existing works in the literature.</p><p>In summary, our contributions revolve around showing that adversarial ML research can leverage control theory to understand and build defenses against strong adversaries. This can accelerate the research progress in this area. We prove that with our proposed training approach, the perturbation to the final activation function (∆ n ∈ R d ) is bounded by ∆ n 2 ≤ √ c • , where c is a constant determined by the hyper-parameters chosen and models the adversarial perturbations. Our bound is tight, and applies to all possible inputs, and to attacks applied at any layer of the network (input, or hidden), with no distributional assumptions. Our analysis shows how Residual Blocks can aid adversarial attacks, and extensive empirical tests show that our approach's defensive advantages increase with the adversary's freedom .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>To provide certifiable defenses, complex optimization schemes are usually adopted to show that all the data points inside an p ball around a sample data point have the same prediction <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. The bounds provided by these methods are usually loose, and the computational costs associated with them increase exponentially with the size of the network. These approaches are only applicable to parts of the input space for which feasible solutions exist. Works such as <ref type="bibr" target="#b38">[39]</ref> have empirically shown that bounding a layer's response to the input generally improves robustness. Works such as <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> focus on certifying robustness for a DNN by calculating the bounds for the activation functions' responses to the inputs. The closest to our work are the results given in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>. <ref type="bibr" target="#b7">[8]</ref> utilizes Lipschitz properties of the DNN to improve robustness against adversarial attacks. Unlike <ref type="bibr" target="#b7">[8]</ref>, our approach does not require a predetermined set of hyper-parameters to prove robustness. Our analysis provides a range of possible values which determine different levels of robustness and may be selected per application. Similarly, <ref type="bibr" target="#b27">[28]</ref> empirically explores the benefits of bounding the response of the DNN by regularizing the spectral norm of layers based on the Lipschitz properties of the DNN. These Lipschitz based approaches may be seen as one subset of our Lyapunov based approach. As we will describe, our Lyapunov based analysis is built upon a more general input-output nonlinear mapping which does not necessarily depend on restricting the networks's Lipschitz property. <ref type="bibr" target="#b36">[37]</ref> explores the benefits of training networks with spectral regularization for improving generalizability against input perturbations. However, their work bounds the spectral norm of all layers by 1. As we will show, this approach limits the performance on the clean data set and does not produce the most robust DNN. <ref type="bibr" target="#b10">[11]</ref> uses PAC-Bayes generalization analysis to estimate the robustness of DNNs trained by spec-tral regularization against adversarial attacks. The analysis given in their work however, requires the same regularization condition enforced across all layers of the DNN. Our work provides per layer conditions for robustness, which may be utilized for the selection of the best regularization parameters for each layer independently, given a data set and architecture through cross-validation. Although, we have not empirically shown in the paper, our approach theoretically should provide a level of robustness against intermediate level attacks recently introduced in <ref type="bibr" target="#b17">[18]</ref>. Our work provides a theoretical backing for the empirical findings in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref> which state that Leaky ReLu, a modified version of the ReLu function, may be more robust comparatively. Finally, only one prior work has looked at control theory for adversarial defense, but their method covered only a toy adversary constrained to perturbing the input by a constant <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries and Motivation</head><p>In this work, we address the machine learning problem of adversarial attacks on DNNs from a control theoretic perspective. To aid in bridging the gap between these two fields, we will briefly review the core control theory results needed to understand our work, with further exposition in Appendix A for less familiar readers. The design of stable and robust systems that maintain their desired performance in the presence of external noise and disturbance has been studied in the control theory research literature. Our work is based on the Lyapunov theory of stability and robustness of nonlinear systems which dates back to more than a century ago <ref type="bibr" target="#b21">[22]</ref>. We treat each layer of the DNN as a nonlinear system and model the DNN as a cascade connection of nonlinear systems. A nonlinear system is defined as a system which produces an output signal for a given input signal through a nonlinear relationship. More specifically, consider the following general definition for the nonlinear system H (Fig. <ref type="figure" target="#fig_1">1</ref>),</p><formula xml:id="formula_0">H : ẋ = f (x, u) y = h(x, u), where x ∈ X ⊆ R n , u ∈ U ⊆ R m</formula><p>, and y ∈ Y ⊆ R k are respectively the state, input and output of the system, and X, U and Y are the local state, input and output sub-spaces around the current operating points. The nonlinear mappings f and h model the relationship among the input signal u, the internal states of the system x and the output signal y. To help connect these control theory notations and definitions to our analysis of DNN models, see Table <ref type="table" target="#tab_0">1</ref>.</p><p>The behavior of a layer inside the DNN may be modeled as a nonlinear system defined above. More specifically, for the layer l, the input signal u takes the size of the layer l − 1 and stands for the input to the layer l before it is transformed by the weights and biases. y has the size of layer l and may be defined as the output of the layer l after Our analysis is based on Lyapunov theory which gives us the freedom to define stability and robustness purely based on the input-output relationship of the layers without the exact knowledge of the internal state changes ẋ (i.e., we do not need to know the specific weight and bias val-ues). More specifically, Lyapunov theory combined with the fact that we are showing bounded-input-bounded-output (BIBO) stability and robustness of the layer, allows us to abstract out the transient behavior of the nonlinear systems during the training iterations t, i.e., ẋl (t) = f l (x l (t), u l (t)) where x l (t) = {W l (t), B l (t)}, from the robustness analysis and focus only on the steady-state behavior of the nonlinear system, i.e., the input-output mapping of the layer, y l = h l (x l , u l ) = h l ({W l , B l }, u l ) where h l (.) models the nonlinear transformation. By analyzing h l (.), we can derive robust conditions to be enforced during training for the states of a layer {W l , B l }.</p><p>Next, we define an input-output stability and robustness Lyapunov criterion for a nonlinear system. A nonlinear system is said to be BIBO stable, if it produces bounded outputs for bounded input signals. A nonlinear system is said to be stable and robust, if it produces bounded output signals that are close in the Euclidean space, when their respective input signals are also close enough in the Euclidean space <ref type="bibr" target="#b21">[22]</ref>. Mathematically, these definitions can be represented as follows, Definition 1 <ref type="bibr" target="#b21">[22]</ref> System H is instantaneously incrementally finite-gain (IIFG) 2 -stable and robust, if for any two inputs u 1 , u 2 ∈ U , there exists a positive gain γ, such that the relation, If a system is IIFG stable and robust, then the changes in the output of the entire system are bounded by the changes in the input of the entire system. As a result, if the changes in the input are minuscule, which is the assumption for the majority of 2 adversarial attacks, then the changes in their respective outputs are also minuscule. This is the exact behavior that we would like to encourage for each layer of the DNN so that the entire network is robust against adversarial attacks.</p><formula xml:id="formula_1">||y 2 − y 1 || 2 ≤ γ||u 2 − u 1 || 2 .</formula><p>Remark 1 It is important to note that the relationship given in Definition 1 is more general than enforcing Lipschitz continuity. In particular, the above relationship should only hold locally for the input signals u 1 and u 2 for the DNN to be IIFG. Further, the above assumption does not place any constraints on the initial conditions of the DNN. Additionally, Lipschitz continuity implies uniform continuity, but the relationship given above potentially allows for discontinuous distributions and does not enforce any continuously differentiable condition on the mapping from the input to the output of the DNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>. Finally, we will show that the enforcement of the relationship given in Definition 1 locally at the layer level is not necessary. We will show that by enforcing a much looser condition at each layer, we can encourage the behavior given in Definition 1 globally for the DNN.</p><note type="other">Output Input</note><p>Lyapunov theory of dissipativity provides a fundamental framework for the stability and robustness analysis of systems based on a generalized notion of the "energy" supplied and dissipated in the system <ref type="bibr" target="#b21">[22]</ref>. This generalized notion of "energy" is defined based on a relationship that solely relies on the input and output of the system. This theory states that a passive system which dissipates energy is robust and stable. The benefit of this approach is that by only enforcing a relationship between the input and output of the DNN, we can define a measure of stability and robustness for the entire DNN against adversarial attacks and characterize robust conditions for the states of the DNN (weights and biases). This is done by first defining an input-output mapping which characterizes a specific relationship between the input and output of the layers inside the DNN and then enforcing that the relationship should hold for the DNN for all the inputs supplied to the model and the outputs produced by the model during training so that the same will hold during inference.</p><p>The following definition provides the mathematical representation of the aforementioned concept, Definition 2 <ref type="bibr" target="#b37">[38]</ref> System H is considered to be instantaneously Incrementally Input Feed-Forward Output Feedback Passive (IIFOFP), if it is dissipative with respect to the inputoutput mapping ω(•, •),</p><formula xml:id="formula_2">ω(u 2 − u 1 , y 2 − y 1 ) = (u 2 − u 1 ) T (y 2 − y 1 ) − δ(y 2 − y 1 ) T (y 2 − y 1 ) − ν(u 2 − u 1 ) T (u 2 − u 1 ),</formula><p>for some value ν ∈ R and δ ∈ R where ν × δ ≤ 0.25.</p><p>Remark 2 Although ν and δ may take negative or positive values, our goal is to design layers that have positive δ's. Further, a positive set of δ and ν for a layer implies IIFG stability and robustness for that layer. Nevertheless, as we will show, for the entire network to be robust and stable, the ν and δ for an individual layer can take any value as long as certain conditions are met. ν and δ define a range of possible stability and robustness properties for a system. The lack of stability and robustness in one layer may be compensated for by an excess of robustness and stability in another layer <ref type="bibr" target="#b21">[22]</ref>. In Subsection 4.3, we provide an interpretation of the above relations and outline the implications behind the selection of ν and δ.</p><p>Theorem 1 <ref type="bibr" target="#b21">[22]</ref> If the nonlinear system H is IIFOFP with δ &gt; 0, then it is IIFG stable and robust with the finite gain γ = 1 δ .</p><p>Our goal in this paper is to connect Definition 2 to Definition 1 to achieve robustness and the property given in Theorem 1. By enforcing the looser condition given in Definition 2, where ν and δ can take a range of values locally at the layer level, we encourage a robust global behavior for the entire DNN as given in Definition 1. According to Lyapunov theory for a layer to have the instantaneously IIFOFP property in Definition 2, the following condition should hold for the input-output mapping of all the inputs u 1 , u 2 fed to the layer and their respective output signals</p><formula xml:id="formula_3">y 1 , y 2 : ω(u 2 − u 1 , y 2 − y 1 ) &gt; 0 [22]</formula><p>. If this holds, then the layer is dissipative with respect to the nonlinear relation given in Definition 2.</p><p>Our results will show how we can reach Definition 1 and robustness globally by enforcing Definition 2 locally. Lastly, we will use the following matrix properties in our proofs,</p><p>Theorem 2 <ref type="bibr" target="#b19">[20]</ref> A square matrix A is a quasi-dominant matrix (diagonally dominant), if there exists a positive diagonal matrix P = diag{p 1 , p 2 , ..., p n } such that a ii p i ≥ j =i |a ij |p j , ∀i, and/or a jj p j ≥ i =j |a ji |p i , ∀j. If these inequalities are met strictly, then the matrix is said to be strictly row-sum (or column-sum) quasi-dominant. If P can be chosen as the identity matrix, then the matrix is said to be row-or column-diagonally dominant.</p><p>Corollary 1 <ref type="bibr" target="#b32">[33]</ref> Every symmetric quasi-dominant matrix is positive definite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Theoretical Analysis and Main Results</head><p>In our analysis, we treat each layer of the DNN as a nonlinear system as defined in Section 3 i.e., H i for all layers i = 1, ..., n (Fig. <ref type="figure" target="#fig_1">1</ref>). A nonlinear system is defined as a layer in the network that accepts an input vector from the previous layer and produces an output vector with the size of the current layer after the weights, biases and activation functions are applied to the input signal. We prove the conditions under which each layer H i is instantaneously IIFOFP with specific hyper-parameters δ i and ν i . One can interpret the values of δ i and ν i as measures of robustness for the specific layer i.</p><p>Our results place specific constraints on the weight matrices at the given layer based on the values of the hyper-parameters δ i and ν i . We train the model and enforce these conditions during back-propagation. Consequently, we can show that the entire DNN is instantaneously IIFOFP and IIFG stable and robust. This means that the DNN maintains a level of robustness against adversarial changes added to the input for up to a specific adversarial 2 norm . This is because the changes in the output of the DNN are now bounded by the changes introduced to the input by the adversary. Robustness in this sense means that the adversary now needs to add a larger amount of noise to the input of the DNN in order to cause larger changes in the output of the DNN and affect the decision making process. Our approach improves robustness against adversarial changes added to the input signal and the output of a hidden layer before it is fed into the next layer.</p><p>In our approach, we consider Leaky ReLU activation functions. Our results and the Lyapunov theory suggest that Leaky ReLU is a more robust activation function than ReLu. We expand on this in Subsection 4.3. ∆ is a measure for intervention. ∆ models the extent of adversarial noise introduced to the input by the adversary. The effects of the majority of -based attacks of different norms such as the fast gradient method (FGM) and projected gradient descent (PGD) method may be modeled by ∆ <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Given a DNN, we are interested in the (local) robustness of an arbitrary natural example u by ensuring that all of its neighborhood has the same inference outcome. The neighborhood of u may be characterized by an 2 ball centered at u. We can define an adversarial input as follows, Definition 3 Consider the input u of the layer of size n, i.e., u ∈ R n and the perturbed input signal u + ∆ where ∆ ∈ R n is the attack vector that can take any value. The perturbed input vector u+∆ is within a</p><formula xml:id="formula_4">∆ 0 -bounded 2 -ball centered at u if u + ∆ ∈ B 2 (u, ∆ 0 ), where B 2 (u, ∆ 0 ) := {u + ∆| ||u + ∆ − u|| 2 = ||∆|| 2 ≤ ∆ 0 }.</formula><p>Geometrically speaking, the minimum distance of a misclassified nearby instance to u is the smallest adversarial strength needed to alter the DNN's prediction, which is also the largest possible robustness measure for u. We will use the conic behavior of the activation function, spectral norm of the weights and their relation to Lyapunov theory to train DNNs that are stable and robust against the adversary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The robustness analysis of each layer inside the deep neural network</head><p>Each layer of a DNN can be modeled as y l = h l (W l u l + b l ) for l = 1, ..., n for some n &gt; 2, where u l ∈ R n l−1 is the input of the l-th layer, and W l ∈ R n l ×n l−1 and b l ∈ R n l are respectively the layer-wise weight matrix and bias vector applied to the flow of information from the layer l − 1 to the layer l. h l : R n l−1 → R n l models the entire numerical transformation at the l-th layer including the (nonlinear) activation functions. n l−1 and n l represent the number of neurons in layers l − 1 and l. For a set of weight and bias parameters, {W l , b l } n l=1 , we can model the behavior of the entire DNN as</p><formula xml:id="formula_5">H {W l ,b l } n l=1 (u 1 ) = y n where H {W l ,b l } n l=1 : R n 1 → R n n</formula><p>and u 1 is the initial input to the DNN. Given the training data set of size K, (u i , y i ) K i=1 , where u i ∈ R n 1 and y i ∈ R n n , the loss function is defined as</p><formula xml:id="formula_6">1 K L(H {W l ,b l } n l=1 (u i ), y i ),</formula><p>where L is usually selected to be cross-entropy or the squared 2 -distance for classification and regression tasks, respectively. The model parameters to be learned is x. We consider the problem of obtaining a model that is insensitive to the perturbation of the input. The goal is to obtain parameters, {W l , b l } n l=1 , such that the 2 -norm of h(u + ∆) − h(u) is small, where u ∈ R n 1 is an arbitrary vector and ∆ ∈ R n 1 is an engineered perturbation vector with a small 2 -norm added by the adversary. To be more general and further investigate the properties of the layers, we assume that each activation function, modeled by h l is a modified version of element-wise ReLU called the Leaky ReLU: h l (y l ) = max(y l , ay l ), where 0 &lt; a &lt; 1 (our results stand for simple ReLu as well). It follows that, to bound the variations in the output of the DNN by the variations in the input, it suffices to bound these variations for each l ∈ {1, ..., n}. Here, we consider that the attack variations ∆ are added by the adversary into the initial input or the input of the hidden layers. This motivates us to consider a new form of regularization scheme, which is based on an individual layer's Lyapunov property. The first question we seek to answer is the following: what are the conditions under which a layer l is IIFOFP with a positive δ l and a ν l that may take any value? In practice, it is best to train layers so that both ν l and δ l are positive, as this means a tighter bound and a more robust layer (Subsection 4.3), however, this is not a necessary condition for our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3</head><p>The numerical transformation at the hidden layer l of the DNN as defined in Subsection 4.1 is instantaneously IIFOFP and consequently IIFG stable and robust, if the spectral norm of the weight matrix for the layer satisfies the following condition,</p><formula xml:id="formula_7">ρ(W l ) ≤ 1 δ 2 l + 2|ν l | δ l</formula><p>where ρ(W l ) is the spectral norm of the weight matrix at the layer l, and the hyper-parameters δ l &gt; 0 and ν l meet the condition δ l × ν l ≤ 0.25.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof in Appendix B</head><p>Remark 3 It is important to note that the above theorem shows a relationship between the spectral norm of the weight matrix at the layer l and instantaneously IIFG stability and robustness of the layer as defined in the Definition 1 and Theorem 1 through the hyper-parameters δ l and ν l . Namely, a larger value for ν l leads to a larger upper-bound for the spectral norm of the weight matrix at layer l. Larger values of δ l however, have a reciprocal relation to the spectral norm of the weight matrix. The relationship between parameters δ l , ν l and the spectral norm of the weight matrix may be utilized during the robust training of DNNs through the spectral regularization enforced at each layer.</p><p>We can implement the above condition for each layer during the training of the network. If the above condition is met for each layer, then we can posit that the DNN is stable and robust in Lyapunov sense. The exact measure of stability and robustness depends on the selection of δ l and ν l . The global effects of this choice are outlined in Subsection 4.2. The extension of Theorem 3 to convolutional layers follows in a similar pattern and is given in Appendix C. Appendix D includes the robustness analysis of ResNet building blocks. It is important to note that ν l and δ l are design hyper-parameters that are selected before the training starts.</p><p>The only real conditions placed on the hyper-parameters are that δ l should be positive and δ l × ν l ≤ 0.25. The exact implications of choosing the hyper-parameters and their effects on the robustness of the layer against adversarial noise are detailed in Subsection 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The robustness analysis of the entire Deep Neural Network</head><p>Theorem 4 Consider the cascade interconnection of hidden layers inside the DNN as given in Fig. <ref type="figure" target="#fig_1">1</ref> where n &gt; 2, and each layer H l for l = 1, ..., n is instantaneously IIFOFP with their respective ν l and δ l as defined in Theorem 3, i.e., for any two incremental inputs u l1 , u l2 for the layer l we have,</p><formula xml:id="formula_8">ω(u l2 − u l1 , y l2 − y l1 ) = (u l2 − u l1 ) T (y l2 − y l1 ) − δ l (y l2 − y l1 ) T (y l2 − y l1 ) − ν l (u l2 − u l1 ) T (u l2 − u l1 ),</formula><p>as the nonlinear input-output mapping of the layer where ω(u l2 − u l1 , y l2 − y l1 ) &gt; 0. Then the entire DNN is also instantaneously IIFOFP with the hyper-parameters ν, δ and input-output mapping,</p><formula xml:id="formula_9">ω(u 2 − u 1 , y 2 − y 1 ) = (u 2 − u 1 ) T (y 2 − y 1 ) − δ(y 2 − y 1 ) T (y 2 − y 1 ) − ν(u 2 − u 1 ) T (u 2 − u 1 ),</formula><p>where u 1 and u 2 are the initial inputs to the DNN and y 1 and y 2 are their respective output signals, if the matrix −A is quasi-dominant, where A is defined as,</p><formula xml:id="formula_10">A =        ν − ν 1 1 2 0 . . . − 1 2 1 2 −δ 1 − ν 2 1 2 . . . 0 . . . . . . . . . . . . . . . 0 . . . 1 2 −δ n−1 − ν n 1 2 − 1 2 0 . . . 1 2 δ − δ n       </formula><p>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof in Appendix E</head><p>Remark 4 For the DNN to be IIFOFP with δ &gt; 0 and ν &gt; 0 and consequently stable and robust, we need to set the hyper-parameters for training such that δ l &gt; 0 for l = 1, ..., n, δ n &gt; δ &gt; 0, ν 1 &gt; 0, and ν 1 &gt; ν &gt; 0. The rest of ν l 's are selected such that the matrix −A is quasi-dominant. Note that theoretically, the δ l 's or ν l 's for some hidden layers may take negative values, as long as the matrix −A stays quasi-dominant, i.e., δ l + ν l+1 &gt; 1 for l = 1, ..., n − 1 and δ n−1 + ν n &gt; 1. By selecting the hyper-parameters according to Theorem 4, one is indirectly setting the spectral regularization rule for each layer. Appendix F details an example on the selection of these hyper-parameters.</p><p>Theorem 4 points to an interesting fact that the first and last hidden layer may have the largest effect on the robustness of the DNN. To keep the matrix −A quasi-dominant, δ and ν have a direct dependence on the values of δ n and ν 1 . Next, we can characterize a relationship between the incremental changes in the input signals of a DNN, i.e., ∆ 1 , and their effects on the output of the DNN, i.e., ∆ n .</p><p>Corollary 2 Consider the cascade interconnection of hidden layers inside the DNN as given in Fig. <ref type="figure" target="#fig_1">1</ref> where n &gt; 2, if each layer H l is instantaneously IIFOFP with their respective ν i and δ i , and the DNN is trained to meet the conditions given in Theorem 4, then the entire DNN is also instantaneously IIFOFP with its respective ν and δ and the input-</p><formula xml:id="formula_11">output mapping ω(u 2 −u 1 , y 2 −y 1 ) = (u 2 −u 1 ) T (y 2 −y 1 )− δ(y 2 − y 1 ) T (y 2 − y 1 ) − ν(u 2 − u 1 ) T (u 2 − u 1 )</formula><p>where δ &gt; 0.</p><p>One can show that the variations in the final output of the entire DNN (∆ n ) are upper-bounded (limited) by the variations in the input signal (∆ 1 ) through the following relation,</p><formula xml:id="formula_12">||∆ n || 2 2 ≤ 1 δ 2 + 2ν δ ||∆ 1 || 2 2 = 1 δ 2 + 2ν δ 2</formula><p>where the design parameter δ and ν are both positive.</p><p>Proof in Appendix G [38] was the first work that connected Lyapunov notion of stability and robustness to the conicity (bounded-ness) behavior of the input-output mapping of a nonlinear system. According to <ref type="bibr" target="#b37">[38]</ref>, a nonlinear numerical transformation is stable, if it produces bounded outputs for bounded inputs. The same nonlinear transformation is also robust, if it produces outputs that are insensitive to small changes added to the input. A stable and robust nonlinear numerical transformation then exhibits a conic behavior.</p><p>According to <ref type="bibr" target="#b37">[38]</ref>, a nonlinear transformation exhibits a conic behavior, if the mapping between the changes in the input and the respective changes in the output, given the nonlinear transformation, always fits inside a conic sector on the input-output plane i.e., a conic nonlinear numerical transformation in the Hilbert space is one whose input changes ∆u and output changes ∆y are restricted to some conic sector of the ∆U × ∆Y inner product space as given in Fig. <ref type="figure">2</ref>. This conic behavior is usually defined by the center line of the cone c and the radius r:</p><p>Definition 4 <ref type="bibr" target="#b37">[38]</ref> A relation H is interior conic, if there are real constants r ≥ 0 and c for which ||∆y − c∆u|| 2 ≤ r||∆u|| 2 is satisfied. This is the exact behavior that we are encouraging for each layer of the DNN so that the outputs of each layer become insensitive to small changes in the input. In particular, we have ∆u = ∆ 1 , ∆y = ∆ n , c = a+b 2 and r = b−a 2 where a and b are the slopes of the lower and upper bounds of the cone, and c and r are the center and radius of the cone. One can show that,</p><formula xml:id="formula_13">(∆ n − c∆ 1 ) T (∆ n − c∆ 1 ) ≤ r 2 ∆ T 1 ∆ 1 → (∆ n − ( a + b 2 )∆ 1 ) T (∆ n − ( a + b 2 )∆ 1 ) ≤ ( b − a 2 ) 2 ∆ T 1 ∆ 1 → 0 ≤ ∆ T 1 ∆ n − ( 1 a + b )∆ T n ∆ n − ( ba a + b )∆ T 1 ∆ 1 .</formula><p>Hence by selecting δ = 1 a+b and ν = ba b+a , we are bounding the output changes by the changes in the input as depicted in Fig. <ref type="figure">2</ref> and by that, we make the numerical transformations occurring at each layer of the DNN insensitive to small changes in the input. Particularly, a positive δ implies b &gt; 0 with a larger δ implying a smaller positive b and a larger distance between the slope of the upper-bound of the cone and the ∆Y axis. This implies that ∆Y increases in a slower rate with increases in ∆U . A positive ν implies a &gt; 0 with a larger ν implying a larger a and a larger distance between the lower-bound of the cone and the ∆U axis <ref type="bibr" target="#b21">[22]</ref>. We are encouraging the pair (∆ 1 , ∆ n ) to be instantaneously confined to a sector of the plane as depicted. The conic interpretation described here combined with the results given in the previous sections support the findings presented in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b35">36]</ref> that the use of Leaky Relu activation in the architecture of DNNs may contribute to robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We validate our results by performing a diverse set of experiments on a variety of architectures (fully-connected, AlexNet, ResNet) and data sets (MNIST, CIFAR10, SVHN and ImageNet). Appendix L contains the details on hyperparameters and training process for the above architecture and data set combinations. The experiments are implemented in TensorFlow <ref type="bibr" target="#b0">[1]</ref> and the code will be made readily available. We test the DNNs against the fast gradient method (FGM) attack <ref type="bibr" target="#b14">[15]</ref> with Frobenius 2 norm of ∈ [0.1, 0.4], and the iterative projected gradient descent (PGD) attack <ref type="bibr" target="#b24">[25]</ref> with 100 iterations, α = 0.02 and the same range of epsilons. Further, we show in Appendix J that our approach provides improved robustness against the Carlini &amp; Wagner (C&amp;W) attack <ref type="bibr" target="#b5">[6]</ref>. Our robust Lyapunov training method regularizes the spectral norm of a layer l so that, ρ(W l ) ≤ β where</p><formula xml:id="formula_14">β = 1 δ 2 l + 2|ν l | δ l .</formula><p>Given n layers, one can pick n different combinations of (ρ l , ν l ) for a given data set and architecture as long as the conditions given in Theorem 3, Theorem 4 and Remark 4 are met. Our proofs tell us that if we wish to constrain the adversarial perturbations, we should set the values of (ρ 1 , ν 1 ) and (ρ n , ν n ) with more care. The values of (ρ i , ν i ), for 1 &lt; i &lt; n, however, may be selected more freely to allow for a greater flexibility to learn while not giving the adversary further advantage. Appendix F outlines the process for selecting the Lyapunov hyper-parameters according to our proofs. The different sets of Lyapunov design parameters used in our experiments are detailed in Appendix H. We represent a robust DNN with its global Lyapunov parameters (δ, ν). It is important to note that the greater flexibility (higher expressiveness <ref type="bibr" target="#b9">[10]</ref>) allowed for the intermediate layers leads to a better generalization on the clean and adversarial data sets. This is not true for DNNs trained by weight-decay or spectral norm regularization against a single threshold β. All the previous works on this subject <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b7">8]</ref>, keep β constant across layers. These harder constraints over-regularize and thus impair the DNNs ability against attacks. Our results outlined in Appendix K show that our Lyapunov DNNs are more robust and perform better in comparison to the aforementioned works.</p><p>Table <ref type="table" target="#tab_1">2</ref> details the effectiveness of our approach in bounding the incremental changes in the output of the DNN caused by the attack. The results show that the proposed bounds offered in Corollary 1 are met. Our bounds are never violated, however as gets larger, the output changes get closer to our proposed bounds. Given the empirically tightness of the bounds, one may be able to a-priori determine the worst case vulnerabilities against an attack for a network. We leave this to our future work in this area. When we consider the accuracy under attack, our Lyapunov approach dominates prior baselines. <ref type="bibr" target="#b10">[11]</ref> for instance, obtained an accuracy of 62% at = 0.1 with adversarial training on CIFAR10 under the 2 PGD adversary with a lower number of iterations. Our approach obtains an accuracy of ≥ 73% at = 0.1 and still dominates with an accuracy of ≥ 63% at = 0.4 (Table <ref type="table" target="#tab_0">11</ref> in Appendix I). Fig. <ref type="figure">3</ref> reports the CIFAR10 test accuracy under the iterative PGD and FGM attacks for different values of . As noted in Section 4, DNNs trained with larger global (δ, ν) maintain their robustness in a more consistent way for larger 's. This is because enforcing a larger global (δ, ν) leads to DNNs with a more restricted conic behavior able to more effectively bound the negative effects of the adversarial noise. 2 weight decay training seems ineffective against the attacks and cannot utilize adversarial training to improve its performance (Fig. <ref type="figure">4</ref> in Appendix I). Lastly, to show that our approach scales to larger architectures and data sets, Table <ref type="table" target="#tab_2">3</ref> represents our results for Lyapunov-based robust ResNet50 architectures trained on the ImageNet data set. A comprehensive set of results for a larger set of experiments is given in Appendix I. This appendix also includes our mathematical proofs on how Lyapunov based spectral regularization of the weights can improve the robustness of residual blocks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we analyzed the robustness of forward, convolutional, and residual layers against adversarial attacks </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Brief Primer on Control Theory</head><p>Stability and Robustness of nonlinear systems have been studied in the field of control theory for more than a century. A nonlinear system is said to be stable, if given an input signal to the system, it can produce outputs that are expected by the user. A nonlinear system is said to be stable and robust, if given an input signal to the system and in the presence of external noise and disturbance, it can produce outputs that are expected by the user. In our work, we focus on bounded-input-bounded-output (BIBO) stability of nonlinear systems. Definition 1 in Preliminaries provides the mathematical definition for incremental BIBO stability. The Definition shows that the difference between two incremental outputs of the system should be bounded by the difference between their respective incremental inputs times γ. γ is a positive constant (gain) indicating the extent of boundedness property of the system. A small γ points to a more stable and robust system behavior. Our goal is to design a stable system that produces bounded outputs for the bounded signals it receives. In other words, we want to design a stable system that produces outputs close to the desired behavior defined by the user for the inputs from the domain of possible inputs. For instance, a DNN trained on the Imagenet data set is stable, if it produces desired classification decisions for any input image similar to the images in Imagenet data set. In our work, we focus on both stability and robustness. A robust stable DNN should be able to produce outputs close to the desired behavior defined by the user for the inputs from the domain possible inputs in cases where some amount noise has been added to the inputs. In our case, this noise is added by the adversary to the inputs.</p><p>A nonlinear system, given an input, state and output, is mathematically defined as follows,</p><formula xml:id="formula_15">H : ẋ = f (x, u) y = h(x, u), where x ∈ X ⊆ R n , u ∈ U ⊆ R m</formula><p>, and y ∈ Y ⊆ R k are respectively the state, input and output of the system, and X, U and Y are the local state, input and output sub-spaces around the current operating points. The nonlinear mappings f and h model the relationship among the input signal u, the internal states of the system x and the output signal y. For a DNN trained on the ImageNet data set, U represents the images in the train and test data sets, Y represents the domain of possible class decisions, and X represents the domain of possible weight and bias values that could be assigned to the DNN's weights and biases. Further, x, u and y represents the possible realizations of these domains. As an example, x can represent the values assigned to the weights and biases of the DNN after training and convergence. ẋ = f (x, u) represents the dynamic behavior of the DNN during training, where u represents the training inputs to the DNN, and f models the updates to the states of the DNN during the t training iterations. This is also called the transient behavior of the system and ẋ is a derivative taken over training iterations modeling the fact that the weights and biases are changing during training. In control theory, the transient behavior of a system is defined as the system's behavior before it reaches the stable equilibrium (steady-state). Lyapunov theory provides the foundation for defining stability and robustness for nonlinear systems solely based on their input-ouput behavior. This means that we can use Lyapunov results to define stability and robustness criteria for the system using the desired steady-state behavior y = h(x, u l ) = h l ({W, B}, u). This in return will determine the properties that the states (weights and biases) of the DNN should exhibit after the training is over.</p><p>Lyapunov theory treats the input-output relationship as an energy based concept and proves that if the input-output behavior of a nonlinear systems meets the relationship given in Definition 2 then the system is stable and robust. Lyapunov theory states that a stable and robust nonlinear system dissipate energy and as a results they have to be dissipative with respective to the relationship given in Definition 2.</p><p>In our work, we treat each layer inside a DNN as a nonlinear system, and use the relationship given in Definition 2, to characterize the conditions which the weights and biases of the layer should meet for that layer to be stable and robust. We make sure that these conditions are met during training so that once the training is over, the layer is stable and robust. Further, we define conditions under which the entire cascade of the systems (the entire DNN) is stable and robust and we make sure that these condition are met after the training as well. As a result, we can provide a set of design options for spectral regularization of the weights for each layer inside the DNN, and bound the response of each layer and eventually the response of the entire network against adversarial attacks and the adversarial noise added to the inputs. Given our results, we can train DNNs that outperform other state-of-the-art robust solutions in the current literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Theorem 3</head><p>Given Definition 2, we can show that for the two incremental inputs to the layer l, i.e., u l,1 = u l and u l,2 = u l + ∆ l we have,</p><formula xml:id="formula_16">ω(u l + ∆ l − u l , h(W l [u l + ∆ l ] + b l ) − h(W l u l + b l )) = ∆ T l Λ l W l ∆ l − ∆ T l (ν l I l )∆ l − ∆ T l W T l Λ T l (δ l I l )Λ l W l ∆ l<label>(1)</label></formula><p>where, </p><formula xml:id="formula_17">Λ l =            <label>1</label></formula><formula xml:id="formula_18">          </formula><p>. and I l is a diagonal identity matrix of size layer l. Further for a layer to be instantaneously IIFOFP with some ν l and a positive δ l , one needs to show the following,</p><formula xml:id="formula_19">0 ≤ [u l + ∆ l − u l ] T [h l (u l + ∆ l ) − h l (u l )] − δ l [h l (u l + ∆ l ) − h l (u l )] T [h l (u l + ∆ l ) − h l (u l )] − ν l [u l + ∆ l − u l ] T [u l + ∆ l − u l ]</formula><p>Given (1) and for δ l &gt; 0, the above can be represented as,</p><formula xml:id="formula_20">0 ≤ ( 1 2δ l + |ν l |)||∆ l || 2 2 − δ l 2 ||Λ l W l ∆ l || 2 2<label>(2)</label></formula><p>Further, the following properties hold,</p><formula xml:id="formula_21">||Λ l W l ∆ l || 2 2 &lt; ||Λ l || 2 2 ||W l || 2 2 ||∆ l || 2 2</formula><p>, and since 0 &lt; a &lt; 1, we have</p><formula xml:id="formula_22">||Λ l || 2 2 &lt; λ max (Λ T l Λ l ) ≤ 1</formula><p>where λ max (.) stands for the largest eigenvalue (singular value) of a matrix. Also we can show that ρ(W l ) &lt; ||W l || 2 2 &lt; ρ(W l ) + σ where ρ(W l ) is the spectral radius of W l and σ is a small positive number. Simplifying (2) further we have,</p><formula xml:id="formula_23">0 ≤ ( 1 2δ l + |ν l |)||∆ l || 2 2 − δ l 2 ||Λ l W l ∆ l || 2 2 ≤ ( 1 2δ l + |ν l |)||∆ l || 2 2 − δ l 2 ||Λ l || 2 2 ||W l || 2 2 ||∆ l || 2 2 ≤ ( 1 2δ l + |ν l |)||∆ l || 2 2 − δ l 2 [(ρ(W l ) + σ)||∆ l || 2 2 ] ≈ [ 1 2δ l + |ν l | − δ l 2 ρ(W l )]||∆ l || 2 2<label>(3)</label></formula><p>For the relation (3) to be positive, the term inside the bracket needs to be positive. This gives us a measure for spectral regularization of the weights between each two hidden layers of a DNN. For layer l we have,</p><formula xml:id="formula_24">ρ(W l ) ≤ 1 δ 2 l + 2|ν l | δ l ,</formula><p>which proves the theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robustness analysis of a convolutional layer</head><p>The transformations before the activation functions at the convolutional layers are linear, and the same isomorphism as for the linear layers can be exploited for the convolutional layers to have the same final relationships as given in Theorem 3. More specifically, we can follow the steps given in <ref type="bibr" target="#b15">[16]</ref> to define the transformation occurring at a convolutional layer l for output feature i with any padding and stride design as,</p><formula xml:id="formula_25">φ conv l,i (u l,i ) = M l−1 j=1 f j,i * u l,j,i + b l,i</formula><p>Each f j,i is a filter applied to the input feature and each u l,j,i is an input feature map from the previous layer. b l,i is an appropriately shaped biased tensor adding the same value to every element resulting from the convolutions. M l−1 is the number of feature maps in the previous layer. One can represent the above relation as a matrix-vector multiplication by defining the serialized version of the input, U l,i = [u l,1,i , ..., u l,M l−1 ,i ] and then representing the filter coefficients in the form of a doubly block circulant matrix <ref type="bibr" target="#b29">[30]</ref>. Specifically, if F j,i is a matrix that encompasses convolution of f j,i with the j-th feature map in a vector form, then to represent convolutions associated with different input feature maps and the same output feature map i.e., f j,i 's over M l−1 input features, one can horizontally concatenate the filter matrices to define</p><formula xml:id="formula_26">F i = [F 1,i , F 2,i , ..., F M l−1 ,i ].</formula><p>Then the complete transformation performed by a convolutional layer to generate M l output feature maps can be represented as,</p><formula xml:id="formula_27">h l (U l ) = h l (W U l + B l )</formula><p>where,</p><formula xml:id="formula_28">W =    F 1,1 . . . F M l−1 ,1 . . . . . . . . . F 1,M l . . . F M l−1 ,M l   </formula><p>and where vector B l is the larger version of b l i 's for all input feature maps and U l = [U l,1 , ..., U l,M l−1 ]. Consequently, the spectral norm of W should meet the conditions given in Theorem 3 for the layer l to be IIFOFP and IIFG stable with bounded incremental outputs. We use the power iteration method to estimate the spectral norm of the weight matrix at a specific layer during training as proposed in <ref type="bibr" target="#b10">[11]</ref>. Further, the pooling layers inside a DNN do not affect the conic behavior of the sub-systems given the properties of conic systems as described in <ref type="bibr" target="#b37">[38]</ref>. More specifically, depending on how the pooling layer is designed, an adjustment is made to the 2 norm of the incremental output changes for the sub-system containing the pooling layer. Max (average) pooling decreases the norm of the changes and as a a result the conditions given in ( <ref type="formula" target="#formula_20">2</ref>) is still met after the pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Robustness analysis of a ResNet building block</head><p>The following represents the input-output mapping of a building block l for incremental inputs u l2 , u l1 and outputs y l2 , y l1 in a ResNet layer <ref type="bibr" target="#b16">[17]</ref>, where y li = u li + F(u li , {W l }),</p><formula xml:id="formula_29">M l = (u l2 − u l1 ) T (y l2 − y l1 ) − δ l (y l2 − y l1 ) T (y l2 − y l1 ) − ν l (u l2 − u l1 ) T (u l2 − u l1 ) = (u l2 − u l1 ) T (u l2 + F 2 − u l1 − F 1 ) − δ l (u l2 + F 2 − u l1 − F 1 ) T (u l2 + F 2 − u l1 − F 1 ) − ν l (u l2 − u l1 ) T (u l2 − u l1 ) = (u l2 − u l1 ) T (F 2 − F 1 ) − δ l (u l2 + F 2 − u l1 − F 1 ) T (u l2 + F 2 − u l1 − F 1 ) + (1 − ν l )(u l2 − u l1 ) T (u l2 − u l1 ) = (1 − 2δ l )(u l2 − u l1 ) T (F 2 − F 1 ) − δ l (F 2 − F 1 ) T (F 2 − F 1 ) − (ν l + δ l − 1)(u l2 − u l1 ) T (u l2 − u l1 ) = (u l2 − u l1 ) T (F 2 − F 1 ) − δ l 1 − 2δ l (F 2 − F 1 ) T (F 2 − F 1 ) − ν l + δ l − 1 1 − 2δ l (u l2 − u l1 ) T (u l2 − u l1 )</formula><p>where we have replaced F i (u li , {W l }) with F i for simplicity. The above relation tells us that the feed-forward connection from the input to the output degrades the robustness of the ResNet block. This is because for a ResNet block l to be stable and robust, the following should hold for the Lyapunov parameters of the block excluding the feed-forward connection: 0 &lt; δ l &lt; 1 2 and ν l + δ l &gt; 1 where δ l × ν l &lt; 0.25. So that the entire block can have the robustness properties: δ l = ν l +δ l −1 1−2δ l and ν l = ν l +δ l −1 1−2δ l . This means that to maintain robustness and stability for a ResNet block, one needs to enforce stricter conditions on the Lyapunov design hyper-parameters of the sub-layers inside the block and consequently the spectral norm of the weights during the training of the DNN. In a sense, this means that a ResNet block is less robust against adversarial attacks in comparison to a simple feedforward or convolutional layer. This is expected because the output of a ResNet block consists of the block's output and the input signal fed into the block. This means that if the input signal is perturbed by adversarial noise, then under this architecture, the adversarial noise can easily propagate to the output of the block and throughout the DNN. The negative effects of feed-forward connections on robustness of nonlinear systems have been explored in control theory <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Proof of Theorem 4</head><p>The input-output mapping of each layer can be represented as M l = (u l2 − u l1 ) T (y l2 − y l1 ) − δ l (y l2 − y l1 ) T (y l2 − y l1 ) − ν l (u l2 − u l1 ) T (u l2 − u l1 ) &gt; 0 for layers l = 1, .., N , one needs to show that,</p><formula xml:id="formula_30">0 ≤ n l=1 M l ≤ (u 2 − u 1 ) T (y 2 − y 1 ) − δ(y 2 − y 1 ) T (y 2 − y 1 ) − ν(u i2 − u 1 ) T (u 2 − u 1 ).</formula><p>The summation n l=1 M l is positive if the sub-layers are trained according to Theorem 3 and as a result one can show that the above relation is equivalent to,</p><formula xml:id="formula_31">n l=1 M l − (u 2 − u 1 ) T (y 2 − y 1 ) + δ(y 2 − y 1 ) T (y 2 − y 1 ) + ν(u 2 − u 1 ) T (u 2 − u 1 ) ≤ 0.<label>(4)</label></formula><p>We can define the following matrices,</p><formula xml:id="formula_32">A 1 =         −ν 1 0 . . . 0 − 1 2 1 2 −ν 2 . . . 0 . . . . . . . . . . . . . . . 0 . . . 1 2 −ν n 0 0 . . . 0 1 2 δ        </formula><p>, and,</p><formula xml:id="formula_33">A 2 =          ν 0 . . . 0 − 1 2 1 2 −δ 1 . . . 0 0 1 2 −δ 2 . . . . . . . . . . . . . . . . . . 0 0 . . . 0 1 2 −δ n          .</formula><p>As a result, (4) may be represented as,</p><formula xml:id="formula_34">[u T y T ](A T 1 + A 2 )[u T y T ] T = [u T y T ]A[u T y T ] T ,</formula><p>where,</p><formula xml:id="formula_35">A = A T 1 + A 2 =        ν − ν 1 1 2 0 . . . − 1 2 1 2 −δ 1 − ν 2 1 2 . . . 0 . . . . . . . . . . . . . . . 0 . . . 1 2 −δ n−1 − ν n 1 2 − 1 2 0 . . . 1 2 δ − δ n        .</formula><p>According to Corollary 1, if −A is quasi-dominant, then−A is positive definite, which means that A is negative definite and we have [u T y T ]A[u T y T ] T ≤ 0 and the relation given in (4) is met. As a result the DNN is instantaneously IIFOFP with indices δ and ν. For this to hold, the only condition is for the hyper-parameters to be selected such that the matrix −A is quasi-dominant. For the case that we are interested in, we need the hyper-parameters to be selected such that δ &gt; 0, ν &gt; 0 where δ n &gt; δ &gt; 0 and ν 1 &gt; ν &gt; 0, δ i &gt; 0 for i = 1, ..., n and ν i for i = 1, ..., n are selected such that the matrix −A is quasi-dominant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. An example on how the Lyapunov parameters are selected</head><p>Here, we provide an example of parameter selection for the fully connected DNN used in some of our experiments with the global Lyapunov property of δ = 1.0, ν = 0.24. For a 3 layer fully connected forward-net, the following Lyapunov hyperparameters should be selected: δ 1 , ν 1 (ρ(W 1 )), δ 2 , ν 2 (ρ(W 2 )), δ 3 , ν 3 (ρ(W 3 )), which then determine the global Lyapunov properties: δ, ν. The following conditions should be met for the matrix −A to be quasi-dominant:</p><formula xml:id="formula_36">ν &lt; ν 1 , δ &lt; δ 3 , δ 1 + ν 2 &gt; 1, δ 2 + ν 3 &gt; 1. The matrix −A is: −A =     ν 1 − ν − 1 2 0 1 2 − 1 2 δ 1 + ν 2 − 1 2 0 0 − 1 2 δ 2 + ν 3 − 1 2 1 2 0 − 1 2 δ 3 − δ     .</formula><p>For δ = 1.0, we have a range of choices for δ 3 (δ &lt; δ 3 ), we select δ 3 = 1.08 which is a robust choice due to its relatively large value (Subsection 4.3).This gives us the condition ν 3 &lt; 0.25/1.08 = 0.231, where ν 3 = 0.23 is selected to meet the condition. As a result, the allowed range for spectral regularization for the last layer is ρ(W 3 ) &lt; 1 δ 2 3 + 2|ν3| δ3 = 1.283. In a similar manner for ν = 0.24, we have a range of choices for ν 1 (ν &lt; ν 1 ), we select ν 1 = 0.27. This gives us the condition δ 1 &lt; 0.25/0.27 = 0.925, where δ 1 = 0.92 is selected to meet the condition. As a result, the allowed range for spectral regularization of the first layer becomes ρ(W 1 ) &lt; </p><formula xml:id="formula_37">−A =     0.03 − 1 2 0 1 2 − 1 2 1.24 − 1 2 0 0 − 1 2 1.11 − 1 2 1 2 0 − 1 2 0.08     ,</formula><p>which is a quasi-dominant (diagonally dominant) matrix and positive definite with the spectral norm of 1.791 and Frobenius norm of 2.185.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Proof of Corollary 2</head><p>Given Theorem 4, the definitions for the following vectors, ∆ n = y 2 − y 1 representing the changes in the output of the DNN, ∆ 1 = u 1 + ∆ 1 − u 1 representing the changes in the input of the DNN injected by the attacker and according to Theorem 3, we have,</p><formula xml:id="formula_38">0 ≤ ∆ T 1 ∆ n − ∆ T n δI∆ n − ∆ T 1 νI∆ 1</formula><p>Given that δ &gt; 0 and ν &gt; 0, the above can be represented as,</p><formula xml:id="formula_39">0 ≤ ( 1 2δ + ν)||∆ 1 || 2 2 − δ 2 ||∆ n || 2<label>2</label></formula><p>Finally, if we move the appropriate terms to the left side of the above inequalities we have,</p><formula xml:id="formula_40">||∆ n || 2 2 ≤ ( 1 δ 2 + 2ν δ )||∆ 1 || 2 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Experiment design and hyper-parameter details</head><p>This appendix includes all hyper-parameters used in the experiments. Please note that, each layer has their own δ and ν associated with them. The pair (δ, ν) then determine the level of spectral regularization, given in parenthesis, enforced at the layer during the training of network. The δ's and ν's are selected according to the conditions presented in Definition 2, Theorem 1, Theorem 3, Theorem 4 and Corollary 1. The selections of the pairs (δ, ν) for the layers then lead to a global Lyapunov pair (δ, ν) for the entire network which then later is used to present a specific network in the tables.</p><p>H.1. The Lyapunov design parameters for Forward-Net (trained on the MNIST dataset)      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Further details on the experiments</head><p>We validate our results by training several 3-layer fully connected forward-nets on the MNIST data set using the Adam optimizer with a learning rate of 0.001. Further, we train several AlexNet architectures on the CIFAR10 data set using Stochastic Gradient Descent optimization with a learning rate of 0.01 and momentum of 0.9. Additionally, we train several ResNet architectures on the SVHN data set and ResNet50 architectures on the Imagenet data set using Stochastic Gradient Descent optimization with a learning rate of 0.01 and momentum of 0.9. All networks are trained for 200 epochs. The pixel values for the input images are normalized to take values in [−0.5, +0.5]. When considering the baseline defense of weight decay (i.e., 2 regularized weights) networks, we performed cross-validation to select λ ∈ [0.01, 0.05, 0.1]. The parameters for our robust Lyapunov approach for each network architecture used in the experiments are specified in Appendix H. Baseline represents a DNN with no regularization enforced during training.</p><p>All experiments were performed on a single Nvidia Tesla V100-SXM2-16GB GPU. The MNIST data set was downloaded from http://yann.lecun.com/exdb/mnist/. The data is randomly divided into the training, testing and validation data sets of size: 60000, 10000 and 5000 receptively. The CIFAR10 data set was downloaded from https://www.cs. toronto.edu/ ˜kriz/cifar.html. The data is randomly divided into the training, testing and validation data sets of size: 45000, 10000 and 5000 receptively. The SVHN data set was downloaded from http://ufldl.stanford.edu/ housenumbers/. The data is randomly divided into the training, testing and validation data sets of size: 73257, 26032 and 500 receptively. The ImageNet data set was downloaded from http://image-net.org/download. No sample was excluded.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>holds. Here, ||y 2 − y 1 || 2 and ||u 2 − u 1 || 2 represent the Frobenius 2 -norm of the input signals u 1 and u 2 and their respective output signals y 1 and y 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A nonlinear system (top). The DNN modeled as a cascade of nonlinear systems (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 3 .Figure 2 :</head><label>32</label><figDesc>Figure 2: A depiction of the interior conic behavior of a nonlinear system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2 2 Figure 3 :</head><label>223</label><figDesc>Figure 3: Accuracy of the DNN under PGD attack (k = 100 iterations) (top) and FGM attack (bottom) using AlexNet on CIFAR10. Plots share the same legend and axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>with Freb.2 reg., λ = 0.05) with Freb.2 reg., λ = 0.1) H.2. The Lyapunov design parameters for AlexNet (trained on the CIFAR-10 dataset)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>δ</head><label></label><figDesc>= 0.92, ν = 0.27 (1.76) δ = 0.90, ν = 0.27 δ = 0.92, ν = 0.27 (1.76) δ = 0.78, ν = 0.32 (2.46) δ = 0.78, ν = 0.32 (2.46) δ = 0.78, ν = 0.32 (2.46) δ = 1.08, ν = 0.23 (1.01) δ = 1.07, ν = 0.22 with Freb. 2 reg., λ = 0.01) spectral reg. &lt;1 for each layer) H.3. The Lyapunov design parameters for the ResNet architecture (trained on the SVHN dataset) H.4. The Lyapunov design parameters for the ResNet50 architecture (trained on the ImageNet dataset)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>spectral reg. &lt;2.0 across all layers)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>A control theory to machine learning mapping.</figDesc><table><row><cell></cell><cell>Control Theory</cell><cell>Context for our work</cell></row><row><cell></cell><cell>Meaning</cell><cell></cell></row><row><cell cols="2">u Input of the nonlinear</cell><cell>Inputs to the DNN (e.g.,</cell></row><row><cell></cell><cell>system</cell><cell>image) or the inputs to a</cell></row><row><cell></cell><cell></cell><cell>hidden layer from the</cell></row><row><cell></cell><cell></cell><cell>previous layer</cell></row><row><cell>y</cell><cell>Output of the</cell><cell>Output of the DNN or</cell></row><row><cell></cell><cell>nonlinear system</cell><cell>the output of any hidden</cell></row><row><cell></cell><cell></cell><cell>layer</cell></row><row><cell cols="2">x States of the nonlinear</cell><cell>Weights and biases of a</cell></row><row><cell></cell><cell>system</cell><cell>layer</cell></row><row><cell cols="2">ẋ Transient changes of</cell><cell>Changes of the weights</cell></row><row><cell></cell><cell>the states over discrete</cell><cell>and biases during t</cell></row><row><cell></cell><cell>or continuous steps</cell><cell>training steps</cell></row><row><cell cols="2">f (.) Nonlinear function</cell><cell>Models the updates</cell></row><row><cell></cell><cell>modeling the transient</cell><cell>applied to the weights'</cell></row><row><cell></cell><cell>changes of the states</cell><cell>and biases' of a layer</cell></row><row><cell></cell><cell>of the system based on</cell><cell>over the t training steps</cell></row><row><cell></cell><cell>the previous state and</cell><cell></cell></row><row><cell></cell><cell>current input</cell><cell></cell></row><row><cell cols="2">h(.) Nonlinear function</cell><cell>Models the input-output</cell></row><row><cell></cell><cell>modeling the</cell><cell>relationship of a hidden</cell></row><row><cell></cell><cell>steady-state behavior</cell><cell>layer given the current</cell></row><row><cell></cell><cell>of the system given the</cell><cell>values of weights, biases</cell></row><row><cell></cell><cell>current state and input</cell><cell>and input</cell></row><row><cell cols="2">ρ, ν Lyapunov parameters</cell><cell>Values determining the</cell></row><row><cell></cell><cell>modeling the</cell><cell>extent of spectral</cell></row><row><cell></cell><cell>input-output behavior</cell><cell>regularization enforced</cell></row><row><cell></cell><cell>(robustness and</cell><cell>at a layer given the</cell></row><row><cell></cell><cell>stability properties) of</cell><cell>desired level of</cell></row><row><cell></cell><cell>the system (explained</cell><cell>robustness and accuracy</cell></row><row><cell></cell><cell>in Subsection 4.3)</cell><cell>(Theorem 3, Corollary 2)</cell></row><row><cell cols="3">the activation functions. The weights and biases of the DNN</cell></row><row><cell cols="3">are the states of the nonlinear systems. In this vein, h and f</cell></row><row><cell cols="3">are general functions which model the relationship between</cell></row><row><cell cols="3">the states x, and the nonlinear transformation applied to the</cell></row><row><cell cols="3">input of the layer u to produce the output y of the layer after</cell></row><row><cell cols="3">activation functions. The states are dynamically updated</cell></row><row><cell cols="3">through gradient descent, given the inputs from the training</cell></row><row><cell cols="3">data set during the training iterations t. ẋl is the derivative</cell></row><row><cell cols="3">taken over training iterations indicating that the weights</cell></row><row><cell cols="3">and biases are changing during training, and f models this</cell></row><row><cell cols="3">nonlinear behavior during the training iterations. Please see</cell></row><row><cell cols="2">Appendix A for further details.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The incremental output variations (∆ n ) for the 3 layer forward-net given an attack strength and the bounds calculated according to Corollary 2 (MNIST, PGD attack)</figDesc><table><row><cell></cell><cell cols="3">Mean output change ∆ n 2 ≤ Lyapunov Bound</cell></row><row><cell>Lyapunov Parameters</cell><cell>= 0.1</cell><cell>= 0.2</cell><cell>= 0.3</cell></row><row><cell>δ = 0.89, ν = 0.28</cell><cell cols="3">0.244 ≤ 0.435 0.490 ≤ 0.615 0.736 ≤ 0.753</cell></row><row><cell>δ = 0.95, ν = 0.26</cell><cell cols="3">0.202 ≤ 0.407 0.406 ≤ 0.576 0.610 ≤ 0.706</cell></row><row><cell>δ = 1.1, ν = 0.22</cell><cell cols="3">0.170 ≤ 0.352 0.340 ≤ 0.497 0.511 ≤ 0.609</cell></row><row><cell>Base Model</cell><cell>1.233</cell><cell>2.446</cell><cell>3.641</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experiment results for ResNet50 trained on the ImageNet dataset under the FGM attack</figDesc><table><row><cell></cell><cell cols="4">Accuracy on the Adversarial Test Dataset</cell></row><row><cell>Network Type</cell><cell cols="2">(Attack Strength )</cell><cell></cell><cell></cell></row><row><cell></cell><cell>= 0.1</cell><cell>= 0.2</cell><cell>= 0.3</cell><cell>= 0.4</cell></row><row><cell>δ = 0.95, ν = 0.26</cell><cell>0.61</cell><cell>0.51</cell><cell>0.44</cell><cell>0.38</cell></row><row><cell>δ = 0.86, ν = 0.29</cell><cell>0.60</cell><cell>0.51</cell><cell>0.43</cell><cell>0.37</cell></row><row><cell>δ = 0.74, ν = 0.33</cell><cell>0.60</cell><cell>0.50</cell><cell>0.42</cell><cell>0.36</cell></row><row><cell>Base Training</cell><cell>0.57</cell><cell>0.45</cell><cell>0.40</cell><cell>0.34</cell></row><row><cell>Base Training with Freb. 2 reg., λ = 0.01</cell><cell>0.58</cell><cell>0.46</cell><cell>0.39</cell><cell>0.32</cell></row><row><cell cols="5">based on Lyapunov theory of stability and robustness. We</cell></row><row><cell cols="5">proposed a new robust way of training which improves ro-</cell></row><row><cell cols="5">bustness and allows for independent selection of the regu-</cell></row><row><cell cols="5">larization parameters per layer. Our work bounds the layers'</cell></row><row><cell cols="5">response to the adversary and gives more insights into how</cell></row><row><cell cols="5">different architectures, activation functions, and network de-</cell></row><row><cell cols="2">signs behave against attacks.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>0 . . . . . . . . .</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>. . .</cell><cell>. . .</cell><cell></cell><cell>. . .</cell></row><row><cell>. . .</cell><cell cols="2">. . . 1</cell><cell>0</cell><cell>. . .</cell></row><row><cell>. . .</cell><cell></cell><cell>0</cell><cell>a</cell><cell>. . .</cell></row><row><cell>. . .</cell><cell></cell><cell></cell><cell>. . .</cell><cell>. . . 0</cell></row><row><cell></cell><cell cols="2">. . . . . .</cell><cell></cell><cell>0 a</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Lastly, the values of ν 2 and δ 2 should be selected such that ν 2 &gt; 0.08 and δ 2 &gt; 0.769, we select δ 2 = 0.78 which gives us ν 2 = 0.32 and the spectral regularization range for the second layer becomes, ρ(W 2 ) &lt; 1 For our experiment, we use cross-validation to select the following spectral regularization set [1.76, 2.46, 1.01]. The matrix −A becomes,</figDesc><table><row><cell>1 δ 2 1 δ1 = 1.768. δ 2 + 2|ν1| 2 + 2|ν2| δ2 = 2.464.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The hyper-parameters for the fully connected Forward-Net (3 layers of sizes[50,<ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b9">10]</ref>) used in the experiments</figDesc><table><row><cell cols="2">Forward-Net Layer 1 (linear)</cell><cell>Layer 2 (linear)</cell><cell>Layer 3 (linear)</cell><cell>Global Lyapunov Property</cell></row><row><cell></cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν</cell></row><row><cell></cell><cell>δ = 0.86, ν = 0.29</cell><cell></cell><cell></cell></row><row><cell>Design</cell><cell>(1.83)</cell><cell></cell><cell></cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The hyper-parameters for the AlexNet architecture used in the experiments</figDesc><table><row><cell>AlexNet:</cell><cell>Layer 1 (conv.)</cell><cell>Layer 2 (conv.)</cell><cell>Layer 3 (conv.)</cell><cell>Layer 4 (linear)</cell><cell>Layer 5 (linear)</cell><cell>Global Lyapunov Property</cell></row><row><cell></cell><cell>δ, ν</cell><cell>δ, ν</cell><cell>δ, ν</cell><cell>δ, ν</cell><cell></cell><cell></cell></row><row><cell></cell><cell>(spect. norm reg.)</cell><cell>(spect. norm reg.)</cell><cell>(spect. norm reg.)</cell><cell>(spect. norm reg.)</cell><cell></cell><cell></cell></row><row><cell>Design</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 15 :</head><label>15</label><figDesc>The hyper-parameters for the AlexNet architecture used in the experiments</figDesc><table><row><cell>AlexNet:</cell><cell>Layer 1 (conv.)</cell><cell>Layer 2 (conv.)</cell><cell>Layer 3 (conv.)</cell><cell>Layer 4 (linear)</cell><cell>Layer 5 (linear)</cell><cell>Global Lyapunov Property</cell></row><row><cell></cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν (spect. norm reg.)</cell><cell>δ, ν</cell></row><row><cell></cell><cell>δ = 0.81, ν = 0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Design</cell><cell>(1.49)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Parameters</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>δ = 0.83, ν = 0.30 (1.80) δ = 0.80, ν = 0.28 δ = 0.9, ν = 0.27 (1.76) δ = 1.47, ν = 0.17 δ1 = 0.37, ν1 = 0.66, δ2 = 0.37, ν2 = 0.66 (2.48), (2.48) δ = 1.47, ν = 0.17 δ1 = 0.37, ν1 = 0.66, δ2 = 0.37, ν2 = 0.66 (2.48), (2.48) δ = 1.47, ν = 0.17 δ1 = 0.37, ν1 = 0.66, δ2 = 0.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Experiment results</head><p>This appendix includes the experiment results for all the above architectures against FGM and Iterative PGD attacks. A pair of results are presented per DNN for the cases where a DNN was trained with or without adversarial training.</p><p>I.1. The experiment results for the Forward-Net architecture (trained on the MNIST dataset)     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Robustness performance against the Carlini &amp; Wagner (C&amp;W) attack</head><p>We tried our approach against the C&amp;W attack. The parameters were chosen according to the tutorials presented in <ref type="bibr" target="#b26">[27]</ref>. This appendix includes the results for this experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Robustness performance comparison with prior spectral normalization approaches</head><p>This appendix details a set of experiments performed to provide a robustness comparison between our proposed approach and the works given in <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b7">[8]</ref>. <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b10">[11]</ref> propose training networks with the same spectral regularization enforced across the entire network where ρ(W l ) ≤ β for l = 1, ..., n and β is a constant. We select 3 values of β from their papers: β = 1.0, 1.6, 2.0. The 2 works given in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b7">[8]</ref> may be seen as subsets of the works given in <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b10">[11]</ref>, where ρ(W l ) ≤ β for l = 1, ..., n and β = 1.0 are selected. All the aforementioned works are special cases of our proposed Lyapunov robust solution. Our experiments confirm that our approach provides 3 main benefits that the other works do not provide. 1-Our work provides the theory, reasoning and interpretation behind why spectral regularization enhances robustness, and in particular how the spectral regularization hyper-parameters for each layer should be selected. 2-Our work provides a higher level of flexibility and freedom in selecting the hyper-parameter for training based the interpretations and reasoning behind our theory and work. 3-Our proposed approach produces trained DNNs that are more robust and perform better than the other works in this area. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</title>
		<author>
			<persName><forename type="first">Anish</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple classifier systems for robust classifier design in adversarial environments</title>
		<author>
			<persName><forename type="first">Battista</forename><surname>Biggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Fumera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Roli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="41" />
			<date type="published" when="2010">12 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Unrestricted Adversarial Examples. arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum resilience of artificial neural networks</title>
		<author>
			<persName><forename type="first">Chih-Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Nührenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harald</forename><surname>Ruess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Automated Technology for Verification and Analysis</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="251" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parseval networks: Improving robustness to adversarial examples</title>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="854" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Jeremy M Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J Zico</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02918</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The coupling effect of lipschitz regularization in deep neural networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Couellan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06253</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalizable adversarial training via spectral normalization</title>
		<author>
			<persName><forename type="first">Farzan</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Tse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07457</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial vulnerability for any classifier</title>
		<author>
			<persName><forename type="first">Alhussein</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamza</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Fawzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08686</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maithra</forename><surname>Samuel S Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Goodfellow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Adversarial Spheres. In ICLR Workshop</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Cree</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04368</idno>
		<title level="m">Regularisation of neural networks by enforcing lipschitz continuity</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeqi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isay</forename><surname>Katsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pian</forename><surname>Pawakapan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08458</idno>
		<title level="m">Intermediate level adversarial attack for enhanced transferability</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Stability-certified reinforcement learning: A control-theoretic perspective</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javad</forename><surname>Lavaei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11505</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Matrix diagonal stability in systems and computation</title>
		<author>
			<persName><forename type="first">Eugenius</forename><surname>Kaszkurewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Bhaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reluplex: An efficient smt solver for verifying deep neural networks</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Julian</surname></persName>
		</author>
		<author>
			<persName><surname>Mykel</surname></persName>
		</author>
		<author>
			<persName><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="97" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">K</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><surname>Khalil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinear Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1996">1996</date>
			<publisher>Prentice Hall New Jersey</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust Linear Regression Against Training Data Poisoning</title>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeniy</forename><surname>Vorobeychik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security, AISec &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Distributional smoothing with virtual adversarial training</title>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin-Ichi</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shin</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yash</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Matyasko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vahid</forename><surname>Behzadan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Lin</forename><surname>Juang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhibhav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willi</forename><surname>Gierke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">on the cleverhans v2.1.0 adversarial examples library</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><surname>Mark N Wegman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07896</idno>
		<title level="m">L2-nonexpansive neural networks</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Connecting Lyapunov Control Theory to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">Arash</forename><surname>Rahnama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Raff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AdvML19: Workshop on Adversarial Learning Methods for Machine Learning and Data Mining at KDD</title>
				<meeting>AdvML19: Workshop on Adversarial Learning Methods for Machine Learning and Data Mining at KDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Hanie</forename><surname>Sedghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">M</forename><surname>Long</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10408</idno>
		<title level="m">The singular values of convolutional layers</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">One pixel attack for fooling deep neural networks</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><forename type="middle">Vasconcellos</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakurai</forename><surname>Kouichi</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A recurring theorem on determinants</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Taussky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Mathematical Monthly</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">10P1</biblScope>
			<biblScope unit="page" from="672" to="676" />
			<date type="published" when="1949">1949</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibani</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<title level="m">Robustness may be at odds with accuracy. stat</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Towards fast computation of certified robustness for relu networks</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Tsui-Wei Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duane</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Boning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Inderjit S Dhillon</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09699</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Provable defenses against adversarial examples via the convex outer adversarial polytope</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5283" to="5292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spectral norm regularization for improving the generalizability of deep learning</title>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10941</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the input-output stability of time-varying nonlinear feedback systems part one: Conditions derived using concepts of loop gain, conicity, and positivity</title>
		<author>
			<persName><forename type="first">George</forename><surname>Zames</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on automatic control</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient defenses against adversarial attacks</title>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria-Irina</forename><surname>Nicolae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ambrish</forename><surname>Rawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security</title>
				<meeting>the 10th ACM Workshop on Artificial Intelligence and Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="39" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient neural network robustness certification with general activation functions</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsui-Wei</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Daniel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4939" to="4948" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
