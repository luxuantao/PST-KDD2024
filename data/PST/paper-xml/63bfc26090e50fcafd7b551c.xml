<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-04-03">3 Apr 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chaoyi</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaoman</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
							<email>yazhang@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
							<email>wangyanfeng@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Cooperative Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-04-03">3 Apr 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2301.02228v3[eess.IV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider enhancing medical visuallanguage pre-training (VLP) with domain-specific knowledge, by exploiting the paired image-text reports from the radiological daily practice. In particular, we make the following contributions: First, unlike existing works that directly process the raw reports, we adopt a novel triplet extraction module to extract the medical-related information, avoiding unnecessary complexity from language grammar and enhancing the supervision signals; Second, we propose a novel triplet encoding module with entity translation by querying a knowledge base, to exploit the rich domain knowledge in medical field, and implicitly build relationships between medical entities in the language embedding space; Third, we propose to use a Transformer-based fusion model for spatially aligning the entity description with visual signals at the image patch level, enabling the ability for medical diagnosis; Fourth, we conduct thorough experiments to validate the effectiveness of our architecture, and benchmark on numerous public benchmarks e.g., ChestX-ray14, RSNA Pneumonia, SIIM-ACR Pneumothorax, COVIDx CXR-2, COVID Rural, and EdemaSeverity. In both zero-shot and fine-tuning settings, our model has demonstrated strong performance compared with the former methods on disease classification and grounding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the rapid development of deep learning, numerous works have been proposed to facilitate computer-aided diagnosis in the medical field <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b18">19]</ref>. Despite the tremendous progress, these models are normally trained to recognize or segment the structures that fall into a certain closed set of anatomical or disease categories, whenever a new disease comes to be of interest, a costly procedure for data annotation, model re-training are required, fundamentally limiting its practical values. As an alternative, recent Figure <ref type="figure">1</ref>: Our method mainly considers combining medical knowledge with VLP. We propose Triplet Extraction and Entity Translation modules, so that the network can be supervised with detailed entity-level signals.</p><p>research considers to train the model on the corpus, consisting of large amount of multi-modal data, that is generated from daily clinical routine, for instance, the most common example is the dataset of X-ray images with paired radiological reports <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31]</ref>. This paper presents our preliminary investigation on vision-language representation learning in the medical domain, with the goal of better zero-shot disease diagnosis (classification) and grounding. Undoubtedly, these tasks have also been widely investigated in the computer vision community, with significant progress made on developing Foundational Models in the past years, for example, CLIP <ref type="bibr" target="#b47">[48]</ref>, ALBEF <ref type="bibr" target="#b32">[33]</ref>, BLIP <ref type="bibr" target="#b31">[32]</ref>, etc. However, to achieve such a goal in the medical domain, different challenges must be resolved, that requires research efforts from the community: First, data availability, training Foundation Models in computer vision normally require over millions of image-text pairs, while in the medical domain, only a few hundred thousand pairs are available <ref type="bibr" target="#b30">[31]</ref>. The limited data challenges language models to understand the reports in free form <ref type="bibr" target="#b5">[6]</ref>. Second, the problem considered in computer-aided diagnosis is naturally fine-grained, that requires distinguishing the medical concepts to understand the disease, as a consequence, domain knowledge is essential; Third, robustness is crucial, it is, therefore, preferable to have explainability, where diagnosis results come along with the visual grounding, to help radiologists understand the system, and build trust between human and machines.</p><p>Existing work in medical VLP (Vision-Language Pretraining) <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref> follows a straightforward training paradigm by matching raw reports with image scans, as shown in Fig. <ref type="figure">1A</ref>, ignoring the medical prior knowledge, and, thus, we propose a novel knowledge-enhanced visuallanguage model as shown in Fig. <ref type="figure">1B</ref>. First, we propose a triplet extraction module to extract useful medical entities (keywords) from raw reports, and simplify each report into sets of triplets, denoted as {entity, position, exist}. Decomposing reports into triplets leads to an effective representation of the reports with minimal information loss due to the structural prior in reports; Second, we translate the medical entities into fine-grained descriptions by leveraging a well-defined medical word knowledge base, that tends to explain diseases with common vocabulary. Thus, computing text embeddings for these descriptions enables to implicitly establish relationships between medical entities; Third, we view the entities as a query set and adopt a transformer-based architecture for aligning the image patches with entity descriptions, that enables explicit supervision signals at entity level. Consequently, we can simultaneously infer the likelihood of certain diseases with the visual evidence in the form of a spatial heatmap, i.e., providing rough grounding for explainability.</p><p>We pre-train the model on one widely-used medical image-report dataset MIMIC-CXR <ref type="bibr" target="#b30">[31]</ref>, and rigorously evaluate on the task of disease diagnosis across numerous public benchmarks, e.g., ChestX-ray14 <ref type="bibr" target="#b55">[56]</ref>, RSNA Pneumonia <ref type="bibr" target="#b48">[49]</ref>, SIIM-ACR Pneumothorax <ref type="bibr" target="#b0">[1]</ref>, COVIDx CXR-2 <ref type="bibr" target="#b45">[46]</ref>, COVID Rural <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b14">15]</ref>, and EdemaSeverity <ref type="bibr" target="#b6">[7]</ref>. We get state-of-the-art performance on zero-shot classification and grounding on different diseases, spanning different image distributions, with further fine-tuning, our model still exceeds previous models significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>General Vision-Language Pre-training (VLP) Models. In computer vision, such line of research has gained tremendous success in the recent literature, generally speaking, the developed architectures can either be two-stream <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b29">30]</ref>, i.e., dual encoders, or those based on single-stream methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9]</ref>, that favors visual-language fusion. In particular, several works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b63">64]</ref> consider considers to combine the commonsense knowledge into the vision-language pre-training, however, in this paper, we focus on medical domain, which is clearly more fine-grained and requires significantly more expertise.</p><p>Medical Named-Entity-Recognition (NER) Models. Various natural language processing (NLP) approaches have been proposed to extract information from radiology reports <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50]</ref>. These early methods considered only the disease, thus causing information loss. Further state-ofthe-art works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref> are proposed to extract relationship between different entities without demand of pre-defined close disease set, retaining most of useful information with high accuracy. In weakly supervision <ref type="bibr" target="#b64">[65]</ref> and report generation fields <ref type="bibr" target="#b13">[14]</ref>, NER methods have shown great impact, and greatly inspired us for more effective vision-language pre-training with medical domain knowledge injected.</p><p>Medical Knowledge Enhanced Models. Leveraging external medical knowledge to enhance deep learning models is not a new topic <ref type="bibr" target="#b59">[60]</ref>. Depending the approaches of using medical knowledge, They can be classified into modelbased or input-based. In model-based approaches, the authors aim to imitate the radiological or diagnosis practice to design models <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b11">12]</ref>. While in inputbased approaches, the knowledge is treated as an extra input for computing features <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b10">11]</ref> or to guide the final training loss <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b40">41]</ref>, commonly used in report generation tasks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11]</ref>. However, none of these works are targeting on vision-language pre-training in medical domain with image-report.</p><p>Concurrent Works in Medical VLP. Existing medical VLP methods follow the two-stream flow <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b9">10]</ref>, i.e., use contrastive learning and without fusion module, for example, ConVIRT <ref type="bibr" target="#b65">[66]</ref> initially proposed to use contrastive loss as a proxy task for aligning the medical scan and corresponding reports, LoVT and GLoRIA then focus on improving the local alignment performance <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref>. BioViL notices the language pattern in reports is different from natural texts and re-designs the language model <ref type="bibr" target="#b5">[6]</ref>. Align <ref type="bibr" target="#b9">[10]</ref> improves the performance of VLP model on medical Visual Question Answering (VQA). The recent arxiv preprint, MedCLIP <ref type="bibr" target="#b57">[58]</ref>, considers leveraging unpaired data to make up data scarcity. The most related to ours is CheXzero <ref type="bibr" target="#b53">[54]</ref>, which targets at zero-shot diagnosis. Despite significant contribution has been made by existing work <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b53">54]</ref>, they still treat medical texts and images as common natural data and do not explicitly leverage the rich prior knowledge from medical domain. In this paper, we consider to incorporate domain knowledge, re-design the pre-training pipeline delicately and target at accurate diagnosis in X-ray scans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we start by describing the considered problem scenario in Sec. 3.1, followed by our report preprocess operation with triplet extraction in Sec.  visual-language signals. In Sec. 3.4, we describe the training procedure with the paired image-reports sourced from the daily routine X-ray scans and, in Sec.3.5, we introduce the procedure for inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Scenario</head><p>Assuming we are given a training set with N samples, i.e., D train = {(X 1 , T 1 ), . . . , (X N , T N )}, where X i , T i refer to the X-ray image and its corresponding medical report generated in the daily routine scans, respectively, our goal is to train a visual-language model that enables us to diagnose the existence of certain diseases and localize the visual evidence spatially. Specifically, at inference time, we can freely ask the system to identify the likelihood of the patient getting a certain disease (seen or unseen during training):</p><p>?i , mi = ? fusion (? visual (X i ), ? textual ([description])), <ref type="bibr" target="#b0">(1)</ref> where X i ? R H?W ?3 refers to an image sample from the test set, with H, W denoting height and width respectively. ?i ? [0, 1] refers to the inferred likelihood of the patient having a certain disease indicated by the input description, and mi ? R H?W ?1 denotes a predicted spatial heatmap, with high activation on pixels that potentially provide the visual indication for such disease. In the following section, we will introduce our report pre-process operation with triplet extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Report Pre-processing</head><p>To start with, we propose to pre-process medical reports with a Triplet Extraction module by removing the unnecessary complexity from language grammar. Note that, we hereon only consider single sampled image-reports pair (X i , T i ), and ignore the subscript in notations for simplicity.</p><p>We condense the original reports with an off-shelf medical Named Entity Recognition (NER) method, namely Rad-Graph <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b64">65]</ref>, transforming reports into a set of triplets, as shown in Figure <ref type="figure" target="#fig_0">2A</ref>. In detail, the medical key words can be extracted and classified as "entity" or "position" with the NER module. "Entity" refers to some clinical observations, like "Opacity". "Position" refers to the anatomical body part that occurs in a radiology report, like "right lower lobe". Besides, the NER module will also provide an "exist" label to conclude whether an entity is claimed to be exist, absent or uncertain in reports. Based on this, we can use a set of triplets, i.e., {entity, position, exist}, to reformulate the sentence in reports, for example, the triplet {Opacity, Right lower lobe, True} represents "It is true that there is opacity located at right lower lobe". Note that, the triplets with a specific "position" are not always termed as True in "exist" as radiologists may point out entities absent at some specific position.</p><p>Therefore, given a report T with multiple sentences, T = {s 1 , s 2 , ..., s M }, the extraction module independently operates on each of the sentences, and construct a number of triplets from the report:</p><formula xml:id="formula_0">? ex (s j ) = {entity n , position n , exist n }, n ? [0, t j ],<label>(2)</label></formula><p>where t j represents the total number of entities contained in one sentence, with n = 0 indicating the special case that there is no valid entity. After the triplet extraction, each report is equal to a set of triplets. Discussion. In contrast to natural texts, information in medical reports tends to be more condensed, with radiologists pointing out the existence of abnormality and their positions in the image. Meanwhile, medical terminologies tend to be professional, and within certain vocabulary (mostly listed in UMLS <ref type="bibr" target="#b4">[5]</ref>), specially designed NER methods <ref type="bibr" target="#b28">[29]</ref> demonstrate great performance on reports. Therefore, adopting the Triplet Extraction operation in medical VLP can avoid unnecessary complexity from understanding grammar, while still retaining the useful information in reports.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Architecture</head><p>In this section, we detail our proposed framework, consisting of three components, namely, visual encoding, knowledge-enhanced triplet encoding, and fusion module, as shown in Fig. <ref type="figure" target="#fig_0">2B</ref> and Fig. <ref type="figure" target="#fig_0">2C</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Visual Encoding</head><p>Given an X-ray image scan X ? R H?W ?3 , we can compute the features with a visual backbone:</p><formula xml:id="formula_1">V = ? visual (X ) ? R h?w?d ,<label>(3)</label></formula><p>h, w, d refer to the height, width, and feature dimension of the output, in our case, we adopt ResNet-50 as the visual backbone, and take the output from the 4th residual block.</p><p>Note that, we make the architectural choice for fair comparison with existing work <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref>, while other visual backbones, e.g., ViT <ref type="bibr" target="#b16">[17]</ref>, can equally be applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Knowledge-enhanced Triplet Encoding</head><p>The goal of this module is to encode the triplets extracted from reports by incorporating medical domain knowledge as shown in Fig. <ref type="figure" target="#fig_0">2B</ref>. Given a triplet as {entity, position, exist}, it is easy to code the "exist" as it only has three outcomes. We use l ? {0, 1, -1} to tokenize it, 1 for True, 0 for False, -1 for uncertain. For the "entity" words, we translate them into detailed descriptions by querying some easy-access medical knowledge bases 12 , e.g., Description(["Pneumonia"])="It is a condition of the lung primarily . . . present with opacities and pleural effusion . . . ". Despite its simplicity, converting the entities into descriptions is crucial for more reliable and zero-shot diagnosis, as it further decomposes the professional medical entities into basic attributes that are shared by different diseases, encouraging the model to capture a deep understanding of the visual evidence. For the "position" words, we use a prompt as "It is located at {position}" to form a sentence. Finally, we use ClinicalBERT <ref type="bibr" target="#b2">[3]</ref> as a pre-trained text encoder, to compute the embedding for the "entity" and "position", and then adopt a linear MLP to project the embedding to desired dimensions:</p><formula xml:id="formula_2">e = ? textual (Description({entity}) ) ? R d ,<label>(4)</label></formula><formula xml:id="formula_3">p = ? textual ("It is located at {position}") ? R d .<label>(5)</label></formula><p>Each triplet has now been embedded into {e, p, l}. Discussion. The extracted entities are medical terminologies that are only understandable to audiences with a medical background, while enriching them with detailed descriptions helps the model to capture a deep understanding of visual evidence for diseases. Such patterns can be generalized across diseases, as many attribute descriptions tend to be shared, enabling the model to build implicit relationships on seen classes and understand descriptions for unseen ones.</p><p>1 Wikipedia https://en.wikipedia.org/wiki/ 2 UMLS <ref type="bibr" target="#b4">[5]</ref> https://www.nlm.nih.gov/research/umls/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Fusion Module</head><p>With the triplets from reports, we can supervise the model on the entity level instead of the entire report level. The "position" and "exist" parts in triplets can be naturally seen as more fine-grained supervision labels. Specifically, we adopt a Transformer-based architecture, use the embedding of entities as query, iteratively attending the image embeddings, and output exist and position predictions of entities.</p><p>In detail, we select the top |Q| most commonly appearing entities' embeddings in all training reports, to form an entity query set Q = {e 1 , e 2 , ..., e |Q| }. The details of the entity query set is provided in the supplementary material (Sec. A). Then Q will be passed into a fusion module with the image representation V for alignment. The fusion module consists of multiple Transformer Decoder layers, with Q as Query, and V as Key and Value. The outputs are further fed into two MLPs, independently infer the existence of the entity and the entity's position:</p><formula xml:id="formula_4">{?, p, m} = ? fusion (V, Q),<label>(6)</label></formula><p>where ? ? R |Q| represents the existence prediction for each entity query, and p ? R |Q|?d represents the predicted position for all entities. Note that, m ? R H?W denotes the average of the cross-attention maps sourced from Transformer layers and is up-sampled to the size of input image with nearest interpolation. m is used for grounding at inference, as it naturally acts as a segmentation heatmap. During training, we will not directly calculate any loss on it. Discussion. Adopting Transformer decoder enables to compute correspondences between entities and images at patch-level. Consequently, image features V are more suitable for downstream segmentation tasks and the average of cross-attention maps in each layers can be used directly for zero-shot grounding, providing explainable for diagnosis.</p><p>Besides, the default self-attention on queries in Transformer structure can also build relationships across entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training</head><p>Given a set of encoded triplets {e, p, l} extracted form the pairing reports T , we can compute training loss on the output of fusion module. For the existence prediction ?, we use binary cross-entropy with the corresponding "exist" labels l, and if l is -1 we just pass this label, denoted as L cls . To supervise the position prediction for each entity query, we adopt contrastive learning. We form a position set with top |P | common position embeddings as a position set, P = {p 1 , p 2 , ? ? ? , p |P | }, randomly sample M negative position embeddings from it, and use the corresponding position embedding p from triplets as positive:</p><formula xml:id="formula_5">L loc = - 1 |Q| |Q| k=1 e pk ,p k e pk ,p k + M u=1 e pk ,P I(k,u) ,<label>(7)</label></formula><p>where ?, ? represents the inner product of two vectors and I(?, ?) is a random index sampling function. The position embeddings are un-normalized in calculation. Note that, some entities may not be mentioned in the report and thus, we can not find corresponding labels in triplets. We simply ignore the corresponding predictions while computing loss. The final loss is the sum of the two:</p><formula xml:id="formula_6">L total = ? 1 L loc + ? 2 L cls ,<label>(8)</label></formula><p>where ? 1 , ? 2 refer to two hyper-parameters controlling the ratio of the two losses, and we set them to be 1.0 by default.</p><p>Discussion. In contrast to the existing approaches <ref type="bibr" target="#b65">[66]</ref> that align images with entire reports, our training paradigm with triplets provides supervision at a more fine-grained entity level, rather than the global alignment between image and reports as has often done in existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Inference</head><p>At inference time, given a test image, we can directly infer the existence of certain entities/disease, and ground their visual evidence. In particular, for the entities that have appeared in the entity query set Q, we simply adopt the corresponding elements from Q, while for those unseen ones, we replace the entity with a brief description provided by the user, and treat it as an extra query added to entity query set Q, resembling zero-shot inference. The existence output ? can be directly applied for classification, the average cross-attention m between the target entity and the visual features are used for grounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>In this section, we start by introducing the dataset used for experiments, e.g., pre-training, and various downstream datasets. Then we describe the implementation details and the considered baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Dataset</head><p>MIMIC-CXR v2 <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b21">22]</ref> consists of over 227k studies of paired image-report data, they are sourced from 65,379 patients at different scanning. Each study can have one or two images (different scan views), totaling 377,110 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets for Downstream Tasks</head><p>ChestX-ray14 <ref type="bibr" target="#b55">[56]</ref> contains 112,120 frontal-view X-ray images of 30,805 unique patients, collected from the year of 1992 to 2015 by NIH(National Institutes of Health), with labels of 14 common diseases provided. We split the dataset into 0.8/0.1/0.1 for train/valid/test. RSNA Pneumonia <ref type="bibr" target="#b48">[49]</ref> contains more than 260k frontalview chest X-rays with corresponding pneumonia opacity masks collected by RSNA (Radiological Society of North America). Commonly, it is treated as a classification tasks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6]</ref>. We split the dataset into 0.6/0.2/0.2 for train/valid/test. <ref type="bibr" target="#b0">[1]</ref> contains more than 12k frontal-view chest X-rays with pneumothorax masks collected by SIIM-ACR (Society for Imaging Informatics in Medicine and American College of Radiology). Similarly to RSNA Pneumonia dataset, it can be both used as classification and segmentation tasks. We split the dataset into 0.6/0.2/0.2 for train/valid/test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SIIM-ACR Pneumothorax</head><p>COVIDx CXR-2 <ref type="bibr" target="#b45">[46]</ref> and COVID Rural <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b14">15]</ref>  Edema Severity <ref type="bibr" target="#b6">[7]</ref> contains 6,524 examples from MIMIC-CXR with pulmonary edema severity labels (0 to 3, increasing severity) extracted from the radiology reports. Of these, 141 radiologists were examined by radiologists, and consensus was reached on severity level. It can be seen as a typical fine-grained classification task. We split the dataset into 0.6/0.2/0.2 for train/valid/test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation</head><p>This section describes the implementation for architectures. In Pre-training, the triplets extraction module and text encoders used in triplets encoding are all fixed, while the visual encoder and fusion module are trained end-toend on the image-text pairs. In Fine-tuning, we adopt ResNet50 <ref type="bibr" target="#b23">[24]</ref> initialized with image encoder for classification, and ResUNet <ref type="bibr" target="#b15">[16]</ref> initialize its encoder with our pretrained image encoder for segmentation. More details about exact values of different parameters and training progress can be found in supplementary material (Sec. B)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Baselines</head><p>We compare with various existing state-of-the-art medical image-text pre-train methods, namely, ConVIRT <ref type="bibr" target="#b65">[66]</ref>, GLoRIA <ref type="bibr" target="#b24">[25]</ref>, BioViL <ref type="bibr" target="#b5">[6]</ref> and CheXzero <ref type="bibr" target="#b53">[54]</ref>. Since Con-VIRT and GLoRIA are pre-trained on an in-house dataset, we re-train their models on MIMIC-CXR dataset for fair comparison. For BioViL, we use the officially released models by the authors. For zero-shot setting, we use the prompt as mentioned by BioViL <ref type="bibr" target="#b5">[6]</ref> and compare to the very recent method (CheXzero <ref type="bibr" target="#b53">[54]</ref>) that has shown to have better zero-shot diagnosis ability than radiologists. For finetuning, we all use the same setting as described in Sec. <ref type="bibr" target="#b3">4</ref> The ACC score is also calculated under this threshold.</p><p>Pointing Game is used for evaluating the grounding performance. In specific, we extract the region with max response in the output heat-map, for one instance, if the region hit the ground-truth mask, it is considered a positive prediction, otherwise negative. Finally, accuracy can be calculated as the pointing game score.</p><p>Dice and IOU are commonly used for segmentation tasks.</p><p>For zero-shot segmentation, we search the segmentation threshold with 0.01 interval for all methods, and report the maximal Dice score for each model.</p><p>Precision and Recall refer to the detection Precision and Recall. For medical, it is important that lesions are detected even without fine segmentation. Additionally, in some hard cases, especially for the zero-shot setting, Dice and IOU may be too strict to reflect the performance difference. Precision and recall scores can compensate for these.</p><p>We choose the IOU threshold as 0.1 to calculate the scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section, we will report the experimental results. In general, we split the results into two parts: zero-shot setting and fine-tuning setting. In the zero-shot case (Sec. 5.1), we carry out the ablation study and compare it with the other SOTA image-text pre-train methods. We mainly consider classification and segmentation tasks; In the fine-tuning case (Sec. 5.2), we evaluate the model's transferability by fine-tuning the model with 1%, 10%, and 100% data por-tion. Additionally, we also add a disease grading downstream task, which can be seen as a fine-grade classification task, showing that our pre-trained model can be transferred to the downstream tasks at ease.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Zero-shot Evaluation</head><p>In this section, we compare our method with other stateof-the-art methods under zero-shot setting, on classification and grounding. Due to the space limitation, we include the entire ablation study in the supplementary material (Sec. C), referring to it for more details and analysis, and all comparisons here are made using our best model with position contrastive loss and entity description encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Classification</head><p>Seen Diseases. As shown in Tab. 1, we compare with existing methods on three widely-used datasets, demonstrating consistent performance improvement. Specifically, on pneumonia and pneumothorax datasets, despite the images being collected by different clinics with different diseases, our model improves the AUC score from 0.83 to 0.87 on RSNA pneumonia dataset and from 0.71 to 0.89 on SIIM-ACR pneumothorax dataset, as shown in Tab. 1. This shows that our method can better deal with the multi-center and multi-disease data distribution in medical. While on ChestX-ray14 dataset, we improve the average AUC scores from 0.69 to 0.77, we refer the reader to supplementary material (Sec. D) for detailed comparison of 14 diseases.</p><p>Unseen Diseases. Here, we are considering a strict setting for openset classification, in particular, we use covid- <ref type="bibr" target="#b18">19</ref>   unseen diseases. As shown in Tab. 2, existing approaches that only rely on disease name struggles to make the correct diagnosis. While, our proposed approach, after introducing medical knowledge, i.e., using entity descriptions, can understand the complex medical entity descriptions unseen in the training set, and significantly boost the performance from 0.66 to 0.74 on AUC and from 0.59 to 0.70 on ACC, demonstrating entity translation is vital for unseen diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Grounding</head><p>In addition to the plain diagnosis, explainability can be equally critical in healthcare, improving the reliability and trustiness of the machine learning systems. Here, we consider providing explainability by grounding the abnormality in the prediction and compare against the existing approaches. Similarly, we split the diseases into seen and unseen ones, depending on whether their names have appeared in the medical reports. Specifically, "Pneumonia" and "Pneumothorax" are treated as seen, and "Covid-19" is treated as unseen. Due to the space limitation, we include visualization results in supplementary material (Sec. E). Seen Diseases. We show the results for grounding on RSNA Pneumonia opacity and SIIM-ACR Pneumothorax collapse in Tab. 3. As shown in Tab. 3a, our proposed model surpasses existing approaches on all metrics, for example, we improve the pointing game score from 0.83 to 0.87, the detection Recall from 0.85 to 0.87, the detection precision from 0.50 to 0.64, the IOU from 0.30 to 0.32 and the Dice from 0.44 to 0.46. While on SIIM-ACR dataset (Tab. 3b), the pneumothorax region tends to be thin and narrow, local-izing it can often be more challenging than that of opacity grounding <ref type="bibr" target="#b5">[6]</ref>, we thus only consider pointing game, recall, and precision. Similarly, our method can achieve significantly better performance than prior approaches.</p><p>Unseen Diseases. We also conduct the zero-shot grounding experiment on the unseen disease, namely, Covid-19, as shown in Tab. 4. Our model has shown consistent improvements in all metrics, e.g., boosting the pointing game score from 0.40 to 0.58. One observation to be noticed is that, results in Tab. 4 are mostly consistent with those in Tab. 2, i.e., better classification results tend to lead to better grounding. Overall, our model with knowledge-enhanced language encoding has facilitated the visual encoder to learn underlying evidence relating to the diseases, therefore, leading to more interpretable representations than prior approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Fine-tuning Evaluation</head><p>In this section, we consider the fine-tuning scenario, with the pre-trained model as initialization, and trained end-toend on the downstream tasks. We test on three different tasks, namely, classification, segmentation, and grading. In classification and segmentation, the test splits and metrics are the same as in the "zero-shot" section. Grading is a new task we introduce in fine-tuning setting, which can be seen as a fine-grained classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Classification</head><p>We experiment on four different datasets, using 1%, 10%, 100% of the data for fine-tuning, that is consistent with the existing work <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6]</ref>. As shown in Tab. demonstrated substantial improvements in the AUC scores over the existing approaches across all datasets, reflecting that our pre-trained representation is of higher quality than existing models. We refer the readers to the supplementary material (Sec. D) for more detailed comparison results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Segmentation</head><p>In Tab. 6, we conduct fine-tuning experiments on three different diseases for segmentation. We pick 1%, 10%, 100% of the data for fine-tuning. For all three different diseases with different image distributions, our methods surpass the existing state-of-the-art methods by a large margin, especially under the low-data regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Grading</head><p>Besides diagnosis, grading the disease severity level also plays an important role. Here, we adopt our pre-trained features and train them for the multi-class classification task, with 0 to 3 representing different severity levels. As shown in Tab. 7, for each level, the AUC, F1, and ACC scores are calculated as one class against all other ones, for example, 0 vs {1, 2, 3}. Final macro average scores of four levels are computed. On the majority of severity levels, our method can achieve the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduce a novel medical knowledge enhanced VLP model. First, we propose a triplet extraction module to extract useful medical-related triplets as more useful supervision signals, simplifying complex raw reports with minimal information loss. Second, we translate the entities in extracted triplets into detailed medical descriptions and embed them with a text encoder enabling the network to understand complex medical expert-level knowledge. Finally, a transformer-based structure is proposed to do local region alignment. In experiments, We evaluate our method on different datasets under various settings. Our method shows strong zero-shot classification and grounding abilities, even facing unseen diseases. Additionally, with finetuning, our method still outperforms state-of-the-art methods significantly, showing the superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Entity Description Base and Position Set</head><p>We leverage the rad-graph NER extraction results provided by <ref type="bibr" target="#b64">[65]</ref> and further add extra descriptions. Tab. 8 shows the descriptions we used to translate different entities. We have kept 75 entities in entity query set Q, following <ref type="bibr" target="#b64">[65]</ref>, which covers 90% entities in reports. "Tail abnorm obs" entity represents some tailed entities and "exluded obs" represents some entities useless for diagnosis. The last "covid-19" row is only referred for inference since it never appear in pre-train reports. Additionally, we keep 51 positive positions, following <ref type="bibr" target="#b64">[65]</ref>, to form the position set P , as {trachea, left hilar, right hilar, hilar unspec, left pleural, right pleural, pleural unspec, heart size, heart border, left diaphragm, right diaphragm, diaphragm unspec, retrocardiac, lower left lobe, upper left lobe, lower right lobe middle right lobe, upper right lobe, left lower lung, left mid lung, left upper lung left apical lung, left lung unspec, right lower lung, right mid lung, right upper lung right apical lung, right lung unspec, lung apices, lung bases, left costophrenic right costophrenic, costophrenic unspec, cardiophrenic sulcus, mediastinal, spine clavicle, rib, stomach, right atrium, right ventricle, aorta, svc, interstitium, parenchymal, cavoatrial junction, cardiopulmonary, pulmonary, lung volumes, unspecified, other}. "Other" is used to reprepresnt some tailed positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Model architecture. As input to the model, images are resized into 224 ? 224 ? 3. We use the first four layers of ResNet50 <ref type="bibr" target="#b23">[24]</ref> as our visual backbone (? visual ), and adopt a MLP layer to transform the output feature dimension into d = 256. As a result, the output feature maps from visual encoder is V ? R 14?14?256 . On the report side, we extract the triplets with a pre-trained NER module, as described in <ref type="bibr" target="#b28">[29]</ref>, and compute the entity and position embedding with a pre-trained ClinicalBERT <ref type="bibr" target="#b1">[2]</ref>, its default embedding dim is d = 768. We obtain |Q| = 75 entities and |P | = 51 positions that most frequently appear in the reports, following <ref type="bibr" target="#b64">[65]</ref>. We sample M = 7 negative positions for each entity to calculate contrastive loss. In the fusion module, We adopt 4 Transformer Decoder layers with 4 heads. Pre-training. At this stage, both the pre-process operation and language encoding use pre-trained networks, while the visual encoder and fusion module are trained end-to-end. We use AdamW <ref type="bibr" target="#b39">[40]</ref> optimizer with lr = 1 ? 10 -4 and lr warm up = 1 ? 10 -5 . We train on a GeForce RTX 3090 GPU with batch size 32 for 60 epochs. First 5 epochs are set for warming up. Fine-tuning. For the downstream tasks, with large amount of training data, we can fine-tune the model end-to-end, with our pre-trained visual backbone as initialization. Specifically, for image classification task, we adopt ResNet50 <ref type="bibr" target="#b23">[24]</ref> and initialize its first four layers with our pre-trained visual encoder. For image segmentation task, we use ResUNet <ref type="bibr" target="#b15">[16]</ref> as backbone and initialize its encoder with our pre-trained image encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Study</head><p>Our final method mainly contains three key parts, transformer-based entity-query fusion module, position location contrastive loss (PosCL), and entity translation (ET) encoding. We gradually remove the modules to analyze their effectiveness. "w/o (ET)" refers to removing ET module and "w/o (PosCL + ET)" refers to only maintaining the fusion module with basic CE loss. We cannot dismiss the transformer-based entity-query fusion module as it is the most basic module to support our pre-training. Tab. 9 and Tab. 10 shows the quantitative results. Entity-query Fusion module. The lines about "w/o (PosCL + ET)" in tables demonstrate the performance of the basic model modified only by base entity existence CE loss. This model can exceed many former methods. This proves our assumption that the complex syntax will hurt the network to capture the useful entities significantly and our pre-process operation and entity-level supervision can greatly relieve the problem. Position Contrastive Loss. The PosCL can significantly help the network to ground the abnormalities. As shown in the results by adding PosCL the classification results can be further improved, e.g., from 0.75 to 0.76 on AUC in ChestX-ray14 dataset. Besides classification, location contrastive loss can bring more gain in grounding. These results show position is another vital element in reports especially for grounding tasks. Our extracted triplets can conclude and clean the reports with little information loss and make the network learn the report information more straightforward. Entity Translation. By adding entity translation, we want to realize two goals. First, in addition to just learning from the image-report data, the network can actively learn the relationship between different entities based on the entity descriptions. As shown in tables, adding descriptions in most scenarios can help the network better understand the entity and bring gain to the final metric scores. Second, more importantly, the entity translation enables our model to handle openset new diseases. If excluding TE and prompting the entities only with their names as former works, the performance of our method will drop significant when facing unseen diseases which is discussed in zero-shot classification for Covid-19 at main body.  Fig. <ref type="figure" target="#fig_2">4</ref> shows visualization results of our model on zero-shot grounding task. As shown in figure, the ground truth of "Pneumonia" is given by bounding box and generally related to a large area region. Thus the metrics on this are higher than other two datasets. Our network captures its regions very well. For "Pneumothorax", its abnormality pattern is different from other diseases, which aim to capturing the collapsed part of the lung, rendering darker areas on the images rather than brighter opacity. Its ground-truth masks are generally thin and narrow while our network can still highlight its location. For "Covid-19", its image textual was similar to "Pneumonia", but since this is a totally new disease, grounding its regions is much more challenging. It requires the model to build relationships between them based on their complex definition and symptoms. The visualization results suggest that our model successfully achieve this, supporting that, for other unseen diseases, our model can also understand their complex descriptions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The whole framework of our method. We first pre-process the report into triplets leveraging triplet extraction module. Then we encode the extracted triplets and it is worth emphasizing that we translate the entities into detailed descriptions during encoding, by querying the medical knowledge base. Finally we change the training flow with triplets, i.e., we query a transformer-based fusion module at entity level, which provides more detailed supervision signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>b) Zero-shot grounding on Pneumothorax Table 10: Ablation study on zero-shot grounding tasks. (a) shows the results on RSNA Pneumonia dataset. (b) shows the results on SIIM-ACR Pneumothorax dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The visualization of zero-shot grounding results of our method. Each column represents the results on one disease and the left in it is the ground-truth and right is the heatmap predication of our model. The brighter the red on the figure, the more likely the model considering this region to be associated with abnormalities.</figDesc><graphic url="image-13.png" coords="16,297.93,573.08,75.63,75.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Knowledge-enhanced Triplet Encoding</head><label></label><figDesc>3.2. Then we introduce our proposed knowledge-enhanced architecture in Sec. 3.3, including, visual encoder, knowledgeenhanced triplet encoder, and the fusion module for aligning</figDesc><table><row><cell></cell><cell>C. Training Flow</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Contrastive Head Contrastive Head</cell><cell>CE Head CE Head</cell></row><row><cell>Entity Translation Knowledge Base?</cell><cell>Visual Encoder Visual Encoder</cell><cell>? ?</cell><cell cols="2">Fusion Module Fusion Module Contrastive Loss CE Loss +</cell></row><row><cell></cell><cell></cell><cell></cell><cell>? ?</cell></row><row><cell>Text Encoder Text Encoder</cell><cell></cell><cell></cell><cell>Entity Query Set</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Corresponding Encoded</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Whole Encoded Triplets</cell><cell>Triplets: {e, p, l}</cell></row></table><note><p><p><p><p>B.</p>Pneumonia is an condition of ? Pneumothorax is an abnormal ? Opacity is defined as an area ? ?</p>Entity Translation Knowledge Base?</p>Pneumonia is an condition of ? Pneumothorax is an abnormal ? Opacity is defined as an area ? ? Positions p+ Top Common</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>aim to evaluate on diagnosing COVID-19. COVIDx CXR-3 contains 29,986 images from 16,648 patients with COVID-19 classification labels. We use it as a classification dataset and split it into 0.7/0.2/0.1 for train/valid/test. Additionally, we use COVID Rural dataset for COVID-19 segmentation. It contains more than 200 chest X-rays with segmentation masks, and we split it into 0.6/0.2/0.2 for train/valid/test.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>.<ref type="bibr" target="#b2">3</ref>. Comparison with other state-of-the-art methods on zero-shot classification task. AUC, F1 and ACC scores are reported. For ChestX-ray14, the metrics all refer to the macro average on the 14 diseases.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell cols="2">RSNA Pneumonia</cell><cell></cell><cell cols="3">SIIM-ACR Pneumothorax</cell><cell></cell><cell>ChestX-ray14</cell></row><row><cell>Methods</cell><cell></cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell></row><row><cell cols="2">ConVIRT [66]</cell><cell>0.8042</cell><cell>0.5842</cell><cell>0.7611</cell><cell>0.6431</cell><cell>0.4329</cell><cell>0.5700</cell><cell>0.6101</cell><cell>0.1628</cell><cell>0.7102</cell></row><row><cell cols="2">GLoRIA [25]</cell><cell>0.7145</cell><cell>0.4901</cell><cell>0.7129</cell><cell>0.5342</cell><cell>0.3823</cell><cell>0.4047</cell><cell>0.6610</cell><cell>0.1732</cell><cell>0.7700</cell></row><row><cell>BioViL [6]</cell><cell></cell><cell>0.8280</cell><cell>0.5833</cell><cell>0.7669</cell><cell>0.7079</cell><cell>0.4855</cell><cell>0.6909</cell><cell>0.6912</cell><cell>0.1931</cell><cell>0.7916</cell></row><row><cell cols="2">CheXzero [54]</cell><cell>0.8579</cell><cell>0.6211</cell><cell>0.7942</cell><cell>0.6879</cell><cell>0.4704</cell><cell>0.5466</cell><cell>0.7296</cell><cell>0.2141</cell><cell>0.8278</cell></row><row><cell>Ours</cell><cell></cell><cell>0.8694</cell><cell>0.6342</cell><cell>0.8002</cell><cell>0.8924</cell><cell>0.6833</cell><cell>0.8428</cell><cell>0.7676</cell><cell>0.2525</cell><cell>0.8619</cell></row><row><cell>Prompt Type</cell><cell></cell><cell cols="2">Direct Covid-19</cell><cell cols="3">Covid-19 Description</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ConVIRT [66]</cell><cell>0.6159</cell><cell>0.7057</cell><cell>0.6113</cell><cell>0.5208</cell><cell>0.6902</cell><cell>0.5266</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GLoRIA [25]</cell><cell>0.6319</cell><cell>0.6938</cell><cell>0.5710</cell><cell>0.6659</cell><cell>0.7007</cell><cell>0.6083</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BioViL [6]</cell><cell>0.6137</cell><cell>0.6958</cell><cell>0.5461</cell><cell>0.5382</cell><cell>0.6910</cell><cell>0.5375</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CheXzero [54]</cell><cell>0.6462</cell><cell>0.7369</cell><cell>0.6629</cell><cell>0.6667</cell><cell>0.6400</cell><cell>0.6578</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>0.6561</cell><cell>0.7066</cell><cell>0.5917</cell><cell>0.7396</cell><cell>0.7670</cell><cell>0.7006</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison with other state-of-theart methods on zero-shot Covid-19 classification task. AUC, F1 and ACC scores are reported. "Direct covid-19" refers to directly use "Covid-19" to construct the prompt sentence while "Covid-19 Description" refers to replace the name "Covid-19" with its description.</figDesc><table><row><cell>4.5. Metrics</cell></row></table><note><p>AUC, F1 and ACC are measured for classification tasks. F1 comprehensively measures the recall and precision of the model, and ACC is the short of Accuracy. The final binary prediction threshold is chosen to maximise the F1 score.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>to evaluate the systems. Covid-19 is a new disease that only appeared in 2019, MIMIC-CXR reports collected in the year 2015 do not include any entity of covid-19, thus it requires the system to have the ability to diagnose truly</figDesc><table><row><cell>Methods</cell><cell cols="3">Pointing Game? Recall? Precision?</cell><cell>IoU?</cell><cell>Dice?</cell><cell>Methods</cell><cell cols="3">Pointing Game? Recall? Precision?</cell></row><row><cell>GLoRIA [25]</cell><cell>0.7607</cell><cell>0.8330</cell><cell>0.1621</cell><cell cols="2">0.2182 0.3468</cell><cell>GLoRIA [25]</cell><cell>0.0651</cell><cell>0.2377</cell><cell>0.0585</cell></row><row><cell>BioViL [6]</cell><cell>0.8342</cell><cell>0.8521</cell><cell>0.5034</cell><cell cols="2">0.3029 0.4386</cell><cell>BioViL [6]</cell><cell>0.0252</cell><cell>0.1963</cell><cell>0.1429</cell></row><row><cell>Ours</cell><cell>0.8721</cell><cell>0.8661</cell><cell>0.6420</cell><cell cols="2">0.3172 0.4649</cell><cell>Ours</cell><cell>0.1975</cell><cell>0.3562</cell><cell>0.1940</cell></row><row><cell></cell><cell cols="4">(a) Zero-shot grounding on Pneumonia</cell><cell></cell><cell cols="4">(b) Zero-shot grounding on Pneumothorax</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison with other state-of-the-art methods on zero-shot region grounding tasks. (a) shows the results on RSNA Pneumonia dataset. (b) shows the results on SIIM-ACR Pneumothorax dataset. The pneumothorax region tends to be thin and narrow and much more challenging for grounding, we thus only consider pointing game, recall, and precision. Our method can achieve better performance on different metrics, especially on the pointing game score. ConVIRT and CheXzero can not realize this function.</figDesc><table><row><cell>Prompt Type</cell><cell></cell><cell cols="2">Direct covid-19</cell><cell></cell><cell></cell><cell cols="2">Covid-19 Description</cell></row><row><cell>Methods</cell><cell cols="6">Pointing Game? Recall? Precision? IoU? Dice? Pointing Game? AR?</cell><cell>AP?</cell><cell>IoU? Dice?</cell></row><row><cell>GLoRIA [25]</cell><cell>0.0364</cell><cell>0.2906</cell><cell>0.1073</cell><cell>0.0645 0.1141</cell><cell>0.2727</cell><cell cols="2">0.2821 0.1336 0.0596 0.1075</cell></row><row><cell>BioViL [6]</cell><cell>0.4000</cell><cell>0.2564</cell><cell>0.2703</cell><cell>0.1198 0.1967</cell><cell>0.1818</cell><cell cols="2">0.2393 0.1637 0.0861 0.1427</cell></row><row><cell>Ours</cell><cell>0.1818</cell><cell>0.1880</cell><cell>0.1497</cell><cell>0.0747 0.1289</cell><cell>0.5818</cell><cell cols="2">0.5214 0.4959 0.1373 0.2278</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note><p>Comparison with other state-of-the-art methods on zero-shot covid-19 opacity region grounding task. "Direct covid-19" refers to directly use "Covid-19" to construct the prompt sentence for entity encoding while "Covid-19 Description" refers to replace the name "Covid-19" with its description. Our method can achieve better performance on different metrics.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc><ref type="bibr" target="#b4">5</ref>, our model has Comparison of AUC scores with other state-of-the-art methods on fine-tuning classification task. The macro average of AUC scores on 14 diseases are reported for ChestX-ray14 dataset.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>Pneumonia</cell><cell></cell><cell cols="2">Pneumothorax</cell><cell></cell><cell></cell><cell>Covid-19</cell><cell></cell><cell cols="2">ChestX-ray14</cell><cell></cell></row><row><cell>Data Portion</cell><cell>1%</cell><cell>10%</cell><cell>100%</cell><cell>1%</cell><cell>10%</cell><cell>100%</cell><cell>1%</cell><cell>10%</cell><cell>100%</cell><cell>1%</cell><cell>10%</cell><cell>100%</cell></row><row><cell>Scratch</cell><cell cols="3">0.7107 0.8150 0.8626</cell><cell cols="3">0.4347 0.6120 0.6571</cell><cell cols="3">0.7861 0.9162 0.9554</cell><cell cols="3">0.6005 0.7365 0.7924</cell></row><row><cell cols="4">ConVIRT [66] 0.8398 0.8562 0.8761</cell><cell cols="3">0.7134 0.7826 0.9004</cell><cell cols="3">0.8675 0.9541 0.9726</cell><cell cols="3">0.6615 0.7658 0.8128</cell></row><row><cell>GLoRIA [25]</cell><cell cols="3">0.8599 0.8666 0.8846</cell><cell cols="3">0.7439 0.8538 0.9014</cell><cell cols="3">0.9065 0.9381 0.9728</cell><cell cols="3">0.6710 0.7642 0.8184</cell></row><row><cell>BioViL [6]</cell><cell cols="3">0.8233 0.8538 0.8836</cell><cell cols="3">0.6948 0.7775 0.8689</cell><cell cols="3">0.8989 0.9529 0.9729</cell><cell cols="3">0.6952 0.7527 0.8245</cell></row><row><cell>Ours</cell><cell cols="3">0.8731 0.8799 0.8931</cell><cell cols="3">0.8527 0.9071 0.9188</cell><cell cols="3">0.9224 0.9657 0.9729</cell><cell cols="3">0.7721 0.7894 0.8323</cell></row><row><cell>Diseases</cell><cell></cell><cell cols="2">Pneumonia</cell><cell></cell><cell></cell><cell cols="2">Pneumothorax</cell><cell></cell><cell></cell><cell>Covid-19</cell><cell></cell><cell></cell></row><row><cell>Data Portion</cell><cell></cell><cell>1%</cell><cell>10%</cell><cell>100%</cell><cell>1%</cell><cell></cell><cell>10%</cell><cell>100%</cell><cell>1%</cell><cell>10%</cell><cell cols="2">100%</cell></row><row><cell>Scratch</cell><cell></cell><cell>0.4347</cell><cell>0.6047</cell><cell>0.7068</cell><cell>0.2133</cell><cell cols="2">0.3323</cell><cell>0.7447</cell><cell>0.1481</cell><cell>0.2367</cell><cell cols="2">0.3228</cell></row><row><cell cols="2">ConVIRT [66]</cell><cell>0.5706</cell><cell>0.6491</cell><cell>0.7201</cell><cell>0.5406</cell><cell cols="2">0.6121</cell><cell>0.7352</cell><cell>0.1995</cell><cell>0.2724</cell><cell cols="2">0.3737</cell></row><row><cell cols="2">GLoRIA [25]</cell><cell>0.6555</cell><cell>0.6907</cell><cell>0.7328</cell><cell>0.5673</cell><cell cols="2">0.5778</cell><cell>0.7694</cell><cell>0.1889</cell><cell>0.2809</cell><cell cols="2">0.3869</cell></row><row><cell>BioViL [6]</cell><cell></cell><cell>0.6824</cell><cell>0.7038</cell><cell>0.7249</cell><cell>0.6267</cell><cell cols="2">0.6998</cell><cell>0.7849</cell><cell>0.2113</cell><cell>0.3239</cell><cell cols="2">0.4162</cell></row><row><cell>Ours</cell><cell></cell><cell>0.7064</cell><cell>0.7162</cell><cell>0.7579</cell><cell>0.6659</cell><cell cols="2">0.7210</cell><cell>0.7937</cell><cell>0.2445</cell><cell>0.3539</cell><cell cols="2">0.4399</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of Dice scores with other state-of-the-art methods on fine-tuning segmentation tasks. Three diseases are reported, and for each disease, three data portions, 1%, 10%, 100% are adopted to show the performance change under different data amounts.</figDesc><table><row><cell>Methods</cell><cell>AUC?</cell><cell>0 F1?</cell><cell>ACC? AUC?</cell><cell>1 F1?</cell><cell>ACC? AUC?</cell><cell>2 F1?</cell><cell>ACC? AUC?</cell><cell>3 F1?</cell><cell>ACC? AUC?</cell><cell>AVG F1?</cell><cell>ACC?</cell></row><row><cell>Scratch</cell><cell cols="11">0.7631 0.7036 0.6738 0.5383 0.3593 0.3223 0.6692 0.4328 0.7012 0.8420 0.5694 0.8770 0.7031 0.5163 0.6436</cell></row><row><cell cols="12">ConVIRT [66] 0.8453 0.7769 0.7793 0.6099 0.3938 0.4629 0.7202 0.4843 0.6445 0.9047 0.6154 0.8809 0.7700 0.5676 0.6919</cell></row><row><cell cols="12">GLoRIA [25] 0.8304 0.7577 0.7520 0.6208 0.3991 0.4922 0.7339 0.4958 0.7037 0.9246 0.6667 0.9102 0.7774 0.5798 0.7145</cell></row><row><cell>BioViL [6]</cell><cell cols="11">0.8034 0.7378 0.7148 0.6035 0.3912 0.4570 0.6860 0.4497 0.6777 0.9229 0.6500 0.9160 0.7540 0.5572 0.6914</cell></row><row><cell>Ours</cell><cell cols="11">0.8502 0.7646 0.7539 0.6641 0.4140 0.5392 0.7605 0.5266 0.7031 0.8845 0.6250 0.9160 0.7898 0.5826 0.7280</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparison with other state-of-the-art methods on fine-tuning edema severity grading multi-class classification task. AUC score is reported in the Table. "0,1,2,3" in the table represents the severity level and final average scores are reported.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>The Entity description used for translate single entity name. The description can be easily found from website. absence of diseases and infirmity, indicating the structure is normal. clear The lungs are clear and normal. No evidence for other diseases on lung. sharp This means that an anatomical structure s boundary or edge is clear and normal, meaning it is free of diseases. sharply "Sharply seen means that an anatomical structure is clearly visible. unremarkable This represents some anatomical structures are normal, usually modifying cardiac and mediastinal silhouettes. intact The bonny structure is complete and normal, meaning no fractures. stable The modified anatomical structures are normal and stable. No evidence for diseases. free It usually refers to free air and is associate with pneumothorax, atelectasis, pneumoperitoneum and emphysema. effusion A pleural effusion is accumulation of excessive fluid in the pleural space, the potential space that surrounds each lung. A pleural effusion infiltrates the space between the visceral pleura and the parietal pleura. opacity It is defined as an area of hazy opacification due to air displacement by fluid, airway collapse, fibrosis, or a neoplastic process. It is causes include infections, interstitial lung disease, and pulmonary edema. pneumothorax A pneumothorax is an abnormal collection of air in the pleural space between the lung and the chest wall. It may be caused by pneumonia or fibrosis and other diseases. edema Pulmonary edema, also known as pulmonary congestion, is excessive liquid accumulation in the tissue and air spaces of the lungs. It will show fluid in the alveolar walls. atelectasis It is the collapse or closure of a lung resulting in reduced or absent gas exchange. Findings can include lung opacification and loss of lung volume. tube It is a surgical drain that is inserted through the chest wall and into the pleural space or the mediastinum to remove undesired substances such as air. consolidation It is a region of normally compressible lung tissue that has filled with liquid instead of air. Consolidation must be present to diagnose pneumonia: the signs of lobar pneumonia are characteristic and clinically referred to as consolidation. process Acute process means there is abnormality in the anotomy structure. abnormality It means the exist of diseases and infirmity, indicating the structure is abnormal. enlarge It usually modifies cardiac silhouette and heart. Cardiomegaly is a medical condition in which the heart is enlarged. tip It refers to the top head of the tube. low The presence of low lung volumes may be a sign of a restrictive lung condition such as pulmonary fibrosis or sarcoidosis. pneumonia Pneumonia is an inflammatory condition of the lung primarily small air sacs known as alveoli. Pneumonia may present with opacities. Complications such as pleural effusion may also be found increasing the diagnostic accuracy of lung consolidation and pleural effusion line It refers to venous access line ot PICC lines. congestion Pulmonary congestion is defined as accumulation of fluid in the lungs, resulting in impaired gas exchange and arterial hypoxemia. catheter catheter is a tube placed in the body to drain and collect urine from the bladder cardiomegaly Cardiomegaly (sometimes megacardia or megalocardia) is a medical condition in which the heart is enlarged. or pulmonary nodule is a relatively small focal density in the lung. it may be confused with the projection of a structure of the chest wall or skin, such as a nipple, a healing rib fracture or lung cancer. wire Sternotomis wires means the center line of the chest. fluid It refers to the water of liquid in the lung and it may indicate edema and other diseases. degenerative Degenerative disease is the result of a continuous process based on degenerative cell changes pacemaker pacemaker device usually represents the one kind of medical equipments. an increase in the bulkiness of one or both of the pulmonary pleurae. It may cause by pulmonary Infection, empyema, tuberculosis or lung cancer. marking It represents interstitial markings or bronchovascular markings scar A scar (or scar tissue) is an area of fibrous tissue that replaces normal tissues after an injury. hyperinflate Hyperinflated lungs are larger-than-normal lungs as a result of trapped air. blunt Blunting of the costophrenic angles is usually caused by a pleural effusion, as already discussed. Other causes of costophrenic angle blunting include lung disease in the region of the costophrenic angle, and lung hyperexpansion. loss The etiology of lung volume loss can be listed as follow: airway obstruction or compression, obesity, scoliosis, restrictive diseases such as pulmonary fibrosis and interstitial lung disease, tuberculosis. widen The mediastinum is not widened or enlarged. collapse Collapse lung refers to pneumothorax or atelectasis. density The density (more precisely, the volumetric mass density; also known as specific mass), of a substance is its mass per unit volume. emphysema Emphysema, or pulmonary emphysema, is a lower respiratory tract disease, characterized by air-filled spaces (pneumatosis) in the lungs, that can vary in size and may be very large. aerate Aeration (also called aerification or aeriation) is the process by which air is circulated through, mixed with or dissolved in a liquid or other substances that act as a fluid (such as soil). mass A lung mass is an abnormal growth or area in the lungs and it can also view as lung cancer. crowd Crowding of the bronchovascular structures is an important direct sign of volume loss. The atelectatic lung enhances densely after contrast administration because of closeness of the pulmonary arteries and arterioles within the collapsed lobe. infiltrate A pulmonary infiltrate is a substance denser than air, such as pus, blood, or protein, which lingers within the parenchyma of the lungs. Pulmonary infiltrates are associated with pneumonia, tuberculosis and sarcoidosis. obscure Some anatomy structures are not clear and is difficult to understand or see. deformity It means some body parts are abnormal or unjuried. hernia Lung hernia (Sibson hernia) is a protrusion of lung outside of thoracic wall. the hernia is noted after chest trauma, thoracic surgery or certain pulmonary diseases. drainage Tube drainage represents the one kind of medical equipment. distention Distension generally refers to an enlargement, dilation, or ballooning effect. It may refer to: Abdominal distension. shift The mediastinal shift is the deviation of the mediastinal structures towards one side of the chest cavity, usually seen on chest radiograph. It indicates a severe asymmetry of intrathoracic pressures. stent Tracheal stent represents the one kind of medical equipments pressure Pulmonary venous pressure is intermediate between mean PAP and LAP over all physiologic pressures lesion Lung nodules, pulmonary nodules, white spots, lesions-these terms all describe the same phenomenon: an abnormality in the lungs. finding Some observation on body parts, usually indicating abnormalty. borderline Borderline size of the cardiac silhouette means the cardiac silhouette is not enlarged and normal. hardware It represents the one kind of medical equipments. dilation The state of being larger or more open than normal chfHeart failure -sometimes known as congestive heart failure -occurs when the heart muscle doesn't pump blood as well as it should. When this happens, blood often backs up and fluid can build up in the lungs, causing shortness of breath. redistributionIf the pulmonary edema is due to heart failure or fluid overload, you may also see cardiomegaly and distension of the pulmonary veins, particularly in the upper lung fields. aspiration Aspiration pneumonia occurs when food or liquid is breathed into the airways or lungs, instead of being swallowed.</figDesc><table><row><cell>Entity</cell><cell>Description</cell></row><row><cell>normal</cell><cell>It means the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Ablation study on zero-shot classification task. AUC, F1 and ACC scores are reported. For ChestX-ray 14, the metrics all refer to the macro average on the 14 diseases.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell></cell><cell cols="2">RSNA Pneumonia</cell><cell cols="5">SIIM-ACR Pneumothorax</cell><cell>ChestX-ray14</cell></row><row><cell>Methods</cell><cell></cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell><cell>AUC?</cell><cell></cell><cell></cell><cell>F1?</cell><cell>ACC?</cell><cell>AUC?</cell><cell>F1?</cell><cell>ACC?</cell></row><row><cell cols="2">w/o (PosCL + ET)</cell><cell>0.8532</cell><cell>0.6079</cell><cell>0.7669</cell><cell cols="2">0.8768</cell><cell cols="2">0.6672</cell><cell>0.8187</cell><cell>0.7502</cell><cell>0.2374</cell><cell>0.8541</cell></row><row><cell>w/o (ET)</cell><cell></cell><cell>0.8537</cell><cell>0.6241</cell><cell>0.8146</cell><cell cols="2">0.9017</cell><cell cols="2">0.7008</cell><cell>0.8584</cell><cell>0.7621</cell><cell>0.2452</cell><cell>0.8606</cell></row><row><cell>Ours</cell><cell></cell><cell>0.8694</cell><cell>0.6342</cell><cell>0.8002</cell><cell cols="2">0.8924</cell><cell cols="2">0.6833</cell><cell>0.8428</cell><cell>0.7676</cell><cell>0.2525</cell><cell>0.8619</cell></row><row><cell>Methods</cell><cell cols="4">Pointing Game? Recall? Precision?</cell><cell>IoU?</cell><cell>Dice?</cell><cell></cell><cell></cell><cell>Methods</cell><cell>Pointing Game? Recall? Precision?</cell></row><row><cell>w/o (PosCL + ET)</cell><cell></cell><cell>0.7979</cell><cell>0.8961</cell><cell>0.4036</cell><cell cols="3">0.2783 0.4230</cell><cell cols="2">w/o (PosCL + ET)</cell><cell>0.1786</cell><cell>0.3151</cell><cell>0.1336</cell></row><row><cell>w/o (ET)</cell><cell></cell><cell>0.8424</cell><cell>0.8226</cell><cell>0.6520</cell><cell cols="3">0.3118 0.4610</cell><cell></cell><cell>w/o (ET)</cell><cell>0.2080</cell><cell>0.3178</cell><cell>0.1711</cell></row><row><cell>Ours</cell><cell></cell><cell>0.8721</cell><cell>0.8661</cell><cell>0.6420</cell><cell cols="3">0.3172 0.4649</cell><cell></cell><cell>Ours</cell><cell>0.1975</cell><cell>0.3562</cell><cell>0.1940</cell></row><row><cell cols="5">(a) Zero-shot grounding on Pneumonia</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed results on ChestX-ray14</head><p>We further show the detailed performance of 14 different diseases on ChestX-ray14 dataset. Tab. 11 shows the results on the zero-shot setting. Our method can exceed the former methods for most diseases. The radar Fig. <ref type="figure">3Y</ref> shows more visually how our model compares with other solutions under the zero-shot setting. Our method can exceed the former methods for most diseases. Under 100% fine-tuning settings, we achieved similarly excellent results as shown in Tab. 12.  AUC scores are reported and, as shown, our method exceeds the previous state-of-the-art on most diseases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Society for imaging informatics in medicine: Siim-acr pneumothorax segmentation</title>
		<ptr target="https://www.kaggle.com/c/siim-acr-pneumothorax-segmentation" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Publicly available clinical BERT embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">R</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew Ba</forename><surname>Wa Redmond</surname></persName>
		</author>
		<author>
			<persName><surname>Mcdermott</surname></persName>
		</author>
		<title level="m">Publicly available clinical bert embeddings. NAACL HLT 2019</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">72</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Attanasio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Pisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Terragni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Sarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sri</forename><surname>Lakshmi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.08688</idno>
		<title level="m">Contrastive language-image pre-training for the italian language</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The unified medical language system (umls): integrating biomedical terminology</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bodenreider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>suppl 1):D267-D270</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Making the most of text semantics to improve biomedical vision-language processing</title>
		<author>
			<persName><forename type="first">Benedikt</forename><surname>Boecking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruthi</forename><surname>Bannur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Wetscherek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><surname>Alvarez-Valle</surname></persName>
		</author>
		<ptr target="https://github.com/microsoft/hi-ml/tree/main/hi-ml-multimodal" />
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Official Implementation</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint modeling of chest radiographs and radiology reports for pulmonary edema assessment</title>
		<author>
			<persName><forename type="first">Geeticka</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Horng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Golland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic scoring of multiple semantic attributes with multi-task feature leverage: a study on pulmonary nodules in ct images</title>
		<author>
			<persName><forename type="first">Sihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baiying</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianfu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie-Zhi</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="802" to="814" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Align, reason and learn: Enhancing medical vision-and-language pretraining with knowledge</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia</title>
		<meeting>the 30th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generating radiology reports via memory-driven transformer</title>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1439" to="1449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collaborative learning of cross-channel clinical attention for radiotherapy-related esophageal fistula prediction from ct</title>
		<author>
			<persName><forename type="first">Hui</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Duh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enhancing visionand-language semantic alignments via cross-and intra-modal knowledge integration</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Rosita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improving the factual correctness of radiology report generation with semantic rewards</title>
		<author>
			<persName><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bluethgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Almusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.12186</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Shivang</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Baghal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thidathip</forename><surname>Wongsurawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piroon</forename><surname>Jenjaroenpun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaymaa</forename><surname>Al-Shukri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Farmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Blake</surname></persName>
		</author>
		<title level="m">Chest imaging representing a covid-19 positive rural us population. Scientific data</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Resunet-a: A deep learning framework for semantic segmentation of remotely sensed data</title>
		<author>
			<persName><surname>Foivos I Diakogiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Franc ?ois Waldner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Caccetta</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="94" to="114" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-modal data programming enables rapid medical machine learning</title>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">A</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishith</forename><surname>Khandwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hersh</forename><surname>Sagreiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Lee-Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel L Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Patterns</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">100019</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine learning for medical imaging</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Bradley J Erickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynettin</forename><surname>Korfiatis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">L</forename><surname>Akkus</surname></persName>
		</author>
		<author>
			<persName><surname>Kline</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">505</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">Andre</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">A</forename><surname>Novoa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention to lesion: Lesionaware convolutional neural network for retinal optical coherence tomography image classification</title>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Rabbani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><surname>Ary L Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plamen</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Ch Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">B</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Kang</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H Eugene</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="e220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Incorporating the knowledge of dermatologists to convolutional neural networks for skin lesion diagnosis</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Gonzalez-Diaz</surname></persName>
		</author>
		<author>
			<persName><surname>Dermaknet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE journal of biomedical and health informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="547" to="559" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gloria: A multimodal global-local representation learning framework for label-efficient medical image recognition</title>
		<author>
			<persName><forename type="first">Shih-Cheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><surname>Yeung</surname></persName>
		</author>
		<ptr target="https://github.com/marshuang80/gloria" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3942" to="3951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dual-ray net: automatic diagnosis of thoracic diseases using frontal and lateral chest x-rays</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengqi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilu</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Medical Imaging and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="348" to="355" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Risk stratification of lung nodules using 3d cnn-based multitask learning</title>
		<author>
			<persName><forename type="first">Sarfaraz</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulas</forename><surname>Bagci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviana</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behzad</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Shpanskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="590" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Radgraph: Extracting clinical entities and relations from radiology reports</title>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriel</forename><surname>Saporta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Chambon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Matthew P Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mimic-cxr database</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><surname>Horng</surname></persName>
		</author>
		<idno>13026:C2JT1</idno>
	</analytic>
	<monogr>
		<title level="j">PhysioNet</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12086</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9694" to="9705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention based glaucoma detection: a large-scale database and cnn model</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanruo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10571" to="10580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Da Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">Visualbert: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Canet: cross-disease attention network for joint diabetic retinopathy and diabetic macular edema grading</title>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lequan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pheng-Ann</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1483" to="1493" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXX 16</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-task deep convolutional neural network for cancer diagnosis</title>
		<author>
			<persName><forename type="first">Qing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoe</forename><forename type="middle">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page" from="66" to="73" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exploring and distilling posterior and prior knowledge for radiology report generation</title>
		<author>
			<persName><forename type="first">Fenglin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexian</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="13753" to="13762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Training medical image analysis systems like radiologists</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Gabriel Maicas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacinto</forename><forename type="middle">C</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="546" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Chexpert++: Approximating the chexpert labeler for speed, differentiability, and probabilistic output</title>
		<author>
			<persName><forename type="first">Tzu</forename><surname>Matthew Ba Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><forename type="middle">Harry</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="913" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Embedding human knowledge into deep neural network via attention map</title>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Mitsuhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Sakashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Ogata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsubasa</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Fujiyoshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03540</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Improving factual completeness and consistency of image-to-text radiology report generation</title>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5288" to="5304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Joint learning of localized representations from medical images and reports</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Kaissis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congyu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>R?ckert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.02889</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Covid-net cxr-2: An enhanced deep convolutional neural network design for detection of covid-19 cases from chest x-ray images</title>
		<author>
			<persName><forename type="first">Maya</forename><surname>Pavlova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Terhljan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Audrey</forename><forename type="middle">G</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Surana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hossein</forename><surname>Aboutalebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hayden</forename><surname>Gun-Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amer</forename><surname>Alaref</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Medicine</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Negbio: a highperformance tool for negation and uncertainty detection in radiology reports</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AMIA Summits on Translational Science Proceedings</title>
		<imprint>
			<biblScope unit="page">188</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia</title>
		<author>
			<persName><forename type="first">George</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carol</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Safwan</forename><forename type="middle">S</forename><surname>Halabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">D</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><forename type="middle">M</forename><surname>Prevedello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tessa</forename><forename type="middle">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><forename type="middle">K</forename><surname>Amorosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronica</forename><surname>Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Galperin-Aizenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology. Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Combining automatic labelers and expert annotations for accurate radiology report labeling using bert</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Smit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saahil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Pareek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lungren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1500" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Expert knowledge-infused deep learning for automatic lung nodule detection</title>
		<author>
			<persName><forename type="first">Jiaxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumei</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengrong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of X-ray Science and Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deep learning segmentation model for automated detection of the opacity regions in the chest x-rays of the covid-19 positive patients and the application for disease severity</title>
		<author>
			<persName><forename type="first">Haiming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanfei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">medRxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Automated deep-neural-network surveillance of cranial images for acute neurologic events</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Joseph J Titano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javin</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Schefflein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andres</forename><surname>Pain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Swinburne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Bederson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature medicine</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1337" to="1341" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Expert-level detection of pathologies from unannotated chest x-ray images via selfsupervised learning</title>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Tiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellie</forename><surname>Talius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pujan</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to recognize thoracic disease in chest x-rays with knowledge-guided deep zoom neural networks</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luwen</forename><surname>Huangfu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="159790" to="159805" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Chestxray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A medical semantic-assisted transformer for radiographic report generation</title>
		<author>
			<persName><forename type="first">Zhanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingkang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luping</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2022: 25th International Conference</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">September 18-22, 2022. 2022</date>
			<biblScope unit="page" from="655" to="664" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Medclip</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.10163</idno>
		<title level="m">Contrastive learning from unpaired medical images and text</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Chest imagenome dataset for clinical reasoning</title>
		<author>
			<persName><forename type="first">Nkechinyere</forename><surname>Joy T Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ismini</forename><surname>Nneka Agu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Lourentzou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasper</forename><surname>Alexander Paguio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Christopher Dee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">G</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satyananda</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Giovannini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey on incorporating domain knowledge into deep learning for medical image analysis</title>
		<author>
			<persName><forename type="first">Xiaozheng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengsu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaojie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">101985</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Knowledge-based collaborative deep learning for benign-malignant lung nodule classification on chest ct</title>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dagan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fulham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="991" to="1004" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Knowledge matters: Chest radiology report generation with general and specific knowledge</title>
		<author>
			<persName><forename type="first">Shuxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">102510</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Qianqian Du, Guohua Shi, and Muhammad Bilal Zia. Dscgans: Integrate domain knowledge in training dual-path semi-supervised conditional generative adversarial networks and s3vm for ultrasonography thyroid nodules classification</title>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanjuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyun</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="558" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Ernie-vil: Knowledge enhanced visionlanguage representations through scene graphs</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3208" to="3216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Anatomy-guided weaklysupervised abnormality localization in chest x-rays</title>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantanu</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhexiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Deible</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayhan</forename><surname>Batmanghelich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="658" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Contrastive learning of medical visual representations from paired images and text</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<ptr target="https://github.com/edreisMD/ConVIRT-pytorch" />
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare, 2022. Highest Starred Implementation</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
