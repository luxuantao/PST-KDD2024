<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON LEARNING UNIVERSAL REPRESENTATIONS ACROSS LANGUAGES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
							<email>weixiangpeng@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
							<email>wengrx@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Machine Intelligence Technology Lab</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<email>huyue@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luxi</forename><surname>Xing</surname></persName>
							<email>xingluxi@iie.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Engineering</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Cyber Security</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Machine Intelligence Technology Lab</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
							<email>weihua.luowh@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Machine Intelligence Technology Lab</orgName>
								<orgName type="institution">Alibaba Group</orgName>
								<address>
									<settlement>Hangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ON LEARNING UNIVERSAL REPRESENTATIONS ACROSS LANGUAGES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have demonstrated the overwhelming advantage of cross-lingual pre-trained models (PTMs), such as multilingual BERT and XLM, on crosslingual NLP tasks. However, existing approaches essentially capture the cooccurrence among tokens through involving the masked language model (MLM) objective with token-level cross entropy. In this work, we extend these approaches to learn sentence-level representations and show the effectiveness on crosslingual understanding and generation. Specifically, we propose a Hierarchical Contrastive Learning (HICTL) method to (1) learn universal representations for parallel sentences distributed in one or multiple languages and (2) distinguish the semantically-related words from a shared cross-lingual vocabulary for each sentence. We conduct evaluations on two challenging cross-lingual tasks, XTREME and machine translation. Experimental results show that the HICTL outperforms the state-of-the-art XLM-R by an absolute gain of 4.2% accuracy on the XTREME benchmark as well as achieves substantial improvements on both of the highresource and low-resource English→X translation tasks over strong baselines. * Work done at Alibaba Group. Yue Hu and Heng Yu are the co-corresponding authors. We also made an official submission to XTREME (https://sites.research.google/xtreme), with several improved techniques used in <ref type="bibr" target="#b15">(Fang et al., 2020;</ref><ref type="bibr" target="#b34">Luo et al., 2020)</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Pre-trained models (PTMs) like ELMo <ref type="bibr" target="#b40">(Peters et al., 2018)</ref>, GPT <ref type="bibr" target="#b41">(Radford et al., 2018)</ref> and BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> have shown remarkable success of effectively transferring knowledge learned from large-scale unlabeled data to downstream NLP tasks, such as text classification <ref type="bibr" target="#b44">(Socher et al., 2013)</ref> and natural language inference <ref type="bibr" target="#b2">(Bowman et al., 2015;</ref><ref type="bibr" target="#b51">Williams et al., 2018)</ref>, with limited or no training data. To extend such pretraining-finetuning paradigm to multiple languages, some endeavors such as multilingual BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and XLM <ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref> have been made for learning cross-lingual representation. More recently, <ref type="bibr" target="#b10">Conneau et al. (2020)</ref> present XLM-R to study the effects of training unsupervised cross-lingual representations at a huge scale and demonstrate promising progress on cross-lingual tasks. However, all of these studies only perform a masked language model (MLM) with token-level (i.e., subword) cross entropy, which limits PTMs to capture the co-occurrence among tokens and consequently fail to understand the whole sentence. It leads to two major shortcomings for current cross-lingual PTMs, i.e., the acquisition of sentence-level representations and semantic alignments among parallel sentences in different languages. Considering the former, <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> introduced the next sentence prediction (NSP) task to distinguish whether two input sentences are continuous segments from the training corpus. However, this simple binary classification task is not enough to model sentence-level representations <ref type="bibr" target="#b24">(Joshi et al., 2020;</ref><ref type="bibr" target="#b54">Yang et al., 2019;</ref><ref type="bibr" target="#b32">Liu et al., 2019;</ref><ref type="bibr" target="#b28">Lan et al., 2020;</ref><ref type="bibr" target="#b10">Conneau et al., 2020)</ref>. For the latter, <ref type="bibr" target="#b22">(Huang et al., 2019)</ref> defined the cross-lingual paraphrase classification task, which concatenates two sentences from different languages as input and classifies whether they are with the same meaning. This task learns patterns of sentence-pairs well but fails to distinguish the exact meaning of each sentence.</p><p>In response to these problems, we propose to strengthen PTMs through learning universal representations among semantically-equivalent sentences distributed in different languages. We introduce a novel Hierarchical Contrastive Learning (HICTL) framework to learn language invariant sentence representations via self-supervised non-parametric instance discrimination. Specifically, we use a BERT-style model to encode two sentences separately, and the representation of the first token (e.g., <ref type="bibr">[CLS]</ref> in BERT) will be treated as the sentence representation. Then, we conduct instance-wise comparison at both sentence-level and word-level, which are complementary to each other. At the sentence level, we maximize the similarity between two parallel sentences while minimizing which among non-parallel ones. At the word-level, we maintain a bag-of-words for each sentence-pair, each word in which is considered as a positive sample while the rest words in vocabulary are negative ones. To reduce the space of negative samples, we conduct negative sampling for word-level contrastive learning. With the HICTL framework, the PTMs are encouraged to learn language-agnostic representation, thereby bridging the semantic discrepancy among cross-lingual sentences.</p><p>The HICTL is conducted on the basis of XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> and experiments are performed on several challenging cross-lingual tasks: language understanding tasks (e.g., XNLI, XQuAD, and MLQA) in the XTREME <ref type="bibr" target="#b21">(Hu et al., 2020)</ref> benchmark, and machine translation in the IWSLT and WMT benchmarks. Extensive empirical evidence demonstrates that our approach can achieve consistent improvements over baselines on various tasks of both cross-lingual language understanding and generation. In more detail, our HICTL obtains absolute gains of 4.2% (up to 6.0% on zero-shot sentence retrieval tasks, e.g. BUCC and Tatoeba) accuracy on XTREME over XLM-R. For machine translation, our HICTL achieves substantial improvements over baselines on both low-resource (IWSLT English→X) and high-resource (WMT English→X) translation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Pre-trained Language Models. Recently, substantial work has shown that pre-trained models (PTMs) <ref type="bibr" target="#b40">(Peters et al., 2018;</ref><ref type="bibr" target="#b41">Radford et al., 2018;</ref><ref type="bibr" target="#b11">Devlin et al., 2019)</ref> on the large corpus are beneficial for downstream NLP tasks. The application scheme is to fine-tune the pre-trained model using the limited labeled data of specific target tasks. For cross-lingual pre-training, both <ref type="bibr" target="#b11">Devlin et al. (2019)</ref> and <ref type="bibr" target="#b8">Conneau &amp; Lample (2019)</ref> trained a transformer-based model on multilingual Wikipedia which covers various languages, while XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> studied the effects of training unsupervised cross-lingual representations on a very large scale.</p><p>For sequence-to-sequence pre-training, UniLM <ref type="bibr" target="#b12">(Dong et al., 2019)</ref> fine-tuned BERT with an ensemble of masks, which employs a shared Transformer network and utilizing specific self-attention mask to control what context the prediction conditions on. <ref type="bibr" target="#b45">Song et al. (2019)</ref> extended BERT-style models by jointly training the encoder-decoder framework. XLNet <ref type="bibr" target="#b54">(Yang et al., 2019)</ref> trained by predicting masked tokens auto-regressively in a permuted order, which allows predictions to condition on both left and right context. <ref type="bibr" target="#b42">Raffel et al. (2019)</ref> unified every NLP problem as a text-to-text problem and pre-trained a denoising sequence-to-sequence model at scale. Concurrently, BART <ref type="bibr" target="#b29">(Lewis et al., 2020)</ref> pre-trained a denoising sequence-to-sequence model, in which spans are masked from the input but the complete output is auto-regressively predicted.</p><p>Previous works have explored using pre-trained models to improve text generation, such as pretraining both the encoder and decoder on several languages <ref type="bibr" target="#b45">(Song et al., 2019;</ref><ref type="bibr" target="#b8">Conneau &amp; Lample, 2019;</ref><ref type="bibr" target="#b42">Raffel et al., 2019)</ref> or using pre-trained models to initialize encoders <ref type="bibr" target="#b13">(Edunov et al., 2019;</ref><ref type="bibr" target="#b56">Zhang et al., 2019a;</ref><ref type="bibr" target="#b17">Guo et al., 2020)</ref>. <ref type="bibr" target="#b61">Zhu et al. (2020)</ref> and <ref type="bibr" target="#b49">Weng et al. (2020)</ref> proposed a BERTfused NMT model, in which the representations from BERT are treated as context and fed into all layers of both the encoder and decoder. <ref type="bibr" target="#b60">Zhong et al. (2020)</ref> formulated the extractive summarization task as a semantic text matching problem and proposed a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary, which leverages the pre-trained BERT in a Siamese network structure. Our approach also belongs to the contextual pre-training so it could be applied to various downstream NLU and NLG tasks.</p><p>Contrastive Learning. Contrastive learning (CTL) <ref type="bibr" target="#b43">(Saunshi et al., 2019)</ref> aims at maximizing the similarity between the encoded query q and its matched key k + while keeping randomly sampled </p><formula xml:id="formula_0">XLM-R ℳ(•) XLM-R ℳ(•)</formula><formula xml:id="formula_1">L ctl = − log exp(s(q, k + )) exp(s(q, k + )) + i exp(s(q, k − i )) ,<label>(1)</label></formula><p>where the score function s(q, k) is essentially implemented as the cosine similarity q T k q • k . q and k are often encoded by a learnable neural encoder, such as BERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> or ResNet <ref type="bibr" target="#b19">(He et al., 2016)</ref>. k + and k − are typically called positive and negative samples. In addition to the form illustrated in Eq. (1), contrastive losses can also be based on other forms, such as margin-based loses <ref type="bibr" target="#b18">(Hadsell et al., 2006)</ref> and variants of NCE losses <ref type="bibr" target="#b36">(Mnih &amp; Kavukcuoglu, 2013)</ref>.</p><p>Contrastive learning is at the core of several recent work on unsupervised or self-supervised learning from computer vision <ref type="bibr" target="#b52">(Wu et al., 2018;</ref><ref type="bibr" target="#b38">Oord et al., 2018;</ref><ref type="bibr" target="#b55">Ye et al., 2019;</ref><ref type="bibr" target="#b20">He et al., 2019;</ref><ref type="bibr" target="#b4">Chen et al., 2020;</ref><ref type="bibr" target="#b46">Tian et al., 2020)</ref> to natural language processing <ref type="bibr" target="#b35">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b36">Mnih &amp; Kavukcuoglu, 2013;</ref><ref type="bibr" target="#b11">Devlin et al., 2019;</ref><ref type="bibr" target="#b7">Clark et al., 2020b;</ref><ref type="bibr" target="#b16">Feng et al., 2020;</ref><ref type="bibr" target="#b5">Chi et al., 2020)</ref>. <ref type="bibr" target="#b25">Kong et al. (2020)</ref> improved language representation learning by maximizing the mutual information between a masked sentence representation and local n-gram spans. <ref type="bibr" target="#b7">Clark et al. (2020b)</ref> utilized a discriminator to predict whether a token is replaced by a generator given its surrounding context. <ref type="bibr">Iter et al. (2020)</ref> proposed to pre-train language models with contrastive sentence objectives that predict the surrounding sentences given an anchor sentence. In this paper, we propose HICTL to encourage parallel cross-lingual sentences to have the identical semantic representation and distinguish whether a word is contained in them as well, which can naturally improve the capability of cross-lingual understanding and generation for PTMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">HIERARCHICAL CONTRASTIVE LEARNING</head><p>We propose hierarchical contrastive learning (HICTL), a novel comparison learning framework that unifies cross-lingual sentences as well as related words. HICTL can learn from both non-parallel and parallel multilingual data, and the overall architecture of HICTL is illustrated in Figure <ref type="figure">1</ref>. We represent a training batch of the original sentences as x = {x 1 , x 2 , ..., x n } and its aligned counterpart is denoted as y = {y 1 , y 2 , ..., y n }, where n is the batch size. For each pair x i , y i , y i is either the translation in the other language of x i when using parallel data or the perturbation through reordering tokens in x i when only monolingual data is available.x \i is denoted as a modified version of x where the i-th instance is removed.</p><p>Sentence-Level CTL. As illustrated in Figure <ref type="figure">1a</ref>, we apply the XLM-R as the encoder to represent sentences into hidden representations. The first token of every sequence is always a special token </p><formula xml:id="formula_2">+ = k + − q 2 )</formula><p>in the embedding space represents a manifold near in which sentences are semantically equivalent. We can generate a coherent sample (i.e., k− ) that interpolate between known pair q and k − . The synthetic negative k− can be controlled adaptively with proper difficulty during training. The curly brace in green indicates the walking range of hard negative samples, the closer to the circle the harder the sample is.</p><p>(e.g., [CLS]), and the final hidden state corresponding to this token is used as the aggregate sentence representation for pre-training, that is, r x = f • g(M(x)) where g(•) is the aggregate function and f (•) is a linear projection, • denotes the composition of operations. To obtain universal representation among semantically-equivalent sentences, we encourage r xi (the query, denoted as q) to be as similar as possible to r yi (the positive sample, denoted as k + ) but dissimilar to all other instances (i.e., y \i ∪ x \i , considered as a series of negative samples, denoted as {k</p><formula xml:id="formula_3">− 1 , k − 2 , ..., k − 2n−2 }) in a training batch.</formula><p>Formally, the sentence-level contrastive loss for x i is defined as</p><formula xml:id="formula_4">L sctl (x i ) = − log exp •s(q, k + ) exp •s(q, k + ) + |y \i ∪x \i | j=1 exp •s(q, k − j )</formula><p>.</p><p>(2)</p><p>Symmetrically, we also expect r yi (the query, denoted as q) to be as similar as possible to r xi (the positive sample, denoted as k+ ) but dissimilar to all other instances in the same training batch, thus,</p><formula xml:id="formula_5">L sctl (y i ) = − log exp •s(q, k+ ) exp •s(q, k+ ) + |y \i ∪x \i | j=1 exp •s(q, k− j )</formula><p>.</p><p>(3)</p><p>The sentence-level contrastive loss over the training batch can be formulated as</p><formula xml:id="formula_6">L S = 1 2n n i=1 L sctl (x i ) + L sctl (y i ) .<label>(4)</label></formula><p>For sentence-level contrastive learning, we treat other instances contained in the training batch as negative samples for the current instance. However, such randomly selected negative samples are often uninformative, which poses a challenge of distinguishing very similar but nonequivalent samples. To address this issue, we employ smoothed linear interpolation <ref type="bibr" target="#b3">(Bowman et al., 2016;</ref><ref type="bibr" target="#b59">Zheng et al., 2019)</ref> between sentences in the embedding space to alleviate the lack of informative samples for pre-training, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. Given a training batch { x i , y i } n i=1 , where n is the batch size. In this context, having obtained the embeddings of a triplet, an anchor q and a positive k + as well as a negative k − (supposing q, k + and k − are representations of sentences x i , y i and y − i ∈ x \i ∪ y \i , respectively), we construct a harder negative sample k− to replace k − j :</p><formula xml:id="formula_7">k− = q + λ(k − − q), λ ∈ ( d + d − , 1] if d − &gt; d + ; k − if d − ≤ d + . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>where d + = k + − q 2 and d − = k − − q 2 . For the first condition, the hardness of k− increases when λ becomes smaller. To this end, we intuitively set λ as</p><formula xml:id="formula_9">λ = d + d − ζ•p + avg , ζ ∈ (0, 1)<label>(6)</label></formula><p>where</p><formula xml:id="formula_10">p + avg = 1 100 ∈[−100,−1] e −L ()</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S</head><p>is the average log-probability over the last 100 training batches and L S formulated in Eq. ( <ref type="formula" target="#formula_6">4</ref>) is the sentence-level contrastive loss of one training batch. During pre-training, when the model tends to distinguish positive samples easily, which means negative samples are not informative already. At this time, p + avg ↑ and d + d − ↓, which leads λ ↓ and harder negative samples are adaptively synthesized in the following training steps, vice versa. As hard negative samples usually result in significant changes of the model parameters, we introduce the slack coefficient ζ to prevent the model from being trained in the wrong direction, when it accidentally switch from random negative samples to very hard ones. In practice, we empirically set ζ = 0.9.</p><p>Word-Level CTL. Intuitively, predicting the related words in other languages for each sentence can bridge the representations of words in different languages. As shown in Figure <ref type="figure">1b</ref>, we concatenate the sentence pair x i , y i as</p><formula xml:id="formula_11">x i • y i : [CLS] x i [SEP] y i [SEP]</formula><p>and the bag-of-words of which is denoted as B. For word-level contrastive learning, the final state of the first token is treated as the query (q), each word w t ∈ B is considered as the positive sample and all the other words (V\B, i.e., the words in V that are not in B where V indicates the overall vocabulary of all languages) are negative samples. As the vocabulary usually with large space, we propose to only use a subset S ⊂ V\B sampled according to the normalized similarities between q and the embeddings of the words. As a result, the subset S naturally contains the hard negative samples which are beneficial for learning high-quality representations <ref type="bibr" target="#b55">(Ye et al., 2019)</ref>. Specifically, the word-level contrastive loss for x i , y i is defined as</p><formula xml:id="formula_12">L wctl (x i , y i ) = − 1 |B| |B| t=1 log exp •s(q, e(w t )) exp •s(q, e(w t )) + wj ∈S exp •s(q, e(w j )) . (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>where e(•) is the embedding lookup function and |B| is the number of unique words in the concatenated sequence x i • y i . The overall word-level contrastive loss can be formulated as:</p><formula xml:id="formula_14">L W = 1 n n i=1 L wctl (x i , y i ).<label>(8)</label></formula><p>Multi-Task Pre-training. Both MLM and translation language model (TLM) are combined with HICTL by default, as the prior work <ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref> has verified the effectiveness of them in XLM. In summary, the model can be optimized by minimizing the entire training loss:</p><formula xml:id="formula_15">L = L LM + L S + L W ,<label>(9)</label></formula><p>where L LM is implemented as either the TLM when using parallel data or the MLM when only monolingual data is available to recover the original words of masked positions given the contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CROSS-LINGUAL FINE-TUNING</head><p>Language Understanding. The representations produced by HICTL can be used in several ways for language understanding tasks whether they involve single text or text pairs. Concretely, (i) the [CLS] representation of single-sentence in sentiment analysis or sentence pairs in paraphrasing and entailment is fed into an extra output-layer for classification. (ii) The pre-trained encoder can be used to assign POS tags to each word or to locate and classify all the named entities in the sentence for structured prediction, as well as (iii) to extract answer spans for question answering.</p><p>𝑥 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-trained Encoder</head><p>Randomly Initialized Decoder</p><formula xml:id="formula_16">𝑐𝑙𝑠 𝑥 0 𝑥 2 𝑠𝑒𝑝 𝑦 1 𝑏𝑜𝑠 𝑦 0 𝑦 2 𝑦 3 𝑦 1 𝑦 0 𝑦 2 𝑦 3 𝑒𝑜𝑠 𝑐𝑙𝑠 Figure 3: Fine-tuning on NMT task.</formula><p>Language Generation. We also explore using HICTL to improve machine translation. In the previous work, <ref type="bibr" target="#b8">Conneau &amp; Lample (2019)</ref> has shown that the pre-trained encoders can provide a better initialization of both supervised and unsupervised NMT systems. <ref type="bibr" target="#b33">Liu et al. (2020b)</ref> has shown that NMT models can be improved by incorporating pre-trained sequence-to-sequence models on various language pairs but highest-resource settings. As illustrated in Figure <ref type="figure">3</ref>, we use the model pre-trained by HICTL as the encoder, and add a new set of decoder parameters that are learned from scratch. To prevent pre-trained weights from being washed out by supervised training, we train the encoder-decoder model in two steps. In the first step, we freeze the pre-trained encoder and only update the decoder. In the second step, we train all parameters for a relatively small number of iterations. In both cases, we compute the similarities between the [CLS] representation of the encoder and all target words in advance. Then we aggregate them with the logits before the softmax of each decoder step through an element-wise additive operation. The encoder-decoder model is optimized by maximizing the log-likelihood of bitext at both steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We consider two evaluation benchmarks: nine cross-lingual language understanding tasks in the XTREME benchmark and machine translation tasks (IWSLT'14 English↔German, IWSLT'14 English→Spanish, WMT'16 Romanian→English, IWSLT'17 English→{French, Chinese} and WMT'14 English→{German, French}). In this section, we describe the data and training details, and provide detailed evaluation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATA AND MODEL</head><p>During pre-training, we follow <ref type="bibr" target="#b10">Conneau et al. (2020)</ref> to build a Common-Crawl Corpus using the CCNet <ref type="bibr" target="#b50">(Wenzek et al., 2019</ref>) tool<ref type="foot" target="#foot_0">1</ref> for monolingual texts. Table <ref type="table" target="#tab_7">7</ref> (see appendix A) reports the language codes and data size in our work. For parallel data, we use the same (English-to-X) MT dataset as <ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref>, which are collected from MultiUN <ref type="bibr" target="#b14">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type="bibr" target="#b26">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018 for Turkish, Vietnamese and Thai, the EUbookshop corpus for German, Greek and Bulgarian, Tanzil for both Urdu and Swahili, and GlobalVoices for Swahili. Table <ref type="table" target="#tab_8">8</ref> (see appendix A) shows the statistics of the parallel data.</p><p>We adopt the Transformer-Encoder <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> as the backbone with 12 layers and 768 hidden units for HICTL Base , and 24 layers and 1024 hidden units for HICTL. We initialize the parameters of HICTL with XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>. Hyperparameters for pre-training and fine-tuning are shown in Table <ref type="table">9</ref> (see appendix B). We run the pre-training experiments on 8 V100 GPUs, batch size 1024. The number of negative samples m=512 for word-level contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EXPERIMENTAL EVALUATION</head><p>Cross-lingual Language Understanding (XTREME) There are nine tasks in XTREME that can be grouped into four categories: (i) sentence classification consists of Cross-lingual Natural Language Inference (XNLI) <ref type="bibr" target="#b9">(Conneau et al., 2018)</ref> and Cross-lingual Paraphrase Adversaries from Word Scrambling (PAWS-X) <ref type="bibr" target="#b57">(Zhang et al., 2019b)</ref>. (ii) Structured prediction includes POS tagging and NER. We use POS tagging data from the Universal Dependencies v2.5 <ref type="bibr" target="#b37">(Nivre et al., 2018)</ref> treebanks. Each word is assigned one of 17 universal POS tags. For NER, we use the Wikiann dataset <ref type="bibr" target="#b39">(Pan et al., 2017)</ref>. (iii) Question answering includes three tasks: Cross-lingual Question Answering (XQuAD) <ref type="bibr">(Artetxe et al., 2019)</ref>, Multilingual Question Answering (MLQA) <ref type="bibr" target="#b30">(Lewis et al., 2019)</ref>, and the gold passage version of the Typologically Diverse Question Answering dataset (TyDiQA-GoldP) <ref type="bibr" target="#b6">(Clark et al., 2020a)</ref>. (iv) Sentence retrieval includes two tasks: BUCC <ref type="bibr" target="#b62">(Zweigenbaum et al., 2017)</ref> and Tatoeba <ref type="bibr" target="#b0">(Artetxe &amp; Schwenk, 2019)</ref>, which aims to extract parallel sentences between the English corpus and target languages. As XTREME provides no training data, thus we directly evaluate pre-trained models on test sets.</p><p>Table <ref type="table" target="#tab_1">1</ref> provides detailed results on four categories in XTREME. First, compared to the state of the art XLM-R baseline, HICTL further achieves significant gains of 1.43% and 2.80% on average on nine tasks with cross-lingual zero-shot transfer and translate-train-all settings, respectively. Second, mining hard negative samples via smoothed linear interpolation play an important role in contrastive learning, which significantly improves accuracy by 1.6 points on average. Third, HICTL with hardness aware augmentation delivers large improvements on zero-shot sentence retrieval tasks (scores 5.8 and 6.0 points higher on BUCC and Tatoeba, respectively). Following <ref type="bibr" target="#b21">(Hu et al., 2020)</ref>, we directly evaluate pre-trained models on test sets without any extra labeled data or fine-tuning techniques used in <ref type="bibr" target="#b15">(Fang et al., 2020;</ref><ref type="bibr" target="#b34">Luo et al., 2020)</ref>. These results demonstrate the capacity of HICTL on learning cross-lingual representations. We also compare our best model with two existing models: FILTER <ref type="bibr" target="#b15">(Fang et al., 2020)</ref> and VECO <ref type="bibr" target="#b34">(Luo et al., 2020)</ref>. The results demonstrate that HICTL achieves the best performance on most tasks with less monolingual data.</p><p>Ablation experiments are present at Table <ref type="table" target="#tab_3">3</ref>. Comparing the full model, we can draw several conclusions: (1) removing the sentence-level CTL objective hurts performance consistently and significantly, (2) the word-level CTL objective has least drop compared to others, and (3) the parallel (MT) data has a large impact on zero-shot multilingual sentence retrieval tasks. Moreover, Table <ref type="table" target="#tab_2">2</ref> provides the comparisons between HICTL and existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Machine Translation</head><p>The main idea of HICTL is to summarize cross-lingual parallel sentences into a shared representation that we term as semantic embedding, using which semantically related words can be distinguished from others. Thus it is natural to apply this global embedding to text generation. We fine-tune the pre-trained HICTL with the base setting on machine translation tasks with both low-resource and high-resource settings. For the low-resource scenario, we choose IWSLT'14 English↔German (En↔De)<ref type="foot" target="#foot_1">2</ref> , IWSLT'14 English→Spanish (En→Es), WMT'16 Romanian→English (Ro→En), IWSLT'17 English→French (En→Fr) and English→Chinese (En→Zh) translation<ref type="foot" target="#foot_2">3</ref> . There are 160k, 183k, 236k, 235k, 0.6M bilingual sentence pairs for En↔De, En→Es, En→Fr, En→Zh and Ro→En tasks. For the rich-resource scenario, we work on WMT'14 En→{De, Fr}, the corpus sizes are 4.5M and 36M respectively. We concatenate newstest 2012 and newstest 2013 as the validation set and use newstest 2014 as the test set.</p><p>During fine-tuning, we use the pre-trained model to initialize the encoder and introduce a randomly initialized decoder. We develop a shallower decoder with 4 identical layers to reduce the computation overhead. At the first fine-tune step, we concatenate the datasets of all language pairs in either low-resource or high-resource settings to optimize the decoder only until convergence<ref type="foot" target="#foot_3">4</ref> . Then we tune the whole encoder-decoder model using a per-language corpus at the second step. The initial learning rate is 2e-5 and inverse sqrt learning rate <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> scheduler is also adopted. For WMT'14 En→De, we use beam search with width 4 and length penalty 0.6 for inference. For other tasks, we use width 5 and a length penalty of 1.0. We use multi-bleu.perl to evaluate IWSLT'14 En↔De and WMT tasks, but sacreBLEU for the remaining tasks, for fair comparison with previous work.</p><p>Results on both high-resource and low-resource tasks are reported in Table <ref type="table" target="#tab_5">4 and Table 5</ref>, respectively. We implemented standard Transformer (apply the base and big setting for IWSLT and WMT tasks respectively) as baseline. The proposed HICTL can improve the BLEU scores of the eight tasks by <ref type="bibr">3.34, 2.95, 3.24, 3.45, 2.8, 6.37, 4.4, and 3.4</ref>. In addition, our approach also outperforms the BERT-fused model <ref type="bibr" target="#b53">(Yang et al., 2020)</ref>, a method treats BERT as an extra context We also evaluate our model on tasks where no bi-text is available for the target language pair. Following mBART <ref type="bibr" target="#b33">(Liu et al., 2020b)</ref>, we adopt the setting of language transfer. That is, no bi-text for the target pair is available, but there is bi-text for translating from some other language into the target language. For explanation, supposing there is no parallel data for the target language pair Italian→English (It→En), but we can transfer knowledge learned from Czech→English (Cs→En, a high-resource language pair) to It→En. We consider X→En translation, covering Indic languages (Ne, Hi, Si, Gu) and European languages (Ro, It, Cs, Nl). For European languages, we fine-tune on Cs→En translation, the parallel data is from WMT'19 that contains 11M sentence pairs. We test on {Cs, Ro, It, Nl}→En, in which test sets are from previous WMT (Cs, Ro) or IWSLT (It, Nl) competitions. For Indic languages, we fine-tune on Hi→En translation (1.56M sentence pairs are from IITB <ref type="bibr" target="#b27">(Kunchukuttan et al., 2018b</ref>)), and test on {Ro, It, Cs, Nl}→En translations.</p><p>Results are shown in Table <ref type="table" target="#tab_6">6</ref>. We can always obtain reasonable transferring scores at low-resource pairs over different fine-tuned models. However, our experience shows that the randomly initialized models without pre-training always achieve near 0 BLEU. The underlying scenario is that multilingual pre-training produces universal representations across languages so that once the model learns to translate one language, it learns to translate all languages with similar representations. Moreover, a failure happened in Gu→En translation, we conjecture that we only use 0.3GB monolingual data for pre-training, which is difficult to learn informative representations for Gujarati.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have demonstrated that pre-trained language models (PTMs) trained to learn commonsense knowledge from large-scale unlabeled data highly benefit from hierarchical contrastive learning (HICTL), both in terms of cross-lingual understanding and generation. Learning universal representations at both word-level and sentence-level bridges the semantic discrepancy across languages. As a result, our HICTL sets a new level of performance among cross-lingual PTMs, improving on the state of the art by a large margin.  <ref type="table" target="#tab_7">7</ref> reports the language codes and data size in our work. For parallel data, we use the same (English-to-X) MT dataset as <ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref>, which are collected from MultiUN <ref type="bibr" target="#b14">(Eisele &amp; Yu, 2010)</ref> for French, Spanish, Arabic and Chinese, the IIT Bombay corpus <ref type="bibr" target="#b26">(Kunchukuttan et al., 2018a)</ref> for Hindi, the OpenSubtitles 2018 for Turkish, Vietnamese and Thai, the EUbookshop corpus for German, Greek and Bulgarian, Tanzil for both Urdu and Swahili, and GlobalVoices for Swahili. Table <ref type="table" target="#tab_8">8</ref> shows the statistics of the parallel data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPERPARAMETERS FOR PRE-TRAINING AND FINE-TUNING</head><p>As shown in Table <ref type="table">9</ref>, we present the hyperparameters for pre-training HICTL. We use the same vocabulary as well as the sentence-piece model with XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>. During finetuning on XTREME, we search the learning rate over {5e-6, 1e-5, 1.5e-5, 2e-5, 2.5e-5, 3e-5} and batch size over {16, 32} for BASE-size models. And we select the best LARGE-size model by searching the learning rate over {3e-6, 5e-6, 1e-5} as well as batch size over {32, 64}. of HICTL on learning universal representations across different languages. Note that the t-SNE visualization of HICTL still demonstrates some noises, we attribute them to the lack of hard negative examples for sentence-level contrastive learning and leave this to future work for consideration.   We collect 10 sets of samples from WMT'14-19, each of them contains 100 parallel sentences distributed in 5 languages (i.e., English, French, German, Russian, and Spanish). Each set is identified by a color and different languages marked by different shapes. We can see that a set of sentences under the same meaning are clustered more densely for HICTL than XLM-R, which reveals the strong capability of HICTL on learning universal representations across different languages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Illustration of constructing hard negative samples (HNS). A circle (the radius is d + = k + − q 2 ) in the embedding space represents a manifold near in which sentences are semantically equivalent. We can generate a coherent sample (i.e., k− ) that interpolate between known pair q and k − . The synthetic negative k− can be controlled adaptively with proper difficulty during training. The curly brace in green indicates the walking range of hard negative samples, the closer to the circle the harder the sample is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Visualizations (t-SNE projection) of sentence embeddings output by HICTL (left) and XLM-R (right). We collect 10 sets of samples from WMT'14-19, each of them contains 100 parallel sentences distributed in 5 languages (i.e., English, French, German, Russian, and Spanish). Each set is identified by a color and different languages marked by different shapes. We can see that a set of sentences under the same meaning are clustered more densely for HICTL than XLM-R, which reveals the strong capability of HICTL on learning universal representations across different languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Illustration of Hierarchical Contrastive Learning (HICTL). n is the batch size, m denotes the number of negative samples for word-level contrastive learning. B and V indicates the bag-ofwords of the instance x i , y i and the overall vocabulary of all languages, respectively.</figDesc><table><row><cell cols="2">Contrastive loss</cell><cell></cell><cell></cell><cell>Contrastive loss</cell></row><row><cell>𝑠(𝑞, 𝑘)</cell><cell></cell><cell></cell><cell></cell><cell>𝑠(𝑞, 𝑘)</cell></row><row><cell>𝑞</cell><cell>{𝑘 + , 𝒌 𝟏 − , 𝒌 𝟐 − , ⋯ , 𝒌 𝟐𝒏−𝟐 −</cell><cell>}</cell><cell>𝑞</cell><cell cols="2">{𝑘 + , 𝒌 𝟏 − , 𝒌 𝟐 − , ⋯ , 𝒌 𝒎 − }</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Embedding Lookup</cell></row><row><cell></cell><cell></cell><cell></cell><cell>XLM-R</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>ℳ(•)</cell><cell cols="2">Negative Sampling</cell></row><row><cell>𝑥 𝑖</cell><cell>{𝑦 𝑖 , 𝐲 \𝑖 ∪ 𝐱 \𝑖 }</cell><cell></cell><cell>𝑥 𝑖 ∘ 𝑦 𝑖</cell><cell>𝑤 𝑡 ∈ ℬ</cell><cell>𝒱</cell></row><row><cell cols="2">(a) Sentence-Level CTL</cell><cell></cell><cell></cell><cell>(b) Word-Level CTL</cell></row><row><cell cols="6">Figure 1: keys {k − 0 , k − 1 , k − 2 , ...} faraway from it. With similarity measured by a score function s(q, k), a form</cell></row><row><cell cols="6">of a contrastive loss function, called InfoNCE (Oord et al., 2018), is considered in this paper:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overall results on XTREME benchmark. Results of mBERT<ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, XLM<ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref> and XLM-R<ref type="bibr" target="#b10">(Conneau et al., 2020)</ref> are from XTREME<ref type="bibr" target="#b21">(Hu et al., 2020)</ref>. Results of ‡ are from our in-house replication. HNS is short for "Hard Negative Samples".Translate-train-all (models are trained on English training data and its translated data on the target language)</figDesc><table><row><cell>Model</cell><cell cols="4">Pair sentence XNLI PAWS-X POS Structured prediction NER</cell><cell>XQuAD</cell><cell cols="4">Question answering MLQA TyDiQA-GoldP BUCC Tatoeba Sentence retrieval</cell><cell>Avg.</cell></row><row><cell>Metrics</cell><cell>Acc.</cell><cell>Acc.</cell><cell>F1</cell><cell>F1</cell><cell>F1 / EM</cell><cell>F1 / EM</cell><cell>F1 / EM</cell><cell>F1</cell><cell>Acc.</cell><cell></cell></row><row><cell cols="6">Cross-lingual zero-shot transfer (models are trained on English data)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBERT</cell><cell>65.4</cell><cell>81.9</cell><cell>70.3</cell><cell>62.2</cell><cell cols="2">64.5 / 49.4 61.4 / 44.2</cell><cell>59.7 / 43.9</cell><cell>56.7</cell><cell>38.7</cell><cell>59.6</cell></row><row><cell>XLM</cell><cell>69.1</cell><cell>80.9</cell><cell>70.1</cell><cell>61.2</cell><cell cols="2">59.8 / 44.3 48.5 / 32.6</cell><cell>43.6 / 29.1</cell><cell>56.8</cell><cell>32.6</cell><cell>55.5</cell></row><row><cell>XLM-RBase</cell><cell>76.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>63.7 / 46.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HICTLBase</cell><cell>77.3</cell><cell>84.5</cell><cell>71.4</cell><cell>64.1</cell><cell cols="2">73.5 / 58.7 65.8 / 47.6</cell><cell>61.9 / 42.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>XLM-R</cell><cell>79.2</cell><cell>86.4</cell><cell>73.8</cell><cell>65.4</cell><cell cols="2">76.6 / 60.8 71.6 / 53.2</cell><cell>65.1 / 45.0</cell><cell>66.0</cell><cell>57.3</cell><cell>68.2</cell></row><row><cell>HICTL</cell><cell>81.0</cell><cell>87.5</cell><cell>74.8</cell><cell>66.2</cell><cell cols="2">77.9 / 61.7 72.8 / 54.5</cell><cell>66.0 / 45.7</cell><cell>68.4</cell><cell>59.7</cell><cell>69.6</cell></row><row><cell>mBERT</cell><cell>75.1</cell><cell>88.9</cell><cell>-</cell><cell>-</cell><cell cols="2">72.4 / 58.3 67.6 / 49.8</cell><cell>64.2 / 49.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>XLM-R  ‡</cell><cell>82.9</cell><cell>90.1</cell><cell>74.6</cell><cell>66.8</cell><cell cols="2">80.4 / 65.6 72.4 / 54.7</cell><cell>66.2 / 48.2</cell><cell>67.9</cell><cell>59.1</cell><cell>70.6</cell></row><row><cell>HICTL</cell><cell>84.5</cell><cell>92.2</cell><cell>76.8</cell><cell>68.4</cell><cell cols="2">82.8 / 67.3 74.4 / 57.1</cell><cell>69.7 / 52.5</cell><cell>71.8</cell><cell>63.1</cell><cell>73.2</cell></row><row><cell>+ HNS</cell><cell>84.7</cell><cell>92.8</cell><cell>77.2</cell><cell>69.0</cell><cell cols="2">82.9 / 67.4 74.8 / 57.3</cell><cell>71.1 / 53.2</cell><cell>77.6</cell><cell>69.1</cell><cell>74.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison with existing methods on XTREME tasks.</figDesc><table><row><cell>Model</cell><cell cols="2">Pair sentence XNLI PAWS-X</cell><cell cols="2">Structured prediction POS NER</cell><cell>XQuAD</cell><cell cols="2">Question answering MLQA TyDiQA-GoldP</cell></row><row><cell>Metrics</cell><cell>Acc.</cell><cell>Acc.</cell><cell>F1</cell><cell>F1</cell><cell>F1 / EM</cell><cell>F1 / EM</cell><cell>F1 / EM</cell></row><row><cell cols="2">Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FILTER</cell><cell>83.9</cell><cell>91.4</cell><cell>76.2</cell><cell>67.7</cell><cell>82.4 / 68.0</cell><cell>76.2 / 57.7</cell><cell>68.3 / 50.9</cell></row><row><cell>VECO</cell><cell>83.0</cell><cell>91.1</cell><cell>75.1</cell><cell>65.7</cell><cell>79.9 / 66.3</cell><cell>73.1 / 54.9</cell><cell>75.0 / 58.9</cell></row><row><cell>HICTL</cell><cell>84.7</cell><cell>92.8</cell><cell>77.2</cell><cell>69.0</cell><cell>82.9 / 67.4</cell><cell>74.8 / 57.3</cell><cell>71.1 / 53.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on XTREME tasks.</figDesc><table><row><cell>Model</cell><cell cols="3">XNLI PAWS-X POS NER Acc. Acc. F1 F1</cell><cell>XQuAD F1 / EM</cell><cell>MLQA F1 / EM</cell><cell cols="3">TyDiQA-GoldP BUCC Tatoeba Avg. F1 / EM F1 Acc.</cell></row><row><cell>FULL MODEL</cell><cell>84.7</cell><cell>92.8</cell><cell cols="3">77.2 69.0 82.9 / 67.4 74.8 / 57.3</cell><cell>71.1 / 53.2</cell><cell>77.6</cell><cell>69.1</cell><cell>74.8</cell></row><row><cell>w/o Sentence-CTL</cell><cell>82.9</cell><cell>90.5</cell><cell cols="3">75.9 67.8 82.3 / 66.7 74.3 / 56.5</cell><cell>69.7 / 52.3</cell><cell>71.4</cell><cell>62.6</cell><cell>72.4</cell></row><row><cell>w/o Word-CTL</cell><cell>84.3</cell><cell>92.1</cell><cell cols="3">76.3 68.4 82.5 / 66.9 74.1 / 56.7</cell><cell>70.2 / 52.5</cell><cell>76.8</cell><cell>68.4</cell><cell>74.2</cell></row><row><cell>w/o MT data</cell><cell>84.2</cell><cell>92.4</cell><cell cols="3">76.6 68.2 82.6 / 67.0 74.5 / 56.8</cell><cell>70.1 / 52.3</cell><cell>74.7</cell><cell>66.8</cell><cell>73.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>BLEU scores [%] on high-resource tasks. Results with † and ‡ are from VECO<ref type="bibr" target="#b34">(Luo et al., 2020)</ref> and our in-house implementation, respectively. In our implementation, we use XLM-R and the best version of HiCTL (pre-traind with CCNet-100 and hard negative samples) to initialize the encoder, respectively.</figDesc><table><row><cell>Model</cell><cell cols="4">Layers Encoder Decoder En→De En→Fr WMT'14</cell></row><row><cell>Randomly Initialize</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer-Big (Vaswani et al., 2017)</cell><cell>6</cell><cell>6</cell><cell>28.4</cell><cell>41.0</cell></row><row><cell>Deep-Transformer (Liu et al., 2020a)</cell><cell>60</cell><cell>12</cell><cell>30.1</cell><cell>43.8</cell></row><row><cell>Deep MSC Model (Wei et al., 2020)</cell><cell>18</cell><cell>6</cell><cell>30.56</cell><cell>-</cell></row><row><cell>Pre-trained Models Initialize</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CTNMT (Yang et al., 2020)</cell><cell>18</cell><cell>6</cell><cell>30.1</cell><cell>42.3</cell></row><row><cell>BERT-fused NMT (Zhu et al., 2020)</cell><cell>18</cell><cell>6</cell><cell>30.75</cell><cell>43.78</cell></row><row><cell>mBART  † (Liu et al., 2020b)</cell><cell>12</cell><cell>12</cell><cell>30.0</cell><cell>43.2</cell></row><row><cell>VECO (Luo et al., 2020)</cell><cell>24</cell><cell>6</cell><cell>31.5</cell><cell>44.4</cell></row><row><cell>XLM-R  ‡</cell><cell>24</cell><cell>6</cell><cell>30.91</cell><cell>43.27</cell></row><row><cell>HICTL</cell><cell>24</cell><cell>6</cell><cell>31.74</cell><cell>43.95</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>BLEU scores [%] on low-resource tasks. Results with ‡ are from our in-house implementation. We provide additional experimental results (to follow experiments in Zhu et al. (2020)) on IWSLT'14 English→Spanish (En→Es) task. HICTL Base represents the BASE sized model that is pre-trained on CCNet-100 with hard negative samples.</figDesc><table><row><cell>Model</cell><cell cols="3">IWSLT'14 En→De De→En En→Es</cell><cell>WMT'16 Ro→En</cell><cell cols="2">IWSLT'17 En→Fr En→Zh</cell></row><row><cell>Transformer (Vaswani et al., 2017)  ‡</cell><cell>28.64</cell><cell>34.51</cell><cell>39.3</cell><cell>33.51</cell><cell>35.8</cell><cell>26.5</cell></row><row><cell>BERT-fused NMT (Zhu et al., 2020)</cell><cell>30.45</cell><cell>36.11</cell><cell>41.4</cell><cell>39.10</cell><cell>38.7</cell><cell>28.2</cell></row><row><cell>HICTLBase</cell><cell>31.88</cell><cell>37.96</cell><cell>42.1</cell><cell>39.88</cell><cell>40.2</cell><cell>29.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>BLEU scores [%] on Zero-shot MT via Language Transfer. We bold the highest transferring score for each language family. and fuses the representations extracted from BERT with each encoder and decoder layer. Note we achieve new state-of-the-art results on IWSLT'14 En→De, IWSLT'17 En→{Fr, Zh} translations. These improvements show that mapping different languages into a universal representation space is beneficial for both low-resource and high-resource translations.</figDesc><table><row><cell></cell><cell cols="3">Fine-tuning Languages</cell></row><row><cell>Test Languages</cell><cell cols="2">Cs→En</cell><cell cols="2">Hi→En</cell></row><row><cell></cell><cell cols="4">mBART HiCTL mBART HiCTL</cell></row><row><cell>Cs→En</cell><cell>21.6</cell><cell>22.4</cell><cell>-</cell></row><row><cell>Ro→En</cell><cell>19.5</cell><cell>19.0</cell><cell>-</cell></row><row><cell>It→En</cell><cell>16.7</cell><cell>18.6</cell><cell>-</cell></row><row><cell>Nl→En</cell><cell>17.0</cell><cell>18.1</cell><cell>-</cell></row><row><cell>Hi→En</cell><cell>-</cell><cell></cell><cell>23.5</cell><cell>25.2</cell></row><row><cell>Ne→En</cell><cell>-</cell><cell></cell><cell>14.5</cell><cell>16.0</cell></row><row><cell>Si→En</cell><cell>-</cell><cell></cell><cell>13.0</cell><cell>14.7</cell></row><row><cell>Gu→En</cell><cell>-</cell><cell></cell><cell>0.0</cell><cell>0.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The statistics of CCNet corpus used for pretraining.</figDesc><table><row><cell cols="10">Code Size (GB) Code Size (GB) Code Size (GB) Code Size (GB) Code Size (GB)</cell></row><row><cell>af</cell><cell>1.3</cell><cell>et</cell><cell>6.1</cell><cell>ja</cell><cell>24.2</cell><cell>mt</cell><cell>0.2</cell><cell>sq</cell><cell>3.0</cell></row><row><cell>am</cell><cell>0.7</cell><cell>eu</cell><cell>2.0</cell><cell>jv</cell><cell>0.2</cell><cell>my</cell><cell>0.9</cell><cell>sr</cell><cell>5.1</cell></row><row><cell>ar</cell><cell>20.4</cell><cell>fa</cell><cell>21.6</cell><cell>ka</cell><cell>3.4</cell><cell>ne</cell><cell>2.6</cell><cell>su</cell><cell>0.1</cell></row><row><cell>as</cell><cell>0.1</cell><cell>fi</cell><cell>19.2</cell><cell>kk</cell><cell>2.6</cell><cell>nl</cell><cell>15.8</cell><cell>sv</cell><cell>10.8</cell></row><row><cell>az</cell><cell>3.6</cell><cell>fr</cell><cell>46.5</cell><cell>km</cell><cell>1.0</cell><cell>no</cell><cell>3.7</cell><cell>sw</cell><cell>1.6</cell></row><row><cell>be</cell><cell>3.5</cell><cell>fy</cell><cell>0.2</cell><cell>kn</cell><cell>1.2</cell><cell>om</cell><cell>0.1</cell><cell>ta</cell><cell>8.2</cell></row><row><cell>bg</cell><cell>22.6</cell><cell>ga</cell><cell>0.5</cell><cell>ko</cell><cell>17.2</cell><cell>or</cell><cell>0.6</cell><cell>te</cell><cell>2.6</cell></row><row><cell>bn</cell><cell>7.9</cell><cell>gd</cell><cell>0.1</cell><cell>ku</cell><cell>0.4</cell><cell>pa</cell><cell>0.8</cell><cell>th</cell><cell>14.7</cell></row><row><cell>br</cell><cell>0.1</cell><cell>gl</cell><cell>2.9</cell><cell>ky</cell><cell>1.2</cell><cell>pl</cell><cell>16.8</cell><cell>tl</cell><cell>0.8</cell></row><row><cell>bs</cell><cell>0.1</cell><cell>gu</cell><cell>0.3</cell><cell>la</cell><cell>2.5</cell><cell>ps</cell><cell>0.7</cell><cell>tr</cell><cell>17.3</cell></row><row><cell>ca</cell><cell>10.1</cell><cell>ha</cell><cell>0.3</cell><cell>lo</cell><cell>0.6</cell><cell>pt</cell><cell>15.9</cell><cell>ug</cell><cell>0.4</cell></row><row><cell>cs</cell><cell>16.3</cell><cell>he</cell><cell>6.7</cell><cell>lt</cell><cell>7.2</cell><cell>ro</cell><cell>8.6</cell><cell>uk</cell><cell>9.1</cell></row><row><cell>cy</cell><cell>0.8</cell><cell>hi</cell><cell>20.2</cell><cell>lv</cell><cell>6.4</cell><cell>ru</cell><cell>48.1</cell><cell>ur</cell><cell>5.0</cell></row><row><cell>da</cell><cell>15.2</cell><cell>hr</cell><cell>5.4</cell><cell>mg</cell><cell>0.2</cell><cell>sa</cell><cell>0.3</cell><cell>uz</cell><cell>0.7</cell></row><row><cell>de</cell><cell>46.3</cell><cell>hu</cell><cell>9.5</cell><cell>mk</cell><cell>1.9</cell><cell>sd</cell><cell>0.4</cell><cell>vi</cell><cell>44.6</cell></row><row><cell>el</cell><cell>29.3</cell><cell>hy</cell><cell>5.5</cell><cell>ml</cell><cell>4.3</cell><cell>si</cell><cell>2.1</cell><cell>xh</cell><cell>0.1</cell></row><row><cell>en</cell><cell>49.7</cell><cell>id</cell><cell>10.6</cell><cell>mn</cell><cell>1.7</cell><cell>sk</cell><cell>4.9</cell><cell>yi</cell><cell>0.3</cell></row><row><cell>eo</cell><cell>0.9</cell><cell>is</cell><cell>1.3</cell><cell>mr</cell><cell>1.3</cell><cell>sl</cell><cell>2.8</cell><cell>zh</cell><cell>36.8</cell></row><row><cell>es</cell><cell>44.6</cell><cell>it</cell><cell>19.8</cell><cell>ms</cell><cell>3.2</cell><cell>so</cell><cell>0.4</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Parallel data used for pre-training.</figDesc><table><row><cell cols="3">Code Sentence Pair (#millions) Code Sentence Pair (#millions)</cell></row><row><cell>en-ar</cell><cell>9.8 en-ru</cell><cell>11.7</cell></row><row><cell>en-bg</cell><cell>0.6 en-sw</cell><cell>0.2</cell></row><row><cell>en-de</cell><cell>9.3 en-th</cell><cell>3.3</cell></row><row><cell>en-el</cell><cell>4.0 en-tr</cell><cell>0.5</cell></row><row><cell>en-es</cell><cell>11.4 en-ur</cell><cell>0.7</cell></row><row><cell>en-fr</cell><cell>13.2 en-vi</cell><cell>3.5</cell></row><row><cell>en-hi</cell><cell>1.6 en-zh</cell><cell>9.6</cell></row><row><cell>A PRE-TRAINING DATA</cell><cell></cell><cell></cell></row><row><cell cols="3">During pre-training, we follow Conneau et al. (2020) to build a Common-Crawl Corpus using the</cell></row><row><cell cols="2">CCNet (Wenzek et al., 2019) tool 5 for monolingual texts. Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>PAWS-X accuracy scores for each language.</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>avg</cell></row><row><cell>Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R</cell><cell cols="8">95.7 92.2 92.7 92.5 84.7 85.9 87.1 90.1</cell></row><row><cell>HICTL, Wiki-15 + MT</cell><cell cols="8">96.6 93.2 93.3 92.9 86.5 87.3 88.6 91.2</cell></row><row><cell>HICTL, CCNet-100 + MT</cell><cell cols="8">96.9 93.8 94.4 94.3 88.0 88.2 89.4 92.2</cell></row><row><cell cols="9">+HARD NEGATIVE SAMPLES 97.4 94.2 95.0 94.2 89.1 89.5 90.2 92.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>POS results (Accuracy) for each language.</figDesc><table><row><cell>Model</cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell></row><row><cell>Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R</cell><cell cols="17">90.6 67.4 89.1 89.9 86.8 96.3 89.6 87.1 74.0 70.8 86.0 87.7 68.6 77.4 82.8 72.6 91.1</cell></row><row><cell>HICTL, Wiki-15 + MT</cell><cell cols="17">91.0 69.3 89.1 89.4 87.8 97.6 88.2 88.2 74.8 72.0 86.7 87.9 70.2 79.0 84.2 74.3 90.8</cell></row><row><cell>HICTL, CCNet-100 + MT</cell><cell cols="17">91.8 70.2 90.7 90.8 89.0 98.3 89.7 90.1 76.2 73.0 88.5 90.2 70.7 80.0 86.4 74.5 92.0</cell></row><row><cell cols="18">+HARD NEGATIVE SAMPLES 92.2 71.0 91.5 91.3 90.0 97.7 91.0 89.4 75.7 73.5 88.8 90.1 71.1 79.7 85.4 75.1 91.7</cell></row><row><cell></cell><cell>ja</cell><cell>kk</cell><cell>ko</cell><cell>mr</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>yo</cell><cell>zh</cell><cell>avg</cell></row><row><cell>Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R</cell><cell cols="17">17.3 78.3 55.5 82.1 89.8 88.9 89.8 65.7 87.0 48.6 92.9 77.9 71.7 56.8 24.7 27.2 74.6</cell></row><row><cell>HICTL, Wiki-15 + MT</cell><cell cols="17">28.4 79.2 54.2 80.7 90.9 88.4 90.5 67.3 89.1 48.7 92.2 77.6 72.0 58.8 27.2 27.1 75.5</cell></row><row><cell>HICTL, CCNet-100 + MT</cell><cell cols="17">30.2 80.4 55.1 82.1 91.2 90.2 90.7 68.1 90.1 50.3 95.2 78.7 73.3 59.2 27.8 27.9 76.8</cell></row><row><cell cols="18">+HARD NEGATIVE SAMPLES 31.9 80.9 57.0 83.5 91.7 91.0 91.2 69.5 90.8 50.3 94.8 79.4 73.4 59.5 28.6 28.7 77.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>NER results (F1) for each language. 82.3 55.2 84.7 79.0 81.2 80.1 81.6 79.8 61.4 61.9 82.8 80.5 60.4 74.6 79.8 54.8 83.5 24.9 66.1 HICTL, CCNet-100 + MT 88.6 80.9 55.4 85.6 81.8 82.0 82.5 80.8 81.2 62.5 64.2 81.2 83.0 60.3 77.3 84.4 55.8 83.7 26.0 65.0 +HARD NEGATIVE SAMPLES 88.9 82.0 56.6 83.7 83.4 82.8 84.8 83.0 83.8 65.4 65.4 82.0 82.6 60.5 74.7 81.5 58.1 84.7 27.9 65.9 .7 62.2 69.4 68.8 57.9 55.6 87.9 84.2 71.9 74.4 61.6 59.2 2.2 74.2 79.5 58.1 83.0 35.2 33.0 HICTL, CCNet-100 + MT 72.8 57.6 64.6 70.4 71.5 61.1 59.0 87.7 85.1 70.3 74.3 60.6 57.9 5.6 77.5 79.0 59.8 83.7 37.7 36.9 +HARD NEGATIVE SAMPLES 76.8 60.9 65.0 71.4 72.5 59.0 56.3 85.9 84.5 71.4 75.6 62.9 58.8 3.9 77.7 80.4 59.1 83.6 37.7 37.2</figDesc><table><row><cell>Model</cell><cell>en</cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell><cell>jv</cell></row><row><cell>Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R</cell><cell cols="20">86.8 81.4 55.2 82.9 81.1 79.1 81.5 81.1 81.3 60.6 64.1 80.6 83.2 60.1 76.1 79.4 53.2 80.7 22.7 63.9</cell></row><row><cell>HICTL, Wiki-15 + MT</cell><cell>87.0 ka</cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>ms</cell><cell>my</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>yo</cell><cell>zh</cell></row><row><cell>XLMR</cell><cell cols="20">74.2 58.0 63.3 68.3 69.8 59.5 57.5 86.2 82.3 68.5 70.7 59.8 58.5 2.4 72.6 75.9 59.7 79.4 37.0 35.4</cell></row><row><cell>HICTL, Wiki-15 + MT</cell><cell cols="2">75.0 56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>Tatoeba results (Accuracy) for each language .5 72.2 45.4 89.5 61.3 77.6 51.7 38.6 71.7 72.8 76.9 66.3 73.1 65.1 77.5 68.5  63.1 HICTL, Wiki-15 + MT 61.5 51.4 76.1 47.9 92.1 63.4 80.5 55.9 37.8 74.6 76.7 78.0 68.4 74.5 68.8 80.4 70.2 63.9 HICTL, CCNet-100 + MT 63.0 50.9 76.8 47.0 94.6 68.8 80.9 59.3 41.5 77.3 78.2 80.3 70.2 77.9 72.1 81.3 73.7 66.2 +HARD NEGATIVE SAMPLES 68.9 57.7 83.2 55.4 98.2 74.5 88.5 62.4 47.7 80.2 82.9 85.5 79.1 85.0 76.8 90.3 80.8 72.7 .3 51.2 63.1 66.2 59.0 81.0 84.4 76.9 19.8 28.3 37.8 28.9 36.7 68.9 26.6 77.9 69.8 HICTL, Wiki-15 + MT 18.7 55.8 51.0 65.5 67.3 61.2 82.9 84.4 78.3 22.2 28.6 41.4 33.5 41.6 71.2 26.7 80.2 73.6 HICTL, CCNet-100 + MT 19.6 57.3 54.6 68.0 71.8 62.0 88.1 88.9 77.7 26.1 32.9 39.5 32.9 43.2 71.2 27.8 79.9 74.7 +HARD NEGATIVE SAMPLES 27.2 63.0 61.5 72.6 75.3 67.8 92.8 92.8 85.4 32.0 36.7 47.8 41.5 49.8 77.0 34.3 84.3 81.3</figDesc><table><row><cell>Model</cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell></row><row><cell>Translate-train-all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>XLM-R</cell><cell cols="2">59.7 50jv ka</cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell></row><row><cell>XLM-R</cell><cell cols="2">15.8 53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">https://github.com/facebookresearch/cc_net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">We split 7k sentence pairs from the training dataset for validation and concatenate dev2010, dev2012, tst2010, tst2011, tst2012 as the test set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">https://wit3.fbk.eu/mt.php?release=2017-01-ted-test</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><ref type="bibr" target="#b58">Zhao et al. (2020)</ref> conducted a theoretical investigation on learning universal representations for the task of multilingual MT, while we directly use a shared encoder and decoder across languages for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">https://github.com/facebookresearch/cc_net</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for the helpful comments. We also thank Jing Yu for the instructive suggestions. This work is supported by the National Key R&amp;D Program of China under Grant No.2017YFB0803301 and No. 2018YFB1403202.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">10</ref>: Results on Cross-lingual Natural Language Inference (XNLI) for each language. We report the accuracy on each of the 15 XNLI languages and the average accuracy of our HICTL as well as five baselines: BiLSTM <ref type="bibr" target="#b9">(Conneau et al., 2018)</ref>, mBERT <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref>, XLM <ref type="bibr" target="#b8">(Conneau &amp; Lample, 2019)</ref>, Unicoder <ref type="bibr" target="#b22">(Huang et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b10">(Conneau et al., 2020)</ref>. Results of ‡ are from our in-house replication. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D VISUALIZATION OF SENTENCE EMBEDDINGS</head><p>We collect 10 sets of samples from WMT'14-19, each of them contains 100 parallel sentences distributed in 5 languages. As the t-SNE visualization in Figure <ref type="figure">4</ref>, a set of sentences under the same meaning are clustered more densely for HICTL than XLM-R, which reveals the strong capability</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11856</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1075</idno>
		<ptr target="https://www.aclweb.org/anthology/D15-1075" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="632" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Józefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1002</idno>
		<ptr target="https://doi.org/10.18653/v1/k16-1002" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
				<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08-11">2016. August 11-12, 2016. 2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="10719" to="10729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Infoxlm: An information-theoretic framework for crosslingual language model pre-training</title>
		<author>
			<persName><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2007.07834</idno>
		<ptr target="https://arxiv.org/abs/2007.07834" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05002</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ELECTRA: pretraining text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020. OpenReview.net</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS 2019</title>
				<meeting>of NIPS 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7059" to="7069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">October-November 2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unified language model pre-training for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Wuen</forename><surname>Hon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="13063" to="13075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pre-trained language model representations for language generation</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1409</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1409" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4052" to="4059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multiun: A multilingual corpus from united nation documents</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Eisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Language Resources &amp; Evaluation</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">FILTER: an enhanced fusion method for cross-lingual language understanding</title>
		<author>
			<persName><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno>CoRR, abs/2009.05166</idno>
		<ptr target="https://arxiv.org/abs/2009.05166" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Languageagnostic BERT sentence embedding. CoRR, abs</title>
		<author>
			<persName><forename type="first">Fangxiaoyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.01852" />
		<imprint>
			<date type="published" when="2007">2007.01852. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Incorporating bert into parallel sequence decoding with adapters</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Hao-Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxing</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/7a6a74cbe87bc60030a4bd041dd47b78-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="10843" to="10854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2006)</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno>CoRR, abs/1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno>CoRR, abs/2003.11080</idno>
		<ptr target="https://arxiv.org/abs/2003.11080" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1252" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
				<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pretraining with contrastive sentence objectives improves discourse performance of language models</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Iter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Lansing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.439" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<biblScope unit="page" from="4859" to="4870" />
		</imprint>
	</monogr>
	<note>Online, 2020. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1853" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A mutual information maximization perspective of language representation learning</title>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>ICLR 2020. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The IIT bombay english-hindi parallel corpus</title>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018. European Language Resources Association (ELRA)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation, LREC 2018. European Language Resources Association (ELRA)</meeting>
		<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The IIT Bombay English-Hindi parallel corpus</title>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratik</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/L18-1548" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
				<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</meeting>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">ALBERT: A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1eA7AEtvS" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
				<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mlqa: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07475</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep transformers for neural machine translation</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07772</idno>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multilingual denoising pre-training for neural machine translation</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/2001.08210</idno>
		<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">VECO: variable encoder-decoder pre-training for cross-lingual understanding and generation</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<idno>CoRR, abs/2010.16046</idno>
		<ptr target="https://arxiv.org/abs/2010.16046" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2265" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Universal Dependencies 2.2</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeljko</forename><surname>Agic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01930733" />
	</analytic>
	<monogr>
		<title level="m">LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics ( ÚFAL), Faculty of Mathematics and Physics</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Charles University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>CoRR, abs/1807.03748</idno>
		<ptr target="http://arxiv.org/abs/1807.03748" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
		<ptr target="https://www.aclweb.org/anthology/P17-1178" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<title level="s">Long Papers</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1202</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<ptr target="https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrishikesh</forename><surname>Khandeparkar</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v97/saunshi19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15">09-15 Jun 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5628" to="5637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D13-1170" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">MASS: masked sequence to sequence pre-training for language generation</title>
		<author>
			<persName><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
				<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5926" to="5936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58621-8_45</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-58621-8_" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">August 23-28, 2020. 2020</date>
			<biblScope unit="volume">12356</biblScope>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-58621-8_45</idno>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiscale collaborative deep models for neural machine translation</title>
		<author>
			<persName><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.40" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online</publisher>
			<date type="published" when="2020-07">July 2020</date>
			<biblScope unit="page" from="414" to="426" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Acquiring knowledge from pre-trained model to neural machine translation</title>
		<author>
			<persName><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanbo</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9266" to="9273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00359</idno>
		<title level="m">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1101</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-1101" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards making the most of BERT in neural machine translation</title>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://aaai.org/ojs/index.php/AAAI/article/view/6479" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence</title>
				<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9378" to="9385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
				<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1499</idno>
		<ptr target="https://www.aclweb.org/anthology/P19-1499" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">July 2019a</date>
			<biblScope unit="page" from="5059" to="5069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PAWS: Paraphrase adversaries from word scrambling</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1131</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1131" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019b</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1298" to="1308" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On learning language-invariant representations for universal machine translation</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v119/zhao20b.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
				<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hardness-aware deep metric learning</title>
		<author>
			<persName><forename type="first">Wenzhao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaodong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Jie</surname></persName>
		</author>
		<ptr target="http://openaccess.thecvf.com/content_CVPR_2019/html/Zheng_Hardness-Aware_Deep_Metric_Learning_CVPR_2019_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
				<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">June 16-20, 2019. 2019</date>
			<biblScope unit="page" from="72" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Extractive summarization as text matching</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.552" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
				<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6197" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Incorporating BERT into neural machine translation</title>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hyl7ygStwB" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020. OpenReview.net</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Overview of the second BUCC shared task: Spotting parallel sentences in comparable corpora</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Zweigenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Sharoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reinhard</forename><surname>Rapp</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-2512</idno>
		<ptr target="https://www.aclweb.org/anthology/W17-2512" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Workshop on Building and Using Comparable Corpora</title>
				<meeting>the 10th Workshop on Building and Using Comparable Corpora<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">August 2017</date>
			<biblScope unit="page" from="60" to="67" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
