<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Joseph</forename><surname>Antony</surname></persName>
							<email>joseph.antony2@mail.dcu.ie</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Insight Centre for Data Analytics</orgName>
								<orgName type="department" key="dep2">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Insight Centre for Data Analytics</orgName>
								<orgName type="department" key="dep2">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O'connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Insight Centre for Data Analytics</orgName>
								<orgName type="department" key="dep2">Dublin City University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kieran</forename><surname>Moran</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Insight Centre for Data Analytics</orgName>
								<orgName type="department" key="dep2">Dublin City University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Health and Human Performance</orgName>
								<orgName type="institution">Dublin City University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Quantifying Radiographic Knee Osteoarthritis Severity using Deep Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">F58B57F894BA5C9FAFFE3DCA7B3C0082</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knee osteoarthritis</term>
					<term>KL grades</term>
					<term>Convolutional neural network</term>
					<term>classification</term>
					<term>regression</term>
					<term>wndchrm</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a new approach to automatically quantify the severity of knee osteoarthritis (OA) from radiographs using deep convolutional neural networks (CNN). Clinically, knee OA severity is assessed using Kellgren &amp; Lawrence (KL) grades, a five point scale. Previous work on automatically predicting KL grades from radiograph images were based on training shallow classifiers using a variety of hand engineered features. We demonstrate that classification accuracy can be significantly improved using deep convolutional neural network models pre-trained on ImageNet and fine-tuned on knee OA images. Furthermore, we argue that it is more appropriate to assess the accuracy of automatic knee OA severity predictions using a continuous distance-based evaluation metric like mean squared error than it is to use classification accuracy. This leads to the formulation of the prediction of KL grades as a regression problem and further improves accuracy. Results on a dataset of X-ray images and KL grades from the Osteoarthritis Initiative (OAI) show a sizable improvement over the current state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The increasing prevalence of knee osteoarthritis (OA), a degenerative joint disease, and total joint arthoplasty as a serious consequence, means there is a growing need for effective clinical and scientific tools to diagnose knee OA in the early stage, and to assess its severity in progressive stages <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Detecting knee OA and assessing the severity of knee OA are crucial for pathology, clinical decision making, and predicting disease progression <ref type="bibr" target="#b2">[3]</ref>. Joint space narrowing (JSN) and osteophytes (bone spurs) formation are the key pathological features of knee OA <ref type="bibr" target="#b0">[1]</ref>, which are easily visualized using radiographs <ref type="bibr" target="#b2">[3]</ref>.</p><p>The assessment of knee OA severity has traditionally been approached as an image classification problem <ref type="bibr" target="#b1">[2]</ref>, with the KL grades being the ground truth for classification. Radiographic features detectable through a computer-aided analysis are clearly useful to quantify knee OA severity, and to predict the future development of knee OA <ref type="bibr" target="#b1">[2]</ref>. However, based on the results reported, the accuracy of both the multi-class and consecutive grades classification is far from ideal. Previous work on classifying knee OA from radiographic images have used Wndchrm, a multipurpose bio-medical image classifier <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. The feature space used by Wndchrm includes hand-crafted features to capture these characteristics based on polynomial decomposition, contrast, pixel statistics, textures and also features extracted from image transforms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Instead of hand-crafted features, we propose that learning feature representations using a CNN can be more effective for classifying knee OA images to assess the severity condition. Feature learning approaches provide a natural way to capture cues by using a large number of code words (sparse coding) or neurons (deep networks), while traditional computer vision features, designed for basic-level category recognition, may eliminate many useful cues during feature extraction <ref type="bibr" target="#b5">[6]</ref>. Manually designed or hand-crafted features often simplify machine learning tasks. Nevertheless, they have a few disadvantages. The process of engineering features requires domain-related expert knowledge, and is often very time consuming <ref type="bibr" target="#b6">[7]</ref>. These features are often low-level as prior knowledge is hand-encoded, and features in one domain do not always generalize to other domains <ref type="bibr" target="#b7">[8]</ref>. In recent years, learning feature representations is preferred to hand-crafted features, particularly for fine-grained classification, because rich appearance and shape features are essential for describing subtle differences between categories <ref type="bibr" target="#b5">[6]</ref>.</p><p>A convolutional neural network (CNN) typically comprises multiple convolutional and sub-sampling layers, optionally followed by fully-connected layers like a standard multi-layer neural network. A CNN exploits the 2D spatial structure images to learn translation invariant features. This is achieved with local connections and associated weights followed by some form of pooling. The main advantage of CNN over fullyconnected networks is that they are easier to train and have fewer parameters with the same number of hidden units <ref type="bibr" target="#b8">[9]</ref>.</p><p>In this work, first, we investigated the use of well-known CNNs such as the VGG 16-layer net <ref type="bibr" target="#b9">[10]</ref>, and comparatively simpler networks like VGG-M-128 <ref type="bibr" target="#b10">[11]</ref>, and BVLC reference CaffeNet <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> (which is very similar to the widely-used AlexNet model <ref type="bibr" target="#b13">[14]</ref>) to classify knee OA images. These networks are pre-trained for color image classification using a very large dataset such as the ImageNet LSVRC dataset <ref type="bibr" target="#b14">[15]</ref>, which contains 1.2 million images with 1000 classes. Initially, we extracted features from the convolutional, pooling, and fully-connected layers of VGG16, VGG-M-128, and BVLC CaffeNet, and trained linear SVMs to classify knee OA images.</p><p>Next, motivated by the transfer learning approach <ref type="bibr" target="#b15">[16]</ref>, we fine-tuned the pre-trained networks. We adopted transfer learning as the OAI dataset we work with is small, containing only a few thousand images. In this setting, a base network is first trained on external data, and then the weights of the initial n layers are transferred to a target network <ref type="bibr" target="#b15">[16]</ref>. The new layers of the target network are randomly initialized. Intuitively, the lower layers of the networks contain more generic features such as edge or texture detectors useful for multiple tasks, while the upper layers progressively focus on more task specific cues <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>. We used this approach for both classification and regression, adding new fully-connected layers and use backpropagation to fine tune the weights for the complete network on the target loss.</p><p>The primary contributions of this paper are the use of CNNs and regression loss to quantify knee OA severity. We propose the use of mean squared error for assessing the performance of an automatic knee OA severity assessment instead of binary and multi-class classification accuracy. We show that the inferred CNN features from the fine-tuned BVLC reference CaffeNet provide higher classification accuracy in comparison to the state-of-the-art. We also present an SVM-based method to automatically detect and extract the knee joints from knee OA radiographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>The data used for the experiments are bilateral PA fixed flexion knee X-ray images, taken from the baseline (image release version O.E.1) radiographs of the Osteoarthritis Initiative (OAI) dataset containing an entire cohort of 4, 476 participants. This is a standard dataset for studies involving knee OA. Figure <ref type="figure" target="#fig_0">1</ref> shows some samples from the dataset. In the entire cohort, Kellgren &amp; Lawrence (KL) grades are available for both knee joints in 4, 446 radiographs and these images were used for this study. The distribution of the knee joint images (in total 8, 892) conditioned on the KL grading scale are: Grade 0 -3433, Grade 1 -1589, Grade 2 -2353, Grade 3 -1222, and Grade 4 -295. The KL grading system uses 5 grades to classify knee OA severity from the radiographs <ref type="bibr" target="#b16">[17]</ref>, where 'Grade 0' corresponds to the normal knee, and the other grades correspond to the progression of the disease, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. Automatically detecting, and extracting the knee joint region from the radiographs is an important pre-processing step and Shamir et. al. <ref type="bibr" target="#b1">[2]</ref> proposed the template matching method for this. Though this method is simple to implement, the accuracy of detecting the knee joints is low for our dataset. To improve detection, we propose an SVM-basd method.</p><p>1) Template matching: As a baseline, we adapted the template matching approach <ref type="bibr" target="#b1">[2]</ref> for detecting the knee joint center, to an image patch of size 20×20 pixels. The radiographs are first down-scaled to 10% of the original size and subjected to histogram equalization for intensity normalization. An image patch (20×20 pixels) containing the knee joint center is taken as a template. 10 image patches from each grade, so that in total 50 patches were pre-selected as templates. Each input image is scanned by an overlapping sliding window (20×20 pixels). At each window the Euclidean distance between the image patch and the 50 templates are calculated, and the shortest distance is recorded. After scanning an entire image with the sliding window, the window that records the smallest Euclidean distance is recorded as the knee joint center.</p><p>2) Proposed method for detecting the knee joints: We propose an approach using a linear SVM and the Sobel horizontal image gradients as the features for detecting the knee joint centers. The well-known Sobel edge detection algorithm uses the vertical and the horizontal image gradients. The motivation for this is that knee joint images primarily contain horizontal edges. The image patches (20×20 pixels) containing the knee joint center are taken as the positive training samples and the image patches (20×20 pixels) excluding the knee joint center are taken as the negative training samples. After extracting Sobel horizontal gradients for the positive and negative samples, a linear SVM was trained. To detect the knee joint center from both left and right knees, input images are split in half to isolate left and right knees separately. A sliding window (20×20 pixels) is used on either half of the image, and the Sobel horizontal gradient features are extracted for every image patch. The image patch with the maximum score based on the SVM decision function is recorded as the detected knee joint center, and the area (300×300 pixels) around the knee joint center is extracted from the input images using the corresponding recorded coordinates. Figure <ref type="figure" target="#fig_2">3</ref> shows an example of a detected and extracted knee joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Assessing the knee OA severity using CNNs</head><p>In this study, we investigate the use of CNN for assessing the severity of knee OA through classification and regression. For this, we used two approaches: 1. Pre-trained CNN for fixed feature extraction, 2. Fine-tuning the pre-trained CNN following the transfer learning approach. For benchmarking the classification results obtained by the proposed methods, we have used Wndchrm, an open source utility for medical image classification that has been applied to this task in the literature <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>1) Classification using features extracted from pre-trained CNNs: As our initial approach, we trained VGG16 <ref type="bibr" target="#b9">[10]</ref> with the OAI dataset. We used the Caffe <ref type="bibr" target="#b11">[12]</ref> framework for implementing and training the CNN, and to extract features from the CNN. We extracted features from the different layers of the VGG net such as fully-connected (fc7), pooling (pool5), and convolutional (conv5 2) layers to identify the most discriminating set of features. Linear SVMs (trained using LIBLINEAR <ref type="bibr" target="#b17">[18]</ref>) were trained with the extracted CNN features for classifying knee OA images, where the ground truth was labeled images conditioned on the KL grades. Next, we investigated the use of simpler pre-trained CNNs such as VGG-M-128 <ref type="bibr" target="#b10">[11]</ref> and BVLC CaffeNet <ref type="bibr" target="#b11">[12]</ref> for classifying the knee OA images. These networks have fewer layers and parameters in comparison to VGG16.</p><p>2) Fine-tuning the CNNs for classification and regression: Our next approach fine-tuned the BVLC CaffeNet <ref type="bibr" target="#b11">[12]</ref> and VGG-M-128 <ref type="bibr" target="#b10">[11]</ref> networks. We chose these two smaller networks, both which contain fewer layers and parameters (∼62M), over the much deeper VGG16, which has ∼138M parameters. We replace the top fully-connected layer of both networks and retrain the model on the OAI dataset using backpropagation. The lower-level features in the bottom layers are also updated during fine-tuning. Standard softmax loss was used as the objective for classification, and accuracy layers were added to monitor training progress. A Euclidean loss layer (mean squared error) was used for the regression experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RESULTS AND DISCUSSION</head><p>A. Automatic detection of the knee joints Standard template matching <ref type="bibr" target="#b1">[2]</ref> produces poor detection accuracy on our dataset. To improve this, we used a linear SVM with the Sobel horizontal image gradients as features to detect the knee joints. The proposed method is approximately 80× faster than template matching; for detecting all the knee joints in the dataset comprising 4, 492 radiographs, the proposed method took ∼9 minutes and the template matching method took ∼798 minutes.</p><p>Image patches containing the knee joint center (20×20 pixels) were used as positive examples and randomly sampled patches excluding the knee joint as negative samples. We used 200 positive and 600 negative training samples. The samples were split into 70% training and 30% test set. Fitting a linear SVM produced 95.2% 5-fold cross validation and 94.2% test accuracies. Table <ref type="table" target="#tab_0">I</ref> shows the precision, recall, and F 1 scores of this classification.</p><p>To evaluate the automatic detection, we generated the ground truth by manually annotating the knee joint centers (20×20 pixels) in 4,496 radiographs using an annotation tool that we developed, which recorded the bounding box (20×20 pixels) coordinates of each annotation.</p><p>We use the well-known Jaccard index to give a matching score for each detected instance. The Jaccard index J(A,D) is given by,</p><formula xml:id="formula_0">J(A, D) = A ∩ D A ∪ D<label>(1)</label></formula><p>where A, is the manually annotated and D is the automatically detected knee joint center using the proposed method. Table <ref type="table" target="#tab_1">II</ref> shows the resulting average detection accuracies based on thresholding of Jaccard indices. The mean Jaccard index for the template matching and the classifier methods are 0.1 and 0.36. From Table <ref type="table" target="#tab_1">II</ref>, it is evident that the proposed method is more accurate than template matching. This is due to the fact that template matching relies upon the intensity level difference across an input image. Thus, it is prone to matching a patch with small Euclidean distance that does not actually correspond to the joint center. We also varied the templates in a set, and observed that the detection is highly dependent on the choice of templates: template matching is similar to a k-nearest neighbor classifier with k = 1. The reason for higher accuracy in the proposed method is the use of horizontal edge detection instead of intensity level differences. The knee joints primarily contain horizontal edges and thus are easily detected by the classifier using horizontal image gradients as features. Despite sizable improvements in accuracy and speed using the proposed approach, detection accuracy still falls short of 100%. We therefore decided to use our manual annotations so as to investigate KL grade classification performance independently of knee joint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification of the knee joints using pre-trained CNNs</head><p>The extracted knee joint images were split into training (∼70%) and test (∼30%) as per the KL grades. For classifying the knee joint images, we extracted features from fully-connected, pooling and convolution layers of VGG16, VGG-M-128, and BVLC CaffeNet. For binary and multi-class classifications, linear SVMs were trained individually with the extracted features. The classification results achieved with the CNNs are compared to knee classification of OA images using the Wndchrm <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>Table <ref type="table" target="#tab_2">III</ref> shows the test set classification accuracies achieved by Wndchrm and the CNN features. The CNN features consistently outperform Wndchrm for classifying healthy knee samples against the progressive stages of knee OA. The features from conv4 layer with dimension 512×13×13 and pool5 layer 256×13×13 of VGG-M-128 net, and conv5 layer with dimension 512×6×6 and pool5 layer with dimension 256×6×6 of BVLC reference CaffeNet give higher classification accuracy in comparison to the fully-connected fc6 and fc7 layers of VGG nets and CaffeNet. We also extracted features from further bottom layers such as pool4, conv4 2, pool3, pool2 and trained classifiers on top of these features. As the dimension of the bottom layers are high, significantly more time was required for training but without improvement in classification accuracy.</p><p>In a fine-grained classification task such as knee OA images classification, the accuracy of classifying successive classes tends to be low, as the variations in the progressive stages of the disease are minimal, and only highly discriminant features can capture these variations. From the experimental results, as shown in Table <ref type="table" target="#tab_2">III</ref>, the features extracted from CNNs provide significantly higher classification accuracy in comparison to the Wndchrm, and these features are effective and promising for classifying the consecutive stages of knee OA.</p><p>We performed multi-class classifications using linear SVMs with the CNN features (Table <ref type="table" target="#tab_2">III</ref>, multi-class). Again, the CNN features perform significantly better than the Wndchrmbased approach. The classification accuracies obtained using convolutional (conv4, conv5) and pooling (pool5) layers are slightly higher in comparison to fully-connected layer features. There are minimal variations in classification accuracy obtained with the features extracted from VGG-M-128 net and BVLC reference CaffeNet in comparison to VGG16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification of the knee joints using fine-tuned CNNs</head><p>Table <ref type="table" target="#tab_2">III</ref> shows the multi-class classification results for the fine-tuned BVLC CaffeNet and VGG-M-128 networks. We omitted the VGG16 network in these experiment since the variation in accuracy among the pre-trained CNNs was small, and fine-tuning VGG16 is significantly more computationally expensive. The dataset was split into training (60%), validation (10%) and test (30%) sets for fine-tuning. To increase the number of training samples, we included the right-left flipped knee joint images in the training set. The networks were finetuned for 20 epochs using a learning rate of 0.001 for the transferred layers, and boosting it on newly introduced layers by a factor of 10. The performance of fine-tuned BVLC CaffeNet was slightly better than VGG-M-128. Hence, we only show  here the results of fine-tuning CaffeNet. Figure <ref type="figure" target="#fig_3">4</ref> shows the learning curves for training and validation loss, and validation accuracy. The decrease in loss and increase in accuracy shows that the fine-tuning is effective and makes the CNN features more discriminative, which improves classification accuracy (Table <ref type="table" target="#tab_2">III</ref>). The features extracted from the fully connected (fc7) layer provide slightly better classification in comparison to pooling (pool5) and convolution (conv5) layers.</p><p>D. Regression of KL grades using fine-tuned CNNs.</p><p>Existing work on automatic measurement of knee OA severity treats it as an image classification problem, assigning each KL grade to a distinct category <ref type="bibr" target="#b1">[2]</ref>. To date, evaluation of automatic KL grading algorithms has been based on binary and multi-class classification accuracy with respect to these discrete KL grades <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>. KL grades are not, however, categorical, but rather represent an ordinal scale of increasing severity. Treating them as categorical during evaluation means that the penalty for incorrectly predicting that a subject with Grade 0 OA has Grade 4 is the same as the penalty for predicting that the same subject has Grade 1 OA. Clearly the former represents a more serious error, yet this is not captured by evaluation measures that treat grades as categorical variables. In this setup, permuting the ordering of the grades has no effect on classification performance. Moreover, the quantization of the KL grades to discrete integer levels is essentially an artifact of convenience; the true progression of the disease in nature is continuous, not discrete.</p><p>We therefore propose that it is more appropriate to measure the performance of an automatic knee OA severity assessment system using a continuous evaluation metric like mean squared error. Such a metric appropriately penalizes errors in proportion to their distance from the ground truth, rather than treating all errors equally. Directly optimizing mean squared error on a training set also naturally leads to the formulation of knee OA assessment as a standard regression problem. Treating it as such provides the model with more information on the structure and relationship between training examples with successive KL grades. We demonstrate that this reduces both the mean squared error and improves the multi-class classification accuracy of the model. We fine-tuned the pre-trained BVLC CaffeNet model using both classification loss (cross entropy on softmax outputs) and regression loss (mean squared error) to compare their performance in assessing knee OA severity. In both cases, we replace fc7 with a randomly initialized layer and fine tune for 20 epochs, selecting the model with the highest validation performance. The classification network uses a 5D fully connected layer and softmax following the fc7 layer, and the regression network uses a 1D fully connected node with a linear activation.</p><p>We compare the models using both mean squared error (MSE) and standard multi-class classification metrics. We calculated the mean squared error using the standard formula:</p><formula xml:id="formula_1">M SE = 1 n n i=1 (y i -ŷi ) 2 , (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where n is the number of test samples, y i is the true (integer) label and ŷi is the predicted label. For the classification network the predicted labels y i are integers and for the regression network they are real numbers. We also test a configuration where we round the real outputs from the regression network to produce integer labels. Table <ref type="table">V</ref> shows the MSE for classification using the Wndchrm and the CNN trained with classification loss (CNN-Clsf), regression loss (CNN-Reg), and regression loss with rounding (CNN-Reg*). Regression loss clearly achieves significantly lower mean squared error than both the CNN classification network and the Wndchrm features.</p><p>To demonstrate that the regression loss also produces better classification accuracy, we compare the classification accuracy from the network trained with classification loss and the network trained with regression loss and rounded labels. Rounding, in this case, is necessary to allow for using standard classification metrics. Table VI compares the resulting precision, recall, and F 1 scores. The multi-class (grade 0-4) classification accuracy of the network fine-tuned with regression loss is 59.6%. The network trained using regression loss clearly gives superior classification performance. We suspect this is due to the fact that using regression loss gives the network more information about the ordinal relationship between the KL grades, allowing it to converge on parameters that better generalize to unseen data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION AND FUTURE WORK</head><p>This paper investigated several new methods for automatic quantification of knee OA severity using CNNs. The first step in the process is to detect the knee joint region. We propose training a linear SVM on horizontal image gradients as an alternative to template matching, which is both more accurate and faster than template matching.</p><p>Our initial approach to classifying the knee OA severity used features extracted from pre-trained CNNs. We investigated three pre-trained networks and found that the BVLC reference CaffeNet and VGG-M-128 networks perform best. A linear SVM trained on features from these networks achieved significantly higher classification accuracy in comparison to the previous state-of-the-art. The features from pooling and convolutional layers were found to be more accurate than the fully connected layers. Fine-tuning the networks by replacing the top fully connected layer gave further improvements in multi-class classification accuracy.</p><p>Previous studies have assessed their algorithms using binary and multi-class classification metrics. We propose that it is more suitable to treat KL grades as a continuous variable and assess accuracy using mean squared error. This approach allows the model to be trained using regression loss so that errors are penalized in proportion to their severity, producing more accurate predictions. This approach also has the nice property that the predictions can fall between grades, which aligns with a continuous disease progression.</p><p>Future work will focus on improving knee joint detection accuracy using a CNN or region-based CNN instead of the proposed linear model on Sobel gradients, and on further improving assessment of knee OA severity. It is clear that the distribution of images in ImageNet and those of knee radiographs are very different. Given a large number of training examples, it would be possible to train a model from scratch on the knee OA images, which would likely be better adapted to the domain. In the absence of a large number of labeled examples, semi-supervised approaches such a ladder networks <ref type="bibr" target="#b18">[19]</ref> may prove more effective than the domain adaptation approach used here. Currently, the detection of knee joints, feature extraction, and classification/regression are separate steps. Future work will also investigate an end-to-end deep learning system by combining these steps.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A few samples of bilateral PA fixed flexion knee OA radiographs.</figDesc><graphic coords="2,50.46,50.54,231.34,128.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The KL grading system to assess the severity of knee OA. Source: http://www.adamondemand.com/clinical-management-of-osteoarthritis/</figDesc><graphic coords="2,313.49,50.54,231.31,128.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Detecting the knee joint centers and extracting the knee joints.</figDesc><graphic coords="3,63.32,50.54,205.62,102.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Learning curves for training and validation loss (left) and validation accuracy (right) during fine-tuning.</figDesc><graphic coords="4,429.15,611.89,125.52,87.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I CLASSIFICATION</head><label>I</label><figDesc>METRICS OF THE SVM FOR DETECTION.</figDesc><table><row><cell>Class</cell><cell cols="2">Precision Recall</cell><cell>F 1 score</cell></row><row><cell>Positive</cell><cell>0.93</cell><cell>0.84</cell><cell>0.88</cell></row><row><cell>Negative</cell><cell>0.95</cell><cell>0.98</cell><cell>0.96</cell></row><row><cell>Mean</cell><cell>0.94</cell><cell>0.94</cell><cell>0.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF AUTOMATIC DETECTION USING THE TEMPLATE MATCHING AND THE PROPOSED METHOD BASED ON JACCARD INDEX (J).</figDesc><table><row><cell>Method</cell><cell>J = 1</cell><cell>J ≥ 0.5</cell><cell>J &gt; 0</cell></row><row><cell>Template Matching</cell><cell>0.3 %</cell><cell>8.3 %</cell><cell>54.4 %</cell></row><row><cell>Proposed Method</cell><cell>1.1 %</cell><cell>38.6 %</cell><cell>81.8 %</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III CLASSIFICATION</head><label>III</label><figDesc>ACCURACY (%) ACHIEVED BY THE WNDCHRM AND PRE-TRAINED CNN FEATURES.</figDesc><table><row><cell></cell><cell>Classification</cell><cell>Wndchrm</cell><cell cols="3">VGG 16-Layers Net</cell><cell cols="3">VGG-M-128 Net</cell><cell cols="3">BVLC ref CaffeNet</cell></row><row><cell></cell><cell></cell><cell></cell><cell>fc7</cell><cell cols="2">pool5 conv5 2</cell><cell>fc6</cell><cell cols="2">pool5 conv4</cell><cell>fc7</cell><cell cols="2">pool5 conv5</cell></row><row><cell></cell><cell>Grade 0 vs Grade 1</cell><cell>51.5</cell><cell>56.3</cell><cell>61.3</cell><cell>63.5</cell><cell>56.5</cell><cell>63.2</cell><cell>64.7</cell><cell>62.0</cell><cell>64.3</cell><cell>63.3</cell></row><row><cell>Progressive</cell><cell>Grade 0 vs Grade 2 Grade 0 vs Grade 3</cell><cell>62.6 70.6</cell><cell>68.6 86.4</cell><cell>74.3 91.4</cell><cell>76.7 92.4</cell><cell>67.8 88.5</cell><cell>75.5 90.2</cell><cell>77.6 92.9</cell><cell>69.6 87.9</cell><cell>73.6 92.5</cell><cell>73.9 91.5</cell></row><row><cell></cell><cell>Grade 0 vs Grade 4</cell><cell>82.8</cell><cell>98.1</cell><cell>98.6</cell><cell>99.3</cell><cell>98.8</cell><cell>99.3</cell><cell>99.2</cell><cell>98.5</cell><cell>99.4</cell><cell>99.1</cell></row><row><cell></cell><cell>Grade 1 vs Grade 2</cell><cell>48.8</cell><cell>60.0</cell><cell>64.7</cell><cell>67.3</cell><cell>57.9</cell><cell>63.5</cell><cell>65.3</cell><cell>61.2</cell><cell>65.8</cell><cell>62.8</cell></row><row><cell>Successive</cell><cell>Grade 2 vs Grade 3</cell><cell>54.5</cell><cell>69.8</cell><cell>76.4</cell><cell>77.0</cell><cell>73.0</cell><cell>77.3</cell><cell>79.0</cell><cell>70.3</cell><cell>78.1</cell><cell>77.1</cell></row><row><cell></cell><cell>Grade 3 vs Grade 4</cell><cell>58.6</cell><cell>85.2</cell><cell>88.8</cell><cell>90.0</cell><cell>85.0</cell><cell>90.4</cell><cell>91.2</cell><cell>87.4</cell><cell>91.6</cell><cell>91.4</cell></row><row><cell></cell><cell>Grade 0 to Grade 2</cell><cell>39.9</cell><cell>51.1</cell><cell>53.4</cell><cell>56.9</cell><cell>51.1</cell><cell>55.0</cell><cell>57.4</cell><cell>51.1</cell><cell>54.8</cell><cell>54.4</cell></row><row><cell>Multi-class</cell><cell>Grade 0 to Grade 3</cell><cell>32.0</cell><cell>44.6</cell><cell>48.7</cell><cell>53.9</cell><cell>45.4</cell><cell>50.2</cell><cell>53.3</cell><cell>46.9</cell><cell>51.6</cell><cell>50.2</cell></row><row><cell></cell><cell>Grade 0 to Grade 4</cell><cell>28.9</cell><cell>42.6</cell><cell>47.6</cell><cell>53.1</cell><cell>43.8</cell><cell>49.5</cell><cell>53.4</cell><cell>44.1</cell><cell>50.8</cell><cell>50.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV CLASSIFICATION</head><label>IV</label><figDesc>ACCURACY (%) ACHIEVED WITH THE FEATURES EXTRACTED FROM FINE-TUNED BVLC NET.</figDesc><table><row><cell>Classification</cell><cell cols="3">Before Fine-Tuning</cell><cell cols="3">After Fine-Tuning</cell></row><row><cell></cell><cell>fc7</cell><cell cols="2">pool5 conv5</cell><cell>fc7</cell><cell cols="2">pool5 conv5</cell></row><row><cell cols="2">Grade 0 vs Grade 1 62.0</cell><cell>64.3</cell><cell>63.3</cell><cell>63.3</cell><cell>64.3</cell><cell>61.9</cell></row><row><cell cols="2">Grade 0 vs Grade 2 69.6</cell><cell>73.6</cell><cell>73.9</cell><cell>76.3</cell><cell>77.2</cell><cell>74.1</cell></row><row><cell cols="2">Grade 0 vs Grade 3 87.9</cell><cell>92.5</cell><cell>91.5</cell><cell>96.7</cell><cell>96.0</cell><cell>96.3</cell></row><row><cell cols="2">Grade 0 vs Grade 4 98.5</cell><cell>99.4</cell><cell>99.1</cell><cell>99.8</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell cols="2">Grade 1 vs Grade 2 61.2</cell><cell>65.8</cell><cell>62.8</cell><cell>63.3</cell><cell>66.7</cell><cell>62.7</cell></row><row><cell cols="2">Grade 2 vs Grade 3 70.3</cell><cell>78.1</cell><cell>77.1</cell><cell>85.8</cell><cell>83.9</cell><cell>83.3</cell></row><row><cell cols="2">Grade 3 vs Grade 4 87.4</cell><cell>91.6</cell><cell>91.4</cell><cell>94.4</cell><cell>93.6</cell><cell>92.6</cell></row><row><cell>Grade 0 to Grade 2</cell><cell>51.1</cell><cell>54.8</cell><cell>54.4</cell><cell>57.4</cell><cell>57.0</cell><cell>52.0</cell></row><row><cell>Grade 0 to Grade 3</cell><cell>46.9</cell><cell>51.6</cell><cell>50.2</cell><cell>57.2</cell><cell>56.5</cell><cell>51.8</cell></row><row><cell>Grade 0 to Grade 4</cell><cell>44.1</cell><cell>50.8</cell><cell>50.0</cell><cell>57.6</cell><cell>56.2</cell><cell>51.8</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGMENT This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/12/RC/2289. The OAI is a public-private partnership comprised of five contracts (N01-AR-2-2258; N01-AR-2-2259; N01-AR-2-2260; N01-AR-2-2261; N01-AR-2-2262) funded by the National Institutes of Health, a branch of the Department of Health and Human Services, and conducted by the OAI Study Investigators. Private funding partners include Merck Research Laboratories; Novartis Pharmaceuticals Corporation, GlaxoSmithKline; and Pfizer, Inc. Private sector funding for the OAI is managed by the Foundation for the National Institutes of Health.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully automatic quantification of knee osteoarthritis severity on plain radiographs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Oka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Akune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mabuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yoshida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kawaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Osteoarthritis and Cartilage</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1300" to="1306" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Early detection of radiographic knee osteoarthritis using computer-aided analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Osteoarthritis and Cartilage</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1307" to="1312" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diagnosis of osteoarthritis: Imaging</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Braun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bone</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="288" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Source code for biology and medicine</title>
		<author>
			<persName><forename type="first">L</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Eckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Macura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Source code for biology and medicine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Wnd-charm: Multi-purpose image classification using compound image transforms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Orlov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Macura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Eckley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1684" to="1693" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Feature Engineering in Fine-Grained Image Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>University of Washington</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via sparse hierarchical representations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scalable feature learning</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep feature learning for knee cartilage segmentation using a triplanar convolutional neural network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prasoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lauze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<biblScope unit="page" from="246" to="253" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Recognizing image style</title>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Trentacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Winnemoeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.3715</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A practical mri grading system for osteoarthritis of the knee: association with kellgren-lawrence radiographic scores</title>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Jeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European journal of radiology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="117" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A library for large linear classification</title>
		<author>
			<persName><forename type="first">R.-E</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with ladder networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3532" to="3540" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
