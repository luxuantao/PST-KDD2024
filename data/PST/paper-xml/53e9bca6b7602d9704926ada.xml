<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Silhouette-based gesture and action recognition via modeling trajectories on Riemannian shape manifolds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2010-12-17">17 December 2010</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><forename type="middle">F</forename><surname>Abdelkader</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wael</forename><surname>Abd-Almageed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anuj</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution" key="instit1">Florida State University</orgName>
								<orgName type="institution" key="instit2">FSU</orgName>
								<address>
									<postCode>106D OSB, 32306</postCode>
									<settlement>Tallahassee</settlement>
									<region>FL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Electrical and Computer Engineering</orgName>
								<orgName type="department" key="dep2">Center for Automation Research</orgName>
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Silhouette-based gesture and action recognition via modeling trajectories on Riemannian shape manifolds</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2010-12-17">17 December 2010</date>
						</imprint>
					</monogr>
					<idno type="MD5">22A26DA1547E9652D42E311ABABF365A</idno>
					<idno type="DOI">10.1016/j.cviu.2010.10.006</idno>
					<note type="submission">Received 28 February 2010 Accepted 1 October 2010</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Gesture recognition Action recognition Riemannian manifolds Shape space Silhouette-based approaches</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper addresses the problem of recognizing human gestures from videos using models that are built from the Riemannian geometry of shape spaces. We represent a human gesture as a temporal sequence of human poses, each characterized by a contour of the associated human silhouette. The shape of a contour is viewed as a point on the shape space of closed curves and, hence, each gesture is characterized and modeled as a trajectory on this shape space. We propose two approaches for modeling these trajectories. In the first template-based approach, we use dynamic time warping (DTW) to align the different trajectories using elastic geodesic distances on the shape space. The gesture templates are then calculated by averaging the aligned trajectories. In the second approach, we use a graphical model approach similar to an exemplar-based hidden Markov model, where we cluster the gesture shapes on the shape space, and build non-parametric statistical models to capture the variations within each cluster. We model each gesture as a Markov model of transitions between these clusters. To evaluate the proposed approaches, an extensive set of experiments was performed using two different data sets representing gesture and action recognition applications. The proposed approaches not only are successfully able to represent the shape and dynamics of the different classes for recognition, but are also robust against some errors resulting from segmentation and background subtraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problems of modeling and recognition of human gestures and actions from a sequence of images have received considerable interest in the computer vision community, as one of the many problems that aim at achieving a high level automated understanding of video data. This interest is motivated by many applications in different areas in human-computer interaction <ref type="bibr" target="#b0">[1]</ref>, robotics <ref type="bibr" target="#b1">[2]</ref>, security, and multimedia analysis.</p><p>Existing literature in human movement visual analysis (see reviews <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>) uses the terms gesture and action interchangeably to refer to sequences of human poses corresponding to different activities. However, there are slight variations among these terms. Gesture recognition from video refers to the problem of modeling and recognizing full body gestures performed by an individual in the form of a sequence of body poses and captured by a video camera. These gestures are typically used to communicate certain control commands and requests to a machine equipped with vision capabilities. This scenario usually arises in applications such as human-computer interaction and robotics. On the other hand, action recognition refer to the more general case of modeling and recognizing different human actions-such as walking, running, jumping, etc., performed under different scenarios and conditions. This problem is more prominent in applications such as smart surveillance and media indexing. The main difference between the two problems is that in the former the human subject can be more cooperating, which reduces the need for building view-invariance into the models. The models proposed in this paper can be used for either of the two problems. This is demonstrated by using two different experimental datasets for gesture and action recognition respectively. We will use the term gesture to refer to both gestures and simple actions throughout the paper.</p><p>Many of the existing approaches for gesture recognition model gestures as a temporal sequence of feature points representing the human pose at each time instant. The choice of these features usually depends on the application domain, image quality or resolution, and computational constraints. Features such as exemplar key frames <ref type="bibr" target="#b6">[7]</ref>, optical flow <ref type="bibr" target="#b7">[8]</ref> and feature points trajectories <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> have been frequently used to represent the raw, highdimensional video data in several approaches. The major challenge of most of these features is that they require highly accurate low-level processing tasks such as tracking of interest points. This accuracy turns out to be very hard to achieve in gesture recognition scenarios because of fast articulation, self-occlusion, and different resolution levels that are encountered in different applications.</p><p>In order to overcome this limitation, silhouette-based approaches have been receiving increasing attention recently <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. These approaches focus on the use of the shape of the binary silhouette of the human body as a feature for gesture recognition. They rely on the observation that most human gestures can be recognized using only the shape of the outer contour of the body as shown in Fig. <ref type="figure" target="#fig_0">1</ref> for a ''Turn Right'' control gesture. The most important advantage of these features is being easy to extract from the raw video frames using object localization and background subtraction algorithms, which are low-level processing tasks and relatively higher accuracy can be achieved in these tasks under different conditions.</p><p>An important question in silhouette-based approaches is: how can we represent the shape of these silhouettes in an efficient and robust way? Several shape representation features have been used in the literature for this purpose, including chain codes <ref type="bibr" target="#b13">[14]</ref>, Fourier descriptors <ref type="bibr" target="#b14">[15]</ref>, shape moments <ref type="bibr" target="#b15">[16]</ref>, and shape context <ref type="bibr" target="#b16">[17]</ref>. For most of these features, the feature vector is treated as a vector in a Euclidean space in order to use standard vector space methods for modeling and recognition. This assumption is not usually valid as these feature lie in a low-dimensional, non-Euclidean space. Working directly on these nonlinear manifolds can provide models and discriminative measures that may result in an improved performance. One way to explore this lower-dimensional space is to try to learn its structure from training data using dimensionality reduction techniques combined with a suitable notion of local discriminative measure between the visual data features. This technique was recently used <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13]</ref> for human action recognition and pose recovery. The problems with this technique come from the limitations of data-driven manifolds such as a lack of robust statistical models, and the difficulty in extrapolation and matching of new data.</p><p>The limitations of data-driven manifolds methods noted above have shifted the attention of many computer vision researchers towards the use of analytic differential geometry. This shift was also supported by the fact that many features in computer vision lie on curved space because of the geometric nature of the problems. Several of these manifolds were used in problems like object detection and tracking <ref type="bibr" target="#b19">[20]</ref>, affine invariant shape clustering <ref type="bibr" target="#b20">[21]</ref>, and activity modeling <ref type="bibr" target="#b21">[22]</ref>. The use of such manifolds offers a wide variety of statistical and modeling tools that arise from the field of differentiable geometry. These tools have found applications in problems such as target recognition <ref type="bibr" target="#b22">[23]</ref>, parameter estimation <ref type="bibr" target="#b23">[24]</ref>, clustering and dimensionality reduction <ref type="bibr" target="#b24">[25]</ref>, classification <ref type="bibr" target="#b25">[26]</ref>, and statistical analysis <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>The choice of the right feature and space to model the shape of the silhouettes is not the only issue in silhouette-based methods. An equally important problem is the efficient modeling of the dynamics of temporal variations of these feature as the gesture progresses. The importance of both shape and dynamic cues for modeling human movement was noted and demonstrated experimentally in <ref type="bibr" target="#b27">[28]</ref>. Various models were used for modeling these dynamics, ranging between statistical generative models <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>, and the more recent discriminative models <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. The invariance to temporal rate of execution of action in such models is crucial for achieving accurate recognition <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation and overview of approach</head><p>In this paper, we explore the use of shape analysis on manifolds for human actions and gesture recognition. Our approach falls into the category of the silhouette-based approaches described earlier. Each silhouette is represented by a planar closed curve corresponding to the contour of this silhouette, and we are interested in evolving shapes of these curves during actions and gestures. We will use a recent approach for shape analysis <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>, that uses differential geometric tools on the shape spaces of closed curves. Similar ideas have also been presented in <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref>. While there are several ways to analyze shapes of closed curves, an elastic analysis of the parameterized curves is particularly appropriate in this application. This is because: (1) the elastic matching of curves allows nonlinear registration and improved matching of features (e.g. body parts) across silhouettes, (2) this method uses a square-root representation under which the elastic metric reduces to the standard L 2 met- ric and thus simplifies the analysis, and (3) under this metric the re-parameterizations of curves do not change Riemannian distances between them and thus help remove the parametrization variability from the analysis. Furthermore, such geometric approaches are useful because they allow us to perform intrinsic statistical analysis tasks, such as shape modeling and clustering, on such Riemannian spaces <ref type="bibr" target="#b41">[42]</ref>.</p><p>Using a square-root representation of contours, each human gesture is transferred into a sequence of points on the shape space of closed curves. Thus, the problem of action recognition becomes a problem of modeling and comparing dynamical trajectories on the shape space. We propose two different approaches to model these trajectories.</p><p>In the first approach, we propose a template-based approach to learn a unique template trajectory representing each gesture. One of the main challenges in template-based method is to account for variation in temporal execution rate. To deal with this problem, we use a modified version of the Dynamic Time Warping (DTW) algorithm to learn the warping functions between the different realizations of each gesture. We use the geodesic distances on the shape space to match different points on the trajectories in order to learn the warping functions. An iterative approach is then used to learn a mean trajectory on the shape space and to compute the temporal warping functions.</p><p>In the second approach, we utilize the geometry of the shape space more efficiently in order to cope with the different variations within each gesture caused by changes in execution style, body shape and noise. Each gesture is modeled as a Markov model to represent the transition among different clusters on the shape space of closed curve. We learn these models by decoupling the problem into two stages. In the first stage, we cluster the individual silhouette shapes using the Affinity Propagation (AP) clustering technique <ref type="bibr" target="#b42">[43]</ref>, and build statistical model of variation within each cluster. In the second stage, a Hidden Markov Model (HMM) is used to learn the transition between different clusters for each gesture.</p><p>Extensive experiments were conducted to test the performance of our algorithms. We used two different data sets of video sequences representing different control gestures and regular actions, with a total of 226 video sequences. The data sets contained many variations in terms of the number of subjects, execution styles, and temporal execution rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Contributions</head><p>Our contribution in this work can be summarized as 1. Posing the problem of gesture and action recognition as one of classifying the trajectories on a Riemannian shape space of closed curves. 2. Proposing a template-based model and a Markovian graphical model for modeling the time-series data of points on the shape manifold. These models were designed to fully adhere to the geometry of the manifold and to model the statistical variation of the data on this manifold. 3. Provide a comprehensive set of experimental analysis of the proposed models on two different datasets for gesture and action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Organization</head><p>The reminder of this paper is organized as follows: Section 2 reviews some of the work in different areas related to this paper. In Section 3, we review some notations from Riemannian geometry and then describe the square-root representation of closed curves and the resulting shape space. We also give a brief overview of the computation of distances and statistics on this manifold. Section 4 describes the two dynamical model approaches used for gesture modeling. Experimental results validating the proposed method for human gesture recognition are introduced in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The problems of gesture and action recognition have received great attention in the literature. Several survey papers <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> have tried to group and analyze the existing body of work. The reader is referred to these review papers for a complete overview of the related work in the field. Meanwhile, we will give a brief review of some of the approaches in three different areas that are of most relevance to this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Gesture and action modeling</head><p>Wang and Suter recently proposed two approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13]</ref> for modeling motion subspaces associated with human actions represented by silhouettes. In the first approach <ref type="bibr" target="#b33">[34]</ref>, they represent the silhouettes using block based features, where each block is represented by the normalized value of the intensity values within the block. They then use the KPCA technique to learn a nonlinear subspace of the action data. The dynamics of the action is then modeled using a discriminative factorial conditional random field model (FCRF). In the second approach <ref type="bibr" target="#b12">[13]</ref>, they use features based on distance transform (DT) and use local preserving projections (LPP) to project the feature data onto the lower-dimensional manifold. They use a simpler model for recognition based on comparing the trajectories on the embedding space. In both approaches, the main assumption was that the features will have an inherently low-dimensional structure, and that data-driven nonlinear dimensionality techniques will successfully capture this structure.</p><p>Another related class of approaches for silhouette-based action recognition is called exemplar-based modeling <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, where the different features corresponding to gestures are clustered into a set of exemplars or key poses, and the dynamics of the action are learnt as a set of transitions between these exemplars. A recent example of these approaches is <ref type="bibr" target="#b45">[46]</ref>, where each gesture is represented using a sequence of shape-motion prototypes. The training data is used to build a prototype tree in the joint shape and motion space via hierarchical k-means clustering. A k-NN classifier is used to classify gestures based on the frame to prototype distance of the test sequence. Exemplar-based hidden Markov models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48]</ref> were used in <ref type="bibr" target="#b43">[44]</ref> to model body gestures using Chamfer distance as a matching measure between different poses. The same tool was used in <ref type="bibr" target="#b44">[45]</ref> for view invariant modeling of actions using 3D exemplars. Our technique for learning the graphical model for trajectories presented in Section 4, comes under this category, if we consider the different clusters as exemplar points on the shape space. However, we use the Riemannian structure of the manifold to build statistical models around these exemplars. This enables us to perform the statistical inference task more efficiently than using only the exemplars.</p><p>In robotics and human-robot interaction domains, several others aspects of the gesture recognition problem are more demanding. In <ref type="bibr" target="#b48">[49]</ref>, problems like gesture spotting and interpretation from a continues sequence of unsegmented video sequence were handled using HMMs. A special transition gesture was modeled to account for transition among different learnt gesture models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Shape spaces in computer vision</head><p>Among the different geometric manifolds that are of interest in computer vision applications, shape spaces have received the most attention in the literature. The goal of these spaces is building tools for shape detection, tracking and analysis that are invariant to different shape-preserving transformation groups. The work in this area can be classified into two main categories. The first category is based on landmark-based analysis, where the shape of the object is presented by a discrete sampling of the object contour <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. This formulation was used to represent shape spaces that are invariant under transformation groups such as similarity <ref type="bibr" target="#b51">[52]</ref>, affine <ref type="bibr" target="#b52">[53]</ref>, and projective transformation <ref type="bibr" target="#b53">[54]</ref>. The limitation with this class of approaches is its dependence on automatic detection and tracking of landmark points. A simple landmark-based approach is Active Shape Model (ASM) <ref type="bibr" target="#b54">[55]</ref>, that assumes that shape belongs to a Euclidean space and uses principal component analysis (PCA) of the landmark location to model the shape variability within that space. Although this approach has found many applications because of its simplicity, it ignores the nonlinear geometry of the shape space which limits its representative power under different geometric transformations. The second category of shape spaces can be linked to Grenander's formulation <ref type="bibr" target="#b55">[56]</ref>, where shapes are modeled as points on infinite-dimensional manifolds, and the variations between the shapes are modeled by the actions of Lie-groups (diffeomorphisms) on these manifolds. This class of approaches avoid the problems associated with landmark tracking. However it suffers from the high computational cost of such highdimensional models.</p><p>Shape spaces have also been used for activity recognition in several computer vision publications. Kendall's statistical shape theory was used to model the interactions of a group of people and objects in <ref type="bibr" target="#b56">[57]</ref>, as well as the motion of individuals <ref type="bibr" target="#b27">[28]</ref>. In both approaches, assuming the stationarity of the shape sequence of an activity, the authors project the sequence of shape points into the tangent plane at the mean shape. The dynamics of the projected points are then modeled using regular vector space approaches on this single tangent plane. In a more recent work <ref type="bibr" target="#b57">[58]</ref>, a non stationary model was proposed. It builds an auto regressive (AR) process on the shape space represented by a sequence of tangent planes. They use an alignment step to align the tangent planes at two consecutive time steps. The proposed model was used for tracking of the shape using a particle filter algorithm and for action synthesis. There were no experimental results for using this approach for action classification and recognition.</p><p>Although landmark-based shape space methods have proven successful in several vision problems, the performance usually relies on accurate detection and tracking of key landmark points. This turns out to be a very challenging task especially under rapid motion and self-occlusion, which are common in gesture recognition problems. For this reason, instead of using landmarkbased shape representation and space, our approach models the shape of the contour at each frame using a closed curve representation.</p><p>Recently, there has been a growing interest in building mathematical representations and metrics for modeling the shapes of closed curves. We will employ one of the recent approaches <ref type="bibr" target="#b58">[59]</ref>, which uses differential geometry tools for analyzing the space of closed curves using elastic string models. Similar ideas have also been presented in <ref type="bibr" target="#b40">[41]</ref>. Using Riemannian analysis of such space, statistical models and efficient methods were built to perform tasks such as shape modeling and clustering <ref type="bibr" target="#b41">[42]</ref>. Later, the same methods were combined with a square-root representation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b37">38]</ref> to construct a shape space of closed curves in R 2 and to compute geodesics between 2D and 3D closed curves. Invariance to different shape preserving geometric transformation and reparametrization were incorporated into the computation of geodesics between shapes and subsequently used for shape analysis of curves in <ref type="bibr" target="#b36">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Manifold representation of silhouettes</head><p>As mentioned earlier, we will use the square-root elastic representation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> to construct a shape space of closed curves in R 2 . Under this framework, the contour of the human silhouette in each frame represents a point in the shape space and each gesture represents a temporal trajectory on that space. We then use principles from Riemannian geometry combined with the structure of the shape space to build statistical models for these trajectories for representation and recognition.</p><p>For the sake of completeness, we will first review some of the general notations in Riemannian geometry. We will then briefly introduce the reader to the square-root elastic representation of curves and some properties of the space of curves under such representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Review of Riemannian geometry</head><p>We will briefly review the basics of Riemannian geometry <ref type="bibr" target="#b59">[60]</ref>. A topological manifold is a set that is, among other things, locally Euclidean. This means that for an n-dimensional manifold M, there exists a set of neighborhoods that cover M and that are homeomorphic to corresponding open sets in R n . If these local mappings are diffeomorphic, and the different neighborhoods are (maximally) smoothly compatible, then M is a differentiable manifold. Fig. <ref type="figure" target="#fig_1">2</ref> shows an example of a two-dimensional manifold in R 3 . The (infinite-dimensional) Hilbert manifold is a set that is locally diffeomorphic to an (infinite-dimensional) Hilbert space. The tangent space T x M at each point x 2 M is the vector space that contains the velocity vectors of all the differentiable curves on M passing through x, at x. A Riemannian metric on a manifold M is an inner product h., .i x on the tangent space T x M that varies smoothly with x. The norm of the vector v 2 T x M is given by kvk = hv, vi x (1/2) . If a : ½0; 1 ! M is a differentiable path in M, then its length is given by L½a ¼ R 1 0 k _ aðtÞkdt. The Riemannian distance between two points x; y 2 M, denoted as d(x, y), is defined as the minimum length over all paths on the manifold between x and y.</p><p>A geodesic path is a path that locally minimizes the length between points.</p><p>Given a tangent vector t 2 T x M, there exists a locally unique geodesic, c t (t), starting at x with t as its initial velocity and traveling with constant speed. The Riemannian exponential map, exp x : T x M ! M maps a tangent vector t to a point on the manifold that is reached in unit time by the geodesic c t (t). The inverse of exp x is known as the logarithm map and is denoted by log x : M ! T x M. Thus, for a point y in the domain of log x , the geodesic distance between x and y is given by dðx; yÞ ¼ klog x ðyÞk: ð1Þ</p><p>As explained in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, the exponential map realizes a chart called the the exponential chart. In this chart, geodesics starting from x are straight lines, and the distance from the development point are preserved. This chart is considered the ''most linear'' chart of the manifold with respect to the point x.</p><p>In order to build efficient and robust models of trajectories on Riemannian manifolds, we need to use some tools for statistical analysis of points on Riemannian manifolds. We start with calculating the intrinsic mean and principal components for a set of points x 1 ; . . . ; x n 2 M that lie on a sufficiently small neighborhood on M.</p><p>The intrinsic mean or Karcher mean l is defined as a local minimizer in M of the sum-of-squared Riemannian distances to each point. Thus, this intrinsic mean is given by</p><formula xml:id="formula_0">l ¼ argmin x2M X N i¼1 dðx; x i Þ 2 ;<label>ð2Þ</label></formula><p>where d(., .) is the Riemannian geodesic distance defined in <ref type="bibr" target="#b0">(1)</ref>. A gradient descent approach was proposed in <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref> to solve this minimization problem. Building high order statistical models directly on Riemannian manifolds is rather difficult. This is mainly due to the non-linearity of these manifolds. So a common approach is to build the statistical model on the tangent plane of the manifold at some reference point, which is a vector space and hence more conventional statistics can be applied. Usually, the reference point is chosen to be the mean point of the set of samples of the distribution. Under the assumption that the data points lie in a small neighborhood about the mean point l, it was shown in <ref type="bibr" target="#b62">[63]</ref> that solving for the principal geodesic components, the equivalent of principal component vectors in Euclidean space, boils down to performing PCA for the tangent vectors log l ðx i Þ 2 T l M. This method is called principal geodesic analysis (PGA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Representation and shape space of closed curves</head><p>We represent the shape of a contour curve b, parameterized arbitrary, by its square-root velocity function: q : S 1 ! R 2 , where</p><formula xml:id="formula_1">qðtÞ ¼ _ bðtÞ ffiffiffiffiffiffiffiffiffiffiffiffiffi k _ bðtÞk q ;<label>ð3Þ</label></formula><p>where k.k is the Euclidean norm in R 2 , and t is an arbitrary coordinate on S 1 . The quantity kq(t)k is the square-root of the instantaneous speed, and the ratio q(t)/kq(t)k represents the instantaneous direction along the curve. The curve b can be recovered within a translation, using bðtÞ ¼ R t 0 kqðsÞkds. Let C be the set of all unit-length, closed planar curves that are represented by their square-root velocity functions. The closure condition states that the square-root representation function q for closed curves should satisfy the condition that R 1 0 qðtÞkqðtÞkdt ¼ 0. If we do not impose the closure condition then this set is a Hilbert sphere; with the closure constraint C is a submanifold of the unit sphere. C is called a pre-shape space.</p><p>The geodesics between points in C are computed using a pathstraightening approach. In this approach, first introduced in <ref type="bibr" target="#b63">[64]</ref>, the geodesic path between the two points is first initialized with an arbitrary path. Then, this path is iteratively straightened using a gradient approach and the limit point of this algorithm is a geodesic path. To be effective for shape analysis, the representation and the geodesics between the points must be invariant to shape-preserving transformations. These transformations include pose (rotation, scale, and location) and the parameterizations of the curves. Note that the square-root velocity representation is already invariant to scale and translation. The remaining two are studied as groups acting on C: rotation by the action of SO(2) and re-parametrization by the action of D, where D ¼ fc : S 1 ! S 1 g is the space of all orientation-preserving diffeomorphisms. The resulting (transformation invariant) shape space S is defined as a quotient space as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S ¼ C=ðSOð2Þ Â DÞ: ð4Þ</head><p>Elements of this quotient space are equivalence classes of the type:</p><formula xml:id="formula_2">½q ¼ Oðq cÞ ffiffiffi _ c q jO 2 SOð2Þ; c 2 D :</formula><p>These are the orbits of SOð2Þ Â D in C and each such orbit represents a shape uniquely. The problem of finding the geodesics between two shapes, or two orbits, relies on first solving the optimization problem:</p><formula xml:id="formula_3">dð½q 0 ; ½q 1 Þ ¼ min O2SOð2Þ;c2D d q 0 ; Oðq cÞ ffiffiffi _ c q :<label>ð5Þ</label></formula><p>This is iteratively solved using a gradient descent approach that finds these optimal transformation parameters. Once the optimal rotation and re-parameterization are obtained, one can draw a geodesic path between q 0 and O</p><formula xml:id="formula_4">Ã ðq c Ã Þ ffiffiffiffi ffi _ c Ã</formula><p>p in C using path straightening. This results in a geodesic path between the orbits [q 0 ] and [q 1 ] in the shape space S. The reader is referred to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b38">39]</ref> for more details on algorithms for finding shape geodesics. In the remainder of the paper, we will use q to denote the equivalence class [q] to reduce notation.</p><p>In order to build models for gesture and action recognition on the described shape space, one needs to build statistical tools to characterize the different distribution of the data samples and to cope with random noise and errors in low-level processing. The simplest statistical measure, represented by the mean of a group of points in S was presented in <ref type="bibr" target="#b38">[39]</ref>. It uses the Karcher mean formulation as presented in <ref type="bibr" target="#b1">(2)</ref>, to compute the intrinsic mean of a set of points in S.</p><p>Higher order statistical models for shapes in S were presented in <ref type="bibr" target="#b41">[42]</ref>. In that approach, all the sample points are projected to the tangent plane T l ðSÞ 2 L 2 , where l is the mean shape of these sample points. In order to overcome the infinite-dimensionality of S, they propose to approximate the tangent function g 2 T l ðSÞ by a finite basis; in this case this finite basis comes from Fourier analysis. They further reduce the dimensionality on this vector space by assuming that the data lies on a lower-dimensional subspace, and use PCA on the tangent plane to learn this subspace and project all the points into it. The projected points on the lowdimensional subspace can then be modeled either in a parametric form as a multi-variate normal distribution, or in a non-parametric form using kernel density function. We refer the reader to <ref type="bibr" target="#b41">[42]</ref> for more details and experimental analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modeling gesture dynamics</head><p>Using the described shape model for closed curves, the dynamic sequence of shapes corresponding to a particular gesture or action will correspond to a sequence of m points on the form Q ¼ q 1 ; q 2 ; . . . ; q m ; ð6Þ</p><p>where q i 2 S for i = 1, 2, . . ., m, and S is the quotient shape space described in Section 3. We propose to use the trajectories on the shape space corresponding to these sequence as a feature for modeling and recognition of different gestures. Because of the special nature of the curved shape space, vector space methods can not be directly applied. Hence, a modified versions of these methods that obey the space geometry is proposed.</p><p>In this section, we present two approaches for modeling and recognition of these trajectories. The first is a template-based non-parametric approach, where a template model is learnt from different instances of the same gesture. In order to account for variations in the execution rates within different realizations, we apply an iterative approach utilizing the DTW approach <ref type="bibr" target="#b64">[65]</ref> to align the different trajectories temporarily and then learn an average template representation on the manifold for recognition. In the second approach, we build a parametric model by clustering the different points on the manifold into several clusters using the Affinity Propagation (AP) clustering technique <ref type="bibr" target="#b42">[43]</ref>. Each gesture trajectory is modeled as a sequence of Markovian jumps between some of these clusters. The transition probabilities of the Markov model are learnt using the standard forward-backward approach <ref type="bibr" target="#b28">[29]</ref>, while non-parametric statistical observation models are directly built on the Riemannian manifold.</p><p>In each of these approaches we address two different stages of the problem:</p><formula xml:id="formula_5">1. The learning problem: Given N labeled realizations Q 1 , Q 2 , . . .,</formula><p>Q N of a gesture, where each realization is a time series of shape points in the form shown in ( <ref type="formula">6</ref>), we would like to learn a nominal (or average) trajectory model of this gesture. 2. The classification problem: Given a test sequence Q and M different gesture template models. How can we classify the test sequence into one of these models?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Template-based model</head><p>In this model, we learn a template trajectory Q Ã for each gesture given the training realizations of this gesture. A usual choice is to compute Q Ã as the average of all the training trajectories at each time instants. This choice is optimal in terms of minimizing the sum of distances between the template and all of the training examples at each time instant.</p><p>This approach has two limitations. The first is the fact that trajectories are not of the same length due to variations in execution rates of the same gesture. This also results in a lack of temporal synchronization between different segments of these trajectories. Therefore, a temporal alignment of these trajectories is crucial before computing the template. The second issue is that all of the points on the different learning trajectories reside on the shape space S. Thus we need to make sure that both the temporal matching of shape points and computation of the template Q Ã follow the underlying geometry of that manifold.</p><p>In order to solve the temporal variation problem, we propose using the DTW algorithm <ref type="bibr" target="#b64">[65]</ref>, which is used to find the optimal non-linear warping function to match a given time-series to a template while adhering to certain restrictions such as the monotonicity of the mapping in the time domain. This approach is very popular in speech recognition and has been used in several vision applications such as shape averaging <ref type="bibr" target="#b65">[66]</ref>, and rate-invariant activity recognition <ref type="bibr" target="#b34">[35]</ref>. The optimization process is usually performed using dynamic programming approaches given a measure of similarity between the features of the two sequences at different time instants.</p><p>Adapting the DTW algorithm to features that reside on Riemannian manifolds is a straightforward task, since DTW can operate with any measure of similarity between the different temporal features. Hence, we use the geodesic distance between the different shape points d(q i , q j ) given in <ref type="bibr" target="#b4">(5)</ref> as a distance function between the shape features at different time instants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Iteratively learning the template trajectory on the manifold</head><p>Given the N labeled realization Q 1 , Q 2 , . . ., Q N of a certain gesture, we solve for the nominal shape sequence Q Ã 2 S that has the minimum distance to all the N realizations, while accounting for the different non-linear temporal warpings between these realization and Q Ã . This can be formulated as computing the average trajectory on the shape space S of all the warped versions of the N realizations as described next.</p><formula xml:id="formula_6">Q Ã ¼ AVG S ðf 1 ðQ 1 Þ; f 2 ðQ 2 Þ; . . . ; f N ðQ N ÞÞ:<label>ð7Þ</label></formula><p>where, AVG S stands for the average of the realization computed with respect to the shape manifold S, and f i (Q i ) represent the non-linear warped function that was calculated using DTW to match Q i to the template Q Ã . According to <ref type="bibr" target="#b6">(7)</ref>, given a set of warping functions f i , i = 1:N, we can solve for Q Ã . This is achieved by computing the intrinsic mean on S of all the warped sequences at each time instants. We use the Karcher mean on the manifold as described in Section 3 for this computation. However, a template Q Ã is needed in order to compute these warping functions.</p><p>We solve this dual problem in an iterative manner as shown in Algorithm 1 by iterating between computing the non-linear warping functions to a given template and updating the template as the average of the warped realizations.</p><p>The initialization of this algorithm is performed by choosing one of the N labeled realizations as the initial Q Ã . This choice can be done randomly. However, we found that better performance can be achieved if we choose the sequence that best represents all of the N labeled realization. This is the sequence that achieves the minimum average distance with all the labeled realizations when picked as a template and all the sequences are warped to it.</p><formula xml:id="formula_7">Algorithm 1. Learning gesture template Q Ã from N labeled reali- zation Q 1 , Q 2 , . . ., Q N 1: Initialize Q Ã to one of the N realizations 2: repeat 3:</formula><p>for n=1 to N do 4:</p><p>Find DTW to warp Q n to Q Ã 5:</p><p>end for 6:</p><p>Update Q Ã as the Karcher mean of all the N warped realizations 7: until Convergence or t times</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Classification of a test sequence</head><p>In the testing stage, we are given test shape sequence Q t and M different gesture templates. We want to classify Q t as one of these gestures. This classification problem is solved using the nearest neighbor rule, by warping all the models to Q t using the geodesic distance in S as a distance measure and computing the total warping distance to each of these models. Q t is then assigned to the model with the smallest warping distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Graphical-based statistical model</head><p>The simplicity of template-based approaches encouraged researchers to use it in many practical applications, where they have shown great success under conditions of small variations and low noise within the training data. Under these conditions, the mean can be sufficient to characterize the time-series. Unfortunately, in many other scenarios, these conditions do not hold. Hence, we need higher order statistical models to capture the variation within the time-series data.</p><p>Graphical models such as the Hidden Markov Models (HMM) have proven more effective than template-based methods in modeling time-series data, specially in areas such as speech processing <ref type="bibr" target="#b28">[29]</ref>. This is so because these models can learn more information about the special statistical variation of both the observation data and the dynamical transition between different states.</p><p>A standard HMM with continuous observation is shown in Fig. <ref type="figure" target="#fig_2">3</ref>. This is a statistical generative model that represent the time-series of observations in the form of a set of transitions between several abstract hidden states, where the state at time t is denoted by S t . The transition between these states is governed by a Markovian model parameterized by the state transition probability P(S t jS tÀ1 ). The observation at each time instant q t is statistically dependent on the state S t according to an observation probability density function f(q t jS t ). A common choice for this density function is a mixture of Gaussians. The model parameters are learnt using the standard Baum-Welch algorithm <ref type="bibr" target="#b66">[67]</ref>. This ensures that the state transition and observation density functions are optimized in the maximum likelihood sense.</p><p>To the best of our knowledge, most of the published work on graphical models such as HMM studies modeling time-series data on Euclidean spaces. However, several challenges appear if we want to generalize these methods to time-series data on special manifolds. One of the challenges is that in order to solve for the optimal parameters in the maximum likelihood sense, we need to provide an analytical form for the observation density functions and compute the gradient of the likelihood of a sequence in terms of the parameters of these functions.</p><p>In order to avoid the mathematical difficulty of directly learning a graphical model on the Riemannian manifold, we propose to decouple the two problems of learning the abstract state transitions and learning the observation distributions. This is accomplished by introducing a new layer of intermediate observations X t in the form illustrated in Fig. <ref type="figure" target="#fig_3">4</ref>. The resulting model in this case is similar to the decoupled exemplar-based HMM presented in <ref type="bibr" target="#b43">[44]</ref>. The difference is that we will learn these exemplars to represent different clusters of data on the shape manifold. We will also learn the observation density functions as a non-parametric density function for each cluster on that manifold.</p><p>In our approach we model the dynamics of each gesture as a Markov model of L hidden abstract states H (1) , H (2) , . . ., H (L) . Let S t denotes the state of the system at time t. The state of the system is emitting an intermediate discrete observation representing the cluster (exemplar) of the system and denoted by x t . This observation can take any value in the set X ¼ fx k jk ¼ 1; . . . Kg, where K is the number of the learnt clusters on the shape manifold. Hence, the system dynamics is determined by the state transition P(s t js tÀ1 ), and the exemplar observation probabilities P(x t js t ).</p><p>The final observation q t corresponds to the contour curve velocity function for a certain frame, and represents a point on the shape space S. These observations are modeled using a mixture distribution such that the observation probability for an observation q t at time t given the state of the system s t can ba calculated as</p><formula xml:id="formula_8">f ðq t js t Þ ¼ X K k¼1 f ðq t jx t ¼ x k ÞPðx t ¼ x k js t Þ ð<label>8Þ</label></formula><p>where, f(q t jx t = x k ) represents the non-parametric probability density function for cluster k on the shape space S.</p><p>Learning the model for each gesture includes learning the state transition probabilities P(S t jS tÀ1 ), the exemplar clusters observation probabilities P(x t jS t ), and the shape observation density functions given each cluster f(q t jx t = x k ). We follow a decoupling approach for learning these parameters. We first cluster the gesture shape points into several clusters and build a non-parametric statistical model for the data points in each cluster to represent the observation density function. The state transition and cluster observation probabilities are then learnt from the training samples using the standard forward-backward technique commonly used in applications that use discrete observation HMMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Clustering of gesture shapes</head><p>The clustering of the shape points to learn the exemplar clusters can be done directly on the quotient shape space S by using a stochastic simulated annealing approach to search for the optimal cluster assignment in the configuration space <ref type="bibr" target="#b41">[42]</ref>. However, this method requires a high computation burden and is not guaranteed to converge to a globally optimal solution. We use a simpler and computationally efficient AP clustering approach on the matrix of the pairwise geodesic distances to find the cluster assignment.</p><p>Affinity propagation is an unsupervised clustering technique presented by Frey and Dueck <ref type="bibr" target="#b42">[43]</ref>. It finds a set or exemplar points to represent the data and assigns every data point to one of these exemplars. The search for these exemplars is performed in an iterative manner using a message passing algorithm on a graph, where each data point is represented by a node on this graph. This method was proven superior to other clustering approaches in many applications such as clustering images of faces, detection of genes in micro-array data, and identification of cities that are efficiently accessed by airline travel. It was also shown to be more computationally efficient in terms of processing time and handling sparse data.</p><p>There are two main advantages of using AP for our problem. The first is that this method only requires a notion of similarity between every pair of data points and does not assume a Euclidean space. Therefore, we can use the negative of the geodesic distance between the shape points as a geometrically accurate similarity measure, resulting in accurate clustering on the shape manifold. The second advantage is that this method does not require any prior knowledge of the number of clusters and the final clustering is not dependent on the initialization as in k-means clustering.</p><p>Given all of the training data shape points, we run the AP algorithm on the matrix of the pairwise similarity between shapes, calculated as the negative of the geodesic distance between theses shapes on the shape space S. The output of the algorithm is a set of K exemplar shapes q 1 , q 2 , . . ., q K , and an assignment of each data point to one of these exemplar. An example of these exemplar shapes for different clusters of the control gesture data set is shown in Fig. <ref type="figure" target="#fig_4">5</ref>. We observe that some similar shapes may be split between two close clusters. This may be a problem with clustering. However, our HMM dynamic model is robust to these errors as each state can be associated with many exemplar states.  An important question in this step is the amount of data to use for learning these exemplar clusters. We have two choices. The first is to make these clusters gesture dependent by using the shape data corresponding to each gesture to learn a different set of clusters. The second choice is to perform the clustering on all the training data for all the gestures which result in a shared set of exemplars for all the different HMM's. Although the first choice may reduce the computational complexity as we need to calculate significantly fewer geodesic distances, we adopt the second approach for the following reasons. First, the physical nature of the problem suggests that usually many gestures share a set of poses which are similar or very close in shape. The second reason is that global clustering results in more data points per cluster which helps in providing a better estimate for the non-parametric observation density functions. Finally, the global clustering approach has also the intuitive idea of representing the whole manifold as a mixture of distributions representing our observation model. Our experimental analysis also proves the superiority of global clustering approach as will be shown in the result section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Building the observation model</head><p>Given the clustering result of the AP algorithm, we learn a nonparametric density function f(q t jx t = x k ) for each cluster x k . This density function captures the variability between the different samples of the cluster and determines the main mode of variation within each cluster. However, the learning of these density functions on the shape space is difficult, mainly because of the two problems of non-linearity and infinite-dimensionality of the shape space S.</p><p>We deal with these challenges using the method described in <ref type="bibr" target="#b41">[42]</ref> and summarized in 2. The problem of non-linearity is avoided by building these distributions on the tangent space T l k S to S at the mean shape of the cluster l k rather than on S itself. This approximation is valid because the data within the cluster are very close to each other and will be in a small neighborhood around l k . Therefore, for every shape q 2 S in the k th cluster we compute a tangent vector g 2 T l k S, where g represents the initial velocity of the geodesic path between the mean shape l k and the shape q.</p><p>The problem of dimensionality is solved by assuming that the variations in the tangent vectors g are mostly restricted to an m dimensional subspace. We use PCA to learn these low-dimensional subspaces for each cluster. Each of the m coefficients representing the low-dimensional projections g is then modeled with a nonparametric density function using kernel density estimation (with a Gaussian kernel) technique.</p><p>Algorithm 2. Learning the observation non-parametric density function for the k-th cluster f(q t jx t = x k ) from the clustered shape points q 1 , q 2 , . . ., q z 1: Compute the intrinsic mean l k for the z data points using (2) 2: for i=1 to z do 3:</p><p>Compute the tangent element g i 2 T k l ðSÞ, such that geodesics from l k reaches q i in unit time 4: end for 5: Perform a local PCA on the data points g 1 , g 2 , . . ., g z to learn an m-dimensional subspace 6: for i=1 to z do 7:</p><p>Project the data point g i into gi on the m-dimensional subspace learnt 8: end for 9: Learn a non-parametric density function for each of the m coefficients of gi ; i ¼ 1 : z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Learning the dynamical model</head><p>For each gesture, we want to train a HMM that captures the underlying dynamics of transition between the states (state transition probabilities P(S t jS tÀ1 )), and the intermediate (cluster) discrete observation probability P(x t jS t ). This is a standard problem in HMM and we solve it this using the Baum-Welch method for discrete output HMM <ref type="bibr" target="#b28">[29]</ref>.</p><p>Given N realization of each gesture, where each m-point realization is in the form Q ¼ q 1 ; q 2 ; . . . ; q m ; q i 2 S, we assign each of these points to one of the learnt clusters to obtain a sequence of cluster assignments x1 ; x2 ; . . . ; xm ; x 2 X. These sequence of clusters are used as discrete observation vectors in the Baum-Welch algorithm to learn the model for each gesture.</p><p>The way we assign each shape observation to a cluster can largely affect the performance and accuracy of the model. A simple choice for this assignment is to use a nearest-neighbor rule where the shape point is assigned to its nearest cluster, which is calculated by computing the geodesic distance between this point and all the cluster means, and picking the cluster with the minimum geodesic distance. This can be formulated in the following way</p><formula xml:id="formula_9">xi ¼ min K j¼1 dðq i ; l j Þ; for i ¼ 1 : m<label>ð9Þ</label></formula><p>where l j is the intrinsic mean for the jth cluster computed on the shape manifold, and d(., .) is the geodesic distance on S.</p><p>Although nearest-neighbor assignment performs relatively well, it sometimes produces wrong assignments as it only uses the mean point as a representative of each cluster and ignores the statistical variations within the cluster. These wrong assignments result in inaccurate HMM model parameters.</p><p>Therefore, in order to fully utilize the information we learnt about the statistical properties of the data within each cluster we use ML assignment. In this method we use the cluster observation models learnt in the previous steps f(q t jx t = x k ) and assign q t to the most likely cluster as follows</p><formula xml:id="formula_10">xi ¼ max K j¼1 f ðq i jx t ¼ x j Þ; for i ¼ 1 : m<label>ð10Þ</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Classification of a test sequence</head><p>Given a sequence of observation shapes Q ¼ q 1 ; q 2 ; . . . ; q m ; q i 2 S and a set of M HMM models corresponding to M different gestures, we first compute the conditional observation likelihood f(q t js t ) using the learnt cluster density functions f(q t jx t = x k ) and the exemplar clusters observation probabilities p(x t = x k js t ). This is calculated by summing over all possible clusters as shown in <ref type="bibr" target="#b7">(8)</ref>.</p><p>Given these observations likelihood and the state transition probabilities p(S t jS tÀi ), computing the likelihood of the test sequence is a simple HMM evaluation problem. We solve it using the standard forward technique and dynamic programming <ref type="bibr" target="#b28">[29]</ref>. The test sequence is then classified using the maximum likelihood rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head><p>We carried out an extensive set of experiments to evaluate and verify the effectiveness of using the contour curve shape manifold and the two proposed methods in modeling and recognition of human actions and gestures. The experiments also investigate the effect on performance with changing some of the system choices like the cluster assignment method and whether the clusters are shared among different gesture models or not.</p><p>These experiments were performed using two different datasets representing the human action and control gesture scenarios respectively. As in all silhouette-based methods, we assume the availability of a relatively clean background subtracted images representing the silhouettes. In order to avoid dealing with low-level processing steps such as background subtraction and object segmentation, the existence of such images was an important criteria in choosing the datasets used in these experiments. For both datasets, background subtracted images with relatively good resolution and quality were available. We used these images to extract the contour of the human as the boundary of the main binary silhouette in the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">UMD common actions dataset</head><p>In the first set of experiments we use the UMD common activity dataset <ref type="bibr" target="#b67">[68]</ref> to perform action modeling and recognition. This dataset consists of 10 common activities with 10 different instants of each activity performed by the same actor, which brings the total number of sequence to 100 sequences. Each of these sequences is 80 frames long, which means a total of 8000 frames are in the whole dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1.">Template-based approach</head><p>We first use the dataset to evaluate the template-based approach presented in Section 4.1. We split the dataset into two disjoint sets, of five sequence each. We used the even sequence for learning the template trajectory on the shape manifold while we tested using the remaining five odd sequences per gesture. Fig. <ref type="figure">6a</ref> shows the 10 Â 50 similarity matrix between each of the test sequences and each of the learnt action models. Each of these distances corresponds to the warping distance computed on the shape manifold between the test sequence and the model templates. Each row corresponds to a single action model, while each column corresponds to a different test sequence. The strong block diagonal nature of the similarity matrix indicates that the modeling is accurate and is effective for recognition.</p><p>We use this similarity matrix to perform an action recognition experiment, where we assign each test sequence to the model with the smallest warping distance. The resulting confusion matrix for this experiment is shown in Fig. <ref type="figure">6b</ref>, showing an correct recognition rate of 98%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2.">Graphical-model based approach</head><p>We used the same dataset to test the graphical model approach presented in Section 4.2. In the first stage we use AP to cluster the data points on the manifold using the pairwise geodesic distances between every pair of shapes. We use the preference parameter value, as mentioned in <ref type="bibr" target="#b42">[43]</ref>, to change the number of clusters to check the effect of this change on our performance. As in the template-based method experiment, we use the data corresponding to the odd sequences for learning the clusters and their statistical observation density functions.</p><p>The clustering results along with statistical models of each cluster are used to learn the dynamics of each action as presented in Section 4.2. We first assign the gesture data from the training set into different clusters and use the cluster assignment streams to learn the HMM dynamics of the action. We used two different assignment approaches as described in Section 4.2. In the NN assignment method we assign the shape points into the nearest cluster in terms of the geodesic distances with the mean of the cluster, while in the ML assignment we use the learnt statistical model to assign the shape observation into the most likely cluster in terms of the likelihood function.</p><p>Fig. <ref type="figure">7</ref> shows the correct classification rates for two validation experiments under different number of clusters and different assignment methods. The first experiment, shown in Fig. <ref type="figure">7a</ref>, is similar to the template-based experiment, where we split the data Table <ref type="table">1</ref> A summary of the recognition results of our various approaches and evaluation methods on the UMD common actions dataset as compared to the results reported in <ref type="bibr" target="#b34">[35]</ref> on the same dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Evaluation method</head><p>Recog. rate % Template-based approach Split 98 Graphical-based approach with NN assignment Split 96.67 Graphical-based approach with NN assignment Leave one out 95.67 Graphical-based approach with ML assignment Split 99.67 Graphical-based approach with ML assignment Leave one out 99.67 Veeraraghavan et al. <ref type="bibr" target="#b34">[35]</ref> Leave one out 100 sequences into two disjoint set and use the first set for training and use the second for testing. The second experiment, shown in Fig. <ref type="figure">7b</ref>, is using the leave-one-out cross validation method, where we test each sequence using a model learnt using all the other sequences.</p><formula xml:id="formula_11">(1)<label>(2) (3) (4) (5) (6) (7)</label></formula><formula xml:id="formula_12">(8) (9) (10) (11) (12) (13)<label>(14)</label></formula><p>The performance results are very close, however we can note a couple of observations:</p><p>1. The NN assignment method performs slightly poorly with increasing number of clusters. On the other hand, the ML performance slightly increases with the same number of clusters change. This can be due to the fact that with more clusters the NN assignment can easily assign similar shapes to different clusters, while the ML method does not make the same mistake. 2. Although both assignment methods are performing well, the ML method is performing slightly better with average recognition rate of 99.67% in both experiments, while NN assignment methods achieved an average recognition rate 96.67% and 95.67% in the first and second experiments respectively. This is expected because the statistical model allows for learning the modes of variation within each cluster, which result in a more accurate modeling of the within cluster variation. We grouped the recognition rates for all of our experiments in Table <ref type="table">1</ref>. We also include the results reported in <ref type="bibr" target="#b34">[35]</ref> on the same data set. We can note that our HMM approach has a comparable result to the perfect recognition rate reported in <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Control gesture dataset</head><p>The UMD action dataset has been used in several publication on action modeling and recognition. However, the level of variability within the dataset is very limited by the single actor performing the actions. For this reason, we carried out another set of experiments using a different dataset of human control gestures recently introduced in <ref type="bibr" target="#b45">[46]</ref>. The dataset consists of 14 arm control gestures performed by three different actors with different clothes and body frames. Each person repeats a gesture for 3-5 cycles. The length of each cycle varies a lot between 80 and 250 frames for the shortest and longest sequences respectively. Fig. <ref type="figure">8</ref> shows the different gestures in the dataset performed by the first person with an overlay of the extracted contour curves captured every 10 frames.</p><p>For this dataset, the authors provided us with a set of background subtracted frames with good resolution. However, the quality of background subtraction was not consistent within all the frames, with the result of added background pixels and poor contour extraction as shown in Fig. <ref type="figure">9</ref>. Our method, as in most silhouette-based methods, assumes a relatively clean background subtraction. However, we did not manually fix these errors and use it to test the robustness of our approaches. Another issue was that the different cycles for each person performing a gesture is provided in terms of a long continuous sequence of frames. We had to manually perform a temporal segmentation of the data in order to obtain a separate sequences for each cycle. The quality of such segmentation was crucial for the DTW template-based approach, where we will show the results before and after fixing the segmentation errors in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Template-based approach</head><p>We ran our template-based approach for learning a template for each gesture and used that template for recognition. Fig. <ref type="figure" target="#fig_0">10</ref> shows the correct classification rates for the different experiments we performed. In the first experiment, we achieved a very poor recognition rate of 42%, although we have been using all the sequences for training. After carefully inspecting the resulting templates, we found out that poor segmentation of the sequences results in poor performance. This shows the importance of using cleanly   segmented sequences for learning templates when the DTW method is used. In experiment 2, we fixed the segmentation issue, which improved the classification rate to around 60%. In the last two experiments, we changed the way we initialize the computation of the mean trajectory. Instead of initializing using a random trajectory of the training sequences, we compute the DTW between each training sequence as a model and the rest of the sequences. We initialize the mean trajectory computation using the sequence with the smallest average DTW error to all the other sequences. This change resulted in increasing the correct classification rate to around 69% while using all the sequences for training and testing as a baseline experiment. In the last experiment (Experiment 4), we perform a leave-one-sequence-out cross validation, where we build a model for each test sequence with a training set that exclude that sequence and use all the remaining sequences. Surprisingly, this performed better than the baseline results of using all the data for training, with a correct classification rate of 72%. The similarity matrix and confusion matrix corresponding to the different experiments are shown in Figs. 11 and 12 respectively.</p><p>We further analyzed the classification results by examining the correct recognition rate for each gesture and each person individually, these results are shown in Fig. <ref type="figure" target="#fig_2">13a</ref> and<ref type="figure">b</ref> respectively. From these figures, we can note the recognition rate varies dramatically across different gestures. For example the Go Back gesture was recognized correctly in only 10% of the cases, while the Flap, Stop Left, and Stop Right gestures achieved perfect recognition rates in most of the experiments. This is due to the fact that some gestures are more distinguishable using only the silhouette information than other gestures. On the other hand, the recognition rates were very close for different persons performing the gestures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Graphical-model based approach</head><p>We repeated the same experiments performed on the UMD action dataset for the control gestures dataset but with a slight variation in the setup. Experiment 1 represents a baseline experiment where we use all of the training sequence for learning the model. This should perform really well and it can point out to any problems within the modeling itself regardless of the generalization of the model to unseen sequences. Experiment 2 represents the same leave-one-sequence-out experiment performed previously for the UMD action dataset. Due to the fact that we have three different persons performing each gesture, we performed a third experiment representing a leave-one-person-out cross validation, where we exclude all the sequences corresponding to one person from training the model used for the test sequences corresponding to that person. This will test the generalization capabilities of our model for new body frames that have not been used in the training phase. This can be a very challenging problem because of the limited number of training sequences remaining after excluding all the sequences for one person.</p><p>Figs. 14 and 15 shows the correct classification rates using HMM with five and seven states respectively. The figures shows the performance variations while using different number of clusters and switching between ML and NN cluster assignment methods. Fig. <ref type="figure" target="#fig_0">16</ref> shows the overall confusion matrix for these experiments combining the results for each experiment and assignment method under different number of clusters and HMM states.</p><p>Form these graphs we notice the following:</p><p>1. The ML assignment method is performing slightly better than the NN rule with maximum recognition rates of 100%, 94%, and 82% for experiments 1, 2, and 3 respectively. compared to maximum correct recognition rates of 99%, 94%, and 76% for the NN assignment methods. Also, the average correct classification rate of ML assignment was 97%, 90%, and 73% for the experiments 1, 2, and 3 respectively, compared to an average rate of 91%, 85%,and 70% for the NN assignment method. 2. A careful study of the confusion matrices in Fig. <ref type="figure" target="#fig_0">16</ref> shows that certain gestures are very confusing even in the baseline experiment. which lower the average classification rate. For instance, the two gestures of Go Back and Come Near (gestures 11 and 14 in the tables) are usually confused alot. These two gestures are similar except for the orientation of the palm of the hand and the direction of the motion. This makes them highly indistinguishable using only the silhouette images, even for an human observer. 3. The recognition rates are very similar when we change the number of states, which suggests that the performance does not vary much with the HMM design parameters. 4. The performance is relatively bad in the leave-one-person-out experiment. We believe that this may be mainly due to the low number of training sequences in this case.</p><p>Due to the recent introduction of the dataset, we could only compare our recognition rates to the results reported in <ref type="bibr" target="#b45">[46]</ref>. Table <ref type="table" target="#tab_1">2</ref> group our results for different approaches and evaluation methods as compared to <ref type="bibr" target="#b45">[46]</ref>. We can note that our method managed to achieve comparable performance with 94% maximum recognition rate as compared 95.24% recognition rate reported in <ref type="bibr" target="#b45">[46]</ref>. However, we were able to achieve this results using only the silhouette of the background subtracted sequences, while <ref type="bibr" target="#b45">[46]</ref> was combining both the shape and motion information obtained using the color images and optical flow computations. In fact, our results exceed the performance reported in the same paper using only the shape cues for recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we presented a novel gesture recognition technique using shape manifolds. Contours of the silhouette are extracted and represented as 2D closed elastic curves parameterized using the square-root parametrization. This representation is intrinsically invariant to both translation,scale, and re-parametrization of the curve. Each gesture is modeled as a temporal trajectory on the resulting Riemannian manifold of 2D elastic curves. We proposed template and graphical-based HMM approaches for modeling these trajectories. The two approaches capture the statistical variability of the data, while adhering to the underlying geometry of the manifold. The two approaches were proven successful experimentally using two data sets of human control gesture and actions. In future, we plan to extend the proposed approach to other vision problems that can be modeled using dynamical trajectory on differentiable manifolds. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An example of different frames of a humans performing a ''Turn Right'' control gesture. (a) The original video frames. (b) The background subtracted images. (c) The sequence of contours. Most of the information about the gesture can be modeled using only the contour curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. A two-dimensional manifold M in R 3 ; TxM the tangent plane at x 2 M, and the exp and log maps relating x; y 2 M.</figDesc><graphic coords="4,314.78,566.45,225.09,152.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. A graphical model of unfolding a standard continuous state HMM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A graphical model of unfolding the exemplar-based HMM used in modeling the gesture dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig.5. Some exemplar shapes representing clusters computed on the shape manifold using AP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Fig. 6. Results of using the template-based method for classification on the UMD action dataset. (a) The 10 Â 50 similarity matrix between the 50 test sequences and the 10 action models learnt (better viewed in color). (b) The confusion matrix for action classification. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. The different control gestures in the data set used in our gesture recognition experiment [46]: (1) Turn Right, (2) Turn Left, (3) Attention Right, (4) Attention Left, (5) Flap, (6) Stop Right, (7) Stop Left, (8) Stop Both, (9) Attention Both, (10) Start Engine, (11) Go Back, (12) Close Distance, (13) Speed Up, and (14) Come Near.</figDesc><graphic coords="10,80.84,499.49,420.94,86.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>Fig. 10. The correct classification results for using the template-based approach on the gesture dataset. Experiment 1: Bad segmentation of sequences and random initialization. Experiment 2: Correct segmentation of sequences and random initialization. Experiment 3: Initialization using the best sequence, all sequence used for training. Experiment 4: Initialization using the best Sequence, leave-oneout cross validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. The confusion matrix for gesture classification using the template-based method for experiments 1-4 are shown in a-d respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .Fig. 14 .Fig. 15 .Fig. 16 .</head><label>13141516</label><figDesc>Fig. 13. Gesture dataset classification results using the template-based approach. (a) The correct classification rates for each gestures in different experiments. (b) The correct classification rates for each person in different experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>A summary of the recognition results of our various approaches and evaluation methods on the control gesture dataset as compared to the results reported in<ref type="bibr" target="#b45">[46]</ref> on the same dataset.</figDesc><table><row><cell>Approach</cell><cell>Evaluation method</cell><cell>Max recog.</cell></row><row><cell></cell><cell></cell><cell>rate %</cell></row><row><cell>Template-based approach</cell><cell>Leave one out</cell><cell>72</cell></row><row><cell>Graphical-based approach</cell><cell>Leave-one-sequence-out</cell><cell>94</cell></row><row><cell>with NN assignment</cell><cell></cell><cell></cell></row><row><cell>Graphical-based approach</cell><cell>Leave-one-person-out</cell><cell>76</cell></row><row><cell>with NN assignment</cell><cell></cell><cell></cell></row><row><cell>Graphical-based approach</cell><cell>Leave-one-sequence-out</cell><cell>94</cell></row><row><cell>with ML assignment</cell><cell></cell><cell></cell></row><row><cell>Graphical-based approach</cell><cell>Leave-one-person-out</cell><cell>82</cell></row><row><cell>with ML assignment</cell><cell></cell><cell></cell></row><row><cell>Lin et al. [46], shape only</cell><cell>Leave one out</cell><cell>92.86</cell></row><row><cell>Lin et al. [46], shape and motion</cell><cell>Leave one out</cell><cell>95.24</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>M.F. Abdelkader et al. / Computer Vision and Image Understanding 115 (2011) 439-455</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>M.F. Abdelkader et al. / Computer Vision and Image Understanding 115 (2011) 439-455</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially supported by ONR Grant No. N00014-09-1-0664.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive visual gesture recognition for human-robot interaction using a knowledge-based software platform</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hasanuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ampornaramveth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gotoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robot. Auton. Syst</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="643" to="657" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vision based gesture recognition for human-robot symbiosis</title>
		<author>
			<persName><forename type="first">M.-A</forename><surname>Bhuiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Begum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasanuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ueno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Conference on Computer and Information Technology, ICCIT 2007</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human motion: modeling and recognition of actions and interactions</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Symposium on 3D Data Processing, Visualization, and Transmission, 3DPVT 2004</title>
		<meeting>the Second International Symposium on 3D Data Processing, Visualization, and Transmission, 3DPVT 2004<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="640" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The visual analysis of human movement: a survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="82" to="98" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on visual surveillance of object motion and behaviors</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern. -C: Appl. Rev</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="334" to="352" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey of advances in vision-based human motion capture and analysis</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Krüger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="90" to="126" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action recognition using exemplar-based embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognizing action at a distance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the space of a human action</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="144" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning and recognizing human dynamics in video sequences</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="568" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matching shape sequences in video with an application to human movement analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1896" to="1909" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning and matching of dynamic shape manifolds for human action recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1646" to="1661" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the encoding of arbitrary geometric configurations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRE Trans. Electron. Comput</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="260" to="268" />
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fourier descriptors for plane closed curves</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roskies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="269" to="281" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual pattern recognition by moment invariants</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="179" to="187" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shape matching and object recognition using shape contexts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puzicha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="509" to="522" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inferring 3D body pose from silhouettes using activity manifold learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="681" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Parameterized modeling and recognition of activities</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Bombay</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="120" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning on lie groups for invariant detection and tracking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, AK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Affine invariance revisited</title>
		<author>
			<persName><forename type="first">E</forename><surname>Begelfor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, FL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="2087" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Statistical analysis on Stiefel and Grassmann manifolds with applications in computer vision</title>
		<author>
			<persName><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hilbert-Schmidt lower bounds for estimators on matrix Lie groups for ATR</title>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="790" to="802" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monte Carlo extrinsic estimators for manifold-valued parameters</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="299" to="308" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Clustering and dimensionality reduction on riemannian manifolds</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on riemannian manifolds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust statistics on riemannian manifolds via the geometric median</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Anchorage, Alaska</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Role of shape and kinematics in human movement analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="730" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="257" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recognizing human action in time-sequential images using hidden Markov model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Urbana-Champaign, IL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="379" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coupled hidden Markov models for complex action recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="994" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Large-scale event detection using semi-hidden Markov models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hongeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1455" to="1462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hidden conditional random fields for gesture recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Demirdjian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recognizing human activities from silhouettes: motion subspace and factorial discriminative graphical model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rateinvariant recognition of humans and their activities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1326" to="1339" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel representation for riemannian analysis of elastic curves in Rn</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conf. on Computer Vision and Pattern Recognition<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Removing shape-preserving transformations in square-root elastic (SRE) framework for shape analysis of curves</title>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR)</title>
		<meeting>International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition (EMMCVPR)<address><addrLine>Ezhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="387" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shape analysis of elastic curves in euclidean spaces</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Analysis of planar shapes using geodesic paths on shape spaces</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="372" to="383" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computable elastic distance between shapes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="565" to="586" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A metric on shape space with explicit geodesics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Younes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Michor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lincei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matematica E Applicazioni</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="25" to="57" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Statistical shape analysis: clustering, learning, and testing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="590" to="602" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Clustering by passing messages between data points</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="page" from="972" to="976" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning dynamics for exemplarbased gesture recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Madison, WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="571" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Action recognition from arbitrary views using 3d exemplars</title>
		<author>
			<persName><forename type="first">D</forename><surname>Weinland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ronfard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognizing actions by shape-motion prototype trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision<address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="444" to="451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flexible models: a powerful alternative to exemplars and explicit models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Workshop on Models vs. Exemplars in Computer Vision</title>
		<meeting><address><addrLine>Honolulu, Hawaii</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Probabilistic tracking with exemplars in a metric space</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="9" to="19" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gesture spotting and recognition for human-robot interaction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Robot</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="256" to="270" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Shape and Shape Theory</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
		<title level="m">Statistical Shape Analysis</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Carne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Shape and Shape Theory</title>
		<imprint>
			<publisher>John Wiley and Sons</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Affine shape analysis image analysis, Stochastic Geometry, Biological Structure and Images</title>
		<author>
			<persName><forename type="first">V</forename><surname>Patrangenaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
		<respStmt>
			<orgName>Dept. of Statistics, University of Leeds</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Projective shape analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Goodall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mardia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Graph. Stat</title>
		<imprint>
			<biblScope unit="page" from="143" to="168" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Active shape models: their training and application</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="38" to="59" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<title level="m">General Pattern Theory</title>
		<imprint>
			<publisher>Oxford university Press</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Activity recognition using the dynamics of the configuration of interacting objects</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>WI</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Nonstationary shape activities: dynamic models for landmark shape change and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="579" to="592" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On shape of plane elastic curves</title>
		<author>
			<persName><forename type="first">W</forename><surname>Mio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="307" to="324" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Carmo</surname></persName>
		</author>
		<title level="m">Riemannian Geometry, BirkhSuser, Englewoods Cliffs</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Probabilities and statistics on riemannian manifolds: Basic tools for geometric measurements</title>
		<author>
			<persName><forename type="first">X</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">P</forename><surname>Epidaure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Nonlinear Signal and Image Processing</title>
		<meeting><address><addrLine>Antalya, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="194" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Locating frechet means with application to shape spaces</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Appl. Prob</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="324" to="338" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Principal geodesic analysis for the study of nonlinear statistics of shape</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="995" to="1005" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Geodesics between 3d closed curves using pathstraightening</title>
		<author>
			<persName><forename type="first">E</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="95" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Fundamentals of speech recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-H</forename><surname>Juang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Prentice-Hall, Inc</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Dynamic shapes average</title>
		<author>
			<persName><forename type="first">P</forename><surname>Maurel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second IEEE Workshop Variational, Geometric and Level Set Methods in Computer Vision</title>
		<meeting>Second IEEE Workshop Variational, Geometric and Level Set Methods in Computer Vision<address><addrLine>Nice, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Soules</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Stat</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">The function space of an activity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="959" to="968" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
