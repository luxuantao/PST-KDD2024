<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dependency-based Mixture Language Models</title>
				<funder>
					<orgName type="full">Yes Yes No ON-LSTM</orgName>
				</funder>
				<funder ref="#_75fHpcP">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder>
					<orgName type="full">State Key Laboratory of Media Convergence Production Technology and Systems</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-19">19 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhixian</forename><surname>Yang</surname></persName>
							<email>yangzhixian@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Peking University Center for Data Science</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<orgName type="institution" key="instit3">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
							<email>wanxiaojun@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Peking University Center for Data Science</orgName>
								<orgName type="laboratory">The MOE Key Laboratory of Computational Linguistics</orgName>
								<orgName type="institution" key="instit1">Wangxuan Institute of Computer Technology</orgName>
								<orgName type="institution" key="instit2">Peking University</orgName>
								<orgName type="institution" key="instit3">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Dependency-based Mixture Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-19">19 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.10256v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Various models have been proposed to incorporate knowledge of syntactic structures into neural language models. However, previous works have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN), which makes themselves unwieldy in practice to fit into other neural language models, such as Transformer and GPT-2. In this paper, we introduce the Dependency-based Mixture Language Models. In detail, we first train neural language models with a novel dependency modeling objective to learn the probability distribution of future dependent tokens given context. We then formulate the next-token probability by mixing the previous dependency modeling probability distributions with selfattention. Extensive experiments and human evaluations show that our method can be easily and effectively applied to different neural language models while improving neural text generation on various tasks. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Syntactic structures serve as the principle of how words are correctly combined to form sentences. It is widely acknowledged that learning syntactic structures should improve neural text generation <ref type="bibr" target="#b35">(Shen et al., 2018;</ref><ref type="bibr" target="#b31">Peng et al., 2019;</ref><ref type="bibr" target="#b8">Du et al., 2020)</ref>. Even though current neural language models, such as Transformer <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> and GPT-2 <ref type="bibr" target="#b32">(Radford et al., 2019)</ref> have achieved outstanding performance without explicitly modeling latent syntactic structures, these models still fail to learn the long-range syntactic dependencies <ref type="bibr" target="#b22">(Kuncoro et al., 2018;</ref><ref type="bibr" target="#b46">Xu et al., 2021)</ref>.</p><p>To leverage explicit syntactic knowledge in natural language generation (NLG), many methods have been proposed (Wu et al., 2017; Shen et al.,   1 Our code is available at https: //github.com/FadedCosine/ Dependency-Guided-Neural-Text-Generation 2018; <ref type="bibr" target="#b50">Zhang et al., 2019;</ref><ref type="bibr" target="#b20">Kim et al., 2019;</ref><ref type="bibr" target="#b8">Du et al., 2020)</ref>. We conclude from previous works that knowledge of syntactic structures can bring four advantages to neural language models:</p><p>(1) Syntactic structures can be modeled to obtain better representations of natural language sentences <ref type="bibr" target="#b19">(Jacob et al., 2018;</ref><ref type="bibr" target="#b44">Williams et al., 2018;</ref><ref type="bibr" target="#b43">Wang et al., 2019)</ref>.</p><p>(2) Jointly training syntactic structure parsing and language modeling can contribute to each other <ref type="bibr" target="#b35">(Shen et al., 2018;</ref><ref type="bibr" target="#b9">Dyer et al., 2016;</ref><ref type="bibr" target="#b20">Kim et al., 2019;</ref><ref type="bibr" target="#b8">Du et al., 2020;</ref><ref type="bibr">Shen et al., 2021b)</ref>.</p><p>(3) Syntactic structures can be used to directly model the composition of language <ref type="bibr" target="#b39">(Socher et al., 2013;</ref><ref type="bibr" target="#b3">Casas et al., 2020)</ref> and help with the longrange dependency problem by providing shortcuts for gradient backpropagation <ref type="bibr" target="#b5">(Chung et al., 2017)</ref>.</p><p>(4) Integrating syntactic structures into a neural network can improve generalization via a better inductive bias <ref type="bibr" target="#b36">(Shen et al., 2019;</ref><ref type="bibr" target="#b50">Zhang et al., 2019)</ref>.</p><p>Despite these advantages, it is not trivial to incorporate knowledge of syntactic structures into neural language models effectively and efficiently. Several practical problems arise:</p><p>(1) Previous works <ref type="bibr" target="#b5">(Chung et al., 2017;</ref><ref type="bibr" target="#b35">Shen et al., 2018;</ref><ref type="bibr" target="#b9">Dyer et al., 2016;</ref><ref type="bibr" target="#b20">Kim et al., 2019;</ref><ref type="bibr" target="#b36">Shen et al., 2019)</ref> have relied heavily on elaborate components for a specific language model, usually recurrent neural network (RNN) <ref type="bibr" target="#b40">(Sutskever et al., 2014)</ref>. These methods are difficult to be adapted to other neural language models, such as Transformer and GPT-2.</p><p>(2) If jointly modeling language modeling and syntactic structure parsing, it will require much more time/memory during training or inference.</p><p>To address these problems while keeping the advantages, we explore incorporating knowledge of syntactic structures in a different manner. In this work, we propose a novel dependency modeling objective to train neural language models to directly predict the current token's future dependent tokens given the history. We define the future dependent tokens of a specific token in a sentence as its children and parent in the dependency parse tree that will appear in the rest of the sentence. Further, we propose Dependency-based Mixture Language Models (DMLM) that, at each timestep, mixes the previous dependency modeling probability distributions with self-attention to get the next-token probability. As shown in Table <ref type="table">1</ref>, the proposed method can be adapted to any neural language model without adding external networks or parameters.</p><p>Our core idea can be illustrated in Figure <ref type="figure">1</ref> and Figure <ref type="figure" target="#fig_0">2</ref>: when predicting the next-token "indicate" after reading "red figures on the screen", common language models are easy to predict an incorrect word, such as "indicates", since the prediction of these models relies heavily on the recent word, "screen" in this case. However, our propose DMLM will directly look back into the long-range context, and select the next-token from all the future dependent tokens predicted by previous tokens. According to the underlying dependency structure, DMLM pays different weights to different tokens' future dependent tokens. Thus, the model is more likely to predict "indicate" since DMLM tends to think of the next-token as a future dependent token of "figures" rather than "screen".</p><p>We conduct experiments with different neural language models including LSTM <ref type="bibr" target="#b17">(Hochreiter and Schmidhuber, 1997)</ref>, Transformer <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref>, and GPT-2 <ref type="bibr" target="#b32">(Radford et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methodology</head><p>Our goal is to propose a simple yet effective method that can improve neural text generation by learning from the underlying syntactic structure, and can fit into any auto-regressive generation model without using additional elaborate components. We first introduce a novel dependency modeling objective to force the model to directly predict the future dependent tokens of the current token. Based on the dependency modeling, we then present the proposed DMLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dependency Modeling</head><p>It has been a challenge to equip neural language models with the capability of modeling long-range dependency in text <ref type="bibr" target="#b6">(Dai et al., 2019)</ref>. In particular, previous works <ref type="bibr" target="#b45">(Wu et al., 2017)</ref> observe that vanilla RNN can hardly capture many sub- tle long-range token dependencies effectively. On the other hand, though self-attention mechanisms can build direct connections between long-distance token pairs, it is still elusive for Transformer to be aware of syntactic dependency structures while also obtaining strong language modeling performance <ref type="bibr">(Shen et al., 2021a)</ref>.</p><p>The current neural language models are mostly trained purely using the language modeling objective with Maximum Likelihood Estimation (MLE). With the auto-regressive factorization, language modeling can be reduced to modeling the conditional distribution of the next-token x t given the context x &lt;t = {x 1 , . . . , x t-2 , x t-1 }. However, in order to make neural language models aware of long-range dependency and syntactic structures, we propose the dependency modeling objective to train models to learn the probability distribution of the future dependent tokens directly. Following <ref type="bibr" target="#b0">Ahmed et al. (2019)</ref>, we define the future dependent tokens of a specific token in a sentence as its children and parent in the dependency parse tree that will appear in the rest of the sentence. Taking Figure <ref type="figure">1</ref> as an example, the future dependent tokens of "figures" are "screen" and "indicate", since "red" does not appear after "figures" in this sentence.</p><p>Specifically, given a token sequence x = {x 1 , . . . , x T -1 , x T } where T ? N denotes the sequence length, we first use dependency parser to generate a dependency tree. Then, we derive the future dependent tokens set Z t for each token x t-1 , where Z t = {x i | i ? t, x i is the child or parent of x t-1 }. We train a lan-guage model ? to maximize the log-likelihood sum of tokens in Z t . This equals to minimize:</p><formula xml:id="formula_0">L DM (?) = - T t=1 zt?Zt log p dep ? (z t | x &lt;t ) , (1)</formula><p>which is the dependency modeling objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dependency-based Mixture Language Models</head><p>To give a categorical probability distribution over the next-token, a standard approach for the current neural language models is to encode the context into a fixed-size vector followed by an output embedding layer and a softmax function.</p><p>In our case, given the context x &lt;t , we first train the language model to directly learn the probability distribution of x t-1 's future dependent tokens p dep ? (w | x &lt;t ) by dependency modeling (Section 2.1).</p><p>We then propose DMLM (depicted in Figure <ref type="figure" target="#fig_0">2</ref>) that mixes dependency modeling probability distributions</p><formula xml:id="formula_1">P dep = {p dep ? (w | x &lt;1 ) , . . . , p dep ? (w | x &lt;t-1 ) , p dep ? (w | x &lt;t )}.</formula><p>All the probability distributions in P dep are weighed by self-attention, and summed to obtain the final next-token probability distribution.</p><p>We can easily implement a self-attention in both Transformer-based and RNN-based language models. For example, in Transformer and GPT-2, the penultimate layer seems to naturally learn alignments <ref type="bibr" target="#b12">(Garg et al., 2019)</ref>, so we use its average attention weights over all the attentions heads as the dependency attention distribution. In RNN-based models, inspired by <ref type="bibr" target="#b26">Merity et al. (2017)</ref> and <ref type="bibr" target="#b42">Vaswani et al. (2017)</ref>, at each timestep, we linearly project the current hidden state h t ? R H to a query vector q t = W Q h t and a key vector k t = W K h t , where W Q ? R H?H , W K ? R H?H , q t ? R H , and k t ? R H . To generate the dependency attention, we compute the match between the query q t and the context's keys {k 1 , . . . , k t-1 , k t } by taking the inner product, followed by a softmax to obtain the dependency attention distribution:</p><formula xml:id="formula_2">e (t) = {e (t) 1 , . . . , e (t) t-1 , e (t) t }, e (t) i = q T t k i , 1 ? i ? t, a (t) = softmax( e (t) ? H ),</formula><formula xml:id="formula_3">a (t) = {a (t) 1 , . . . , a<label>(t)</label></formula><p>t-1 , a</p><formula xml:id="formula_4">(t) t },<label>(2)</label></formula><p>where e (t) ? R t , and a (t) ? R t . We scale the dot products by 1 ? H following <ref type="bibr" target="#b42">Vaswani et al. (2017)</ref>. The dependency attention distribution reveals which token in the context may have a strong dependency relation with the token to be predicted. Thus, the neural language model should pay more attention to previous tokens with high dependency attention scores, i.e., the next-token is more likely to be the future dependent token of those tokens in the context. Formally, the next-token probability is the sum of the context's dependency modeling probability distributions weighed by the dependency attention scores:</p><formula xml:id="formula_5">p ? (w | x &lt;t ) = t ? =1 a (t) ? p dep ? (w | x &lt;? ) .<label>(3)</label></formula><p>where p dep ? (w | x &lt;? ) is the probability distribution of x ? -1 's future dependent tokens, since till now the neural language model is only trained by dependency modeling. Then, we further finetune the neural language model using MLE, but with respect to our modified probability distribution given in Equation 3:</p><formula xml:id="formula_6">L LM (?) = - T t=1 log p ? (x t | x &lt;t ) .<label>(4)</label></formula><p>For each timestep during inference, DMLM outputs a dependency modeling distribution, and we store it in a list. To predict the next-token, DMLM applies self-attention in Equation 2 to produce a dependency attention distribution over the context, and then the next-token probability can be calculated by Equation <ref type="formula" target="#formula_5">3</ref>, where the list preserves all the</p><formula xml:id="formula_7">p dep ? (w | x &lt;? ) , 1 ? ? ? t.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Despite previous works mainly focusing on language modeling, it has always been a thorny issue whether better language models lead to better performance in downstream tasks. Therefore, we showcase the performance of our proposed DMLM in three different tasks: conditional text generation (Section 3.1), unconditional text generation (Section 3.2), and language modeling (Section 3.3).</p><p>To verify the effectiveness and architecturally generalizability of our method, we conduct the generation tasks with three dominant neural language models, including LSTM, Transformer and GPT-2. We prefix the base model name with "DM-" to denote the corresponding Dependency-based Mixture language model. Specifically, we adopt AWD-LSTM <ref type="bibr" target="#b25">(Merity et al., 2018)</ref> as our base LSTM, and further compare our DM-LSTM with PRPN <ref type="bibr" target="#b35">(Shen et al., 2018)</ref> and ON-LSTM <ref type="bibr" target="#b36">(Shen et al., 2019)</ref> which also incorporate knowledge of syntactic structures, and are built on LSTM. In the same task, we use exactly the same hyper-parameters and setups for the pairs of base models and corresponding DM-models. Other details of the experimental setup for each task can be seen in Appendix A.</p><p>For all the tasks, we use a state-of-the-art parser, HPSG Parser<ref type="foot" target="#foot_0">2</ref>  <ref type="bibr" target="#b51">(Zhou and Zhao, 2019)</ref> to get the dependency parse tree for each sentence in the datasets. We discuss the impact of the dependency parser in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Text Generation</head><p>Setup We take the story ending generation as the conditional text generation task, and evaluate our method on the ROCStories corpus <ref type="bibr" target="#b29">(Mostafazadeh et al., 2016)</ref>, which consists of 98,161 five-sentences. We follow the preprocessing<ref type="foot" target="#foot_1">3</ref> of <ref type="bibr" target="#b21">Kong et al. (2021)</ref> to randomly split ROCStories by 8:1:1 for training/validation/test, respectively, and delexicalize stories by masking all the male/female/unknown names with "[MALE]"/"[FEMALE]"/"[NEUTRAL]". We finally get a word-level vocabulary with 31, 216 unique tokens. The conditional text generation task is to generate a reasonable ending given a foursentence story context. For all models, we generate stories using nucleus sampling <ref type="bibr">(Holtzman et al.</ref>,  </p><formula xml:id="formula_8">Models UNION ? BERTScore ? B-1 ? B-2 ? D2 ? D3 ? SB-2 ? SB-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2020</head><p>) with p = 0.5. We measure the generated story endings by the following automatics metrics: (1) UNION (Guan and Huang, 2020): It is a learnable unreferenced metric for evaluating the quality of generated stories; (2) BERTScore <ref type="bibr" target="#b49">(Zhang et al., 2020)</ref>: The metric measures the semantic consistency between the generated and the referenced ones by BERT <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>; (3) BLEU (B-n) <ref type="bibr" target="#b30">(Papineni et al., 2002)</ref>: BLEU evaluates n-gram overlap between the generated stories and the references; (4) Distinct (D-n) <ref type="bibr" target="#b23">(Li et al., 2016)</ref>: The proportions of distinct n-grams in the outputs to evaluate the diversity of generated results. Since Distinct score will become extremely low for small n, we calculate it with n = 2, 3; (5) Self-BLEU (SB-n) <ref type="bibr" target="#b52">(Zhu et al., 2018)</ref>: The metric is calculated by computing n-grams (n = 2, 3) BLEU score of each generated text with all other generated ones as references. Smaller Self-BLEU scores indicate better diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results of baselines and corresponding DM-models are shown in Table 2. Note that we do not conduct significant tests on Distinct since it is a document-level metric. We can see that, all the DM-models significantly outperform baseline models on almost all the metrics. Furthermore, compared with PRPN and ON-LSTM, our DM-LSTM performs signifi- cantly better in all the metrics. This indicates that incorporating knowledge of syntactic structures in our proposed way can effectively contribute to both the quality and diversity of the story ending generation. Moreover, no matter what the base model is, our DM-model can substantially improves the conditional text generation. This demonstrates that our method can be effectively adapted to different neural language models, such as the large scale language model, GPT-2, while previous models like ON-LSTM can only be built on LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human evaluation</head><p>To further evaluate the fluency and logic of generated stories, following <ref type="bibr">(Guan et al., 2020)</ref> To reach a good trade-off between quality and diversity, we adopt nucleus sampling with p = 0.7 for all the models to generate samples.</p><p>baselines. We randomly sample 100 story endings from each model. For each pair of stories (one by the DM-model and the other by the baseline, along with the beginning), five annotators are hired to give a preference (win, lose, or tie) from the following two aspects: (1) Grammaticality: whether a story ending is natural and fluent;</p><p>(2) Logicality: whether a story is coherent to the given beginning and reasonable in terms of causal and temporal dependencies in the context. The detailed questionnaire and other details are shown in Appendix D.</p><p>The average win/lose/tie rates of the human evaluation are shown in Table <ref type="table" target="#tab_2">3</ref>. To measure the interannotator agreement, we calculate Krippendorff's alpha <ref type="bibr" target="#b16">(Hayes and Krippendorff, 2007)</ref> for each pair-wise comparison, and all the results are fair agreement (0.2 ? ? ? 0.4) or moderate agreement (0.4 ? ? ? 0.6). The results show that our DMmodels significantly outperform baseline models in both the grammaticality and logicality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unconditional Text Generation</head><p>Setup We perform experiments of unconditional text generation on EMNLP2017 WMT News dataset<ref type="foot" target="#foot_2">4</ref> . We use the preprocessed data of a recent work<ref type="foot" target="#foot_3">5</ref>  <ref type="bibr" target="#b2">(Caccia et al., 2020)</ref>  sentences of DM-models are of better quality, while the consistently lower RLM scores also demonstrate that DM-models can generate more diverse sentences meanwhile.</p><p>In addition, each model is used to generate 1, 000 sentences with various sampling hyper-parameters, and GPT-2 Perplexity is further calculated. As shown in Table <ref type="table" target="#tab_4">5</ref>, our proposed method can make neural language models perform significantly better in terms of generation fluency. In particular, Transformer-based models can gain more significant improvement from DMLM. We conjecture that this is because, in our implementation, we directly uses the penultimate multi-head attention layer of Transformer to obtain the dependency attention distribution of DMLM. Thus, it can easily inherit all the strengths of Transformer-based models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human evaluation</head><p>Following previous work <ref type="bibr" target="#b48">(Yu et al., 2017;</ref><ref type="bibr" target="#b15">Guo et al., 2018)</ref>, we conduct a Turing test to further evaluate the generated text. In practice, we mix 100 randomly sampled sentences from each model, and another 100 sentences from the real test set. Five annotators are hired to judge whether each of the 900 sentences is created by human or machines. Each sentence gets +1 score when it is regarded as a real one, and 0 score otherwise. The detailed questionnaire and other details are shown in Appendix D.</p><p>The average score for each model is shown in Table <ref type="table" target="#tab_5">6</ref>, from which we can see all the DM-models surpass the baselines. Both automatic evaluations and human evaluations indicate that DMLM can help neural language models generate more readable, fluent, and natural sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Language Modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>We evaluate the proposed method with the word-level language modeling task by measuring Perplexity (PPL) on the Penn Treebank (PTB) <ref type="bibr" target="#b24">(Marcus et al., 1993;</ref><ref type="bibr" target="#b27">Mikolov et al., 2012)</ref> corpora.</p><p>The PTB dataset has a vocabulary size of 10, 000 unique words, and the training/validation/test set consists of 42, 068/3, 370/3, 761 sentences.</p><p>For this task, we mainly implement the DMLM on the RNN-based language model, i.e., AWD-LSTM <ref type="bibr" target="#b25">(Merity et al., 2018)</ref>. For a fair comparison, our DM-LSTM uses exactly the same hyperparameters and setups as AWD-LSTM. Since Transformer-based models' strong performance relies on training with large datasets, it will perform worse than random when trained on a small dataset <ref type="bibr">(Shen et al., 2021a)</ref>. We still report Transformer-based models' language modeling results on PTB in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We compare our method with its base model, AWD-LSTM, and we report the results along with other state-of-the-art models in Table <ref type="table" target="#tab_6">7</ref>. Compared with the AWD-LSTM, our DM-LSTM reduces the perplexity by 1.4 on the validation set and 1.1 on the test set, indicating that incorporating knowledge of syntactic structures in our proposed manner can substantially improve language modeling. Compared with other models that also leverage syntactic knowledge, our DM-LSTM strongly outperforms RNNG, PRPN, and URNNG. Moreover, though DM-LSTM does not make any changes to the architecture of the AWD-LSTM language model, it still achieves a comparable perplexity with ON-LSTM. Note that, since our method is model-agnostic, it can be harmonically combined with other state-of-the-art models, such as MoS <ref type="bibr" target="#b47">(Yang et al., 2018)</ref> and DOC <ref type="bibr" target="#b41">(Takase et al., 2018)</ref>.</p><formula xml:id="formula_9">5 2 2 7 U H G I L J X U H V R Q W K H V F U H H Q L Q G L F D W H I D O O L Q J V W R F N V</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visualization</head><p>We show how our proposed method works by visualizing the dependency attention distributions.</p><p>We use DM-Transformer to generate a sentence: "red figures on the screen indicate falling stocks."</p><p>For each generation step, we record this step's dependency attention distribution. When we finally generate the whole sentence, we get 9 distributions and plot Figure <ref type="figure" target="#fig_1">3</ref> from them. Each row in Figure <ref type="figure" target="#fig_1">3</ref> shows the dependency attention distribution of the model when generating the corresponding Y-axis token. When predicting the token "indicate", DMLM pays great attention to "figures". This is because these two tokens have a direct dependency connection in the dependency parse tree, and our method successfully captures this relationship. In addition, DMLM also helps the model better organize dependency information when the next-tokens, such as "screen" and "stocks", have dependencies on more than one token in the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Study</head><p>We perform case studies for a better understanding of the model performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Computational Complexity</head><p>Compared with vanilla RNN, our DM-RNN indeed increases the computational complexity from O(T ) to O(T 2 ). In practice, we can follow <ref type="bibr" target="#b26">Merity et al. (2017)</ref> to set a context window that allows DMLM looks L timesteps into the past at most, where L is the context length. However, our DMLM can efficiently apply to Transformer-based models without additional computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Works</head><p>Many previous studies have shown that leveraging the knowledge of syntactic structures can improve NLG <ref type="bibr" target="#b4">(Chelba, 1997;</ref><ref type="bibr" target="#b33">Roark, 2001;</ref><ref type="bibr" target="#b10">Emami and Jelinek, 2005;</ref><ref type="bibr" target="#b1">Buys and Blunsom, 2015)</ref>. <ref type="bibr" target="#b28">Mirowski and Vlachos (2015)</ref> incorporated syntactic dependencies into the RNN formulation, but they limited the scope to the scoring of complete sentences, not to next word prediction. Some other efforts have been done to integrate dependency structure into neural machine translation (NMT) from both the source and target side. However, all these methods, mainly based on RNN <ref type="bibr" target="#b40">(Sutskever et al., 2014)</ref>, incorporate knowl-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Story context:</head><p>[FEMALE] bought packets of vegetable seeds from the store . she dug up the dirt in her garden .</p><p>[FEMALE] planted onions , cilantro , and tomatoes . <ref type="bibr">[FEMALE]</ref> watered the garden every night .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Golden Text:</head><p>by the end of the summer [FEMALE] had enough vegetables to make salsa .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRPN:</head><p>she got to work in the morning and was happy to have a garden .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LSTM:</head><p>[FEMALE] planted the plants and made it a huge success . AWD-LSTM:</p><p>[FEMALE] was happy to be helping her plants .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-LSTM:</head><p>soon , [FEMALE] had enough vegetables to grow in her garden ! Transformer:</p><p>she went to the store to buy the seeds . DM-Transformer: soon , [FEMALE] had her garden full of vegetables ! GPT-2:</p><p>[FEMALE] 's garden grew very quickly and dry . DM-GPT-2:</p><p>[FEMALE] now has fresh fruits and vegetables in her garden .</p><p>Table <ref type="table">8</ref>: Examples of conditional text generation on ROCStories dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Golden Text:</head><p>what this group does is to take down various different websites it believes to be criminal and leading to terrorist acts .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRPN:</head><p>the right point to pay for the purchase of a bike , that ' s all we want to do to build , build together the support that i need to get here .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LSTM:</head><p>it ' s great to know that my experience has changed my mind because i ' m not going to work because i ' ve had to talk about that .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWD-LSTM:</head><p>this is a tragic attack and it is understood that the pair will come up with a package of documents which may be possible .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-LSTM:</head><p>the win over bernie sanders was an emotional moment for clinton , who was running in the general election , though she lost their state of vermont . Transformer:</p><p>' i ' ve just been in that position so i ' ve never seen anything like this before , but it ' s something i have to say and i ' m going to go to and win this series . DM-Transformer: in the second quarter of 2015 , the outlook for consumer spending rose 8 . 6 per cent , but for the fourth quarter , the company said it expects to expand by 0 . 7 per cent .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2:</head><p>if i had said a bit of pressure , i would probably be in a different position if i was a coach . DM-GPT-2: they ' ve also said that it ' s difficult to know how many emails clinton actually sent to her in recent weeks or whether she would be the nominee . edge of syntactic structures by introducing complex architectural changes. Therefore, it can get very unwieldy to adapt them to other neural language models, such as Transformer and GPT-2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduce Dependency-based Mixture Language Models, which can incorporate knowledge of dependency structures into arbitrary auto-regressive generation models without any changes to the original architectures. Both automatic and human evaluation results in extensive experiments across different tasks and different architectures demonstrate the effectiveness and generalizability of our method.</p><p>In the future, we will explore to incorporate the dependency labels into our method, and combine our DMLM with more neural language models. Second, we would like to integrate other linguistic knowledge, such as constituency structures and semantic information, into neural language models in our manner. is set to 5e -5. GPT-2 is trained for 80 epochs, while DM-GPT-2 is first trained by dependency modeling for 40 epochs, and then trained by language modeling in Equation 4 for 40 epochs.</p><p>For all the models, we select the best checkpoint according to the loss of validation set for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Unconditional Text Generation</head><p>The dataset statistics of EMNLP2017 WMT News dataset is reported in Table <ref type="table" target="#tab_9">11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>Validation Test #Stories 268,586 10,000 10,000 The context length for DM-LSTM is set to 36. LSTM baselines are trained for 500 epochs with batch size 300. DM-LSTM is first trained by dependency modeling objective for 100 epochs with batch size 300, and then by language modeling for 400 epochs with batch size 200. Besides, all the other experimental setups are the same with those for the conditional text generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Language Modeling</head><p>The dataset statistics of Penn Treebank dataset is reported in Table <ref type="table" target="#tab_10">12</ref>.</p><p>Train Validation Test #Stories 42,068 3,370 3,761 The context length for DM-LSTM is set to 16. DM-LSTM is trained for 1000 epochs with batch size 20, following <ref type="bibr" target="#b25">(Merity et al., 2018)</ref>. Besides, all the other experimental setups are the same with those for the conditional text generation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Impact of the Dependency Parser</head><p>In our work, we use an off-the-shelf dependency parser to get the dependency parse trees for dependency modeling. Consequently, the better the quality of dependency parsing, the better the performance of our method. HPSG Parser <ref type="bibr" target="#b51">(Zhou and Zhao, 2019)</ref>, the dependency parser we use, is one of the state-state-of-the-art parsers. This ensures the high quality of parsing results. <ref type="bibr" target="#b51">Zhou and Zhao (2019)</ref> trained HPSG Parser with the training set of PTB, and kept the test set held-out. So, when we do language modeling on PTB, the parser will not inject any future predictions that contribute to testing.</p><p>HPSG Parser maintains high-quality on outof-domain text, as shown in its paper <ref type="bibr" target="#b51">(Zhou and Zhao, 2019)</ref>. Most importantly, even on the out-of-domain datasets, i.e., ROCStories and EMNLP2017 WMT News, our work can still obtain a significant improvement, as shown in Section 3.1 and Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Language Modeling on</head><p>Transformer-based Models</p><p>The language modeling results of Transformerbased models evaluated on PTB dataset are shown in following Table <ref type="table" target="#tab_11">13</ref>. The good performance of Transformer-based models often rely on training with large datasets, but PTB is a very small dataset. Therefore, Transformer-based models perform worse than LSTM-based models, as shown in Table <ref type="table" target="#tab_6">7</ref> and Table 13. However, our DM-models still substantially reduce the perplexity compared with base models. DM-Transformer improves the base Transformer by over 20 perplexity points on both the validation and test set, and DM-GPT-2 also improves the base GPT-2 by almost 4 perplexity points. These results further confirm the effectiveness our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human Evaluation</head><p>We post the human evaluation questionnaire, as shown in Table <ref type="table" target="#tab_12">14</ref> and<ref type="table" target="#tab_13">Table 15</ref>, and then recruit five workers with sufficient high English skills. We pay each worker 45 US dollars, and let them complete the evaluation within a week.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Generated Examples</head><p>For a more general comparison, we present more generated examples of unconditional text generation in Table <ref type="table" target="#tab_14">16</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Description</head><p>Each story contains about five sentences. For each story, we will put the first four sentences into two different systems, and then systems generate the last sentence. The requirement for this manual evaluation is to judge which story better complies with the English grammar norm, and is more logically related to the first four sentences. NOTE that the names in all stories are replaced with "[MALE]" or "[FEMALE]" or "[NEUTRAL]", and all the sentences are preprocessed by lowercasing, separating punctuation, and splitting conjunctions. They are not grammar errors. Please ignore these when evaluating and do not allow them to affect your judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Criterion</head><p>You need to compare the stories from two metrics: grammaticality and logicality. And the two metrics are independent of each other. One of the judgments should not have any influence on the other one. Specific criteria for evaluating are as follows: 1. Grammaticality In the process of evaluating grammaticality, it should be considered whether the statement itself complies with the English standard usage. Then annotate which story is better at grammaticality. You may not care about what the generated sentences are saying but only if there are any grammatical problems in the sentence itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Logicality</head><p>In the process of evaluating logicality, you need to carefully read the whole story including the first four sentences and the generated sentence, and compare stories in logicality. Then annotate which story is better at logicality in terms of the coherence to the given beginnings and the inter-sentence causal and temporal dependencies. In this process, you may encounter sentences that are not completely grammatical.Please make a logical evaluation based on the main part of the sentence (such as some keywords, etc.) and what you can intuitively feel. Under the circumstances, the story can be judged totally illogical only if the grammar is too poor to understand the meaning or the logic is unreasonable. Notes ? Again, the grammaticality and logicality of the story are two independent metrics. Some very logically inappropriate generated stories are good in the grammaticality part, and there are some stories with obvious grammatical errors but they don't affect the respective judgment.</p><p>? Sometimes, there may be more than one kind of reasonable story for a beginning. Please do not limit your imagination. As long as the story is logically reasonable, direct, and able to make sense, it can be judged good in logicality.</p><p>? Some stories may not be accurately judged. In the process of determining the comparison of this type of two stories, according to your own understanding of the examples and the subjective feelings of the stories, choose a better story you think is the most appropriate. Please ensure that your evaluation criterion for different stories is the same. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task Description</head><p>In this review, you will read 900 sentences. For each sentence, you should determine whether the sentence is written by human. Note: All the sentences are preprocessed by lowercasing, separating punctuation, and splitting conjunctions. They are not grammar errors. Some sentences may have a specific context, or they may be talking about completely fictitious things. Please ignore these when evaluating and do not allow them to affect your judgments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Criterion</head><p>The judgment can mainly depend on your own understanding and the subjective feelings. But fluency, readability, engagement (whether you felt interested about the sentence), and anything else that you think is important can also help you make a decision. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Golden Text:</head><p>over 1 , 600 a day have reached greece this month , a higher rate than last july when the crisis was already in full swing . " we ' re working through a legacy period , with legacy products that are 10 or 20 years old , " he says . ' the first time anyone says you need help , i ' m on the defensive , but that ' s all that i know . out of those who came last year , 69 per cent were men , 18 per cent were children and just 13 per cent were women .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRPN:</head><p>as a mother , i can ' t work to be working on some kind of stuff , but i ' m not really sure that the single market is going to be as bad as i ' m on . in fact , there is a good position to focus on this and that will be a clear opportunity for the us to make sure that we do not have any concerns . there ' s still more opportunities than that , but this is what you ' re talking about , but it ' s not right . as well as a labour party , the former party member who claimed the vote in the referendum on whether to vote to leave the eu should be questioned .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LSTM:</head><p>so they did that because we ' ve been saying they ' re going to be fighting for this state , but they ' re going to keep going . the official said they were hoping to make a contribution in its strong inflation growth in the future , and that a more conservative leader could look for jobs and be stronger . it ' s something that i think are a good team , the first place to do it and i ' m really happy . ' there ' s no question that the person we ' re going to take is probably an important thing to be asked , " said john .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AWD-LSTM:</head><p>in this month ' s election , the u . s . economy has fallen in the past few years , a higher than a decade ago . in the last year i had been an 18 -year -old woman in my two -year -old son . it was a great test for me to try to get back on the bench and be there , it ' s a huge challenge for us . i just think it ' s important for us to do something that would help them in the best way we can to do it .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-LSTM:</head><p>" the united states has to come to mind that the threat of climate change is less of a serious issue , " the pentagon said in a statement . in the event of an initial campaign for the democratic nomination , he had released some of the most controversial ads that they had been speaking about since he was a president . there is an example of a presidential candidate who has been on the debate trail for more than a year . the central bank of japan is set to raise its benchmark interest rate at its first time in nearly a decade .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer:</head><p>you can ' t get away with things that are better than you did at home and hopefully get better than not the first team . in the case of the cases , the nsw government said it would accept 10 , 000 additional emergency costs if it did not help the industry . if there is an oil price that is at stake , it is not as far as the price of oil . the country has promised to build a nationwide population of about 150 , 000 to more than 2 , 000 , with a budget to help in building more affordable housing .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-Transformer:</head><p>in this particular area , as in the modern world , he is seen as someone who takes the risk of suffering a heart attack . that ' s why we ' re talking about the second half of the year , and a lot of people have asked us to do the best we can . the vast majority of american voters , particularly those who chose trump , said that he had changed the result . so this is a big step , and i ' m really excited to be part of the new york olympics .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT-2:</head><p>the reason is that the student community who doesn ' t know what he ' s talking about , or who ' s not even a businessman , he ' s going to take care of itself . the difference is that the reality of " brexit " has been the single largest trading partner in the world , and now is it . the game is now used to push for players to learn from them and learn from them and also play in the front of them . the first woman to run for president is to make a case for a woman she wants to make as president of the united states .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DM-GPT-2:</head><p>" i just thought that the whole picture was a strange story , " he said in a telephone interview on thursday . " the importance of local authorities is very strong , " she said in an interview on friday afternoon . we are working closely with the government to resolve this issue and have to work with local authorities to resolve the problem . a final verdict will be held on thursday at the supreme court in washington on march 15 , 2017 . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>figures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Visualization of dependency attention distributions. We left-shift the sentence by one step in the y-axis to better display the attention between the predicted next-token and the context in each row.</figDesc><graphic url="image-1.png" coords="8,99.30,74.15,160.90,160.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc><ref type="bibr" target="#b11">Eriguchi et al. (2016)</ref> proposed a tree-to-sequence attentional NMT model where source-side parse tree was used.<ref type="bibr" target="#b45">Wu et al. (2017)</ref> involved target syntactic trees into NMT model to jointly learn target translation and dependency parsing.<ref type="bibr" target="#b3">Casas et al. (2020)</ref> introduced a syntactic inductive bias to NLG in an iterative nonautoregressive way.For neural language models, recently, Dyer et al. (2016) proposed recurrent neural network grammar (RNNG) to jointly model syntax and surface structure by incrementally generating a syntax tree and sentence. Subsequent work<ref type="bibr" target="#b20">(Kim et al., 2019)</ref> extended the model to an unsupervised version. Shen et al. (2018) introduced the Parsing-Reading-Predict Networks (PRPN) to calculate syntactic distances among words and use self-attention to compose previous states. Its subsequent work (Shen et al., 2019) transferred the distance notion to LSTM cell, and introduced Ordered Neurons LSTM (ON-LSTM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation results for the conditional text generation task on Rocstories dataset. denotes that DM-model significantly outperforms the second best model for t-test (p-value&lt;0.05).</figDesc><table><row><cell>3 ?</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p><p><p>Human evaluation results for the conditional text generation task on Rocstories dataset. ? denotes the interannotator agreement Krippendorff's alpha</p><ref type="bibr" target="#b16">(Hayes and Krippendorff, 2007)</ref> </p>score. means statistical significance for Wilcoxon signed-rank test (p-value&lt;0.01). Note that, it is relatively easy for both models to generate a single sentence that is grammatically correct, so the rate of "tie" in Grammaticality is relatively high.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results of global metrics for the unconditional text generation task on EMNLP2017 WMT News.</figDesc><table><row><cell>Models</cell><cell cols="2">LM score ? RLM score ?</cell></row><row><cell>PRPN</cell><cell>5.24</cell><cell>5.75</cell></row><row><cell>ON-LSTM</cell><cell>5.20</cell><cell>5.59</cell></row><row><cell>AWD-LSTM</cell><cell>5.18</cell><cell>5.64</cell></row><row><cell>DM-LSTM</cell><cell>5.14</cell><cell>5.52</cell></row><row><cell>Transformer</cell><cell>5.00</cell><cell>5.59</cell></row><row><cell>DM-Transformer</cell><cell>4.97</cell><cell>5.49</cell></row><row><cell>GPT-2</cell><cell>4.89</cell><cell>5.55</cell></row><row><cell>DM-GPT-2</cell><cell>4.67</cell><cell>5.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>GPT-2 Perplexity on 1, 000 random samples with various sampling hyper-parameters generated by models trained on EMNLP2017 WMT News dataset. Nucleus sampling is used here with various p. denotes that DMmodel significantly outperforms the second best model for t-test (p-value&lt;0.05).</figDesc><table><row><cell>, we conduct pair-wise com-</cell></row><row><cell>parisons between DM-models and corresponding</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Turing test results of the samples generated by models trained on EMNLP2017 WMT News dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>that contains 5, 268 distinct words with maximum sentence length 51. The training/validation/test set consists of 268, 586/10, 000/10, 000 sentences.Following<ref type="bibr" target="#b2">Caccia et al. (2020)</ref>, we evaluate the models with the global metrics<ref type="bibr" target="#b34">(Semeniuta et al., 2018)</ref>: (1) Language Model score (LM score):We use the oracle Language Model to evaluate the negative log-likelihood of generated text as the metric to reflect quality; (2) Reverse Language Model score (RLM score) We train a new Language Model on the generated text, and then evaluate the negative log-likelihood of a held-out set of real text. This metric can measure text diversity since the generated text with better diversity would have a broader coverage over the real data space, and the new Language Model can be trained better, thus leading to lower RLM score. Both the LM score and RLM score are usually evaluated on the sentences generated by purely random sampling. Besides, to further measure the generation fluency, we directly use the public GPT-2 checkpoint of pretrained parameters without finetuning to calculate GPT-2 Perplexity of generated samples. Results Table4shows the results of global metrics obtained by various models. All the DMmodels again outperform the baselines. The consistently lower LM scores indicate that the generated Various language models' perplexity evaluated on validation and test sets of Penn Treebank dataset.<ref type="bibr" target="#b47">Yang et al. (2018)</ref> and<ref type="bibr" target="#b41">Takase et al. (2018)</ref> focus on improving the softmax of LSTM LM, which are orthogonal to ours.</figDesc><table><row><cell>Models</cell><cell cols="3">#Params Dev PPL Test PPL</cell></row><row><cell cols="2">Pointer Sentinel-LSTM (Merity et al., 2017) 21M</cell><cell>72.4</cell><cell>70.9</cell></row><row><cell>RNNG (Dyer et al., 2016)</cell><cell>-</cell><cell>-</cell><cell>88.7</cell></row><row><cell>Variational RHN (Zilly et al., 2017)</cell><cell>23M</cell><cell>67.9</cell><cell>65.4</cell></row><row><cell>PRPN (Shen et al., 2018)</cell><cell>-</cell><cell>-</cell><cell>62.0</cell></row><row><cell>Fraternal dropout (Zolna et al., 2018)</cell><cell>24M</cell><cell>58.9</cell><cell>56.8</cell></row><row><cell>URNNG (Kim et al., 2019)</cell><cell>-</cell><cell>-</cell><cell>85.9</cell></row><row><cell>ON-LSTM (Shen et al., 2019)</cell><cell>25M</cell><cell>58.3</cell><cell>56.2</cell></row><row><cell>AWD-LSTM (Merity et al., 2018)</cell><cell>24M</cell><cell>60.0</cell><cell>57.3</cell></row><row><cell>DM-LSTM (Ours)</cell><cell>24M</cell><cell>58.6</cell><cell>56.2</cell></row><row><cell>AWD-LSTM-MoS(Yang et al., 2018)</cell><cell>22M</cell><cell>56.5</cell><cell>54.4</cell></row><row><cell>AWD-LSTM-DOC(Takase et al., 2018)</cell><cell>23M</cell><cell>54.1</cell><cell>52.4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 8 provides examples of conditional text generation produced by our DM-models and other baselines. Obviously, all the DM-models can generate more reasonable and coherent story endings. Additionally, some examples of unconditional text generation are shown in Table 9 and Appendix E. These examples show that our DMLM can help base models generate more reasonable, readable, fluent, and natural sentences.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Examples of unconditional text generation on EMNLP2017 WMT News dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 11 :</head><label>11</label><figDesc>Statistics of EMNLP2017 WMT News dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>Statistics of Penn Treebank dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>Transformer-based models' perplexity evaluated on validation and test sets of Penn Treebank dataset.</figDesc><table><row><cell>Models</cell><cell cols="3">#Params Dev PPL Test PPL</cell></row><row><cell>Transformer</cell><cell>24M</cell><cell>100.7</cell><cell>106.7</cell></row><row><cell cols="2">DM-Transformer 24M</cell><cell>80.6</cell><cell>84.6</cell></row><row><cell>GPT-2</cell><cell>163M</cell><cell>62.6</cell><cell>55.2</cell></row><row><cell>DM-GPT-2</cell><cell>163M</cell><cell>58.8</cell><cell>51.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>Human evaluation questionnaire for conditional text generation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 :</head><label>15</label><figDesc>Human evaluation questionnaire for unconditional text generation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 16 :</head><label>16</label><figDesc>Examples of unconditional text generation on EMNLP2017 WMT News dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/DoodleJZ/ HPSG-Neural-Parser</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>We use the preprocessed data in https://github.com/thucoai/Stylized-Story-Generation-with-Style-Guided-Planning</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://statmt.org/wmt17/ translation-task.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/pclucas14/ GansFallingShort/tree/master/real_data_ experiments/data/news</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/pytorch/fairseq</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Key R&amp;D Program of China</rs> (No.<rs type="grantNumber">2018YFB1005100</rs>), <rs type="projectName">Bejing Academy of Artificial Intelligence (BAAI)</rs> and <rs type="funder">State Key Laboratory of Media Convergence Production Technology and Systems</rs>. We appreciate the anonymous reviewers for their helpful comments. <rs type="person">Xiaojun Wan</rs> is the corresponding author.</p></div>
			</div>
			<div type="funding">
<div><p>External Parameters? External Networks? Architecture Agnostic? RNNG (Dyer et al., 2016) Yes Yes No PRPN (Shen et al., 2018) Yes Yes No URNNG (Kim et al., 2019) <rs type="funder">Yes Yes No ON-LSTM</rs> (Shen et al., 2019) Yes No No DMLM (Ours) No or Negligible No Yes</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_75fHpcP">
					<idno type="grant-number">2018YFB1005100</idno>
					<orgName type="project" subtype="full">Bejing Academy of Artificial Intelligence (BAAI)</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>All the algorithms are implemented in Pytorch and trained on a machine with 8 NVIDIA GTX 2080Ti GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Conditional Text Generation</head><p>The dataset statistics of ROCStories dataset is reported in Table <ref type="table">10</ref>.  In this task, both the DM-LSTM and base LSTM are built on a AWD-LSTM language model with an embedding size of 400 and hidden layer units 1150. The dropout rates are 0.4, 0.25, 0.4 for the output of the last layer, outputs between LSTM layers, and input embedding layers, respectively. The weight dropout for the RNN hidden to hidden matrix is 0.5, and the dropout rate to remove words from embedding layer is 0.1. The context length for DM-LSTM is set to 56. For PRPN and ON-LSTM, we keep their original settings.</p><p>In this task, all the models are trained on a singe GPU with learning rate 30, weight decay 1.2e -6. LSTM baselines are trained for 500 epochs with batch size 100. DM-LSTM is first trained by dependency modeling objective for 100 epochs with batch size 80, and then by language modeling in Equation 4 for 400 epochs with batch size 60 due the computational budgets limit.</p><p>For both the DM-Transformer and base Transformer, we use a standard 6-layer Transformer language model with 8 attention heads, embedding dimension 512, projection dimension 2048 and dropout rate 0.1. During training, we use Adam optimizer with ? 1 = 0.9, ? 2 = 0.98, weight decay 0.01 and learning rate 5e -4, and apply the dynamic batching provided by fairseq 6 to train both the models with 4 GPUs. Transformer is trained for 60 epochs, while DM-GPT-2 is first trained by dependency modeling for 30 epochs, and then trained by language modeling in Equation 4 for 30 epochs.</p><p>We use the pretrained GPT-2-base model for both the DM-GPT-2 and base GPT-2. In this comparison, we apply the same training settings with Transformer-base models except that learning rate</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">You only need attention to traverse trees</title>
		<author>
			<persName><forename type="first">Mahtab</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Rifayat Samee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mercer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="316" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative incremental dependency parsing with neural networks</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26">2015. 2015. July 26-31, 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="863" to="869" />
		</imprint>
	</monogr>
	<note>Beijing</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language gans falling short</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Syntax-driven iterative expansion language models for controllable text generation</title>
		<author>
			<persName><forename type="first">Noe</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><forename type="middle">R</forename><surname>Fonollosa</surname></persName>
		</author>
		<author>
			<persName><surname>Costajuss?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Structured Prediction for NLP@EMNLP 2020, Online, November 20</title>
		<meeting>the Fourth Workshop on Structured Prediction for NLP@EMNLP 2020, Online, November 20</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A structured language model</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>UNED</publisher>
			<date type="published" when="1997-07-12">1997. 7-12 July 1997</date>
			<biblScope unit="page" from="498" to="500" />
		</imprint>
		<respStmt>
			<orgName>Universidad Nacional de Educaci?n a Distancia</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hierarchical multiscale recurrent neural networks</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploiting syntactic structure for better language modeling: A syntactic distance approach</title>
		<author>
			<persName><forename type="first">Wenyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>O'donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online</meeting>
		<imprint>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="6611" to="6628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent neural network grammars</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="199" to="209" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A neural syntactic language model</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Jelinek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="195" to="227" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tree-to-sequence attentional neural machine translation</title>
		<author>
			<persName><forename type="first">Akiko</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016-08-07">2016. 2016. August 7-12, 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Jointly learning to align and translate with transformer models</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Udhyakumar</forename><surname>Nallasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Paulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="4452" to="4461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A knowledge-enhanced pretraining model for commonsense story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="93" to="108" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">UNION: an unreferenced metric for evaluating open-ended story generation</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="9157" to="9166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long text generation via adversarial training with leaked information</title>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)<address><addrLine>Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>New Orleans</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="5141" to="5148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Answering the call for a standard reliability measure for coding data</title>
		<author>
			<persName><forename type="first">F</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><surname>Krippendorff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communication methods and measures</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="89" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning hierarchical structures on-the-fly with a recurrent-recursive model for sequences</title>
		<author>
			<persName><forename type="first">Athul</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP, Rep4NLP@ACL 2018</title>
		<meeting>The Third Workshop on Representation Learning for NLP, Rep4NLP@ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-20">2018. July 20, 2018</date>
			<biblScope unit="page" from="154" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised recurrent neural network grammars</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G?bor</forename><surname>Melis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1105" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stylized story generation with style-guided planning</title>
		<author>
			<persName><forename type="first">Xiangzhe</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziquan</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="page" from="2430" to="2436" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better</title>
		<author>
			<persName><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1426" to="1436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A diversity-promoting objective function for neural conversation models</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Santorini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcinkiewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularizing and optimizing LSTM language models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Conference Track Proceedings</publisher>
			<date type="published" when="2018-04-30">2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Statistical language models based on neural networks. Presentation at Google, Mountain View</title>
		<author>
			<persName><forename type="first">Tom??</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-04-02">2012. 2nd April</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dependency recurrent neural language models for sentence completion</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2015-07-26">2015. 2015. July 26-31, 2015</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="511" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>San Diego California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-12">2016. June 12-17, 2016</date>
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Palm: A hybrid parser and language model</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. 2019. November 3-7, 2019</date>
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Probabilistic top-down parsing and language modeling</title>
		<author>
			<persName><forename type="first">Brian</forename><surname>Roark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="276" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On accurate evaluation of gans for language generation</title>
		<author>
			<persName><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno>CoRR, abs/1806.04936</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural language modeling by jointly learning syntax and lexicon</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chin-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. 2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">2021a. Explicitly modeling syntax in language models with incremental parsing and a dynamic oracle</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siva</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">June 6-11, 2021</date>
			<biblScope unit="page" from="1660" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">2021b. Structformer: Joint unsupervised induction of dependency and constituency structure from masked language modeling</title>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="page" from="7196" to="7209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-10">2013. 18-21 October 2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
	<note>A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. 2014. December 8-13 2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Direct output connection for a high-rank language model</title>
		<author>
			<persName><forename type="first">Sho</forename><surname>Takase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="4599" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tree transformer: Integrating tree structures into self-attention</title>
		<author>
			<persName><surname>Yau-Shian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-03">2019. 2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1061" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do latent tree learning models identify meaningful structure in sentences?</title>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Drozdov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="253" to="267" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence-to-dependency neural machine translation</title>
		<author>
			<persName><forename type="first">Shuangzhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017-07-30">2017. July 30 -August 4</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="698" to="707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Syntaxenhanced pre-trained model</title>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Virtual Event</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5412" to="5422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank RNN language model</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. 2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Seqgan: Sequence generative adversarial nets with policy gradient</title>
		<author>
			<persName><forename type="first">Lantao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-02-04">2017. February 4-9, 2017</date>
			<biblScope unit="page" from="2852" to="2858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with BERT</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Syntax-infused variational autoencoder for text generation</title>
		<author>
			<persName><forename type="first">Xinyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2069" to="2078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Head-driven phrase structure grammar parsing on penn treebank</title>
		<author>
			<persName><forename type="first">Junru</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28">2019. July 28-August 2, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2396" to="2408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Texygen: A benchmarking platform for text generation models</title>
		<author>
			<persName><forename type="first">Yaoming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sidi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxian</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1097" to="1100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rupesh</forename><forename type="middle">Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08">2017. August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="4189" to="4198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fraternal dropout</title>
		<author>
			<persName><forename type="first">Konrad</forename><surname>Zolna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dendi</forename><surname>Suhubdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30">2018. 2018. April 30 -May 3, 2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
