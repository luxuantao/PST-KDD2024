<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Fusion-based Enhancing Method for Weakly Illuminated Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-05-31">May 31, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Delu</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yinghao</forename><surname>Liao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
						</author>
						<author>
							<persName><forename type="first">John</forename><surname>Paisley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Columbia University</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<settlement>Xiamen</settlement>
									<region>Fujian</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Fusion-based Enhancing Method for Weakly Illuminated Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-05-31">May 31, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">71E21D758EE8267409541EAD993A2246</idno>
					<idno type="DOI">10.1016/j.sigpro.2016.05.031</idno>
					<note type="submission">Preprint submitted to Signal Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>image enhancement</term>
					<term>multi-scale fusion</term>
					<term>weakly illumination</term>
					<term>weights</term>
					<term>illumination adjustment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a straightforward and efficient fusion-based method for enhancing weakly illumination images that uses several mature image processing techniques. First, we employ an illumination estimating algorithm based on morphological closing to decompose an observed image into a reflectance image and an illumination image. We then derive two inputs that represent luminance-improved and contrast-enhanced versions of the first decomposed illumination using the sigmoid function and adaptive histogram equalization. Designing two weights based on these inputs, we produce an adjusted illumination by fusing the derived inputs with the corresponding weights in a multi-scale fashion. Through a proper weighting and fusion strategy, we blend the advantages of different techniques to produce the adjusted illumination. The final enhanced image is obtained by compensating the adjusted illumination back to the reflectance. Through this synthesis, the enhanced image represents a trade-off among detail enhancement, local contrast improvement and preserving the natural feel of the image. In the proposed fusion-based framework, images under different weak illumination conditions such as backlighting, non-uniform illumination and nighttime can be enhanced.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In weakly illuminated environments, the quality of images and video captured by optical imaging devices are often degraded. This can reduce the performance of particular systems, such as those used in intelligent traffic analysis, visual surveillance, and consumer electronics. For example, the low lighting conditions in nighttime environments can produce images and video with low contrast, reducing visibility. Another example is backlighting, where objects and details cannot be simultaneously captured in both bright regions (background) and dark regions (foreground) because of limitations in the exposure setting of many imaging systems <ref type="bibr" target="#b0">[1]</ref>.</p><p>In this paper, we present a fusion-based method for enhancing weakly illuminated images that can handle different weak illumination settings within the same framework. There are three main contributions of our method. First, our method is applicable to singles image, which differs from several other fusion techniques that require multiple images of the same scene to perform fusion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Second, we propose a simple illumination estimating algorithm based on morphological closing. This estimated illumination represents the naturalness and luminance. Third, we propose an effective fusion-based algorithm to adjust the estimated illumination. In the fusion stage, different mature techniques can be directly adopted to generate different results, then different advantages of these results can be blended to obtain an enhanced image. Practically speaking, using simple and mature algorithms in the proposed way allows ensures for simple updating of existing systems. Since most calculations are at the pixel-level, the proposed method is computationally very efficient. Before giving an overview of the proposed method in Section 1.2, we give a review of the main approaches and major techniques to the image enhancement problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Many image enhancement techniques have been proposed to improve the quality of degraded images captured in varying circumstances. Three major approaches include histogram-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, Retinex-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and filtering-based methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. We briefly review these below, as well as potential shortcomings that we aim to address in our proposed algorithm.</p><p>Methods based on histograms aim to generate an output image having a histogram with a desired shape. Some conventional histogram-based meth-ods, such as histogram equalization (HE) <ref type="bibr" target="#b4">[5]</ref> and contrast-limited adaptive histogram equalization (CLAHE) <ref type="bibr" target="#b5">[6]</ref>, are widely used because of their simplicity and effectiveness in enhancing low contrast images, but can lead to contrast over-enhancement and noise amplification <ref type="bibr" target="#b10">[11]</ref>. This is because HEbased methods assume that the target histogram is uniformly distributed and mechanically stretch the dynamic range without paying attention to the shape of input histogram. When high peaks exist in input histograms, the enhanced result by HE may appear unnatural or have artifacts. Recently, a global contrast enhancement algorithm was proposed that uses spatial information to preserve the shape of the input histogram and suppress overenhancement <ref type="bibr" target="#b12">[13]</ref>. Variational methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> use different regularization terms on the histogram. For example, in <ref type="bibr" target="#b9">[10]</ref> contextual and variational contrast enhancement (CVC) is performed by a histogram mapping that emphasizes large gray-level differences. The method of <ref type="bibr" target="#b9">[10]</ref> was improved by using a layered difference representation (LDR) of 2-D histograms <ref type="bibr" target="#b0">[1]</ref>. However, variational methods tend to fail on images with very dark regions since the stretching range of histogram is constrained in the model.</p><p>In Retinex theory <ref type="bibr" target="#b28">[29]</ref>, the image is taken to be a product of reflectance and illumination. Some Retinex-based algorithms take the reflectance as the enhanced result by estimating and removing illumination. The representative algorithms such as the single-scale Retinex (SSR) <ref type="bibr" target="#b14">[15]</ref> and the multiscale Retinex (MSR) <ref type="bibr" target="#b15">[16]</ref> use local Gaussian filters to separate reflectance and illumination. Details and luminance can be improved by eliminating the influence of illumination, but results obtained by such center/surround approaches are often over-enhanced; since the human visual system reduces the dynamic range of scenes rather than fully removing the illumination, illumination is arguably essential to a natural representation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20]</ref>. The "naturalness-preserving enhancement algorithm" (NPEA) addresses this issue in non-uniformly illuminated images <ref type="bibr" target="#b19">[20]</ref> by first designing a bright-pass filter to estimate the illumination, and then proposing a bi-log transformation to adjust the illumination.</p><p>A final set of methods are based on image filtering. In <ref type="bibr" target="#b21">[22]</ref>, a globally optimized linear windowed (GOLW) tone mapping algorithm compresses high dynamic range using local linear filtering. This method is not intended for the extremely weak illumination setting, such as nighttime and backlighting, and is also computationally intensive. In <ref type="bibr" target="#b22">[23]</ref>, the authors propose a generalized unsharp masking (GUM) algorithm in which they decompose an image into high-frequency and low-frequency parts for separate processing. Other approaches include <ref type="bibr" target="#b23">[24]</ref>, where an automatic exposure correction is performed on image regions, and <ref type="bibr" target="#b24">[25]</ref>, where illumination from a naturally-lit image and details from a flash image are combined for image enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Overview of proposed method</head><p>We focus on the case of image enhancement in very weakly illumination conditions. Our goal is to develop a fast and effective fusion method that blends the advantages of different mature image processing techniques. Before giving more details in Section 3, we outline the main idea of the proposed method below and depict an overview of the proposed method in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>First, we propose an algorithm for estimating illumination based on a morphologically closing operation. This step is inspired by the previous Retinex-based method <ref type="bibr" target="#b19">[20]</ref>, which estimates the illumination using a brightpass filter. However, the filtering operation requires a high computational time, which is not suitable for practical applications. In contrast, the proposed algorithm calculates the illumination efficiently with fewer parameters. The estimated illumination effectively represents the naturalness and luminance for further processing in our method.</p><p>Second, we propose a fusion-based enhancement algorithm to adjust the illumination. Since many mature methods have specific enhancing effect and are easy to implement, e.g., CLAHE <ref type="bibr" target="#b5">[6]</ref> enhances local contrast and can be accelerated using interpolation techniques, their respective advantages should be combined. We derive two new inputs that represent an improved global luminance and an enhanced local contrast of the estimated illumination, and then attempt to blend the advantages of each into a single output. To this end, we design two weights for fusion where higher value is assigned to pixels that play a more essential role to the quality of the desired image according to a quality measure. We also introduce a multi-scale framework to avoid artifacts. The final enhanced image is generated by combining the adjusted illumination with the reflectance.</p><p>The rest of this paper is organized as follows. Section 2 briefly describes the motivation of the proposed algorithm. The algorithm is detailed in Section 3. Performances of the proposed method is evaluated both quantitatively and qualitatively in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation</head><p>For weakly illuminated images, a natural and intuitive way to solve this problem is simply to adjust illumination since it is linked to the naturalness of the image <ref type="bibr" target="#b19">[20]</ref> and the Human Vision System is sensitive to variations in luminance <ref type="bibr" target="#b29">[30]</ref>. In other words, both the objective (detail enhancement) and subjective (naturalness) quality of the image is linked to its illumination. However, for some illumination-estimating algorithms based on Retinex theory, parameters are not easily designed <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, while other methods require a high computation time due to solving a large set of linear equations <ref type="bibr" target="#b18">[19]</ref> or employing patch-based filtering <ref type="bibr" target="#b19">[20]</ref>. Hence the first important task is to estimate the illumination effectively and efficiently.</p><p>The second task is to properly adjust the estimated illumination for weakly illuminated images. As is well-known, image quality assessment is related to the Human Vision System, which is sensitive to luminance and contrast <ref type="bibr" target="#b30">[31]</ref>. However, local contrast may be reduced when global luminance is compressed to enhance dark regions in an image. Therefore, finding a trade-off between luminance and contrast is an important criterion for enhancing images. As shown in Figure <ref type="figure" target="#fig_1">2</ref> (b) and (c), dynamic range compression by gamma correction and the sigmoid function can produce images where global luminance improves, but local contrast is reduced. In the other direction shown in Figure <ref type="figure" target="#fig_1">2</ref> (d) and (e), global and local contrasts are enhanced by histogram equalization (HE) <ref type="bibr" target="#b4">[5]</ref> and contrast-limited adaptive histogram equalization (CLAHE) <ref type="bibr" target="#b5">[6]</ref> at the cost of enhancing dark regions.</p><p>Our goal is to combine these methods such that their strengths are used, while their shortcomings are suppressed. The enhanced result in Figure <ref type="figure" target="#fig_1">2</ref> (f) illustrates the effectiveness of this proposed fusing strategy. The corresponding histogram shows that the histogram is well-stretched while still preserving its original shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Fusion-based Algorithm</head><p>The fundamental idea of the proposed fusion-based approach is to blend several inputs and weights derived from a single estimated illumination. As discussed above, the three criteria for enhancing weakly illuminated images are global luminance improvement, local contrast enhancement and preservation of naturalness. Hence inputs and weights are designed and processed based on these criteria. Four main steps constitute the proposed enhancing algorithm corresponding to the next four subsections: 1) illumination estimation, 2) input derivation, 3) weight definition and 4) multi-scale for input and weight fusion. We describe these separately below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Step 1: Illumination estimation</head><p>For weakly illuminated images we use the simplified physical model of light reflection. According to Retinex theory <ref type="bibr" target="#b28">[29]</ref>, a captured image is the product of reflectance and illumination written as</p><formula xml:id="formula_0">S c (x, y) = R c (x, y)I(x, y), (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where S is the measured image, R is the reflectance, I is the illumination, c is the color channel of RGB (red, green, blue) space and (x, y) is the pixel location. For simplicity, the three color channels are assumed to have the same illumination so that the color information is preserved in the reflectance. We aim to take into account the fact that the illumination contains information about luminance variance and naturalness <ref type="bibr" target="#b19">[20]</ref> and the common assumption that illumination is locally smooth <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. Many techniques exist for estimating the illumination and reflectance layers. For example, <ref type="bibr" target="#b31">[32]</ref> proposes a fast separation method by building two likelihoods from the gradient histograms. This method decomposes an image into a smooth layer and a detail layer, which removes reflection interference; <ref type="bibr" target="#b32">[33]</ref> decomposes an image into a structure layer and a texture layer to simultaneously enhance contrast and suppress jpeg artifacts.</p><p>Inspired by the dark channel prior <ref type="bibr" target="#b33">[34]</ref>, which calculates the local minimum in the three color channels for image dehazing, we propose the following illumination estimating algorithm. First we obtain an image lightness from the maximum value of its three color channels to represent luminance variance, which we can write as</p><formula xml:id="formula_2">L(x, y) = max c∈{R,G,B} S c (x, y). (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>This operation is the reverse of the first step for obtaining the dark channel, which finds the minimum of the three color channels. Second, since the illumination is locally smooth <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, some filter operations should be adopted to refine L. The authors of the dark channel further uses the minimum filter in an overlapped pattern to obtain the final dark channel image. However, this operation requires a high computational time. Since our goal is to develop an enhancing method that is computationally inexpensive, we wish to avoid using such computationally intensive methods such as those in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. To achieve both local smoothness and computational efficiency, we propose a simple but effective algorithm for illumination estimation by using a morphologically closing operation:</p><formula xml:id="formula_4">I = L • P 255 ,<label>(3)</label></formula><p>where P denotes the structuring element and • is the closing operation. Dividing by 255 is used to map the range to [0, 1] for the downstream operations. The morphologically closing operation smooths an image by fusing narrow breaks and filling gaps on the contours without over smoothing to produce halo effect. Through the observation of our experiments, choosing disk as the structuring element can obtain the best result. This operation is mature and easily implemented for practical applications <ref type="bibr" target="#b34">[35]</ref>.</p><p>The illumination can be effectively estimated by Equation (3). We refine the estimated illumination with a guided filter <ref type="bibr" target="#b35">[36]</ref> to preserve the shape of large contours. The filtering output is expressed as a weighted average at a pixel i:</p><formula xml:id="formula_5">I i ← j W ij (g)I j ,<label>(4)</label></formula><p>where i and j are pixel indexes, W is the filter kernel that is a function of the guidance image g. We use the kernel where ω k is a window centered at the pixel k and |ω| is the number of pixels in the window, µ k and σ 2 k are the mean and variance of g, ε is a regularization parameter. In this paper, the guidance image g is the V layer in HSV color space of the input image S.</p><formula xml:id="formula_6">W ij (g) = 1 |ω| 2 k:(i,j)∈ω k (1 + (g i -µ k )(g j -µ k ) σ 2 k + ε ) (<label>5</label></formula><formula xml:id="formula_7">)</formula><formula xml:id="formula_8">Images with k I , Weights B k W , Weights C k W Weights k W -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Step 2: Inputs derivation</head><p>In our proposed fusion-based approach, three inputs are derived from the estimated illumination. The first input, I 1 , is the original estimated illumination I. This contains information about the original structure of the image that allows us to avoid distortion. For example, the sky region in the top image in Figure <ref type="figure" target="#fig_2">3</ref> provides good contrast information.</p><p>The second input I 2 is designed to address the global luminance in order to help clarify the dark regions of the image. Many enhancing operators and functions can be adopted to improve global luminance, such as gamma correction and the sigmoid function. In this work, we compute the second input by using the arc tangent transformation of I:</p><formula xml:id="formula_9">I 2 (x, y) = 2 π arctan(λI(x, y)),<label>(6)</label></formula><p>where λ is a parameter that controls the degree of luminance. Multiplication by 2/π maps the range to [0, 1). Since different images have different degrees of luminance, λ should be changed adaptively. Thus an automatic λ setting is introduced based on our experiments:</p><formula xml:id="formula_10">λ = 10 + 1 -I mean I mean ,<label>(7)</label></formula><p>where I mean is the mean of estimated illumination I. It is obvious that a smaller I mean indicates a darker luminance, and thus a higher λ is obtained to make a higher level of the enhancement. Since the dynamic range is compressed after improving global luminance, local contrast is reduced (see the middle-left image in Figure <ref type="figure" target="#fig_2">3</ref>). Thus, the third input, I 3 , is designed to enhance local contrast by using "contrast local adaptive histogram equalization" (CLAHE) <ref type="bibr" target="#b5">[6]</ref>, which we apply directly to the estimated illumination I. CLAHE is useful for expanding the local contrast between adjacent structures. Additionally, this method is mature and can be accelerated using interpolation techniques. We therefore select CLAHE over other more complex methods, which may also be used to generate the third input. As the bottom-left image shows in Figure <ref type="figure" target="#fig_2">3</ref>, CLAHE enhances local contrast at the cost of reducing global luminance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Step 3: Weights definition</head><p>The next step involves designing weights for fusing the three input images derived above. We design pixel-level weights for fusion.</p><p>A brightness weight W B assigns high values to well-exposed pixels. A high quality image should have a natural-looking brightness, i.e., be neither too bright (over-exposed) nor too dark (under-exposed). To support this, we statistically evaluated 2000 well-exposed images from Google image. Using the illumination image of each obtained according to the previous section, we calculated the mean, the standard deviation and the histogram of the illumination values for each of 2000 images. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, the mean of the mean approximately equals 0.5 and the mean of the standard deviations approximately equals 0.25 after mapping the range to [0, 1]. Based on this observation, we set the brightness weight to be</p><formula xml:id="formula_11">W B,k (x, y) = exp - (I k (x, y) -0.5) 2 2(0.25) 2 ,<label>(8)</label></formula><p>where k denotes the kth derived input discussed in the previous section. A high value of W B means a well-exposed pixel, while a low value of W B is associated with a over-or under-exposed pixel. (See the second column in Figure <ref type="figure" target="#fig_2">3</ref>.) A second weight, which we call the chromatic contrast weight W C , evaluates the contrast by combing the estimated illumination with chromatic information. Color contrast is an important feature of image quality <ref type="bibr" target="#b36">[37]</ref>. This weight is computed based on the chromatic filtering formula <ref type="bibr" target="#b37">[38]</ref>,</p><formula xml:id="formula_12">W C,k (x, y) = I k (x, y)(1 + cos(αH(x, y) + φ)S(x, y)),<label>(9)</label></formula><p>where H is the hue and S is the saturation in HSV color space of the original input color image. The parameter α is used to preserve the color opponency and φ represents the offset angle of the color wheel. According to <ref type="bibr" target="#b37">[38]</ref>, we set α = 2 and φ = 250 • to preserve the most salient regions. The impact of this weight is to highlight regions containing high contrast caused by both illumination and color. (See the third column in Figure <ref type="figure" target="#fig_2">3</ref>.) We use these two weights to calculate a third weight W k ,</p><formula xml:id="formula_13">W k (x, y) = W B,k (x, y)W C,k (x, y).<label>(10)</label></formula><p>We then normalize this to give the final weight,</p><formula xml:id="formula_14">Wk (x, y) = W k (x, y) k W k (x, y) . (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the final weights Wk for the respective I k . As is evident, each I k has "good" regions and "bad" regions according to these weights. For example, the sky region in inputs I 1 and I 3 , and people in I 2 will be combined for global luminance improvement, and high contrast regions in I 3 will be used for local contrast enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Step 4: Multi-scale fusion</head><p>We use the above information to obtain the adjusted illumination as follows,</p><formula xml:id="formula_16">I f usion (x, y) = k Wk (x, y)I k (x, y).<label>(12)</label></formula><p>Since the sum of each Wk equals 1, the result's intensity scale is guaranteed to be the same as the inputs. However, as shown in Figure <ref type="figure" target="#fig_4">5</ref>(b), the naive fusing of Equation ( <ref type="formula" target="#formula_16">12</ref>) produces visually disappointing artifacts in the enhanced result. This is mainly caused by strong transitions of the weight maps. To overcome this limitation, multi-scale linear <ref type="bibr" target="#b38">[39]</ref> or non-linear filters <ref type="bibr" target="#b39">[40]</ref> are commonly adopted. The non-linear filters (e.g. WLS <ref type="bibr" target="#b39">[40]</ref>) generate similar result while requiring a complex computation. To improve the fused result, a multi-scale pyramidal technique <ref type="bibr" target="#b38">[39]</ref> is adopted. In <ref type="bibr" target="#b38">[39]</ref>, based on the Laplacian operator, each input image is represented as a sum of patterns computed at different scales. The inputs are convolved by a Gaussian kernel to generate a low pass filtered versions. In our case, we decompose each derived input I k into a Laplacian pyramid to extract image features, and each normalized weight Wk into a Gaussian pyramid to smooth the strong transitions. This method is effective since it blends image features instead of intensities. Moreover, both the Laplacian and Gaussian operators are mature techniques, which are easy to implement in practical applications.</p><p>Both the Gaussian and Laplacian pyramids have the same number of levels. We then fuse the pyramids by mixing each level of the Gaussian and Laplacian pyramids, written as</p><formula xml:id="formula_17">F l (x, y) = k G l { Wk (x, y)}L l {I k (x, y)}, (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>where l is the number of the pyramid levels, set equal to 6 in this paper, L{I k } is the Laplacian version of the derived input I k and G{ Wk } is the Gaussian version of the normalized map Wk . Each pyramid layer is successively obtained in a bottom-up way. The refined adjusted illumination is computed by summing each level of the fused pyramid,</p><formula xml:id="formula_19">I f inal (x, y) = l U d (F l (x, y)),<label>(14)</label></formula><p>where U d is the up-sampling operator with factor d = 2 l-1 . The final enhanced color image is generated by compensating the adjusted illumination back to the reflectance,</p><formula xml:id="formula_20">S c enhanced (x, y) = R c (x, y)I f inal (x, y). (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>In Figure <ref type="figure" target="#fig_4">5</ref>(c) we show the results of our fusion approach using this multiscale technique. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Discussion</head><p>We present experimental results to demonstrate the performance of the proposed fusion method. We ran all experiments using Matlab R2014a on a PC with a 2.60GHz Intel Pentium Dual Core Processor and 4G RAM. More enhanced image/video results and the Matlab source code can be found on our website: http://smartdsp.xmu.edu.cn/weak-illumination.html.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparisons with other methods</head><p>We first present our qualitative and quantitative comparisons. In the next subsection we follow this with a more detailed empirical analysis of the proposed method itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Qualitative assessments</head><p>We tested a large number of images under different weak illumination conditions. We show five representative images below, including those with backlighting, low light and non-uniform illumination. All other images can be found on the website given above. We compare the proposed method with six other image enhancement methods: Two Retinex-based methods, MSR <ref type="bibr" target="#b15">[16]</ref> and NPEA <ref type="bibr" target="#b19">[20]</ref>, two histogram-based methods, CVC <ref type="bibr" target="#b9">[10]</ref> and LDR <ref type="bibr" target="#b0">[1]</ref>, and two filtering-based methods, GUM <ref type="bibr" target="#b22">[23]</ref> and GOLW <ref type="bibr" target="#b21">[22]</ref>. The major parameters of the compared methods are set as follows: For MSR, patch sizes of the three Gaussian functions are respectively set to 15, 80, and 250. For CVC, the parameters are set to α = γ = 1/3, β = 2 and a 7×7 patch size is used. For LDR, the controllable parameter is set to 2.5. For GUM, the maximum gain is set to 5 and the contrast enhancement factor is set to 0.005. For GOLW, the patch size is set to 3×3, the parameters are set to β 1 = 0.4, β 2 = 0.2, β 3 = 0.05. In-depth descriptions of these parameters are given in their respective papers. In the proposed method, the structuring element of closing operation is disk and the patch size is 7×7.</p><p>Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the results of back-lit images, in which a bright background and dark foreground exist in the same scene. Certain details and objects in dark regions cannot be captured clearly while the image has a wide dynamic range. As can be seen in the results of the "Street" image in Figure <ref type="figure">6</ref>, LDR and GUM algorithms cannot handle dark regions properly. This is because the three algorithms aim to enhance contrast at the cost of improving luminance. Meanwhile GUM creates an unnatural appearance due to over-enhancement, such as in the sky region. The luminance of results yielded by MSR, NPEA, CVC and GOLW are improved and details can be seen clearly.</p><p>For the "Woman" image in Figure <ref type="figure">7</ref>, all compared methods are unsuccessful at enhancing the dark regions on the woman's clothing except MSR and NPEA. However, NPEA's result shows a white appearance because it over-compresses the illumination. All results produced by MSR are gray and of unnatural appearance since the illumination is completely removed. Comparatively, the proposed method enhances the dark foreground while maintaining the bright background without introducing artifacts.</p><p>Figure <ref type="figure" target="#fig_6">8</ref> shows a non-uniformly illuminated image in which both dark and bright areas coexist in one region, as shown on the girl's clothing. It is clear that CVC and GUM create results with over-stretched contrast. LDR maintains naturalness while failing to improve dark areas. MSR provides an enhanced result that over-enhances in areas and so loses some details, such as the bright regions on the girl's skirt. NPEA and GOLW compress the dynamic range to lighten dark regions and give bad contrast. We consider these drawbacks to be addressed well by our proposed method.</p><p>Figures <ref type="figure" target="#fig_7">9</ref> and<ref type="figure" target="#fig_8">10</ref> show images that suffer from low light conditions. As with back-lit images, GUM algorithms fails to fully highlight objects in the dark regions. The results by LDR and the proposed method look similar, when the overall brightness is close. This is because the LDR method aims to enhance the global contrast. the overall brightness is dark, LDR can effectively utilize the whole dynamic range and map low pixel intensities to high pixel intensities. However, when an input image has a high dynamic range, such as Figure <ref type="figure">6</ref>, LDR method may fail to enhance the image since the dynamic range is already wide enough. Both in the "Nighttime" image and the "Snacks" image, GOLW generates an obvious overly-enhanced result. Results processed by MSR, NPEA and CVC have a decent appearance at highlighting unclear objects. However, the results are too bright for visual perception especially in the "Nighttime" image. The proposed method appears to resolve these issues. In summary, compared with other methods, the proposed algorithm subjectively appears as an improvement for processing weakly illuminated images. By blending features from the various derived inputs, all enhanced results achieve a good trade-off among luminance improvement, contrast enhancement and naturalness preservation. </p><formula xml:id="formula_22">(a) (b) (c) (d) (e) (f) (g) (h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Quantitative assessments</head><p>Since image assessment is closely related to human visual perception, it is difficult to find a universal measure to quantify the quality of an enhanced image. In general, image quality assessment (IQA) can be classified into those that are full reference and no reference. In this paper, we select one full reference IQA and one no reference IQA for quantitative evaluation. For the full reference IQA we use the gradient magnitude similarity deviation (GMSD) metric <ref type="bibr" target="#b40">[41]</ref> to evaluate visual distortion between the original and enhanced images. The lower a GMSD value is, the less visually distorted it is. For the no reference IQA we use the natural image quality evaluator (NIQE) blind image quality assessment <ref type="bibr" target="#b41">[42]</ref>. This metric is based on statistical regularities that are derived from natural and undistorted images. A lower NIQE value represents a higher image quality. Table <ref type="table">4</ref>.1.2 lists both GMSD and NIQE values for several images. As can be seen, the proposed method has the lowest average in both of the two metrics, which indicates that the enhanced images have the smallest distortion and best natural appearance.</p><p>Table <ref type="table">1</ref>: GMSD (G) and NIQE (N) value comparison for different algorithms and images. </p><formula xml:id="formula_23">Original MSR NPEA CVC LDR GUM GOLW Proposed Image G N G N G N G N G N G N G N G N Street - 2.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">User study</head><p>To quantify the subjective evaluation of our method, we constructed an independent user study. In this experiment, we use 40 different kinds of weakly illumination images an enhance them using <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> and our proposed method. For each test image, we randomly order the outputs of the seven algorithms and the original image and display them on a screen. We separately asked 10 participants to select the image that he/she thinks has the best visual quality. From these 400 total trails, the percentage of times a viewer selected the output of the proposed method is 77.5%. Input images and other six methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref> correspond to 0.75%, 1.75%, 2.5%, 1%, 13.25%, 2% and 1.25%, respectively. This small-scale experiment gives additional support for our conclusions in the qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4.">Comparison of computational cost</head><p>The computational running time of the different algorithms are shown for different image sizes in Table <ref type="table" target="#tab_1">2</ref>. The proposed method requires a slightly longer running time than LDR <ref type="bibr" target="#b0">[1]</ref> and GUM <ref type="bibr" target="#b22">[23]</ref>, while significantly less time than CVC <ref type="bibr" target="#b9">[10]</ref>, NEPA <ref type="bibr" target="#b19">[20]</ref> and GOLW <ref type="bibr" target="#b21">[22]</ref>. We mention that our implementation is in Matlab using a straightforward optimization, and so can be further accelerated using faster programming languages and computing devices, such as Graphic Processing Unit (GPU). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5.">Comparison of various degrees of brightness</head><p>In Figure <ref type="figure" target="#fig_0">11</ref>, we make a further comparison using various degrees of brightness. We apply a global brightness adjustment to generate test images with different brightness (i.e., multiplying the V layer by 1  2 , 1 4 and 1 8 ). As can be seen, MSR <ref type="bibr" target="#b15">[16]</ref> improves visibility while distorting color. As the global brightness darkens, LDR <ref type="bibr" target="#b0">[1]</ref> and GUM <ref type="bibr" target="#b22">[23]</ref> fail to improve the visibility and the building cannot be seen clearly. Meanwhile, CVC <ref type="bibr" target="#b9">[10]</ref> generates obvious over-enhanced results when the brightness darkens. On the contrary, NEPA <ref type="bibr" target="#b19">[20]</ref>, GOLW <ref type="bibr" target="#b21">[22]</ref> and the proposed method generate consistent well-enhanced results that are comparable with the proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6.">Comparison with other fusion-based methods</head><p>In Figure <ref type="figure" target="#fig_1">12</ref>, we compare our algorithm with two other fusion-based methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref>. These three methods all use the same multi-scale fusion technique to avoid artifacts. The major difference between the proposed method and <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref> is that we estimate and fuse the illumination, while methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b42">43]</ref> implement the fusing operation directly on the image. Method <ref type="bibr" target="#b2">[3]</ref> requires multiple images of varying exposures (shown in Figures <ref type="figure" target="#fig_1">12(a),</ref><ref type="figure">(b</ref>) and (c)) as well as three weights -contrast, saturation and exposure -to generate the enhanced result (Figure <ref type="figure" target="#fig_1">12(d)</ref>). Method <ref type="bibr" target="#b42">[43]</ref> requires a single observed image and fuses two derived inputs -color correction and contrast improvementto remove haze. However, this method is not suitable for weakly illuminated images, as shown in Figure <ref type="figure" target="#fig_1">12</ref>(e), since weakly illuminated and hazy images have different characteristics. The proposed method can generate a similar result (Figure <ref type="figure" target="#fig_1">12</ref>(f)) with method <ref type="bibr" target="#b2">[3]</ref> while requiring only one input.</p><p>As another comparison, we use a single input image and generate two inputs using I 2 and I 3 as mentioned in Section 3. As can be seen in Figure <ref type="figure" target="#fig_10">13</ref>(d), the method of <ref type="bibr" target="#b2">[3]</ref> has a slight over-enhancement in the sky region, which is not present in the proposed method in Figure <ref type="figure" target="#fig_10">13</ref>(f). Method <ref type="bibr" target="#b42">[43]</ref> (a) fails to enhance the image. We also test the impact of fusing the reflectance and illumination layers by using the gradient based fusion method <ref type="bibr" target="#b43">[44]</ref>. As shown in Figures <ref type="figure" target="#fig_11">14(b</ref>) and (e), the gradient based method obviously boosts edge details. However, the global visual effect is not satisfactory due to the over-enhancement. On the contrary, our directly fusing results have a natural appearance while details are appropriately enhanced. This is mainly because our initial image model follows Retinex theory, which corresponds to the direct multiplication of the reflectance and the illumination (Equation ( <ref type="formula" target="#formula_0">1</ref>)). </p><formula xml:id="formula_24">(b) (c) (d) (e)<label>(f)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experiments analyzing the method itself</head><p>We next investigate the different aspects of our algorithm to better understand how it performs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Analysis of the impact of I k</head><p>To better understand complementary relationships among the three illuminations, results processed with different I k are shown in Figure <ref type="figure" target="#fig_13">15</ref>. As can be seen in Figure <ref type="figure" target="#fig_13">15</ref>(a), the result using I 1 and I 2 has an improved global luminance while local contrast is not enhanced. Also, the white sky region is over-enhanced since I 3 , which represents the local contrast enhancement, is not used. In Figure <ref type="figure" target="#fig_13">15(b)</ref>, by only using I 1 and I 3 local contrast is enhanced while the result is dark and objects cannot be seen clearly. This is because the improved global luminance I 2 has the highest impact on the visibility. Figure <ref type="figure" target="#fig_13">15(c</ref>) is visually approaching the proposed method in Figure <ref type="figure" target="#fig_13">15(d)</ref> since I 1 is the original estimated illumination used to avoid distortion.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Analysis of detail preservation</head><p>In this test, the effect of details preservation is discussed. As mentioned in Section 3, the proposed method decomposes one image into reflectance and illumination, with details contained in the reflectance. By properly adjusting the illumination, detail can be preserved and emphasized in the enhanced image. To demonstrate this, we compare the proposed method with a method that enhances locally in the gradient domain <ref type="bibr" target="#b26">[27]</ref>. As shown in Figure <ref type="figure" target="#fig_14">16</ref>, both methods can improve visibility in dark regions, but the proposed method has a slightly better preservation of detail. Method <ref type="bibr" target="#b26">[27]</ref> over-enhances the image, making the details less clear. We believe our method benefits in this case by only adjusting illumination and keeping details in the reflectance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Color distorted images</head><p>For color distorted images, the assumption that three color channels have the same illumination is inappropriate. As shown in Fig. <ref type="figure" target="#fig_15">17(b</ref>), the directly enhanced results have an unsatisfactory visual effect due to color distortion. This problem can be addressed by adopting some mature color constancy methods as a pre-processing step. Fig. <ref type="figure" target="#fig_15">17(c</ref>) shows the results generated by using the gray-world color constancy algorithm <ref type="bibr" target="#b44">[45]</ref> with the proposed algorithm. As can be seen, not only color is corrected but also lightness and contrast are improved. Therefore, our fusion method works well in combination with such color-correcting algorithms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Noise suppression</head><p>One important problem of enhancing weakly illumination images is that noise is easily amplified, especially in dark regions. Since our method decomposes the image into the reflectance and the illumination in the first step, the noise is contained in the reflectance due to the smooth illumination. Some mature denosing algorithms can be directly applied in the reflectance to suppress noise. In this test, the famous denoising algorithm BM3D <ref type="bibr" target="#b45">[46]</ref> is used to suppress the noise in the reflectance. The image in Figure <ref type="figure" target="#fig_17">18</ref>(a) is contaminated with additive white Gaussian noise with σ = 5. As can be seen in Figures <ref type="figure" target="#fig_17">18(c</ref>) and (g), the directly enhanced result has an obvious noise amplification. While the proposed method combined with BM3D can suppress noise without losing enhancing effect, as shown in Figures <ref type="figure" target="#fig_17">18(d</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Extensions</head><p>Our method can also be used in post-processing the outputs of other algorithms, which we discuss here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Post-processing of haze removal</head><p>The proposed enhancing algorithm is also suitable for post-processing the output of other enhancement algorithms that address other deficiencies in an image. As an example, consider the popular de-hazing algorithm based on the dark channel prior (DCP) <ref type="bibr" target="#b33">[34]</ref>. As shown in Figures <ref type="figure" target="#fig_18">19(b</ref>) and (f), though the method of <ref type="bibr" target="#b33">[34]</ref> effectively removes haze, the restored results have a dim appearance and certain objects cannot be seen clearly. By adding the proposed method as a post-processing technique for DCP, both the luminance and contrast have an obvious improvement; objects and details are more prominent, such as the people in Figure <ref type="figure" target="#fig_18">19</ref>(d) and the train in Figure <ref type="figure" target="#fig_18">19</ref>(h). We also compare our results with a state-of-the-art de-hazing method <ref type="bibr" target="#b46">[47]</ref>, shown in Figures <ref type="figure" target="#fig_18">19(c</ref>) and (g), which use the gamma correction for post-processing. It is clear that our result has a better luminance and con- trast than method <ref type="bibr" target="#b46">[47]</ref>. This experiment demonstrates that the proposed algorithm can further improve the visual qualities of de-hazing algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Generality of proposed fusion framework</head><p>Since the basic idea of this work is to effectively blend features, different complementary algorithms can be directly fused in this framework. Figure <ref type="figure" target="#fig_19">20</ref> shows a result produced by fusing the two outputs of GUM <ref type="bibr" target="#b22">[23]</ref> and global tone mapping operator in <ref type="bibr" target="#b47">[48]</ref>. Since GUM enhances local contrast and the tone mapping operator improves global luminance, the enhanced result is comparable with the proposed algorithm in Section 3. This test demonstrates that the proposed fusion strategy is a general framework for image processing. For specific problems, users can choose different mature algorithms to generate inputs and design proper weights to highlight desired features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have introduced a fusion-based enhancing method to deal with weakly illuminated images. An illumination-estimating algorithm is first proposed to extracts luminance and represent naturalness. By choosing appropriate inputs and weights from the estimated illumination, the proposed framework can effectively deal with images under different illumination conditions, such as back-lit images, low light images and non-uniformly illuminated images. To reduce artifacts, a multi-scale strategy is adopted in the fusion process. Since different features are blended, the enhanced image achieves a good trade-off of improving luminance, enhancing contrast and preserving naturalness. Experimental results demonstrate that the proposed algorithm generates high quality images in both qualitative and quantitative aspects. Additionally, the proposed algorithm is computationally efficient and straightforward to implement and can be used as post-processing for other applications, e.g., haze removal, to further improve image quality. The proposed fusion framework is also very general in that it can fuse different mature enhancing methods other than those selected in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the proposed method. The original image is decomposed into illumination and reflectance. Three inputs are derived from the estimated illumination. These three inputs are weighted and then blended together by adopting a multi-scale strategy to avoid artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Enhancement results and corresponding histograms using different methods. (a) Original image; (b) gamma correction; (c) sigmoid function; (d) HE; (e) CLAHE; (f) proposed method.</figDesc><graphic coords="7,118.80,290.00,121.61,141.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Derived images and corresponding normalized weights: Brightness weight W B , chromatic weight W C and W . To better demonstrate the complementary impact, three derived inputs multiply reflectance and weights W are shown in pseudo-color.</figDesc><graphic coords="10,399.23,317.48,91.78,70.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Statistics of brightness. (a) Histogram of the mean intensity of each estimated illumination; (b) histogram of the standard deviation of each estimated illumination; (c) histogram of the 2000 estimated illuminations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of fusion strategies. (a) Input image; (b) Enhanced result by naive fusion; (c) Enhanced result by multi-scale fusion.</figDesc><graphic coords="14,114.14,366.67,124.72,124.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Figure 6: Results of "Street" image. (a) Input image; Results of (b) MSR; (c) NPEA; (d) CVC; (e) LDR; (f) GUM; (g) GOLW; (h) proposed method.</figDesc><graphic coords="16,125.21,340.03,87.03,116.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Results of "Girl" image. (a) Input image; Results of (b) MSR; (c) NPEA; (d) CVC; (e) LDR; (f) GUM; (g) GOLW; (h) proposed method.</figDesc><graphic coords="17,112.77,469.75,93.25,142.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Results of "Nighttime" image. (a) Input image; Results of (b) MSR; (c) NPEA; (d) CVC; (e) LDR; (f) GUM; (g) GOLW; (h) proposed method.</figDesc><graphic coords="18,112.77,418.19,93.25,61.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Results of "Snacks" image. (a) Input image; Results of (b) MSR; (c) NPEA; (d) CVC; (e) LDR; (f) GUM; (g) GOLW; (h) proposed method.</figDesc><graphic coords="19,112.77,210.41,93.24,61.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Comparison using various degrees of brightness. From left to right, the V layer is multiplied by 1, 1 2 , 1 4 and 1 8 , respectively.</figDesc><graphic coords="21,110.85,170.47,388.52,422.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison with other fusion-based methods. (a) input image; (b) derived input using I 2 ; (c) derived input using I 3 ; (d) method [3] using (a), (b) and (c); (e) method [43]; (f) proposed method.</figDesc><graphic coords="23,111.81,248.39,126.27,94.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: (a) and (d): Input images; (b) and (e): results of fusing reflectance and illumiantion by method [44]; (c) and (f): results of fusing reflectance and illumination by Equation (1).</figDesc><graphic coords="24,242.57,248.58,125.11,82.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) using I 1 and I 2 (b) using I 1 and I 3 (c) using I 2 and I 3 (d) using I 1 , I 2 , I 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Impact of each illumination I k on the final result.</figDesc><graphic coords="25,112.77,125.80,93.24,69.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Details Preservation. (a) Input image; (b) method [27]; (c) proposed method; (d)-(f): enlargements of (a)-(c) in the red rectangles.</figDesc><graphic coords="25,111.81,551.23,126.28,63.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Color distorted images. (a) Input images; (b) directly enhanced results; (c) results of adding gray-world algorithm with the proposed algorithm.</figDesc><graphic coords="26,117.44,375.80,121.22,89.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Figures18(c) and (g), the directly enhanced result has an obvious noise amplification. While the proposed method combined with BM3D can suppress noise without losing enhancing effect, as shown in Figures18(d) and (h).</figDesc><graphic coords="27,112.77,184.58,93.25,61.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Noise suppression. (a) Input image; (b) noisy image; (c): directly enhanced result; (c): combined with BM3D [46]; (e)-(h): enlargements of (a) to (d) in the red rectangles.</figDesc><graphic coords="27,112.77,274.42,93.25,58.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Results of post-precessing for haze removal. (a) and (e): Hazy images; (b) and (f): Results from DCP [34]; (c) and (g): Results from [47]; (d) and (h): Results after adding post-processing the DCP output with the proposed method.</figDesc><graphic coords="28,112.77,215.86,93.25,62.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Generality of the fusion framework. Results of (a) GUM; (b) global tone mapping operator; (c) fusing (a) and (b); (d) proposed algorithm in Section 3.</figDesc><graphic coords="29,112.77,131.77,93.24,69.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Computation time (s) in seconds of each algorithm</figDesc><table><row><cell cols="7">Image size MSR NPEA CVC LDR GUM GOLW Proposed</cell></row><row><cell>300 × 400</cell><cell>0.67</cell><cell>7.35</cell><cell>6.64</cell><cell>0.15 0.53</cell><cell>7.62</cell><cell>0.59</cell></row><row><cell>600 × 800</cell><cell cols="2">1.75 27.90</cell><cell cols="2">25.62 0.34 0.96</cell><cell>30.95</cell><cell>1.17</cell></row><row><cell cols="6">1200 × 1600 4.40 111.60 103.29 0.93 2.07 131.91</cell><cell>2.94</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgment</head><p>The project was supported in part by the National Natural Science Foundation of China under Grants 30900328, 61172179, 61103121, 81301278, 61571382 and 61571005, in part by the Fundamental Research Funds for the Central Universities under Grant 20720150169 and 20720150093, in part by the Natural Science Foundation of Fujian Province of China under Grant 2012J05160, and in part by the Research Fund for the Doctoral Program of Higher Education under Grant 20120121120043.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2d histograms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale weighted gradient-based fusion for multi-focus images</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fus</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="60" to="72" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exposure fusion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Pacific Conference on Computer Graphics and Applications</title>
		<meeting>15th Pacific Conference on Computer Graphics and Applications</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="382" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detail-enhanced exposure fusion</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rahardja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4672" to="4676" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<meeting><address><addrLine>Englewood Cliffs, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice-Hall</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram image processing to improve the detection of simulated spiculations in dense mammograms</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Pisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Hemminger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="200" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contrast enhancement using brightness preserving bihistogram equalization</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Consum. Electron</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exact histogram specification</title>
		<author>
			<persName><forename type="first">D</forename><surname>Coltuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bolon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Chassery</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1143" to="1152" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A histogram modification framework and its application for image contrast enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Arici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dikbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Altunbasak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1921" to="1935" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual and variational contrast enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3344" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generalized equalization model for image enhancement</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="68" to="82" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Power-constrained contrast enhancement for emissive displays based on histogram equalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="93" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatial entropy-based global and local image contrast enhancement</title>
		<author>
			<persName><forename type="first">T</forename><surname>Celik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5298" to="5308" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Histogram statistics based variance controlled adaptive threshold in anisotropic diffusion for low contrast image enhancement, Signal Process</title>
		<author>
			<persName><forename type="first">N</forename><surname>Khann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Arya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pattanaik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1684" to="1693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Properties and performance of a center/surround retinex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="451" to="462" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A multi-scale retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Jobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">U</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A variational framework for retinex</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shaked</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="23" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A total variation model for retinex</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Imaging Sci</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="345" to="365" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Variational bayesian method for retinex</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="page" from="3381" to="3396" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Retinex method based on adaptive smoothing for illumination invariant face recognition, Signal Process</title>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="1929" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Globally optimized linear windowed tone mapping</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="663" to="675" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A generalized unsharp masking algorithm</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1249" to="1261" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic exposure correction of consumer photographs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="771" to="785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flash photography enhancement via intrinsic relighting</title>
		<author>
			<persName><forename type="first">E</forename><surname>Eisemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="673" to="678" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video enhancement using per-pixel virtual exposures</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="845" to="852" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient domain high dynamic range compression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic local exposure correction using bright channel prior for under-exposed images, Signal Process</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="3227" to="3238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lightness and retinex theory</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Land</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Thresholding for edge detection using human psychovisual phenomena</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="433" to="441" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-linear direct multiscale image enhancement based on the luminance and contrast masking characteristics of the human visual system</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Nercessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3549" to="3561" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A contrast enhancement framework with jpeg artifacts suppression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image analysis using mathematical morphology</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Sternberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1987</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4672" to="4676" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast preserving decolorization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computational Photography</title>
		<meeting>IEEE Int&apos;l Conf. Computational Photography</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enhancing by saliency-guided decolorization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Commun</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Edge-preserving decomposition for multi-scale tone and detail manipulation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Farbman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Making a completely blind image quality analyzer</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Single image dehazing by multi-scale fusion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3271" to="3282" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Image fusion with local spectral consistency and dynamic gradient sparsity</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2760" to="2765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A spatial processor model for object colour perception</title>
		<author>
			<persName><forename type="first">G</forename><surname>Buchsbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Frank. Inst</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dabo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dehazing using color-lines</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIG-GRAPH)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName><forename type="first">E</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. (Proc. SIGGRAPH)</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
