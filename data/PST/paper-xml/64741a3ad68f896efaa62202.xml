<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression</title>
				<funder ref="#_4qkVP9k">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-05-25">25 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Yihao</forename><surname>Xue</surname></persName>
							<email>&lt;yihaoxue@g.ucla.edu&gt;.</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siddharth</forename><surname>Joshi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><surname>Gan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Uni-versity of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Which Features are Learnt by Contrastive Learning? On the Role of Simplicity Bias in Class Collapse and Feature Suppression</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-25">25 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.16536v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning (CL) has emerged as a powerful technique for representation learning, with or without label supervision. However, supervised CL is prone to collapsing representations of subclasses within a class by not capturing all their features, and unsupervised CL may suppress harder class-relevant features by focusing on learning easy class-irrelevant features; both significantly compromise representation quality. Yet, there is no theoretical understanding of class collapse or feature suppression at test time. We provide the first unified theoretically rigorous framework to determine which features are learnt by CL. Our analysis indicate that, perhaps surprisingly, bias of (stochastic) gradient descent towards finding simpler solutions is a key factor in collapsing subclass representations and suppressing harder class-relevant features. Moreover, we present increasing embedding dimensionality and improving the quality of data augmentations as two theoretically motivated solutions to feature suppression. We also provide the first theoretical explanation for why employing supervised and unsupervised CL together yields higher-quality representations, even when using commonly-used stochastic gradient methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Learning high-quality representations that generalize well to a variety of downstream prediction tasks has been a long-standing goal of machine learning <ref type="bibr" target="#b17">(Hinton et al., 2006;</ref><ref type="bibr" target="#b36">Ranzato et al., 2006)</ref>. Contrastive learning (CL) has emerged as an effective approach for solving this problem, both with and without supervision <ref type="bibr" target="#b4">(Chen et al., 2020;</ref><ref type="bibr" target="#b6">Chuang et al., 2020;</ref><ref type="bibr" target="#b10">Grill et al., 2020;</ref><ref type="bibr" target="#b23">Khosla et al., 2020)</ref>. Unsupervised CL learns representations of training examples by maximizing agreement between augmented views of the same example. Similarly, supervised CL maximizes agreement between augmented views of examples in the same class. Despite their empirical success, both supervised and unsupervised contrastive learning fail to capture all semantically relevant features in the data. In particular, supervised CL can fall prey to class collapse <ref type="bibr" target="#b9">(Graf et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2022)</ref>, where representations of subclasses within a class may no longer be distinguishable from each other; thus, yielding a poor classification performance at the subclass level. Similarly, unsupervised CL can be afflicted with feature suppression <ref type="bibr" target="#b5">(Chen et al., 2021;</ref><ref type="bibr" target="#b38">Robinson et al., 2021)</ref> where easy but class-irrelevant features suppress the learning of harder class-relevant ones; deteriorating the generalizability of the obtained representations.</p><p>In spite of the significance of these failure modes, there is no clear theoretical understanding of them and consequently, no rigorous solution. Feature suppression has not been studied theoretically by prior work and the only theoretical work on class collapse <ref type="bibr" target="#b9">(Graf et al., 2021)</ref> cannot explain why we observe class collapse at test time.</p><p>Addressing class collapse and feature suppression requires a theoretical understanding of which features CL learns. However, existing CL theory <ref type="bibr">(Wang &amp; Isola, 2020;</ref><ref type="bibr" target="#b9">Graf et al., 2021;</ref><ref type="bibr" target="#b26">Lee et al., 2021;</ref><ref type="bibr">Tosh et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr">Arora et al., 2019b;</ref><ref type="bibr" target="#b46">Tsai et al., 2020;</ref><ref type="bibr" target="#b15">HaoChen et al., 2021;</ref><ref type="bibr" target="#b49">Wen &amp; Li, 2021;</ref><ref type="bibr" target="#b20">Ji et al., 2021)</ref> only explains how semantically relevant features are learned. The implicit assumption is that all semantically relevant features are learned, but the occurrence of class collapse and feature suppression proves otherwise. We propose the first unified (i.e. for both supervised and unsupervised CL) framework to answer which semantically relevant features are learned. We then leverage this framework to characterize class collapse and feature suppression. Table <ref type="table" target="#tab_0">1</ref> summarizes the main findings in this paper, which are detailed below.</p><p>Class Collapse in Supervised CL. We prove that, perhaps surprisingly and in contrast to the current understanding <ref type="bibr" target="#b9">(Graf et al., 2021)</ref>, global minimizers of the supervised con- We then study minimizing the supervised contrastive loss using (S)GD and show that, interestingly, subclass features are learned early in training. However, we verify empirically, that as training proceeds, (S)GD forgets the learned subclass features and collapses class representations.</p><p>Altogether, our findings indicate that the bias of SGD towards finding simpler solutions <ref type="bibr" target="#b30">(Lyu et al., 2021)</ref> is the main deriving factor in collapsing class representations.</p><p>Feature Suppression in Unsupervised CL. We provide the first theoretical characterization of feature suppression in unsupervised CL. In particular, we show that the minimum norm global minimizer of the unsupervised contrastive loss results in feature suppression, when the embedding dimensionality is small or when data augmentations preserve class-irrelevant features better than class-relevant features. Again, our results identify the simplicity bias of (S)GD as a key factor in suppressing features of the input data. In addition, our findings suggest practical solutions to the problem of feature suppression: increasing embedding dimensionality and/or improving the quality of data augmentations.</p><p>Theoretical Justification for Combining Supervised and Unsupervised CL to Obtain Superior Representations.</p><p>Finally, we prove that the minimum norm global minimizer of the joint loss (weighted sum of the supervised and unsupervised contrastive loss) does not suffer from class collapse or feature suppression, explaining why <ref type="bibr" target="#b3">Chen et al. (2022)</ref>; <ref type="bibr" target="#b19">Islam et al. (2021)</ref> observes this empirically (i.e. even when using SGD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Theory of CL. While there has been much progress in theoretically understanding CL, most prior work <ref type="bibr">(Wang &amp; Isola, 2020;</ref><ref type="bibr" target="#b9">Graf et al., 2021;</ref><ref type="bibr" target="#b26">Lee et al., 2021;</ref><ref type="bibr">Tosh et al., 2021a;</ref><ref type="bibr">b;</ref><ref type="bibr">Arora et al., 2019b;</ref><ref type="bibr" target="#b46">Tsai et al., 2020;</ref><ref type="bibr" target="#b15">HaoChen et al., 2021)</ref> are focused on understanding how CL clusters examples using semantically meaningful information or providing generalization guarantees on downstream tasks. Feature learning has only been studied by <ref type="bibr" target="#b49">(Wen &amp; Li, 2021;</ref><ref type="bibr" target="#b20">Ji et al., 2021)</ref> which show that CL learns semantically meaningful features from the data. In contrast, we show that CL may not learn all semantically relevant features.</p><p>Other important recent work <ref type="bibr" target="#b40">(Saunshi et al., 2022;</ref><ref type="bibr" target="#b14">HaoChen &amp; Ma, 2022)</ref> studied the role of inductive bias of the function class in the success of CL. Our analysis, however, is focused on understanding failures modes of CL i.e. class collapse and feature suppression.</p><p>Class Collapse in Supervised CL.  <ref type="bibr" target="#b27">Li et al. (2020)</ref> shows that InfoNCE has local minimums that exhibit feature suppression, thus attributing this phenomenon to failure of optimizing the loss. However, <ref type="bibr" target="#b38">Robinson et al. (2021)</ref> shows that the InfoNCE loss can be minimized by many models, some of which learn all task-relevant features, while others do not. We put forth the only theoretical characterization of feature suppression and consequently, use this understanding to suggest practical solutions to remedy this problem.</p><p>Joint Supervised and Unsupervised Contrastive Loss.</p><p>Recently, several versions of loss functions that combine supervised and unsupervised contrastive losses <ref type="bibr" target="#b19">(Islam et al., 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2022)</ref> have been empirically observed to have superior transfer learning performance, by avoiding class collapse. We provide the first theoretically rigorous analysis of which features the minimum norm global minimizer of the joint loss learns, provably demonstrating that it can avoid class collapse and feature suppression. To the best of our knowledge, this is the only theoretical result that can be used to understand the empirical success of joint losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data distribution</head><p>We define data distribution D orig below. Each example (x, y, y sub ) ? D orig is generated as follows:</p><formula xml:id="formula_0">x =u + ?, where u = (y? 1 + ? 1 )v 1 +(y sub ? 2 + ? 2 )v 2 + (? k ? k + ? k )v k ,</formula><p>and k is uniformly selected from 3, . . . , K; and y, y sub , ? k are uniformly sampled from {-1, 1}.</p><p>Features and Noise. We assume features and noise form an orthonormal basis of ? d , i.e., a set of unit orthogonal vectors {v 1 , . . . , v d } in ? d . W.l.o.g., one can let v's be the standard basis, where the first K basis are feature vectors. {? 1 , . . . , ? K } are constants indicating the strength of each feature, and {? 1 , . . . , ? K } are the means of the corresponding entries in the feature vectors. In particular:</p><formula xml:id="formula_1">? Class Feature: v 1 . ? Subclass Feature: v 2 .</formula><p>? (Class and subclass) irrelevant features: 1 v 3 , . . . , v K .</p><p>? Noise ? ? D ? : D ? is a uniform distribution over features ? ? v 1 , . . . , ? ? v d , where ? ? indicates the variance of the noise. 2   We sample n examples from D orig to form the original dataset Dorig . Assumption 3.1 (Balanced Dataset). All combinations of (y i , y sub,i , k i , ? i ) are equally represented in Dorig . 3   A Concrete Example of the Above Data Distribution.</p><p>1 In the rest of the paper, we use irrelevant features to refer to features that may have semantic meaning but are irrelevant to class and subclass.</p><p>2 This definition of noise is nearly identical to Gaussian noise</p><formula xml:id="formula_2">N (0, ? 2 ? d I d )</formula><p>in the high-dimensional regime but keeps the analysis clear. Our results can be extended to the Gaussian noise setting.</p><p>3 This can be approximately achieved when n is sufficiently larger than K. While our analysis can be generalized to consider imbalanced data, this is outside the scope of this work.</p><p>Let y = 1 be dogs and y = -1 be cats, y sub = 1 if they are fluffy and y sub = -1 if they are not-fluffy. Then (? 1 + ? 1 )v 1 + (? 2 + ? 2 )v 2 denotes a fluffy dog. Here, the background can be interpreted as an irrelevant feature: let ? 3 = 1 for grass and ? 3 = -1 for forest. Then (? 1 + ? 1 )v 1 + (? 2 + ? 2 )v 2 + (? 3 + ? 3 )v 3 represents a fluffy dog on grass. Note that each example only selects one irrelevant feature, which mimics the real world, where examples do not necessarily have all types of objects in the background i.e. many examples have neither grass or forests as their background.</p><p>Rationale for Including Feature Means ? i . In general, it is unreasonable to expect all features to have 0 expectation over entire data, thus we introduce ? to further generalize our analysis. We find that considering a non-zero mean for the subclass feature is sufficient to provide novel insights into class collapse (Theorem 4.5). Therefore, for clarity, we set all the ?'s except ? 2 to zero. incorporating a non-zero mean for the subclass feature alone is sufficient to provide novel insights into class collapse Relation to Sparse Coding Model. This data distribution is a variant of the sparse coding model which is usually considered as a provision model for studying the feature learning process in machine learning (e.g., <ref type="bibr" target="#b55">(Zou et al., 2021;</ref><ref type="bibr" target="#b49">Wen &amp; Li, 2021;</ref><ref type="bibr" target="#b28">Liu et al., 2021)</ref>). It naturally fits into many settings in machine learning, and in general mimics the outputs of intermediate layers of neural networks which have been shown to be sparse <ref type="bibr" target="#b33">(Papyan et al., 2017)</ref>. It is also used to model the sparse occurrences of objects in image tasks <ref type="bibr" target="#b32">(Olshausen &amp; Field, 1997;</ref><ref type="bibr" target="#b47">Vinje &amp; Gallant, 2000;</ref><ref type="bibr" target="#b8">Foldiak, 2003;</ref><ref type="bibr" target="#b35">Protter &amp; Elad, 2008;</ref><ref type="bibr" target="#b51">Yang et al., 2009;</ref><ref type="bibr" target="#b31">Mairal et al., 2014)</ref> and polysemy of words in language tasks <ref type="bibr" target="#b0">(Arora et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augmentation A (?)</head><p>For each example in Dorig , we generate m augmentations to form Daug . We consider the following augmentation strategy: given an example x = u+?, its augmentation is given by A (x) = u+? ? , where ? ? is a new random variable from D ? independent of ?. This is an abstract of augmentations used in practice where two augmentations from the same example share certain parts of the features and have the correlation between their noise parts removed or weakened. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Linear Model</head><p>We consider a linear model with p outputs. The model has weights W ? ? p?d and bias b ? ? p where p ? 3. The function represented by the model is f ? (x) = W x + b, where we define ? ? ? p?(d+1) as the concatenated parameter [W b]. We establish theoretical proofs of class collapse and feature suppression for linear model, and also empirically verified them for (non-linear) deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Loss function</head><p>For unsupervised contrastive learning, we use the unsupervised spectral contrastive loss popular in prior theoretical and empirical work <ref type="bibr" target="#b15">(HaoChen et al., 2021;</ref><ref type="bibr" target="#b40">Saunshi et al., 2022;</ref><ref type="bibr" target="#b14">HaoChen &amp; Ma, 2022)</ref> and for supervised contrastive learning, we consider the natural generalization of this loss to incorporate supervision. Let A i denote the set of augmentations in Daug generated from the i-th original example with A (?). Let S +1 and S -1 denote the set of augmentations in Daug with class labels +1 and -1, respectively. Let ? denote the empirical expectation. Then we have the following loss functions:</p><formula xml:id="formula_3">L UCL (?) = -2 ? i?[n],x?Ai,x + ?Ai f ? (x) ? f ? (x + ) + ? x? Daug,x -? Daug (f ? (x) ? f ? (x -)) 2<label>(1)</label></formula><formula xml:id="formula_4">L SCL (?) = -2 ? c?{-1,1},x?Sc,x + ?Sc f ? (x) ? f ? (x + ) + ? x? Daug,x -? Daug (f ? (x) ? f ? (x -)) 2 .</formula><p>(2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simplicity Bias Contributes to Class Collapse in Supervised CL</head><p>We make two key observations through our theoretical analysis and experiments (henceforth we refer to class collapse at test time simply as 'class collapse'):</p><p>1. Theoretically, not all global minimizers exhibit class collapse, but the minimum norm minimizer does.</p><p>2. Theoretically and empirically, when the model is trained using (S)GD, some subclasses are provably learned early in training. Empirically, however, those subclasses will eventually be unlearned i.e. S(GD) converges to minimizers that exhibit class collapse.</p><p>Altogether, these observations suggest that class collapse, which has been observed in practice when certain gradientbased algorithms are used to minimize the loss, cannot be explained by simply analyzing the loss function. This highlights the importance of studying the dynamics and inductive bias of training algorithms in contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">What Minimizers Have Class Collapse?</head><p>We first define class collapse in terms of the alignment between the model weights and the subclass feature. Definition 4.1 (Exact class collapse). We say exact class collapse happens at test time when: </p><formula xml:id="formula_5">?? ? ? p , Pr (x,y,ysub)?Dorig (y sub ? ? f ? (x) &gt; 0) = 1/2.</formula><formula xml:id="formula_6">(y sub ? ? W * x &gt; 0|y) = 1 -o(1).</formula><p>We prove the theorem in Appendix D. The proof utilizes Lemma C.1 which implies that, due to the highdimensionality, the noise vectors have non-trivial effects on the empirical covariance matrix by rotating its kernel space. This results in the kernel space to have a ?(</p><formula xml:id="formula_7">? ?</formula><p>? mn ) alignment with the subclass feature. Since minimizers of the loss can behave arbitrarily on this kernel space, without any additional restriction, they can have any alignment with the subclass feature.</p><p>- Then with high probability i.e. at least 1 -O( m 2 n 2 d ) = 1o(1), W * * has no alignment with subclass feature v 2 i.e.</p><formula xml:id="formula_8">W * * v 2 = 0.</formula><p>This means class collapse occurs at test time (Definition 4.1), and no linear classifier does better than random guess for predicting subclass labels.</p><p>Theorems 4.3 and 4.4 show that minimizing the training loss does not necessarily lead to class collapse on test data, but does with additional constraint on the weights of the model. This is not due to a degenerate solution, as we show that both minimizers learn the class feature v 1 (see corollary C.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Intriguing Properties of GD</head><p>We now further our theoretical characterization of class collapse by investigating the setting where L SCL is minimized by GD. This is an important step toward understanding class collapse in practice, where similar optimization algorithms are used to minimize the loss. Our findings indicate that it is likely the simplicity bias of commonly used optimization algorithms that eventually leads to class collapse.</p><p>We consider GD with a constant learning rate ?. The weights are initialized from a Gaussian distribution, i.e., the initial weight ? 0 has each of its element drawn from N (0,</p><formula xml:id="formula_9">? 2 0 d ).</formula><p>And the weights at training epoch t are given by:</p><formula xml:id="formula_10">? t = ? t-1 -?? ? L SCL (? t-1 ).</formula><p>Early in Training Some Subclasses are Provably Learned. By analyzing the training dynamics of GD, we find that subclasses are learned early in training. </p><formula xml:id="formula_11">+ ? 2 &gt; ? 2 1 , then with probability at least 1 -O( m 2 n 2 d + 1 poly(p) ) = 1 -o(1) the following holds: ? W 0 v 2 = o(1). ? ?t = O(ln( 1 ?0 d p )), s.t. W t v 2 = ?(1), and ? ??, s.t. Pr (x,y,ysub)?Dorig (y sub ? ? W t x &gt; 0|y) = 1 -o(1).</formula><p>The above theorem shows that there exists an epoch where the weights have constant alignment with the subclass feature and produce distinguishable subclass embeddings (proof in Appendix G).</p><p>The key step of our analysis is showing that early in training, GD aligns the weights with the first eigenvector of the covariance matrix of class centers. This alignment grows exponentially faster than alignments with any other directions. When 1 + ? 2 &gt; ? 2 1 , the subclass feature has a constant projection onto the first eigenvector and is therefore learned by the model. More importantly, the same phenomenon can be observed in neural networks. We use SGD to train a ResNet18 <ref type="bibr" target="#b16">(He et al., 2016)</ref> on CIFAR-100 <ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref> with supervised CL loss <ref type="bibr" target="#b23">(Khosla et al., 2020)</ref> with 20 class (superclass) labels, and perform linear evaluation on embeddings of test data with 100 subclass (class) labels (see details in Appendix H). We observe that the subclass accuracy increases during the first 200 epochs before it starts to drop (Figure <ref type="figure">3</ref>(a)). Some subclasses can even achieve a high accuracy around 80% (Figure <ref type="figure">3</ref>(b)). This is surprising as it confirms that models trained with commonly used loss functions do learn subclass features early in training.</p><p>Empirical Evidence Showing that Class Collapse Eventually Happens in (S)GD. We simulate our theoretical analysis using numerical experiments to show that gradient descent converges to a minimizer that exhibits class collapse, despite learning subclasses early in training. We visualize the embeddings of test data at different epochs in Figure <ref type="figure">1</ref>, and plot the alignment between weights and class/subclass features in Figure <ref type="figure">2</ref>. Subclasses are perfectly separated and the weights align with both v 1 and v 2 after around 100 epochs of training. The model then starts unlearning v 2 which causes the alignment to drop, thus subclasses are merged in the embedding space.</p><p>We also confirm that same conclusion holds for neural networks in realistic settings. In Figure <ref type="figure">3</ref>, we see that the subclass accuracy drops after around 200 epochs of training and eventually reaches a low value. In contrast, the class accuracy does not drop during training.</p><p>Minimum Norm Minimizer Exhibits Class Collapse.</p><p>Note that in Theorem 4.5, assuming ? = 0 leads us to discovering that subclasses are learned early in training. Here, we extend Theorem 4.4 to this setting under asymptotic class collapse.</p><p>Definition 4.6 (Asymptotic Class Collapse). We say asymptotic class collapse happens when</p><formula xml:id="formula_12">W v 2 = O( ? ? ? mn ) = o(1).</formula><p>This definition implies that: (1) representations of subclasses are not well separated, hence it is nearly impossible to distinguish between them, and (2) the distinguishability of subclasses is at odds with generalization, which improves as number of augmented views per example m and size of training data n increase. Thus, while this definition is a relaxation of Definition 4.1, practically, this results in equally severe class collapse. </p><formula xml:id="formula_13">? * * = arg min ? * ? * F s.t. ? * ? arg min ? L SCL (?).</formula><p>Then with probability at least</p><formula xml:id="formula_14">1 -O( m 2 n 2 d ) = 1 -o(1), asymptotic class collapse happens, i.e., W * * v 2 = O( ? ? ? mn ) = o(1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Simplicity Bias of (S)GD</head><p>We reiterate our main findings:</p><p>1. Minimizing the supervised contrastive loss does not necessarily lead to class collapse.</p><p>2. However, simpler minimizers of the supervised contrastive loss (e.g. minimum norm) do suffer from class collapse.</p><p>3. Optimizing with (S)GD does learn the subclass features early in training, but eventually unlearns them, resulting in class collapse.</p><p>These coupled with the fact that (S)GD is known to have a bias towards simpler solutions <ref type="bibr" target="#b22">(Kalimeris et al., 2019)</ref> prompt us to conjecture:</p><p>The simplicity bias of (S)GD leads it to unlearn subclass features, thus causing class collapse.</p><p>The simplicity bias of (S)GD has not been rigorously studied for CL, and our results indicate the surprising role it may play in class collapse. Note that, the supervised contrastive loss is different than common supervised objectives, where the role of such bias of (S)GD is understood better <ref type="bibr" target="#b12">(Gunasekar et al., 2018;</ref><ref type="bibr" target="#b42">Soudry et al., 2018;</ref><ref type="bibr" target="#b21">Ji &amp; Telgarsky, 2019;</ref><ref type="bibr" target="#b50">Wu et al., 2019;</ref><ref type="bibr" target="#b30">Lyu et al., 2021)</ref>. Rather, the supervised CL objective can be reformulated as a matrix factorization objective (Eq. 39), where the debate on the bias of (S)GD (e.g., minimum norm <ref type="bibr" target="#b11">(Gunasekar et al., 2017)</ref> or rank <ref type="bibr">(Arora et al., 2019a;</ref><ref type="bibr" target="#b37">Razin &amp; Cohen, 2020)</ref>) is still ongoing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Understanding Feature Suppression in Unsupervised CL</head><p>Empirically, feature suppression can be observed due to a variety of reasons <ref type="bibr" target="#b27">(Li et al., 2020;</ref><ref type="bibr" target="#b5">Chen et al., 2021;</ref><ref type="bibr" target="#b38">Robinson et al., 2021)</ref>. Easy features for unsupervised CL are those that allow the model to discriminate between examples (highly discriminative). Here, we consider different ways irrelevant features can be easy (highly discriminative) and characterize how this can lead to feature suppression. We show that the types of feature suppression we consider can be largely attributed to insufficient embedding dimensionality and/or poor data augmentations. Surprisingly, we find again that the minimum norm simplicity bias is critical in explaining this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Feature Suppression due to Easy Irrelevant Features and Limited Embedding Space</head><p>In Theorem 5.1, we show that easy (discriminative) irrelevant features can suppress the class feature when the embedding dimensionality is limited. For clarity, we let ? 2 = 0.</p><p>Theorem 5.1 (Feature Suppression 1). Assume p ? K. Let L be the</p><formula xml:id="formula_15">(K + 1)-element tuple 1, ? 2 1 , ? 2 2 , ? 2 3 K-2 , . . . , ? 2 K K-2</formula><p>whose last K elements are the variances of features. If ? 2 1 is not among the p largest elements in L, then with probability at least <ref type="formula" target="#formula_3">1</ref>): (1) there exists a global minimizer ? * of L UCL such that W * v 1 = ?(1), (2) However, the minimum norm minimizer ? * * satisfies W * * v 1 = 0.</p><formula xml:id="formula_16">1 -O( m 2 n 2 d ) = 1 -o(</formula><p>We prove the theorem in Appendix E. The elements except the first one in tuple L can be interpreted as the variance of examples at each coordinate v k , k = 1, 2, . . . , K, which indicates how much the examples are discriminated by each feature. The theorem shows that when the embedding space is not large enough to represent all the K features (which requires K + 1 dimensions), the minimum norm minimizer only picks the most discriminative ones. In practice, the embedding space in unsupervised CL is relatively low-dimensional (compared to input dimensionality) and thus the model cannot fit all the information about inputs into the embedding space. As is suggested by Theorem 5.1, if the training algorithm prefers functions with certain simple structures, only the easiest (most discriminative) features that can be mapped into the embedding space by less complex functions (e.g., smaller norm) are learned. The class features are suppressed if they are not amongst the easiest ones. Remark 5.2. Following the same analysis we can also show that when ? 1 is among the p largest elements in L, i.e., the class feature is among the easiest (most discriminative) ones, the class feature v 1 is learned by the minimum norm 1, ? = 0,</p><formula xml:id="formula_17">? 2 k K-2 &gt; ?1, ?k ? [K -1]</formula><p>and vary ?K . Thus whether ? 2 1 is among the p largest variances only depends on ?K. We train the linear model to convergence. Plots show that the alignment between the trained weights and v1 drops when ?K increases. We report the average of 10 runs. The result diverges at</p><formula xml:id="formula_18">? 2 K K-2 = ? 2 1</formula><p>indicating that the model can learn either v1 or vK in this case. minimizer; when ? 1 is exactly on par with some other element as the p-th largest, there exist both minimum norm minimizers that learn and do not learn the class feature v 1 .</p><p>Numerical Experiments with GD. Our theory for the minimum norm minimizer matches the experimental results for models trained with GD. We let p = K and let</p><formula xml:id="formula_19">1 ? ? 2 2 ? ? 2 3 K-2 ? ? ? ? ? ? 2 K-1 K-2 &gt; ? 2</formula><p>1 so that ? 2 1 must be among the smallest two variances i.e. v 1 is among the two most difficult features. Then we vary ? K and see how the trained weights align with v 1 . Consistent with Theorem 1, Figure <ref type="figure" target="#fig_4">4</ref> shows that v 1 is suppressed when</p><formula xml:id="formula_20">? 2 K K-2 &gt; ? 2</formula><p>1 . Interestingly, we also see that the result at</p><formula xml:id="formula_21">? 2 K K-2 = ? 2</formula><p>1 diverges, indicating that GD can find both minimizers that learn and do not learn v 1 when the variances at v 1 and v K are the same.</p><p>Empirically Verifying Benefits of Larger Embedding Size. Theorem 5.1 also provides one practical solution for feature suppression due to limited embedding size: increasing the embedding size so that every feature can be learned </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Feature Suppression due to High-dimensional Irrelevant Features and Imperfect Augmentation</head><p>Empirically, another form of feature suppression has been observed that cannot be remedied by larger embedding dimensionality <ref type="bibr" target="#b27">(Li et al., 2020)</ref>. We characterize this form of feature suppression by defining easy irrelevant features as being:</p><p>(1) drawn from a high dimensional space so that the collection of irrelevant features is large and discriminating based on irrelevant features is easier, (2) less altered by data augmentation compared to the class feature.</p><p>For (1), formally we assume K = ?(n 2 ), as opposed to assumption 3.1 which implies that K is smaller than n. A consequence of this assumption is that with high probability the n original examples each have a unique irrelevant feature. For (2) we consider the following imperfect data augmentation: Definition 5.3 (Imperfect data augmentation A ? (?)). For a given example x = ? + ? ? Dorig ,</p><formula xml:id="formula_22">A ? (x) =u + ? ? v 1 + ? ?? v 2 + ? ? , where ? ? ? N (0, ? ?2 ? ), ? ?? ? N (0, ? ??2 ? ), ? ?2 ? , ? ??2 ? = 0 and ? ? is a new random variable drawn from N (?, ? ? ) with rank(? ? ) ? m 2 .</formula><p>In the definition, the data augmentation adds small perturbations (? ? and ? ?? ) to class and subclass features, weakly alters the noise, but preserves the irrelevant features. For example, on Colorful-Moving-MNIST <ref type="bibr" target="#b43">(Tian et al., 2020)</ref> constructed by assigning each MNIST digit a background object image selected randomly from STL-10, the colorful background objects are high-dimensional and the colors are invariant to data augmentations without color distortion. Theorem 5.4 (Feature Suppression 2).</p><formula xml:id="formula_23">If K = ?(n 2 ) and augmentation is A ? (?), with probability ? 1 -o( n 2 m 2 d + 1 n ) = 1 -o(1), the minimum norm minimizer ? * = [W * , b * ] satisfies W * v 1 = 0.</formula><p>This theorem shows that feature suppression can happen even when embedding dimensionality p is arbitrarily large and helps understand empirical observations made both in our work (Figure <ref type="figure" target="#fig_5">5</ref>.1, the line with 15 bits) and previous work. For example <ref type="bibr" target="#b27">Li et al. (2020)</ref> showed that on Colorful-Moving-MNIST, the colorful background can suppress learning the digits especially when color distortion is not used in augmentation, and increasing embedding size does not address the issue.</p><p>In conclusion, Theorem 5.4 highlights that designing data augmentations that disrupt the highly-discriminative irrelevant features is a key to addressing feature suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Combining Supervised and Unsupervised CL Losses Can Avoid Both Class Collapse and Feature Suppression</head><p>We now consider the following loss which is a weighted sum of the supervised and unsupervised CL loss functions: unsupervised contrastive loss leads to better transferability of the learned models than their supervised counterparts. However, we still lack a theoretical understanding of why this weighted sum of losses can outperform both losses.</p><formula xml:id="formula_24">L joint,? (?) = ?L SCL (?) + (1 -?)L UCL (?).</formula><p>From our investigation of class collapse and feature suppression, the benefit of the joint objective L joint becomes evident: the unsupervised term in L joint increases the chance of learning features that do not appear relevant to the labels but might be useful for downstream tasks, while the supervised term in L joint ensures that even hard-to-learn class features are learnt. Thus, L joint can learn rich representations capturing more task relevant information than either L UCL (?) or L SCL (?). We show below that with an appropriate choice of ?, L joint can provably succeed where L SCL fails due to collapse and L UCL fails due to feature suppression (for clarity, we let ? = 0).</p><p>Theorem 6.1. W.L.O.G., assume</p><formula xml:id="formula_25">? 3 ? ? 4 ? ? ? ? ? ? K . If p ? K, ? 2 2 &gt; ? 2 p-2 K-2 and ? 2 1 &lt; ? 2 p-1</formula><p>K-2 , then by Theorem 4.4 the minimum norm minimizer of L SCL suffers from class collapse and by Theorem 5.1 the minimum norm minimizer of L UCL suffers from feature suppresion. However, for constant ? ? (0, 1), the minimum norm minimizer of L joint,? , denoted by</p><formula xml:id="formula_26">? * = [W * b * ], satisfies W * v 1 = ?(1) and W * v 2 = ?(1).</formula><p>Empirically Verifying Benefits of the Joint Loss. We empirically examine the impact of the joint loss on MNIST RandBit, CIFAR-100, and CIFAR-100 RandBit. The training details are in Appenidx H.2. The results indicate that the joint loss significantly improves performance in scenarios where SCL suffers from class collapse (Table <ref type="table" target="#tab_5">3</ref>) and UCL suffers from feature suppression (Table <ref type="table">4</ref>). Furthermore, on CIFAR-100 RandBit dataset, where both phenomena can occur simultaneously, the joint loss effectively alleviates both issues (Table <ref type="table" target="#tab_6">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>Negative Impact of Simplicity Bias in Deep Learning. The simplicity bias of optimization algorithms has been studied as a key beneficial factor in achieving good generalization <ref type="bibr" target="#b11">(Gunasekar et al., 2017;</ref><ref type="bibr">2018;</ref><ref type="bibr" target="#b42">Soudry et al., 2018;</ref><ref type="bibr" target="#b21">Ji &amp; Telgarsky, 2019;</ref><ref type="bibr" target="#b50">Wu et al., 2019;</ref><ref type="bibr" target="#b30">Lyu et al., 2021)</ref>. However, our study reveals the negative impact of simplicity bias in CL. In fact, it has also been conjectured to lead to undesirable outcomes in other scenarios, such as learning spurious correlations <ref type="bibr" target="#b39">(Sagawa et al., 2020)</ref> and shortcut solutions <ref type="bibr" target="#b38">(Robinson et al., 2021)</ref>. We hope our study can inspire further theoretical characterization of the negative role of simplicity bias in these scenarios, thereby deepening our understanding and fostering potential solutions.</p><p>Connection to Neural Collapse. Neural collapse (NC) <ref type="bibr" target="#b34">(Papyan et al., 2020)</ref> refers to the collapse of representations within each class in supervised learning. Similar to the rationale in this study, overparameterized models that exhibit NC on training data can demonstrate different behaviors on test data due to their capacity to implement training set NC in various ways, and it is worth considering whether current theoretical frameworks <ref type="bibr" target="#b13">(Han et al., 2021;</ref><ref type="bibr" target="#b54">Zhu et al., 2021;</ref><ref type="bibr">Zhou et al., 2022b;</ref><ref type="bibr">a;</ref><ref type="bibr" target="#b29">Lu &amp; Steinerberger, 2022;</ref><ref type="bibr" target="#b7">Fang et al., 2021)</ref> can effectively capture NC on test data. In fact, the empirical results in <ref type="bibr" target="#b18">(Hui et al., 2022)</ref> emphasize the distinction between NC on training and test data, as there can be an inverse correlation between the two. Our results suggest that analyzing the learned features and considering the inductive bias of training algorithms can aid in this distinction.</p><p>Theoretical Characterization of Class Collapse in (S)GD. The results in Section 4.2 highlight the need for theoretical characterization of class collapse in (S)GD. We provide two potential approaches for future investigation.</p><p>(1) Given that the objective can be reformulated as matrix factorization (Eq. 39), and our Theorems 4.4 and 4.7 on minimum norm minimizer, it is reasonable to investigate whether the implicit bias of (S)GD is to seek the minimum norm solution. We note that understanding the implicit bias in matrix factorization is a longstanding pursuit in the machine learning community, with no consensus reached thus far (see Appendix I.1). Hence, further effort is still needed. (2) As elaborated in Appendix I.2, the gradient consists of two terms with distinct roles. One promotes alignment with the subclass feature, while the other counteracts its influence. The relative scale of these two terms undergoes a phase transition (Figure <ref type="figure">6</ref>), and analyzing this can provide insights into class collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>To conclude, we present the first theoretically rigorous characterization of the failure modes of CL: class collapse and feature suppression at test time. We explicitly construct minimizers of supervised contrastive loss to show that optimizing this loss does not necessarily lead to class collapse. Then we show that the minimum norm minimizer does exhibit class collapse. Our analysis also reveals a peculiar phenomenon for supervised CL, when optimized with (S)GD: subclass features are learned early in training and then unlearned. To analyze feature suppression, we consider two formalisms of easy features that can prevent learning of class features and provably attribute feature suppression to insufficient embedding space and/or imperfect data augmentations; thus, motivating practical solutions to this problem. The unified framework we develop to determine which features are learnt by CL allows us to also offer the only theoretical justification for recent empirical proposals to combine unsupervised and supervised contrastive losses. Perhaps, most surprisingly, our findings from this theoretical study indicate that simplicity bias of (S)GD is likely the driving factor behind class collapse and feature suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Effective dataset</head><p>Analyzing training a linear model with bias on the data is equivalent to analyzing trainng a linear model without bias on:</p><formula xml:id="formula_27">{ 1 x i : x i ? D aug }.</formula><p>Equivalently we can consider a dataset distribution where</p><formula xml:id="formula_28">x =u + ?, where u =v 0 + y? 1 v 1 + (y sub ? 2 + ?)v 2 + ?? k v k .</formula><p>The definitions are identical to the one in Section 3.1 except that each data now is in ? d+1 and has one constant feature v 0 orthogonal to other v's. We train a linear model f (x) = W x on such data. The definition of other notations such as Daug in the following analysis are also adapted to this dataset accordingly. Other notations such as Daug in the subsequent analysis are adjusted accordingly to accommodate this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Loss functions</head><p>The loss functions can be rewritten as follows</p><formula xml:id="formula_29">L SCL = -2 ? i?[n],x?Ai,x + ?Ai x ? W ? W x + + ? x? Daug,x -? Daug x ? W ? W x -2 (3) = -Tr(2M + W W ? ) + Tr(M W ? W M W ? W ) L UCL = -2 ? c?{-1,1},x?Sc,x + ?Sc x ? W ? W x + + ? x? Daug,x -? Daug x ? W ? W x -2 (4) = -Tr(2 M W W ? ) + Tr(M W ? W M W ? W ) L joint =(1 -?)L SCL + ?L UCL (5) = -Tr(2 M W W ? ) + Tr(M W ? W M W ? W ),</formula><p>where we define the following Definition A.1. M , M + , M are the covariance matrices of training examples, class centers and augmentation centers, respectively</p><formula xml:id="formula_30">M = 1 mn mn i=1 x i x ? i M + = 1 2 c?{-1,1} ( 2 mn x?Sc x)( 2 mn x?Sc x) ? M = 1 n n i=1 ( 1 m x?Ai x)( 1 m x?Ai x) ? ,</formula><p>and</p><formula xml:id="formula_31">M =(1 -?)M + + ? M .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Minimizers of Loss Functions</head><p>We start with a technical lemma which we will need: Lemma B.1. The product of two positive semidefinite matrices is diagonalizable.</p><p>Next, we present a lemma that facilitates the analysis of minimizers for various contrastive loss functions. To apply the lemma, simply substitute the respective covariance matrices (M , M + , M ) into P and Q as indicated.</p><p>Lemma B.2. Let P , Q ? ? d+1 be positive semidefinite matrices such that colsp(P ) ? colsp(Q). Consider the function L : ? p?(d+1) ? ? given by</p><formula xml:id="formula_32">L(W ) = Tr[-2W ? W P + W ? W QW ? W Q] (6)</formula><p>Then W is a global minimizer of L if and only if</p><formula xml:id="formula_33">W ? W Q = [Q ? P ] p</formula><p>where notation [A] p represents the matrix composed of the first p eigenvalues and eigenvectors of a positive semidefinite</p><formula xml:id="formula_34">A (if p ? rankA then [A] p = A).</formula><p>Moreover, if p ? rank(P ), then W * * is a minimum norm global minimizer if and only if</p><formula xml:id="formula_35">W * * ? W * * = Q ? P Q ? Proof.</formula><p>First consider points that satisfy the first order condition</p><formula xml:id="formula_36">? W (L) = -4W P + 4W QW ? W Q = 0 (7)</formula><p>where ? W (L) is the matrix of partial derivatives of L with respect to each entry of W .</p><p>Since Q is positive semidefinite, it decomposes ? d+1 into the orthogonal direct sum ker(Q) ? colsp(Q). Observe that both subspaces are invariant under both P and Q.</p><formula xml:id="formula_37">Now let v ? colsp(Q) ? ker(W Q). Note that P v ? colsp(Q), so P v = QQ ? P v.</formula><p>Then from equation 7,</p><formula xml:id="formula_38">0 = (W P -W QW ? W Q)v = W Q(Q ? P -W ? W Q)v (8)</formula><p>If in addition we assume v ? ker(W Q), then</p><formula xml:id="formula_39">0 = W Q(Q ? P v) namely Q ? P v ? ker(W Q). But colsp(Q) is also Q ? -invariant, so Q ? P v ? colsp(Q). We conclude that ker(W Q) ? colsp(Q) is Q ? P -invariant.</formula><p>Since Q ? and P are positive semidefinite, by B.1 Q ? P is diagonalizable. The only invariant subspaces of a diagonalizable operator are spans of its eigenvectors, so ker</p><formula xml:id="formula_40">(W Q) ? colsp(Q) is the span of eigenvectors of Q ? P . Let colsp(Q) = ker(W Q) ? colsp(Q) ? U</formula><p>, where U is the span of the remaining eigenvectors of Q ? P in colsp(Q). Then by equation 8,</p><formula xml:id="formula_41">W ? W Q = Q ? P on U .</formula><p>Thus we have a basis v 1 , . . . , v r , . . . , v s , . . . ,</p><formula xml:id="formula_42">v d s.t. Span(v 1 , . . . , v r ) = U, Span(v r+1 , . . . , v s ) = ker(W Q) ? colsp(Q), Span(v s+1 , .</formula><p>. . , v d+1 ) = ker Q, and in this basis</p><formula xml:id="formula_43">Q ? P = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 . . . ? r ? r+1 . . . ? s 0 . . . 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? W ? W Q = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 . . . ? r 0 . . . 0 0 . . . 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? W ? W P = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 2 1 . . . ? 2 r 0 . . . 0 0 . . . 0 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>with ? 1 , . . . , ? r , . . . , ? q = 0 for some r ? q ? s, where r = rankW ? p, q = rank(P ).</p><p>Then for all such W ,</p><formula xml:id="formula_44">L = Tr[-2W ? W P + W ? W QW ? W Q] = -2 r i=1 ? 2 i + r i=1 ? 2 i = - r i=1 ? 2 i</formula><p>It is clear from the above expression that the minimum among critical points is achieved if and only if</p><formula xml:id="formula_45">W ? W Q = [Q ? P ] p</formula><p>(note that if matching anything beyond the qth eigenvalue is trivial since all such eigenvalues are zero).</p><p>It remains to check the behavior as W F grows large. Equivalently, W ? W has a large eigenvalue ?. Let w be a corresponding eigenvector. If w ? ker Q, then Qw = P w = 0, so we see that the loss is unchanged. Otherwise, w has some nonzero alignment with colsp(W ). But then</p><formula xml:id="formula_46">Tr[W ? W QW ? W Q] grows quadratically in ?, but Tr[-2W ? W P ]</formula><p>grows at most linearly in ?, hence the loss is large. We conclude that the previously found condition in fact specifies the global minimizers of L.</p><p>From now on, assume that p ? q. Then the global minimum is achieved if and only if</p><formula xml:id="formula_47">W ? W Q = Q ? P (9)</formula><p>Let us now consider the minimum norm solution, i.e. the one that minimizes Tr(W ? W ). Note that W ? W and Q ? P Q ? are positive semidefinite. Let B be an orthonormal basis of eigenvectors for colsp(Q), C an orthonormal basis for ker Q.</p><p>Then in the orthonormal basis B ? C, we have the following block form of</p><formula xml:id="formula_48">Q ? P Q ? Q ? P Q ? = A 0 0 0 (<label>10</label></formula><formula xml:id="formula_49">)</formula><p>where A is positive semidefinite.</p><p>Now equation 9 implies that W W ? has the form</p><formula xml:id="formula_50">W ? W = A B B ? C (<label>11</label></formula><formula xml:id="formula_51">)</formula><p>where C is also positive semidefinite matrix. Then W F = Tr[W ? W ] is minimized exactly when Tr[C] = 0. But this holds if and only if C = 0. Now suppose for the sake of contradiction B = 0, say b ij = 0 for some i, j. Then W ? W contains a submatrix</p><formula xml:id="formula_52">a ii b ij b ij 0<label>(12)</label></formula><p>which has negative determinant. But this implies that W ? W is not positive semidefinite, a contradiction. We conclude that B = 0 so that the minimum norm solution is precisely</p><formula xml:id="formula_53">W * * ? W * * = Q ? P Q ? .</formula><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Some Properties of The Covariance Matrices</head><p>We assume</p><formula xml:id="formula_54">? 2 ? mn = o(1). With probability ? 1 -O( m 2 n 2 d ), we have that ? ? i v k = 0, ?k, i and ? ? i ? j = 0, ?i, j.</formula><p>The following discussion focuses on the properties of M , M + , and M when this condition is met.</p><formula xml:id="formula_55">Write X = V S ? ? I mn where V = [v 0 , v 1 . . . v K . . . v K+1 . . . v mn+K ]</formula><p>where v K+i is the noise vector selected by example x i , and</p><formula xml:id="formula_56">S = ? ? ? ? ? ? ? ? ? ? ? 1 1 . . . 1 y 1 ? 1 y 2 ? 1 . . . y mn ? 1 ? + y sub,1 ? 2 ? + y sub,2 ? 2 . . . y sub,mn ? 2 ? 1 ? k1=3 ? 3 ? 2 ? k2=3 ? 3 . . . ? mn ? kmn=3 ? 3 ? 1 ? k1=4 ? 4 ? 2 ? k2=4 ? 4 . . . ? mn ? kmn=4 ? 4 . . . . . . . . . . . . ? 1 ? k1=K ? K ? 2 ? k2=K ? K . . . ? mn ? kmn=K ? K ? ? ? ? ? ? ? ? ? ? ? =S ? ? ,<label>(13)</label></formula><p>where</p><formula xml:id="formula_57">S ? := ? ? ? ? ? ? ? ? ? ? ? ? mn 0 0 0 . . . 0 0 ? mn? 1 0 0 . . . 0 ? mn? 0 ? mn? 2 0 . . . 0 0 0 0 mn K-2 ? 3 . . . 0 . . . . . . . . . . . . . . . . . . 0 0 0 0 . . . mn K-2 ? K ? ? ? ? ? ? ? ? ? ? ? ,<label>and</label></formula><formula xml:id="formula_58">? := ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 ? mn 1 ? mn . . . 1 ? mn y 1 1 ? mn y 2 1 ? mn . . . y mn 1 ? mn y sub,1 1 ? mn y sub,2 1 ? mn . . . y sub,mn 1 ? mn ? 1 ? k1=3 K-2 mn ? 2 ? k2=3 K-2 mn . . . ? mn ? kmn=3 K-2 mn ? 1 ? k1=4 K-2 mn ? 2 ? k2=4 K-2 mn . . . ? mn ? kmn=4 K mn . . . . . . . . . . . . ? 1 ? k1=K K-2 mn ? 2 ? k2=K K-2 mn . . . ? mn ? kmn=K K-2 mn ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>It should be noted that the rows of ? are orthonormal due to the assumption of a balanced dataset. Consequently, to obtain the singular value decomposition (SVD) of S, it suffices to find the SVD of S ? = P ? ? ? Q ?? . Moreover, the right singular vectors of S with non-zero singular values are given by the rows of Q ?? ? .</p><p>We write M as V GV ? where G is given by </p><formula xml:id="formula_59">1 mn SS ? ? ? mn S ? ? mn S ?</formula><formula xml:id="formula_60">? 2 ? mn + ? 2 1 mn , ? 2 ? mn + ? 2 2 mn , . . . , ? 2 ? mn + ? 2 K mn , ? 2 ? mn , . . . , ? 2 ? mn , with the corresponding eigenvectors ? ? 1 ? 1+r 2 1 p 1 r1 ? 1+r 2 1 q 1 ? ? , ? ? 1 ? 1+r 2 2 p 2 r2 ? 1+r 2 2 q 2 ? ? , . . . , ? ? 1 ? 1+r 2 K p K rK ? 1+r 2 K q K ? ? , 0 K q K+1 , . . . , 0 K q mn , where r k = ? ? ? k .</formula><p>Proof. Let P a Qb where a ? ? K and b ? ? mn be an eigenvector of G. By the definition of eigenvector there should exist ? such that G P a Qb = ? P a Qb , i.e.,</p><formula xml:id="formula_61">1 mn P ?? ? a + ? ? mn P ?b = ?P a ? ? mn Q? ? a + ? 2 ? mn Qb = ?Qb, which reduces to (?I K -1 mn ?? ? )a = ? ? mn ?b ? ? mn ? ? a = (? - ? 2 ? mn )b.</formula><p>Firstly, we observe that the rank of G is at most mn because G = 1 mn S ? ? I mn S ? ? I mn ? . Then it is easy to check that the eigenvalues and eigenvectors in Lemma C.1 satisfy the above conditions and the eigenvectors are indeed orthonormal, which completes the proof.</p><p>Corollary C.2. The projection of v 2 onto ker M has magnitude ?(</p><formula xml:id="formula_62">? ? ? mn ). Corollary C.3. . Assuming the dataset is balanced, then v ? 2 M ? M + M ? v 2 = 0, if ? = 0 O( ? ? ? mn ), if ? = 0 and ? = ?(1).</formula><p>Proof. Let LAL ? be the eigendecomposition of G. Then</p><formula xml:id="formula_63">M ? v 2 = V LA ? L ? ? ? ? ? ? ? ? ? ? 0 0 1 0 . . . 0 ? ? ? ? ? ? ? ? ?</formula><p>.</p><p>When ? = 0, we can express the SVD of S (equation 13) and apply Lemma C.1 to obtain the following result.</p><formula xml:id="formula_64">? 3 = ? mn? 2 , a 3 = ? 2 ? mn + ? 2 2 , r 3 = ? ? ? mn? 2 p k = e k , ?k ? [K] and q 3 = ? ? ? ? ? ? 1 ? mn y sub,1 1 ? mn y sub,2 . . . 1 ? mn y sub,mn ? ? ? ? ? ?</formula><p>, and</p><formula xml:id="formula_65">l 3 = ? ? 1 ? 1+r 2 3 p 3 r3 ? 1+r 2 3 q 3 ? ? .</formula><p>Thus</p><formula xml:id="formula_66">M ? v 2 = 1 a 3 1 + r 2 3 V l 3 = 1 a 3 1 + r 2 3 ( 1 1 + r 2 3 v 2 + r 3 1 + r 2 3 mn i=1 1 ? mn y sub,i v K+i ).</formula><p>Let xy be the average of examples with label y and let S y collects indices of examples with label y. Then</p><formula xml:id="formula_67">xy = v 0 + v 1 + 2? ? mn i?Sy v K+i ,<label>(14)</label></formula><p>and</p><formula xml:id="formula_68">x? y M ? v 2 = r 3 a 3 (1 + r 2 3 ) 2? ? mn i?Sy 1 ? mn y sub,i = 0.</formula><p>Write M + as</p><formula xml:id="formula_69">M + = 1 2 ( x+1 x? +1 + x-1 x? -1 ). Then v ? 2 M ? M + M ? v 2 = 1 2 ((v ? 2 M ? x+1 ) 2 + (v ? 2 M ? x-1 ) 2 ) = 0.</formula><p>When ? = 0, then there are at most two of p k 's that are not orthogonal to e 3 (say p 1 and p 3 ). Additionally, all of their elements, except for the first one, are zero. The remaining corresponding quantities satisfy.</p><formula xml:id="formula_70">? 1 , ? 3 = ?( ? mn), a 1 = ? 2 1 mn + ? 2 ? mn , a 3 = ? 2 3 mn + ? 2 ? mn r 1 = ? ? ? 1 , r 3 = ? ? ? 3 ,</formula><p>and q 1 and q 3 are just linear combinations of ?sub and 1 ? mn 1, where ?sub is a vector whose i-th element is 1 ? mn y sub,i . Then</p><formula xml:id="formula_71">M ? v 2 =V 1 a 1 1 1 + r 2 1 c 3,1 l 1 + 1 a 3 1 1 + r 2 3 c 3,3 l 3</formula><p>where c i,j = p ? j e i are constants. For i = 0, 2</p><formula xml:id="formula_72">v ? 0 M ? v 2 =e ? 1 1 a 1 1 1 + r 2 1 c 3,1 l 1 + 1 a 3 1 1 + r 2 3 c 3,3 l 3 = 1 a 1 1 1 + r 2 1 c 3,1 c 1,1 + 1 a 3 1 1 + r 2 3 c 3,3 c 1,3 =( mn ? 2 1 -?( ? 2 ? mn ))(1 -?( ? ? ? 1 ))c 3,1 c 1,1 + ( mn ? 2 3 -?( ? 2 ? mn ))(1 -?( ? ? ? 3 ))c 3,3 c 1,3 = where |? 1 | = O( ? ? ? mn ). Similarly, v ? 2 M ? v 2 = mn ? 2 1 c 3,1 c 3,1 + mn ? 2 3 c 3,3 c 3,3 + ? 2 ,</formula><p>where</p><formula xml:id="formula_73">|? 2 | = O( ? ? ? mn ). For i &gt; K v ? i M ? v 2 =v i V 1 a 1 1 1 + r 2 1 c 3,1 l 1 + 1 a 3 1 1 + r 2 3 c 3,3 l 3 =e ? i 1 a 1 1 1 + r 2 1 c 3,1 l 1 + 1 a 3 1 1 + r 2 3 c 3,3 l 3 =? 3 , where |? 3 | = O( ? ? mn ). Additionally, xy = v 0 + v 1 + ?v 2 + 2? ? mn i?Sy v K+i .</formula><p>Then</p><formula xml:id="formula_74">x? y M ? v 2 = mn ? 2 1 c 3,1 c 1,1 + mn ? 2 3 c 3,3 c 1,3 + mn ? 2 1 c 3,1 c 3,1 + mn ? 2 3 c 3,3 c 3,3 + O( ? ? ? mn ).</formula><p>By straightforward calculation, we can verify that mn</p><formula xml:id="formula_75">? 2 1 c 3,1 c 1,1 + mn ? 2 3 c 3,3 c 1,3 + mn ? 2 1 c 3,1 c 3,1 + mn ? 2 3 c 3,3 c 3,3 = 0.</formula><p>This equation can be equivalently examined as the satisfaction of the following condition:</p><formula xml:id="formula_76">[1 ?] 1 ? ? ? 2 + ? 2 2 -1 0 1 = 0. Therefore | x? y M ? v 2 | = O( ? ? ? mn ), and consequently v ? 2 M ? M + M ? v 2 = O( ? ? ? mn ). Corollary C.4. Similar to Corollary C.3, we also have v ? k M ? M + M ? v k = 0, k = 3, 4, . . . , K. Corollary C.5. v ? 1 M ? M + M ? v 1 = ?(1).</formula><p>It can be proved using the same strategy as in Corollary C.3. Lemma C.6. (1) The first K eigenvectors/eigenvalues of M match those of M . (2) M ? M is identity on colsp( M ) and null on ker( M ), i.e., M ? M = M ? M .</p><p>Proof. We assign indices to the training examples such that the augmented examples from the same original example are indexed from (l -1) ? m + 1 to l ? m, where l ranges from 1 to n. Next, we define matrix ? = [? 1 , ?2 , . . . , ?n ] ? ? d?n , where</p><formula xml:id="formula_77">?i =v i , ?1 ? i ? K, ?i = 1 ? m m j=1 v K+(i-1)?m+j , ?K + 1 ? i ? n.</formula><p>In other words, ? can be written as</p><formula xml:id="formula_78">? = V T ,</formula><p>where</p><formula xml:id="formula_79">T = ? ? ? ? ? I K 0 K?n 0 mn?K ? ? ? 1 ? m 1 m?1 0 0 . . . 0 0 1 ? m 1 m?1 0 . . . 0 0 0 1 ? m 1 m?1 . . . 0 ? ? ? ? ? ? ? ?</formula><p>Note that, by the definition of our augmentation, the center of augmentations of the i-th original example, i.e., xi = 1 m m j=1 x K+(i-1)?m+j , can be considered as an example with the same features as x i but with an added noise term of ? ? ? m ?i . Therefore we can change the basis to ? and express M as</p><formula xml:id="formula_80">M = ? G ? ? , where G = 1 n S S? 1 n ? ? ? m S 1 n ? ? ? m S? 1 n ? 2 ? m I n and S = S? ?orig ,<label>(15)</label></formula><p>where</p><formula xml:id="formula_81">S? := ? ? ? ? ? ? ? ? ? ? ? ? n 0 0 0 . . . 0 0 ? n? 1 0 0 . . . 0 ? n? 0 ? n? 2 0 . . . 0 0 0 0 n K-2 ? 3 . . . 0 . . . . . . . . . . . . . . . . . . 0 0 0 0 . . . n K-2 ? K ? ? ? ? ? ? ? ? ? ? ? ,<label>and</label></formula><formula xml:id="formula_82">?orig := ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 ? n 1 ? n . . . 1 ? n y 1 1 ? n y 2 1 ? n . . . y n 1 ? n y sub,1 1 ? n y sub,2 1 ? n . . . y sub,n 1 ? n ? 1 ? k1=3 K-2 n ? 2 ? k2=3 K-2 n . . . ? n ? kn=3 K-2 n ? 1 ? k1=4 K-2 n ? 2 ? k2=4 K-2 n . . . ? n ? kn=4 K n . . . . . . . . . . . . ? 1 ? k1=K K-2 n ? 2 ? k2=K K-2 n . . . ? n ? kn=K K-2 n ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? orig . (<label>16</label></formula><formula xml:id="formula_83">)</formula><p>We note that we use the subscript 'orig' of a matrix to indicate that its elements represent the corresponding quantities on the original dataset (e.g., y i is the label of the i-th original example). Let P ? ?? Q?? be the SVD of S. Similar to equation 13, we observe that P ? ?? ( Q?? ?orig ) serves as an eigendecomposition of S.</p><p>Now we make the following observations:</p><p>1. By Lemma C.1 (with G replaced by G) and the fact that ?? collects the eigenvalues of S, the eigenvalues of G are</p><formula xml:id="formula_84">? 2 ? mn + ? ?2 1 n , ? 2 ? mn + ? ?2 2 n , . . . , ? 2 ? mn + ? ?2 K n , ? 2 ? n , . . . , ?<label>2</label></formula><p>? n , which are also the eigenvalues of M because ? has orthonormal columns. With the observation that S? = 1 ? m S ? (S ? is defined in equation 13), we further conclude that the above eigenvalues equal eigenvalues of G and therefore M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Let qi be the</head><formula xml:id="formula_85">i-th column of ? ? orig Q? . By Lemma C.1 (substitute G with G), the i-th (i ? K) eigenvector of G is given by 1 ? 1+r 2 i p? i ri 1+r 2 i qi = 1 ? 1+r 2 i p i ri 1+r 2 i qi</formula><p>, where ri =</p><formula xml:id="formula_86">? ? ? m? ? i = ? ? ?i . The corresponding eigenvector of M is V T 1 ? 1+r 2 i p i ri 1+r 2 i qi . Observe that T ? ? orig = ? ? , therefore V T 1 ? 1+r 2 i p i ri 1+r 2 i qi = V 1 ? 1+r 2 i p i ri 1+r 2 i q i</formula><p>which is the i-th eigenvector of M .</p><p>Combining the above two leads to the conclusion that the first K eigenvectors/eigenvalues of M and M match. Additionally, we observe that colsp( M ) ? colsp(M ). Therefore the span of the last n -K eigenvectors of M is a subspace of the span of the last mn -K eigenvectors of M . Since Lemma C.1 tells us that the remaining mn -K eigenvalues of M are equal, M is identity on the span of the last mn -K eigenvectors. Thus M is identity on the span of the last n -K eigenvectors of M . Now we can conclude that M ? M = M ? M .</p><p>Lemma C.7. Suppose that the first mn 2 examples have class label +1 and the others have class label -1. Let L + A + L +? (where L + ? ? d?2 ) be the eigendecomposition of M + , then </p><formula xml:id="formula_87">l + 1 = V ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 1 1+? 2 + ? 2 ? mn 0 ? 1+? 2 + ? 2 ? mn 0 (K-2)?1 ? ? mn 1+? 2 + ? 2 ? mn 1 mn 2 ?1 ? ? mn 1+? 2 + ? 2 ? mn 1 mn 2 ?1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? , l + 2 = V ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 ?1 ? 2 1 + ? 2 ? mn 0 0 (K-2)?1 ? ? mn ? 2 1 + ? 2 ? mn 1 mn 2 ?1 -? ? mn ? 2 1 + ? 2 ? mn 1 mn 2 ?1 ? ? ? ? ? ? ? ? ? ? ? ? ? ? , a 1 = 1 + ? 2 + ? 2 ? mn , a 2 = ? 2 1 + ? 2 ? mn<label>(</label></formula><p>Then, for x from D orig the following holds true</p><formula xml:id="formula_89">W * x = c 0 v 0 + c 1 yv 1 + y sub c 2 v 2 + h x + W * ?,</formula><p>where c 1 , c 2 are ?(1), and h x is orthogonal to v k , k = 0, . . . , K and h x ? colsp(M ) (by Lemmas C.1, C.5, C.4, equation 18 and that</p><formula xml:id="formula_90">W * v 2 = ?(1)). Let ? = c 2 v 2 , then ? ? W * x = y sub c 2 2 + ? ? W * ?.</formula><p>With probability ? 1 -mn d , ? / ? {v k } mn k=1 , which indicates that W * ? = 0 by Lemma C.1 and equation 18. Therefore we can conclude </p><formula xml:id="formula_91">Pr(y sub ? ? W * x &gt; 0|y) ? 1 - mn d = 1 -o(1</formula><formula xml:id="formula_92">W ? W M = p i=1 r i r ? i ,<label>(19)</label></formula><p>where {r i } p i=1 can be an orthonormal basis of any p-dimensional subspace of colsp( M ). By equation 13 and Lemmas C.1 and C.6, M and M each have an eigenvector c 1 with eigenvalue</p><formula xml:id="formula_93">? 2 ? mn + ? 2 1 and a 1 1+ ? 2 ? mn? 2 1</formula><p>alignment with v 1 , with the other eigenvectors having no alignment with v 1 . Thus if we include c 1 in {r i } p i=1 and let W ? W be null on ker M , then the constructed W is a minimizer of L UCL with ?(1) alignment with v 1 . Now let's look at the minimum norm minimizer, which should satisfy</p><formula xml:id="formula_94">W ? W = p i=1 r i r ? i M ? ,</formula><p>where {r i } p i=1 is selected such that W has the smallest norm. By Lemma C.6, {r i } p i=1 should be the peigenvectors of M with largest eigenvalues (so that the inverse of the eigenvalues are among the smallest). If among </p><formula xml:id="formula_95">(1+? 2 +? 2 2 )+ ? (1+? 2 +?2) 2 -4? 2 2 2 , (1+? 2 +? 2 2 )- ? (1+? 2 +?2) 2 -4? 2 2 2 , ?3 ? K-2 , . . . ,</formula><formula xml:id="formula_96">a i v ? i ]</formula><p>, by orthogonality we have</p><formula xml:id="formula_97">0 = n n yi v yi 2 a i v ? yi [z yi z ? yi ]z + [z yi a ? i ]a = n n yi v yi 2 a i v ? yi [(v yi + u yi )z ? yi ]z + [(v yi + u yi )a ? i ]a = n n yi v yi 2 a i v ? yi [u yi (z ? yi z + a ? i a)] + [v yi (z ? yi z + a ? i a)] = n n yi v yi 2 a i v ? yi [v yi (z ? yi z + a ? i a)] = C c=1 1 nn c v c 2 yi=c a i v ? c v c n c z ? c z + yi=c a ? i a = C c=1 1 nn c yi=c a i n c z ? c z + yi=c a i a = 1 n 2 C c=1 1 n yi=c a i z ? c z + 1 nn c yi=c a i yi=c a i ? a = E[a i z ? yi ]z + A + a</formula><p>Now substituting into the second equation, we find that</p><formula xml:id="formula_98">(A -A + )a = 0<label>(33)</label></formula><p>But our assumptions imply that a = 0. Returning to the first equation, we now have Zz = 0. But since Z is diagonalizable, Z must be invertible on its image, hence z = 0. We conclude that v = 0. This completes the proof.</p><p>We now want to show that we can simplify some of the conditions of the previous lemma to linear independence.</p><p>Lemma E.2. Suppose d ? 3n -2 and x 1 , . . . , x n ? R d are linearly independent. Then there exists a set of nonzero orthogonal vectors v 1 , . . . , v n s.t. x i = v i + u i and v i , u j are orthogonal for all i, ? {1, . . . , n}.</p><p>Proof. WLOG assume the x i are contained in the span of the first n basis vectors. The lemma amounts to finding an</p><formula xml:id="formula_99">orthonormal matrix ? = A B C D s.t. A B C D X 0 = AX CX = ? F (<label>34</label></formula><formula xml:id="formula_100">)</formula><p>where ? is diagonal. Since the x i are linearly independent, X is invertible, so there exists A ? s.t. A ? X is diagonal.</p><p>We now want to construct a matrix C such that A ? C ? has orthogonal columns, all with norm l &gt; 0. Note that C ? has at least 2n -2 rows. Set C ? 11 = 1, and the remaining entries in the first row so that when considering A and the first row of C ? , the first column is orthogonal to every other column. Now leave C ? 21 = 0, set C ? 22 = 1, and fill out the remaining entries in the second row so that when considering A and the first two rows of C ? , the second column is orthogonal to the remaining columns. Note that the first column remains orthogonal to all other columns. Continuing in this fashion, we can use the first n -1 rows of C ? to guarantee that all n columns are orthogonal. Finally, suppose without loss of generality that whhen considering the A ? and the first n -1 rows of C ? , the first column has the largest norm l. For each of the remaining n -1 rows, set the jth row to have all zero entries except possibly in the (j + 1)-th column, which is set so that the jth column will also have norm l. Note that the columns remain orthogonal under this construction.</p><formula xml:id="formula_101">Pr(Z ? 2 a 2 ? t + 2 a ? t) ? e -t , Pr(Z ? -2 a 2 ? t) ? e -t .<label>(35)</label></formula><p>Lemma G.2 (Mills' ratio. Exercise 6.1 in <ref type="bibr" target="#b41">(Shorack &amp; Shorack, 2000)</ref>.). Let v be a Gaussian random variable drawn from N (0, 1). Then for all ? &gt; 0,</p><formula xml:id="formula_102">? ? 2 + 1 1 ? 2? e -? 2 2 &lt; Pr(v ? ?) &lt; 1 ? 1 ? 2? e -? 2 2 .</formula><p>Corollary G.3. Given a vector q, and a random vector z drawn from N (0,</p><formula xml:id="formula_103">? d I d ), w.p. ? 1 -O( ? ? log 1/? ), |z ? q| = O( q ? ? log 1 ? ? d</formula><p>).</p><p>Lemma G.6. At initialization the following holds with probability ?</p><formula xml:id="formula_104">1 -O( 1 poly(p) ) ? B (0) ? 1 (0) =O(1) ? i (0) =? 0 p d 1 ? O( log p p ) , i = 1, 2, 3 ? ? (0) =? 0 p d 1 ? O( log p p ) Proof. We first bound ? B (0) ? B (0) = mn i=4 a i W 0 l i 2 = mn i=4 a i p j=1 w ? 0,j l i 2 ? ? 2 max K -2 K+1 i=4 p j=1 w ? 0,j l i 2 + ? 2 ? mn mn i=K+2 p j=1 w ? 0,j l i 2 =O( p? 2 0 d + ? 2 ? p? 2 0 d ) 1 =O(? 0 p d ).</formula><p>Inequality 1 holds with probability ? 1-O( 1 poly(mnp) ). It is obtained by obsreving that w ? 0,j l i 's are independent Gaussian variables (by the orthogonality of l i 's) and applying Lemma G.1 to the sum of w ? 0,j l i 2 's.</p><p>By Lemma G.4 and the above, at initialization the following holds with probability ?</p><formula xml:id="formula_105">1 -O( 1 poly(p) + 1 poly(mnp) ) ? B (0) ? 1 (0) =O(1) ? i (0) =? 0 p d 1 ? O( log p p ) , i = 1, 2, 3 ? ? (0) =? 0 p d 1 ? O( log p p ) .</formula><p>Let ?, ? B be constants. Define</p><formula xml:id="formula_106">? := ? B (0) ? 1 (0) = O(1) by Lemma G.6 ? :=(c(1 + 2(1 + ?) 2 ) + (? + ? B ) 2 ) 3/2 = ?(1).</formula><p>Let ? be a constant satisfying the following  <ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref>. In the case of CIFAR-10, the 'classes' refer to the original 10 classes defined in the dataset, while we define 'subclasses' as two subclasses: vehicles (airplane, automobile, ship, truck) and animals (bird, cat, deer, dog, frog, horse). On CIFAR-100, we refer to the 10 super-classes (e.g. aquatic mammals, fish, flowers) as our 'classes' and the 100 classes as our 'subclasses'. These two datasets illustrate a natural setting where class collapse is extremely harmful, as it results in learning representations that do not capture much of the semantically relevant information from the data.</p><formula xml:id="formula_107">? ? min ? ? ? (a + 1 -a + 2 )? ? (s + s?) , a + 1 ? ? (s + s?) , a + 1 ? B ? (h + s? B ) , a + 1 -a + 2 ? s ? ? ? .</formula><p>MNIST RandBit. The MNIST RandBit dataset Chen et al. ( <ref type="formula">2021</ref>) is created by setting n, the # of bits that specifies how easy the useless feature will be. Larger n makes the feature more discriminative, thus 'easier' and more problematic for feature suppression. An extra channel is concatenated to MNIST images where each value in the feature map corresponds to a random integer between 0 and 2 n .</p><p>CIFAR-10/100 RandBit. The two datasets are constructed in a similar way as MNIST RandBit, but with images from CIFAR-10/100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2. Training details</head><p>For the experiments on CIFAR-10/100 or CIFAR-100 RandBit, we use a ResNet-18 trained with (Momentum) SGD using learning rate = 0.01 and momentum = 0.9. We train with batch size set to 512 for 1000 epochs. For data augmentations, we consider the standard data augmentations from <ref type="bibr" target="#b4">Chen et al. (2020)</ref>.</p><p>For the feature suppression experiments on MNIST RandBit, we directly use the code provided by <ref type="bibr" target="#b5">Chen et al. (2021)</ref>. We consider a 5-Layer convolutional network. For our data augmentations, we consider the standard set of data augmentations for images and do not alter the useless feature (extra channel concatenated of RandBits).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3. Details and additional experiments on varying embedding size</head><p>In the experiments presented in Table <ref type="table" target="#tab_4">2</ref>, we vary the width, denoted by w, of the ResNet, which is controlled by the number of convolutional layer filters. For width w, there are w, 2w, 4w, 8w filters in each layer of the four ResNet blocks.</p><p>In addition, we explore an alternative way of varying the embedding size, which isolates the effect of the last layer's embedding size from the size of the lower layers. Specifically, we set the width parameter w = 4 and multiply the width of only the last ResNet block by a factor k. It is worth noting that doing this requires a much smaller total number of parameters. Table <ref type="table" target="#tab_9">6</ref> presents the results on CIFAR-10 RandBit. We observe that increasing k also effectively improves the accuracy. Although the improvement is not as substantial as in the previous case where we increase w, it confirms the same trend predicted by the theory, supporting the conclusion that increasing the embedding size alleviates feature suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Potential Approaches to Theoretical Characterization of Class Collapse in (S)GD</head><p>The most crucial aspect that remains to be tackled is how (S)GD unlearns subclass features that have already been learned early in training. We offer two potential approaches that could help in achieving this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.1. Through implicit bias of (S)GD in matrix factorization</head><p>The contrastive loss we are considering can be reformulated as a matrix factorization objective: Figure <ref type="figure">6</ref>. We plot the ratio between norms of term 1 and term 2 in orange, and the projection of the weights W onto the subclass feature ( W v2 ) in blue. The ratio between the two norms initially starts at a very large value, then decreases until it reaches a plateau around 1. The point at which the dropped to around 1 coincided almost precisely with the peak of W v2 .</p><formula xml:id="formula_108">min f (W ? W ) = 1 n 2 i,j (x ? i W ? W x j -a ij ) 2 , (<label>39</label></formula><formula xml:id="formula_109">)</formula><p>where a ij := 2, if x i and x j are from the same class 0. else This opens up the possibility of leveraging the rich literature on matrix factorization to find a solution. Since we have already proven in Theorems 4.4 and 4.7 that the minimum norm minimizer of the loss function exhibits class collapse, and our experiments confirm that (S)GD does converge to a minimizer that exhibits class collapse, it is reasonable to investigate whether the implicit bias of (S)GD in our setting, specifically matrix factorization, is to seek the minimum norm solution.</p><p>We note that understanding the implicit bias in matrix factorization is a longstanding pursuit in the machine learning community. <ref type="bibr" target="#b11">(Gunasekar et al., 2017)</ref> have provided empirical and theoretical evidence that under certain conditions, gradient descent converges to the minimum nuclear norm solution. Therefore, one can examine whether similar existing results can be applied to our setting and then combine that with our Theorems 4.4 or 4.7 to show class collapse in GD. However, <ref type="bibr">(Arora et al., 2019a)</ref> and <ref type="bibr" target="#b37">(Razin &amp; Cohen, 2020)</ref> suggested that the implicit bias may be explained by rank rather than norm when the depth of a network ? 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. Through analyzing the two terms in the gradient</head><p>Let's take a closer look at the update of the weights in GD, i.e., learning rate times minus gradient 4W M + -4W M W ? W M (see Equation ). There are two terms 4W M + and -4W M W ? W M which play different roles in the high level. Here M + is the covariance of class centers, and M is the covariance of all training examples, as defined in Definition A.1.</p><p>Term 1 (4W M + ): The first term aligns the weights with M + which has alignment with the subclass feature. This aligning effect of term 1 has already been theoretically characterized in our proof (Appendix G) for Theorem 4.5.</p><p>Term 2 (-4W M W ? W M ): Although the effect of the second term is not entirely straightforward, we can gain some intuition by considering the simplest case where the embedding is one-dimensional. In this case, the second term takes the form of a negative scalar times W M , which can be seen as trying to 'discourage' alignment with M , the covariance of all training examples.</p><p>In our numerical experiment, we observe that the ratio between norms of term 1 and term 2 initially starts at a very large value, then decreases until it reaches a plateau around 1, as shown in Figure <ref type="figure">6</ref>. Interestingly, the point at which the ratio dropped to around 1 coincided almost precisely with the peak of the projection of the weights W onto the subclass feature. This leads us to the following intuition, which may serve as a proof sketch for showing class collapse at the end of training.</p><p>In the following, we first describe what happens in the early phase (which we have already proven in the paper), then outline the high-level idea of how subclasses are eventually unlearned.</p><p>Phase I where the model learns the subclass feature: We have already proved this part in Appendix G. In summary, the intuition is that early in training, the scale of term 1 dominates over term 2, aligning the model with M + , which in turn aligns with the subclass feature. Therefore, the model learns the subclass feature during this phase.</p><p>Phase II where the model unlearns the subclass feature but the class feature remains: Note that the scale of the second term also increases during Phase I as M and M + share certain components. Once the scale of term 2 reaches that of term 1, the effect of term 2 becomes more pronounced and Phase II begins. Since M exhibits a stronger correlation with the subclass feature than M + does, the overall effect of the sum of term 1 and term 2 is to reduce alignment with the subclass feature. Thus, over time, the model unlearns the subclass feature. In contrast, for the class feature, M + has a stronger correlation, causing GD to continue aligning the model with the class feature.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Assumption 3.2 (High dimensional regime). d is at least ?(n 2 m 2 ). Assumption 3.3 (Sufficient sample size). The noise-tosample-size ratio is not too large ? 2 ? mn = o(1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Theorem 4.5 (Early in training subclass features are learned). Assume ? 0 p d = o(1) and ? ? = o(1). If the subclass feature has a constant non-zero mean such that 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3. (a) Average subclass accuracy and class accuracy. (b) Accuracy in subclasses 'road', 'rocket' and 'sea'. In both plots, the subclass accuracy increases and then decreases, which confirms that subclasses are learned early in training before class collapse happens. The class accuracy only increases during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 4. 7 (</head><label>7</label><figDesc>Extension of Theorem 4.4 for ? 2 = 0). Let ? * * = [W * * b * * ] be the minimum norm minimizer of L SCL :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The irrelevant feature suppresses the class feature when its variance is beyond the variance of the class feature (the red vertical line). We let d = 2000, p = K, ?1 = 0.8, ?2 =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Effect of embedding size on feature suppression in MNIST RandBit<ref type="bibr" target="#b5">(Chen et al., 2021)</ref>. Legends show the number of bits in the extra channel which indicates how easy (discriminative) the irrelevant features are. We observe that (1) increasing the easiness of irrelevant features exacerbates feature suppression; (2) increasing the embedding size alleviates feature suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>.</head><label></label><figDesc>Now we are ready to show the following lemma which describes the SVD of G.Lemma C.1. Let S ? ? K?nm be a rank-K matrix with SVD P ?Q ? , where P ? ? K?K , ? ? ? K?mn and Q ? ? mn?mn . The mn none-zero eigenvalues of the following matrix G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Let l ? be the projection of v 2 onto ker M . By Corollary C.2, l ? = ?(? ? ? mn ). Let a = mn ? 2 ? l ? .We can construct a W * that satisfies the followingW * ? W * = M ? M + M ? + aa ? ,which, by Lemma B.2, satisfies the condition for being a minimizer of the loss. In the meantime, W * also satisfies W * v 2 = ?(1) by Corollary C.2. Note that both v 2 and the projection of v 2 onto colsp(M ) is orthogonal to v k (1 ? k ? K, k = 2) as well as v k (k &gt; mn) by Lemma C.1, therefore l ? is also orthogonal to v k , for any k s.t. 1 ? k ? K, k = 2 and k &gt; mn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>A concise overview of the key findings in this research. In the table, 'CC' and 'FS' refers to class collapse and feature suppression, respectively. 'Thm' and 'Exp' refers to theorem and experiment, respectively.Joint Joint loss can avoid both CC and FS Thm 6.1 &amp; Exp Justification of joint loss trastive loss do not necessarily collapse the representations of the subclasses at test time. We find, however, that the minimum norm global minimizer does suffer from class collapse on test data.</figDesc><table><row><cell cols="2">Loss Finding</cell><cell>Thm/Exp</cell><cell>Implication</cell></row><row><cell>SCL</cell><cell>min loss ? CC (min loss &amp; min norm) ?CC (S)GD learns subclasses early in training</cell><cell>Thm 4.3 Thm 4.4 &amp; 4.7 Thm 4.5 &amp; Exp</cell><cell>Simplicity bias of (S)GD contributes to CC</cell></row><row><cell></cell><cell>(S)GD eventually unlearns subclasses, leading to CC</cell><cell>Exp</cell><cell></cell></row><row><cell>UCL</cell><cell>With insufficient embedding size, (min loss &amp; min norm) ? FS With imperfect data augmentation, (min loss &amp; min</cell><cell cols="2">Thm 5.1 &amp; Exp Simplicity bias of (S)GD contributes to FS; Larger embedding size/better Thm 5.4</cell></row><row><cell></cell><cell>norm) ? FS, even with sufficient embedding size</cell><cell></cell><cell>augmentation alleviates FS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>For any ? * ? min ? L SCL (?), we havef ? * (x i ) = f ? * (x j ) for all x i , x j in the training set Daug such that y i = y j . ] such that ? * ? min ? L SCL (?) W * has constant alignment with subclass feature v 2 i.e.</figDesc><table><row><cell cols="2">The definition means that no linear classifier on the embed-</cell></row><row><cell cols="2">dings of examples drawn from D orig can predict the sub-class label with accuracy beyond random guess. 4</cell></row><row><cell cols="2">This is different from class collapse on the training set</cell></row><row><cell cols="2">which is not defined on the population set D orig but on the training samples Dorig .</cell></row><row><cell cols="2">Proposition 4.2. This directly implies that minimizing the loss results in</cell></row><row><cell cols="2">class collapse on the training set. However, the follow-</cell></row><row><cell cols="2">ing theorem 4.3 shows that minimizing the loss does not</cell></row><row><cell cols="2">necessarily lead to class collapse on the test set. To deter-</cell></row><row><cell cols="2">mine whether class collapse occurs, we need to determine</cell></row><row><cell cols="2">whether the model learns the subclass feature. With a lin-</cell></row><row><cell cols="2">ear model, this exactly corresponds to constant alignment</cell></row><row><cell cols="2">between weights and the subclass feature.</cell></row><row><cell cols="2">Theorem 4.3 (Minimizing L SCL ? Class Collapse). With high probability i.e. at least 1-O( m 2 n 2 d ) = 1-o(1), there exists ?  *  = [W  W  *  v 2 = ?(1).</cell></row><row><cell cols="2">Hence, there exists a linear classifier in the embedding</cell></row><row><cell cols="2">space that can predict subclass labels almost perfectly. I.e.,</cell></row><row><cell>??, s.t.</cell><cell>Pr (x,y,ysub)?Dorig</cell></row></table><note><p>* b *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Colors represent combinations of class and subclass labels (y, ysub). We use test examples for the plots. At epoch 45, the four groups of examples are well separated in the embedding space. However groups in the same classes are merged afterwards.</figDesc><table><row><cell></cell><cell></cell><cell>(1, -1)</cell><cell>(-1, -1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>(1, 1)</cell><cell>(-1, 1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00015</cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00010</cell><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>0.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00005</cell><cell></cell><cell>0.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00000</cell><cell></cell><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5 0.6</cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-0.00005</cell><cell></cell><cell>0.55</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>-0.00010</cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00015</cell><cell></cell><cell>0.80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.2</cell><cell></cell><cell>1.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.00010</cell><cell></cell><cell>0.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell>1.0</cell></row><row><cell></cell><cell cols="3">0.00010 -0.00005 0.00000 0.00005 0.00010 0.00015 -0.00015 -0.00010 -0.00005 0.00000 0.00005</cell><cell cols="2">-0.26 -0.24 -0.22 -0.20 -0.18 -0.16 -0.14 -0.12 -0.10 0.50 0.55 0.60 0.65 0.70</cell><cell>-0.8</cell><cell>-0.6</cell><cell>-0.4</cell><cell>-0.2</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.2</cell><cell>0.4</cell><cell>0.6 0.8</cell><cell>-0.2 -0.4 -0.6 -0.8</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4</cell><cell>0.2 0.4 0.6 0.8</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) epoch 0</cell><cell cols="2">(b) epoch 45</cell><cell></cell><cell></cell><cell cols="7">(c) epoch 60</cell><cell></cell><cell cols="2">(d) epoch 100</cell></row><row><cell cols="18">Figure 1. Visualization of the embedding space at different epochs. We let p = 3 so that we can see the whole embedding space from a</cell></row><row><cell cols="18">3D plot. Other parameters: n = 1000, m = 5, d = 2000, K = 4, ?1 = ?2 = ?3 = ?4 = 1, ? = 1, ? = 2, ?0 = 0.001, ? = 0.05.</cell></row><row><cell>alignment between weights and feature</cell><cell>0.2 0.4 0.6 0.8 1.0</cell><cell>|Wv1| |Wv2|</cell><cell>alignment between weights and feature</cell><cell>0.2 0.4 0.6 0.8 1.0</cell><cell>|Wv1| |Wv2|</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell>0.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10 1</cell><cell>10 2</cell><cell></cell><cell>10 1</cell><cell>10 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>epoch</cell><cell></cell><cell cols="2">epoch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(a) p = 3</cell><cell></cell><cell cols="2">(b) p = 500</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Figure 2. Wtv1 and Wtv2 at different epochs. Both fea-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">tures are learned early in training, but v2 is unlearned later.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Next, we show that, the minimum norm minimizer exhibits</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">class collapse.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">Theorem 4.4 (Minimizing L SCL + Minimum Norm =? Class Collapse). Assume ? 2 = 0. Let ?  *  *  = [W  *  *  b  *  *  ]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">be the minimum norm minimizer of L SCL , i.e., ?  *  *  = arg min ?  *  ?  *  F s.t. ?  *  ? arg min</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>?</p>L SCL (?).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Effect of embedding size on feature suppression in CIFAR-10/100 RandBit. 'Acc' refers to class accuracy and 'Sub Acc' refers to subclass accuracy. We see that increasing embedding size alleviates feature suppression, improving class/subclass accuracy.First, we train 5-layer convolutional networks on the RandomBit dataset with the same setup as in<ref type="bibr" target="#b5">(Chen et al., 2021)</ref>, but we vary the embedding size (see details in Appendix H). Varying the # bits in the extra channel intuitively controls how discriminative the irrelevant feature are, i.e., how easy-to-learn it is for CL. In this setting, the random bit can suppress the MNIST digits. We make two observations in Figure5.1: (1) with a fixed embedding size, increasing easiness (number of random bits) of the irrelevant features exacerbates feature suppression; (2) with a fixed easiness of irrelevant features, increasing the embedding size alleviates feature suppression.</figDesc><table><row><cell>w</cell><cell cols="4">CIFAR-10 RandBit CIFAR-100 RandBit Sub Acc Acc Sub Acc Acc</cell></row><row><cell>4</cell><cell>34.38</cell><cell>86.73</cell><cell>11.67</cell><cell>23.53</cell></row><row><cell>64</cell><cell>71.96</cell><cell>96.82</cell><cell>34.11</cell><cell>52.32</cell></row><row><cell>128</cell><cell>76.69</cell><cell>97.65</cell><cell>38.51</cell><cell>57.40</cell></row><row><cell cols="5">by the model. To provide empirical evidence for this, we</cell></row><row><cell cols="3">conduct two sets of experiments:</cell><cell></cell><cell></cell></row></table><note><p><p><p><p><p><p><p>Second, we train ResNet18</p><ref type="bibr" target="#b16">(He et al., 2016)</ref> </p>on the CIFAR-10/100 RandBit Dataset, constructed similarly to the MNIST RandBit dataset but with images from CIFAR-10/100</p><ref type="bibr" target="#b24">(Krizhevsky et al., 2009)</ref> </p>(see Appendix H.1). For CIFAR-10, we use 2 random bits, and for CIFAR-100, we use one random bit as the class irrelevant features. Table</p>2</p>presents the test performance for different values of the model width w, where a larger w indicates a larger embedding size (see Appendix H.3 for details). On both datasets, increasing the embedding size alleviates feature suppression, leading to improvements in both class and subclass accuracies. We also provide additional experiments and discussion in Appendix H.3. Both experimental results confirm the conclusion drawn from the theoretical analysis.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Joint loss alleviates class collapse on CIFAR-100.</figDesc><table><row><cell>Loss</cell><cell>Subclass Acc</cell></row><row><cell>SCL</cell><cell>26.11</cell></row><row><cell>Joint loss (? = 0.8)</cell><cell>41.37</cell></row><row><cell cols="2">Table 4. Joint loss alleviates feature suppresion on MNIST Rand-</cell></row><row><cell>Bit.</cell><cell></cell></row><row><cell>Loss</cell><cell>Class Acc</cell></row><row><cell>UCL</cell><cell>61.21</cell></row><row><cell>Joint loss (? = 0.5)</cell><cell>79.37</cell></row></table><note><p><p><p><p><p>Similar loss functions have been proposed recently with notable empirical success. For example,</p><ref type="bibr" target="#b3">Chen et al. (2022)</ref> </p>put forth a weighted sum of supervised CL loss and classconditional InfoNCE (which has similar effect as L UCL in our setting) to avoid class collapse.</p><ref type="bibr" target="#b19">Islam et al. (2021)</ref> </p>empirically observed that the joint objective of supervised and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Joint loss alleviates both class collapse and feature suppresion on CIFAR-100 RandBit.</figDesc><table><row><cell>Loss</cell><cell cols="2">Subclass Acc Class Acc</cell></row><row><cell>SCL</cell><cell>28.13</cell><cell>61.10</cell></row><row><cell>UCL</cell><cell>34.11</cell><cell>52.32</cell></row><row><cell>Joint loss (? = 0.8)</cell><cell>35.72</cell><cell>63.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>).</figDesc><table><row><cell>D.2. Proof of Theorems 4.4 and 4.7</cell></row><row><cell>Theorems 4.4 and 4.7 and can be proved by invoking Lemma B.2 and Corollary C.3.</cell></row><row><cell>E. Feature Suppression in Unsupervised CL</cell></row><row><cell>E.1. Feature Suppression 1</cell></row><row><cell>By Lemmas B.2 and C.6, when p &lt; K, any global minimizer of L UCL satisfies</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc>increasing k improves both subclass and class accuracies on CIFAR-10 RandBit.</figDesc><table><row><cell cols="2">k Sub Acc</cell><cell>Acc</cell></row><row><cell>1</cell><cell>34.38</cell><cell>86.73</cell></row><row><cell>16</cell><cell>58.12</cell><cell>94.09</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>Actually we are able to analyze a stronger version of class collapse: Pr (x,y,y sub )?D orig (f?(x)|ysub) = Pr (x,y,y sub )?D orig (f?(x)), which means the distributions of embeddings given and not given the subclass label are exactly the same. Nonetheless, we present this simpler formulation for clarity.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This research was supported by the <rs type="funder">National Science Foundation</rs> <rs type="grantNumber">CAREER Award 2146492</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_4qkVP9k">
					<idno type="grant-number">CAREER Award 2146492</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Feature Suppression 2</head><p>We first present our result under slightly technical conditions. Lemma E.1. Let v 1 , . . . , v C ? R d be nonzero and orthogonal, U, A are subspaces that are orthogonal to each other and all the v i . Suppose we have a data distribution D = {(v yi + u yi + a i , y i )} n i=1 ? R d ? {1, . . . , C}, where u i ? U, a i ? A for all i ? {1, . . . , n} (namely all examples in the same class c share the same v c and u c ).</p><p>Denote z yi = v yi + u yi , and let M , M + be the matrices defined for this dataset, and let Z, Z + and A, A + be the corresponding matrices when the data is {(z yi , y i )} and {(a i , y i )}, respectively. Suppose that (A -A + )v = 0 for all v ? R d s.t. Av = 0 and the output dimension p ? C. Then W ? W = Z ? is the minimum norm solution to the contrastive learning objective on D.</p><p>Proof. In this proof, we will use to represent the empirical expectation over the dataset D. Also, let n c denote the number of examples in class c.</p><p>We first derive the following expression for A + :</p><p>Now we show that W ? W = Z ? is a global minimizer. It suffices to show that M W ? W M = M + . Note that by assumption, we have z i , a j = 0 for all i ? {1, . . . , C}, j ? {1, . . . , n}, so we have</p><p>We now want to show that this is the minimum norm solution. It is sufficient to show that im(W ? W ) = im(Z ? ) = im(Z) ? im(M ). Note that im(M ) ? im(A) ? im(Z), so we can restrict M to this subspace. We will show that M is invertible on im(A) ? im(Z).</p><p>C ? has orthonormal columns and 1 l A ? X is still diagonal. By Gram-Schmidt, we can fill out the remaining columns of ? to construct an orthonormal matrix.</p><p>We now present the feature result with simplified assumptions.</p><p>Lemma E.3. Let Z, A be orthogonal subspaces. Suppose we have a data distribution D = {(z yi + a i , y i )} n i=1 ? R d ? {1, . . . , C}, where z i ? Z, a i ? A for all i ? {1, . . . , n}, and the z i are linearly independent.</p><p>Let M , M + be the matrices defined for this dataset, and let Z, Z + and A, A + be the corresponding matrices when the data is {(z yi , y i )} and {(a i , y i )}, respectively. Suppose that (A -A + )v = 0 for all v ? R d s.t. Av = 0 and the output dimension p ? C. Then W ? W = Z ? is the minimum norm solution to the contrastive learning objective on D.</p><p>Proof. Assume that d ? 3C-2, otherwise embed the distribution in a space of sufficiently large dimension. By Lemma B.2, the minimum norm minimizer is unaffected by adding extra dimensions. Then Lemma E.2 applies, so linear independence of the z yi is sufficient to be able to construct v 1 , . . . , v C , y 1 , . . . , y C satisfying Lemma E.1, from which the conclusion follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Minimizer of The Joint Loss</head><p>For simplicity we assume ? = 0. Same strategy can be applied to prove the theorem when ? = 0 but a more detailed discussion on the selection of ? may be required. By Lemmas C.7 and C.1 and the expression of S (equation 13), we observe that the two eigenvectors of M + match two of the eigenvectors of M . By combining this with Lemma C.6, we obtain that</p><p>1 and l + 2 are the two eigenvectors of ?M ? M + + (1?)M ? M with largest eigenvalues. For the remaining eigenvectors, since they have equally large eigenvalues (same as analyzed in E), the minimum norm minimizer will select the largest p -2 of them. In the setting of Theorem 6.1</p><p>) is one of the p -2 largest of the remaining. As a result, both components aligned with v 1 and v 2 are selected by the minimum norm minimizer of the joint loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Early in Training Subclasses Are Learned</head><p>We assume ? ? = O(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Lemmas</head><p>Lemma G.1 (Laurent-Massart <ref type="bibr" target="#b25">(Laurent &amp; Massart, 2000)</ref> Lemma 1, page 1325). Let v 1 , . . . , v d be i.i.d. Gaussian variables drawn from N (0, 1). Let a = (a 1 , . . . , a d ) be a vector with non-negative components. Let Z = d i=1 a i (v 2 i -1). The following inequalities hold for any positive t:</p><p>Proof. This can be proven by considering the fact that q ? z is a Gaussian variable and applying Lemma G.2.</p><p>Lemma G.4. Let each element of W 0 ? ? p?d be randomly drawn from N (0, ? 2 0 d I d ). Let u ? ? d be a unit vector. With probability at least 1?, we have</p><p>Proof. Firstly rewrite W 0 u as</p><p>By spherical symmetric, each</p><p>u is a random Gaussian variable drawn from N (0, 1). By lemma G.1 we have</p><p>which completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Proof of Theorem 4.5</head><p>We assume the dataset satisfies the condition in Section C (wich holds with probability 1 -O( m 2 n 2 d )). Let LAL ? (where C ? ? d?mn ) be the eigendecomposition of M . By equation 13 and Lemma C.7 and Lemma C.1, we observe that when ? = 0 all but three of 's eigenvectors are orthogonal to l + 1 , l + 2 . W.L.O.G., let l 1 , l 2 and l 3 be those three eigenvectors. The corresponding three eigenvalues are all constants. Let l + 3 be a unit vector in span({l 1 , l 2 , l 3 })span({l + 1 , l + 2 }).</p><p>Decompose v 2 as</p><p>Then we bound</p><p>where c is a constant because a 1 , a 2 , a 3 are all O(1) (by Lemma C.1) and each l i (i = 1, 2, 3) is a linear combination of l ++ 1 , l ++ 2 , l + 3 with O(1) coefficients, with l ++ 1 , l ++ 2 representing the projections of l + 1 , l + 2 onto span({l i } 3 i=1 ). By the rule of gradient descent we have</p><p>This is followed by Lemma G.5.</p><p>Lemma G.5. By the update rule of GD we have the following recurrence relations</p><p>Then we prove the following Lemma Note that a + 1a + 2 &gt; 0 because ? 2 + 1 &gt; ? 2 1 . Additionally, we define the following shorthand</p><p>Now we are ready to prove the following Lemma.</p><p>Lemma G.7. If ?t ? T , ? 1 (t) ? ?. For any constants ?, ? B , the following holds ?t ? T + 1 with probability 1 -O( 1 poly(p) ),</p><p>Proof. Let S(k) be the following statement: ?t ? such that 0 ? t ? ? k, the following holds</p><p>? ? 1 (t) ? ?t ? 1 (0),</p><p>By Lemma G.6, S(0) holds with high probability. Next we show that, ?t ? [0, T + 1], if S(t -1) holds then S(t) also holds. By Lemma G.5, the induction hypothesis and ? 2 , ? 3 , ? ? , ? B &lt; 1 , ? 1 (t -1) ? ?, we have the following</p><p>By the construction of our ?'s, ?'s, ?'s and ?'s, the last three items in statement S(t) hold. Combining the induction hypothesis with equations 37 and 38 yields the first two items in S(t), which completes the proof. ? If ?t ? T , ? 1 (t) ? ?, by Lemma G.7 we have ? 1 (T + 1) ? ? and ? ? (T + 1) ? (o(1) + ?)? 1 (T + 1). Then</p><p>=?(1).</p><p>? If ?t ? T s.t. ? 1 (t) &gt; ?, we define T * = ln( ? ? 1 (0) ) ln ? and t * = min t s.t. ? 1 (t) &gt; ?. It follows that ?t ? t * -1, ? 1 (t) ? ?. Then we can apply Lemma G.7 to obtain ? 1 (t * ) ? ?t * ? 1 (0). If t &lt; T * , the above yields ? 1 (t * ) &lt; ?, which contradicts the definition of t * . Therefore we conclude t * ? T * . Lemma G.7 also tells that ? ? (t * ) ? (? t * ? +?)? 1 (t * ). Since t * ? T * and ? ? &lt; 1, we have ? t * ? ? ( ?1(0) ? )</p><p>ln(1/? 3 ) ln ?</p><p>= o(1). Therefore ? ? (t * ) ? (o(1) + ?)? 1 (t * ). By the definition of t * , ? 1 (t * ) &gt; ?. Then we can lower bound W t * v 2 in the same way as in the previous case </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A theoretical analysis of contrastive unsupervised representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Khandeparkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Plevrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09229</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Perfectly balanced: Improving transfer and robustness of supervised contrastive learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fatahalian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>R?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3090" to="3122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2002.05709</idno>
		<ptr target="https://arxiv.org/abs/2002.05709v3" />
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Intriguing properties of contrastive losses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11834" to="11845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Debiased Contrastive Learning</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/63c" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8765" to="8775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring deep neural networks via layer-peeled model: Minority collapse in imbalanced training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">43</biblScope>
			<date type="published" when="2021">2103091118. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sparse coding in the primate cortex. The handbook of brain theory and neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Foldiak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dissecting supervised constrastive learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kwitt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3821" to="3830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised Learning</title>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<ptr target="http://arxiv.org/abs/2006.07733" />
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Implicit regularization in matrix factorization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Implicit bias of gradient descent on linear convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02073</idno>
		<title level="m">Neural collapse under mse loss: Proximity to and dynamics on the central path</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A theoretical study of inductive biases in contrastive learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.14699</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Provable guarantees for self-supervised deep learning with spectral contrastive loss</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5000" to="5011" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Limitations of neural collapse for understanding generalization in deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08384</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A broad study on the transferability of visual representations with contrastive learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-F</forename><forename type="middle">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8845" to="8855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The power of contrast for feature learning: A theoretical analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.02473</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on nonseparable data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1772" to="1798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Sgd on neural networks learns functions of increasing complexity. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kalimeris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18661" to="18673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptive estimation of a quadratic functional by model selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="1302" to="1338" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting what you already know helps: Provable self-supervised learning</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="309" to="323" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Addressing feature suppression in unsupervised visual representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Katabi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.09962" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Selfsupervised learning is more robust to dataset imbalance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05025</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural collapse under crossentropy loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="224" to="241" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient descent on two-layer nets: Margin maximization and simplicity bias</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12978" to="12991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse modeling for image and vision processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends? in Computer Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="85" to="283" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1? Vision research</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional neural networks analyzed via convolutional sparse coding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2887" to="2938" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prevalence of neural collapse during the terminal phase of deep learning training</title>
		<author>
			<persName><forename type="first">V</forename><surname>Papyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="24652" to="24663" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image sequence denoising via sparse and redundant representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="35" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energybased model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Implicit regularization in deep learning may not be explainable by norms</title>
		<author>
			<persName><forename type="first">N</forename><surname>Razin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21174" to="21187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Can contrastive learning avoid shortcut solutions?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.11230" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An investigation of why overparameterization exacerbates spurious correlations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8346" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.14037</idno>
		<title level="m">Understanding contrastive learning requires incorporating inductive biases</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Probability for statisticians</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Shorack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shorack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">951</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The implicit bias of gradient descent on separable data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nacson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2822" to="2878" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning?</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6827" to="6839" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contrastive estimation reveals topic posterior information to linear models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="281" to="281" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Contrastive learning, multi-view redundancy, and linear models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1179" to="1206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-supervised learning from a multi-view perspective</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05576</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Sparse coding and decorrelation in primary visual cortex during natural vision</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">5456</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Toward understanding the feature learning process of self-supervised contrastive learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11112" to="11122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Towards understanding the generalization bias of two layer convolutional linear classifiers with gradient descent</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1070" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">On the optimization landscape of neural collapse under mse loss: Global optimality with unconstrained features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27179" to="27202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.02192</idno>
		<title level="m">Are all losses created equal: A neural collapse perspective</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A geometric analysis of neural collapse with unconstrained features</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sulam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="29820" to="29834" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Understanding the generalization of adam in learning neural networks with proper regularization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.11371</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
