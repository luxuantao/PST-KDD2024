<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-07-13">13 Jul 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">? Xian-Ling</forename><surname>Mao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-07-13">13 Jul 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2307.06962v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The dominant text generation models compose the output by sequentially selecting words from a fixed vocabulary. In this paper, we formulate text generation as progressively copying text segments (e.g., words or phrases) from an existing text collection. We compute the contextualized representations of meaningful text segments and index them using efficient vector search toolkits. The task of text generation is then decomposed into a series of copy-and-paste operations: at each time step, we seek suitable text spans from the text collection rather than selecting from a standalone vocabulary. Experiments on the standard language modeling benchmark (WikiText-103) show that our approach achieves better generation quality according to both automatic and human evaluations. Besides, its inference efficiency is comparable to token-level autoregressive models thanks to the reduction of decoding steps. We also show that our approach allows for effective domain adaptation by simply switching to domain-specific text collection without extra training. Finally, we observe that our approach attains additional performance gains by simply scaling up to larger text collections, again without further training. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Most neural language models (LMs) process text generation tasks by making a series of next-token predictions in an autoregressive manner <ref type="bibr" target="#b26">(Radford et al., 2019;</ref><ref type="bibr" target="#b6">Dai et al., 2019;</ref><ref type="bibr" target="#b14">Khandelwal et al., 2020;</ref><ref type="bibr" target="#b31">Shi et al., 2022)</ref>. Specifically, LMs generate the next-token distribution over a fixed vocabulary for any given prefix. Then, the next token is selected by a chosen decoding method, such as greedy search and nucleus sampling <ref type="bibr" target="#b11">(Holtzman et al., 2020)</ref>. This process continues until some stop condition is reached. For example, a special end-of-generation token is emitted, or the generated text reaches the maximum length limit.</p><p>Unlike traditional neural language models, we reformulate text generation by copying text segments from existing text collections. The text segments can be of variable lengths, including single words and multi-word phrases. For clarity, we will use the term "phrase" to refer to any contiguous text segments, and a single word can also be seen as a phrase of length 1. We compute a contextualized vector representation for each phrase and pack them into an offline index. At each decoding step, a suitable phrase is retrieved from the offline index and appended to the current prefix. In other words, the next-token predictions in traditional neural language models are replaced by a series of copy-and-paste operations.</p><p>Our proposed model, named COG (short for COPY-GENERATOR), enjoys the following advantages. First, our method selects phrases in specific contexts rather than standalone tokens in a fixed vocabulary. It potentially allows for more accurate candidate representation and selection. Second, our method allows training-free adaptation to new knowledge sources because the text collection can be updated in a plug-and-play fashion. It could benefit application scenarios such as domain adaptation and data expansion/filtering. Third, our method allows a sequence of multiple tokens (i.e., multi-word 2 BACKGROUND: NEURAL TEXT GENERATION Neural text generation can be divided into two categories: (1) unconditional text generation; (2) conditional text generation. Unconditional text generation (or language modeling) aims to generate a coherent text continuation given a prefix. In this case, language models perform generation using a density estimation over sequences p ? (x). Conditional text generation aims to generate text with some condition c and instead estimates the probability of p ? (x|c). Its typical applications include machine translation <ref type="bibr" target="#b33">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b1">Bahdanau et al., 2015)</ref>, summarization <ref type="bibr" target="#b28">(See et al., 2017)</ref>. Throughout this paper, our discussion will be focused on unconditional text generation, however, our approach can be readily adapted to conditional text generation as well.</p><p>The canonical approach to language modeling factors the generation in an autoregressive left-to-right manner p ? (x 0:n ) = n i=1 p(x i |x &lt;i ). In this case, text generation is reduced to the task of repeatedly predicting the next token conditioned on the partial sequence (i.e., prefix) generated so far p(x i |x &lt;i ). The model often consists of two parts: (1) a prefix encoder and (2) a set of token embeddings. The prefix encoder is often parameterized by the Transformer architecture <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref>, which transforms any prefix into a fixed-sized vector representation h i ? R d = PrefixEncoder(x &lt;i ). Then, the probability of the next token being w is calculated as</p><formula xml:id="formula_0">p ? (x i = w|x &lt;i ) = exp(v w ? h i ) w?V exp(v w ? h i )</formula><p>, where v w is the context-independent token embedding representing the token w, and V is the predefined vocabulary consisting of all possible tokens. Based on the chosen decoding method, such as greedy search and nucleus sampling <ref type="bibr" target="#b11">(Holtzman et al., 2020)</ref>, the next token is selected according to the probability distribution over the fixed vocabulary V . This process is repeated in an autoregressive manner, until some stop condition is reached, e.g., the maximum length of generation is reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COPY-GENERATOR</head><p>Unlike traditional language models that compute the next token distribution over a fixed vocabulary that is usually composed of words or sub-words <ref type="bibr" target="#b29">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b17">Kudo &amp; Richardson, 2018)</ref>, our proposed COG has a dynamic "vocabulary" that is dependent on the available source text collections. Each item in the "vocabulary" corresponds to a text segment (termed as phrase in this paper) in the source text collection. Importantly, all phrases are context-sensitive. That is, the same phrases in different contexts are considered to be different. The overall framework is depicted in Figure <ref type="figure">1</ref>.</p><p>Formally, our approach assumes a set of source documents {D 1 , . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>the . Source Text Collection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximum Inner Product Search [?][?] [?][?]</head><p>Copy Figure 1: The overview of our proposed COG. Given the prefix The Dune film was released, COG retrieve 3 phrases (in different colors) from the documents and generates 3 tokens (Before, that, and the comma ,) from the fixed vocabulary to form the whole generation. and end positions of the phrase in the document, respectively. We denote all the phrases in the source text collection as P. For a given prefix x &lt;i , we aim to select the best phrases that can form a coherent text continuation following the prefix. To this end, we compute a contextualized representation for each phrase p k ? R d = PhraseEncoder(s, e, D i ) using a phrase encoder. Thus, a phrase table {(k, p k )|k ? P} can be constructed. Similar to traditional language models, at test time, COG also employs a prefix encoder to map the prefix x &lt;i into a vector representation q i . The fitness of a phrase k to the prefix x &lt;i is then measured by the dot product of their vector representations p k and q i :</p><formula xml:id="formula_1">p(k|x &lt;i ) ? exp(p k ? q i ).<label>(1)</label></formula><p>At each time step, a suitable phrase is selected and appended to the current prefix accordingly.</p><p>Note that the size of the phrase table can be up to billions. To search over this large candidate pool, we pre-compute the phrase representations and use a coarse-to-fine search pipeline based on maximum inner product search (MIPS) <ref type="bibr" target="#b12">(Johnson et al., 2019)</ref>. The details are deferred to Section 4.2. Moreover, to support the scenarios where no suitable phrases are available, we also add the context-independent token embeddings {(w, v w )|w ? V } in standard LMs to the phrase table.</p><p>Ethical Consideration The text generated by COG contains text segments copied from other documents, which may cause copyright disputes in real-world applications. Therefore, there are a few things to be considered: (1) The copyright of the source text documents needs to be carefully checked.</p><p>One should not use documents with strict copyright protection and/or private information;</p><p>(2) It is recommended to quote the original source explicitly, especially when the retrieved phrases are long.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MODEL ARCHITECTURE</head><p>As illustrated in Figure <ref type="figure">1</ref>, our proposed model consists of three major components: (1) a prefix encoder that maps prefixes to fixed-sized representations;</p><p>(2) a context-dependent phrase encoder that computes the vector representations of the phrases in the source text collection; (3) a set of context-independent token embeddings similar to the one used in standard neural language models.</p><p>Prefix Encoder The prefix encoder is responsible for encoding the prefix x &lt;i into a vector representation for the next-phrase prediction. We treat the prefix as a sequence of tokens (previously predicted phrases are split into tokens as well) and encode them using the standard Transformer architecture with causal attention <ref type="bibr" target="#b34">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b26">Radford et al., 2019)</ref>. Causal attention only allows each position in the input sequence to attend to its preceding positions. Therefore, the prefix representation can be computed incrementally as the generation progresses, leading to faster inference. Concretely, the prefix encoder transforms a prefix x &lt;i of length i into a matrix H i ? R i?dL , where d is the hidden dimension and L is the number of Transformer layers. The computation can be written as:</p><formula xml:id="formula_2">H i+1 = PrefixEncoder(x i , H i ).</formula><p>We use the hidden state of the last token as the prefix representation q i .</p><p>Phrase Encoder Given a set of source documents {D 1 , ..., D n }, the phrase encoder computes the vector representations of all the phrases in the documents. Inspired by previous work <ref type="bibr" target="#b20">(Lee et al., 2016;</ref><ref type="bibr" target="#b30">Seo et al., 2018;</ref><ref type="bibr">Lee et al., 2021)</ref>, we construct context-dependent phrase representations as follows.</p><p>For a document D = D 1 , . . . , D m of length m, we first apply a deep bidirectional Transformer <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> to obtain contextualized token representations D ? R m?dt , where d t is the dimension of token representations. Then, we apply two MLPs models, MLP start and MLP end , to convert D into start and end token representations D start , D end ? R m? d 2 , respectively:</p><formula xml:id="formula_3">D start = MLP start (D), D end = MLP end (D).</formula><p>For each phrase D s:e that starts at s and ends at e in the document, we use the concatenation of the corresponding start and end vectors as the phrase representation.</p><formula xml:id="formula_4">PhraseEncoder(s, e, D) = [D start [s]; D end [e]] ? R d (2)</formula><p>The advantages of the above representation method are that (1) we only need to encode the document once to obtain all phrase representations; and (2) we only need to store all the token representations instead of all phrase representations.</p><p>Context-Independent Token Embeddings Although COG can copy phrases from other documents, we would like to retain the generalization capability to compose output with standalone tokens. This can be especially useful when there is no suitable phrase in the source text collection. Therefore, we also add the traditional context-independent token embeddings V ? R |V |?d to our phrase table. These tokens can be seen as phrases of length 1 without any context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MODEL TRAINING</head><p>COG decomposes the task of text generation into a series of copy-and-paste operations: at each time step, it selects the next phrase either from the source text collection or the fixed token vocabulary. In other words, phrases are used as the basic building blocks for text generation. To train COG, each document in the training set is chunked into a sequence of phrases in a similar spirit. Specifically, we propose a greedy segmentation algorithm based on forward maximum matching. Taking a document D = D 1 , . . . , D m of m tokens as an example, our algorithm segments the document from left to right. The first i tokens will be cut out as a phrase if it can be found as a sub-sequence in other documents and i is the maximum valid value. The above process is repeated until all tokens are cut out. Note that some resultant phrases can be single tokens in the fixed token vocabulary when no proper matching can be found. Detailed explanations of the phrase segmentation algorithm can be found in Appendix D.</p><p>Suppose that a document D has been split into n phrases D = p 1 , . . . , p n . If the k-th phrase p k is copied from another document, let D k be the source document and let s k , e k be the start and end positions of p k in D k , the phrase encoder is used to extract its context-dependent phrase representations PhraseEncoder(s k , e k , D k ) (Eq. 2). On the other hand, we directly retrieve the context-independent token embedding of p k if it is copied from the fixed token vocabulary. As illustrated by Eq. 1, COG relies on a shared vector space of prefix and phrase representations, where the representations of semantically coherent prefixes and phrases should be closer to each other while others should be pushed apart. We define the training loss for next-phrase predictions by using the InfoNCE loss with in-batch negatives <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref>:</p><formula xml:id="formula_5">L p = - 1 n n k=1 log exp(q k ? p k ) p?P k exp(q k ? p p ) + w?V exp(q k ? v w )</formula><p>where P k consists of all the phrases in the source document D k , V is the set of all tokens in the token vocabulary, and q k denotes the representation of the prefix preceding the phrase p k in D.</p><p>Additionally, to retain the capability of token-level generation, we also train COG with the standard token-level autoregressive loss.</p><formula xml:id="formula_6">L t = - 1 m m i=1 log exp(q i , v Di )</formula><p>w?V exp(q i , v w )</p><p>where q i denotes the prefix representation preceding the token D i in D. Finally, the training loss is the sum of these two losses:</p><formula xml:id="formula_7">L = L p + L t 4 EXPERIMENTAL SETUP 4.1 BASELINES</formula><p>We compare COG with the following three baselines:</p><p>? Transformer <ref type="bibr" target="#b34">(Vaswani et al., 2017)</ref> has been the de facto model for neural language models. Concretely, we fine-tune the pre-trained GPT2 model <ref type="bibr" target="#b26">(Radford et al., 2019)</ref> in our experiments.</p><p>? kNN-LM <ref type="bibr" target="#b14">(Khandelwal et al., 2020</ref>) is a retrieval-augmented generation model, which extends a pre-trained neural language model by linearly interpolating its next token distribution with a k-nearest neighbors (kNN) model.</p><p>? RETRO <ref type="bibr" target="#b2">(Borgeaud et al., 2022)</ref> <ref type="foot" target="#foot_0">2</ref> is another retrieval-augmented generation model which combines a frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism to predict next tokens. Since there is no pre-trained RETRO model that could be accessed, we train it from scratch on the WikiText-103 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">IMPLEMENTATION DETAILS</head><p>All the baselines and our source codes are based on the popular Huggingface transformers package <ref type="bibr" target="#b37">(Wolf et al., 2020)</ref>. For a fair comparison, the prefix encoders in Transformer, kNN-LM, and COG use the same model architecture as the pre-trained GPT2 model (12 layers, 12 heads, and 768 hidden dimensions) <ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. For the phrase encoder in COG, we fine-tune the pre-trained BERT-base-cased model <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref> (12 layers, 12 heads, and 768 hidden dimensions). We train baselines and COG for 400,000 steps on 8 Tesla-V100 GPUs. For all the baselines, the learning rate, dropout rate, and gradient clipping are set as 5e-5, 0.1, and 1.0, respectively. Due to memory limitation, the batch size is set to contain 256 phrases. For the BERT model in the phrase encoder, the maximum sequence length is set as 256. For the GPT2 model in the prefix encoder, the maximum sequence length is set as 512. Our proposed COG contains overall 248M parameters from BERT and GPT2 models, and other baselines contain over 124M parameters. As suggested by <ref type="bibr" target="#b2">Borgeaud et al. (2022)</ref>, the hyper-parameters ? and ? of kNN-LM are set as 0.118 and 0.00785, respectively.</p><p>To improve the inference efficiency of COG, we encode all the documents in the source text collections offline. Note that retrieving from such a super large phrase collection faces severe challenges on the engineering side. This paper uses a coarse-to-fine pipeline to address this challenge. Specifically, we first use a document retriever to retrieve top-k related documents for each given prefix. Then, their corresponding phrase representations are collected for selection. In this paper, a popular semantic matching model, DPR <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref> and a vector search toolkit, FAISS <ref type="bibr" target="#b12">(Johnson et al., 2019)</ref> are used as the document retriever, which can recall documents that have similar topics with the prefix. The value k is empirically set to 1024.</p><p>COG can be used with both greedy search and nucleus sampling. For greedy search, COG selects the phrase that has the highest fitness score at each time step. As for nucleus sampling, we first obtain the next-phrase distribution by using the softmax function over the fitness scores of all candidate phrases. Then, the next phrase is sampled over this distribution.</p><p>More details of the implementation can be found in Appendix A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">AUTOMATIC EVALUATION METRICS</head><p>For each document in the test set, we use the first 32 tokens as the prefix. The baselines and our proposed COG generate text continuations of length 128 based on the same prefix. Following conventions <ref type="bibr" target="#b35">(Welleck et al., 2020;</ref><ref type="bibr" target="#b32">Su et al., 2022)</ref>, we use greedy search and nucleus sampling <ref type="bibr" target="#b11">(Holtzman et al., 2020</ref>) (p = 0.95) throughout our experiments. Following previous work <ref type="bibr" target="#b35">(Welleck et al., 2020;</ref><ref type="bibr" target="#b32">Su et al., 2022)</ref> and report the results on the following evaluation metrics:</p><p>? MAUVE <ref type="bibr" target="#b25">(Pillutla et al., 2021)</ref>, an efficient, interpretable, practical automatic evaluation, is highly coherent with human judgments and widely used to evaluate modern text generation models <ref type="bibr" target="#b32">(Su et al., 2022;</ref><ref type="bibr" target="#b16">Krishna et al., 2022)</ref>. In this paper, MAUVE leverages the GPT2large model to generate the scores, and the scaling factor is set as 2.0.</p><p>? Rep-n <ref type="bibr" target="#b35">(Welleck et al., 2020)</ref> measures the sequence-level repetition as the portion of duplicate n-grams in the generated text <ref type="bibr" target="#b35">(Welleck et al., 2020)</ref>. For a generation text x, Rep-n can be formulated as: 100</p><formula xml:id="formula_8">? (1.0 -|unique n-gram(x)| |total n-gram(x)| ).</formula><p>Higher Rep-n denotes the severe degeneration problem in generations.</p><p>? Diversity <ref type="bibr" target="#b35">(Welleck et al., 2020)</ref> measures the diversity of the generations, which is formulated as ? 4 n=2 (1 -Rep-n 100 )). Generations that have higher Diversity scores usually are more informative.</p><p>Note that previous work <ref type="bibr" target="#b14">(Khandelwal et al., 2020;</ref><ref type="bibr" target="#b6">Dai et al., 2019)</ref> often uses perplexity as the primary evaluation metric to measure the performance of language modeling. However, since our proposed COG does not calculate next-token distributions over a fixed vocabulary, the comparison of perplexities is not reliable and thus omitted. However, we can test the perplexity of generated text using an external language model, and the results are shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>In this paper, we evaluate baselines and our proposed COG in three different settings: (1) standard language modeling; (2) domain adaption; (3) enlarged phrase index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LANGUAGE MODELLING ON WIKITEXT-103</head><p>In this setting, models are trained on the training set of the WikiText-103 dataset and evaluated on its test set. The WikiText-103 dataset <ref type="bibr" target="#b23">(Merity et al., 2017)</ref> contains an extensive collection of Wikipedia articles with over 100 million words, which is widely used to evaluate the performance of universal language modeling <ref type="bibr" target="#b14">(Khandelwal et al., 2020;</ref><ref type="bibr" target="#b6">Dai et al., 2019;</ref><ref type="bibr" target="#b32">Su et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Decoding  Inference Speed Furthermore, we also compare the average time cost of different methods for completing the generation on the test set. Since the phrase representations in COG are precomputed offline, its encoding time cost is not included. The results are reported in Table <ref type="table" target="#tab_1">1</ref>. As seen, COG still achieves comparable inference efficiency with the standard Transformer baseline. The reason is that the copied phrases usually contain multiple tokens (the statistics of phrase length are shown in Table <ref type="table" target="#tab_2">2</ref>). As a result, COG uses fewer decoding steps when generating the text of the same length. Unlike COG that uses a coarse-to-fine search pipeline, kNN-LM conducts large-scale vector search at every decoding step. Its inference latency is much higher than Transformer, and COG, which is aligned with previous work <ref type="bibr" target="#b0">(Alon et al., 2022)</ref>.  Human Evaluation To ensure the reliability of our evaluations, we also run human evaluation with three native-speaker graders from a third-party grading platform. Specifically, we randomly select 100 test prompts. For each test prompt, the annotators are given two continuations, in random order, which are generated by COG and Transformer respectively. The annotators are asked to decide which one is better by considering the following aspects:</p><p>? Fluency: Whether the generated text is fluent and easy to understand.</p><p>? Informativeness: Whether the generated text is diverse and contains interesting content.</p><p>When annotators make different decisions on the same sample, we ask them to have a discussion and make the final decision. As shown in Table <ref type="table" target="#tab_4">3</ref>, our proposed COG model significantly outperforms strong Transformer baseline, indicating its better generation quality.</p><p>Case Study For a better understanding of the performance of COG, we present an example of the text continuations generated by our proposed COG in Figure <ref type="figure">2</ref>. It can be found that COG can retrieve phrases that are semantically coherent and fluent for given prefixes. For example, at the second decoding step, COG generate the punctuations [", .] from the pre-defined vocabulary to close the film name "The Man Trap" and the sentence. Besides, at the ninth decoding step, COG directly copied the named entity Magic Roundabout television series from the related document. More examples can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">DOMAIN ADAPTION ON LAW-MT</head><p>In the domain adaption setting, the models trained on the WikiText-103 dataset are tested on a specific domain. Following previous work <ref type="bibr" target="#b10">(He et al., 2021;</ref><ref type="bibr" target="#b0">Alon et al., 2022)</ref>, we use the English part of Law-MT <ref type="bibr" target="#b15">(Koehn &amp; Knowles, 2017)</ref>, which is an English-German translation dataset for law documents. The memory of kNN-LM, RETRO and COG are constructed from the training set of Law-MT. We also present the performance of Transformer baselines with or without further fine-tuning on the training set of Law-MT. Table <ref type="table">5</ref>: Human evaluation on Law-MT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human Evaluation</head><p>We also conduct the human evaluation on the Law-MT corpus, which has a similar setup to that in ( ?5.1). Table <ref type="table">5</ref> shows that most of COG's generations are better than a strong Transformer baseline. This observation demonstrates that COG can even outperform the fine-tuned Transformer baseline without any domain-specific training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ENLARGED PHRASE INDEX WITH EN-WIKI</head><p>In the enlarged phrase index setting, we make use of a large text collection, the En-Wiki corpus, and test baselines on the test set of WikiText-103. The En-Wiki corpus contains a large-scale collection of Wikipedia articles with over 3 billion words, whose size is much larger than the WikiText-103 dataset. The memory of kNN-LM, RETRO, and COG are built from the training set of En-Wiki<ref type="foot" target="#foot_1">3</ref> . Similar to the domain adaption setting, we also present the results of Transformer baselines with or without further fine-tuning on the En-Wiki corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results are shown in Table <ref type="table" target="#tab_6">6</ref>. COG with En-Wiki memory surpasses other strong baselines and COG with WikiText-103 memory. This is especially remarkable because COG does not require any additional training, suggesting we can train COG with a smaller corpus but leverage additional information in a larger corpus in a plug-and-play fashion. Similar to the domain adaption setting, we also notice that, although kNN-LM baseline improves Diversity scores, it obtains a much lower MAUVE score than COG (23.39 vs. 26.97). Note that the Transformer w/ FT is slightly worse than that without fine-tuning on the En-Wiki dataset. This phenomenon is mainly because there are deviations between En-Wiki and WikiText-103 datasets.</p><p>Effects of Index Size To further investigate how the size of the phrase index affects the generation quality, we randomly sample several subsets of the En-Wiki dataset with proportions from 0.1% to 100%. As shown in Figure <ref type="figure" target="#fig_0">3</ref>, when the proportion is less than 1%, COG exhibits a similar quality, which is unsurprising since few enlarged documents are added to the phrase index. In contrast, once the proportion is larger than 1%, the larger the phrase index becomes, the better generation quality the model achieves.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Dense Retrieval The dense retrieval technique <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref> has been widely used in many downstream NLP tasks, such as open-domain question answering <ref type="bibr" target="#b13">(Karpukhin et al., 2020;</ref><ref type="bibr">Lee et al., 2021)</ref>, open-domain dialogue systems <ref type="bibr" target="#b18">(Lan et al., 2021)</ref> and machine translation <ref type="bibr" target="#b18">(Cai et al., 2021)</ref>. Different from the traditional sparse retrieval system, such as BM25 and TF-IDF <ref type="bibr" target="#b27">(Robertson &amp; Zaragoza, 2009)</ref>, dense retrieval learns a shared vector space for queries and documents, where relevant pairs of query and document have smaller distances (i.e., higher similarities) than the irrelevant pairs.</p><p>The most closely related work to our study is DensePhrase <ref type="bibr">(Lee et al., 2021)</ref>. DensePhrase reformulates the question-answering task as a phrase retrieval problem, where phrases are directly retrieved and returned as answers to factual questions. Differently, our work aims to generate coherent text continuations through multiple rounds of phrase retrieval. Since the connection between two adjacent phrases should be coherent and fluent in the text generation task, it is much more difficult.</p><p>Retrieval-Augmented Text Generation (RAG) Retrieval-augmented text generation has gained increasing interest recently. Most prior work improves the generation quality (e.g., informativeness) of language models by grounding the generation on a set of retrieved materials (e.g., relevant documents) <ref type="bibr" target="#b22">(Li et al., 2022;</ref><ref type="bibr" target="#b8">Guu et al., 2020;</ref><ref type="bibr" target="#b9">Hashimoto et al., 2018;</ref><ref type="bibr" target="#b36">Weston et al., 2018;</ref><ref type="bibr">Cai et al., 2019a;</ref><ref type="bibr" target="#b9">b;</ref><ref type="bibr" target="#b14">Khandelwal et al., 2020;</ref><ref type="bibr" target="#b38">Wu et al., 2019;</ref><ref type="bibr" target="#b8">Guu et al., 2020;</ref><ref type="bibr">Lewis et al., 2020;</ref><ref type="bibr" target="#b2">Borgeaud et al., 2022;</ref><ref type="bibr">Yang et al., 2023)</ref>. Our work is on this line of research but takes a radical step forward. Unlike prior work that builds the combinations of retrieval and generation, retrieval is generation in COG.</p><p>One contemporary work to our work is <ref type="bibr" target="#b24">Min et al. (2022)</ref>, which shares the idea of replacing the fixed vocabulary with a nonparametric phrase table. However, <ref type="bibr" target="#b24">Min et al. (2022)</ref> focuses on masked language modeling while our focus is on causal language modeling and text generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we reformulated text generation as progressively copying phrases from the massive text collection.  We calculate the perplexity of the generated texts under a large pre-trained language model (GPT2-Large).</p><p>As shown in Table <ref type="table" target="#tab_8">8</ref>, it can be found texts generated by greedy search can achieve very low perplexity scores (even much lower than the ground-truth)<ref type="foot" target="#foot_2">4</ref> . This is expected as greedy search targets at likelihood maximization. Sampling-based decoding methods give much higher perplexity scores. Moreover, it is worth noting that COG achieves the closest perplexity score to ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D THE PHRASE SEGMENTATION ALGORITHM</head><p>COG takes phrases as the minimum units that can be put together to form a coherent document. To train COG, we design a phrase segmentation algorithm to split each document in the training set into a sequence of phrases. This algorithm makes use of a forward maximum matching strategy to identify phrases. Maximum matching is one of the most popular structural segmentation algorithms. This method favors long phrases and is a greedy algorithm by design. Specifically, we treat each document as a sequence of tokens and scan each document from left to right. At each step, we search for the longest prefix of the unsegmented part that is also a sub-sequence of other documents other than the current document. If the length of that prefix is bigger than 2, we take that prefix as the next phrase. Otherwise, we take the first token as the next phrase and it is labeled as coming from the fixed token vocabulary. In both cases, we process the rest part of the current document recurrently. The algorithm can be very time-consuming because exhaustive searches over millions of documents are compute-intensive. Therefore, we propose an efficient approximation as follows. First, we retrieve the top-k most similar documents for each document using the popular DPR model <ref type="bibr" target="#b13">(Karpukhin et al., 2020)</ref> <ref type="foot" target="#foot_3">5</ref> , and vector search toolkits, FAISS <ref type="bibr" target="#b12">(Johnson et al., 2019)</ref>. Then, the phrase search only runs on the corresponding top-k documents. The relevant documents usually have similar topics to the current document. The value of k is set as 1024 in our experiments. The details of our proposed phrase segmentation algorithm can be found in Algorithm 1: SearchPhrase is a function that searches the cached token sequence (i.e., the current candidate for the next phrase) among the most relevant documents. It returns a label that denotes whether the phrase can be found and its position in the relevant documents. , where p i,x denotes the x-th phrase in d i that also appears in another document d j in position j. ||d i || p denotes the number of the collected phrases in d i .</p><p>1 Preprocess: split each document into token-level pieces by using the off-the-shelf tokenizer.</p><p>The preprocessed document set can be formulated as D = {{t </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Generation quality of COG with different sizes of the phrase index. For each proportion (point in the X-axis), we sample 10 times and record the averaged MAUVE score. A proportion of 0.0 indicates that only documents from WikiText-103 are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Published as a conference paper at ICLR 2023 Algorithm 1: Phrase Segmentation Algorithm Data: Document set: D = {d i , {d j } K j=1 } N i=1 , where d i denotes the i-th document. K denotes the number of retrieved documents. N denotes the number of documents in the training set. The pre-defined maximum and minimum phrase lengths are L max and L min . Result: Segmented document set by phrase granularity: D ? = {{(p i,x , (d j , pos j ))} ||di||p x=1 } N i=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The automatic evaluation on the test set of WikiText-103. As for each model with nucleus sampling, we run 10 times and recorded the average MAUVE and Diversity scores. An example generated by COG on the test set of WikiText-103. The dotted squares denote that the content (highlighted in red )is copied from the token vocabulary, and the solid squares denote that the content (highlighted in blue ) is copied from other documents. observation demonstrates that COG is more robust and less prone to the degeneration problem, which can be considered as an additional bonus.</figDesc><table><row><cell></cell><cell></cell><cell cols="5">MAUVE? Rep-2? Rep-3? Rep-4 ? Diversity? Latency (s)?</cell></row><row><cell>Transformer</cell><cell>greedy nucleus</cell><cell>19.87 23.43</cell><cell>43.56 38.55 5.10 1.33</cell><cell>35.5 0.50</cell><cell>22.37 93.22</cell><cell>1.32 1.48</cell></row><row><cell>kNN-LM</cell><cell>greedy nucleus</cell><cell>19.92 22.50</cell><cell>43.79 38.76 3.33 0.69</cell><cell>35.69 0.21</cell><cell>22.13 95.8</cell><cell>10.36 10.42</cell></row><row><cell>RETRO</cell><cell>greedy nucleus</cell><cell>21.19 22.86</cell><cell>44.65 39.63 6.21 1.93</cell><cell>36.6 0.86</cell><cell>21.19 91.19</cell><cell>4.39 4.51</cell></row><row><cell>COG</cell><cell>greedy nucleus</cell><cell>26.01 26.14</cell><cell>28.14 23.80 7.31 2.66</cell><cell>21.40 1.28</cell><cell>43.03 89.07</cell><cell>1.29 1.54</cell></row></table><note><p><p><p><p><p><p>Results</p>Table</p>1</p>shows the performance comparison between the baselines and our proposed COG on the test set of the WikiText-103 corpus. It can be found that our proposed COG substantially outperforms the Transformer and kNN-LM baselines on most metrics. Specifically, COG improves MAUVE score over the best baseline (Transformer with nucleus sampling) from 23.43 to 26.14an improvement of 2.71%. Interestingly, although it is well known that greedy search could raise severe degeneration problems</p><ref type="bibr" target="#b35">(Welleck et al., 2020)</ref></p>, COG with greedy search still outperforms the standard Transformer baseline with nucleus sampling, with 2.58% improvements on MAUVE. This</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The statistics on the length of the copied phrases (on the test set of WikiText-103).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation on the WikiText-103 corpus.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The automatic evaluation on Law-MT.</figDesc><table><row><cell>Results As shown in Table 4, it can be ob-</cell><cell>Model</cell><cell cols="4">Decoding MAUVE ? Diversity ?</cell></row><row><cell>served that COG even outperforms the Trans-former model further fine-tuned on the Law-</cell><cell>Transformer w/o FT</cell><cell></cell><cell>greedy nucleus</cell><cell>20.32 25.21</cell><cell>70.66 93.88</cell></row><row><cell>MT corpus (Transformer w/ FT). Specifically, COG outperforms Transformer w/ FT by 2.93%</cell><cell>Transformer w/ FT</cell><cell></cell><cell>greedy nucleus</cell><cell>23.00 26.85</cell><cell>80.52 90.14</cell></row><row><cell>MAUVE score. The results indicate that COG allows a single model to be specialized in dif-ferent domains, by simply switching the source text collection. Although kNN-LM brings in higher Diversity scores, COG surpasses it by</cell><cell>kNN-LM RETRO COG</cell><cell></cell><cell>greedy nucleus greedy nucleus greedy nucleus</cell><cell>23.31 24.75 18.70 20.35 21.31 28.14</cell><cell>19.85 94.60 71.14 94.81 84.32 92.56</cell></row><row><cell>3.39% MAUVE score, which shows COG has</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>higher generation quality in general.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Comparison</cell><cell></cell><cell cols="3">Better No Prefer. Worse</cell></row><row><cell></cell><cell cols="2">COG vs. Transformer w/ FT</cell><cell>52%</cell><cell>12%</cell><cell>36%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The automatic evaluation on the test set of WikiText-103, the memory is built on the train set of En-Wiki. Transformer w/ FT and Transformer w/o FT denote the Transformer baseline with and without further fine-tuning on the train set of En-Wiki, respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">Decoding MAUVE ? Diversity ?</cell></row><row><cell>Transformer w/o FT</cell><cell>greedy nucleus</cell><cell>19.87 23.43</cell><cell>22.37 93.22</cell></row><row><cell>Transformer w/ FT</cell><cell>greedy nucleus</cell><cell>20.21 21.31</cell><cell>19.62 92.92</cell></row><row><cell>kNN-LM</cell><cell>greedy nucleus</cell><cell>23.21 23.39</cell><cell>20.33 96.37</cell></row><row><cell>RETRO</cell><cell>greedy nucleus</cell><cell>19.75 22.87</cell><cell>21.15 91.09</cell></row><row><cell>COG</cell><cell>greedy nucleus</cell><cell>24.68 26.97</cell><cell>40.45 90.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The number of sentences in the WikiText-103 and Law-MT datasets. ) the phrases in a batch of training documents. During inference, the dynamic vocabulary consists of the word-level vocabulary and the phrases extracted from the Top-k retrieved documents (k=1024 in this paper). The size of the pre-defined word-level vocabulary contains 50257 subwords. Since there are only a few documents encoded to extract the phrase representations, the average number of the phrase representations is 950,942.4 in the WikiText-103 test set when K = 1024.</figDesc><table><row><cell>Benchmarks</cell><cell>Train</cell><cell>Dev</cell><cell>Test</cell></row><row><cell cols="4">WikiText-103 1,801,350 3,760 4,358</cell></row><row><cell>Law-MT</cell><cell>389,292</cell><cell cols="2">2,000 2,000</cell></row><row><cell cols="2">B MORE IMPLEMENTATION DETAILS</cell><cell></cell><cell></cell></row><row><cell cols="5">During training, the dynamic vocabulary of COG contains two parts: (1) word-level vocabulary size</cell></row><row><cell cols="2">(50257 in GPT2 vocabulary); (2C PERPLEXITY OF GENERATED TEXT</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Models</cell><cell>Perplexity greedy nucleus</cell></row><row><cell></cell><cell></cell><cell cols="2">Transformer</cell><cell>3.26</cell><cell>37.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell>kNN-LM</cell><cell>3.48</cell><cell>78.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RETRO</cell><cell>3.27</cell><cell>36.40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>COG</cell><cell>10.41</cell><cell>27.24</cell></row><row><cell></cell><cell></cell><cell cols="2">Ground-Truth</cell><cell>18.64</cell></row></table><note><p>Following this formalization, we proposed a novel neural text generation model, named COG, which generates text by retrieving semantically coherent and fluent phrases from other documents. Experimental results proved the advantages of COG over the strong baselines on three experimental settings: standard language modeling (WikiText-103), domain adaptation (Law-MT), and enlarged phrase index (En-Wiki).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The perplexity on the test set of WikiText-103.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>4</head><label></label><figDesc>i,x } ||di||t x=1 , {d j } K j=1 } N i=1 , where t i,x is the x-th token of d i , which consists of ||d i || t tokens. Prepare the empty list D ? = {}, empty phrase cache cache p ={}, and cached search success label label last . PhraseCollection={} 5 while cursor? ||d i || t do 6 if L min ? len(cache p ) ? L max then 7 label now , rest=SearchPhrase(cache p ) last is False and label now is False then 19 PhraseCollection.append( cache p , None)</figDesc><table><row><cell>8</cell><cell>else</cell></row><row><cell>9</cell><cell>if len(cache p ) &gt; L max then</cell></row><row><cell>10</cell><cell>cache p ={}</cell></row><row><cell>11</cell><cell>end</cell></row><row><cell>12</cell><cell>end</cell></row><row><cell>13</cell><cell>if label last is True and label now is False then</cell></row><row><cell>14</cell><cell>cursor -= 1</cell></row><row><cell>15</cell><cell>PhraseCollection.append(cache p , rest)</cell></row><row><cell>16</cell><cell>cache p ={}</cell></row><row><cell>17</cell><cell>else</cell></row><row><cell cols="2">18 if label 20 cache p ={}</cell></row><row><cell>21</cell><cell>end</cell></row><row><cell>22</cell><cell>end</cell></row><row><cell>23</cell><cell>cursor += 1</cell></row><row><cell>24</cell><cell>label now =label last</cell></row><row><cell>25</cell><cell>end</cell></row><row><cell>26</cell><cell></cell></row></table><note><p>2 for i ? 1 to N do 3 cursor=0</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/lucidrains/RETRO-pytorch.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Due to the hardware limitation, RETRO uses the subset of the En-Wiki corpus (over 6 million chunks).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that the original perplexity of GPT2-Large model on the test set of WikiText-103 is 22.05<ref type="bibr" target="#b26">(Radford et al., 2019)</ref>. The gap between it and our results is caused by the different number of samples. In this study, we only use samples that have more than 32 tokens to generate text.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>JUSTIFICATION OF CHANGES</head><p>Note that the experimental results in the current version have some changes from the previous version that has been reviewed. We made a number of revisions to the experiments according to the valuable suggestions from the reviewers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The authors thank the anonymous reviewers for their valuable suggestions and comments on our paper, which significantly improves the quality of our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E MORE CASES</head><p>In this section, we present some generated examples of COG given some specific prefixes. As shown in Figure <ref type="figure">4</ref>, 5 and 6, it can be observed that the generated continuations are fluent and coherent with the given prefix. However, we also notice some flaws. For example, as shown in Figure <ref type="figure">5</ref>, COG copied the phrase 75 mph from the document ... sustained winds of at least 120 km / h (75 mph), which is incoherent with the previous copied phrase 106 km / h. Moreover, as shown in Figure <ref type="figure">6</ref>, COG copied the phrase Rhine and Main from the document (Bt the terms of the Peace of Basel (22 July 1795), the Prussian army was to leave the Rhine and Main river valleys ...). However, the complete phrase should be Rhine and Main river valleys, and COG only copy a part of it, leading to inaccurate generation results (rivers). In the Philippines, officials evacuated over 14 @,@ 000 people. Imbudo was the strongest typhoon to strike since Typhoon Zeb five years prior, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Prefix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neuro-symbolic language modeling with automaton-augmented retrieval</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudipta</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving language models by retrieving from trillions of tokens</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliza</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katie</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">Bm</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Damoc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton-toresponse: Dialogue generation guided by retrieval memory</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>a</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Retrieval-guided dialogue response generation via a matching-to-generation framework</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaopeng</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1195</idno>
		<ptr target="https://aclanthology.org/D19-1195" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">November 2019</date>
			<biblScope unit="page" from="1866" to="1875" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Neural machine translation with monolingual translation memory</title>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wai</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zora</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08909</idno>
		<title level="m">Realm: Retrievalaugmented language model pre-training</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A retrieve-and-edit framework for predicting structured outputs</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tatsunori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><forename type="middle">S</forename><surname>Oren</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04212</idno>
		<title level="m">Efficient nearest neighbor language models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Billion-scale similarity search with GPUs</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization through memorization: Nearest neighbor language models</title>
		<author>
			<persName><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Six challenges for neural machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Knowles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Neural Machine Translation</title>
		<meeting>the First Workshop on Neural Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Rankgen: Improving text generation with large ranking models</title>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yapei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.09726</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring dense retrieval for dialogue response selection</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06612</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning dense representations of phrases at scale</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mujeen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Ankur Parikh, Dipanjan Das, and Jonathan Berant. Learning recurrent span representations for extractive question answering</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimi</forename><surname>Salant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01436</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive NLP tasks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>K?ttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020, virtual, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A survey on retrieval-augmented text generation</title>
		<author>
			<persName><forename type="first">Huayang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemao</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv, abs/2202.01110</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. April 24-26, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.01349</idno>
		<title level="m">Nonparametric masked language modeling</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mauve: Measuring the gap between neural text and human text using divergence frontiers</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Pillutla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Thickstun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The probabilistic relevance framework: Bm25 and beyond</title>
		<author>
			<persName><forename type="first">S</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zaragoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointer-generator networks</title>
		<author>
			<persName><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Phraseindexed question answering: A new challenge for scalable document comprehension</title>
		<author>
			<persName><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Effidit: Your ai writing assistant</title>
		<author>
			<persName><forename type="first">Shuming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiyun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyang</forename><surname>Ma</surname></persName>
		</author>
		<idno>ArXiv, abs/2208.01815</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A contrastive framework for neural text generation</title>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08">2014. December 8-13 2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04">2017. December 4-9, 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural text generation with unlikelihood training</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Kulikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Retrieve and refine: Improved sequence generation models for dialogue</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SCAI@EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Response generation by context-aware prototype editing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Inference with reference: Lossless acceleration of large language models, 2023. A DATASET STATISTICS The experiments in this paper include three benchmarks</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binxing</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>WikiText-103; (2) English part of Law-MT</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The statistics of these benchmarks are shown in Table 7. En-Wiki corpus is used for the enlarged phrase index settings</title>
		<author>
			<persName><surname>En-Wiki</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in this paper, containing over 4,848,348 long English Wikipedia documents</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
