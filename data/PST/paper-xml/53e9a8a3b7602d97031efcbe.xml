<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Hierarchies for Object Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Boris</forename><surname>Epshtein</surname></persName>
							<email>boris.epshtein@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics Weizmann Institute of Science Rehovot</orgName>
								<address>
									<postCode>76100</postCode>
									<country key="IL">ISRAEL</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
							<email>shimon.ullman@weizmann.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Applied Mathematics Weizmann Institute of Science Rehovot</orgName>
								<address>
									<postCode>76100</postCode>
									<country key="IL">ISRAEL</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Feature Hierarchies for Object Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">182FBC261BAFAAB76CA72AA2C8CE35A6</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The paper describes a method for automatically extracting informative feature hierarchies for object classification, and shows the advantage of the features constructed hierarchically over previous methods. The extraction process proceeds in a top-down manner: informative top-level fragments are extracted first, and by a repeated application of the same feature extraction process the classification fragments are broken down successively into their own optimal components. The hierarchical decomposition terminates with atomic features that cannot be usefully decomposed into simpler features. The entire hierarchy, the different features and sub-features, and their optimal parameters, are learned during a training phase using training examples. Experimental comparisons show that these feature hierarchies are significantly more informative and better for classification compared with similar non-hierarchical features as well as previous methods for using feature hierarchies.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The selection of effective image features is a crucial component of a successful classification scheme. A number of recent classification methods have used features composed of image patches, or fragments, selected from training images during a learning stage <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref>. The success of these methods is mainly due to two reasons: first, they identify common object parts that characterize the different objects within the class, and second, the parts are combined in a manner that allows variations learned from training data. This notion is extended in the present work from the representation of objects to the representation of their constituent parts. Instead of representing a local part by a fixed image fragment, the part itself (such as an eye in face detection) is decomposed into its own optimal components (e.g. eyelid, eye corner, eye pupil, etc.), and the allowed variations in the configuration of the sub-parts are learned from the training data. The decomposition into sub-parts continues recursively and terminates at the level of 'atomic fragments', which cannot be broken down further without loss in mutual information. We describe in this paper an algorithm for obtaining informative feature hierarchies, and show that the resulting hierarchies are more informative and better for classification compared with holistic features. The input to the algorithm is a set of images belonging to the same object class and a set of non-class images. The output is a set of hierarchical features together with the learned parameters (combination weights, geometric relations) suitable for the recognition of novel instances of the learned class. Examples of the hierarchical features obtained by the algorithm are shown in Figures <ref type="figure" target="#fig_0">1,</ref><ref type="figure" target="#fig_6">5</ref>. Experimental evaluations show that the decomposition by our method increases the amount of information delivered by the fragments by a wide margin, improves the detection rate, and increases the tolerance for local distortions and illumination changes.</p><p>The rest of the paper is organized as follows. In the next section we briefly review previous relevant approaches to the problem of selecting and combining features for recognition. In Section 3 we describe the proposed algorithm. In Section 4 we show experimental results, comparing the use of hierarchical features with holistic features for object detection on several object classes. We conclude with general remarks on possible extensions and applicability of the method in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous work</head><p>In several recent schemes for object detection and classification, the basic features used for classification are local image fragments, or patches, depicting significant object components, and selected from training images during a learning stage <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b3">[3]</ref><ref type="bibr" target="#b4">[4]</ref><ref type="bibr" target="#b5">[5]</ref>. The features can be selected from a large pool of candidate fragments <ref type="bibr" target="#b0">[1]</ref> or from a set of regions selected by interest operators <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">3]</ref>. During the classification stage, the features are located in the image being classified, and then combined using a number of possible methods including a na√Øve-Bayesian combination <ref type="bibr" target="#b0">[1]</ref>, a probabilistic model combining appearance, shape and scale <ref type="bibr" target="#b3">[3]</ref>, the output of a classifier network <ref type="bibr" target="#b1">[2]</ref> or an SVM-based classifier <ref type="bibr" target="#b4">[4]</ref>. The features used by these methods were non-hierarchical, that is, they were not broken down into distinct simpler sub-parts, but detected directly by comparing the fragment to the image. Their similarity can be measured by different measures, including normalized cross-correlation, affine-invariant measures <ref type="bibr" target="#b6">[6]</ref>, and the SIFT measure <ref type="bibr" target="#b7">[7]</ref>.</p><p>A number of classification schemes have also used feature hierarchies rather than holistic features. Such schemes were often based on biological modeling, motivated by the structure of the primate visual system, which has been shown to use a hierarchy of features of increasing complexity, from simple local features in the primary visual cortex, to complex shapes and object views in higher cortical areas. In a number of these models, <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref>, the architecture of the hierarchy (size, position and shape of features and their sub-features) is pre-defined rather than learned for different classification tasks. The learning of a particular class was obtained by the combination of weights from the upper level of the hierarchy. A hierarchical model trained by examples was studied in <ref type="bibr" target="#b10">[10]</ref>. The study uses a network model in which both the combination weights and the convolution templates were learned from examples by backpropagation, whereas the number of hierarchy levels and positional tolerance were pre-defined. Previous comparisons <ref type="bibr" target="#b11">[11]</ref> as well as our experiments (Section 4) show that the features used by these hierarchical models are not as informative and useful for classification as the classification features extracted by the methods reviewed above, and this accounts in part for limitations in their performance. In summary, classification features used in the past were either highly informative but non-hierarchical, or hierarchical features which were less informative and not as useful.</p><p>In the present work, we combine the advantages of learning informative classification fragments, with the learning of hierarchical structure with adaptive parameters. Informative object components are used for classification, but they are represented and detected using a hierarchy of simpler sub-parts. The next section describes the method of extracting the full hierarchy and its associated parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The construction of the feature hierarchies</head><p>In this section, we describe the algorithm for obtaining the feature hierarchies. The algorithm proceeds along the following main stages. First, initial informative fragments are selected (Section 3.1). Second, the selected fragments are used to define new training sets for the selection of sub-features (3.2). These two steps are applied recursively until a level of 'atomic fragments' is reached. Third, parameters of the features hierarchy are optimized <ref type="bibr">(3.3)</ref>. Finally, the classification using the derived hierarchy is described in <ref type="bibr">(3.4)</ref>. We begin with a description of the initial selection of informative image fragments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Selecting informative image fragments</head><p>We use a method for extracting good initial features similar to <ref type="bibr" target="#b11">[11]</ref>. The process identifies fragments that deliver the maximal amount of information about the class. A large number (tens of thousands) of candidate fragments are extracted from the training images. We consider as initial candidates rectangular fragments of class images at multiple sizes and positions. We used fragments sizes ranging from 10% up to 50% of image size in each dimension, with scaling step of 1.2. For each fragment size, we examine fragments in positions placed on a regular grid with step equal to 1/3 of the size of a fragment. For every fragment, the optimal detection threshold is determined by maximizing the mutual information between the fragment and the class, as explained below. The normalized cross-correlation was used as similarity measure, but other measures, such as SIFT <ref type="bibr" target="#b7">[7]</ref>, can also be used. A binary variable is associated with every fragment in the following way:</p><formula xml:id="formula_0">Ô£≥ Ô£≤ Ô£± &gt; = otherwise f I S if I f i i i i , 0 ) , (<label>, 1 ) , ( Œ∏ Œ∏ (1)</label></formula><p>Here S(I, f i ) is the maximal visual similarity between fragment f i and image I, Œ∏ i is the detection threshold associated with f i. A binary variable C(I) is used to represent the class, namely, C(I) = 1 if the image I belongs to the class being detected and 0 otherwise. Candidate fragments are evaluated by the amount of mutual information <ref type="bibr" target="#b13">[13]</ref> they deliver about the class. The advantages of selecting features by their MI are discussed in <ref type="bibr" target="#b14">[14]</ref>. The mutual information between the two binary variables is defined as:</p><formula xml:id="formula_1">‚àë = = ‚â° } 1 , 0 { } 1 , 0 { ) ( ) ) ( )) ( ( ) ), ( ( log( ) ), (<label>(</label></formula><formula xml:id="formula_2">) ); ( ( C f i i i i i i i i i i C p f p C f p C f p C f MI Œ∏ Œ∏ Œ∏ Œ∏ Œ∏ (2)</formula><p>The mutual information is a function of the detection threshold Œ∏ i . If the threshold is too low, the information delivered by the fragment about the class will be low, because the fragment will be detected with high frequency in both class and non-class images. A high threshold will also yield low mutual information, since the fragment will be seldom detected in both class and non-class images. At some intermediate value of threshold, the mutual information reaches a maximum. The detection threshold for each fragment is selected to maximize the information MI(f i ;C) between the fragment and the class. From the initial pool of candidate fragments, the most informative fragments are selected successively. After finding the fragment with the highest mutual information score, the search identifies the next fragment that delivers the maximal amount of additional information with respect to previously selected fragments. At iteration i the fragment f i was selected to increase the mutual information of the fragment set by maximizing the minimal addition in mutual information with respect to each of the first i-1 fragments.</p><p>)</p><formula xml:id="formula_3">) ; ( ) ; , ( ( min max arg C f MI C f f MI f j j k S f K f i i j i k - = ‚àà ‚àà (3)</formula><p>Here K i is the set of candidate fragments, S i is the set of selected fragments up to iteration i, f i is the fragment to be selected at iteration i. The min is taken over all previously selected f j , to avoid redundancy: if f k is similar to one of the selected fragments, this minimum will be small. The max stage then finds the candidate in the pool with the largest additional contribution. In empirical testing, this algorithm was shown to select highly effective classification features <ref type="bibr" target="#b12">[12]</ref>.</p><p>We drop the dependence on thresholds Œ∏ since they are already set to the optimal value for each fragment separately. The update rule for the fragment sets is:</p><formula xml:id="formula_4">} { \ 1 f K K i i = + (4) } { 1 f S S i i ‚à™ = +</formula><p>The initial K 0 is the set of all candidate fragments; S 0 is the set containing the fragment with highest mutual information with the class. The iterations end when the increment in mutual information gained by a new feature is less than some small threshold Œµ (0.08) or until the number of selected fragments has reached a pre-selected limit.</p><p>For each fragment the extraction process determines an allowed region, or Region of Interest (ROI) within which the fragment is searched. The size of the ROI is also set by an information maximization process described further in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selecting optimal sub-fragments</head><p>The top-level classification features described above appear often in the images containing object to be detected and seldom in non-class images. In a similar manner, useful sub-features should appear often in the regions containing 'parent' feature and seldom elsewhere. To identify such sub-features, we construct for each fragment f a set of positive examples, which are image regions containing the fragment f, and negative examples, where detection of it should be avoided as much as possible. The negative examples are selected from nonclass images that give "false alarms", and therefore lie close to the class/non-class boundary. The positive examples for the fragment f were provided by identifying all the locations in the class images where the fragment f was detected. This set was then increased, since the goal of the fragment decomposition is to successfully detect additional examples that were not captured by the fragment f alone. For this end, the positive set was increased by lowering the detection threshold of the fragment f, yielding examples where f is either detected or almost detected. The reduced threshold was determined to increase the positive set by 20%. This amount of increase was chosen to add a significant number of almostdetected examples, and avoid examples that are dissimilar to f. A set of negative examples was similarly derived from the non-class images. Figure <ref type="figure" target="#fig_1">2</ref> shows the example of an informative fragment together with positive and negative examples of this fragment extracted from the training data.</p><p>For the extraction of the sub-fragments of a feature on lower level of the hierarchy, the same procedure of obtaining positive and negative examples is used. Positive examples come from regions in class images where the parent feature was detected or almost detected within its ROI, and negative examples come from regions in the non-class images where the feature was detected. In this case, the feature position in the training images was determined by the computation of optimal positions of all the hierarchy nodes together (Section 3.4), so that at most one example was taken from each training image.</p><p>Once the positive and negative sets of examples are established, sub-fragments are selected by exactly the same information maximization procedure used at the first level. The candidate sub-fragments in this case are the sub-images with their center point within the parent fragment, and having area not greater than ¬º of the parent's area. Sub-features are added to the tree until the additional information was lower than a threshold (0.08) or their number reached a pre-defined maximum (10 fragments). Experimentally, fragments with smaller contributions did not improve significantly the detection of the parent feature. If the decomposition of f into simpler features increased the delivered information, the same decomposition was also applied to f's sub-features. Each of the sub-fragments was considered in turn a parent fragment, positive and negative examples were found and the set of its informative sub-fragments was selected. Otherwise, decomposition was terminated, with f considered an atomic fragment. Atomic fragments were usually simple, typically containing edges, corners or lines. Hierarchy examples are shown in Figures <ref type="figure" target="#fig_0">1,</ref><ref type="figure" target="#fig_6">5</ref>.</p><p>As explained in Section 3.4, during the classification stage, only the atomic features are directly correlated with the input image, and their responses are combined using weights learned at the training stage.</p><p>Fragment to be detected </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Optimizing the regions of interest</head><p>For each fragment, a region of positional tolerance is extracted, called the feature's region of interest (ROI). The ROI defines the area in novel images where the fragment is searched for. The locations of the ROIs of sub-fragments in every image are determined relative to the detected position of their parent fragment. The amount of information a fragment delivers about the class depends on the size of its ROI. When the ROI is too small, the information is low, because in many class images the fragment will fall outside the ROI and therefore will not be detected. If the size of ROI is too large, the number of false detections will increase. At some intermediate size of the ROI, the mutual information reaches a maximum (Figure <ref type="figure" target="#fig_2">3</ref>). The size of ROI for a fragment f was therefore chosen to maximize the mutual information MI(f;C). For first-level fragments, the optimization process evaluated different candidate ROI sizes from zero to half the size of the search window, and found the size that brought the MI to the maximum. The search window is a fixed region within the input image, where the algorithm looks for the entire object. This window was set in our experiments to size 200x200 pixels. To detect an object within a larger image, the search window can either scan the image or move only to selected salient locations <ref type="bibr" target="#b16">[16]</ref>. The locations of the ROIs of first-level fragments were defined relative to the center of the search window. During the hierarchy construction, the initial ROI size of a sub-fragment was set to be equal to the size of its parent. After the hierarchy was completed, additional optimization of the ROI sizes was performed in a topdown manner: first, the ROI of the uppermost node was optimized to maximize the mutual information between the class variable and hierarchy's detection variable, while all other ROIs were fixed. A similar process was then applied to its children, and the optimization proceeded down the hierarchy, where at each stage the ROIs of the higher levels are kept fixed.</p><p>Another set of hierarchy parameters is the combination weights of the sub-features responses. The optimization of the combination weights is described in Section 3.4 below together with the use of these weights in the classification process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classification by feature hierarchies</head><p>Performance of the hierarchical features was evaluated using a network model similar to HMAX <ref type="bibr" target="#b9">[9]</ref>, with layers performing max and weighted sum operations. For a given feature, the maximal response of each sub-feature is taken over the sub-feature's ROI, and then the responses of all sub-features are combined linearly:</p><formula xml:id="formula_5">‚àë = + = n i i i s w w r 1 0 (5)</formula><p>where r is the combined response, s i the maximal response of sub-feature i within its ROI, w i are the weights of the combination, and n the number of subfeatures. For the atomic sub-features, the response was equal to the maximal normalized cross-correlation between the sub-feature and the image within the ROI. The final response s p of the parent feature was obtained by a sigmoid function, The response of the topmost node of the hierarchy, corresponding to the entire object, is then compared to 0. Positive response means that object is detected. The amount of information about the class carried by the hierarchy is defined as the mutual information between the class variable C and the hierarchy detection variable H, equal 1 when the response of the topmost node is positive and zero otherwise.</p><p>The combination weights were adjusted during training using iterative optimization that alternates between optimizing positions and weights, as described below. First, the weights are initialized randomly in the range [0..1]. The scheme then alternates between the following two steps.</p><p>Positions Step: fix weights, optimize feature positions. For every position of the parent fragment within its ROI the positions of sub-fragments (within their relative ROIs) that maximize the responses of the sub-fragments were found. Then, the position of the parent fragment that maximizes its response s p is chosen. This routine can be implemented efficiently using Dynamic Programming.</p><p>Weights Step: fix feature positions, optimize weights. The combination weights of the features were optimized using the standard Back-Propagation algorithm with batch training protocol. The algorithm ends when no feature changes its position at Positions Step.</p><p>This weight selection procedure can be shown to converge to a local minimum of classification error. Experimentally, we found that the algorithm converged in less than 10 iterations. The obtained optimum is stable, starting from multiple random initial weights we end up with similar performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Summary of the hierarchy construction algorithm</head><p>The full process of constructing hierarchical features can be summarized by the following steps.</p><p>INPUT: A set P of class images. A set N of non-class images.</p><p>OUTPUT: A feature hierarchy H -a tree with nodes corresponding to parts and sub-parts of the object being recognized together with a set of associated parameters: ROI of every node and combination weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HIERARCHY CONSTRUCTION ALGORITHM:</head><p>1. Initialize H as a tree containing a single node (root) f 0 , corresponding to the entire object.</p><p>2. Using the original training sets (P and N), extract a set S(f 0 ) of first-level informative fragments as described in Section 3.1. Add the fragments from S(f 0 ) as children to f 0 . Evaluate the mutual information MI(H;C) as described in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>For each leaf fragment f determine the sets P(f) and N(f) of positive and negative examples as in Section 3.2. 4. Find the set S(f) of the most informative sub-fragments of f . 5. Add the fragments from S(f) as children of f and reevaluate MI(H;C). If it does not increase compared with the case without S(f) -remove S(f), mark the leaf node f as 'atomic' fragment. Otherwise leave S(f) in H.</p><p>6. Repeat steps 3 -5 until all leaf fragments are marked as 'atomic'. OUTPUT: A binary decision variable S (1 if the object was found in I, and 0 otherwise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CLASSIFICATION ALGORITHM:</head><p>1. Compute the correlations of all the leaf nodes of H with the image I. Call the 2D arrays (of size equal to the size of I) containing the correlation values the response maps.</p><p>2. For every node of H whose children's response maps have been computed, compute its own response map: for each position of the feature within the image I, find maximal responses of its children within their ROIs and combine their responses using equations ( <ref type="formula">5</ref>) and <ref type="bibr" target="#b6">(6)</ref>. Store the response of the node in its own response map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Repeat</head><p>Step 2 from the bottom nodes to their parents until the top of the hierarchy is reached.</p><p>4. Using the response map of the topmost node, find the maximal response within its ROI (eq. 6) and compare it to 0. If the response is greater than 0, set S to 1, otherwise set it to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Hierarchical features were extracted and compared with holistic (non-hierarchical) ones using their mutual information, and by comparing classification performance. The mutual information measures the quality of the features directly in a manner that does not depend on the particular classifier <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b14">14]</ref>.</p><p>The information carried by the hierarchical and holistic features was compared using 3 object classes: faces (200 faces, 500 non-faces in the training set, 800 faces, 1500 non-faces in the test set), cows (100 cows, 500 non-cows in the training set, 220 cows, 2500 non-cows in the test set) and airplanes (320 airplanes, 500 non-airplanes in the training set, 750 airplanes, 2500 non-airplanes in the test set).</p><p>For each object class, the most informative holistic feature was first determined using the algorithm described in Section 3.1. For comparison, a hierarchy of subfeatures was extracted from this feature. In computing the ROC curves of a feature <ref type="bibr" target="#b15">[15]</ref>, the hits and false alarms were defined by using the hierarchy as a single feature classifier. That is, test images were classified based on the feature in question; hits corresponded to class image identified correctly, false alarms to non-class images identified incorrectly. The experiment was repeated 50 times for each class, the image database each time split randomly into training and test set. Overall, 150 top-level fragments were extracted, and for each one a hierarchy was constructed using the algorithm above. The information supplied by the first-level hierarchical features increased in the test set for all fragments (n=150, 3 classes) by a large amount compared with the corresponding holistic features (average increase 46.6%, s.d. 30.5%, p &lt; 10 -9 one-tailed paired t-test). The holistic and hierarchical features were also compared using their complete ROC curves, showing a significant advantage of the hierarchical detection over the entire range, (0-90% false alarm, n=150, p&lt;0.000001).</p><p>Further decomposition into a multi-level hierarchy provided additional significant gain in information (n=97 features, average increase 10.0%, s.d. 10.7% p &lt; 10 -9 onetailed paired t-test). The ROC detection curves also improved significantly. Results of the comparisons are shown in Figure <ref type="figure" target="#fig_5">4a,</ref><ref type="figure">b</ref>. Figure <ref type="figure" target="#fig_5">4a</ref> shows the comparison of ROC curves obtained by a holistic feature (blue), the same feature decomposed into a single level of subfeatures (magenta), full hierarchy (red), and a decomposition using fixed spacing and sub-fragment sizes (black). Figure <ref type="figure" target="#fig_5">4b</ref> shows the mean difference between the ROC curves of classifier based on a single holistic feature and its hierarchical decomposition (averaged over 50 runs).</p><p>The results show that hierarchical features are significantly more informative and lead to much better classification results compared with holistic features. Significant improvement is obtained already with a single additional level.</p><p>We found in comparisons that optimizing the size and locations of the sub-fragments relative to their parent fragments add significantly to the MI compared with a hierarchy that uses fixed (and optimized) sizes for the sub-fragments and spacing between them. If the subfragments' centers were arranged on a uniform grid, rather than selecting their optimal locations during training, the MI decreases (average 43% s.d. = 35% p &lt; 10 -10 paired t-test), and the detection performance of the units decreases. The fixed spacing was set to the average spacing obtained at each level by the adaptive scheme. Optimizing ROI size also adds significantly to the MI compared with a fixed ROI size that was optimized for each level separately (average 8.1% s.d. 13.7% p &lt; 0.0055).</p><p>The performances of classifiers based on multiple holistic features and multiple hierarchies were also compared. The comparison was performed on 3 object classes: airplanes (same as above), horses (160 horses and 500 non-horses in the training set, 160 horses and 2500 non-horses in the test set) and side views of cars (160 cars and 500 non-cars in the training set, 160 cars and 2500 non-cars in the test set). First, to determine the number of fragments required for full classifier, the Equal Error Probabilities (EEP) were computed for classifiers based on 1 to 50 fragments. The classifier performance asymptoted at 30-40 fragments (Fig. <ref type="figure" target="#fig_5">4c</ref>). Next, the performances of full classifiers using 50 holistic features and 50 hierarchical features were compared (Fig. <ref type="figure" target="#fig_5">4d</ref>). The comparison clearly shows the advantage of hierarchical features.</p><p>The informative sub-fragments selected by our method were also compared with an alternative method in which informative sub-features are obtained by directly maximizing the mutual information using gradient ascent with simulated annealing. For this experiment, a set of 20 informative parent fragments of size 40x40 pixels each was computed from two object classes (faces, horses). For each fragment, a set of positive and negative examples was determined as described in Section 3.2. Two subfeatures were compared: the most informative subfragment of size 20x20 pixels, computed using the method described in Section 3.2 and the sub-feature of the same size computed using gradient ascent with simulated annealing. In this case, the sub-fragment starts either from a uniform or a random grey-level image. These grey levels are then modified by a gradient ascent computation that used MI as the optimization measure. For each parent fragment, the computation was performed 10 times, the image database each time split randomly into training and test set. The comparison shows that the image subfragments are significantly more informative than features learned by the gradient ascent procedure (average MI increase 36% s.d 31%). Selecting sub-features from the training images thus leads to better features than synthesizing new ones by gradient ascent. The likely reason is that the search in the space of all possible subfeatures has multiple local maxima which are significantly lower than the optimal sub-features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We presented a scheme for extracting feature hierarchies for classification. The top-level features are informative image fragments, which are then broken down successively into informative sub-fragments. The extraction is automatic, including the selection of the subfeatures as well as their combination weights and ROIs. The hierarchy outperforms the single-level features by a wide margin, both in the amount of delivered information and recognition performance. The amount of positional tolerance of sub-features and their positions should be learned from examples, since using sub-parts of uniform size and spacing degrades performance.</p><p>Classification using the feature hierarchy was implemented in the current work using a simple feedforward combination scheme, with weights extracted during learning. We also tried a combination scheme using a Bayesian network structure with bi-directional computation. An advantage of the modified scheme, which will be discussed in more detail in future work, is that it uses the entire hierarchy to recognize not only complete objects, but also object parts. In this manner, feature hierarchies can be used to improve the performance of recognition and classification schemes, and also to extend them to provide a fuller description of the objects together with their parts and sub-parts at different levels.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Examples of the hierarchies obtained by the algorithm.</figDesc><graphic coords="1,329.52,476.40,209.64,99.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The positive and negative examples for fragment detection. The original fragment f is shown on top. Row 1: class image examples where this fragment was detected (left) or almost detected (right). Row 2: the positive examples. Row 3: Nonclass images where the fragment was detected (left) or almost detected (right). Row 4: Negative examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plot of MI of a sub-fragment as a function of its ROI size, the maximum is selected as ROI.</figDesc><graphic coords="4,329.28,382.44,210.12,155.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>normalizes s p to the range [-1,1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7 .</head><label>7</label><figDesc>Optimize the ROI sizes of the hierarchy nodes, as in Section 3.3. The classification stage using the hierarchy of features H can be summarized as follows: INPUT: A novel image I. A feature hierarchy H, extracted from examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>a. Example of ROC curves obtained by a holistic feature and the same feature represented hierarchically (see text). b. Average ROC difference (additional hits) and s.d. of one holistic feature vs. the same feature represented hierarchically for one class (cows, 50 runs). c. Equal Error Probability of classifiers based on hierarchical (red) and holistic (blue) features. d. Example of ROC curve of full classifier based on hierarchical (red) and holistic (blue) features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of class images and computed hierarchies from 5 classes.</figDesc><graphic coords="8,63.00,380.16,488.16,291.72" type="vector_box" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by IMOS Grant 3-992 and conducted at the Moross Laboratory for Vision and Motor Control.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Combining class-specific fragments for object classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th British Machine Vision Conference</title>
		<meeting>10th British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to detect objects in images via a sparse, part-based representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Awan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee Tpami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1475" to="1490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Object Class Recognition by Unsupervised Scale-Invariant Learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. Comp. Vis. Pattern Recog</title>
		<meeting>of the IEEE Conf. Comp. Vis. Pattern Recog</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="264" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Categorization by learning and combining object parts</title>
		<author>
			<persName><forename type="first">B</forename><surname>Heisele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Interleaved object categorization and Segmentation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of British Machine Vision Conference (BMVC&apos;03)</title>
		<meeting>British Machine Vision Conference (BMVC&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant point detectors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="100" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cyber</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Comp. 1 (4)</title>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="page" from="541" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visual features of intermediate complexity and their use in classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vidal-Naquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast Binary Feature Selection with Conditional Mutual Information</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1531" to="1555" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Elements of information theory</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<pubPlace>Wiley, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scalable Discriminant Feature Selection for Image Retrieval and Recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Signal Detection Theory and Psychophysics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Swets</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<pubPlace>Wiley, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A model of saliencybased visual attention for rapid scene analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ch</forename><surname>Kosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
