<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-03">3 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vaidehi</forename><surname>Patil</surname></persName>
							<email>vaidehipatil@ee.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
							<email>partha@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
							<email>sunita@cse.iitb.ac.in</email>
							<affiliation key="aff0">
								<orgName type="institution">Indian Institute of Technology Bombay</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-03">3 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.01976v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained multilingual language models such as mBERT and XLM-R have demonstrated great potential for zero-shot cross-lingual transfer to low web-resource languages (LRL). However, due to limited model capacity, the large difference in the sizes of available monolingual corpora between high web-resource languages (HRL) and LRLs does not provide enough scope of co-embedding the LRL with the HRL, thereby affecting downstream task performance of LRLs. In this paper, we argue that relatedness among languages in a language family along the dimension of lexical overlap may be leveraged to overcome some of the corpora limitations of LRLs. We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE vocabulary generation algorithm which enhances overlap across related languages. Through extensive experiments on multiple NLP tasks and datasets, we observe that OBPE generates a vocabulary that increases the representation of LRLs via tokens shared with HRLs. This results in improved zero-shot transfer from related HRLs to LRLs without reducing HRL representation and accuracy. Unlike previous studies that dismissed the importance of token-overlap, we show that in the low-resource related language setting, token overlap matters. Synthetically reducing the overlap to zero can cause as much as a four-fold drop in zero-shot transfer accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zero-shot cross-lingual transfer is the ability of a model to learn from labeled data in one language and transfer the learning to another language without any labeled data. <ref type="bibr">Transformer (Vaswani et al., 2017)</ref> based multilingual models pre-trained on unlabeled data from multiple languages are the stateof-the-art means for cross-lingual transfer <ref type="bibr" target="#b29">(Ruder et al., 2019;</ref><ref type="bibr">Devlin et al., 2019a)</ref> promise for low web-resource languages (LRLs), such techniques are found to be more effective for transfer within high web-resource languages (HRLs) <ref type="bibr">(Wu and Dredze, 2020)</ref>. Vocabulary generation is an important step in multilingual model training, where vocabulary size directly impacts model capacity. Usually, the vocabulary is generated from a union of HRL and LRL data. This often results in under-allocation of vocabulary bandwidth to LRLs, as LRL data is significantly smaller in size compared to HRL. This under-allocation of model capacity results in lower LRL performance <ref type="bibr">(Wu and Dredze, 2020)</ref>, as mentioned previously. In response, prior research has explored development of region-specific models (Antoun et al.; <ref type="bibr" target="#b16">Khanuja et al., 2021)</ref>, generating vocabulary specific to language clusters <ref type="bibr" target="#b5">(Chung et al., 2020)</ref>, and exploring relatedness among languages to build better LMs for LRLs <ref type="bibr" target="#b17">(Khemchandani et al., 2021)</ref>. However, none of these methods have utilized relatedness among languages for better vocabulary generation during multilingual pre-training.</p><p>In this paper, we hypothesize that exploiting language relatedness can result in an overall more effective vocabulary, which is also better representative of LRLs. Closely related languages (e.g., languages belonging to a single family) have common origins for words with similar meanings. We show some examples across three different families of related languages in Table <ref type="table">8</ref>. Morphological inflections of the root word leads to lexically overlapping tokens across languages. Learning representations for such subwords in lexically overlapping words shared across HRL and its related LRLs can enable better transfer of supervision from HRL to LRLs. During Masked Language Modelling (MLM) pretraining <ref type="bibr">(Devlin et al., 2019a)</ref>, the shared tokens can serve as anchors in learning contextual representations of neighboring tokens. However, choosing the correct granularity of sharing automatically is tricky. On one extreme, we can choose a vocabulary which favours longer units frequent in HRL without regard for sharing, thereby leading to better semantic representation of the tokens but no cross-lingual transfer. On the other extreme, we can choose character-level vocabulary <ref type="bibr" target="#b21">(Ma et al., 2020)</ref>, where every token is shared across languages but have no semantic significance.</p><p>Given text from a mix of high and low Webresource languages (HRL and LRL, respectively), Byte Pair Encoding (BPE) <ref type="bibr">(Sennrich et al., 2016)</ref> and its variants like Wordpiece <ref type="bibr">(Schuster and Nakajima, 2012)</ref> and Sentencepiece <ref type="bibr" target="#b19">(Kudo and Richardson, 2018)</ref> prefer frequent tokens, most of those from the HRLs. This would cause most long HRL tokens to get included, leaving only a limited budget of short tokens for the LRL. Any sub-token level overlap between HRL and LRL could get lost in this process. In a zero-shot setting, since available supervision is HRL based, this creates a bottleneck when transferring supervision from HRL to LRLs. Over-sampling LRLs is a common strategy to offset this imbalance but that hurts HRL performance as shown in <ref type="bibr">(Conneau et al., 2020a)</ref>.</p><p>In this paper, we propose Overlap BPE (OBPE). OBPE chooses a vocabulary by giving token overlap among HRL and LRLs a primary consideration. OBPE prefers vocabulary units which are shared across multiple languages, while also encoding the input corpora compactly. Thus, OBPE tries to balance the trade-off between cross-lingual subword sharing and the need for robust representation of individual languages in the vocabulary. This results in a more balanced vocabulary, resulting in improved performance for LRLs without hurting HRL accuracy. Table <ref type="table" target="#tab_0">1</ref> shows an example to highlight this difference between OBPE and BPE.</p><p>Recently <ref type="bibr">K et al. (2020)</ref>; <ref type="bibr">Conneau et al. (2020b)</ref> concluded that token overlap is unimportant for cross-lingual transfer. However, they studied language pairs where either both languages had a large corpus, or where the languages were not sufficiently related. We focus on related languages within a family and observe drastic drop in zeroshot accuracy when we synthetically reduce the overlap to zero (58% F1 drops to 17% for NER, 72% drops to 30% for text classification).</p><p>This paper offers the following contributions We analyse the reasons behind the gains obtained by OBPE and show that OBPE increases the percentage of LRL tokens in the vocabulary without reducing HRL tokens. This is unlike over-sampling strategies where increasing one reduces the other. ? Through controlled experiments on the amount of token overlap on a related HRL-LRL pair, we show that token overlap is extremely important in the low-resource, related language setting. Recent literature which conclude that token overlap is unimportant may have overlooked this important setting. We plan to make the source code and all resources generated in this paper publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformer-based multilingual language models such as mBERT <ref type="bibr">(Devlin et al., 2019b)</ref> and XLM-R <ref type="bibr">(Conneau et al., 2020a)</ref> are now established as the de-facto method for zero-shot cross-lingual transferability, and thus hold promise for low resource domains. However, recent studies have indicated that even the current state-of-the-art models such as XLM-R (Large) do not yield reasonable transfer performance across low resource target languages with limited data <ref type="bibr">(Wu and Dredze, 2020)</ref>. This has led to a surge of interest in enhancing cross-lingual transfer of multilingual models to the low-resource setting. We categorize existing work based on the stage of the pre-training pipeline where it is relevant:</p><p>Input Data In the data creation stage, <ref type="bibr">Conneau et al. (2020a)</ref> propose over-sampling of LRL documents to improve LRL representation in the vocabulary and pre-training steps. <ref type="bibr" target="#b17">Khemchandani et al. (2021)</ref> specifically target related languages and propose transliteration of LRL documents to the script of related HRL for greater lexical overlap. We deploy both these tricks in this paper.</p><p>Tokenization <ref type="bibr" target="#b30">Rust et al. (2021)</ref> study that even the tokenization step could have a crucial impact on performance accrued to each language in a multilingual models. They propose the use of dedicated tokenizer for each language instead of the automatically generated multilingual mBERT tokenizer. However, they continue to use the default mBERT vocabulary generator.</p><p>Vocabulary Generation Sennrich et al. ( <ref type="formula">2016</ref>) highlighted the importance of subword tokens in the vocabulary and proposed use of the BPE algorithm <ref type="bibr" target="#b11">(Gage, 1994)</ref> for efficiently growing such a vocabulary incrementally. Variants like Wordpiece (Schuster and Nakajima, 2012) and Sentencepiece <ref type="bibr" target="#b19">(Kudo and Richardson, 2018)</ref> either build on top of BPE or follow a very similar process. <ref type="bibr" target="#b18">Kudo (2018)</ref> is a variant method that chooses tokens based on unigram LM score. We obtained better results with BPE and continued with that. All these BPE variants incrementally add subwords based on overall frequency in the combined corpus, and they all ignore language boundaries. Chung et al. ( <ref type="formula">2020</ref>) observed that such a combined approach could under-represent several languages, and proposed instead to separately create vocabularies for clusters of related languages and take a union of each cluster-specific vocabulary. However, within each cluster they continue to use the default vocabulary generator. Our approach can be used as a drop-in replacement to further enhance the quality of the cluster-specific vocabulary that they obtain. <ref type="bibr">Wang et al. (2019)</ref>; <ref type="bibr" target="#b12">Gao et al. (2020)</ref> propose a soft-decoupled encoding approach for exploiting subword overlap between LRLs and HRLs. However, their focus is NMT models and does not easily integrate in existing Multilingual models such as mBERT. <ref type="bibr" target="#b22">(Maronikolakis et al., 2021)</ref> targets tokenization compatibility based purely on vocabulary size and does not focus on choosing the tokens that go in the vocabulary. Pre-Training and Adaptation Several previous works have proposed to include additional alignment loss between parallel <ref type="bibr" target="#b4">(Cao et al., 2020)</ref> or pseudo-parallel <ref type="bibr" target="#b17">(Khemchandani et al., 2021)</ref> sentences to co-embed HRLs and LRLs. Another approach is to design language-specific Adapter layers <ref type="bibr">(Pfeiffer et al., 2020a,b;</ref><ref type="bibr" target="#b1">Artetxe et al., 2020;</ref><ref type="bibr">?st?n et al., 2020)</ref> that can be easily fine-tuned for each new language. <ref type="bibr" target="#b26">Pfeiffer et al. (2021)</ref> leverages the pre-trained embeddings of lexically overlapping tokens between the vocabulary of pre-trained model and that of unseen target language to initialize the corresponding embeddings of target language. However, they did not attempt to increase the fraction of such tokens in the vocabulary.</p><p>We are not aware of any prior work that explicitly promotes overlapping tokens between LRLs and HRLs in the vocabulary of multilingual models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Overlap-based Vocabulary Generation</head><p>We are given monolingual data D 1 , ..., D n in a set of l languages L = {L 1 , ..., L n } and a vocabulary budget V. Our goal is to generate a vocabulary V that when used to tokenize each D i in a multilingual model would provide cross-lingual transfer to LRLs from related HRLs. We use L LRL to denote the subset of the l languages that are low-resource, the remaining languages L -L LRL are denoted as the set L HRL of high resource languages.</p><p>Existing methods of vocabulary creation start with a union D of monolingual data D 1 , ..., D n , and choose a vocabulary V that most compactly represents D. We first present an overview of BPE, a popular algorithm for vocabulary generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background: BPE</head><p>Byte Pair Encoding (BPE) <ref type="bibr" target="#b11">(Gage, 1994)</ref> is a simple data compression technique that chooses a vocabulary V that minimizes total size of D = ? i D i when encoded using V.</p><formula xml:id="formula_0">V = argmin S:|S|=V n i=1 |encode(D i , S)| (1)</formula><p>The size of the encoding |encode(D i , S)| can be alternately expressed as the sum of frequency of tokens in S when D i is tokenized using S. This motivates the following efficient greedy algorithm to implement the above optimization <ref type="bibr">(Sennrich et al., 2016)</ref>. Let f ki denote the frequency of a candidate token k in the corpus D i of language L i . The BPE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Overlap based BPE (OBPE)</head><p>for i ? {1, 2, ..., n} do Split words in Di into characters Ci with a special marker after every word end for</p><formula xml:id="formula_1">V = ? n i=1 Ci while |V| &lt; V do</formula><p>Update token and pair frequency on {Di}, V Add to V token k formed by merging pairs u, v ? V with the largest value of</p><formula xml:id="formula_2">(1 -?) j f kj + ? i?L LRL max h?L HRL f p ki + f p kh 2 1 p</formula><p>end while algorithm grows V incrementally. Initially, V comprises of characters in D. Then, until |V| ? V, it chooses the token k obtained by merging two existing tokens in V for which the frequency in D is maximum.</p><formula xml:id="formula_3">V = V ? argmax k=[u,v]:u,v?V i f ki<label>(2)</label></formula><p>A limitation of BPE on multilingual data is that tokens that appear largely in low-resource D i may not get added to V, leading to sentences in L i being over-tokenized. For a low resource language, the available monolingual data D i is often orders of magnitude smaller than another high-resource language. Models like mBERT and XLM-R address this limitation by over-sampling documents of lowresource languages. However, over-sampling LRLs might compromise learned representation of HRLs where task-specific labeled data is available. We propose an alternative strategy of vocabulary generation called OBPE that seeks to maximize transfer from HRL to LRL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Our Proposal: OBPE</head><p>The key idea in OBPE is to maximize the overlap between an LRL and a closely related HRL while simultaneously encoding the input corpora compactly as in BPE. When labeled data D T h for a task T is available in an HRL L h , then a multilingual model fine-tuned with D T h is likely to transfer better to a related LRL L i when L i and L h share several tokens in common. Thus, the objective that OBPE seeks to optimize when creating a vocabulary is:</p><formula xml:id="formula_4">V = argmin S:|S|=V (1 -?) n i=1 |encode(D i , S)| -? i?L LRL max j?L HRL overlap(L i , L j , S) ? ? (3)</formula><p>where 0 ? ? ? 1 determines importance of the two terms. The first term in the objective compactly represents the total corpus, as in BPE's (Eq (1)). The second term additionally biases towards vocabulary with greater overlap of each LRL to one HRL where we expect task-specific labeled data to be present. There are several ways in which we can measure the overlap between two languages with respect to a current vocabulary. First, we encode each of D i and D j using the vocabulary S, which then yields a multiset of tokens in each corpus. Inspired by the literature on fair allocation <ref type="bibr" target="#b2">(Barman et al., 2021)</ref>, we explore a continuously parameterized function that expresses overlap between two languages' encoding as a generalized mean function as follows:</p><formula xml:id="formula_5">overlap(L i , L h , S) = k?S f p ki + f p kh 2 1 p , p ? 1</formula><p>(4) where f ki denotes the frequency of token k when D i is encoded with S. For different values of p, we get different tradeoffs between fairness to each language and overall goodness. When p = -?, generalized mean reduces to the minimum function, and we get the most egalitarian allocation. However, this ignores the larger of the two frequencies. When p = 1, we get a simple average which is what the first term in Equation (3) already covers. For p = 0, -1, we get the geometric and harmonic means respectively. Due to smaller size of LRL monolingual data, the frequency of a token which is shared across languages is likely to be much higher in HRL monolingual data as compared to that in LRL monolingual data, Hence, setting p to large negative values will increase the weight given to LRLs and thus increase overlap. We will present an exploration of the effect of p on zero-shot transfer in the experiment section.</p><p>The greedy version of the above objective that controls the candidate vocabulary item to be inducted in each iteration of OBPE is thus:</p><formula xml:id="formula_6">V = V ? argmax k=[u,v]:u,v?V (1 -?) j f kj +? i?L LRL max h?L HRL f p ki + f p kh 2 1 p (5)</formula><p>The data structure maintained by BPE to efficiently conduct such merges can be applied with little changes to the OBPE algorithm.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate by measuring the efficacy of zeroshot transfer from the HRL on four different tasks: named entity recognition (NER), part of speech tagging (POS), text classification(TC), and Cross-lingual Natural Language Inference (XNLI).</p><p>Through our experiments, we evaluate the following questions:</p><p>1. Is OBPE more effective than BPE for zeroshot transfer? (Section 4.2) 2. What is the effect of token overlap on overall accuracy? (Section 4.3) 3. How does increased LRL representation in the vocabulary impact accuracy? (Section 4.4) We report additional ablation and analysis experiments in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Pre-training Data and Languages As our pretraining dataset {D i }, we use the Wikipedia dumps of all the languages as used in mBERT. We pretrain with 12 languages grouped into three families of four related languages as shown in Table <ref type="table" target="#tab_3">2</ref>. In each family, we simulate as HRL the most populous language, and call the remaining as LRLs. The number of documents for languages simulated as LRLs is set to 20K. For the HRLs, we consider two corpus distributions:</p><p>? BALANCED : all three HRLs get 160K documents each ? SKEWED : English gets one million, French half million, and Hindi 160K documents We evaluate twelve-language models in each of these settings, and present results for separate four language models per family in Table <ref type="table" target="#tab_12">13</ref> in the Appendix. For the Indo-Aryan languages set, the monolingual data of Punjabi and Gujarati is transliterated to Devanagari, the script of Hindi and Marathi. We use libindic's indictrans library <ref type="bibr">(Bhat et al., 2015)</ref> for transliteration. Languages in the other two sets do not require transliteration as they have a common script. Thus, all four languages in each set are in the same script so their lexical overlap can be leveraged. Pre-Training Details To ensure that LRLs are not under-represented, we over-sample using exponentially smoothed weighting similar to multilingual BERT <ref type="bibr">(Devlin et al., 2019b)</ref> with exponentiation factor 0.7. We perform MLM pretraining on a BERT base model with 110M parameters from scratch. We generate a vocabulary of size of 30k. We chose batch size as 2048, learning rate as 3e-5 and maximum sequence length as 128. Pre-training of BERT was done with duplication factor 5 for for 64k iterations for HRLs. For all LRLs, duplication factor was 20 and training was done for 24K iterations. MLM pre-training was done on Google v3-8 Cloud TPUs where 10K iterations required 2.1 TPU hours. Task-specific Data We evaluate on four downstream tasks: (1) NER: data from WikiANN <ref type="bibr" target="#b23">(Pan et al., 2017)</ref> and XTREME <ref type="bibr" target="#b13">(Hu et al., 2020)</ref>, (2) XNLI: data from <ref type="bibr" target="#b7">(Conneau et al., 2018)</ref>, (3) POS: data from XTREME <ref type="bibr" target="#b13">(Hu et al., 2020)</ref> and TDIL<ref type="foot" target="#foot_0">1</ref> , and (4) Text Classification (TC): data from TDIL and XGLUE <ref type="bibr" target="#b20">(Liang et al., 2020)</ref>. We downsampled the TDIL data for each language to make them class-balanced. The POS tagset for Indo-Aryan languages used was the BIS Tagset <ref type="bibr" target="#b32">(Sardesai et al., 2012)</ref>. Table <ref type="table">9</ref> presents a summary. The test set to compute LRL perplexity os formed by sampling 10K sentences from Samanantar corpus <ref type="bibr" target="#b28">(Ramesh et al., 2021)</ref> for Indic languages and from Tatoeba corpus<ref type="foot" target="#foot_1">2</ref> for other languages. Task-specific fine-tuning details We perform taskspecific fine-tuning of pre-trained BERT on the task-specific training data of HRL and evaluate on all languages in the same family. The results reported for all the experiments are an average of 3 independent runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Effectiveness of OBPE</head><p>We evaluate the impact of OBPE on improving zero-shot transfer from HRLs to LRLs within the same family across four different tasks. We compare with four existing methods that represent different methods of vocabulary creation and allocation of budget across languages: Methods compared 1. <ref type="bibr">BPE (Sennrich et al., 2016)</ref>, the existing default method of vocabulary generation. 2. Clustered vocabulary (CV) <ref type="bibr" target="#b5">(Chung et al., 2020)</ref> Since the paper uses a SentencePiece unigram for vocabulary, we followed the same approach for this comparison. We allocate each family equal number of vocabulary tokens which is V/3. 3. BPE-dropout (BPE-dp) <ref type="bibr" target="#b27">(Provilkov et al., 2020)</ref> uses the vocabulary generated by BPE but tokenizes the text using a dropout rate of 0.1. This allows the training of tokens that are subsumed by larger tokens in the vocabulary. 4. Compatibility of Tokenizations (Tok-Comp) <ref type="bibr" target="#b22">(Maronikolakis et al., 2021)</ref> uses a method to select meaningful vocabulary sizes in an automated manner for all language using compression rates. Since their best performances are found, when the compression rates are similar, we choose a size for each language corresponding to compression rate of 0.5. The tokenizer used in this method is WordPiece. . 5. OBPE (Ours) with default ? = 0.5, p = -?. We also do ablation on these. In Table <ref type="table" target="#tab_4">3</ref> we observe that across all four tasks, zero-shot LRL accuracy improves compared to BPE. For example, the average accuracy on XNLI for the LRL languages improves from 55.6 to 58.1 just by changing the set of tokens in the vocabulary. These gains are obtained without compromising HRL performance on the tasks. The Clustered Vocabulary (CV) approach is much worse than BPE. These experiments are on the Balanced-12 model. In the supplementary section, we report the results on the Skewed-12 (Table <ref type="table" target="#tab_9">10</ref>) and Balanced-4 models (Table <ref type="table" target="#tab_12">13</ref>) and show similar gains even with these models. In this table, we averaged the gains over nine LRLs, and in the Supplementary Table <ref type="table" target="#tab_10">11</ref> we show consistent gains for individual languages.</p><p>In addition to improving zero-shot transfer from HRLs to LRLs on downstream tasks, OBPE also leads to better intrinsic representation of LRLs. We validate that by measuring the pseudoperplexity <ref type="bibr" target="#b31">(Salazar et al., 2020)</ref> of a test set of LRL sentences. We find that average perplexity of LRL sentences drops by 2.6% when we go from the BPE to OBPE vocabulary. More details on this experiment appear in Figure <ref type="figure">3</ref> of supplementary.</p><p>In order to investigate the reasons behind the OBPE gains, we first inspected the percentage of tokens in the vocabulary that belong to LRLs, HRLs, and in their overlap. We find that with OBPE both Figure <ref type="figure">1</ref>: Zero-shot performance vs Overlap of models trained on unicode shifted HRL data to simulate increasing overlap between HRL (SynthHindi) and LRL (mr). Performance is measured on three tasks: Text Classification (Accuracy), NER (F1) and POS (Accuracy). On TC and NER observe the huge drop in LRL accuracy as we decrease overlap from 100 down to 0. Further discussions in Section 4.3.</p><p>LRL tokens and overlapping tokens increase. Either of these could have led to the observed gains.</p><p>We analyze the effect of each of these factors in the following two sections. We present the impact of token overlap via two sets of experiments: first, a controlled setup where we synthetically vary the fraction of overlap and second where we measure correlation between overlap and gains of OBPE on the data as-is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Effect of Token Overlap</head><p>For the controlled setup we follow (K et al., 2020) for synthetically controlling the amount of overlap between HRL and LRL. We trained a bilingual model between Hindi (HRL 160K) and Marathi (LRL 20K) -two closely related languages in the Indo-Aryan family. To find the set of overlapping tokens between Hindi and Marathi, we first run OBPE on Hindi-Marathi language pair to generate a vocabulary and label all tokens present in both languages as overlapping tokens. We then incrementally sample 10%, 40%, 50%, 90% of the tokens from this set. We shift the Unicode of the entire Hindi monolingual data except the set of sam-pled tokens so that there are no overlapping tokens between Hindi (hi) and Marathi (mr) monolingual data other than the sampled tokens. Let us call this Hindi data SynthHindi. We then run OBPE on SynthHindi-Marathi language pair to generate a vocabulary to pretrain the model. The task-specific Hindi data is also converted to SynthHindi during fine-tuning and testing of the model.</p><p>Figure <ref type="figure">1</ref> shows results with increasing overlap. We observe increasing gains in LRL accuracy as we go from no overlap to full overlap on all three tasks. NER accuracy increases from 17% to 58% for the LRL (mr) even while the HRL (hi) accuracy stays unchanged. For TC we observe similar gains. For POS, even without token overlap, we get good cross-lingual transfer because POS tags are more driven by structural similarity, and Hindi and Marathi follow similar structure.</p><p>Our results contradict the conclusions of <ref type="bibr">(K et al., 2020)</ref> which claimed that token overlap is unimportant for cross-lingual transfer. However, there are two key differences with our setting: (1) unlike <ref type="bibr">(K et al., 2020)</ref>, we conduct explore low-resource settings, and (2) except for English-Spanish, the other language pairs they considered are not linguistically related. To explain the importance of both these factors in Table <ref type="table">4</ref> we present accuracy of English-Spanish in a simulated low-resource setting where we sample 20K Spanish documents and 160K English documents. Also, we repeat our Hindi-Marathi experiments where Marathi is not low-resource. We observe that (1) Spanish as LRL benefits significantly on overlap with English. (2) Marathi gains from token overlap with Hindi even in the high resource setting.</p><p>Thus, we conclude that as long as language are related token overlap is important and the benefit from overlap is higher in the low resource setting.   Overlap Vs Gain: Real data setup We further substantiate our hypothesis that the shared tokens across languages favoured by OBPE enable transfer of supervision from HRL to LRL via statistics on real-data. In Table <ref type="table" target="#tab_8">7</ref> we show the Pearson product-moment correlation coefficient between overlap gain and performance gain within LRLs of the same family and task. We get a high positive correlation coefficient, with an average of 0.644.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effect of Increased LRL representation</head><p>We next investigate the impact of increased representation of LRL tokens in the vocabulary. OBPE increases LRL representation by favoring overlapping tokens, but LRL tokens can also be increased by just over-sampling LRL documents. We train another BALANCED12 model but with further oversampling LRLs with S = 0.5 instead of S = 0.7. We observe in Figure <ref type="figure">6</ref> that this increases LRL fraction but reduces HRL tokens in the vocabulary. Table <ref type="table" target="#tab_6">5</ref> also shows the comparison of zero-shot transfer accuracy with over-sampled BPE against over-sampled OBPE. We find that OBPE even with default S achieves highest LRL gains, whereas aggressively over-sampled BPE hurts HRL accuracy. Within the same sampling setting, OBPE is better than corresponding BPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation study</head><p>We conducted experiments for different values of p that controls the amount of overlap in the generalized mean function (Equation ( <ref type="formula">5</ref>)). Setting p = 1 gives the original BPE algorithm. Setting p = 0, -1 gives geometric and harmonic mean respectively, setting p = -? gives minimum. We compare the task-specific results for different values of p as shown in Table <ref type="table" target="#tab_15">16</ref> and find that the gains we obtain are highest in the p = -? (minimum) setting (Figure <ref type="figure">4</ref> in Appendix). We also experiment with ? = 0.7, and find that for most languages the results were not better than our default ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we address the problem of crosslingual transfer from HRLs to LRLs by exploiting relatedness among them. We focus on lexical overlap during the vocabulary generation stage of multilingual pre-training. We propose Overlap BPE (OBPE), a simple yet effective modification to the BPE algorithm, which chooses a vocabulary that maximizes overlap across languages. OBPE encodes input corpora compactly while also balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. We focus on three sets of closely related languages from diverse language families. Our experiments provide evidence that OBPE is effective in leveraging overlap across related languages to improve LRL performance. In contrast to prior work, through controlled experiments on the amount of token overlap between two related HRL-LRL language pairs, we establish that token overlap is important when a LRL is paired with a related HRL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Examples of Token Overlap within Language Families</head><p>Table <ref type="table">8</ref> shows examples of overlapping tokens within three different language families, and Figure <ref type="figure" target="#fig_3">2</ref> shows a real example of how OBPE chooses shared tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Limitations</head><p>? Our approach is expected to improve crosslingual transfer from HRL to LRL only when the HRL and LRL are related linguistically since it relies on the presence of lexically overlapping tokens ? It requires the transliteration of LRL data to the script of its related HRL if LRL does not have the same script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Potential risks</head><p>Language models may amplify bias in data and also introduce new ones. Multilingual models explored Punjabi as it is a LRL and will thus tokenize Niyukata into multiple tokens which do not captures its meaning whereas Niyukata when tokenized by OBPE tokenizer will contain Niyuk which captures most of the meaning of the token Niyukata whose representation will be learnt when pretraining using Punjabi monolingual data in the paper are not immune to such issues. Detecting such biases and mitigating them is a topic of ongoing research. We are hopeful that our focus on better representation of LRLs in the vocabulary is a step towards more inclusive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Replicability</head><p>BERT configuration parameters used in our experiments are as follows:</p><p>"attention_probs_dropout_prob": 0.1, "hid-den_act":</p><p>"gelu", "hidden_dropout_prob": 0.1, "hidden_size": 768, "initial-izer_range": 0.02, "intermediate_size": 3072, "max_position_embeddings": 512, "num_attention_heads": 12, "num_hidden_layers": 12, "type_vocab_size": 2, "vocab_size": 30000 All the task-specific fine-tuning experiments are done using GPUs on Google Colaboratory where each fine-tuning experiment requires 2 GPU hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 License</head><p>Tatoeba data, GLUE data, Wikipedia dumps use the Creative Commons licenses. TDIL data used Table <ref type="table">8</ref>: Lexically overlapping tokens with similar meanings across four languages in each of three families. OBPE, our proposed method, exploits such meaning-preserving overlap among related languages to induce vocabulary for multilingual learning. for Indic languages uses Research license type and Xtreme dataset uses Apache License 2.0. To the best of our knowledge, the use of scientific artifacts in this work is consistent with their intended use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Data bias</head><p>We have used standard Wikipedia corpus, and there have been some studies on bias in such corpus. <ref type="bibr" target="#b14">(Hube, 2017)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Mike</head><label></label><figDesc>Schuster and Kaisuke Nakajima. 2012. Japanese and korean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5149-5152. IEEE. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany. Association for Computational Linguistics. Ahmet ?st?n, Arianna Bisazza, Gosse Bouma, and Gertjan van Noord. 2020. UDapter: Language adaptation for truly Universal Dependency parsing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2302-2315, Online. Association for Computational Linguistics. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ? ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. Xinyi Wang, Hieu Pham, Philip Arthur, and Graham Neubig. 2019. Multilingual neural machine translation with soft decoupled encoding. In International Conference on Learning Representations. Shijie Wu and Mark Dredze. 2020. Are all languages created equal in multilingual BERT? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120-130, Online. Association for Computational Linguistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Similar meaning words with shared root forms across related Indo-Aryan languages. BPE vocabulary does not capture the tokens corresponding to Punjabi as it is a LRL and will thus tokenize Niyukata into multiple tokens which do not captures its meaning whereas Niyukata when tokenized by OBPE tokenizer will contain Niyuk which captures most of the meaning of the token Niyukata whose representation will be learnt when pretraining using Punjabi monolingual data</figDesc><graphic url="image-1.png" coords="11,306.14,70.87,229.68,227.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Task-specific data sizes. Number of sentences in thousands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>. While pretraining based cross-lingual transfer holds great First row shows lexically overlapping tokens in four different languages with their corpus frequen-</figDesc><table><row><cell cols="2">Language</cell><cell>English: University (10), versity (6);</cell></row><row><cell>and</cell><cell>Token</cell><cell>German: Universitaten (2); Dutch: Uni-</cell></row><row><cell cols="2">frequencies</cell><cell>versiteit (1); Western Frisian: Univer-</cell></row><row><cell></cell><cell></cell><cell>siteiten (1)</cell></row><row><cell cols="2">Starting Vocab</cell><cell>Uni, versit, U,n,i,v,e,r,s,i,t,y,a</cell></row><row><cell cols="2">BPE Vocab</cell><cell>versity, Uni, versit, U,n,i,v,e,r,s,i,t,y,a</cell></row><row><cell cols="2">OBPE Vocab</cell><cell>Universit, Uni, versit, U,n,i,v,e,r,s,i,t,y,a</cell></row></table><note><p><p><p>cies (in brackets), with English (En) as the High Web-Resource Language (HRL). From a starting vocabulary shown in the second row, BPE merges tokens based on greater overall frequency, adding new vocabulary item versity as it has the highest overall frequency (</p>16</p>). OBPE instead adds Universit since it also rewards cross-lingual overlap, even though Universit has lower overall frequency (15).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Twelve Languages simulated as HRLs and LRLs across with two different corpus distribution: BALANCED and SKEWED. Number of documents in languages simulated as LRLs is 20K.</figDesc><table><row><cell>frequency in each language in addition to overall</cell></row><row><cell>frequency. Since the time and resources used to cre-</cell></row><row><cell>ate the vocabulary is significantly smaller than the</cell></row><row><cell>model pre-training time, this additional overhead</cell></row><row><cell>to the pre-training step is negligible.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Sennrich et al., 2016)  64.48 65.52 52.07 84.64 83.26 82.07 62.71 95.20  BPE-dp (Provilkov et al., 2020)   63.9264.15 52.66 84.75 81.73 81.07 63.74 94.61  CV (Chung et al., 2020)  59.58 61.91 49.30 81.68 81.15 80.93 64.51 94.47  TokComp (Maronikolakis et al., 2021) 63.79 65.77 53.94 85.49 82.43 80.93 66.10 94.86  OBPE (This paper)   65.72 68.02 54.03 85.26 83.98 81.91 66.27 95.09 Zero-shot performance of models in the Balanced-12 setting trained on 9 LRL and 3 HRL languages. Performance is measured on four tasks: NER (F1), Text Classification (Accuracy), POS (Accuracy), and XNLI (Accuracy). For all metrics, higher is better (?). Zero-shot transfer to LRL improves without hurting HRL accuracy. P-value of paired-t-test between BPE and OBPE LRL gains has values 0.01, 0.04, 0.02, 0.01 for each of the 4 tasks establishing statistical significance. Detailed results for each language is pesented in Table11of Supplementary. Section 4.2 has further discussion. learning-rate 2e-5 and batch size 32, with training duration as 16 epochs for NER, 8 epochs for POS and 3200 iterations for Text Classification and XNLI. The models were evaluated on a separate validation dataset of the HRL and the model with the minimum validation loss, maximum F1-score, accuracy and minimum validation loss was selected for final evaluation for XNLI, NER, POS and Text Classification respectively. All fine-tuning experiments were performed on Google Colaboratory.</figDesc><table><row><cell>Here we used</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Zero-shot performance of models in the same setting as Table3but comparing default sampling with oversampling (S=0.5). Note, even if BPE_overSamp improves LRL somewhat, it causes HRL to drop. OBPE with default sampling is best for both LRLs and HRLs. Also OBPE_overSampled is better than BPE_overSampled (Section 4.4).</figDesc><table><row><cell>Method</cell><cell>LRL Performance (?) NER TC XNLI POS NER TC XNLI POS HRL Performance (?)</cell></row><row><cell>BPE</cell><cell>64.5 65.5 52.1 84.6 83.3 82.1 62.7 95.2</cell></row><row><cell cols="2">+overSample 64.4 67.6 52.1 84.6 82.4 82.0 62.0 95.2</cell></row><row><cell>OBPE</cell><cell>65.7 68.0 54.0 85.3 84.0 81.9 66.3 95.1</cell></row><row><cell cols="2">+overSample 64.6 67.9 53.5 85.1 82.7 81.7 65.7 94.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Percentage rise over BPE in rep-</cell></row><row><cell>resentation of LRL, HRL and Shared (per-</cell></row><row><cell>centage of tokens shared between HRL and</cell></row><row><cell>LRL weighted by frequency) in vocabulary</cell></row><row><cell>generated by OBPE and BPE_overSample</cell></row><row><cell>and OBPE_overSample (Section 4.4).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Correlation coefficient between performance gain and overlap gain within languages in a family for various tasks. (Section 4.3).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Zero-shot performance of models in the Skewed-12 setting of Table2on same four tasks as Table3. OBPE shows gains here too. Detailed numbers in Table12of Supplementary. Section 4.2 has further discussion.</figDesc><table><row><cell cols="4">% decrease in PPL vs. LRL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>% decrease in PPL</cell><cell>2 4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>gu</cell><cell>pa</cell><cell>mr</cell><cell>es</cell><cell>pt</cell><cell>it</cell><cell>fy</cell><cell>nl</cell><cell>de</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LRL</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">Figure 3: Percentage decrease in Pseudo perplexity for</cell></row><row><cell cols="10">different LRLs as we go from BPE to OBPE vocabu-</cell></row><row><cell>lary.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>NER F1 score, Text Classification, POS, XNLI accuracy of combined model trained on all 12 languages</figDesc><table><row><cell cols="2">Lang hi</cell><cell>mr</cell><cell>pa</cell><cell>gu</cell><cell>en</cell><cell>de</cell><cell>nl</cell><cell>fy</cell><cell>fr</cell><cell>es</cell><cell>pt</cell><cell>it</cell><cell>LRL HRL</cell></row><row><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>avg avg</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NER</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">BPE 83.66 45.03 25.85 24.25 75.94 52.42 62.83 62.63 85.75 70.53 68.34 64.34 52.91 81.78</cell></row><row><cell>CV</cell><cell cols="13">83.83 47.67 32.69 33.43 72.35 46.89 55.13 57.88 83.34 71.78 66.45 62.61 52.73 79.84</cell></row><row><cell cols="14">OBPE 85.92 47.55 26.05 32.79 77.15 52.72 62.87 65.55 85.76 73.35 70.25 64.69 55.09 82.94</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">BPE 75.8 51.46 49.88 51.9 88.27 49.5</cell><cell></cell><cell></cell><cell cols="2">76.05 55.64</cell><cell></cell><cell></cell><cell>51.68 80.04</cell></row><row><cell>CV</cell><cell cols="6">76.46 54.37 55.49 56.33 81.94 51.5</cell><cell></cell><cell></cell><cell cols="2">74.81 54.31</cell><cell></cell><cell></cell><cell>54.40 77.74</cell></row><row><cell cols="7">OBPE 76.58 55.38 53.98 54.06 88.3 57.85</cell><cell></cell><cell></cell><cell cols="2">76.06 55.59</cell><cell></cell><cell></cell><cell>55.37 80.31</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">POS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">BPE 93.96 74.84 59.34 65.87 94.81 69.18 74.96</cell><cell></cell><cell cols="5">96.33 86.66 84.67 82.81 74.79 95.03</cell></row><row><cell>CV</cell><cell cols="7">93.67 77.68 71.28 75.81 94.1 67.68 72.75</cell><cell></cell><cell cols="5">96.04 84.33 82.44 81.65 76.70 94.60</cell></row><row><cell cols="8">OBPE 94.11 75.46 58.84 68.5 94.94 68.1 75.18</cell><cell></cell><cell cols="5">96.22 86.54 84.3 83.46 75.05 95.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.05 45.51</cell><cell></cell><cell></cell><cell cols="2">62.87 51.62</cell><cell></cell><cell></cell><cell>48.57 64.96</cell></row><row><cell>CV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">54.87 39.87</cell><cell></cell><cell></cell><cell cols="2">59.48 48.68</cell><cell></cell><cell></cell><cell>44.28 57.18</cell></row><row><cell>OBPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.71 47.33</cell><cell></cell><cell></cell><cell cols="2">63.43 52.69</cell><cell></cell><cell></cell><cell>50.01 65.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>NER F1 score, Text Classification, POS, XNLI accuracy of combined model trained on all 12 languages with varied sizes of HRL monolingual data : English : 1M docs, French: 0.5M docs, Hindi:0.16M docs, the monolignual data of all LRLs has 20k docs, duplication factor is 3 for both HRLs and LRLs</figDesc><table><row><cell>Lang</cell><cell>hi</cell><cell>mr</cell><cell>pa</cell><cell>gu</cell><cell>en</cell><cell>de</cell><cell>nl</cell><cell>fy</cell><cell>fr</cell><cell>es</cell><cell>pt</cell><cell>it</cell><cell>LRL HRL</cell></row><row><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>HRL</cell><cell></cell><cell></cell><cell></cell><cell>avg avg</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BPE</cell><cell cols="13">85.49 54.88 75.35 40.5 74.99 53.16 62.91 66.54 84.24 70.14 70.2 63.86 61.95 81.57</cell></row><row><cell cols="14">OBPE(? = 0.5) 86.59 59.23 76.15 41.84 74.74 56.95 63.19 67.92 83.73 69.99 69.76 64.91 63.33 81.69</cell></row><row><cell cols="14">OBPE(? = 0.7) 85.99 59.54 75.59 41.37 75.36 54.6 63.43 66.86 83.95 71.77 69.27 66.29 63.19 81.77</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bpe</cell><cell cols="6">83.97 68.01 74.24 77.1 88.2 57.6</cell><cell></cell><cell></cell><cell cols="2">77.45 53.45</cell><cell></cell><cell></cell><cell>66.08 83.21</cell></row><row><cell cols="2">OBPE(? = 0.5) 83</cell><cell cols="5">71.78 75.21 78.28 88.28 62.41</cell><cell></cell><cell></cell><cell cols="2">76.88 54.19</cell><cell></cell><cell></cell><cell>68.37 82.72</cell></row><row><cell cols="7">OBPE(? = 0.7) 83.56 69.3 74.84 77.09 87.93 57.9</cell><cell></cell><cell></cell><cell cols="2">77.11 57.84</cell><cell></cell><cell></cell><cell>67.39 82.87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">POS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bpe</cell><cell cols="7">94.14 81.7 86.57 86.86 94.5 69.2 80.39</cell><cell></cell><cell cols="5">95.79 88.62 84.8 85.74 82.99 94.81</cell></row><row><cell cols="8">OBPE(? = 0.5) 94.18 82.79 86.63 86.5 94.6 70.53 79.49</cell><cell></cell><cell cols="5">95.94 88.79 86.62 86.41 83.47 94.91</cell></row><row><cell cols="8">OBPE(? = 0.7) 94.1 81.56 87.04 86.55 94.38 70.67 79.99</cell><cell></cell><cell cols="5">96.17 89.8 87.77 86.19 83.70 94.88</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">XNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>bpe</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">65.79 48.3</cell><cell></cell><cell></cell><cell cols="2">63.21 54.93</cell><cell></cell><cell></cell><cell>51.62 64.50</cell></row><row><cell>OBPE(? = 0.5)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">66.77 50.84</cell><cell></cell><cell></cell><cell cols="2">66.77 53.27</cell><cell></cell><cell></cell><cell>52.06 66.77</cell></row><row><cell>OBPE(? = 0.7)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">66.37 48.54</cell><cell></cell><cell></cell><cell cols="2">63.57 54.85</cell><cell></cell><cell></cell><cell>51.70 64.97</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>NER F1 score, Text Classification, POS, XNLI evaluated on models trained on languages in their respective set,Languages in Romance family show some improvements in ? = 0.7 setting as compared to ? = 0.5</figDesc><table><row><cell></cell><cell>% overlap</cell><cell></cell><cell>En-Es</cell><cell></cell><cell></cell><cell></cell><cell>Hi-Mr</cell></row><row><cell></cell><cell>retained</cell><cell>High</cell><cell></cell><cell>Low</cell><cell></cell><cell>High</cell><cell>Low</cell></row><row><cell></cell><cell></cell><cell>en es</cell><cell>en</cell><cell>es</cell><cell>hi</cell><cell>mr</cell><cell>hi</cell><cell>mr</cell></row><row><cell cols="2">NER 100</cell><cell cols="6">72.3 75.10 63.40 85.90 55.56 86.34 58.18</cell></row><row><cell></cell><cell>0</cell><cell cols="6">70.9 67.74 51.68 82.70 43.38 85.14 16.59</cell></row><row><cell>TC</cell><cell>100</cell><cell></cell><cell cols="5">88.17 63.74 84.38 75.14 84.56 71.39</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell cols="5">82.55 53.80 78.85 72.36 84.5</cell><cell>30.07</cell></row><row><cell>POS</cell><cell>100</cell><cell></cell><cell cols="5">94.65 82.92 94.18 83.25 94.18 81.92</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell cols="5">92.82 60.40 94.02 76.70 94.2</cell><cell>74.07</cell></row><row><cell cols="2">XNLI 100</cell><cell cols="3">61.9 66.57 55.17</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell cols="3">62.6 61.50 53.85</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 :</head><label>14</label><figDesc>Accuracy of Zero-shot transfer from English (En) as HRL to Spanish (Es) and from Hindi(Hi) as HRL to Marathi(Mr) in two settings: (1) High where Es,Mr have sizes comparable to the HRL and (2) Low where their sizes are only 20K. As the percentage of overlapping tokens retained is decreased from 100% to 0%, the accuracy drops but the drop is higher in the low-resource setting.</figDesc><table><row><cell cols="2">Language Pair BPE</cell><cell>min</cell><cell>samp_LRL samp_min</cell></row><row><cell></cell><cell></cell><cell></cell><cell>% vocab LRL</cell></row><row><cell>hi-mr</cell><cell cols="3">39.65 40.53 39.96</cell><cell>41.00</cell></row><row><cell>hi-pa</cell><cell cols="3">45.03 45.92 45.62</cell><cell>46.70</cell></row><row><cell>hi-guj</cell><cell cols="3">42.52 43.33 42.80</cell><cell>43.99</cell></row><row><cell>en-de</cell><cell cols="3">41.32 41.42 42.42</cell><cell>42.67</cell></row><row><cell>en-nl</cell><cell cols="3">38.18 38.69 39.30</cell><cell>39.89</cell></row><row><cell>en-fy</cell><cell cols="3">41.37 41.69 42.70</cell><cell>43.13</cell></row><row><cell>fr-es</cell><cell cols="3">41.44 41.88 42.71</cell><cell>43.17</cell></row><row><cell>fr-pt</cell><cell cols="3">40.75 41.21 41.92</cell><cell>42.51</cell></row><row><cell>fr-it</cell><cell cols="3">43.08 43.35 44.28</cell><cell>44.62</cell></row><row><cell></cell><cell></cell><cell></cell><cell>% vocab HRL</cell></row><row><cell>hi</cell><cell cols="3">62.20 63.15 62.65</cell><cell>63.84</cell></row><row><cell>en</cell><cell cols="3">70.22 70.03 69.57</cell><cell>69.37</cell></row><row><cell>fr</cell><cell cols="3">63.51 63.24 63.59</cell><cell>63.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell>% vocab shared</cell></row><row><cell>hi-mr</cell><cell cols="3">38.81 39.72 39.16</cell><cell>40.23</cell></row><row><cell>hi-pa</cell><cell cols="3">42.92 43.86 43.38</cell><cell>44.55</cell></row><row><cell>hi-guj</cell><cell cols="3">41.44 42.29 41.68</cell><cell>42.92</cell></row><row><cell>en-de</cell><cell cols="3">40.65 40.78 41.74</cell><cell>42.02</cell></row><row><cell>en-nl</cell><cell cols="3">37.73 38.27 38.83</cell><cell>39.47</cell></row><row><cell>en-fy</cell><cell cols="3">39.94 40.35 41.05</cell><cell>41.63</cell></row><row><cell>fr-es</cell><cell cols="3">40.31 40.77 41.55</cell><cell>42.08</cell></row><row><cell>fr-pt</cell><cell cols="3">39.74 40.22 40.88</cell><cell>41.52</cell></row><row><cell>fr-it</cell><cell cols="3">41.76 42.05 42.93</cell><cell>43.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 15 :</head><label>15</label><figDesc>Vocabulary statistics: 12 languages .71 69.71 41.89 77.42 60.14 67.87 69.73 85.79 72.96 71.02 67.31 69.18 0 87.29 61.86 67.21 41.46 76.08 59.50 67.30 66.86 86.02 69.80 70.89 67.54 68.48 -1 86.67 64.19 72.38 39.93 77.17 58.25 67.09 69.86 85.83 72.99 70.43 66.02 69.23 -2 86.17 60.91 67.30 44.43 76.47 59.66 67.13 70.03 85.25 75.02 71.82 66.53 69.23 -3 87.14 62.68 72.25 44.73 77.24 61.41 67.38 69.87 86.15 69.82 71.06 65.37 69.59 -? 87.09 62.96 72.17 44.25 77.93 60.44 68.65 70.23 86.92 74.14 72.55 66.05 70.28 BPE-dp 85.54 62.64 71.46 39.75 75.51 59.29 67.76 70.42 84.15 67.43 68.82 67.74 68.38 TokComp 86.43 61.12 72.82 45.88 76.57 55.25 65.28 67.85 84.22 71.04 68.87 66.00 68.</figDesc><table><row><cell>Lang</cell><cell>hi</cell><cell>mr</cell><cell>pa</cell><cell>gu</cell><cell>en</cell><cell>de</cell><cell>nl</cell><cell>fy</cell><cell>fr</cell><cell>es</cell><cell>pt</cell><cell>it</cell><cell>avg</cell></row><row><cell>p</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NER</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="13">86.57 5944</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="6">80.35 61.45 69.00 72.32 88.63 62.27</cell><cell></cell><cell></cell><cell cols="2">77.23 62.58</cell><cell></cell><cell></cell><cell>71.73</cell></row><row><cell>0</cell><cell cols="6">80.11 64.07 68.26 70.48 87.61 54.96</cell><cell></cell><cell></cell><cell cols="2">76.53 62.23</cell><cell></cell><cell></cell><cell>70.53</cell></row><row><cell>-1</cell><cell cols="6">80.00 64.37 69.10 72.10 87.89 66.25</cell><cell></cell><cell></cell><cell cols="2">77.33 65.36</cell><cell></cell><cell></cell><cell>72.80</cell></row><row><cell>-2</cell><cell cols="6">79.21 64.83 68.58 70.41 88.17 65.76</cell><cell></cell><cell></cell><cell cols="2">76.78 58.71</cell><cell></cell><cell></cell><cell>71.56</cell></row><row><cell>-3</cell><cell cols="6">81.00 62.79 68.17 73.20 89.38 68.34</cell><cell></cell><cell></cell><cell cols="2">77.50 63.84</cell><cell></cell><cell></cell><cell>73.03</cell></row><row><cell>-?</cell><cell cols="6">80.68 68.90 70.03 72.14 87.92 66.05</cell><cell></cell><cell></cell><cell cols="2">77.14 63.00</cell><cell></cell><cell></cell><cell>73.23</cell></row><row><cell>BPE-dp</cell><cell cols="6">79.68 63.45 69.43 70.36 87.39 59.75</cell><cell></cell><cell></cell><cell cols="2">76.15 57.76</cell><cell></cell><cell></cell><cell>70.50</cell></row><row><cell cols="7">TokComp 82.06 67.17 70.42 72.48 88.02 58.47</cell><cell></cell><cell></cell><cell cols="2">77.22 60.29</cell><cell></cell><cell></cell><cell>72.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>POS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell cols="7">94.22 79.60 86.83 86.21 94.91 77.70 82.00</cell><cell></cell><cell cols="5">96.47 89.74 87.79 87.27 87.52</cell></row><row><cell>0</cell><cell cols="7">94.13 76.26 86.53 85.03 94.85 76.09 82.48</cell><cell></cell><cell cols="5">96.31 88.78 87.01 86.62 86.74</cell></row><row><cell>-1</cell><cell cols="7">94.20 79.13 86.23 85.14 94.87 78.22 82.56</cell><cell></cell><cell cols="5">96.32 89.62 87.25 87.27 87.34</cell></row><row><cell>-2</cell><cell cols="7">93.98 81.07 86.54 85.86 94.68 76.80 82.08</cell><cell></cell><cell cols="5">96.23 89.14 86.31 86.45 87.19</cell></row><row><cell>-3</cell><cell cols="7">94.31 79.55 86.67 86.65 95.03 76.34 83.63</cell><cell></cell><cell cols="5">96.30 89.97 87.76 88.00 87.66</cell></row><row><cell>-?</cell><cell cols="7">94.18 81.55 87.01 86.76 94.98 79.28 82.38</cell><cell></cell><cell cols="5">96.40 90.04 88.01 88.21 88.02</cell></row><row><cell>BPE-dp</cell><cell cols="7">93.26 80.03 86.31 85.23 94.49 77.90 83.07</cell><cell></cell><cell cols="5">96.10 90.01 87.84 87.63 87.44</cell></row><row><cell cols="8">TokComp 93.99 80.38 86.75 86.79 94.79 79.50 84.91</cell><cell></cell><cell cols="5">95.80 89.87 87.05 88.70 88.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>XNLI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64.35 50.36</cell><cell></cell><cell></cell><cell cols="2">61.06 53.77</cell><cell></cell><cell></cell><cell>57.39</cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64.33 49.06</cell><cell></cell><cell></cell><cell cols="2">59.96 54.71</cell><cell></cell><cell></cell><cell>57.02</cell></row><row><cell>-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64.35 48.62</cell><cell></cell><cell></cell><cell cols="2">61.40 53.51</cell><cell></cell><cell></cell><cell>56.97</cell></row><row><cell>-2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">65.05 50.36</cell><cell></cell><cell></cell><cell cols="2">64.45 55.31</cell><cell></cell><cell></cell><cell>58.79</cell></row><row><cell>-3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.86 50.64</cell><cell></cell><cell></cell><cell cols="2">64.85 57.11</cell><cell></cell><cell></cell><cell>60.11</cell></row><row><cell>-?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.41 50.76</cell><cell></cell><cell></cell><cell cols="2">65.13 57.29</cell><cell></cell><cell></cell><cell>60.15</cell></row><row><cell>BPE-dp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">64.31 50.16</cell><cell></cell><cell></cell><cell cols="2">63.17 55.17</cell><cell></cell><cell></cell><cell>58.20</cell></row><row><cell>TokComp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">67.98 53.05</cell><cell></cell><cell></cell><cell cols="2">64.21 54.83</cell><cell></cell><cell></cell><cell>60.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 16 :</head><label>16</label><figDesc>NER F1 score, Text Classification, POS, XNLI accuracy of combined model trained on all 12 languages for different values of p.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Technology Development for Indian Languages (TDIL), https://www.tdil-dc.in</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Tatoeba , https://tatoeba.org</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Arabert: Transformer-based model for arabic language understanding</title>
		<author>
			<persName><forename type="first">Fady</forename><surname>Wissam Antoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hazem</forename><surname>Baly</surname></persName>
		</author>
		<author>
			<persName><surname>Hajj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC 2020 Workshop Language Resources and Evaluation Conference 11-16</title>
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Universal and tight online algorithms for generalized-mean welfare</title>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Maiti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Iiit-h system submission for fire2014 shared task on transliterated search</title>
		<idno type="DOI">10.1145/2824864.2824872</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forum for Information Retrieval Evaluation, FIRE &apos;14</title>
		<editor>
			<persName><forename type="first">Ahmad</forename><surname>Irshad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vandan</forename><surname>Bhat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aniruddha</forename><surname>Mujadia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Riyaz</forename><surname>Tammewar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Manish</forename><surname>Ahmad Bhat</surname></persName>
		</editor>
		<editor>
			<persName><surname>Shrivastava</surname></persName>
		</editor>
		<meeting>the Forum for Information Retrieval Evaluation, FIRE &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="48" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multilingual alignment of contextual word representations</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Improving multilingual models with language-clustered vocabularies</title>
		<author>
			<persName><forename type="first">Chung</forename><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiat</forename><surname>Chuan Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Riesa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.367</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4536" to="4546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emerging cross-lingual structure in pretrained language models</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6022" to="6034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A new algorithm for data compression</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Gage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C Users Journal</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="23" to="38" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving target-side lexical transfer in multilingual neural machine translation</title>
		<author>
			<persName><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.319</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3560" to="3566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">XTREME: A massively multilingual multitask benchmark for evaluating cross-lingual generalisation</title>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="4411" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Hube</surname></persName>
		</author>
		<idno type="DOI">10.1145/3041021.3053375</idno>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion, WWW &apos;17 Companion</title>
		<meeting>the 26th International Conference on World Wide Web Companion, WWW &apos;17 Companion</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="717" to="721" />
		</imprint>
	</monogr>
	<note>Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cross-lingual ability of multilingual bert: An empirical study</title>
		<author>
			<persName><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Simran</forename><surname>Khanuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diksha</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvesh</forename><surname>Mehtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atreyee</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Gopalan</surname></persName>
		</author>
		<title level="m">Muril: Multilingual representations for indian languages</title>
		<editor>
			<persName><forename type="first">Dilip</forename><surname>Kumar Margam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pooja</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajiv</forename><surname>Teja Nagipogu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shachi</forename><surname>Dave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shruti</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Subhash</forename><surname>Chandra Bose</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vish</forename><surname>Gali</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Partha</forename><surname>Subramanian</surname></persName>
		</editor>
		<editor>
			<persName><surname>Talukdar</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploiting language relatedness for low web-resource language model adaptation: An Indic languages study</title>
		<author>
			<persName><forename type="first">Yash</forename><surname>Khemchandani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarvesh</forename><surname>Mehtani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaidehi</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijeet</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.105</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1312" to="1323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Australia. Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">XGLUE: A new benchmark datasetfor cross-lingual pre-training, understanding and generation</title>
		<author>
			<persName><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fenfei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhen</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sining</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiun-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winnie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuguang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6008" to="6018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CharBERT: Character-aware pre-trained language model</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenglei</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wine is not v i n. on the compatibility of tokenizations across languages</title>
		<author>
			<persName><forename type="first">Antonis</forename><surname>Maronikolakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.205</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana</addrLine></address></meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2382" to="2399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AdapterHub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">UNKs everywhere: Adapting multilingual language models to new scripts</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.800</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10186" to="10203" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">BPE-dropout: Simple and effective subword regularization</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Provilkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitrii</forename><surname>Emelianenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.170</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1882" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Samanantar: The largest publicly available parallel corpora collection for 11 indic languages</title>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aravinth</forename><surname>Bheemaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mayank</forename><surname>Jobanputra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujit</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshita</forename><surname>Diddee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mahalakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyanshu</forename><surname>Kakwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navneet</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aswin</forename><surname>Pradeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kumar</forename><surname>Deepak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Vivek Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pratyush</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khapra</forename><surname>Shantadevi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transfer learning in natural language processing</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-5004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How good is your tokenizer? on the monolingual performance of multilingual language models</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Rust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3118" to="3135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Masked language model scoring</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Salazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davis</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toan</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Kirchhoff</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.240</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2699" to="2712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BIS annotation standards with reference to Konkani language</title>
		<author>
			<persName><forename type="first">Madhavi</forename><surname>Sardesai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jyoti</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shantaram</forename><surname>Walawalikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edna</forename><surname>Vaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The COLING 2012 Organizing Committee. Indo-Aryan Hindi:Vaapariyo, Marathi:Vaapartat , Punjabi:Vaaparan, Gujarati:Vaaparvana Hindi:Jaate, Marathi:Jaaoon , Punjabi:Jaana, Gujarati:Jaao West-Germanic English:Category, German:Kategorie, Dutch:Categorie, Western Frisian</title>
		<meeting><address><addrLine>Mumbai, India; Italian</addrLine></address></meeting>
		<imprint>
			<publisher>Certificato</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="145" to="152" />
		</imprint>
		<respStmt>
			<orgName>Kategory English:University, German:Universitaten, Dutch:Universiteit, Western Frisian:Universiteiten Romance French:Association, Spanish:Associacion, Portuguese:Associacao</orgName>
		</respStmt>
	</monogr>
	<note>Proceedings of the 3rd Workshop on South and Southeast Asian Natural Language Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
