<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">K.-C</forename><surname>Jim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Research Institute, Inc</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><forename type="middle">G</forename><surname>Horne</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Research Institute, Inc</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">NEC Research Institute, Inc</orgName>
								<address>
									<postCode>08540</postCode>
									<settlement>Princeton</settlement>
									<region>NJ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">466729D44FCCA1AF51ED80B16F086FF1</idno>
					<note type="submission">received August 9, 1994; revised July 11, 1995, February 6, 1996, and May 21, 1996.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An Analysis of' Noise in current Neural Networks: Generalization</head><p>Kam-Chuen Jim, Member, IEEE, C. Lee Giles, Senior Member, IEEE, and Bill G. Horne, Member, IEEE Abstract-There has been much interest in applying noise to feedforward neural networks in order to observe their effect on network performance. We extend these results by introducing and analyzing various methods of injecting synaptic noise into dynamically driven recurrent networks during training. We present theoretical results which show that applying a controlled amount of noise during training may improve convergence and generalization performance. In addition, we analyze the effects of various noise parameters (additive versus multiplicative, cu- mulative versus noncumulative, per time step vs. per string) and predict that best overall performance can be achieved by injecting additive noise at each time step. Noise contributes a second-order gradient term to the error function which can be viewed as an anticipatory agent to aid convergence. This term appears to find promising regions of weight space in the beginning stages of training when the training error is large and should improve convergence on error surfaces with local minima. The first-order term can be interpreted as a regularization term that can improve generalization. Specifically, this term can encourage internal representations where the state nodes operate in the saturated regions of the sigmoid discriminant function. While this effect can improve performance on automata inference problems with binary inputs and target outputs, it is unclear what effect it will have on other types of problems. To substantiate these predictions, we present simulations on learning the dual parity grammar from temporal strings for all noise models, and present simulations on learning a randomly generated six-state grammar using the predicted best noise model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HERE has been a great deal of work on enhancing neural-network performance. Two important performance parameters are convergence and generalization. Convergence is the amount of time, measured in central processing unit (CPU) operations or training epochs, required to find an acceptable solution during training. Generalization measures the ability to correctly classify new unseen data.</p><p>Among some of the important methods that have been introduced to improve convergence is the notion of searching the coarse regions of state space before searching finer regions. Simulated annealing is one such method, and was shown by <ref type="bibr">Kirkpatrick et al. [20]</ref> to be very effective on combinatorial problems like graph partitioning. Similarly, <ref type="bibr">Ackley et al. and Hinton and Sejnowski [l]</ref>, <ref type="bibr">[14]</ref> used noise in Boltzmann machines to escape from local minima. Specifically, it was indicated that at high noise temperatures Boltzmann machines will search the overall structure of state space for a coarse minimum, while subsequently lowering the noise temperature will allow a better minimum to be found within the coarsescale minimum.</p><p>It has been shown that generalization can be improved by removing the redundancies and irrelevancies in a network. There have been at least two approaches: pruning and regularization. An example of pruning is optimal brain damage, which can improve generalization ability and speed of learning by using second-derivative information to remove unimportant weights from the network [SI. Also, Giles and Omlin [ 121 have demonstrated improvement in generalization of recurrent neural networks after pruning. Other pruning methods can be found in <ref type="bibr" target="#b20">[26]</ref>. Weight decay is an example of a regularization method, and was shown by <ref type="bibr">Krogh and Hertz [21]</ref> to improve generalization on feedforward networks by suppressing irrelevant components of the weight vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Previous Work on Training with Noise</head><p>Previous research has investigated the effects of noise on feedforward neural networks. Training with noise can lead to more realistic biological models. For example, <ref type="bibr">Hinton et al.</ref> [ 151 compared noise temperature in the Boltzmann machine to the addition of Gaussian noise at neuronal membrane potentials. <ref type="bibr">Bremermann and Anderson [4]</ref>, <ref type="bibr">[5]</ref> introduced a biased random walk in weight space (the "chemotaxis algorithm") as a biologically plausible learning rule, and Kilis and Ackerman <ref type="bibr">[19]</ref> introduced a stochastic neuron model which uses Monte Carlo techniques to stochastically bias the firing decision of neurons. More relevant to this paper, however, are the experiments that use noise injection during training to enhance network performance.</p><p>Much work in training with noise has been motivated by the desire to improve fault tolerance, i.e., the graceful degradation of performance in the presence of faults. Judd and Munro [ 181 showed that using noisy hidden nodes during training can result in error-correcting codes which increase the tolerance of feedforward nets to unreliable nodes. Similarly, S6quin and Clay <ref type="bibr">[28]</ref> showed that randomly disabling hidden nodes (i.e., clamp to zero) during the training phase increases the tolerance of multilayer perceptrons to node failures. <ref type="bibr">Minnix [23]</ref> also demonstrated improved fault tolerance by training on noisy inputs, and argued that such a method helps prevent pattern memorization. <ref type="bibr">Murray and Edwards [24]</ref> utilized synaptic noise during training to improve fault tolerance and training 1045-9227/96$05.00 0 1996 IEEE quality. Omlin and Giles <ref type="bibr" target="#b19">[25]</ref> have extended the fault tolerance considerations to recurrent neural networks.</p><p>With the exception of <ref type="bibr">Murray and Edwards [24]</ref>, these methods generally indicate convergence as a tradeoff for enhanced performance. Recently, stochastic techniques, such as nodal perturbation, weight perturbation, and summed weight neuron perturbation, have been introduced which in some cases may converge more quickly than pure gradient descent ([7],</p><p>[9], [ 101, [ 171). These methods in general require more epochs to converge but less computation per epoch, mainly because calculation of the true gradient is not required. Also, it has been demonstrated that faster learning rates can be achieved by adding noise to the weight updates and adapting the magnitude of such noise to the output error <ref type="bibr">([6]</ref>). Training with noise can also improve generalization. Holmstrom and Koistinen <ref type="bibr">[16]</ref> showed that training with additive input noise can iimprove generalization in feedforward networks, and provided mathematically justified rules for choosing the characteristics of the noise. <ref type="bibr">Bishop [3]</ref> showed that training with noisy data is equivalent to Tikhonov regularization and suggested directly minimizing the regularized error function as a practical alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of Paper</head><p>To date, there has been little work on applying noise insertion methods to recurrent neural networks. In this paper, we explore the effects of synaptic noise on recurrent neural networks. We introduce several ways to inject noise during training, and demonstrate that utilizing noise-enhanced error functions can improve convergence and generalization. Previous work on improving these two performance measures, as discussed above, focused on simplifying feedforward neural networks and on ways of first searching the coarse regions of state space. The function of synaptic noise in recurrent nets is novel in the sense that synaptic noise can improve convergence by searching for promising regions of state space, and at the same time enhance generalization by favoring saturated state nodes. Promising regions of state space do not necessarily correspond to coarsely spaced local minima, and saturated state nodes are in general different from a simplified network.</p><p>Besides an analysis of noise in recurrent neural networks, we perform many simulations to verify our analysis. For recurrent neural network and problem selection, we use a second-order fully connected recurrent neural network. This recurrent neural network is then trained to learn automata from grammatical strings encoded as temporal sequences. For a small neural network and small automata, we ran many simulations and thus show statistical confidence in our results.</p><p>Section I1 defines the second-order fully connected recurrent neural network used in simulations and introduces several methods of implementing synaptic noise. We discuss different noise models and compare their effects on the error function in Section 111. From this analysis, predictions are made regarding their effects on convergence and generalization performance.</p><p>Section IV presents simulation results on learning the dual parity and a small six-state randomly generated automata from strings encoded as temporal sequences. These results are general to all incremental weight updating methods and should extend easily to other recurrent networks. Conclusions and open questions follow in Section V. s,t+l = g (OF")</p><p>(1) @+I Z</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>W2,kS;I;</head><p>(2) J&gt;k where 0, is the net input to unit i and g(z) is a predetermined sigmoid discriminant function. In the simulations described in Section IV, g(z) is defined as</p><formula xml:id="formula_0">(3)</formula><p>"One-hot" or "unary" input encoding is used. The input symbols are encoded as orthonormal unit vectors in L-dimensional space, so the inputs are in the form I k = Skcv for (0 5 Q: 5 L). For the dual parity DFA, L = 3, in order to represent the binary input characters zero and one, and the empty string e. Hence, the possible symbols are Each input string is presented to the network one character per discrete-time step t. At time t = T , when the entire input string has been presented to the network, the value at a selected neuron SO is defined to be the output. An input string is where d p i s the target output value for pattern p (either one or zero depending on the DFA response of the input string), and E?, is the raw error.</p><formula xml:id="formula_1">accepted if SO &gt; 1 -E , or rejected if SO &lt; E,</formula><p>For complexity reasons we used the backpropagationthrough-time (BPTT) training algorithm <ref type="bibr" target="#b21">[27]</ref>. Incremental learning, as described by <ref type="bibr">Giles et</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Noise Injection</head><p>Murray and Edwards <ref type="bibr">[24]</ref> simulated synaptic noise on multilayered perceptrons (MLP's) by applying noise to the weights of each layer during training. Applying this method to recurrent networks is not straightforward because effectively the same weights are propagated forward in time. This can be seen by recalling the BPTT representation of unrolling a recurrent network in time into T layers with identical weights, where T is the length of the temporal sequence. As in <ref type="bibr">Burton and Mpitsos [6]</ref>, <ref type="bibr">Hanson [13]</ref>, and <ref type="bibr">Murray and Edwards [24]</ref> we will only focus on noise injection in synaptic weights and ignore other methods of noise insertion.</p><p>Because of its temporal nature, synaptic noise injection of recurrent networks introduces two issues which do not pertain to feedforward nets.</p><p>1) What is the frequency of noise injection? 2) Is the noise injection cumulative or noncumulative? This is a direct result of the fact that recurrent networks effectively propagate the same weights forward in time. Should one set of noise values also be propagated forward, or should a different set of noise values be applied at each time step? Or, should noise be applied only at the beginning of each input sequence? If noise is propagated forward in time, should it accumulate in the weights over all time steps, or can the noise be inserted independently each time step?</p><p>In this paper we consider the cases where noise is propagated forward in time. We introduce a per time step method of injecting synaptic noise into recurrent networks which inserts a new random noise value at each time step of the forward pass during training. On the other hand, a per string model can be viewed as a special case of the per time step model such that the same set of noise values is injected for all time steps of each given input string.</p><p>Noncumulative models apply a new noise value Aijk to a noise-free weight at each instance of noise injection, while cumulative models apply a new noise to a previously perturbed weight at each instance. Noncumulative noise can be represented as fl'?,ijk = N(WGk) <ref type="bibr">(5)</ref> where {W%;,} is the noise-free weight set and N is the noise process applied to weight WL;k. Cumulative noise be represented by</p><formula xml:id="formula_2">WO,%Jk = N(WGk) (6) I/Vt,%Jk = N(Wt-1 q k ) . t &gt; 0. (<label>7</label></formula><formula xml:id="formula_3">)</formula><p>An issue common to both feedforward and recurrent networks is whether noise is additive or multiplicative. Additive noise perturbs a weight parameter such that where A,,, is a random noise value. On the other hand, multiplicative noise perturbs each weight such that Table <ref type="table">1</ref> clarifies these issues by illustrating the noise iujection steps for all additive noise processes. The A's are the various noise terms, and W* is the noise-free weight. For per time step noncumulative noise insertion a different noise is added at each time step and the noise from the previous time step does not accumulate in time. For the per time step cumulative noise insertion all previously added noise is carried over to the next time step. For per string (or per sequence) noncumulative, the same noise is inserted at the beginning and used throughout the temporal updates. For per string cumulative, the initial noise accumulates for each time step. Similar results would be seen for the multiplicative noise case.</p><p>We report experiments which explore all combinations of additive versus multiplicative, cumulative versus noncumulative, and per string versus per time step noise models. In all noise models the actual weight updates AWijk's descend the noise-enhanced error term Ep. These updates are applied to the noise-free weight set. This prevents "noisy updates," allows simpler mathematical analysis, and for the purpose of these experiments has the desirable effect of limiting the effect of noise to the error function.</p><p>For all cases the noise source is chosen to be a random zero-mean white noise process uniformly distributed between (-a, a), where a is a positive constant parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">ANALYSIS OF SYNAPTIC NOISE</head><p>The effects of each noise model are analyzed by taking the Taylor expansion on the error function around the noisefree weight set. By truncating this expansion to second-order and lower terms, we can interpret the effect of noise as a set of regularization terms applied to the error function. From these terms predictions can be made about the effects on generalization and convergence. A similar analysis was performed by Murray and Edwards [24]   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUMULATIVE Additive</head><p>Mu1 tiplicative</p><formula xml:id="formula_4">t t W * , j j k = wi ; . k f 1 &amp;,ijk w t , i j k = W G k n (1 + A , , , ) r=o r=O T-1 1 ' -l as; as; -a2</formula><p>Xu--</p><formula xml:id="formula_5">-â‚¬Po2 C t,u=O Cvawt,ijkawu,ijk i j k t,u=O i j k T-1 'IVE</formula><p>SET demonstrate the effects of synaptic noise on fault tolerance and training quality. Le Cun et al.</p><p>[SI also applied a Taylor expansion on the error function to approximate the effects of small weight perturbations and dropped the first-order terms by assuming the network is at convergence. As in [24], we do not assume the networks are at convergence and consider both first-and second-order terms. Tables <ref type="table" target="#tab_3">I1</ref> and<ref type="table" target="#tab_5">111</ref> list the noise injection step and resulting first-and second-order Taylor expansion terms for all noise models. These terms are derived in Appendix A.</p><p>Our results are comparable (in terms of the effects on the error function) to feedforward nets only when the recurrent noise models are noncumulative and per time step. Cumulative noise and per string noise in recurrent nets are not comparable to their feedforward versions because of the absence of error-weight correlations across time in feedforward nets. In feedforward nets, per string noise is identical to per time step noise because the length of each pattern in time is always one. Also, cumulative noise in feedforward networks is identical to noncumulative noise when the weights are updated after the presentation of each training sample, but has different variance when using batch training (see Appendix B). Thus, our noncumulative. Multiplicative per time step analytical results are consistent with Murray and Edwards 1241, who applied multiplicative synaptic noise to MLP's. Other noise models accumulate noise at different rates and have different convergence and generalization effects, as described below and summarized in Tables 11-IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generalization Improvement Due to Synaptic Noise</head><p>We propose that two ways to improve generalization are reduce the raw error; improve the internal representation of information in the network. Reducing the raw error tp improves the tolerance of the network to perturbations in the output nodes. This tolerance provides a buffer from which slight variations in the output node will lead to the same classification, thus improving generalization. At the same time, it has been shown that on networks with too many parameters overtraining can lead to poor generalization, since reducing the raw error to zero causes the network to effectively memorize the training set <ref type="bibr">[2]</ref>.</p><p>One common cause of poor generalization in recurrent networks that are trained on hidden-state problems is the presence of unsaturated state representations <ref type="bibr">[30]</ref>. Typically, a network cannot revisit the exact same point in state space, but tends to wander away from its learned state representation. One approach to alleviate this problem is to encourage state nodes to operate in the saturated regions of the sigmoid. The firstorder error expansion terms of most noise models considered are capable of encouraging the network to achieve saturated states. This can be shown by applying the chain rule to the partial derivative in the first-order expansion terms where 0; is the net input to state node z at time step t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The partial derivatives</head><p>favor internal representations such that the effects of perturbations at the net inputs 0; on the states S," are minimized. While this effect may improve performance on automata inference problems (learning automata from temporal sequences of strings) with binary inputs and target outputs, it is unclear what effect it will have on other types of problems.</p><p>Multiplicative noise implements a form of weight decay because the error expansion terms include the weight products W&amp;,k or Wt.L3kW1L,23k. Although weight decay has been shown to improve generalization on feedforward networks</p><p>[21], we hypothesize that, in general, weight decay will not always improve generalization for recurrent networks that are learning finite-state automata (FSA) problems. Large weights are necessary to saturate the state nodes to the upper and lower limits of the sigmoid discriminant function. Therefore, we predict additive noise will allow better generalization on automata inference problems because it is not a weight decay term, while multiplicative noise will work better on other types of problems where weight decay is appropriate.</p><p>Noise models whose first-order error term contain the expression dS;/dWt,,,k dS;/dW,,l,, will favor saturated states for those partials whose sign correspond to the sign of a majority of all partials. These noise models will favor unsaturated states, operating in the linear region of the sigmoid, for partials whose sign is in the minority. Such sign-dependence is not optimal because not all the weights are encouraged to be saturated.</p><p>The error terms for cumulative per time step noises sum a product with the expression vdSg/dWt,,,k dSg/dW,,l,,,</p><p>where ' U = min(t + 1, U + 1). The effect of cumulative noise increases more rapidly because of zi and thus optimal generalization and detrimental noise effects will occur at lower amplitudes than noncumulative noise.</p><p>For cumulative per string noise models, the products (t + 1)(u + 1) and G't,a3kG'u,lmn in the expansion terms rapidly overwhelm the raw error term. Generalization improvement is not expected for these models.</p><p>We also reason that all generalization enhancements will be valid only for a range of noise values, above which noise overwhelms the raw error information. The differences between the noise models are summarized in Table <ref type="table" target="#tab_6">IV</ref>, where a check indicates that the corresponding enhancement is predicted. It is important to realize that although all noise models are theoretically capable of improving convergence and generalization by searching for promising weights and saturated states, Table IV lists the additional enhancements to the search. From this table it can be inferred that additive per time step noise models should yield the best generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Convergence Improvement Due to Synaptic Noise</head><p>IV. SIMULATION RESULTS</p><p>Convergence can be represented by the number of epochs required to learn the training set. This measure is dependent on the error trajectory followed during the training phase. On error surfaces filled with local minima many epochs may be required before the training set is learned, because the leaming trajectory may enter an unacceptable local minima and face a difficult or impossible task of exiting from it.</p><p>Intuitively, synaptic noise may alleviate this problem by allowing the network to make noisy jumps out of local minima. Obviously, this will be beneficial only to learning problems whose error surfaces contain local minima. On simple error surfaces synaptic noise may worsen generalization performance by perturbing a straightforward training trajectory which would have otherwise led directly to a solution.</p><p>Synaptic noise can improve convergence by favoring promising weights in the beginning stages of training. This can be demonstrated by examining the second-order error expansion term for noncumulative multiplicative per time step noise When ep is negative, solutions with a negative second-order state-weight partial derivative will be destabilized. In other words, when the output 5'; is too small the network will favor updating in a direction such that the first-order partial derivative is increasing. A corresponding relationship can be observed for the case when tp is positive. Thus the secondorder term of the error function will allow a higher raw error cp to be favored if such an update will place the weights in a more promising area, i.e., a region where weight changes are likely to move 5 ' : in a direction to reduce the raw error.</p><p>The anticipatory effect of this term, or look-ahead property as described in <ref type="bibr">[24]</ref>, is more important in the beginning stages of training where tp is large, and will become insignificant in the finishing stages of training as tp approaches zero.</p><p>In general, the second-order terms of all noise models, as shown in Tables <ref type="table" target="#tab_3">I1</ref> and<ref type="table" target="#tab_5">111</ref>, are capable of this anticipatory effect. In all noise models, these terms will favor updating the weights such that the sum of the dependencies of the output node on all weights has the opposite sign of the raw error term tp. In other words, either a majority of the weights are moved to a more promising space, or a minority of the weights have moved to a more promising space with very steep slopes.</p><p>Noncumulative multiplicative per time step noise includes the squared terms W &lt; 7 J k , which are always positive. Thus, this noise model and all additive noise models are guaranteed to have this anticipatory effect. All other multiplicative noise models have the products Wt,23kWu,23fi or W+,L3kWu,~mn, implying that the anticipatory effect is observed only when the majority of the weights are positive. This is summarized in Table <ref type="table" target="#tab_6">IV</ref>, column 3.</p><p>Similar to arguments in Section 111-A, the absence of weight decay will make the learning task easier and improve convergence (see Table <ref type="table" target="#tab_6">IV</ref>, column 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental MethodoLogy</head><p>Two sets of experiments which explore the effects of all combinations of cumulative versus noncumulative, additive versus multiplicative, and per string versus per time step noise are reported in this paper. In order to perform many experiments in a reasonable amount of computation time, we attempt to learn the simple dual parity grammar from sample strings encoded as temporal sequences. Dual parity is a four-state automaton that recognizes binary strings containing an even number of ones and zeros (see Fig. <ref type="figure">1</ref>) and is a classic example of a "hidden-state" problem. Both sets of experiments were performed on the dual parity problem, one set using recurrent networks with three-state neurons, and the other using recurrent networks with four-state neurons. In addition, another set of simulations applies the best predicted noise model (additive, noncumulative per time step) to learning a slightly larger randomly generated six-state automaton (RND6-see Fig. <ref type="figure">2</ref>). Our experiments consist of 500 simulations for each data point and achieve useful (90%) confidence levels.</p><p>Second-order recurrent networks with three input neurons are trained on the dual parity problem. The finite-state automaton (FSA) that recognizes the dual parity grammar is shown in Fig. <ref type="figure">1</ref>. The data consist of 8191 strings of lengths zero to 12. The training set consists of the first 1023 strings of lengths zero to nine, while the initial working set consists of 31 strings of lengths zero to four. Generalization performance of a trained network was obtained by testing on the entire data set of 8191 strings. Training was stopped when all of the training set was correctly classified to the previously discussed error criterion.</p><p>If training persisted beyond 5000 epochs, training was halted and the trained network was not used. Thus, only networks which were perfectly trained on the training set were tested for generalization. For runs where the noise amplitude was small, the networks usually converged in less than 5000 epochs. For higher noise values, up to a quarter of the runs did not correctly classify the training set in the required number of epochs and were discarded.</p><p>Second-order recurrent networks with three input neurons and 10 state neurons were trained on the RND4 grammar.</p><p>The finite-state automaton that recognizes this grammar is shown in Fig. <ref type="figure">2</ref>. The data consist of 1023 strings of length zero to nine. The training set consists of the first 50 strings. If training persisted beyond 500 epochs, training was halted and the trained network was not used.</p><p>As mentioned before the noise source is chosen to be a random zero-mean white noise process uniformly distributed between (-a,a), where a is the amplitude.</p><p>To ensure consistency and fairness across all experiments, the leaming rate and momentum were set to 0.5, and the weights were initialized to random values between [-1 .O, 1 .O] in all simulations. The empirical results presented below represent 90% confidence levels using a two-tailed Student's t-distribution, and were obtained by running 500 simulations for each data point. The convergence and generalization results for the dual parity problem are presented first, followed by the results for RND6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization Pe$ormance</head><p>In Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref>, we see generalization performance on the dual parity problem as a function of noise amplitude for both the three-state and four-state neuron recurrent nets. For each net we see generalization for various noise combinations. For all simulations the zero-noise case is where a = 0. Generalization performance mirrors some of our predictions. However, all curves exhibit the same general shape of generalization performance. The generalization improves as noise increases from the zero-noise point for all noise cases and then starts to degrade after a certain noise-threshold amplitude. After this noise-threshold amplitude, generalization performance eventually becomes worse than the zero-noise case. [The curve in Fig. <ref type="figure" target="#fig_2">3</ref>(a) does increase as a increases, but is not shown.] The cumulative per string noise cases for both additive and multiplicative noise failed to converge for all runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Convergence Time</head><p>Convergence time on the dual parity problem as a function of noise amplitude is plotted in Figs. <ref type="figure" target="#fig_4">5</ref> and<ref type="figure" target="#fig_5">6</ref>. As mentioned above, networks injected with cumulative per string noise do not converge. For most noise models, convergence for added noise initially decreases up to a noise threshold and then starts to increase and finally exceeds that for the nonoise case. Similar behavior for feedforward networks has been observed <ref type="bibr">[24]</ref>. This is more apparent for the three-state neuron simulations than the four-state neuron ones; however, close inspection will show there is a convergence mean less than that for the zero-noise case for the four-state neuron case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects of Each Noise Parameter on Perjormance</head><p>This section makes use of the dual parity simulations to assess the effects of each noise parameter on convergence time and generalization performance. In order to make fair assessments, it is reasonable to compare the best mean performance due to each noise parameter. For example, to compare the effects of additive versus multiplicative noise on generalization, we should compare the mean generalization curves of networks injected with additive cumulative per time step noise with the mean generalization curves of networks injected with multiplicative noncumulative per time step noise. These noises represent the additive and multiplicative noise combinations yielding the best generalization. By looking at the best noise combinations for each parameter, comparisons can be made which are independent of other noise issues. Using the above case as an example, if the best additive noise shows better generalization than the best multiplicative noise, then this is a fair indication that additive noise yields better generalization than multiplicative noise. It cannot be argued that perhaps cumulative noise allows better generalization, since if this was the case then we would be comparing additive cumulative per time step noise with multiplicative cumulative per time step noise-which would be fair if such was the case.</p><p>Simulated performance closely mirrors our predictions. Improvements were observed for all noise models except for cumulative per string noises which failed to converge for all runs. Generalization improvement was more emphasized on networks with four states, while convergence enhancement was more noticeable on three-state networks. The simulations show the following results.</p><p>Additive noise is better tolerated than multiplicative noise, achieves better generalization, and yields slightly better convergence (Fig. <ref type="figure">7</ref>).</p><p>Cumulative noise achieves optimal generalization and convergence at lower amplitudes than noncumulative noise. Cumulative noise also has a narrower range of beneficial noise, which is defined as the range of noise amplitudes which yields better performance than that of a noiseless network <ref type="bibr">[Fig. 8(a)</ref>   Overall, the best performance is obtained by applying cumulative and noncumulative additive noise at each time step. These results closely match the predictions of Section 111-A. The only exceptions are that all multiplicative noise models seem to yield equivalent performance. This discrepancy between prediction and simulation may be due to the detrimental effects of weight decay in multiplicative noise, which can conflict with the advantages of cumulative and per time step noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Noise Max Amp</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Generulizution Versus Convergence</head><p>In most applications both generalization and convergence are important; improving one at the expense of the other becomes a difficult decision. In this section we plot the tradeoff between generalization and convergence in the dual parity results, and show that in many cases it is possible to improve both performance measures simultaneously.</p><p>In Fig. <ref type="figure" target="#fig_11">9</ref> we plot generalization versus convergence. Noise magnitude increases from left to right in each curve. For example, if convergence is not an issue then one would pick the curve that dips the lowest, and choose the corresponding noise amplitude for the noise method corresponding to that curve. These plots clearly illustrate that additive noise methods allow the best generalization and convergence for the four-state neurons networks.</p><p>More importantly, these plots illustrate the cases where both convergence and generalization are improved. In Fig. <ref type="figure" target="#fig_11">9(c)</ref> and<ref type="figure">(d</ref>) the curves clearly curl down and to the left for lower noise amplitudes before rising to the right at higher noise amplitudes. These lower regions are optimal because they represent improved generalization and improved convergence, i.e., by picking a noise amplitude from this "hook' convergence and generalization do not trade off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Convergence and Generalization Results on the RND6 Grammar</head><p>Figs. 10 and 11 plot convergence and generalization results on the RND6 grammar using additive noncumulative per time step noise. These results are obtained from a problem slightly more difficult than the dual parity problem, in the sense that the RND6 grammar is slightly larger and no attempts have been made to optimize the network size for this grammar. A network size of 10 state neurons was chosen arbitrarily, although more neurons could have been added to improve generalization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Noise Max Amp</head><p>We simulated only the best predicted noise model for this set of experiments. The purpose was to observe the performance on a slightly more difficult problem. As shown in Figs. 10 and 11, the results are consistent with those on the dual parity grammar presented above. Both generalization and Convergence improved for a range of noise amplitudes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented several methods of injecting synaptic noise to recurrent neural networks. We give a thorough analysis of these methods and empirically test these methods on the learning dual parity and RND6 grammars from strings encoded as temporal sequences.</p><p>Results show that most of these methods offer several benefits on recurrent networks:</p><p>* ability to improve generalization performance; ability to improve convergence performance; ability to improve generalization and convergence simul-These benefits should apply to feedforward and other recurrent networks. The ability to improve generalization and convergence simultaneously makes these results novel to other methods previously discussed for feedforward nets, which in general cast convergence as a cost for improved generalization performance. These results also offer some insight into what types of synaptic noises are better tolerated when training under noisy environments. If only synaptic noise is applicable, and noisefree updates are allowed, then cumulative multiplicative noise is the most detrimental at high amplitudes. In addition, noise which is uncorrelated in time is less destructive, as illustrated by the fact that injecting a different noise at each time step (for one input string) allows higher detrimental noise thresholds than injecting the same noise at each time step (effectively equivalent to the per string noise models). However, the appropriateness of any of these noise environments needs to be determined. It would be interesting to apply these methods to other problems to study the effect of problem complexity on the range of beneficial noise. Further research with larger networks could determine the effect of network size on noise tolerance and effectiveness on generalization and convergence. Another important issue is the relationship between the range of optimal noise values to network size and problem complexity. Thus it could be possible to determine the usefulness of synaptic noise by allowing an optimal range of noise values to be predicted for each application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SERIES EXPANSIONS OF THE ERROR FUNCTIONS</head><p>The Taylor series expansion of the error functions <ref type="bibr">[SI,</ref><ref type="bibr">[24]</ref> are derived in this section. In all derivations below the noise is assumed to Ibe zero-mean white and uncorrelated in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Noncumulative Noise</head><p>The additive per time step noise models are analyzed in detail below, followed by straightforward extensions to other noncumulative noise cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additive Per Time</head><p>Step: Each weight wt,L3k is perturbed at each time step such that</p><formula xml:id="formula_6">w t L j k -+ wL;k + &amp; , q k (1 1)</formula><p>where {Whyk} 19 the noise-free weight set. Performing a Taylor series expansion on the output of state neuron S z to second order around the noise-free weight set yields</p><p>The error E,, for pattern p on a noiseless network is defined as</p><formula xml:id="formula_7">1 (13) E ---t2 " -2 P</formula><p>where til is e?, = Sgd p . However, the injection of noise represented by (12) augments Ep as follows:</p><p>(16) Since the noise is zero-mean white and uncorrelated, taking the time average over the training phase for the per time step case yields</p><formula xml:id="formula_8">( A t , Z j k ) = 0 (17) (18) 2 ( a t , t j k &amp; , l m n ) CJ &amp;jk,ulmn</formula><p>where St2jk,u171Ln is the Kronecker delta. Using ( <ref type="formula">17</ref>) and ( <ref type="formula">18</ref>) simplifies (1 6) to  (24) in place of (24).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cumulative Noise</head><p>The cumulative noise models are analyzed below. They are based on and described in the same format as the above section on noncumulative noise. where ?I = min(t + 1. U + 1) can be used to simplify the error expression.</p><p>Additive Per String: The noise injection step for the additive per string case can be modeled by ( <ref type="formula">28</ref>)</p><formula xml:id="formula_9">Wt,/jk = wz;k + (t + 1)Aqk. ( ( t + W q k ) = 0</formula><p>The time averages (29)  The time averages ( @ t , i j k ) = 0 (34) (@t,ijk@u,lmn,) = wf12&amp;Jk,lnLn <ref type="bibr">(35)</ref> where w = min(t + 1, U + 1) can be used to simplify the error expression.</p><p>Multiplicative Per String: In the per string case, the noise injection step W t , i j k = WGk(l + A i j ~) ~+ l can be represented by <ref type="bibr">(36)</ref> w . .</p><formula xml:id="formula_10">t , r j k --W;",k + w G k Q t , l j k Q t ; i j k = (1 + A,j,c)t+l -1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>where (37)</head><p>Using binomial expansion and dropping third or higher order terms we get the following approximations: For all cases the weight update AWL,, deccends the noiseenhanced error function (E;;O1'Y).</p><formula xml:id="formula_11">( ~t , i j k ) = [I + (t + 1)(Aij,)<label>(38)</label></formula><formula xml:id="formula_12">-(1 + Aijk)t+l -(I + Azrr,n)u+l + 1) 1 1 + 4t(t + l)(A?jk) + O ( &gt; 3 ) -1 1 2 = -t(t + 1 ) a Z (Qt,ijlcQu,lmn) = ((1 + A i j k ) t + l ( l + A/Tnn)u+l = (( 1 + A i j k ) " + ' ( 1 + Almn)u+l) 1 1 2 2 ~ -t(t + l ) a 2 --u(u + 1 ) a 2 -1 t(t + 1) + u(u + 1) az = [1+ 2 $ 4 --. _ _ ---<label>6</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B CUMULATIVE VERSUS NONCUMULATIVE NOISE IN FEEDFORWARD NETWORKS</head><p>In feedforward networks, cumulative and noncumulative noise models are identical if using per sample training, but can have different variance if batch training is used (see analysis below). The methods described in this paper update the set of noise-free weights and accumulate noise between weight updates, as opposed to the continuous accumulation of noise across weight updates. Per sample updating and batch updating are considered below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Per Sample Updating</head><p>Since the length of each sample (in time) is one, noise cannot accumulate in the weights because we do not keep track of the previou5 noi5e values after each weight update. Thus, cumulative and noncumulative noise models are identical when updating the weights after the presentation of each training sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Butch Training</head><p>In this case, the errors for several samples are computed before the weights are updated. For example, in an additive noise model the weights are augmented ac follows between updates, for each training sample t, and weight W,:  IEEE Trans. Neural Networks, vol. 3, pp. 24-38,  1992.  1171  Var() is the variance, and Cov() is the covariance. Applying this here' and are independent noise a = 1, b = 1, Cov(X,Y) = 0, and the following is obtained: Systems 5, S. J. <ref type="bibr">Hanson, J. D. Cowan, and C. L. Giles, Eds. San Mateo, CA: Morgan Kanfmanu, 1993, pp. 89-96. (191 D.</ref> Kilis and E. Ackerman, "A stochastic neuron model for pattern <ref type="bibr">recognition,</ref><ref type="bibr">" in Proc. Int. Joint Conf Neural Network.r,</ref><ref type="bibr">vol. 2,</ref><ref type="bibr">Washington,</ref><ref type="bibr"></ref> Thus, the noise values for training samples presented later within the batch will observe greater variance.</p><formula xml:id="formula_13">no = a, nl = A, + A, n2 = A, + A, + A,</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where E is the error tolerance and is a positive real number. Weights are updated at the end of each string.(O,O, 11, (0,1,0), and ( L O , 0).During training, an error function is computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig</head><label></label><figDesc>Fig. 1. state. The dual parity finite-state automaton. State qo i s both start and final</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Mean generalization percent error as a function of noise amplitude a for recurrent networks with four-state neurons. The error bars show 90% confidence intervals, obtained by running 500 simulations for each point. All three curves in the top row are for additive noise and all three in the bottom row for multiplicative noise. The generalization curves are for: (a) noncumulative additive per time step, (b) noncumulative additive per string, (c) cumulative additive per time step, (d) noncumulative multiplicative per time step, (e) noncumulative multiplicative per string, and (f) cumulative multiplicative per time step. Experiments with cumulative per string noise do not converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Mean generalization percent error as a function of error magnitude a for recurrent networks with three-state neurons. The error bars show 90% confidence intervals, obtained by running 500 simulations for each data point. All three curves in the top row are for additive noise and all three in the bottom row for multiplicative noise. The generalization curves are for: (a) noncumulative additive per time step, (b) noncumulative additive per string, (c) cumulative additive per time step, (d) noncumulative multiplicative per time step, (e) noncumulative multiplicative per string, and (f) cumulative multiplicative per time step. Experiments with cumulative per string noise do not converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Mean convergence in epochs as a function of noise amplitude (( lor recurrent networks with four-state neurons. The error bars show 90% confidence intervals, obtained by running 500 simulations for each data point. The convergence curves are for: (a) noncumulative additive per time step, (b) noncumulative additive per string, (c) cumulative additive per time step, (d) noncumulative multiplicative per time step, (e) noncumulativemultiplicative per string, and (f) cumulative multiplicative per time step. Experiments with cumulative per string noise do not converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Mean convergence in epochs as a function of noise amplitude a for recurrent networks with three-state neurons. The error bars show 90% confidence intervals, obtained by running 500 simulations for each data point. The convergence curves are for: (a) noncumulative additive per time step, (b) noncumulative additive per string, (c) cumulative additive per time step, (d) noncumulative multiplicative per time step, (e) noncumulative multiplicative per string, and (f) cumulative multiplicative per time step. Experiments with cumulative per string noise do not converge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>l2+</head><label></label><figDesc>Fig. 7. tive per time step. Best convergence/generalization for additive and multiplicative noises: (a) multiplicative noncumulative per time step and (b) additive cumula-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Multiplicative Per TimeStep and Per String: The above per string and per time step derivations can be easily extended to the multiplicative noise models by substituting wt t j k wa;k + Wz*JkAt,ijk.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>+ l)A(jk(?L + l)AITl,L} = (t + I ) ( . + l ) a 2 S t , k Inln can be used to simplify the error expression. time step case, each noise injection step is such that Multiplicative Per Time Step: For the multiplicative per t W t ; i J k = w;, nc1 + &amp;,ilk).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>r=O</head><label></label><figDesc>Fig. 8. (11) Best generalization for noncumulative per time step and per string noises: (a) Per string additive and (b) Per time step additive. (I) Best generalization for cumulative and noncumulative noises: (a) Cumulative additive per time step and (b) Noncumulative additive per time step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Generalization percent error rate versus convergence in epochs. The curves are obtained by using the data in previous figures (recall 500 simulations for each data point). The lines represent the following: -noncumulative per time step, -. noncumulative per string, _.. cumulative per time step. The plots represent the following: (a) additive noise on four-state network, (b) multiplicative noise on four-state network, (c) additive noise on three-state network, and (d) multiplicative noisc on three-state network. Noise magnitude increases from left to right in each curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Generalization percent error versus convergence in epochs for the RND6 grammar. Additive, noncumulative, per time step noiae is inserted into second-order recurrent networks with IO state neurons. Maximum noise magnitude T t t increases from point Q ( r n = 0) to point b ( m = 2.0).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hanson developed a stochastic version of the delta rule which adapted weight means and standard deviations instead of clean weight values, and demonstrated improved convergence rates [ 131.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>which per time step non-cumulative per time step cumulative per string non-cumulative per string cumulative IEEE</head><label></label><figDesc>al. [ 111 is used to minimize the error function. Basically, the network is trained on a subset of the training set, called the working set, TRANSACTIONS ON NEURAL NETWORKS, VOL. 7, NO. 6, NOVEMBER 1996 ...</figDesc><table><row><cell>tl W' + A1 W* + A, W' + A1 W' + A1 t 2 W' + A, W* + A, + A2 W* + A, + n2 + a3 . . . t 3 W" + A3 ... ... W' + Ai W ' + A i W*+2A1 W' f 3A1 . . .</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I SAMPLE</head><label>I</label><figDesc>NOISE INJECTION STEPS FOR ALL ADDITIVE NOISE MODELS AT TIME STEPS t i . t Z , and t 3 , w* IS THE NOISE FREE WEIGHT A'S ARE THE NOISE TERMS FOR EACH TIME STEP</figDesc><table><row><cell>gradually increases in size until the network is able to correctly</cell></row><row><cell>classify the entire training set. Strings from the working set are</cell></row><row><cell>presented to the network in alphabetical order. Incremental</cell></row><row><cell>learning reduces the training time required because often not</cell></row><row><cell>all the strings in the training set need to be seen before the</cell></row><row><cell>network can classify the entire set correctly. Training stops</cell></row><row><cell>when all strings in the training set are correctly classified.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 111 PER</head><label>111</label><figDesc>STRING NOISE INJECTION TERMS AND ERROR FUNCTION EXPANSION TERMS FOR CUMULATIVE AND NONCUMULATIVE NOISE FOR BOTH THE ADDITIVE AND MULTIPLICATIVE CASES w* IS THE NOISE FREE WEIGHT SET</figDesc><table><row><cell>t=O i j k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV GENERALIZATION</head><label>IV</label><figDesc>AND CONVERGENCE IMPROVEMENT CHECKS INDICATE ADDITIONAL ENHANCEMENTS TO THE SEARCH FOR SATURATED STATES/PROMISING WEIGHTS DUE TO EACH NOISE MODEL SIGN-INDEPENDENT ENFORCEMENT OF SATURATED STATES IMPROVES GENERALIZATION (COLUMN 2) THE ANTICIPATORY EFFECT IS GUARANTEED ONLY W H E N THERE ARE EITHER N O WEIGHT TERMS IN THE SECOND-ORDER DERIVATIVES OF TABLES 11 AND 111, OR THE WEIGHT TERMS ARE ALWAYS POSITIVE (COLUMN 3) THE ABSENCE OF WEIGHT TERMS IN TABLES I1 AND 111 IMPROVES CONVERGENCE/GENERALIZATION BY NOT CONSTRAINING THE WEIGHTS TO SMALL AMPLITUDES (COLUMN 4)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>M. Jabri and B. Flower, "Weight perturbation: An optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayer networks," IEEE Trans. NeuralNetworks, vol. 3,    where n and b are constants, X and Y are random variables, [I81 S . Judd and P. W. Munro, "Nets with unreliable hidden nodes learn error-correcting codes." in Advances in Neural Information ProceJsina</figDesc><table><row><cell>t n n \ Iq"'</cell><cell>1992.</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank and acknowledge the reviewers for valuable comments and suggestions, and acknowledge L. C. An for help in the simulations and useful discussions.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher Item Identifier S 1045-9227(96)07461-9.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A learning algorithm for Boltzmann machines</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="147" to="169" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What size net gives valid generalization?</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="151" to="160" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training with noise is equivalent to Tikhonov regularization</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="108" to="116" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An alternative to backpropagation: A simple rule of synaptic modification for neural.net training and memory</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Bremermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Univ. California at Berkeley Center Pure App. Math</title>
		<imprint>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Tech. Rep. PAM-483</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automated Reasoning: Essays in Honor of Woody</title>
		<editor>Bledsoe, R. S. Boyer</editor>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Kluwer</publisher>
			<biblScope unit="page" from="119" to="147" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note>How the brain adjusts synapses-Maybe</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Event-dependent control of&apos; noise enhances learning in neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Burton</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mpitsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="627" to="637" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A fast stochastic error-descent algorithm for supervised learning and optimization</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cauwenberghs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Y L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Infiirmation Processing Systems 5</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Touretzky</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA; San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1990">1993. 1990</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="6053" />
		</imprint>
	</monogr>
	<note>Advances in Neural Infijrmution Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ModelLfree distributed learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="58" to="70" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Summed weight neuron perturbation: An O ( n ) improvement over weight perturbation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Flower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jabri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Cowan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning and extiacting finite-staw dutoinata with second-order recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">2</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lcc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="848" to="851" />
			<date type="published" when="1992">1992. 1994</date>
		</imprint>
	</monogr>
	<note>Neural Computu.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A stochastic version of the delta rule</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="265" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning and relearning in Boltzmann machines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="282" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boltzmann machines: Constraint satisfaction networks that learn</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Ackley</surname></persName>
		</author>
		<idno>CMU-CS-84-119</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dep., Carnegie-Mellon Univ</title>
		<imprint>
			<date type="published" when="1984-05">May 1984</date>
			<pubPlace>Pittsburgh, PA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1990-01">Jan. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimization by simulated annealing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kirkpatrick</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Galett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Vecchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci</title>
		<imprint>
			<biblScope unit="volume">220</biblScope>
			<biblScope unit="page" from="671" to="680" />
			<date type="published" when="1983-05">May 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Moody</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hanson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Lippmann</surname></persName>
		</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="450" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Experimental comparison of the effect of order in recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">B</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Pattern Recognition and Art$ca/ Zntell., Special Issue on Neural Networks and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="849" to="872" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fault tolerance of the backpropagation neural network trained on noisy inputs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">1</forename><surname>Minnix</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. lnt. Joinr Con$ Neural Nemorks</title>
		<meeting>lnt. Joinr Con$ Neural Nemorks</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="792" to="802" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fault-tolerant implementation of finitestate automata in recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Omlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Dep., Rensselaer Polytechnic Inst</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<pubPlace>Troy, NY, Tech</pubPlace>
		</imprint>
	</monogr>
	<note>Rep. 95-3</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pruning algorithms-A survey</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Gradient-based learning algorithms for recurrent networks and their computational complexity</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
		<editor>Backpropagntion: Theory, Architectures, and Applications, Y. Chauvin and D. E. Rumelhart</editor>
		<imprint>
			<date type="published" when="1995">1995</date>
			<publisher>Lawrence Erlbaum</publisher>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="433" to="486" />
			<pubPlace>Hillsdale, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fault tolerance in artificial neural networks</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Scquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Clay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joinr Coqf Neural Networks</title>
		<meeting>Int. Joinr Coqf Neural Networks</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="1" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Induction of finite-state languages using second-order recurrent networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Watrous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computa</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">406</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning finite-state machines with self-clustering recurrent networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurcrl Computa</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="1" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">95) received the B.S.E. degree in electrical engineering and a Certificate of Proficiency in biomedical engineering from Princeton University, NJ, in 1993. Currently, he is an Engineer at Katrix</title>
		<author>
			<persName><forename type="first">Kam-Chuen</forename><surname>Jim</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Inc. in</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">where he designs intelligent agents for computer games and other computer applications</title>
	</analytic>
	<monogr>
		<title level="m">His research interests include neural networks, dynamical systems, and multiagent learning</title>
		<meeting><address><addrLine>Princeton, NJ; Princeton, NJ</addrLine></address></meeting>
		<imprint/>
		<respStmt>
			<orgName>NEC Research Institute</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">80-M&apos;80-SM&apos;95), for a photograph and biography, see this issue</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1335</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Home (S&apos;85-M&apos;87), for a photograph and biography, see this issue</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bill</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
