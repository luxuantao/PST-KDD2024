<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Prefetching Without the Off-Chip Metadata</title>
				<funder ref="#_N3NTHBw">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Intel Corporation</orgName>
				</funder>
				<funder>
					<orgName type="full">NSF/Intel Partnership on Foundational Microarchitecture Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
							<email>haowu@cs.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Calvin</forename><forename type="middle">2019</forename><surname>Lin</surname></persName>
							<email>lin@cs.utexas.edu</email>
						</author>
						<author>
							<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
							<email>krishnendra.nathella@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Pusdesris</surname></persName>
							<email>joseph.pusdesris@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
							<email>dam.sunwoo@arm.com</email>
						</author>
						<author>
							<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
							<email>akanksha@cs.utexas.edu</email>
						</author>
						<author>
							<persName><surname>Temporal</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin Austin</orgName>
								<address>
									<country>Texas Krishnendra Nathella</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Arm Inc</orgName>
								<address>
									<settlement>Austin, Texas Joseph Pusdesris</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Arm Inc</orgName>
								<address>
									<settlement>Austin</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Arm Inc</orgName>
								<address>
									<settlement>Austin, Texas Akanksha Jain</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Austin Austin</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">The University of Texas</orgName>
								<address>
									<settlement>Austin Austin</settlement>
									<region>Texas</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<address>
									<addrLine>MICRO-52, October 12?16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<address>
									<addrLine>MICRO-52, October 12?16</addrLine>
									<postCode>2019</postCode>
									<settlement>Columbus</settlement>
									<region>OH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Prefetching Without the Off-Chip Metadata</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3352460.3358300</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data prefetching</term>
					<term>irregular temporal prefetching</term>
					<term>caches</term>
					<term>CPUs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Temporal prefetching offers great potential, but this potential is difficult to achieve because of the need to store large amounts of prefetcher metadata off chip. To reduce the latency and traffic of off-chip metadata accesses, recent advances in temporal prefetching have proposed increasingly complex mechanisms that cache and prefetch this off-chip metadata. This paper suggests a return to simplicity: We present a temporal prefetcher whose metadata resides entirely on chip. The key insights are (1) only a small portion of prefetcher metadata is important, and (2) for most workloads with irregular accesses, the benefits of an effective prefetcher outweigh the marginal benefits of a larger data cache. Thus, our solution, the Triage prefetcher, identifies important metadata and uses a portion of the LLC to store this metadata, and it dynamically partitions the LLC between data and metadata.</p><p>Our empirical results show that when compared against spatial prefetchers that use only on-chip metadata, Triage performs well, achieving speedups on irregular subset of SPEC2006 of 23.5% compared to 5.8% for the previous state-of-the-art. When compared against state-of-the-art temporal prefetchers that use off-chip metadata, Triage sacrifices performance on single-core systems (23.5% speedup vs. 34.7% speedup), but its 62% lower traffic overhead translates to better performance in bandwidth-constrained 16-core systems (6.2% speedup vs. 4.3% speedup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computer systems organization ? Processors and memory architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>By hiding the long latencies of DRAM accesses, data prefetchers are critical components of modern memory systems. One particularly elusive form of prefetching, known as temporal prefetching, is promising because it identifies arbitrary streams of correlated memory addresses, so it is suitable for irregular workloads, including those that use pointer-based data structures. Unfortunately, temporal prefetching essentially memorizes pairs of correlated addresses, so it requires metadata that are too large to fit on chip.</p><p>Early forms of temporal prefetching explored metadata organizations that reduce the cost of accessing off-chip metadata by amortizing metadata accesses across multiple prefetches <ref type="bibr" target="#b44">[45]</ref>. More recently, the Irregular Stream Buffer (ISB) <ref type="bibr" target="#b23">[24]</ref> introduced an alternative metadata representation that enabled portions of the metadata to be cached on chip. This metadata caching scheme was subsequently revised with a new metadata management scheme that includes a metadata prefetcher, reducing metadata traffic from 482% to 156% <ref type="bibr" target="#b46">[47]</ref>.</p><p>Unfortunately, the presence of off-chip metadata in these solutions presents three issues. First, even 156% metadata traffic overhead can impact performance, particularly in bandwidthconstrained multi-core systems. Second, off-chip metadata traffic consumes significant energy, since DRAM accesses consume more power than on-chip operations. Third, off-chip metadata adds hardware complexity because it requires (1) changes to the memory interface, (2) communication with the OS, and (3) methods for managing the metadata, which can include both a metadata cache replacement policy and a metadata prefetcher <ref type="bibr" target="#b46">[47]</ref>.</p><p>In this paper, we present a new temporal data prefetcher that addresses these issues by not maintaining any off-chip metadata. Our work is motivated by two observations. First, most of the coverage for state-of-the-art temporal prefetchers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref> comes from a small number of metadata entries, so it is possible to get substantial coverage without storing megabytes of metadata (see Figure <ref type="figure" target="#fig_0">1</ref>). Second, the marginal utility of the last-level cache (LLC) <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40]</ref> is typically outweighed by the benefits of an effective prefetcher. For example, for the irregular subset of SPEC2006, reducing the cache by 1 MB reduces performance by 7.4%, but a state-of-the-art irregular prefetcher with unlimited resources can improve performance by 41.7%. Therefore, if we can distinguish the important metadata from the unimportant metadata, it can be profitable to use large portions of the LLC to store important prefetcher metadata. Thus, our prefetcher, which we call the Triage prefetcher, repurposes a portion of the LLC as a metadata store, and any metadata that cannot be kept in the metadata store is simply discarded. To identify important metadata, Triage uses the Hawkeye replacement policy <ref type="bibr" target="#b24">[25]</ref>, which provides significant performance benefits for small metadata stores, as it identifies frequently accessed metadata over a long history of time. Of course, the ideal size of this metadata store varies by workload, so we also introduce a dynamic cache partitioning scheme that determines the amount of the LLC cache that should be provisioned for metadata entries.</p><p>By forsaking off-chip metadata and by intelligently managing onchip metadata, Triage offers vastly different tradeoffs than state-ofthe-art temporal prefetchers. For example, Triage reduces off-chip traffic overhead from 156.4% to 59.3%, it reduces energy consumption for metadata accesses by 4-22?, and it offers a much simpler hardware design. In Section 4, we show that in bandwidth-rich environments these benefits come at the cost of lower performance (due to limited prefetcher metadata), but they translate to better performance in bandwidth-constrained environments.</p><p>Triage's metadata organization has the added benefit of a simplified and compressed metadata representation. In particular, we find that without the need to store metadata off chip, tables <ref type="bibr" target="#b26">[27]</ref> are the most compact data structure for tracking correlated addresses, because they have no redundancy. By contrast, previous solutions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45]</ref> introduce varying degrees of metadata redundancy to facilitate off-chip metadata management. Since our metadata store competes for space in the LLC, this compactness has a direct performance benefit.</p><p>To summarize, this paper makes several contributions:</p><p>? We introduce Triage, the first PC-localized<ref type="foot" target="#foot_0">1</ref> temporal data prefetcher that does not use off-chip metadata. Triage reuses a portion of the LLC for storing prefetcher metadata, and it includes a simple adaptive policy for dynamically provisioning the size of the metadata store.</p><p>? We evaluate the Triage prefetcher using a highly accurate proprietary simulator for single-core simulations and the ChampSim simulator for multi-core simulations.  <ref type="bibr" target="#b46">[47]</ref>). ? On a 4-core system running CloudSuite server benchmarks, BO+Triage improves performance by 13.7%, compared to 8.6% for BO alone. ? On a 16-core system running multi-programmed irregular SPEC workloads, where bandwidth is more precious, Triage's speedup is 6.2%, compared to 4.3% for MISB. ? Triage also works well as part of a hybrid prefetcher that combines a regular prefetcher with a temporal prefetchers (24.8% for Triage+BO vs. 5.8% for BO alone on single-core systems, and 10.0% for BO+Triage vs. 4.4% for BO alone on 16-core systems). ? We outline and analyze the design space for temporal prefetchers along three dimensions, namely, on-chip storage, off-chip traffic, and overall performance, and we show that Triage provides an attractive design point with previously unexplored tradeoffs.</p><p>This paper is organized as follows. Section 2 places our work in the context of prior work. Section 3 then describes our solution, and Section 4 then presents our empirical evaluation. Finally, we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We now discuss related work in data prefetching. We start by contrasting our work with other temporal prefetchers before briefly discussing other classes of prefetchers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Temporal Prefetching</head><p>Temporal prefetchers are quite general because they learn address correlations, that is, correlations between consecutive memory accesses. Unfortunately, considerable state is required to memorize correlations among addresses. To reduce metadata requirements, some prefetchers forgo address correlation to learn weaker forms of correlation, such as delta correlation <ref type="bibr" target="#b32">[33]</ref>, tag correlation <ref type="bibr" target="#b19">[20]</ref>, or context-address pair correlation <ref type="bibr" target="#b35">[36]</ref>, but these simplifications limit the scope of memory access patterns that can be learned. Other prefetchers exploit address correlation by storing metadata in off-chip memory <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, but the use of off-chip metadata has limited the commercial viability of such prefetchers. Triage is the first data prefetcher that reaps the benefit of PC-localized address correlation?the most powerful form of temporal prefetching <ref type="bibr" target="#b23">[24]</ref>? without using any off-chip metadata.</p><p>Temporal prefetchers with off-chip metadata can be placed in three categories based on their metadata organization. The primary focus of all three categories is to reduce the overhead of accessing off-chip metadata. While elements of Triage borrow from this existing work, Triage is designed with a completely different design goal, which is to prioritize the efficiency of the on-chip metadata store. Therefore, Triage rethinks many design decisions for the on-chip setting, removing complexity where possible and adding new design components where necessary.</p><p>Table-Based Temporal Prefetchers. Joseph and Grunwald introduced the idea of prefetching correlated addresses in 1997 with their Markov Prefetcher <ref type="bibr" target="#b26">[27]</ref>. The Markov Prefetcher uses a table to record multiple possible successors for each address, along with a probability for each successor, but unfortunately, it was too large to be stored on-chip despite optimizations that reduced the size of the table <ref type="bibr" target="#b19">[20]</ref>.</p><p>Therefore, early temporal prefetchers explore designs that reduce the traffic and latency costs of accessing an off-chip Markov table <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b42">43]</ref> GHB-Based Temporal Prefetchers. Wenisch et al. find that tables are not ideal for organizing off-chip metadata because temporal streams can have highly variable lengths <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45]</ref>. Their STMS prefetcher <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> instead uses a global history buffer to record a history of past memory accesses in an off-chip circular buffer. The GHB reduces the latency of off-chip metadata accesses by amortizing the cost of off-chip metadata lookup over long temporal streams, and it reduces metadata traffic by probabilistically updating the off-chip structures. While the GHB improves significantly over table-based solutions, it suffers from three drawbacks that are addressed by Triage: <ref type="bibr" target="#b0">(1)</ref> The GHB makes it infeasible to combine address correlation with PC-localization, which is a technique to improve predictability by correlating addresses that belong to the same PC, (2) its metadata cannot be cached because it is organized as a FIFO buffer, and (3) it incurs metadata traffic overhead of 200-400%.</p><p>Irregular Stream Buffer. The Irregular Stream Buffer (ISB) combines address correlation with PC-localization by proposing a new off-chip metadata organization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47]</ref>. In particular, ISB maps PClocalized correlated address pairs to consecutive addresses in a new address space, called the structural address space. Furthermore, ISB caches a portion of the physical-to-structural address mappings on chip by synchronizing the contents of the on-chip metadata cache with the TLB and by hiding the latency of off-chip metadata accesses during TLB misses. While ISB significantly improves coverage and accuracy of temporal prefetchers, it still incurs metadata traffic overheads of 200-400%, and its metadata cache utilization is quite poor due to the absence of spatial locality in the metadata cache.</p><p>MISB <ref type="bibr" target="#b46">[47]</ref> addresses these issues by divorcing ISB's metadata cache from the TLB. In particular, MISB manages the metadata cache at a fine granularity, and it hides the latency of off-chip metadata accesses by employing highly accurate metadata prefetching. As a result, MISB reduces the traffic overhead of temporal prefetchers to 156%. Like MISB, Triage uses fine-grained metadata caching, but there are several differences between MISB and Triage: (1) Triage uses a space-efficient table-based organization that is more suited for on-chip metadata (MISB's metadata footprint is 2? larger than Triage's metadata footprint because it tracks each correlation in two entries, a physical to structural address mapping, and a structural to physical address mapping), (2) Triage uses a smart metadata management policy for its on-chip metadata, and (3) Triage has no metadata traffic overhead.</p><p>Finally, some prefetchers do store their metadata in on-chip caches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44]</ref>, but the metadata storage requirements for these prefetchers is relatively small (hundreds of KB), so there was no question that they would be stored somewhere on chip. By contrast, Triage shows how prefetchers whose metadata are too large to fit on chip can avoid storing off-chip metadata. Finally, metadata optimizations have been explored in the context of memory compression <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b47">48]</ref>, but these techniques are orthogonal to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Non-Temporal Prefetching</head><p>Many prefetchers predict sequential <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b41">42]</ref> and strided <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref> accesses, and while this class of prefetchers has enjoyed commercial success due to their extremely compact metadata, their benefits are limited to regular memory accesses.</p><p>Some irregular memory accesses can be prefetched by exploiting spatial locality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>, such that recurring spatial patterns can be prefetched across different regions in memory. For example, the SMS prefetcher <ref type="bibr" target="#b43">[44]</ref> uses on-chip tables to correlate spatial footprints with the program counter that first accessed a memory region. These spatial locality-based prefetchers tend to be highly aggressive, issuing prefetches for many lines in a region at once. More importantly, they are limited to a very special class of irregular accesses that does not include access to pointer-based data structures, such as trees and graphs.</p><p>Other prefetchers directly target pointers by either using compiler hints or hardware structures to detect pointers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>. For example, Content Directed Prefetching <ref type="bibr" target="#b11">[12]</ref> searches the content of cache lines for pointer addresses and eagerly issues prefetches for all pointers. Such prefetchers waste bandwidth as they prefetch many pointers that will not be used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OUR SOLUTION</head><p>Triage repurposes on-chip cache space to store prefetcher metadata. To effectively utilize valuable on-chip cache space, Triage considers the following design questions:</p><p>? How should metadata be represented to maximize space efficiency? ? Which metadata entries are likely to be the most useful?</p><p>? How much of the last-level cache should be dedicated to the metadata store?</p><p>We now explain how our solution addresses these questions.</p><p>Metadata Representation. Triage learns PC-localized correlated address pairs and records them in a table. For example, the top side of Figure <ref type="figure">2</ref> shows a stream of memory references that is segregated into two PC-localized streams, and the bottom side shows the conceptual organization of Triage's metadata. In particular, each entry in Triage maps an address to its PC-localized neighbor.</p><formula xml:id="formula_0">Global Stream A X Y B Z C PC 1 A B C PC 2 X Y Z Time Addr Neighbor A B B C X Y Y Z Figure 2: Triage's metadata organization.</formula><p>While tables are a poor choice for organizing off-chip metadata (see Section 2), their space efficiency makes them an ideal choice for organizing on-chip metadata. In particular, compared to other metadata organizations <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>, our table-based organization avoids metadata redundancy by representing each correlated address pair only once. One drawback of our table-based organization is that higher degree prefetching requires multiple metadata lookups, but this penalty is significantly lower when the metadata resides completely on chip (~20 cycles for accessing each LLC-resident metadata entry vs. 150-400 cycles for accessing each off-chip metadata entry.) Section 3.2 provides details about Triage's use of compact address representations to further reduce the metadata footprint.</p><p>Metadata Replacement. Triage's metadata replacement policy manages the contents of its on-chip metadata store. We build Triage's metadata replacement policy on three observations. First, most metadata reuse can be attributed to a few metadata entries (see Figure <ref type="figure" target="#fig_0">1</ref>). Second, even among the metadata entries that are frequently reused, fewer still account for prefetches that are not redundant, that is, prefetch requests that do not hit in the cache. Finally, metadata should be managed and evicted at a fine granularity because Triage targets irregular memory accesses, which exhibit poor spatial locality.</p><p>To accomplish these goals, we modify Hawkeye <ref type="bibr" target="#b24">[25]</ref>, a state-ofthe-art cache replacement policy, which learns from the optimal solution for past memory references. To emulate the optimal policy for past memory references, Hawkeye examines a long history of past cache accesses (8? the size of the cache), and it uses a highly efficient algorithm to reproduce the optimal solution. used to train a PC-based predictor; the predictor learns whether loads by a given load instruction (PC) are likely to hit or miss with the optimal solution. On new cache accesses, the predictor informs the cache whether the line should be inserted with high priority or low priority.</p><p>Because the Hawkeye policy can capture long-term reuse, it is a good fit for Triage, where the replacement policy must not be overwhelmed by the many useless metadata entries. We modify the Hawkeye policy so that the policy is trained positively only when the metadata yields a prefetch that misses in the cache. We accomplish this by delaying Hawkeye's training when the prefetch request associated with a metadata entry is actually issued to memory. If the prefetch request hits in the cache, then the metadata reuse is ignored and is not seen by any component of the Hawkeye policy.</p><p>In Section 3.2, we explain how Triage is able to manage metadata at a finer granularity than the line size of the last-level cache.</p><p>Adjusting the Size of the Metadata Store. To avoid interference between application data and metadata, we partition the last-level cache by assigning separate ways to data and metadata. Since different applications require different metadata store sizes, our solution dynamically determines the number of ways that should be allocated to metadata. Our dynamic cache allocation scheme is based on two insights. First, the OPTgen component of Hawkeye can cheaply model the optimal hit rate at different metadata store sizes, so OPTgen can be used to estimate the profitability of devoting more cache space to metadata entries. Second, the optimal hit rate scales linearly with cache size, so we need not estimate optimal hit rate at every possible metadata store size?we can instead estimate hit rate at two points and interpolate.</p><p>More concretely, we maintain two copies of OPTgen (each copy consumes 1KB space), and we use these copies as sandboxes to evaluate the optimal hit rate at different metadata store sizes. If Triage finds that an increase in the metadata store size increases optimal metadata hit rate by more than 5%, it increases the number of ways that are allocated to metadata entries. Similarly, if Triage finds that a reduction of the metadata store size decreases the metadata hit rate by less than 5%, it reduces the number of ways allocated to metadata entries. For simplicity, Triage chooses between three possible allocations for metadata store(0 MB, 512 KB and 1 MB), but our scheme can be extended to any number of partitioning configurations by time-sharing the OPTgen copies to evaluate different metadata store sizes.</p><p>The partition sizes are re-evaluated periodically to adapt to changes in program phases.  Figure <ref type="figure" target="#fig_2">4</ref> shows the overall design of Triage, where we see that a portion of the LLC is re-purposed for Triage's metadata store. On every LLC access, the metadata portion of the LLC is probed with the incoming address to check for a possible metadata cache hit 1 . If the metadata entry is found, it is read to generate a prefetch request 2 . Regardless of whether the load resulted in a metadata hit or miss, the Training Unit is updated (as explained below), and the newly trained metadata entry is added (or updated) in the metadata store 3 . The metadata replacement state is updated on metadata misses and metadata hits that generate a successful prefetch 4 , and the metadata replacement state periodically recomputes the amount of LLC that should be used as a metadata cache 5 . We now explain these operations in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Operation</head><p>Training. The Training Unit keeps the most recently accessed address for each PC. When a new access B arrives for a given PC, the Training Unit is queried for the last accessed address A by the same PC. Addresses A and B are then considered to be correlated, and the entry (A, B) is stored in Triage's metadata store; the metadata store is indexed by the first address in the pair (A in this example).</p><p>To avoid changing entries due to noisy data, each mapping in Triage's metadata store has an additional 1-bit confidence counter. If the Training Unit determines that A's neighbor differs from the value in the metadata store, then the confidence counter is decremented. If the Training Unit determines that A's neighbor matches the value in the metadata store, then the confidence counter is incremented. The neighbor is changed only when the confidence counter drops to 0.</p><p>Prediction. Upon arrival of a new address A, Triage indexes the metadata by address A to find any available metadata entry. If an entry (say (A, B)) is found, Triage issues a prefetch for B. If an entry is not found, no prefetch is issued.</p><p>Metadata Replacement Updates. Our metadata replacement is based on the Hawkeye policy <ref type="bibr" target="#b24">[25]</ref>, and like the Hawkeye policy, our metadata replacement policy is trained on the behavior of a few sampled sets. The metadata replacement predictors are trained on all metadata accesses, except those, that result in redundant prefetches. The replacement predictors are probed on all metadata accesses, including hits and misses, to update the per-metadataentry replacement state. For more details on how the Hawkeye policy works, we refer the reader to the original paper <ref type="bibr" target="#b24">[25]</ref>.</p><p>Metadata Partition Updates. Triage partitions the cache between data and metadata by using way partitioning. The partitions are recomputed every 50,000 metadata accesses. If Triage decides to increase the amount of metadata store, dirty lines are flushed and the newly allocated/deallocated portion of the cache is marked invalid immediately. If Triage decides to decrease the amount of metadata store, lines with metadata entries are marked invalid.</p><p>For shared caches, Triage computes the metadata allocation for each core individually (by using per-core OPTgens) and allots the corresponding portion of the LLC for each core's metadata. For example, if two cores are sharing a 4MB cache, and if core 0 wants 1MB of metadata and core 1 wants 512KB of metadata, then Triage allocates 1.5MB of the shared LLC for metadata, and it partitions the metadata store in a 2:1 ratio among the two cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hardware Design</head><p>Each metadata entry in Triage is 4 bytes long, but LLC line sizes are typically 64 to 128 bytes. Triage's metadata entries within an LLC line must be organized at a fine granularity because metadata entries for irregular prefetchers do not exhibit spatial locality. Therefore, we store multiple tagged metadata entries within each LLC cache line. For example, for a 64 byte LLC line, we store 16 metadata entries within a cache line. The metadata entries within a cache line are stored in the following format: tag-entry-tag-entry-? ? ? -tag-entry. On a metadata lookup, we first choose a physical LLC cache line from the metadata store, and we then find the relevant metadata entry by comparing the sub-tags within each cache line.</p><p>To store the metadata within 4 bytes, we use a compressed tag. To understand our compressed tag, realize that each physical address has a cache line offset of 6 bits and set_id of 11 bits, and the remaining bits are tags. We construct a lookup table to compress the tag to 10 bits. Thus, each metadata entry records the compressed tag of the trigger address and the compressed tag and set_id of the next address, which require a total of 31 bits<ref type="foot" target="#foot_1">2</ref> . The remaining bit is used as a confidence counter.</p><p>To identify the finer-grain metadata entries within a cache line, we require additional logic in the form of comparators and multiplexors. The extra logic is similar to that used in the Amoeba-Cache <ref type="bibr" target="#b30">[31]</ref> and may incur additional latency or pipeline stages, but only for metadata accesses. In Section 4.6, we describe a sensitivity study that penalizes LLC access latencies for both data and metadata by up to 6 cycles, and we see that the performance impact is minimal (around 1% lower speedup on average for the irregular SPEC workloads).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION 4.1 Methodology</head><p>We evaluate Triage on single-core configurations using a cycle-level industrial simulator that models ARMv8 AArch64 CPUs and that has been correlated to be highly accurate against commercial CPU designs. The parameters of the CPU and memory system model used in the simulation are shown in Table <ref type="table" target="#tab_3">1</ref>. This model uses a simple memory model with fixed latency, but it models memory bandwidth constraints accurately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Core</head><p>Out-of-order, 2GHz, For multi-core evaluation of Triage, we use ChampSim <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29]</ref>, a trace-based simulator that includes an out-of-order core model and a detailed memory system. ChampSim's cache subsystem includes FIFO read and prefetch queues, with demand requests having higher priority than prefetch requests. The main memory model simulates data bus contention, bank contention, and bus turnaround delays; bus contention increases memory latency. Our modeled processor for ChampSim also uses the configuration shown in Table <ref type="table" target="#tab_3">1</ref>. We confirm that the performance trends on the two simulators are the same.</p><p>Benchmarks. We present single-core results for a subset of SPEC2006 benchmarks that are memory bound <ref type="bibr" target="#b17">[18]</ref> and are known to have irregular access patterns <ref type="bibr" target="#b23">[24]</ref>. For SPEC benchmarks we use the reference input set. For all single-core benchmarks, we use SimPoints <ref type="bibr" target="#b40">[41]</ref> to find representative regions. Each SimPoint is warmed up for 200 million instruction and run for 50 million instructions, and we generate at most 10 SimPoints for each SPEC benchmark.</p><p>We present multi-core results for CloudSuite <ref type="bibr" target="#b15">[16]</ref> and multiprogrammed SPEC benchmarks. For CloudSuite, we use the traces provided with the 2 nd Cache Replacement Championship. The traces were generated by running CloudSuite in a full-system simulator to intercept both application and OS instructions. Each Cloud-Suite benchmark includes 6 samples, where each sample has 100 million instructions. We warm up for 50 million instructions and measure performance for the next 50 million instructions.</p><p>For multi-programmed SPEC simulations, we simulate 4, 8 and 16 cores, such that each core runs a benchmark chosen uniformly randomly from all memory-bound benchmarks, including both regular and irregular programs. Overall, we simulate 80 4-core mixes, 80 8-core mixes, and 80 16-core mixes. Of the 80 mixes, 30 mixes include random mixes of irregular programs only, and the remaining 50 mixes include both regular and irregular programs. For each mix, we simulate the simultaneous execution of SimPoints of the constituent benchmarks until each benchmark has executed at least 30 million instructions. To ensure that slow-running applications always observe contention, we restart benchmarks that finish early so that all benchmarks in the mix run simultaneously throughout the execution. We warm the cache for 30 million instructions and measure the behavior of the next 30 million instructions.</p><p>Prefetchers. We evaluate Triage against two state-of-the-art onchip prefetchers, namely, Spatial Memory Streaming (SMS) <ref type="bibr" target="#b43">[44]</ref> and the Best Offset Prefetcher (BO) <ref type="bibr" target="#b31">[32]</ref>. SMS captures irregular patterns by applying irregular spatial footprints across memory regions. BO is a regular prefetcher that won the Second Data Prefetching Championship <ref type="bibr" target="#b0">[1]</ref>.</p><p>We also evaluate Triage against existing off-chip temporal prefetchers, namely, Sampled Temporal Memory Streaming (STMS) <ref type="bibr" target="#b44">[45]</ref>, Domino <ref type="bibr" target="#b3">[4]</ref>, and MISB <ref type="bibr" target="#b46">[47]</ref>. STMS, Domino, and MISB represent the state-of-the-art in temporal prefetching. For simplicity, we model idealized versions of STMS and Domino, such that their off-chip metadata transactions complete instantly with no latency or traffic penalty. Our performance results for these prefetchers represent the upper bound performance of these prefetchers. For MISB, we faithfully model the latency and traffic of all metadata requests.</p><p>We evaluate two versions of Triage, static and dynamic. The static version picks a fixed metadata store size that gives the best average performance and statically partitions the LLC using this size; we find that the best static metadata store size for a 2MB LLC is 1MB on both simulators. The dynamic version of Triage modulates the size of the metadata store dynamically as described in Section 3.</p><p>All prefetchers train on the L2 access stream, and prefetches are inserted into the L2. The metadata is stored in L3. Unless specified, all prefetchers use a prefetch degree of 1, which means that they issue at most one prefetch on every trigger access.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison With Prefetchers That Store Metadata On Chip</head><p>Figure <ref type="figure" target="#fig_3">5</ref> shows that Triage outperforms state-of-the-art prefetchers that use only on-chip metadata. In particular, Triage achieves a speedup of 23.4% and 23.5% for the static and dynamic configurations, respectively, whereas BO and SMS see a speedup of 5.8% and 2.2%, respectively. Triage's superior performance can be explained by its higher coverage (42.0% for Triage vs. 13.0% for BO and 4.6% for SMS) and higher accuracy (77.2% for Triage vs. 43.3% for BO and 39.6% for SMS) as shown in Figure <ref type="figure" target="#fig_4">6</ref>.  Triage-Dynamic is slightly better than Triage-Static as it modulates the metadata store size for mcf and sphinx3. As we will see later, the benefit of our dynamic scheme is most pronounced in a shared cache setting where the cache is shared by both regular and irregular benchmarks.</p><p>Figure <ref type="figure">7</ref> sheds more insight on Triage's performance benefits, as we see that the performance benefit of irregular prefetching significantly outweighs the performance loss of reduced LLC capacity. In particular, we see that an optimistic version of Triage that is given a 1 MB on-chip metadata store in addition to its usual LLC capacity achieves a 31.2% speedup. On the other hand, a system with no Triage and a reduced LLC capacity of 1 MB achieves 7.4% lower performance than the baseline. This loss in performance is easily compensated by Triage's benefits as Triage sees an overall speedup of 23.4% with a 1 MB metadata store and a 1 MB LLC. For completeness, Figure <ref type="figure">8</ref> compares all prefetchers on the remaining memory-intensive SPEC 2006 benchmarks <ref type="foot" target="#foot_3">3</ref> . Because these benchmarks are regular, Triage does not outperform BO, but we see that Triage's dynamic partitioning avoids hurting performance on most benchmarks as it picks a 512 KB metadata store instead of a 1 MB metadata store. On bzip2, Triage hurts performance because it detects metadata reuse, but the prefetches issued by these metadata entries are not enough to cover the loss in LLC space. As future work, more sophisticated partitioning schemes that account for cache utility more accurately could help improve Triage in these scenarios.</p><p>Sensitivity to Replacement Policy. Figure <ref type="figure" target="#fig_7">9</ref> compares the performance of Triage at different metadata store sizes and with different replacement policies (assuming no loss in LLC capacity). We make two observations. First, with just 1MB of metadata store, Triage achieves 75% of the performance of an idealized PC-localized temporal prefetcher, which is significant because typical temporal prefetchers consume tens of megabytes of off-chip storage. This result confirms the main insight of Triage that most prefetches can be attributed to a small percentage of metadata entries. Our second observation is that a smart replacement policy can improve the effectiveness of Triage at smaller metadata cache sizes, but when the metadata cache is sufficiently large (1 MB), the gap between LRU and Hawkeye shrinks. In particular, with a 256 KB metadata cache, Triage with an LRU policy achieves 7.7% speedup whereas Triage with the Hawkeye policy sees a 13.7% speedup.</p><p>Hybrid Prefetchers. Since Triage targets irregular memory accesses, it makes sense to evaluate it as a hybrid with regular memory prefetchers, such as BO. Figure <ref type="figure" target="#fig_9">10</ref> shows that a BO+Triage hybrid outperforms BO (24.8% speedup for BO+Triage vs. 5.8% for BO), which shows that Triage successfully prefetches lines that BO cannot.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With Prefetchers That Use Off-Chip Metadata</head><p>Existing temporal data prefetchers use tens of megabytes of off-chip metadata. Compared to these prefetchers, Triage provides a simpler design and a more desirable tradeoff between performance and off-chip metadata traffic. Figure <ref type="figure" target="#fig_0">11</ref> compares Triage against overly optimistic idealized versions of STMS and Domino and against a realistic version of MISB <ref type="bibr" target="#b46">[47]</ref>. We see that Triage outperforms idealized STMS and Domino (23.5% for Triage vs 14.5% for Domino and 15.3% for STMS). Triage doesn't match MISB's 34.7% performance, but we see that it incurs much less traffic overhead (bottom graph in Figure <ref type="figure" target="#fig_0">11</ref>). In particular, compared to a baseline with a 2 MB cache and no prefetching, Triage increases traffic by 59.3%, whereas STMS, Domino and MISB increase traffic by 482.9%, 482.7%, and 156.4% respectively.  To put these results in context, Figure <ref type="figure" target="#fig_12">12</ref> compares all temporal and the Best Offset (BO) prefetcher along two axes, namely performance and traffic overhead. STMS, Domino, and MISB all use off-chip metadata, so they incur high off-chip traffic overheads and are in general more complex due to the complications introduced by storing metadata off chip. Triage outperforms STMS and Domino while eliminating metadata overheads. Triage has lower performance than MISB, but it reduces traffic by more than half, offering an attractive design point for temporal prefetching. In fact, Triage's traffic overhead of 59.3% is comparable to BO's 33.8% traffic overhead. BO's traffic overhead can be attributed to its large volume of inaccurate prefetches on irregular programs. By contrast, Triage is more accurate, but it incurs traffic due to an effectively smaller LLC.  Energy Evaluation. Triage is more energy-efficient than other temporal prefetchers. Figure <ref type="figure" target="#fig_13">13</ref> shows that Triage's metadata accesses are 4 -22? more energy efficient than MISB's. To estimate the energy consumption of Triage's metadata accesses, we count the number of LLC accesses for metadata, assuming 1 unit of energy for each LLC access. To estimate the energy consumption of MISB's memory accesses, we count the number of off-chip metadata accesses and multiply it by the average energy of a DRAM access. Since a DRAM access can consume anywhere from 10? to 50? more energy than an LLC access <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>, we assume that each DRAM access consumes 25 units of energy, and we add error bars to account for the lower bound (10 units of energy per DRAM access) and upper bound (50 units of energy DRAM access) of MISB's overall energy consumption.</p><p>At higher degrees, Triage's table-based design requires multiple LLC lookups, which will increase its overall energy requirements. In particular, we find that Triage's energy consumption doubles at degree 8, which is still much more energy efficient than MISB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation on Server Workloads</head><p>To evaluate its effectiveness for server workloads, we evaluate Triage on the CloudSuite benchmark suite running on a 4-core system (See Figure <ref type="figure" target="#fig_14">14</ref>). On the highly irregular Cassandra, Classification, and Cloud9 benchmarks, Triage improves performance by 7.8%, whereas BO improves performance by 4.8% and SMS sees no performance gains. On the more regular Nutch and Streaming benchmarks, SMS and BO do well (10.9% and 14.7% performance improvement), whereas Triage sees no performance improvement because temporal prefetchers cannot prefetch compulsory misses.</p><p>In a hybrid setting, BO and Triage compose well, as Triage works well for the irregular benchmarks and BO works well for the regular ones. In particular, a BO+Triage hybrid outperforms all other prefetchers as it improves performance by 13.7%, whereas BO alone improves performance by only 8.6% (50.6% miss reduction for BO+Triage vs. 31.4% miss reduction for BO). A BO+SMS hybrid (5.8% speedup) does not provide much improvement and, in fact, degrades performance compared to BO alone, because both BO and SMS target regular access patterns, so when they are combined, their collective inaccuracy creates more contention for bandwidth.</p><p>Figure <ref type="figure" target="#fig_14">14</ref> also shows that Triage-Dynamic provides benefit over a static version of Triage in this setting, so we conclude that our dynamic scheme makes good decisions about trading off cache space for metadata storage. This benefit is most pronounced for the irregular benchmarks (Cassandra, Classification, and Cloud9) where the dynamic version outperforms the static scheme by 2.3% (7.8% for Triage-Dynamic vs. 5.5% for Triage-Static).   These results can be explained by noting that the LLC is a more valuable resource in shared systems. Triage-Dynamic works well in this setting because it can (1) modulate the portion of the LLC dedicated to metadata depending on the expected benefit of irregular prefetching, and (2) distribute the available metadata store among individual applications such that the application which benefits the most from irregular prefetching gets a larger portion of the metadata store.</p><p>Comparison With Prefetchers That Store Metadata On Chip. Figure <ref type="figure" target="#fig_17">16</ref> shows that Triage compares favorably to spatial prefetchers, such as BO, on 4-core systems. In particular, a combination of BO and Triage-Dynamic outperforms BO alone on a 4-core system, as we see that BO improves performance by 10.6%, Triage-Dynamic improves performance by 10.2%, and a combination of BO and Triage-Dynamic improves performance by 15.9%. These results reiterate that Triage can prefetch irregular memory accesses that BO cannot. We observe similar trends on 8-core and 16-core systems. On an 8-core system, BO+Triage improves performance by 12.6% (vs. 7.4% for BO alone), and on a 16-core system, BO+Triage improves performance by 10.0% (vs. 4.4% for BO alone).</p><p>Comparison With Prefetchers That Store Metadata Off Chip. Figure <ref type="figure" target="#fig_18">17</ref> compares the average speedup of Triage with MISB on 2-core, 4-core, 8-core, and 16-core systems where the cache is shared among different irregular programs. We see that while MISB outperforms Triage on a 2-core system (12.1% for Triage vs. 16.0% for MISB), its benefit shrinks on an 8-core system (8.8% for Triage vs. 10.0% for MISB). On a 16-core system, Triage outperforms MISB (6.2% for Triage vs. 4.3% for MISB). These trends suggest that MISB's performance does not scale well to bandwidth-constrained environments because of its large metadata traffic overheads. By contrast, Triage's performance scales well with higher core counts. Comparison On Mixes With Regular Programs. For completeness, Figure <ref type="figure" target="#fig_21">18</ref> shows that Triage composes well with BO when the multiprogrammed mixes include both regular and irregular programs. In particular, for a 4-core system, BO+Triage improves performance by 23%, whereas BO alone improves performance by 19.3%. Triage alone does not work well in this setting (4.3% speedup) because it cannot prefetch compulsory misses for regular programs.   The dynamic version of Triage is essential in these scenarios because the cache is shared among irregular programs?which benefit from Triage?and regular programs?which do not benefit from Triage. For regular programs, a static version of Triage would reduce effective LLC capacity without providing much prefetching benefit. Figure <ref type="figure" target="#fig_22">19</ref> shows the number of ways allocated to each core on this 4-core system, and we see that (1) the total number of ways allocated to the metadata store varies across mixes, and (2) each application receives varying amounts of metadata space depending on a dynamic estimate of the usefulness of the metadata.</p><formula xml:id="formula_2">M I X 1 2 M I X 5 0 M I X 4 6 M I X 2 0 M I X 1 3 M I X 1 4 M I X 1 9 M I X 4 1 M I X 7 M I X 2 6 M I X 3 0 M I X 9 M I X 8 M I X 2 9 M I X 1 7 M I X 4 0 M I X 2 2 M I X 4 4 M I X 3 2 M I X 3 5 M I X 2 3 M I X 1 M I X 2 4 M I X 2 8 M I X</formula><p>For example, the leftmost bar in Figure <ref type="figure" target="#fig_22">19</ref> represents a mix with 1 regular program (milc on core 0), two irregular programs (xalancbmk on core 1 and omnetpp on core 3), and one regular/irregular program (bzip2 on core 2). For this mix, Triage-Dynamic allocates an average of 22% of the LLC capacity to metadata (the maximum metadata allocation can go up to 50% of the LLC). It distributes this metadata store appropriately among different workloads: Milc is not allocated any metadata space because it does not benefit from irregular prefetching, omnetpp is allocated the maximum metadata space (10% of total LLC capacity) because it benefits the most from irregular prefetching, and the other benchmarks are allocated 6% each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Sensitivity Studies</head><p>Sensitivity to Degree. Figure <ref type="figure" target="#fig_24">20</ref> shows the performance of our prefetchers at different prefetch degrees. As we increase degree from 1 to 8, Triage's performance grows from 23.5% to 36.2%, and its performance saturates at a degree of 8. By comparison, BO and SMS at degree 8 improve performance by 11.1% and 7.0% (over a baseline with no prefetcher), respectively. Triage is consistently more accurate than BO: At higher degrees, BO's accuracy is only 21.5% (vs. 50.5% for Triage).</p><p>Sensitivity to Epoch Length. Triage-Dynamic periodically recomputes the fraction of the LLC that should be repurposed for metadata. We find that Triage's metadata partitions are stable over long periods of time and that resizing partitions more frequently than 50,000 LLC accesses does not affect performance. Thus, we conclude that while metadata partition sizes vary significantly across benchmarks, they tend to change infrequently for a given benchmark.</p><p>Sensitivity to Changes in Cache Latency. As discussed in Section 3.2, the metadata portion of the LLC is managed at a finer granularity than the data portion of the LLC. We expect these modifications to only affect the latency of metadata accesses, not the latency of data accesses. However, to study the worst-case scenario, we increase the latency of accessing both data and metadata up to 6 cycles, and we find that even with 6 cycles of additional latency, we only see a 1% drop in performance (normalized to a baseline with no prefetching and no extra latency).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>Temporal prefetchers can be highly effective for irregular memory access patterns, but they have yet to be commercially adopted because they need to store large amounts of metadata in DRAM. This off-chip metadata adds complexity, adds energy overhead, and incurs significant traffic. In this paper, we have presented Triage, m i x 4 7 m i x 4 9</p><p>LLC ways allocated to metadata   a temporal prefetcher that removes the off-chip metadata requirement and stores metadata only on chip, making it practical to implement.</p><note type="other">Core0 Core1 Core2 Core3</note><p>To store its metadata on chip, Triage re-purposes the LLC as a metadata store because for irregular workloads, the marginal benefit of a larger cache is significantly outweighed by the benefit of an effective temporal prefetcher. To ensure that Triage does not degrade performance for workloads that do not benefit from irregular prefetching, Triage includes a dynamic mechanism for intelligently partitioning the cache between data and metadata based on the utility of the metadata.</p><p>We have shown that when compared with prefetchers that only use on-chip metadata, Triage provides a significant performance advantage (23.5% speedup for Triage vs. 5.8% for BO). When compared with state-of-the-art temporal prefetchers that use off-chip metadata, Triage significantly reduces traffic overhead (59.3% traffic overhead for Triage vs. 156.4% for MISB). This traffic reduction translates to better speedup in bandwidth-constrained 16-core systems, where Triage outperforms MISB despite having access to a metadata store that is orders of magnitude smaller. Triage's traffic overhead is comparable to state-of-the-art spatial prefetchers, such as BO, which speaks to its practicality. Overall, Triage provides a new and attractive design point for temporal prefetchers with vastly different tradeoffs than previous solutions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Metadata reuse distribution for the mcf benchmark: For an execution with 60K metadata entries, only 15% of metadata entries are reused more than 15 times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Triage's metadata replacement is based on the Hawkeye [25] cache replacement policy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overview of Triage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Triage outperforms BO and SMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Triage improves coverage and accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Breakdown of Triage's Performance Improvements</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Sensitivity to Metadata Store Size (assuming no loss in LLC capacity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Triage performs well as part of a hybrid prefetcher.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>. 6 SpeedupFigure 11 :</head><label>611</label><figDesc>Figure 11: Triage reduces traffic compared to off-chip temporal prefetchers while offering good performance improvements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Design Space of Temporal Prefetchers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Triage is more energy efficient than MISB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Triage works well for server workloads.</figDesc><graphic url="image-12.png" coords="10,101.63,548.04,174.68,76.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>4. 5</head><label>5</label><figDesc>Figure 15 shows that for multi-programmed mixes of SPEC programs sharing the last-level cache, Triage-Dynamic is a significant improvement over Triage-Static. In particular, for mixes of irregular workloads sharing an 8 MB LLC on a 4-core system, a static version of Triage with 4 MB of metadata and 4 MB of data improves performance by only 4.8%. By contrast, Triage-Dynamic improves performance by 10.2%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Triage-Dynamic improves over Triage-Static for shared caches.</figDesc><graphic url="image-7.png" coords="10,111.10,445.24,173.61,71.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 :</head><label>16</label><figDesc>Figure 16: Triage works well on multi-programmed mixes of irregular programs running on a 4-core system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Triage outperforms MISB in bandwidthconstrained environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Triage works well on multi-programmed mixes of regular and irregular programs running on a 4-core system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Dynamic Triage allocates different metadata store sizes to different cores.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Sensitivity to Prefetch degree.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. For example, Solihin et al., redundantly store a chain of successors in each off-chip table entry, which increases table size but amortizes the cost of fetching metadata for temporal streams by grouping them in a single off-chip access. Triage also uses a table-based organization, but there are two main differences. First, Triage uses PC-localization, which improves coverage and accuracy and eliminates the need to track multiple successors in each table entry, reducing table sizes by 2? to 4?. Second, Triage uses a customized replacement policy that identifies the most useful table entries, removing the need for off-chip metadata.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Machine Configuration</figDesc><table><row><cell></cell><cell>4-wide fetch, decode, and dispatch</cell></row><row><cell></cell><cell>128 ROB entries</cell></row><row><cell>TLB</cell><cell>48-entry fully-assoc L1 I/D-TLB</cell></row><row><cell></cell><cell>1024-entry 4-way assoc L2 TLB</cell></row><row><cell>L1I</cell><cell>64KB, 4-way assoc, 3-cycle latency</cell></row><row><cell>L1D</cell><cell>64KB, 4-way assoc, 3-cycle latency</cell></row><row><cell></cell><cell>Stride prefetcher</cell></row><row><cell>L2</cell><cell>512KB, private, 8-way assoc</cell></row><row><cell></cell><cell>11-cycle load to use latency</cell></row><row><cell>L3</cell><cell>2MB/core, shared, 16-way assoc</cell></row><row><cell></cell><cell>20-cycle load-to-use latency</cell></row><row><cell>DRAM</cell><cell>Single-Core:</cell></row><row><cell></cell><cell>85ns latency, 32GB/s bandwidth</cell></row><row><cell></cell><cell>Multi-Core:</cell></row><row><cell></cell><cell>8B channel width, 800MHz,</cell></row><row><cell></cell><cell>tCAS=20, tRP=20, tRCD=20</cell></row><row><cell></cell><cell>2 channels, 8 ranks, 8 banks, 32K rows</cell></row><row><cell></cell><cell>32GB/s bandwidth</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>PC localization is a method of creating more predictable reference streams by separating streams according to the address of the instruction that issued the load.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The set_id of the trigger address is implicit in a set-associative cache, so it does not need to be stored.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>MICRO-52, October 12?16, 2019, Columbus, OH, USA Hao Wu, Krishnendra Nathella, Joseph Pusdesris, Dam Sunwoo, Akanksha Jain, and Calvin Lin</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>For astar, gcc, and soplex, we show results for the reference inputs, which are more regular.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Jaekyu Lee</rs> for his help in setting up the proprietary simulation infrastructure. This work was funded in part by <rs type="funder">NSF</rs> Grant <rs type="grantNumber">CCF-1823546</rs> and a gift from <rs type="funder">Intel Corporation</rs> through the <rs type="funder">NSF/Intel Partnership on Foundational Microarchitecture Research</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_N3NTHBw">
					<idno type="grant-number">CCF-1823546</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">2nd Data Prefetching Championship</title>
		<ptr target="http://comparch-conf.gatech.edu/dpc2" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">2nd Cache Replacement Championship</title>
		<ptr target="http://crc2.ece.tamu.edu/" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective Hardware-Based Data Prefetching for High-Performance Processors</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tien-Fu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995-05">1995. May 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domino Temporal Data Prefetcher</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pejman</forename><surname>Lotfi-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Shekhar</forename><surname>Borkar</surname></persName>
		</author>
		<ptr target="https://parasol.tamu.edu/pact11/ShekarBorkar-PACT2011-keynote.pdf" />
		<title level="m">The Exascale Challenge</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predictor virtualization</title>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Burcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on Architectural support for programming languages and operating systems (ASPLOS XIII)</title>
		<meeting>the 13th international conference on Architectural support for programming languages and operating systems (ASPLOS XIII)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="157" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Filtering Superfluous Prefetches Using Density Vectors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Fen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCD &apos;01: Proceedings of the International Conference on Computer Design: VLSI in Computers &amp; Processors</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page">133</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Accurate and Complexity-Effective Spatial Pattern Prediction</title>
		<author>
			<persName><forename type="first">Chi</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Se-Hyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th International Symposium on High Performance Computer Architecture (HPCA &apos;04</title>
		<meeting>the 10th International Symposium on High Performance Computer Architecture (HPCA &apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="276" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient Representations and Abstractions for Quantifying and Exploiting Data Reference Locality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName><surname>Chilimbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGPLAN Conference on Programming Language Design and Implementation (PLDI)</title>
		<imprint>
			<date type="published" when="2001">2001. 191?202</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Low-Cost Epoch-Based Correlation Prefetching for Commercial Applications</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Chou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>In MICRO. 301?313</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pointer Cache Assisted Prefetching</title>
		<author>
			<persName><forename type="first">Jamison</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suleyman</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 35th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="62" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A stateless, contentdirected data prefetching mechanism</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Cooksey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Jourdan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2002-10">2002. October 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Enabling technologies for memory compression: Metadata, mapping, and prediction</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Faraboschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafiee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Muralimanohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 34th International Conference on Computer Design (ICCD</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Techniques for bandwidthefficient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">Eiman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Memory-system Design Considerations for Dynamically-scheduled Processors</title>
		<author>
			<persName><forename type="first">Keith</forename><forename type="middle">I</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvonko</forename><surname>Vranesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;97: Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="133" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scaleout workloads on modern hardware</title>
		<author>
			<persName><forename type="first">Almutaz</forename><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stavros</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cansu</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><forename type="middle">Daniel</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems</title>
		<meeting>the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Temporal instruction fetch streaming</title>
		<author>
			<persName><surname>Michael Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Thomas F Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 41st annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SPEC CPU2006 Benchmark Descriptions</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">L</forename><surname>Henning</surname></persName>
		</author>
		<idno type="DOI">10.1145/1186736.1186737</idno>
		<ptr target="https://doi.org/10.1145/1186736.1186737" />
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comput. Archit. News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2006-09">2006. September 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Attache: Towards ideal memory compression by mitigating metadata bandwidth overheads</title>
		<author>
			<persName><forename type="first">Seokin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant Jayaprakash</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bulent</forename><surname>Abali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alper</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyu-Hyoun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Healy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="326" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">TCP: Tag Correlating Prefetchers</title>
		<author>
			<persName><forename type="first">Zhigang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanos</forename><surname>Kaxiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Memory Prefetching Using Adaptive Stream Detection</title>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Symposium on Microarchitecture</title>
		<meeting>the 39th International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">408</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Access Map Pattern Matching for High Performance Data Cache Prefetch</title>
		<author>
			<persName><forename type="first">Yasuo</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kei</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Memory systems: cache, DRAM, disk</title>
		<author>
			<persName><forename type="first">Bruce</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spencer</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linearizing Irregular Memory Accesses for Improved Correlated Prefetching</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">46th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Back to the Future: Leveraging Belady&apos;s Algorithm for Improved Cache Replacement</title>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Run-time spatial locality detection and optimization</title>
		<author>
			<persName><forename type="first">Teresa</forename><forename type="middle">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">C</forename><surname>Merten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><forename type="middle">W</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual ACM/IEEE International Symposium on Microarchitecture</title>
		<meeting>the 30th Annual ACM/IEEE International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="57" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Prefetching Using Markov Predictors</title>
		<author>
			<persName><forename type="first">Doug</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual International Symposium on Computer Architecture</title>
		<meeting>the 24th Annual International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="364" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kill the Program Counter: Reconstructing Program Behavior in the Processor Cache Hierarchy</title>
		<author>
			<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elvira</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Second Int&apos;Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
		<meeting>the Twenty-Second Int&apos;Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="737" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting spatial locality in data caches using spatial footprints</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="357" to="368" />
			<date type="published" when="1998-04">1998. April 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Amoeba-Cache: Adaptive Blocks for Eliminating Waste in the Memory Hierarchy</title>
		<author>
			<persName><forename type="first">Snehasish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arrvindh</forename><surname>Shriraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhya</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lesley</forename><surname>Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 45th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="376" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Best-offset hardware prefetching</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">480</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data Cache Prefetching Using a Global History Buffer</title>
		<author>
			<persName><forename type="first">Kyle</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="90" to="97" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Criticality aware tiered cache hierarchy: a fundamental relook at multi-level cache hierarchies</title>
		<author>
			<persName><forename type="first">Anant</forename><surname>Vithal Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM/IEEE 45th Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="96" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Evaluating Stream Buffers as a Secondary Cache Replacement</title>
		<author>
			<persName><forename type="first">Subbarao</forename><surname>Palacharla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic locality and context-based prefetching using reinforcement learning</title>
		<author>
			<persName><forename type="first">Leeor</forename><surname>Peled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Weiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Etsion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="285" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">Zeshan</forename><surname>Seth H Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Fei</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aamer</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Lien</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kingsum</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Effective jump-pointer prefetching for linked data structures</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gurindar</forename><forename type="middle">S</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Symposium on Computer Architecture (ISCA)</title>
		<meeting>the 26th Annual International Symposium on Computer Architecture (ISCA)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="111" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A decoupled predictordirected stream prefetching architecture</title>
		<author>
			<persName><forename type="first">Suleyman</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2003-03">2003. March 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Farewell My Shared LLC! A Case for Private Die-Stacked DRAM Caches for Servers</title>
		<author>
			<persName><forename type="first">Amna</forename><surname>Shahab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artemiy</forename><surname>Margaritov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Grot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 51st Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="559" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatically characterizing large scale program behavior</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erez</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGOPS Operating Systems Review</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="45" to="57" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequential Program Prefetching in Memory Hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="12" />
			<date type="published" when="1978-12">1978. December 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Using a user-level memory thread for correlation prefetching</title>
		<author>
			<persName><forename type="first">Yan</forename><surname>Solihin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaejin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International Symposium on Computer Architecture. 171?</title>
		<meeting>the 29th Annual International Symposium on Computer Architecture. 171?</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">182</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial Memory Streaming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastassia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA &apos;06: Proceedings of the 33th Annual International Symposium on Computer Architecture</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Practical off-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
		<idno>HPCA. 79?90</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Making Address-Correlated Prefetching Practical</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Babak</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="50" to="59" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient Metadata Management for Irregular Data Prefetching</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnendra</forename><surname>Nathella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akanksha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dam</forename><surname>Sunwoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Calvin</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the 46th International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Enabling Transparent Memory-Compression for Commodity Memory Systems</title>
		<author>
			<persName><forename type="first">Vinson</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Kariyappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moinuddin</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="570" to="581" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
