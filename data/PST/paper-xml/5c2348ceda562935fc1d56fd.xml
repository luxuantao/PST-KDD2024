<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Symbolic Graph Reasoning Meets Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Intelligent Systems Engineering</orgName>
								<orgName type="department" key="dep2">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
							<email>zhitingh@cs.cmu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff2">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University 4 Petuum Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs.cmu.edu</email>
						</author>
						<title level="a" type="main">Symbolic Graph Reasoning Meets Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D381CAEC2285D0376F983C704138A9AB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T07:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Beyond local convolution networks, we explore how to harness various external human knowledge for endowing the networks with the capability of semantic global reasoning. Rather than using separate graphical models (e.g. CRF) or constraints for modeling broader dependencies, we propose a new Symbolic Graph Reasoning (SGR) layer, which performs reasoning over a group of symbolic nodes whose outputs explicitly represent different properties of each semantic in a prior knowledge graph. To cooperate with local convolutions, each SGR is constituted by three modules: a) a primal local-to-semantic voting module where the features of all symbolic nodes are generated by voting from local representations; b) a graph reasoning module propagates information over knowledge graph to achieve global semantic coherency; c) a dual semantic-to-local mapping module learns new associations of the evolved symbolic nodes with local representations, and accordingly enhances local features. The SGR layer can be injected between any convolution layers and instantiated with distinct prior graphs. Extensive experiments show incorporating SGR significantly improves plain ConvNets on three semantic segmentation tasks and one image classification task. More analyses show the SGR layer learns shared symbolic representations for domains/datasets with the different label set given a universal knowledge graph, demonstrating its superior generalization capability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite significant advances in standard recognition tasks such as image classification <ref type="bibr" target="#b11">[12]</ref> and segmentation <ref type="bibr" target="#b5">[6]</ref> achieved by convolution networks, the dominant paradigm lies in the stack of deeper and complicated local convolutions, and we hope it captures everything about the relationship between inputs and targets. But such networks compromise the feature interpretability and also lack the global reasoning capability that is crucial for complicated real-world tasks. Some works <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref> thus formulated graphical models and structure constraints (e.g. CRF <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref>) as recurrent works to effect on final convolution predictions. However, they cannot explicitly enhance feature representations, leading to the limited generalization capability. The very recent capsule network <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b13">14]</ref> extends to learn the sharing of knowledge across locations to find feature clusters, but it can only exploit implicit and uncontrollable feature hierarchy. As emphasized in <ref type="bibr" target="#b2">[3]</ref>, visual reasoning over external knowledge is crucial for human decision-making. The lack of explicitly reasoning over contexts and high-level semantics would hinder the advances of convolution networks in recognizing objects in a large concept vocabulary where exploring semantic correlations and constraints plays an important role. On the other hand, structured knowledge provides rich cues to record human observations and commonsense using symbolic words (e.g. nouns or predicates). It is thus desirable to bridge symbolic semantics with learned local feature representations for better graph reasoning.</p><p>In this paper, we explore how to incorporate rich commonsense human knowledge <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b52">53]</ref> into intermediate feature representation learning beyond local convolutions, and further achieve global semantic coherency. The commonsense human knowledge can be formed as various undirected graphs consisting of rich relationships (e.g. semantic hierarchy, spatial/action interactions and attributes, concurrence) among concepts. For example, "Shetland Sheepdog" and "Husky" share one superclass "dog" due to some common characteristics; people wear a hat and play guitar not vice-versa; orange is yellow color. After associating structured knowledge with the visual domain, all these symbolic entities (e.g. dog) can be connected with visual evidence from images, and human can thus integrate visual appearance and commonsense knowledge to help recognize.</p><p>We attempt to mimic this reasoning procedure and integrate it into convolution networks, that is, first characterize representations of different symbolic nodes by voting from local features; then perform graph reasoning for enhancing visual evidence of these symbolic nodes via graph propagation to achieve semantic coherency; finally mapping the evolved features of symbolic nodes back into facilitating each local representation. Our work takes an important next step beyond prior approaches in that it directly incorporates the reasoning over external knowledge graph into local feature learning, called as Symbolic Graph Reasoning (SGR) layer. Note that, here we use "Symbolic" to denote nodes with explicit linguistic meaning rather than conventional/hidden graph nodes used in graphical models or graph neural networks <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>The core of our SGR layer consists of three modules, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. First, personalized visual evidence of each symbolic node can be produced by voting from all local representations, named as a local-to-semantic voting module. The voting weights stand for the semantic agreement confidence of each local features to a certain node. Second, given a prior knowledge graph, the graph reasoning module is instantiated to propagate information over this graph for evolving visual features of all symbolic nodes. Finally, a dual semantic-to-local module learns appropriate associations between the evolved symbolic nodes and local features to join forces of local and global reasoning. It thus enables the evolved knowledge of a specific symbolic node to only drive the recognition of semantically compatible local features with the help of global reasoning.</p><p>The key merits of our SGR layer lie in three aspects: a) local convolutions and global reasoning facilitated with commonsense knowledge can collaborate by learning associations between imagespecific observations with prior knowledge graph; b) each local feature is enhanced by its correlated incoming local features whereas in standard local convolutions it is only based on a comparison between its own incoming features and a learned weight vector; c) benefiting from the learned representations of universal symbolic nodes, the learned SGR layer can be easily transferable to other dataset domain with discrepant concept sets. And SGR layer can be plugged between any convolution layers and personalized according to distinct knowledge graphs.</p><p>Extensive experiments show superior performance over plain ConvNets by incorporating our SGR layer, especially on recognizing a large concept vocabulary in three semantic segmentation datasets (COCO-Stuff, ADE20K, PASCAL-Context) and image classification dataset (CIFAR100). We further demonstrate its promising generalization capability when transferring SGR layer trained one domain into other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent researches that explored the context modeling for convolution networks can be categorized into two streams. One stream exploits networks for the graph-structured data with a family of graph-based CNNs <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> and RNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> or advanced convolution filters <ref type="bibr" target="#b42">[43]</ref> to discover more complex feature dependencies. In the context of convolutional networks, the graphical models such as conditional random fields (CRF) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19]</ref> can be formulated into a recurrent network by functioning on final predictions of basic convolutions <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast, the proposed SGR layer can be treated as a simple feedforward layer that can be injected between any convolution layers and general-purposed for any networks for large-scale and semantic related recognition. Our work differs in that local features are mapped into meaningful symbolic nodes. The global reasoning over locations is directly aligned with external knowledge rather than implicit feature clusters, which is a more effective and interpretable way to introduce structure constraints.</p><p>Another stream explored external knowledge bases into facilitating networks. For example, Deng et al. <ref type="bibr" target="#b8">[9]</ref> employed a label relation graph to guide network learning while Ordonez et al. <ref type="bibr" target="#b36">[37]</ref> learned the mapping of common concepts to entry-level concepts. Some works regularized the output of networks by resorting to complex graphical inference <ref type="bibr" target="#b8">[9]</ref>, hierarchical loss <ref type="bibr" target="#b37">[38]</ref> or word embedding priors <ref type="bibr" target="#b48">[49]</ref> on final prediction scores. However, their loss constraints can only function on final prediction layer and indirectly guide visual features to be hierarchy-aware, which is hard to be guaranteed. More recently, Marino et al. <ref type="bibr" target="#b31">[32]</ref> used structure prior knowledge to enhance predictions of multi-label classification while our SGR proposes a general neural layer that can be injected into any convolution layers and allows the neural network to leverage semantic constraints derived from various human knowledge. Chen et al. <ref type="bibr" target="#b6">[7]</ref> leverage local region-based reasoning and global reasoning to facilitate object detection. In contrast, our SGR layer directly performs reasoning over symbolic nodes and is seamlessly interacted with local convolution layers for better flexibility. Notably, the earliest efforts in reasoning in artificial intelligence date back to symbolic approaches <ref type="bibr" target="#b34">[35]</ref> by performing reasoning over abstract symbols with the language of mathematics and logic. After grounding these symbols, statistical learning algorithm <ref type="bibr" target="#b22">[23]</ref> is used to extract useful patterns to perform relational reasoning on knowledge bases. An effective reasoning procedure that would be practical enough for advanced tasks should join the force of local visual representation learning and global semantic graph reasoning. Our reasoning layer relates to this line of research by explicitly reasoning over visual evidence of language entities by voting from local representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Symbolic Graph Reasoning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General-purposed Graph Construction</head><p>The commonsense knowledge graph is used to depict distinct correlations between entities (e.g. classes, attributes and relationships) in general, which can be any forms. To support the general purposed graph reasoning, the knowledge graph can be formulated as G = (N , E), where N and E denote the symbol set and edge set, respectively. Here we give three examples: a) class hierarchy graph is constructed by a list of entity classes (e.g. person, motorcyclist) and its graph edges shoulder the responsibility of concept belongings (e.g. "is kind of" or "is part of"). The networks equipped by such hierarchy knowledge can encourage the learning of feature hierarchy by passing the shared representations of parent classes into its child nodes; b) class occurrence graph defines the edges as the occurrence of two classes across images, characterizing the rationality of predictions; c) as a higher-level semantic abstraction, a semantic relationship graph can extend symbolic nodes to include more actions (e.g. "ride", "play"), layouts (e.g. "on top of") and attributes (e.g. color or shape) while graph edges are statistically collected from language descriptions. Incorporating such high-level commonsense knowledge can facilitate networks to prune spurious explanations after knowing the relationship of each entity pair, resulting in good semantic coherency.</p><p>Based on this general formula, the graph reasoning is required to be compatible and general enough for soft graph edges (e.g. occurrence probabilities) and hard edges (e.g. belongings), as well as diverse symbolic nodes. Various structure constraints can thus be modeled as edge connections over symbolic nodes, just like human use language tools. Our SGR layer is designed to achieve the general graph reasoning that is applicable for encoding a wide range of knowledge graph forms. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, it consists of a local-to-semantic voting module, a graph reasoning module and a semantic-to-local mapping module, as presented in following sections.</p><p>! " # $% : 1×1 *+,-.</p><p>/ " ×# " ×0 " # 1 : 1×1 *+,-. </p><formula xml:id="formula_0">/ " ×# " ×2 / " ×# " ×0 3 × 2×/# Softmax / " # " ×0 3 2×0 3 4 Symmetric Normalization # 5 : 67,89: 2×0 3 ReLU × 2×2 2×2 C 2×; &lt; 2×(0 3 + ;) ReLU 2×0 3 / " # " ×2×0 3 C / " # " ×2×0 " Expand Expand / " # " ×2×(0 " +0 3 ) # % : 1×1 *+,-. / " # " ×2 Softmax # %$ : 1×1 *+,-. 2×0 " × + / " # " ×0 " / " ×# " ×0 " / " ×# " ×0 " ! "@A ReLU Local-to-Semantic Voting Graph Reasoning Semantic-to-Local Mapping</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Local-to-Semantic Voting Module</head><p>Given local feature tensors from convolution layers, our target is to leverage global graph reasoning to enhance local features with external structured knowledge. We thus first summarize the global information encoded in local features into representations of symbolic nodes, that is, local features that are correlated to a specific semantic meaning (e.g. cat) are aggregated to depict the characteristic of its corresponding symbolic node. Formally, we use the feature tensor X l ∈ R H l ×W l ×D l after l-th convolution layer as the module inputs, where H l and W l are height and weight of feature maps and D l is the channel number. This module aims to produce visual representations H ps ∈ R M ×D c of all M = |N | symbolic nodes using X l , where D c is the desired feature dimension for each node n, which is formulated as the function φ:</p><formula xml:id="formula_1">H ps = φ(A ps , X l , W ps ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_2">W ps ∈ R D l ×D c</formula><p>is the trainable transformation matrix for converting each local feature x i ∈ X l into the dimension D c , and A ps ∈ R H l ×W l ×M denotes the voting weights of all local features to each symbolic node. Specifically, visual features H ps n ∈ H ps of each node n are computed by summing up all weighted transformed local features via the voting weight a xi→n ∈ A ps that represents the confidence of assigning local feature x i to the node n. More specifically, the function φ is computed as:</p><formula xml:id="formula_3">H ps n = xi a xi→n x i W ps , a xi→n = exp(W a n T x i ) n∈N exp(W a n T x i ) .<label>(2)</label></formula><p>Here W a = {W a n } ∈ R D l ×M is a trainable weight matrix for calculating voting weights. A ps is normalized by using a softmax at each location. In this way, different local features can adaptively vote to representations of distinct symbolic nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Reasoning Module</head><p>Based on visual evidence of symbolic nodes, the reasoning guided by structured knowledge is employed to leverage semantic constraints from human commonsense to evolve global representations of symbolic nodes. Here, we incorporate both linguistic embedding of each symbolic node and knowledge connections (i.e. node edges) for performing graph reasoning. Formally, for each symbolic node n ∈ N , we use the off-the-shelf word vectors <ref type="bibr" target="#b16">[17]</ref> as its linguistic embedding, denoted as S = {s n }, s n ∈ R K . The graph reasoning module performs graph propagation over representations H ps of all symbolic nodes via the matrix multiplication form, resulting in the evolved features H g :</p><formula xml:id="formula_4">H g = σ(A g BW g ),<label>(3)</label></formula><p>where B = [σ(H ps ), S] ∈ R M ×(D c +K) concatenates features of transformed H ps via the activation function σ(•) and the linguistic embedding S.</p><formula xml:id="formula_5">W g ∈ R (D c +K)×(D c</formula><p>) is a trainable weight matrix.</p><p>The node adjacency weight a n→n ∈ A g is defined according the edge connections in (n, n ) ∈ E.</p><p>As discussed in Section 3.1, the edge connections can be soft weights (e.g. 0.8) or hard weight (i.e. {0,1}) according to different knowledge graph resources. The naive multiplication with A g will completely change the scale of the feature vectors. Inspired from graph convolutional networks <ref type="bibr" target="#b17">[18]</ref>, we can normalize A g such that all rows sum to one to get rid of this problem, i.e.</p><formula xml:id="formula_6">Q -1 2 A g Q -1 2</formula><p>, where Q is the diagonal node degree matrix of A g . This symmetric normalization corresponds to taking the average of neighboring node features. This formulation arrives at the new propagation rule:</p><formula xml:id="formula_7">H g = σ( Q-1 2 Âg Q-1 2 BW g ),<label>(4)</label></formula><p>where Âg = A g +I is the adjacency matrix of the graph G with added self-connections for considering its own representation of each node and I is the identity matrix. Qii = j Âg ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semantic-to-Local Mapping Module</head><p>Finally, the evolved global representations H g ∈ R M ×D c of symbolic nodes can be used to further boost the capability of each local feature representation. As the feature distributions of each symbolic node have been changed after graph reasoning, a critical question is how to find most appropriate mappings from the representation h g ∈ H g of each symbolic node to all x i . This can be agnostic to learning the compatibility matrix between local features and symbolic nodes. Inspired by message-passing algorithms <ref type="bibr" target="#b10">[11]</ref>, we compute the mapping weights a h g →xi ∈ A sp by evaluating the compatibility of each symbolic node h g with each local feature x i :</p><formula xml:id="formula_8">a h g →xi = exp(W sT [h g , x i ]) xi exp(W sT [h g , x i ]) ,<label>(5)</label></formula><p>where W s ∈ R D l +D c is a trainable weight matrix. The compatibility matrix A sp ∈ R H×W ×M is again row-normalized. The evolved features X l+1 by graph reasoning, posed as inputs in the l + 1 convolution layer can be updated as:</p><formula xml:id="formula_9">X l+1 = σ(A sp H g W sp ) + X l ,<label>(6)</label></formula><p>where W sp ∈ R D c ×D l is the trainable matrix for transforming the dimension of symbolic node representation back into D l , and we use residual connection <ref type="bibr" target="#b11">[12]</ref> to further enhance local representations with the original local feature tensor X l . Each local feature is updated by the weighted mappings from each symbolic node that represents different characteristics of semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Symbolic Graph Reasoning Layer</head><p>Each symbolic graph reasoning layer is constituted by the stack of a local-to-semantic voting module, a graph reasoning module, and a semantic-to-local mapping module. The SGR layer is instantiated by specific knowledge graph with different numbers of symbolic nodes and distinct node connections. Combining multiple SGR layers with distinct knowledge graphs into convolutional networks can lead to hybrid graph reasoning behaviors. We implement the modules of each SGR via the combination of 1 × 1 convolution operations and non-linear functions, detailed as Figure <ref type="figure" target="#fig_1">2</ref>. Our SGR is flexible and general enough for injecting it between any local convolutions. Nonetheless, as SGR is designated to incorporate high-level semantic reasoning, using SGR in later convolution layers is more preferable, as demonstrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>As we present the proposed SGR layer as a conventional module suitable for any convolution networks, we thus compare it with on both the pixel-level prediction task (i.e. semantic segmentation) on Coco-Stuff <ref type="bibr" target="#b3">[4]</ref>, Pascal-Context <ref type="bibr" target="#b33">[34]</ref> and ADE20K <ref type="bibr" target="#b51">[52]</ref>, and image classification task on CIFAR-100 <ref type="bibr" target="#b20">[21]</ref>. Extensive ablation studies are conducted on Coco-Stuff dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Semantic Segmentation</head><p>Dataset. We evaluate on three public benchmarks for segmenting over large-scale categories, which pose more realistic challenges than other small segmentation datasets (e.g. PASCAL-VOC) and can better validate the necessity of global symbolic reasoning. Specifically, Coco-Stuff <ref type="bibr" target="#b3">[4]</ref>  Implementation. We conduct all experiments using Pytorch, 2 GTX TITAN X 12GB cards on a single server. We use the Imagenet-pretrained ResNet-101 <ref type="bibr" target="#b11">[12]</ref> as basic ConvNet following the procedure of <ref type="bibr" target="#b5">[6]</ref>, employ output stride = 8 and incorporate the SGR layer into it. The detailed implementation of one SGR layer is in Figure <ref type="figure" target="#fig_1">2</ref>. Our final SGR model first employs the Atrous Spatial Pyramid Pooling (ASSP) <ref type="bibr" target="#b5">[6]</ref> modules with pyramids of {6,12,18,24} to reduce 2,048-d features from final ResBlock of ResNet-101 into 256-d features. Upon this, we stack one SGR layer to enhance local features and then a final 1 × 1 convolution layer to produce final pixel-wise predictions. D l and D c for feature dimensions in both local-to-semantic voting module and graph reasoning module are thus set as 256, and we use ReLU activation function for σ(•). Word embeddings from fastText <ref type="bibr" target="#b16">[17]</ref> are used to represent each class, which extracts sub-word information and generalizes well to out-of-vocabulary words, resulting in a K = 100-d vector for each node.</p><p>We use a universal concept hierarchy for all datasets. Following <ref type="bibr" target="#b26">[27]</ref>, starting from the label hierarchy of COCO-Stuff <ref type="bibr" target="#b3">[4]</ref> that includes 182 concepts and 27 super-classes, we manually merge concepts from the rest two dataset together by using WordTree as <ref type="bibr" target="#b26">[27]</ref>. It results in 340 concepts in the final concept graph. Thus, this concept graph makes the symbolic graph reasoning layer can be identical across all three datasets and its weights can be easily shared to each other dataset. We fix the moving means and variations in batch normalization of ResNet-101 during finetuning. We adopt the standard SGD optimization. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we use the "poly" learning rate policy, set the base learning rate to 2.5e-3 for newly initialized layers and 2.5e-4 for pretrained layers. We train 64 epochs for Coco-Stuff and PASCAL-Context, and 120 epochs for ADE20K dataset. For data augmentation, we adopt random flipping, random cropping and random resize between 0.5 and 2 for all datasets. Due to the GPU memory limitation, the batch size is used as 6. The input crop size is set as 513 × 513.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Comparison with the state-of-the-arts</head><p>Table <ref type="table" target="#tab_0">1</ref>, 2, 3 report the comparisons with recent state-of-the-art methods on Coco-Stuff, Pascal-Context and ADE20K dataset, respectively. Incorporating our SGR layer significantly outperforms existing methods on all three datasets, demonstrating its effectiveness of performing explicit graph reasoning beyond local convolutions for large-scale pixel-level recognition. Figure <ref type="figure" target="#fig_4">3</ref> shows the qualitative comparison with the baseline "Deeplabv2 [6]". Our SGR obtains better segmentation performance, especially for some rare classes (e.g. umbrella, teddy bear), benefiting from the joint reasoning with frequent concepts over the concept hierarchy graph. Particularly, applying the techniques of incorporating high-level semantic constraints designed for classification task into pixel-wise recognition is not trivial since associating prior knowledge with dense pixels itself is difficult. The prior works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49</ref>] also attempt to implicitly facilitate the network learning with the hierarchical classification objective. The very recent DSSPN <ref type="bibr" target="#b26">[27]</ref> directly designs a network layer for each parent concept. However, this method is hard to scale up for large-scale concept set and Method mean IoU pixel acc. FCN <ref type="bibr" target="#b30">[31]</ref> 29.39 71.32 SegNet <ref type="bibr" target="#b1">[2]</ref> 21.64 71.00 DilatedNet <ref type="bibr" target="#b46">[47]</ref> 32.31 73.55 CascadeNet <ref type="bibr" target="#b51">[52]</ref> 34 Table <ref type="table">3</ref>: Comparison on the ADE20K val set <ref type="bibr" target="#b51">[52]</ref> (%). "Conditional Softmax <ref type="bibr" target="#b37">[38]</ref>", "Word2Vec <ref type="bibr" target="#b9">[10]</ref>" and "Joint-Cosine <ref type="bibr" target="#b48">[49]</ref>" use VGG as backbone. We use "DeepLabv2 (ResNet-101) <ref type="bibr" target="#b5">[6]</ref>" as baseline. Table <ref type="table">4</ref>: Curves of the training losses on Coco-Stuff for the Deeplabv2 (Baseline) <ref type="bibr" target="#b5">[6]</ref> and our three variants. Following <ref type="bibr" target="#b5">[6]</ref>, the loss is the summations of losses for inputs of three scales (i.e. 1, 0.75, 0.5). results in redundant predictions for pixels that unlikely belongs to a specific concept. Unlike prior methods, the proposed SGR layer can achieve better results by only adding one reasoning layer while preserving both good computation and memory efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Ablation studies</head><p>Which ConvBlock to add SGR layer? Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table">4</ref> compare the variants of adding a single SGR layer into different stages of ResNet-101. "SGR ConvBlock4" means the SGR layer is added to right before the last residual block of res4 while all other variants add SGR layer before the last residual block of res5 (final residual block). The performance of "SGR ConvBlock4" is worse than "Our SGR (ResNet-101)" while using SGR layer for both res4 and res5 ("Our SGR (ResNet-101 2-layer)") can slightly improve the results. Note that in order to use pretrained weights from ResNet-101, "Our SGR (ResNet-101 2-layer)" directly fuses the prediction results from two SGR layers after res4 and res5 via the summation to get the final prediction. One possible explanation for this observation is that the final res5 can encode more semantically abstracted features, which is more suitable for conducting symbolic graph reasoning. Furthermore, we find removing residual connection in Eqn. 6 would decrease the final performance but is still better than other baselines, by comparing "SGR (w/o residual)" with our full SGR. The reason is that the SGR layer induces more smoothing local features enhanced by global reasoning and thus may degrade some discriminative capability in boundaries.</p><p>The effect of semantic-to-local mapping. Note that our SGR learns distinct voting weights and mapping weights in the local-to-semantic modules and semantic-to-local module, respectively. The advantages of reevaluating mapping weights can be seen by comparing "Our SGR (ResNet-101)" with "SGR (w/o mapping)" in both testing performance and training convergence in Table <ref type="table" target="#tab_0">1</ref> and Table <ref type="table">4</ref>. This justifies that estimating new semantic-to-local mapping weights can make the reasoning process better accommodate with the evolved feature distributions after graph reasoning, otherwise the evolved symbolic nodes will be misaligned with local features.</p><p>Different prior knowledge graphs. As discussed in Section 3.1, our SGR layer is general for any forms of knowledge graphs with either soft or hard edge weights. We thus evaluate results of leveraging distinct knowledge graphs in Table <ref type="table" target="#tab_0">1</ref>. First, class concurrence graph is often used to represent the frequency of any two concepts appearing in one image, which depicts inter-class rationality in a statistic view. We calculate the class concurrence graph from all training images on Coco-Stuff and feed it as the input of SGR layer, as "SGR (concurrence graph)". We can see that incorporating a concurrence-driven SGR layer can also boost the segmentation performance, but is slightly inferior to that with concept hierarchy. Second, we also sequentially stack one SGR layer with hierarchy graph and one layer with concurrence graph, leading to a hybrid version as "Our SGR (ResNet-101 Hybrid)". This variant achieves the best performance among all models, verifying the benefits of boosting semantic reasoning capability with the mixtures of knowledge constraints. Finally, we further explore a rich scene graph that includes concepts, attributes and relationships for encoding higher-level semantics, as "SGR (scene graph)" variant. Following <ref type="bibr" target="#b23">[24]</ref>, the scene graph  MethodResNet <ref type="bibr" target="#b12">[13]</ref>Wide <ref type="bibr" target="#b47">[48]</ref>ResNeXt-29 <ref type="bibr" target="#b45">[46]</ref>DenseNet <ref type="bibr" target="#b15">[16]</ref>DenseNet-100 <ref type="bibr" target="#b15">[16]</ref>    <ref type="table" target="#tab_1">2</ref>. "Our SGR (Transfer convs)" denotes only the pretrained weights of residual blocks are used while "Our SGR (Transfer SGR)" is the variant of further using the parameters of SGR layer. We can see that transferring parameters of SGR layer can give more improvements than that of solely transferring convolution blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Image classification results</head><p>We further conduct studies for image classification task on CIFAR-100 <ref type="bibr" target="#b20">[21]</ref> consisting of 50K training images and 10K test images in 100 classes. We explore how much SGR will improve the performance of a baseline network, DenseNet-100 <ref type="bibr" target="#b15">[16]</ref>. We append SGR layers on the final dense block which produces 342 feature maps with 8 × 8 size. We first use a 1 × 1 convolution layer to reduce 342-d feature into 128-d, and then sequentially employ one SGR layer, global average pooling and a linear layer to produce final classification. The concept hierarchy graph with 148 symbolic nodes is generated by mapping 100 classes into WordTree, similar to the strategy used in segmentation experiments, included in Supplementary Material. We set D l and D c as 128. During training, we use a mini-batch size of 64 on two GPUs using a cosine learning rate scheduling <ref type="bibr" target="#b15">[16]</ref> for 600 epochs. More comparisons in Table <ref type="table" target="#tab_4">5</ref> demonstrate that our SGR can improve the performance of the baseline network, benefiting from the enhanced features via global reasoning. It achieves comparable results with state-of-the-art methods with considerable less model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To endow the local convolution networks with the capability of global graph reasoning, we introduce a Symbolic Graph Reasoning (SGR) layer, which harnesses external human knowledge to enhance local feature representation. The proposed SGR layer is general, light-weight and compatible with existing convolution networks, consisting of a local-to-semantic voting module, a graph reasoning module, and a semantic-to-local mapping module. Extensive experiments on both three public benchmarks on semantic segmentation and one image classification dataset demonstrated its superior performance. We hope the design of our SGR can help boost the research of investigating global reasoning property of convolution networks and be beneficial for various applications in the community.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An overview of the proposed SGR layer. Each symbolic node receives votes from all local features via a local-to-semantic voting module (long gray arrows), and its evolved features after graph reasoning are then mapped back to each location via a semantic-to-local mapping module (long purple arrows). For simplicity, we omit more edges and symbolic nodes in the knowledge graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Implementation details of one SGR layer by taking the convolution feature tensors of H l × W l × D l as inputs. ⊗ denotes matrix multiplication, and ⊕ denotes element-wise summation and the circle with C denotes the concatenation. The softmax operation, tensor expansion, ReLU operation are performed when noted. The green boxes denote 1 × 1 convolution or linear layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Qualitative comparison results on Coco-stuff dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>contains 10,000 images with dense annotations of 91 thing (e.g. book, clock) and 91 stuff classes (e.g. flower, Comparison on Coco-Stuff test set (%). All our models are based on ResNet-101.</figDesc><table><row><cell>Method</cell><cell cols="3">Class acc. acc. mean IoU</cell><cell>Method</cell><cell>mean IoU (%)</cell></row><row><cell>FCN [31]</cell><cell>38.5</cell><cell>60.4</cell><cell>27.2</cell><cell>FCN [31]</cell><cell>37.8</cell></row><row><cell>DeepLabv2 (ResNet-101) [6]</cell><cell>45.5</cell><cell>65.1</cell><cell>34.4</cell><cell>CRF-RNN [51]</cell><cell>39.3</cell></row><row><cell>DAG RNN + CRF [42] OHE + DC + FCN [15] DSSPN (ResNet-101) [27]</cell><cell>42.8 45.8 47.0</cell><cell>63.0 66.6 68.5</cell><cell>31.2 34.3 36.2</cell><cell>ParseNet [30] BoxSup [8] HO CRF [1]</cell><cell>40.4 40.5 41.3</cell></row><row><cell>SGR (w/o residual)</cell><cell>47.9</cell><cell>68.4</cell><cell>38.1</cell><cell>Piecewise [29]</cell><cell>43.3</cell></row><row><cell>SGR (scene graph)</cell><cell>49.1</cell><cell>69.6</cell><cell>38.3</cell><cell>VeryDeep [44]</cell><cell>44.5</cell></row><row><cell>SGR (concurrence graph) SGR (w/o mapping) SGR (ConvBlock4)</cell><cell>48.6 47.3 47.6</cell><cell>69.5 67.9 68.3</cell><cell>38.4 37.2 37.5</cell><cell>DeepLab-v2 (ResNet-101) [6] RefineNet (Res152) [28]</cell><cell>45.7 47.3</cell></row><row><cell>Our SGR (ResNet-101)</cell><cell>49.3</cell><cell>69.9</cell><cell>38.7</cell><cell>Our SGR (ResNet-101)</cell><cell>50.8</cell></row><row><cell>Our SGR (ResNet-101 2-layer)</cell><cell>49.4</cell><cell>69.7</cell><cell>38.8</cell><cell>Our SGR (Transfer convs)</cell><cell>51.3</cell></row><row><cell>Our SGR (ResNet-101 Hybrid)</cell><cell>49.8</cell><cell>70.5</cell><cell>39.1</cell><cell>Our SGR (Transfer SGR)</cell><cell>52.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison on PASCAL-Context test set(%). wood), including 9,000 for training and 1,000 for testing. ADE20k [52] consists of 20,210 images for training and 2,000 for validation, annotated with 150 semantic concepts (e.g. painting, lamp). PASCAL-Context [34] includes 4,998 images for training and 5105 for testing, annotated with 59 object categories and one background. We use standard evaluation metrics of pixel accuracy (pixAcc) and mean Intersection of Union (mIoU).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Transferring SGR learned from one domain to other domains. Our SGR layer naturally learns to encode explicit semantic meanings for general symbolic nodes after voting from local features, whose weights can be easily transferred from one domain into other domains only if these domains share one prior graph. Due to the usage of a single hierarchy graph for both Coco-Stuff and PASCAL-Context datasets, we can use the SGR model pretrained on Coco-Stuff to initialize the training on PASCAL-Context dataset, as reported in Table</figDesc><table /><note><p><p><p><p>Comparison of model depth, number of parameters (M), test errors (%) on CIFAR-100. "SGR" and "SGR 2-layer" indicate the results of appending one or two SGR layer on the final denseblock of the baseline network (DenseNet-100), respectively.</p>is constructed from the Visual Genome</p><ref type="bibr" target="#b19">[20]</ref></p>. For simplicity, we only select the object categories, attributes, and predicates, which appear at least 30 times and are associated with our targeted 182 concepts in Coco-Stuff. It leads to an undirected graph with 312 object nodes, 160 attribute nodes, and 68 predicate nodes. "SGR (scene graph)" is slightly worse than "Our SGR (ResNet-101)" but better than "SGR (concurrence graph)". Observed from all these studies, we thus use the concept hierarchy graph for all rest experiments by balancing the efficiency and effectiveness.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>32nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), and in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, and 61836012.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher order conditional random fields in deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="524" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scene perception: Detecting and judging objects undergoing relational violations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Mezzanotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive psychology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="143" to="177" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03716</idno>
		<title level="m">Coco-stuff: Thing and stuff classes in context</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dense and low-rank gaussian crfs using deep embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2007">2016. 1, 6, 7</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative visual reasoning beyond convolutions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Boxsup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale object classification using label relation graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006">2016. 1, 5, 6</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix capsules with em routing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Labelbank: Revisiting global perspectives for semantic segmentation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09891</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03651</idno>
		<title level="m">Fasttext. zip: Compressing text classification models</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in a large scale knowledge base</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable structure-evolving lstm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dynamic-structured semantic propagation network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for highresolution semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3194" to="3203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04844</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Never ending learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Krishnamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2302" to="2310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Physical symbol systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="183" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From large scale image categorization to entry-level categories</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2768" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007">2017. 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fully connected deep structured networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Scene segmentation with dag-recurrent neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bridging category-level and instance-level semantic image segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06885</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<title level="m">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<title level="m">Wide residual networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Open vocabulary scene parsing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2006">2015. 1, 2, 6</date>
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05442</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05670</idno>
		<title level="m">Building a large-scale multimodal knowledge base system for answering visual queries</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
