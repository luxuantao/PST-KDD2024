<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shiqing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Wen</forename><surname>Gao</surname></persName>
						</author>
						<title level="a" type="main">Speech Emotion Recognition Using Deep Convolutional Neural Network and Discriminant Temporal Pyramid Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5A718714F7CB3E40E0E753C9562EDD87</idno>
					<idno type="DOI">10.1109/TMM.2017.2766843</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2766843, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2766843, IEEE Transactions on Multimedia This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TMM.2017.2766843, IEEE Transactions on Multimedia</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Speech Emotion Recognition</term>
					<term>Feature Learning</term>
					<term>Deep Convolutional Neural Network</term>
					<term>Discriminant Temporal Pyramid Matching</term>
					<term>Lp-norm Pooling</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech emotion recognition is challenging because of the affective gap between the subjective emotions and lowlevel features. Integrating multi-level feature learning and model training, Deep Convolutional Neural Networks (DCNN) has exhibited remarkable success in bridging the semantic gap in visual tasks like image classification, object detection. This paper explores how to utilize a DCNN to bridge the affective gap in speech signals. To this end, we firstly extract three channels of log Mel-spectrograms (static, delta and delta-delta) similar to the RGB image representation as the DCNN input. Then the AlexNet DCNN model pre-trained on the large ImageNet dataset is employed to learn high-level feature representations on each segment divided from an utterance. The learned segment-level features are aggregated by a Discriminant Temporal Pyramid Matching (DTPM) strategy. DTPM combines temporal pyramid matching and optimal Lp-norm pooling to form a global utterance-level feature representation, followed by the linear Support Vector Machines (SVM) for emotion classification. Experimental results on four public datasets, i.e., EMO-DB, RML, eNTERFACE05 and BAUM-1s, show the promising performance of our DCNN model and the DTPM strategy. Another interesting finding is that the DCNN model pre-trained for image applications performs reasonably good in affective speech feature extraction. A further fine-tuning on the target emotional speech datasets substantially promotes the recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Speech signals, as one of the most natural media of human communication, not only carry the explicit linguistic contents but also contain the implicit paralinguistic information about the speakers. During the last two decades, enormous efforts have been devoted to developing methods for automatically identifying human emotions from speech signals, which is called speech emotion recognition. At present, speech emotion recognition has become an attractive research topic in signal processing, pattern recognition, artificial intelligence, and so on, due to its importance in human-machine interactions <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>.</p><p>Feature extraction is a critical step to bridge the affective gap between speech signals and the subjective emotions. So far, a variety of hand-designed features have been used for speech emotion recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. However, these handdesigned features are usually low-level, they may hence not be discriminative enough to depict the subjective emotions. It is needed to develop automatic feature learning algorithms to extract high-level affective feature representations for speech emotion recognition.</p><p>To address this issue, the newly-emerged deep learning techniques <ref type="bibr" target="#b5">[6]</ref> provide a possible solution. Among them, two typical deep leaning methods are Deep Neural Networks (DNN) <ref type="bibr" target="#b5">[6]</ref>, and Deep Convolutional Neural Networks (DCNN) <ref type="bibr" target="#b6">[7]</ref>. Here, a DCNN is taken as a deep extension of the conventional Convolutional Neural Networks (CNN) <ref type="bibr" target="#b7">[8]</ref>. Recently, deep learning techniques have been employed to automatically learn high-level feature representations from low-level data in tasks like speech recognition <ref type="bibr" target="#b8">[9]</ref>, image classification and understanding <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, object detection <ref type="bibr" target="#b10">[11]</ref>. As far as speech emotion recognition is concerned, one of the early-used deep learning methods is the DNN method. For instance, in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> a DNN is used to learn high-level feature representations from the extracted low-level acoustic features for emotion classification.</p><p>In recent years, several works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have successfully employed CNNs for feature learning in speech signal processing. In <ref type="bibr" target="#b13">[14]</ref>, a 1-layer CNN is adopted to obtain promising performance for speech recognition. In <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, the authors also employ a 1-layer CNN trained with a Sparse Auto-encoder (SAE) to extract affective features for speech emotion recognition. Recently, Trigeorgis et al., <ref type="bibr" target="#b16">[17]</ref> presents an end-to-end speech emotion recognition system by combining a 2-layer CNN with a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b17">[18]</ref>. Note that they employ 1-D convolution, such as frequency convolution <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> or time convolution <ref type="bibr" target="#b16">[17]</ref>, rather than 2-D convolution widely used in DCNN models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Additionally, these used 1-layer or 2-layer CNNs are much shallower compared with the deep structures in DCNN models <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Accordingly, they may could not effectively learn affective features discriminative enough to distinguish the subjective emotions.</p><p>It has been recently found that, with deep multi-level convolutional and pooling layers, DCNNs usually exhibit much better performance than the shallow CNNs in computer vision <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. This is reasonable because the deep structures of DCNNs can effectively model the hierarchical architecture of information processing in the primate visual perception system <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Motivated by the promising performance of deep models, this work aims to employ DCNNs to develop an effective speech emotion recognition system.</p><p>The success of DCNNs in visual tasks motivates us to test DCNNs in speech emotion recognition. To achieve this, three issues need to be addressed. First, a proper speech representation should be designed as the DCNN input. Previous works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> have employed 1-D speech signals as the CNN inputs, and 1-D convolution is adopted for CNNs. Compared with 1-D convolution, 2-D convolution involves more parameters to capture more detailed temporalfrequency correlations, thus is potential to present stronger feature learning ability. Therefore, it is important to convert 1-D speech signals into suitable 2-D representations as the DCNN input. Second, most existing emotional speech datasets <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> contain limited numbers of samples. They are not sufficient enough to train deep models having a large amount of parameters. Finally, speech signals may have variant time of duration but the DCNN models require fixed input size. It is hence easier to design the DCNN models for speech segments with a fixed length, rather than for the global utterance. Therefore, proper pooling strategies are needed to generate a global utterance-level feature representation based on the segment-level features learned by DCNNs.</p><p>In this paper, we use deep features learned by DCNNs <ref type="bibr" target="#b6">[7]</ref> and propose a Discriminant Temporal Pyramid Matching (DTPM) algorithm to pool deep features for speech emotion recognition. As illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>, three channels of log Mel-spectrograms (static, delta and delta-delta) are extracted as the DCNN input. The DCNN models are trained to produce deep features for each segment. The DTPM pools the learned segment-level features into a global utterance-level feature representation, followed by the linear SVM emotion classifier. Extensive experiments on four public datasets, i.e., the Berlin dataset of German emotional speech (EMO-DB) <ref type="bibr" target="#b20">[21]</ref>, the RML audio-visual dataset <ref type="bibr" target="#b21">[22]</ref>, the eNTERFACE05 audio-visual dataset <ref type="bibr" target="#b22">[23]</ref>, and the BAUM-1s dataset <ref type="bibr" target="#b23">[24]</ref>, demonstrate the promising performance of our proposed method.</p><p>The main contributions of this paper can be summarized as:</p><p>• We propose to use three channels of log Melspectrograms generated from the original 1-D utterances as the DCNN input. This input is similar to the RGB image representation, thus makes it possible to use existing DCNNs pre-trained on image datasets for affective feature extraction. • The proposed DTPM strategy combines temporal pyramid matching and optimal Lp-norm pooling to generate a discriminative utterance-level feature representation from segment-level features learned by DCNNs. • We find that the DCNN model pre-trained for image applications performs reasonably good in affective feature extraction. A further fine-tuning on target speech emotion recognition tasks substantially promotes the recognition performance. The rest of this paper is structured as follows. The related works are reviewed in Section II. Section III describes our DCNN model for affective feature extraction. Section IV presents the details of our DTPM scheme. Section V describes and analyzes the experimental results. Section VI provides discussions, followed by the conclusions in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Generally, feature extraction and emotion classification are two key steps in speech emotion recognition. In this section, we first briefly review emotion classifiers and then focus on feature extraction since it is more relevant to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Emotion Classifier</head><p>For emotion classification various machine learning algorithms have been utilized to constitute a good classifier to distinguish the underlying emotion categories. Early emotion classifiers contain K-Nearest-Neighbor (KNN) <ref type="bibr" target="#b24">[25]</ref> and Artificial Neural Network (ANN) <ref type="bibr" target="#b25">[26]</ref>. Then, a number of statistical pattern recognition approaches, such as Gaussian Mixture Model (GMM) <ref type="bibr" target="#b26">[27]</ref>, Hidden Markov Models (HMM) <ref type="bibr" target="#b27">[28]</ref>, and SVM <ref type="bibr" target="#b28">[29]</ref>, are widely adopted for speech emotion recognition. Recently, some advanced classifiers based on sparse representation <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> have also been studied. Nevertheless, each classifier has its own advantages and disadvantages. To integrate the merits of different classifiers, ensembles of multiple classifiers have been investigated for speech emotion recognition <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>Affective speech features widely used for emotion recognition can be roughly divided into four categories: 1) acoustic features <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, 2) language features, such as lexical information <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, 3) context information, such as subject, gender, culture influences <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, 4) hybrid features <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b39">[40]</ref>, such as the integration of two or three features abovementioned.</p><p>Acoustic features, as one of the most popular affective features, mainly contain prosody features, voice quality features, and spectral features <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Pitch, loudness, and duration are commonly used as prosody features <ref type="bibr" target="#b40">[41]</ref>, since they express the stress and intonation patterns of spoken language. Voice quality features, as the characteristic auditory colouring of an individual voice, have been shown to be discriminative in expressing positive or negative emotions <ref type="bibr" target="#b41">[42]</ref>. The widely used voice quality features are the first three formants (F1, F2, F3), spectral energy distribution, harmonics-to-noise-ratio, pitch irregularity (jitter), amplitude irregularity (shimmer), and so on. Combining prosody features and voice quality features shows better performance than using prosody features alone <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In recent years, glottal features <ref type="bibr" target="#b44">[45]</ref> and voice source parameters <ref type="bibr" target="#b45">[46]</ref> have been used as more advanced voice quality features for speech emotion recognition. The third typical acoustic features are spectral features, computed from the short-term power spectrum of sound, such as Linear Prediction Cepstral Coefficients (LPCC), Log Frequency Power Coefficients (LFPC) and Mel-frequency Cepstral Coefficients (MFCC). Among them, MFCC is the most popular spectral feature, since it is able to model the human auditory perception system. In recent years, modulation spectral features <ref type="bibr" target="#b46">[47]</ref> from an auditory-inspired long-term spectro-temporal representation, and weighted spectral features <ref type="bibr" target="#b47">[48]</ref> based on local Hu moments, have also been studied. In addition, the newly-developed Geneva minimalistic acoustic parameter set (GeMAPS) <ref type="bibr" target="#b4">[5]</ref>, such as frequency, energy, spectral related features, has shown promising performance in speech emotion recognition.</p><p>Language features, which are computed based on the verbal contents of speech, are another important representation conveying emotion information. Note that, language features are usually combined with acoustic features for speech emotion recognition <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In <ref type="bibr" target="#b36">[37]</ref>, language features are extracted with the bag of n-gram and character n-gram approaches. Then the linguistic features are combined with acoustic features to predict dimensional emotions in a 3-D continuous space. In <ref type="bibr" target="#b35">[36]</ref>, by computing the weight of every word, a fourdimensional emotion lexicon for four emotion classes, i.e., anger, joy, sadness and neutral, are obtained. Then, integrating these feature representations via early fusion and late fusion is employed for speech emotion recognition.</p><p>Context information has also been investigated in recent literatures <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> for emotion recognition. In <ref type="bibr" target="#b37">[38]</ref>, the authors present a context analysis of subject and text on speech emotion recognition, and find that gender-based context information enhances recognition performance. In <ref type="bibr" target="#b38">[39]</ref>, the influences of cultural information on speech emotion recognition are explored. The authors claim that intra-cultural and multi-cultural emotion recognition paradigms give better performance than cross-cultural recognition.</p><p>Note that, since these hand-designed features mentioned above are low-level, they may not be discriminative enough to identify the subjective emotions. To tackle this issue, it may be feasible to employ deep learning techniques to automatically learn high-level affective features for speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DCNNS FOR AFFECTIVE FEATURE EXTRACTION</head><p>To utilize DCNNs in speech emotion recognition, three problems should be addressed. First, the DCNN input should be properly computed from 1-D speech signals. Second, DCNN's training requires a large amount of labeled data. Third, a feature pooling strategy is required to generate the global utterance-level feature representation from the DCNN outputs on local segments. In this section, we present the details of how the first two problems are addressed.</p><p>Fig. <ref type="figure" target="#fig_1">2</ref> illustrates the framework for affective feature extraction. From the original 1-D utterance, we first extract the static 2-D log Mel-spectrogram and then reorganize it into three channels of log Mel-spectrograms (static, delta and delta-delta). For data augmentation, the log Mel-spectrogram extracted from an utterance is divided into a certain number of overlapping segments as the DCNN input. More details about data augmentation can be found in Section V-B. Then the AlexNet DCNN model <ref type="bibr" target="#b6">[7]</ref> pre-trained on the large-scale ImageNet dataset is employed to perform fine-tuning tasks for affective feature extraction. We present more details of the two steps in the following two sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generation of DCNN Input</head><p>Because of the limited training data of speech emotion recognition, it is not possible to directly train a robust deep model. Motivated by the promising performance of available DCNN models, we propose to first initialize deep models with available DCNN models like AlexNet <ref type="bibr" target="#b6">[7]</ref>, then fine-tune it for transfer learning on target emotional datasets. Because available DCNN models take 2-D or 3-D images as inputs, we transform the raw 1-D speech spectrogram into 3-D array as the DCNN input.</p><p>In recent years, Abdel-Hamid et al., <ref type="bibr" target="#b13">[14]</ref> adopt the extracted log Mel-spectrogram and organize it into a 2-D array as the CNN input with a shallow 1-layer structure for speech recognition. Specifically, for each frame with a context window of 15 frames and 40 Mel-filter banks, they construct 45 (i.e., 15 × 3) 1-D feature maps with size 40 × 45. Then, the 1-D convolutional kernel is applied along the frequency axis. However, speech emotion recognition using DCNNs is different from speech recognition in <ref type="bibr" target="#b13">[14]</ref>. First, 1-D convolution operation along the frequency axis could not capture the temporal information, which is important for emotion recognition. Second, the divided segments with 15 frames (about 165 ms) used for speech recognition, are too short to distinguish emotions, since it has been found that only a speech segment length of more than 250 ms presents sufficient information for identifying emotions <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>.</p><p>To address these two issues, from the raw 1-D speech signals we generate the following overlapping Mel-spectrogram segments (abbreviated as Mel SS) as the DCNN input</p><formula xml:id="formula_0">Mel SS ∈ R F ×T ×C ,<label>(1)</label></formula><p>where F is the number of Mel-filter banks, T is the segment length corresponding to the frame number in a context window, and C (C = 1, 2, 3) represents the number of channels of Mel-spectrogram. Note that C = 1 denotes one channel of Mel-spectrogram, i.e., the original static spectrogram, C = 2 denotes the static and delta coefficients of Mel-spectrograms, and C = 3 represents three channels of Mel-spectrograms including the static, delta and delta-delta coefficients of Melspectrogram.</p><p>As an example described in Fig. <ref type="figure" target="#fig_1">2</ref>, we extract Mel SS with size 64 × 64 × 3 (F = 64, T = 64, C = 3) as the input of DCNN. This kind of three channels of spectrograms is analogous to the RGB image representation of visual data. In detail, for an utterance we adopt 64 Mel-filter banks from 20 to 8000 Hz to obtain the whole log Mel-spectrogram using a 25ms Hamming window size with 10ms overlapping. Then, a context window of 64 frames is applied to the whole log Melspectrogram to extract the static 2-D Mel-spectrogram segments with size 64×64. A frame shift size of 30 frames is used to produce such overlapping segments of Mel-spectrogram. Each segment hence includes a context window of 64 frames and its length is 10ms × 63+25ms = 655ms. In this case, the segment length is about 2.6 times longer than the suggested length of 250 ms in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, and conveys sufficient clues for emotion recognition.</p><p>Note that we set F as 64 because the input height-width ratio of our DCNN model is 1:1. Besides, F is usually set to be relatively large values for the usage of CNNs. For example, F is set to 40 in speech recognition <ref type="bibr" target="#b13">[14]</ref> and 60 in speech emotion recognition <ref type="bibr" target="#b15">[16]</ref>, respectively. Therefore, it is reasonable to set F as 64 in this work.</p><p>In speech recognition, the first and second temporal derivatives on the extracted acoustic features such as MFCC, are widely used as additional features. Similarly, after extracting the static 2-D Mel-spectrogram, we also calculate the first order and second order regression coefficients along the time axis as the delta and delta-delta coefficients of Mel-spectrogram. In this way, we organize the 1-D speech signals into three channels of Mel-spectrogram segments, i.e., Mel SS with size 64 × 64 × 3 (three channels: static, delta and delta-delta) as the DCNN input. Then, 2-D convolution operation along the frequency axis and time axis can be performed for DCNN's training on this input.</p><p>When using the AlexNet DCNN model <ref type="bibr" target="#b6">[7]</ref> for affective feature extraction, we have to resize the spectrogram 64×64×3 into 227×227×3, which is the input size of AlexNet. Since the extracted three channels of Mel-spectrograms can be regarded as the RGB image representation, we perform the resize operation with bilinear interpolation, which is commonly used for image resizing. Note that, the number of channels of Mel-spectrogram C and the segment length T may have an important impact on the learned deep features. Therefore, we will investigate their effects on the recognition accuracy in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DCNN Architecture</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our DCNN model includes five convolutional layers, three of which are followed by maxpooling layers, and two fully-connected layers. The last fully-connected layer consists of 4096 units, giving a 4096-D feature representation. It can be observed that this structure is identical to the one of AlexNet <ref type="bibr" target="#b6">[7]</ref>, which is trained on the large-scale ImageNet dataset. The initial parameters of this DCNN model can thus be copied from the AlexNet, making this DCNN model easier to train on speech emotion recognition tasks. In the followings, we introduce the computations and principles of convolutional layer, pooling layer and fully-connected layer, respectively.</p><p>Convolutional layer: A convolutional layer employs a set of convolutional filters to extract multiple local patterns at each local region in the input space, and produces many feature maps. This can be denoted as</p><formula xml:id="formula_1">(h k ) ij = (W k ⊗ q) ij + b k ,<label>(2)</label></formula><p>where (h k ) ij denotes the (i, j) element of the k-th output feature map, q represents the input feature maps, W k and b k denotes the k-th filter and bias, respectively. The symbol ⊗ represents 2-D spatial convolution operation.</p><p>Pooling layer: After each convolutional layer, a pooling layer may be used. The pooling layer aims to down-sample the obtained feature maps from the previous convolutional layers and produces a single output from local regions of convolution feature maps. Two widely used pooling operators are max-pooling and average-pooling. A max-pooling or average-pooling layer produces a lower resolution version of convolution layer activations by taking the maximum or average filter activation from different positions within a specified window.</p><p>Fully-connected layer: This layer integrates the outputs from previous layers to yield the final feature representations for classification or regression. The activation function is a sigmoid or tanh function. The output of fully-connected layers is computed by</p><formula xml:id="formula_2">x k = l W kl q l + b k ,<label>(3)</label></formula><p>where x k denotes the k-th output neuron, q l denotes the l-th input neuron, W kl represents the weight value connecting q l with x k , and b k denotes the bias term of y k .</p><p>Since fully-connected layers can be taken as convolutional layers with a kernel size of 1 × 1, Eq. ( <ref type="formula" target="#formula_2">3</ref>) can be reformulated as</p><formula xml:id="formula_3">(x k ) 1,1 = (W k ⊗ q) 1,1 + b k .<label>(4)</label></formula><p>For DCNN's training, Stochastic Gradient Descent (SGD) is commonly employed with parameters like the batch size of examples, the momentum value (e.g., 0.9), and the weight decay value (e.g., 0.0005). In this case, the weight w is updated by</p><formula xml:id="formula_4">v i+1 = 0.9 • v i -0.0005 • η • w i -η • ∂L ∂w |w i Di , w i+1 ⇐ w i +v i+1 ,<label>(5)</label></formula><p>where v denotes the momentum variable, η is the learning rate, i is the iteration number index, and ∂L ∂w |w i Di is the mean of derivatives of the i-th batch D i . The network hence can be updated by back-prorogation. More details of DCNN's training can be found in <ref type="bibr" target="#b6">[7]</ref>.</p><p>In our DCNN's training, we first initialize the network with parameters in the AlexNet, then fine-tune the network in emotion classification tasks, which uses the Mel SS with size 227 × 227 × 3 as input and multiple emotion classes as output. Note that, the number of classes used in the AlexNet model is 1000, but in our emotion classification tasks, the number of emotion categories is 6 or 7. Therefore, our used DCNN model differs from the AlexNet in the last two layers, where our model predicts 6 or 7 emotion categories.</p><p>After fine-tuning the AlexNet model, we take the output of its FC7 layer as the segment-level affective features x. Given N overlapping Mel-spectrogram segments as the inputs of the DCNN model, we can obtain a segment-level feature representation X=(x 1 , x 2 , • • • , x N ) ∈ R d×N with feature dimensionality d = 4096. This representation X hence is used as the input of the following DTPM algorithm to produce the global utterance-level features for emotion classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DTPM FOR UTTERANCE-LEVEL FEATURE REPRESENTATION</head><p>Because of the unfixed time of duration for speech utterances, the above-mentioned segment-level features X have a variant number of segments. This unfixed dimensionality makes such segment-level features not directly useable for emotion recognition. Therefore, we proceed to convert the segment-level features into an utterance-level feature representation with fixed dimensionality. This process, which is also called as feature pooling, is widely used in computer vision to convert the local features into the global features for image classification and retrieval.</p><p>There are two types of widely-used pooling strategies, i.e., average-pooling and max-pooling, which compute the averaged values and max values on each dimension, respectively. Note that, different pooling strategies are suited for different types of features, e.g., max-pooling is suited for sparse features. It is difficult to decide which pooling strategy is optimal for our segment-level affective features. Moreover, most of pooling strategies discard the temporal clues of speech signals, which might be important to distinguish emotions.</p><p>Our DTPM is motivated to simultaneously embed the temporal clues and find the optimal pooling strategy. It is partially inspired by the Spatial Pyramid Matching (SPM) <ref type="bibr" target="#b50">[51]</ref>, which embeds the spatial clues during feature pooling for image classification. In SPM, an image is first divided into regions at different scales, then feature pooling is conducted on each region. The final feature is hence the concatenation of the pooled features at each scale. Similarly, we also divide the segment-level features X into non-overlapping sub-blocks along the time axis at different scales, then conduct feature pooling on each sub-block. The final concatenated feature thus integrates the temporal clues at different scales. The details will be presented in Section IV-A.</p><p>To acquire the optimal pooling strategy, we formulate the feature pooling as where f p (X) denotes the acquired feature after pooling operation, N is the number of segment features, and p controls the pooling strategy. E.g., p = 1 corresponds to average-pooling, whereas p = ∞ corresponds to max-pooling. To testify the advantages of the optimal pooling strategy, we compare it with average-pooling and max-pooling in the latter experiments, as shown in Section VI. From Eq. ( <ref type="formula" target="#formula_5">6</ref>), it can be observed that the parameter p decides the performance of the pooling. In recent years, it has been found in <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> that the p value has an important impact on the image classification accuracy. Therefore, we proceed to acquire an optimal p for our affective features. More details will be presented in Section IV-B.</p><formula xml:id="formula_5">f p (X)=   1 N N j=1 |x j | p   1 p ,<label>(6)</label></formula><p>In a word, given the segment-level features X, DTPM aggregates X to produce the global utterance-level features v p L (X) with the optimal pooling parameter p and pyramid level ℓ = 0, 1, 2, • • • , L. For example, Fig. <ref type="figure" target="#fig_2">3</ref> presents the framework of DTPM. The original segment-level features X are divided at three scales with ℓ = 0, 1, 2. The final feature is generated by concatenating the features at each scale with the optimal pooling parameter p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Temporal Pyramid Matching</head><p>Temporal Pyramid Matching (TPM) first divides the segment-level features X at multiple levels. Specifically,</p><formula xml:id="formula_6">X=(x 1 , x 2 , • • • , x N ) ∈ R d×N is equally divided into 2 ℓ</formula><p>successive non-overlapping sub-blocks along the time axis at different levels with ℓ = 0, 1, 2, • • • , L. For the ℓ-th level, this can be expressed as</p><formula xml:id="formula_7">X = (X 1 , X 2 , • • • , X m ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_8">m=2 ℓ , ℓ = 0, 1, 2, • • • , L. For a sub-block X m = (x 1 , x 2 , • • • , x n ) ∈ R d×n</formula><p>with n segments, we use the pooling strategy in Eq. ( <ref type="formula" target="#formula_5">6</ref>) to produce fixed-length d-dimension feature representation f p (X m ), i.e.,</p><formula xml:id="formula_9">f p (X m )=   1 n n j=1 |x j | p   1 p ,<label>(8)</label></formula><p>where p = 1 corresponds to average-pooling, whereas p = ∞ corresponds to max-pooling. The optimal p will be computed in Section IV-B. The generated feature f p (X m ) at different scales encodes different temporal clues. For example, compared with the feature at 0-th level, the pooled feature at the second level embeds more refined temporal clues. We thus aggregate the pooling results on all sub-blocks at different levels into the final global utterance-level feature representation.</p><p>Let</p><formula xml:id="formula_10">Γ ℓ (X)=(f p (X 1 ), f p (X 2 ), • • • , f p (X m ))</formula><p>denote the concatenated feature of X at pyramid level ℓ. Then we can get the global utterance-level features v p L (X) of TPM by means of concatenating all Γ ℓ (X) at different pyramid level, i.e.,</p><formula xml:id="formula_11">v p L (X) = ( 1 2 L Γ 0 , 1 2 L Γ 1 , 1 2 L-1 Γ 2 , • • • , 1 2 Γ L ),<label>(9)</label></formula><p>where Γ ℓ (X) is abbreviated as Γ ℓ . In the final utterance-level features, we set higher weights for the features on higher levels, which embeds more refined temporal clues. This is also similar to the weighting strategy in SPM <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal Lp-norm Pooling</head><p>To improve the discriminative power of v p L (X), we employ the class separability criteria according to the Marginal Fisher Analysis (MFA) <ref type="bibr" target="#b53">[54]</ref> to learn the optimal Lp-norm pooling. Let u p (X) denote the final utterance-level features after optimal Lp-norm pooling, then we get</p><formula xml:id="formula_12">u p (X) = α T v p L (X), (<label>10</label></formula><formula xml:id="formula_13">)</formula><p>where α is a diagonal matrix used to weight v p L (X), making v p L (X) discriminant. In the following, v p L (X) is abbreviated as v p , and u p (X) is abbreviated as u p . Therefore, the task is acquiring the optimal α and p to make the final feature u p as discriminative as possible.</p><p>To optimize both α and p simultaneously, the objective function should maximize the inter-class separability while minimize the inner-class separability. This induces our objective function, i.e.,</p><formula xml:id="formula_14">α * , p * = arg max α,p Ω(α, p) := α T S b (p)α α T S ω (p)α ,<label>(11)</label></formula><p>where S b (p) represents the inter-class separability, S ω (p) represents the inner-class separability. They are computed by</p><formula xml:id="formula_15">S b (p) = i j∈N - k (i) (v p i -v p j )(v p i -v p j ) T , S ω (p) = i j∈N + k (i) (v p i -v p j )(v p i -v p j ) T ,<label>(12)</label></formula><p>where N - k (i) denotes the index set for k nearest neighbors of the pooling data v p i from different classes, and N + k (i) represents the k nearest neighbors of the pooling data v p i from the same classes.</p><p>Eq. ( <ref type="formula" target="#formula_14">11</ref>) can be solved by optimizing α and p alternatively. When fixing p, this objection function is transformed into the classical Linear Discriminant Analysis (LDA) <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b55">[56]</ref> problem. In this case, S b (p) and S ω (p), represent the between-class scatter matrix and the within-class scatter matrix, respectively. Therefore, the optional solution α * can be obtained with the closed-form solution for a fixed p:</p><formula xml:id="formula_16">α * = arg max λ α , s.t.S b α = λS ω α. (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>The diagonal vector for the optional solution α * is the eigenvector corresponding to the largest eigenevalue λ max . When fixing α, the optimizing problem in Eq. ( <ref type="formula" target="#formula_14">11</ref>) has no closed-form solution. Nevertheless, it can be solved with a gradient descent process in an iterative way. Specifically, with a fixed α, we can get</p><formula xml:id="formula_18">∼ S b (p) = α T S b (p)α = i j∈N - k (i) (u p i -u p j ) 2 , ∼ S ω (p) = α T S ω (p)α = i j∈N + k (i) (u p i -u p j ) 2 . (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>The partial derivatives of ∼ S b (p) and ∼ S ω (p) related to p are then computed by</p><formula xml:id="formula_20">∂ ∼ S b ∂p = 2 i j∈N - k (i) (u p i -u p j )α T (β i -β j ), ∂ ∼ Sω ∂p = 2 i j∈N + k (i) (u p i -u p j )α T (β i -β j ),<label>(15)</label></formula><p>where β denotes the Hadamard product β = v p • ln v. Then we can get the partial derivative of Eq. ( <ref type="formula" target="#formula_14">11</ref>) with respect to p:</p><formula xml:id="formula_21">∇p = ∂ ∂p Ω(α, p) = 1 ∼ S 2 ω ( ∂ ∼ S ω ∂p ∼ S b - ∂ ∼ S b ∂p ∼ S ω ).<label>(16)</label></formula><p>The p value can be updated along the gradient direction with a step size γ, i.e.,</p><formula xml:id="formula_22">p (t+1) = p (t) + γ • ∇p,<label>(17)</label></formula><p>where the superscript t denotes the t-th iteration. In our implementation, the iteration stops if the number of iterations exceeds the permitted number N iter . After acquiring the final feature representation u p (X), we use it for emotional classification with classifiers like SVM.</p><p>Our training strategy divides the utterances into segments. This enlarges the training set for DCNNs, but is potential to make emotion recognition on each segment more difficult if the segment is too short. We have carefully set the length of each segment to 655ms, which is about 2.6 times longer than the suggested 250ms for emotion recognition in <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. Therefore, each segment should preserve sufficient clues for emotion recognition. To conduct utterance-level emotion recognition, we generate utterance-level features with the DTPM, which aggregates segment-level features at different scales with Lp-norm pooling. DTPM is inspired by the Spatial Pyramid Matching (SPM) <ref type="bibr" target="#b50">[51]</ref> commonly used in image classification. SPM aggregates low-level features from image patches to form a global feature discriminative to high-level semantics. Similar to SPM, DTPM is potential of learning a discriminative utterance-level feature from local segment-level features. In the following section, we will testy the validity of this training strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We test the proposed method on four public datasets, including the Berlin dataset of German emotional speech (EMO-DB) <ref type="bibr" target="#b20">[21]</ref>, the RML audio-visual dataset <ref type="bibr" target="#b21">[22]</ref>, the eNTERFACE05 audio-visual dataset <ref type="bibr" target="#b22">[23]</ref>, and the BAUM-1s audio-visual dataset <ref type="bibr" target="#b23">[24]</ref>.</p><p>EMO-DB: The acted EMO-DB speech corpus <ref type="bibr" target="#b20">[21]</ref> contains 535 emotional utterances with seven different acted emotions: anger, joy, sadness, neutral, boredom, disgust and fear. Ten professional native German-speaking actors (five female and five male) are asked to simulate these emotions, giving 10 German utterances (five short and five long sentences) which are able to be used in everyday communication. These actors are required to read these predefined sentences in the targeted seven emotions. The recordings in this dataset are taken in an anechoic chamber with high-quality recording equipment and produced at a sampling rate of 16 kHz with a 16-bit resolution and mono channel. The audio files are on average around 3 seconds long. A human perception test with other 20 subjects is conducted to evaluate the quality of the recorded data.</p><p>RML: The acted RML audio-visual dataset <ref type="bibr" target="#b21">[22]</ref>, collected from Ryerson Multimedia Research Lab, Ryerson University, contains 720 utterances of eight subjects from different gender and culture, in six different speaking languages. It consists of six emotions: anger, disgust, fear, joy, sadness, and surprise. The samples were recorded at a sampling rate of 44,100 Hz with a 16-bit resolution and mono channel. The audio files are on average around 5 seconds long. To ensure the context independency of speech samples, more than ten reference sentences for each emotion are presented. At least two participants who do not know the corresponding language are employed in human perception test to evaluate whether the correct emotion is expressed.</p><p>eNTERFACE05: The eNTERFACE05 <ref type="bibr" target="#b22">[23]</ref> is an induced audio-visual emotion dataset with six basic emotions, i.e., anger, disgust, fear, joy, sadness, and surprise. 42 subjects from 14 different nationalities are included. Each subject is asked to listen to six successive short stories, each of which is used to induce a particular emotion. Two experts are employed to evaluate whether the reaction expresses the intended emotions in an unambiguous way. The speech utterances are pulled from video files of the subjects speaking in English. The sampling rate is 48 kHz for audio. The audio files are on average around 3 seconds long. Overall, the eNTERFACE05 dataset contains 1290 utterances.</p><p>BAUM-1s: The spontaneous BAUM-1s <ref type="bibr" target="#b23">[24]</ref> audio-visual dataset contains eight emotions (joy, anger, sadness, disgust, fear, surprise, boredom and contempt), and four mental states (unsure, thinking, concentrating and bothered). It has 1222 utterances collected from 31 Turkish subjects, 17 of which are female. Emotion elicitation using video clips is employed to get spontaneous audio-visual expressions. Each utterance is given an emotion label by using a majority voting over the five annotators. The audio files have a sampling rate of 48 kHz, and the average time of duration is around 3 seconds. As done in <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, this work aims to identify six basic emotions (joy, anger, sadness, disgust, fear, surprise), giving 521 utterances in total for experiments. Note that, BAUM-1s, is a latest audio-visual emotional data set released in 2016. Moreover, BAUM-1s records spontaneous emotions rather than acted emotions, thus defines a more challenging emotion recognition problem than the aforementioned datasets like EMO-DB and eNTERFACE05. Therefore, BAUM-1s is a reasonable and challenging testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup 1) Details of DCNN Training:</head><p>Each of the four emotional datasets contains a limited number of samples. It is thus desirable to generate more samples for DCNN's training. To address this issue, we directly split an utterance into a certain number of overlapping segments. Each of the segments is labeled with the utterance emotion category for DCNN's training. In this case, the number of training samples is decided by the overlap length (a frame shift size) between two adjacent segments, i.e., smaller overlap results in a larger number of training samples. However, as suggested in <ref type="bibr" target="#b49">[50]</ref>, the overlap length should be larger than 250 ms in speech emotion recognition. Therefore, we set the overlap length as 30 frames, which is about 10ms × 29+25ms = 315ms. As a result, when extracting Mel-spectrogram segments with size 64 × 64 × 3, we can significantly augment the size of training data, i.e., from 535 utterances to 11,842 segments for the EMO-DB dataset, from 720 utterances to 11,316 segments for the RML dataset, from 1290 utterances to 16,186 segments for the eNTERFACE05 dataset, and 521 utterances to 6368 segments for the BAUM-1s dataset, respectively.</p><p>Note that, segmenting an utterance into small segments, was widely used for discrete emotion classification, as in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>. Although it is not necessarily true that the emotion labels in all segments divided from an utterance are equivalent to that of the whole utterance, we can still employ DCNNs to learn effective segment-level features from the segmentlevel emotions, which can be utilized to predict utterance-level emotions.</p><p>The structure of the used DCNN model <ref type="bibr" target="#b6">[7]</ref> is presented in Fig. <ref type="figure" target="#fig_1">2</ref>. The DCNN model is trained with mini-batch size of 30, Stochastic Gradient Descent (SGD) with a momentum of 0.9, and a learning rate of 0.001. The maximum number of epochs is set as 300. We perform DCNNs on the MATLAB2014 platform with the MatConvNet package <ref type="bibr" target="#b58">[59]</ref>, which is a MATLAB toolbox implementing CNNs for computer vision applications. One NVIDIA GTX TITAN X GPU with a 12GB memory is used to train DCNNs with a GPU mode. We employ the LIBSVM package <ref type="bibr" target="#b59">[60]</ref> with the linear kernel function and the one-versus-one strategy for multi-class classification. When implementing optimal Lp-norm pooling, we set the number of permitted iteration N iter = 50, and the number of nearest neighbors k = 20, as done in <ref type="bibr" target="#b51">[52]</ref>.</p><p>It is noted that the used DCNN model called AlexNet, is firstly reported in <ref type="bibr" target="#b6">[7]</ref> with input size of 224 × 224 × 3. However, in many practical implementations such as imagenet-caffe-alex, available at http://www.vlfeat.org/matconvnet/pretrained/, researchers commonly use input size 227 × 227 × 3 rather than 224 × 224 × 3.</p><p>2) Evaluation Methods: As suggested in <ref type="bibr" target="#b60">[61]</ref>, test-runs are implemented by using a speaker-independent Leave-One-Speaker-Out (LOSO) or Leave-One-Speakers-Group-Out (LOSGO) cross-validation strategy, which are usually adopted in most real applications. Specifically, for the EMO-DB and RML datasets, we employ the LOSO scheme. For the eNTER-FACE05 and BAUM-1s datasets, we use the LOSGO scheme with five speakers group, similar to <ref type="bibr" target="#b23">[24]</ref>. Note that, we adopt the speaker-independent test-runs, which is more realistic and challenging than the speaker-dependent test-runs. Therefore, we only compare with works also using the same setting and wont compare with works like <ref type="bibr" target="#b57">[58]</ref> that report speakerdependent results. The Weighted Average Recall (WAR), also known as the standard accuracy, is reported to evaluate the performance of speech emotion recognition. Here, WAR denotes the recognition rates of individual classes weighted by the class distribution.</p><p>We evaluate the performance of two methods, i.e., DCNN-Average, and DCNN-DTPM. The details of these two methods are described below.</p><p>DCNN-Average also uses DCNNs as feature extractor. After extracting features on each Mel-spectrogram segment with DCNNs, the conventional average-pooling is employed over all the segments to produce the final fixed-length global utterance-level features. Then the linear SVM classifier is adopted for emotion identification. Therefore, we compare our method to DCNN-Average to show the validity of the proposed DTPM.</p><p>DCNN-DTPM is our proposed method described in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Results and Analysis</head><p>We use Mel-spectrogram segments with size Mel SS ∈ R F ×T ×C as the DCNN input, where F is the number of Melfilter banks commonly set as 64, T is the number of frames in each segment, and C represents the number of channels of Mel-spectrogram. The parameters C and T largely affects the amount of affective cues DCNNs could perceive. In this part, we first investigate the effects of C and the validity of our DCNN's training strategy. Then we will validate the effects  of T on the recognition accuracy and compare to the state-ofthe-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Effects of the number of channels in Mel-spectrogram:</head><p>To investigate the effects of the number of channels, i.e., C, we use a simplified DCNN model for feature extraction. This DCNN model contains five layers (Conv1-Pool1-Conv2-Pool2-Conv3-Conv4-FC5) and finally generates a 600-D feature representation. Specifically, the size of the input is 64×64×C, the first three convolutional layers (Conv1, Conv2, Conv3) have 128 kernels of size 5 × 5 with a stride of 1. The fourth convolution layer (Conv4) has 256 kernels of size 4 × 4 with a stride of 1. We adopt average-pooling for the pooling layers. Pooling size of 3 × 3 with a stride of 3 is used for Pool1, and 2 × 2 with a stride of 2 is used for Pool2. The fully-connected layer in FC5 has 600 neurons, giving a 600-D feature representation. For the DCNN inputs with different C, we change the number of input channel of this DCNN model.</p><p>Table <ref type="table" target="#tab_3">I</ref> presents performance comparisons with different values of C. Note that, for DCNN-DTPM, we test different pyramid levels with L = 1, 2, and 3, respectively. We present the best performance as well as the corresponding L in Table <ref type="table" target="#tab_3">I</ref>. From the results, we can make the following two observations. First, setting C = 3 shows the best performance at most cases, and constantly outperforms the case when C = 1. This indicates that the first order and second order derivatives of 2-D Mel-spectrogram segments preserve helpful cues for emotion recognition. The fact that, C = 3 slightly outperforms C = 2 indicates that further introducing higher order of derivatives may not significantly boost the performance. Nevertheless, C = 3 results in an input similar to the RGB image representation. Accordingly, we set C = 3 in our following experiments.</p><p>Second, DCNN-DTPM clearly outperforms DCNN-average on four datasets. It is also clear that dividing the segment-level features into multiple levels, i.e., setting L larger than 1, improves the performance of DCNN-DTPM. This demonstrates the advantages of our DTPM over the conventional averagepooling strategy when coding the local segment-level features.  2) The performance of DCNN pre-trained on ImageNet: The above experiment suggests C = 3, corresponding to a DCNN input similar to the RGB image representation. Such input can be directly processed by available DCNNs pretrained on large-scale image datasets. In this experiment, we first directly use the original AlexNet <ref type="bibr" target="#b6">[7]</ref> to extract affective features. Then, we fine-tune the AlexNet on the target emotion recognition tasks and test the performance of the fine-tuned model. Note that, to use the Alexnet we resize 64 × 64 × 3 spectrogram to 227 × 227 × 3 with bilinear interpolation.</p><p>Table <ref type="table" target="#tab_0">II</ref> gives the recognition performance obtained by the AlexNet without fine-tuning. It can be observed that, the AlexNet shows reasonably good performance, e.g., on the RML dataset it gives performance close to the results in Table <ref type="table" target="#tab_3">I</ref> obtained with the simplified DCNN model. This demonstrates that, although the AlexNet is trained on an independent image dataset, it also extracts discriminative affective features from emotional speech datasets with our DCNN input.</p><p>We further show the performance of the fine-tuned AlexNet in Table <ref type="table" target="#tab_1">III</ref>. It is easy to observe that the fine-tuning procedure significantly boosts the discriminative power of the extracted features. After fine-tuning, the best performance of DCNN-DTPM comes up to 87.31%, 69.70%, 76.56%, and 44.61%, respectively on four datasets. Note that the recognition performance on the spontaneous BAUM-1s dataset is much lower than the obtained performance on other three emotional datasets. This shows that the spontaneous emotions are more difficult to be identified well than the acted and induced emotions. It is also clear that the fine-tuned AlexNet significantly outperforms the simplified DCNN in Table <ref type="table" target="#tab_3">I</ref>, which is trained directly on the target datasets. This indicates the advantages of our DCNN's training strategy, i.e., using available models trained on image datasets to initialize our DCNNs for fine-tuning. Moreover, the experimental results also show the validity of our generated DCNN input.</p><p>The comparisons among Table <ref type="table" target="#tab_3">I</ref>, Table <ref type="table" target="#tab_0">II</ref>, and Table III clearly show the advantages of our training strategy, i.e., initialize with the AlexNet, then fine-tune on the target emotional speech datasets. The reason why the AlexNet helps emotion recognition might be because we convert the audio signals into an image-like representation, as well as the deep structure and huge training data of the AlexNet.</p><p>3) Effects of the segment length: The segment length T decides the duration of audio signals the DCNN model processes. It hence may largely affect the discriminative power of the extracted affective features. We thus show the effects of T on the emotion recognition performance.</p><p>The length of the shortest utterance is 1.23 second long  on the EMO-DB dataset, and 1.12 second long on the eN-TERFACE05 dataset. Accordingly, for the EMO-DB dataset and the eNTERFACE05 dataset, we test T ranges in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">80,</ref><ref type="bibr">100,</ref><ref type="bibr">120]</ref>, where T = 120 corresponds to about 1.22 second, which is close to the length of the shortest utterance. The length of the shortest utterance is 3.27 seconds long on the RML dataset. We thus test T ranges in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">80</ref>, 100, 120, 140, • • • , 320] on the RML dataset. On the BAUM-1s dataset, we test T ranges in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr">80]</ref>, since the length of the shortest utterance is 0.768 seconds long. For some certain utterances shorter than T , we simply repeat the first frame and last frame in an utterance so that the length of this utterance equals to T . Note that for T = 15, as a benchmark used in speech recognition, the overlap length of Mel-spectrogram segments is 15 frames,  whereas for T ≥ 30 the overlap length is 30 frames. All spectrograms with different T are resized to be 227 × 227 × 3 with bilinear interpolation as the input of DCNN. Fig. <ref type="figure">4</ref>, Fig. <ref type="figure">5</ref>, Fig. <ref type="figure" target="#fig_5">6</ref>, and Fig. <ref type="figure" target="#fig_6">7</ref> show the effects of T on four datasets. Table <ref type="table" target="#tab_2">IV</ref> presents the best performance and the optimal T on four datasets. From the experimental results, we can draw two conclusions. First, it can be observed that larger T is helpful for better performance. However, too large T does not constantly improve the performance. Table <ref type="table" target="#tab_2">IV</ref> shows that the best performance on four datasets are 87.31%, 75.34%, 79.25%, and 44.61%, respectively. The corresponding optimal T on four datasets are 64, 220, 80, and 64, respectively. This may be because setting larger T decreases the number of generated training samples for DCNNs. Therefore, DCNN-DTPM does not always improve the performance with the increase of the segment length. Second, the four curves shows that the recognition performance of DCNN-DTPM remains stable when T is larger than 64. Setting T = 64 generally gives promising performance on four datasets. This might be because the DTPM also considers the temporal clues, thus makes the algorithm more robust to T . It is also interesting to observe that segment length of 15 frames, i.e., T = 15, widely used for speech recognition <ref type="bibr" target="#b13">[14]</ref>, does not get promising emotion recognition performance. This might be because T = 15 is too short to provide sufficient temporal cues for distinguishing emotions.   To further investigate the recognition accuracy, we present the confusion matrix corresponding to the results of DCNN-DTPM in Table <ref type="table" target="#tab_2">IV</ref>. Fig. <ref type="figure">8</ref> shows that on the EMO-DB dataset, "neutral" is identified with the highest accuracy of 93.15%, and the other six emotions are classified with accuracies higher than 80%. Fig. <ref type="figure">9</ref> indicates that only two emotions, i.e., "anger" and "surprise", are distinguished with accuracies higher than 82% on the RML dataset. On the eNTERFACE05 dataset, "anger", "joy" and "surprise" can be recognized with accuracies of 87.50%, 81.31%, 84.66%, respectively, as shown in Fig. <ref type="figure" target="#fig_0">10</ref>. Fig. <ref type="figure" target="#fig_0">11</ref> indicates that on the BAUM-1s dataset "joy" and "sadness", are classified with accuracies of 50.67%, 41.74%, respectively, whereas the other four emotions are identified with accuracies lower than 40%. The low recognition accuracies on the BAUM-1s dataset demonstrate the difficulty in recognizing spontaneous emotions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Comparisons with the state-of-the-art results:</head><p>We compare our method with some previous works on four public datasets in Table <ref type="table">V</ref>. We compare with these works because they also use the speaker-independent LOSO or LOSGO testruns, which are more reasonable than the speaker-dependent test-runs used in <ref type="bibr" target="#b21">[22]</ref>. Note that some pervious works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b61">[62]</ref> also employ Unweighted Averaged recall (UAR), which is used to better reflect unbalance among classes, as the evaluate measures of recognition performance, although we have presented the common WAR for performance evaluation. Accordingly, we present both WAR and UAR on these four datasets for a fair comparison.</p><p>From Table <ref type="table">V</ref>, we can see that our method is very competitive to the state-of-the-art results. Specially, on the EMO-DB dataset our method performs best, compared with <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. On the RML dataset, our method gives much better performance than <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>. On the eNTERFACE05 dataset, our method obviously outperforms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b23">[24]</ref>, and presents a little lower performance than <ref type="bibr" target="#b61">[62]</ref>. On the BAUM-1s dataset, our method also clearly outperforms <ref type="bibr" target="#b23">[24]</ref>, i.e., our 44.61% vs. 29.41% of <ref type="bibr" target="#b23">[24]</ref> in term of WAR. Therefore, although the BAUM-1s is a relatively small dataset, it defines a challenging emotion recognition problem and also validates the advantages of the proposed algorithm. Note that in <ref type="bibr" target="#b60">[61]</ref>, the authors employ 6552 LLD acoustic features such as prosody and MFCC for emotion classification. This shows the advantages of our learned affective features using DCNNs. <ref type="bibr" target="#b11">[12]</ref> also uses a DNN to learn discriminative features. Different from our work, <ref type="bibr" target="#b11">[12]</ref> learns features from 6552 LLD acoustic features, rather than from the raw speech signals or the spectrogram. This thus clearly shows the advantages of our DCNN model, i.e., using three channels of spectrograms as input and coding raw DCNN features with DTPM to get the final feature representation. <ref type="bibr" target="#b61">[62]</ref> reports the best performance of by using the large AVEC-2013 feature set <ref type="bibr" target="#b64">[65]</ref> on the EMO-DB dataset, and the large ComParE feature set [66] on the eNTERFACE05 dataset.</p><p>Our experimental results show that our method gets impressive recognition accuracies in comparison with the stateof-the-art works. For example, we report an UAR accuracy of 86.30% on the EMO-DB dataset, on which outperforms all the three compared works, i.e., 79.1% by <ref type="bibr" target="#b11">[12]</ref>, 84.6% by <ref type="bibr" target="#b60">[61]</ref>, 86.0% by <ref type="bibr" target="#b4">[5]</ref> and 86.1% by <ref type="bibr" target="#b61">[62]</ref>. As far as we know, this is an early work using DCNNs pre-trained on image domain for emotion recognition. The success of this work guarantees further investigation in this direction. These distinctive characteristics distinguish our work from existing efforts on speech emotion recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSIONS</head><p>The pyramid level L controls the number of levels in DTPM, thus may affect the recognition performance. In our experiments, we investigate the effects of L with a value range between 1 and 3. We do not use L ≥ 4, since the resulted feature dimensionality is too large. As shown in the above experimental results, L = 2 or L = 3 generally gets the optimal results. This indicates that dividing the Melspectrogram into multiple levels, i.e., L ≥ 2, helps to improve the performance. It also can be inferred that our algorithm is not quite sensitive to L, and setting L = 2 or L = 3 is a reasonable option at most cases.</p><p>To verify the effectiveness of our Lp-norm pooling, we compare it with two commonly used pooling methods, i.e., average-pooling and max-pooling, in Table <ref type="table" target="#tab_8">VI</ref>. This is conducted by modifying the value of p in DTPM, e.g., p = 1 corresponds to average-pooling, whereas p = ∞ corresponds to max-pooling. It can be seen from Table VI that our Lp-norm pooling performs better than the other two pooling methods. It also can be seen that, it is hard to decide which pooling strategy performs better for a specific task with experience. E.g., max-pooling performs better than average-pooling on the RML and eNTERFACE05 datasets, but average-pooling performs better on the EMO-DB and BAUM-1s datasets. This thus shows the necessarily of pooling strategy learning.</p><p>Since the Mel-spectrogram domain is represented as a 2-D matrix, it is natural to utilize CNNs to learn emotion information. To this end, it is straightforward to train a deep model on 64 × 64 spectrogram data. However, Table <ref type="table" target="#tab_3">I</ref>  It is a challenging problem to collect and annotate large numbers of utterances for emotion classification due to the difficulty of emotion annotation. At present, on existing small emotional speech datasets, it is a good choice to fine-tune pre-trained deep models. As shown in our experiments, finetuned the AlexNet pre-trained on the ImageNet works well on speech emotion recognition tasks. The reason why the AlexNet helps emotion recognition might be because we convert the audio signals into an image-like representation as well as the strong feature learning ability of the AlexNet, e.g., higher-level convolutions gradually deduce semantics from larger receptive fields. The extracted three channels of Mel-spectrograms are analogous to the RGB image representation. This representation makes it feasible to first generate meaningful lowlevel time-frequency features with low-level 2-D convolutions, then deduce more discriminative features with higher-levels of convolutions. Besides, three channels of Mel-spectrograms may characterize emotions as certain shapes and structures, which are thus able to be effectively perceived by the AlexNet pre-trained on the image domain.</p><p>The proposed method is based on the AlexNet. Similar to the AlexNet for ImageNet large-scale classification, our method is capable of learning on million-scale training data with the commonly used GPU, e.g., NVIDIA TITAN X. It is thus also interesting to retrain deep models on larger emotional speech datasets than the used EMO-DB, eNTERFACE05, and BAUM-1s in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS AND FUTURE WORK</head><p>This paper is motivated by how to employ DCNNs for automatic feature learning on speech emotion recognition tasks. We present a new method combining DCNNs with DTPM for  <ref type="bibr">[69]</ref>. Note that this work focuses on global utterance-level emotion classification and proposes the algorithm accordingly, i.e., first uses DCNNs to extract segment-level feature, then aggregates segment-level features with DTPM to form a global feature, and finally performs emotion classification with the linear SVM. Therefore, this algorithm is still not capable to deal with continuous dimensional emotion recognition. To tackle this problem, one possible way is to consider extra temporal cues and combine CNN and LSTM <ref type="bibr" target="#b16">[17]</ref>, which is commonly used to select and accumulate frame-level features for video categorization. This will be one of our future works. Moreover, there are many open issues that still need to be further studied to make emotion recognition work well in reallife settings. For example, as show in Table <ref type="table">V</ref>, it is more difficult for our model to recognize the spontaneous emotions. It is also necessary to take the personality into consideration because different persons may have different ways to express emotions. Additionally, it is also interesting to apply our proposed method for affective analysis of music video [70].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. ACKNOWLEDGEMENTS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An overview of the proposed speech emotion recognition framework using DCNNs and DTPM: (1) Three channels of log Mel-spectrograms (static, delta and delta-delta) are extracted and divided into N overlapping segments as the DCNN input. (2) A DCNN model is employed for automatic feature learning on each segment to generate segment-level features. (3) A DTPM scheme is designed to concatenate the learned segment-level features to form a global utterance-level feature representation. (4) With utterance-level features, a linear SVM classifier is employed to predict utterance-level emotions.</figDesc><graphic coords="2,102.89,65.89,82.02,144.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The flowchart of our DCNN model for affective feature extraction. Three channels of log Mel-spectrograms with size 64 × 64 × 3 (static, delta and delta-delta) are firstly produced, and then are resized to 227 × 227 × 3 as the DCNN input. The DCNN model is first initialized with the AlexNet [7], then is fine-tuned on target emotional datasets. The 4096-D FC7 output is finally used as the segment-level affective features.</figDesc><graphic coords="4,77.89,77.59,74.32,128.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The framework of Discriminant Temporal Pyramid Matching (DTPM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. The effects of T on the EMO-DB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. The effects of T on the eNTERFACE05 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The effects of T on the BAUM-1s dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>87</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II SPEAKER</head><label>II</label><figDesc>-INDEPENDENT RECOGNITION ACCURACY (%) USING THE ALEXNET WITHOUT FINE-TUNING AS A FEATURE EXTRACTOR. L * DENOTES THE VALUE OF L CORRESPONDING TO THE BEST</figDesc><table><row><cell></cell><cell cols="2">PERFORMANCE.</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="4">EMO-DB RML eNTERFACE05 BAUM-1s</cell></row><row><cell>DCNN-Average</cell><cell>72.35</cell><cell>59.46</cell><cell>51.33</cell><cell>36.10</cell></row><row><cell cols="3">DCNN-DTPM (L  *  ) 76.27 (3) 62.40 (3)</cell><cell>56.08 (2)</cell><cell>38.42 (2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III SPEAKER</head><label>III</label><figDesc>-INDEPENDENT RECOGNITION ACCURACY (%) USING THE FINE-TUNED ALEXNET AS A FEATURE EXTRACTOR. L * DENOTES THE VALUE OF L CORRESPONDING TO THE BEST PERFORMANCE.</figDesc><table><row><cell>Dataset</cell><cell cols="4">EMO-DB RML eNTERFACE05 BAUM-1s</cell></row><row><cell>DCNN-Average</cell><cell>82.65</cell><cell>66.17</cell><cell>72.80</cell><cell>42.26</cell></row><row><cell cols="3">DCNN-DTPM (L  *  ) 87.31 (2) 69.70 (2)</cell><cell>76.56 (2)</cell><cell>44.61 (2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV THE</head><label>IV</label><figDesc>BEST RECOGNITION ACCURACY (%) AND CORRESPONDING T USING THE FINE-TUNED ALEXNET ON FOUR DATASETS. L * DENOTES THE VALUE OF L CORRESPONDING TO THE BEST PERFORMANCE.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I SPEAKER</head><label>I</label><figDesc>-INDEPENDENT ACCURACY (%) COMPARISONS BY SETTING DIFFERENT VALUES OF C USING A SIMPLIFIED DCNN MODEL. THE SIZE OF THE SPECTROGRAM IS 64 × 64 × C . L * DENOTES THE VALUE OF L CORRESPONDING TO THE BEST PERFORMANCE OF DCNN-DTPM.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>EMO-DB</cell><cell></cell><cell></cell><cell>RML</cell><cell></cell><cell cols="3">eNTERFACE05</cell><cell></cell><cell>BAUM-1s</cell><cell></cell></row><row><cell>C</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>DCNN-Average</cell><cell>73.86</cell><cell>77.44</cell><cell>78.92</cell><cell>59.25</cell><cell>61.84</cell><cell>61.19</cell><cell>62.33</cell><cell>65.01</cell><cell>66.42</cell><cell>36.49</cell><cell>38.21</cell><cell>38.62</cell></row><row><cell cols="13">DCNN-DTPM (L  *  ) 77.03 (3) 82.69 (2) 83.53 (3) 61.48 (2) 64.88 (3) 64.21 (3) 65.95 (1) 69.88 (2) 70.25 (2) 38.74 (2) 39.05 (3) 40.57 (2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>and Table IV indicate that directly using 64 × 64 features to train a deep model obtains lower performance than our fine-tuned AlexNet. The reason might be the limited training data of speech emotion recognition. This motivates us to use the pretrained AlexNet, which is already trained with millions of images and shows reasonably good performance in emotion feature extraction as shown in Table II. Therefore, we initialize a deep model with the same structure and parameters of the AlexNet and fine-tune it on target emotional datasets. Experimental results in Table II and Table IV have shown the effectiveness of the pre-trained AlexNet as well as our fine-tuned deep model.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VI RECOGNITION</head><label>VI</label><figDesc>ACCURACY (%) COMPARISON OF THREE POOLING METHODS IN DTPM USING 64 × 64 × 3 MEL-SPECTROGRAM AND L = 2 ON THREE DATASETS.p * DENOTES THE MEAN OPTIMAL VALUES OF p IN LOSO OR LOSGO TEST-RUNS. automatic affective feature learning. A DCNN is used to learn discriminative segment-level features from three channels of log Mel-spectrograms similar to the RGB image representation. DTPM is designed to aggregate the learned segment-level features into the global utterance-level feature representation for emotion recognition. Extensive experiments on four data sets show that our method can yield promising performance in comparison with the state-of-the-arts. In addition, we also find that with our generated DCNN input, DCNN models pretrained on the large-scale ImageNet data could be leveraged in speech affective feature extraction. This makes DCNN's training with a limited amount of annotated speech data easier. The success of this work warranties further investigation on using deep learning in speech emotion recognition. Although this paper focuses on discrete emotion recognition, it is interesting to explore the effectiveness of deep features in continuous dimension emotion recognition on datasets like SEMAINE [67], RECOLA [68] and JESTKOD</figDesc><table><row><cell cols="2">Feature pooling EMO-DB</cell><cell>RML</cell><cell cols="2">eNTERFACE05 BAUM-1s</cell></row><row><cell>Average</cell><cell>83.28</cell><cell>60.73</cell><cell>71.08</cell><cell>41.94</cell></row><row><cell>Max</cell><cell>82.64</cell><cell>63.48</cell><cell>72.75</cell><cell>40.26</cell></row><row><cell>Ours (p  *  )</cell><cell cols="4">87.31(1.12) 69.70(1.50) 76.56(1.58) 44.61(0.21)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>S.Q. Zhang, S.L. Zhang, T.J. Huang, and W. Gao are with the Institute of Digital Media, School of Electronic Engineering and Computer Science, Peking University, China. E-mail: {tzczsq, slzhang.jdl, tjhuang, wgao}@pku.edu.cn, all authors are corresponding authors. S.Q. Zhang is also with the Institute of Intelligent Information Processing, Taizhou University, China. Manuscript received xxxx, 2017; revised xxxx, 2017.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>JOURNAL OF L A T E X CLASS FILES, VOL. XX, NO. XX, MARCH 2016</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by National Science Foundation of China (NSFC) and Zhejiang Provincial National Science Foundation of China under Grant No. 61572050, LY16F020011, 91538111, 61620106009, and the National 1000 Youth Talents Plan.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1520-9210 (c) 2017 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Emotion recognition in human-computer interaction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Douglas-Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tsapatsoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Votsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kollias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fellenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="80" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech emotion recognition approaches in human computer interaction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M El</forename><surname>Emary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telecommunication Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1467" to="1478" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011</title>
		<author>
			<persName><forename type="first">C.-N</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Iliou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Giannoukos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Laukka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016. 1, 2, 3, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1097">2012, pp. 1097-1105. 1, 2, 3, 4, 5, 8, 9</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition(CVPR)<address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5325" to="5334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic emotion recognition: raising the benchmarks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stuhlsatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zieike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using deep neural network and extreme learning machine</title>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tashev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of INTERSPEECH</title>
		<meeting>INTERSPEECH<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName><forename type="first">O</forename><surname>Abdel-Hamid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>-R. Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Penn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1533" to="1545" />
			<date type="published" when="2014">2014. 1, 2, 3, 4, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using CNN</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia<address><addrLine>NewYork, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning salient features for speech emotion recognition using convolutional neural networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2203" to="2213" />
			<date type="published" when="2004">2014. 1, 2, 4</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bruckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Maria</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A database of German emotional speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paeschke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rolfes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Sendlmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<meeting><address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human emotional state from audiovisual signals*</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="936" to="946" />
			<date type="published" when="2008">2008. 2, 7, 8, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The eNTERFACE&apos;05 audiovisual emotion database</title>
		<author>
			<persName><forename type="first">O</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Macq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">22nd International Conference on Data Engineering Workshops</title>
		<meeting><address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="8" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">BAUM-1: a spontaneous audio-visual face database of affective and mental states</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhalehpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transaction on Affective Computing</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2016">2016. 2, 7, 8, 11</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing emotion in speech</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dellaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Polzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Spoken Language (ICSLP&apos;96)</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1970" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emotion recognition in speech using neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nicholson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computing &amp; applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="290" to="296" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Emotional speech classification using Gaussian mixture models and the sequential floating forward selection algorithm</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ververidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1500" to="1503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using hidden Markov models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Nwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C De</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="603" to="623" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine-belief network architecture</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing(ICASSP&apos;04)</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="577" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Robust emotion recognition in noisy speech via sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="1539" to="1553" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spoken emotion recognition via localityconstrained kernel sparse representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ensemble methods for spoken emotion recognition in call-centres</title>
		<author>
			<persName><forename type="first">D</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C De</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="98" to="112" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spoken emotion recognition using hierarchical classifiers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Albornoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Milone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Rufiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="556" to="570" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature analysis and evaluation for automatic emotion identification in speech</title>
		<author>
			<persName><forename type="first">I</forename><surname>Luengo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Navas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Hernáez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Speech emotion recognition using Fourier parameters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="75" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speech emotion recognition with acoustic and lexical features</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>South Brisbane, QLD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4749" to="4753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing affect from linguistic information in 3D continuous space</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="192" to="205" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech emotion analysis: exploring the role of context</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tawari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Considering cross-cultural context in the automatic recognition of emotions</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Quiros-Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Onisawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="127" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Acoustic and lexical representations for affect prediction in spontaneous conversations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Savran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer speech &amp; language</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="203" to="217" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Emotion recognition in speech signal: experimental study, development, and application</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Petrushin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Spoken Language Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="222" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Emotional space improves emotion recognition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kompe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="page" from="2029" to="2032" />
			<date type="published" when="2002">2002</date>
			<pubPlace>Denver,Colorado</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The relevance of voice quality features in speaker independent emotion recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="17" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Emotion recognition in Chinese natural speech by combining prosody and voice quality features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks-ISNN 2008</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spoken emotion recognition through optimum-path forest classification using glottal features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Iliev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Scordilis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Papa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Falcão</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Interdependencies among voice source parameters in emotional speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Björkner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="162" to="174" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automatic speech emotion recognition using modulation spectral features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech communication</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="785" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Weighted spectral features based on local Hu moments for speech emotion recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Identifying salient sub-utterance emotion dynamics using flexible units and estimates of affective flow</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Provost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">LSTMmodeling of continuous emotions in an audiovisual affect recognition framework</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geometric Lp-norm feature pooling for image classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Providence, RI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="2609" to="2704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learned-norm pooling for deep feedforward and recurrent neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="530" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph embedding and extensions: a general framework for dimensionality reduction</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="51" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">The use of multiple measurements in taxonomic problems</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of eugenics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="188" />
			<date type="published" when="1936">1936</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Segment-based approach to the recognition of emotions in speech</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Shami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Amsterdam, Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="4" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Timing levels in segment-based speech emotion recognition</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">INTERSPEECH</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1818" to="1821" />
			<date type="published" when="2006">2006</date>
			<pubPlace>Pittsburgh, Pennsylvania</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">MatConvNet-convolutional neural networks for MATLAB</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4564</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">LIBSVM: a library for support vector machines</title>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Acoustic emotion recognition: a benchmark comparison of performances</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vlasenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wendemuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Automatic Speech Recognition &amp; Understanding</title>
		<meeting><address><addrLine>Merano</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
		<respStmt>
			<orgName>ASRU-2009</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Florian</surname></persName>
		</author>
		<title level="m">Real-time Speech and Music Classification by Large Audio Feature Space Extraction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Information fusion based on kernel entropy component analysis in discriminative canonical correlation space with application to audio emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiview emotion recognition via multi-set locality preserving canonical correlation analysis</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E D</forename><surname>Elmadany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting><address><addrLine>Montral, QC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">AVEC 2013 -the continuous audio/visual emotion and depression recognition challenge</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bilakhia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schnieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cowie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd ACM international workshop on Audio/Visual Emotion Challenge</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
