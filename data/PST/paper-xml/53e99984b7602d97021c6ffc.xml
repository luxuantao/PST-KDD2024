<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Prof</roleName><forename type="first">Foroosh</forename><forename type="middle">M</forename><surname>Hassan</surname></persName>
						</author>
						<author>
							<persName><surname>Mueller</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">P</forename><surname>Karasev</surname></persName>
							<email>pkarasev@gatech.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engi-neering</orgName>
								<orgName type="institution">Georgia Institute of Technology</orgName>
								<address>
									<postCode>30332</postCode>
									<settlement>Atlanta</settlement>
									<region>GA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
								<address>
									<postCode>35294</postCode>
									<settlement>Birmingham</settlement>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
								<address>
									<postCode>35294</postCode>
									<settlement>Birmingham</settlement>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Comprehensive Cancer Center and Department of Electrical &amp; Computer Engineering</orgName>
								<orgName type="institution">University of Alabama at Birmingham</orgName>
								<address>
									<postCode>35294</postCode>
									<settlement>Birmingham</settlement>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">courtesy of United Technologies Research Center</orgName>
								<address>
									<settlement>Connecticut</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">mathematics from</orgName>
								<orgName type="institution">Harvard University</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<region>AL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">85280646AD16A798C37301F85801848D</idno>
					<idno type="DOI">10.1109/TIP.2013.2258353</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optical Flow Estimation for Flame Detection in Videos</head><p>Martin Mueller, Member, IEEE, Peter Karasev, Member, IEEE, Ivan Kolesov, Member, IEEE, and Allen Tannenbaum, Fellow, IEEE Abstract-Computational vision-based flame detection has drawn significant attention in the past decade with camera surveillance systems becoming ubiquitous. Whereas many discriminating features, such as color, shape, texture, etc., have been employed in the literature, this paper proposes a set of motion features based on motion estimators. The key idea consists of exploiting the difference between the turbulent, fast, fire motion, and the structured, rigid motion of other objects. Since classical optical flow methods do not model the characteristics of fire motion (e.g., non-smoothness of motion, non-constancy of intensity), two optical flow methods are specifically designed for the fire detection task: optimal mass transport models fire with dynamic texture, while a data-driven optical flow scheme models saturated flames. Then, characteristic features related to the flow magnitudes and directions are computed from the flow fields to discriminate between fire and non-fire motion. The proposed features are tested on a large video database to demonstrate their practical usefulness. Moreover, a novel evaluation method is proposed by fire simulations that allow for a controlled environment to analyze parameter influences, such as flame saturation, spatial resolution, frame rate, and random noise.</p><p>Index Terms-Fire detection, optical flow, optimal mass transport, video analytics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D ETECTING the break-out of a fire rapidly is vital for prevention of material damage and human casualties. This is a particularly serious problem in situations of congested automobile traffic, naval vessels, and heavy industry. Traditional point-sensors detect heat or smoke particles and are quite successful for indoor fire detection. However, they cannot be applied in large open spaces, such as hangars, ships, or in forests. This paper presents a video-detection approach geared toward these scenarios where point-sensors may fail. In addition to covering a wide viewing range, video cameras capture data from which additional information can be extracted; for example, the precise location, extent, and rate of growth. Surveillance cameras have recently become pervasive, installed by governments and businesses for applications like license-plate recognition and robbery deterrence. Reliable vision-based fire detection can feasibly take advantage of the existing infrastructure and significantly contribute to public safety with little additional cost.</p><p>Scope. Vision-based detection is composed of the following three steps. Preprocessing (1) is necessary to compensate for known sources of variability, e.g., camera hardware and illumination. Feature extraction <ref type="bibr" target="#b1">(2)</ref> is designed for the detection of a specific target; a computation maps raw data to a canonical set of parameters to characterize the target. Classification algorithms (3) use the computed features as input and make decision outputs regarding the target's presence. Supervised machine-learning-based classification algorithms such as neural networks (NN) are systematically trained on a data set of features and ground truth.</p><p>The scope of this paper is illustrated in Fig. <ref type="figure">1</ref>. While aspects of preprocessing <ref type="bibr" target="#b0">(1)</ref> and classification (3) are considered, the paper focuses on feature extraction (2): Motivated by physical properties of fire, a set of novel optical flow features are designed for vision-based fire detection.</p><p>Previous Work. Computer vision concepts are often inspired by human vision. A comprehensive and elegant description of the human perception of fire was presented by the 16th century French poet Du Bartas <ref type="bibr" target="#b0">[1]</ref>: "Bright-flaming, heat-full fire, the source of motion." Whereas Du Bartas missed the characteristic reddish color that almost any vision-based detection algorithm builds upon, e.g., <ref type="bibr" target="#b1">[2]</ref>, his quote covers most of the other features employed by previous methods. High brightness or luminance causes image pixels to saturate-one feature utilized in <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>. The instantaneous flame-like texture or spatial pattern are used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Features related to the flame's shape and change of shape are found in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Also, the flickering, typical of fire, presents a popular feature <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, which has also been analyzed in the wavelet domain <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. This paper is restricted Fig. <ref type="figure">1</ref>. A Visual outline of the scope of this paper. The underlying goal of the research is improved robustness to rigid motion of fire-colored objects and unfavorable backgrounds, which tend to cause many false detections in current systems.</p><p>to studying images in the visible spectrum and emitted heat or infrared light is not considered, but it is used in a number of approaches where infrared sensors are available <ref type="bibr" target="#b9">[10]</ref>. Lastly, fire motion offers a whole suite of possible features. The authors of <ref type="bibr" target="#b10">[11]</ref> consider the temporal variation of pixel intensity for fixed pixel locations. More recently, <ref type="bibr" target="#b11">[12]</ref> develops a more comprehensive statistical approach by computing covariances of image quantities on small blocks, thus making use of spatial and temporal patterns. These statistics are applied directly to the image data and these methods are therefore comparably efficient. Optical flow estimators, on the other hand, transform the image sequence into estimated motion fields, allowing for a more insightful extraction of features. Classical optical flow algorithms are analyzed in <ref type="bibr" target="#b12">[13]</ref> for the recognition of various dynamic textures.</p><p>Contribution. The contributions of this paper are: First, since classical optical flow methods are based on assumptions, e.g., intensity constancy and flow smoothness, which are not met by fire motion, we derive two optical flow estimators specifically designed for the detection of fire; optimal mass transport exploits the dynamic texture of flames, whereas a non-smooth modification of classical optical flow models saturated flames with no dynamic texture. Second, a new set of optical flow features is presented for fire detection; these features characterize magnitude and directionality of motion vectors. Third, thorough analysis on real and synthetic data is performed; it demonstrates the features' ability to reliably detect fire while rejecting non-fire, rigidly moving objects. Finally, a novel way to evaluate fire detectors using fire simulations is proposed; this method allows for the quantitative evaluation of scene variability such as flame saturation, spatial resolution, frame rate and noise.</p><p>The proposed research is different from recent work in dynamic texture analysis <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref> in that our design of the motion estimators and features is geared to the detection of flames. See Table <ref type="table">I</ref> for a comparison to <ref type="bibr" target="#b12">[13]</ref>, which employs classical optical flow methods for the recognition of different dynamic textures. In our work, the optical flow estimators are modified to model the properties of fire motion. In particular, a prior color transformation, plus non-smooth regularization and brightness conserving optical flow are proposed. In Section III-B, detailed discussions explain how the extracted features differ compared to <ref type="bibr" target="#b12">[13]</ref>.</p><p>As one major difference, we compute a spatial structure preserving feature based on a unique property of the optimal mass transport solution. Regarding classification results, <ref type="bibr" target="#b12">[13]</ref> is not comparable to our work, since <ref type="bibr" target="#b12">[13]</ref> only classifies between 26 particular instances of dynamic textures (each one representing one class), and not between the presence/ absence of an event (e.g., "fire") among many videos. Moreover, the data in <ref type="bibr" target="#b12">[13]</ref> is a specialized high quality database, whereas our data comes from mass market and surveillance cameras. Brightness conserving optical flow formulations have been employed in dynamic texture segmentation tasks <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, but no attempt was made in classifying them, e.g., as fire, water, etc., using brightness conserving methods. Moreover, the optimal mass transport introduced here differs in regularization, minimization and underlying data, as explained in more detail in Section II-B1.</p><p>Another related optical-flow-based paper is <ref type="bibr" target="#b16">[17]</ref>, where Lucas and Kanade <ref type="bibr" target="#b17">[18]</ref> optical flow is employed to detect smoke. A series of works <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref> describe dynamic textures with linear dynamical models, for which the dynamics can be learned for classification purposes, but no particular attention is given to fire content in these papers. A joint approach for dynamic texture segmentation using both linear models and optical flow estimation to account for camera motion is presented in <ref type="bibr" target="#b22">[23]</ref>.</p><p>Organization. Following Fig. <ref type="figure" target="#fig_0">2</ref> from left to right, this paper starts with a description of the proposed algorithm. Section II derives two optical flow formulations tailored to the fire detection task: the first is the optimal mass transport (OMT) optical flow <ref type="bibr" target="#b23">[24]</ref> for modeling dynamic textures such as fire, and the second is a non-smooth optical flow model for rigid motion. Section III describes the features extracted from the optical flow fields for classification. Those features include quantities related to the flow magnitude and the flow directions. Section IV adds the auxiliary concepts of candidate regions and proposes to train a neural net (NN) for fire detection. Finally, test results on real and synthetic data from fire simulations are presented in Section V for qualitative and quantitative analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Extraction</head><note type="other">Preprocessing</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. OPTICAL FLOW ESTIMATION</head><p>A comprehensive survey of optical flow since the pioneering papers by Horn/Schunck <ref type="bibr" target="#b24">[25]</ref> and Lucas/Kanade <ref type="bibr" target="#b17">[18]</ref> from 1981 is beyond the scope of this paper. However, the short introduction in Section II-A, should suffice to understand the issues of classical optical flow when applied to fire detection. To ameliorate these issues, Sections II-B and II-C propose the use of two novel optical flow estimations-Optimal Mass Transport (OMT) and Non-Smooth Data (NSD)-that are specifically developed for the fire detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Classical Optical Flow</head><p>Optical flow estimation computes correspondence between pixels in the current and the previous frame of an image sequence. Central to most approaches in establishing this correspondence is the assumption of intensity constancy: moving objects preserve their intensity values from frame to frame. This assumption leads to the optical flow constraint</p><formula xml:id="formula_0">d dt I = I x u + I y v + I t = 0<label>( 1 )</label></formula><p>where I (x, y, t) is a sequence of intensity images with spatial coordinates (x, y) ∈ and time variable t ∈ [0, T ] (subscripts denote partial derivatives). The flow vector (u, v) = (x t , y t ) points into the direction where the pixel (x, y) is moving. In Eq. (1), I x , I y , and I t are given image quantities and the equation is solved for u and v. This problem is ill-posed because there are two unknowns in Eq. ( <ref type="formula" target="#formula_0">1</ref>) and one equation per pixel. This is known as the aperture problem, which states that only the optical flow component parallel to the image gradient can be computed.</p><p>To obtain a unique solution, the optical flow algorithms make further assumptions on the flow field, which is traditionally done by enforcing smoothness. Whereas Lucas-Kanade optical flow <ref type="bibr" target="#b17">[18]</ref> is an early representative of methods that assume flow constancy for pixels in a neighborhood, this paper follows the point-wise approach, which applies conditions per pixel instead of constant neighborhoods. Point-wise methods generally attempt to minimize a functional of the form</p><formula xml:id="formula_1">T 0 r data (I, u, v) + αr reg (u, v) dt dx dy (2)</formula><p>where the data term r data represents the error from the optical flow constraint Eq. ( <ref type="formula" target="#formula_0">1</ref>) and the regularization term r reg quantifies the smoothness of the flow field. The constant α controls regularization. In the pivotal paper by Horn-Schunck <ref type="bibr" target="#b24">[25]</ref>, the data and regularization terms are chosen as</p><formula xml:id="formula_2">T 0 I t + I x u + I y v 2 + α( ∇u 2 2 + ∇v<label>2</label></formula><p>2 ) dt dx dy.</p><p>(3) From this point, numerous advances have been achieved mostly by changing the regularization term to be image-driven or anisotropic. The optical flow constraint Eq. ( <ref type="formula" target="#formula_0">1</ref>) remains central to all those advances. A detailed survey on related optical flow work can be found in <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal Mass Transport (OMT) Optical Flow</head><p>Classical optical flow models based on brightness constancy, d dt I = 0, are inadequate to model the appearance of fire for two reasons. First, fire does not satisfy the intensity constancy assumption Eq. ( <ref type="formula" target="#formula_0">1</ref>), since rapid (both spatially and temporally) change of intensity occurs in the burning process due to fast pressure and heat dynamics. Second, smoothness regularization may be counter-productive to the estimation of fire motion, which is expected to have a turbulent, i.e., non-smooth, motion field. For these reasons, an optical flow estimation modeling fire as a dynamic texture, the optimal mass transport (OMT) optical flow, was introduced in <ref type="bibr" target="#b23">[24]</ref>. A review of the OMT optical flow is given next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Derivation:</head><p>The optical flow problem is posed as a generalized mass-representing image intensity I -transport problem, where the data term enforces mass conservation. The conservation law is written as</p><formula xml:id="formula_3">I t + ∇ • ( uI ) = 0 (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>where u = (u, v) T . With intensity I replaced by mass density, Eq. ( <ref type="formula" target="#formula_3">4</ref>) is known in continuum mechanics <ref type="bibr" target="#b26">[27]</ref> as the continuity equation, which together with conservation of momentum and conservation of energy form the equations of motion for inviscid fluids (liquids and gases), such as fire. Therefore, Eq. ( <ref type="formula" target="#formula_3">4</ref>) models the data term after a physical law fire must obey. Further motivation for the use of intensity conservation for dynamic textures can be found in <ref type="bibr" target="#b14">[15]</ref>. Analogous to standard optical flow, the OMT optical flow model minimizes the total energy defined as 2  2 I dt dx dy <ref type="bibr" target="#b4">(5)</ref> subject to the boundary conditions I (x, y, 0) = I 0 (x, y) and I (x, y, 1) = I 1 (x, y), where I 0 and I 1 are given gray-scale images. The transport energy u 2 2 I , which is the work needed to move mass from a location at t = 0 to another location at t = 1, plays the role of the regularization term in Eq. ( <ref type="formula">5</ref>). A similar formulation of the optimal mass transport problem with mass conservation Eq. ( <ref type="formula" target="#formula_3">4</ref>) being a hard constraint, was introduced by <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_5">min u 1 2 T 0 (I t + ∇ • (I u)) 2 + α u</formula><p>The solution to this minimization problem is obtained through a "discretize-then-optimize" approach: start by discretizing Eq. ( <ref type="formula">5</ref>) as</p><formula xml:id="formula_6">min u α 2 ( u T I u) + 1 2 (I t + D x I D y I u) T (I t + D x I D y I u) (6)</formula><p>where u is a column vector containing u and v, and I is a matrix containing the average intensity values (I 0 + I 1 )/2 on its diagonal. The derivatives are discretized by I t = I 1 -I 0 and the central-difference sparse-matrix derivative operators D x and D y . Now, the quantities b = -I t , A = D x I D y I are defined and it becomes clear that the function to be minimized is quadratic in u</p><formula xml:id="formula_7">min u α 2 ( u T I u) + 1 2 (A u -b) T (A u -b). (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>The solution is</p><formula xml:id="formula_9">u = (α I + A T A) -1 (A T b). (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Since all matrices are sparse, the inversion can be performed quickly with numerical solvers. While intensity conserving methods are not new, we believe that the proposed OMT optical flow is novel for flame detection because of its regularization and numerical solution. For example, <ref type="bibr" target="#b14">[15]</ref> enforces smoothness on the curl and divergence of the flow field, whereas our regularization originates from the optimal mass transport problem (Monge-Kantorovich problem <ref type="bibr" target="#b28">[29]</ref>), which seeks to minimize transport energy. We believe that this choice makes physical sense since brightness conservation may be naturally regarded as mass conservation. Thus utilizing the regularization motivated by Monge-Kantorovich fits quite well into this framework. The solution is then obtained by a discretize-then-optimize approach, whereas <ref type="bibr" target="#b14">[15]</ref> optimizes in the continuous domain and then discretizes the resulting optimality conditions. Moreover, in our approach, generalized mass is defined as fire color similarity (not intensity), which is explained next. 2) Flame Color as Generalized Mass: In the original OMT optical flow <ref type="bibr" target="#b23">[24]</ref> formulation, the generalized mass is the pixel intensity I . This choice is based on the assumption that high gas density translates to high intensity in the image. However, there are other phenomena that map to high intensities also, such as the sky on a sunny day or a white wall. In order to improve the chance of good segmentation between foreground and background, this section introduces a new model for the generalized mass based on flame color.</p><p>Briefly, the generalized mass of a pixel is represented by the similarity to a center fire color in the HSV color space (H, S, V ∈ [0, 1]), which is chosen to be H c = 0.083, S c = V c = 1, a fully color-saturated and bright orange. Given the H , S, and V values of a pixel, the generalized mass is then computed as</p><formula xml:id="formula_11">I = f (min{|H c -H |, 1 -|H c -H |}) • S • V (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where f is the logistic function</p><formula xml:id="formula_13">f (x) = 1 -(1 + exp(-a • (x -b))) -1<label>(10)</label></formula><p>and a = 100, b = 0.11 are chosen to give the weight function in Fig. <ref type="figure" target="#fig_1">3</ref>. The hue bar in the x-axis of Fig. <ref type="figure" target="#fig_1">3</ref> illustrates which colors correspond to different hue values. A larger value of b would include more of the colors green and purple, which are atypical of fire. A smaller value would exclude red and yellow hues. a is the slope of the transition and, from our experience, is not a sensitive parameter, whereas b should be chosen to reasonably capture fire hue. The color value V corresponds to intensity and the color-saturation S plays the role of dismissing camera-saturated pixels. Although pixel saturation (i.e., colorsaturation is low) is sometimes used as a feature for fire detection, saturated regions are useless in the optical flow context due to a lack of image gradient and dynamic texture. The transformation in Eq. ( <ref type="formula" target="#formula_11">9</ref>) will weight highly only the periphery of saturated fire regions where camera-saturation tends to occur less. This property becomes visible in Fig. <ref type="figure" target="#fig_2">4(c</ref>) and (d), where the core of the fire center is saturated. The fire texture in Fig. <ref type="figure" target="#fig_2">4</ref>(a), on the other hand, is preserved in the generalized mass Fig. <ref type="figure" target="#fig_2">4(b)</ref>. Note that fire classification is not performed based on the output of Eq. ( <ref type="formula" target="#formula_11">9</ref>); this is a preprocessing step to obtain the image on which optical flow is computed. For this reason, the proposed color model does not have to be as accurate as when used as a feature. Fig. <ref type="figure" target="#fig_4">5</ref> shows two examples of OMT flow fields computed from the generalized mass images. It illustrates OMT's ability to capture dynamic texture for the fire image and discriminate between the rigid object's flow field, which appears much more structured. Two examples for the generalized mass transformation Eq. ( <ref type="formula" target="#formula_11">9</ref>).  </p><formula xml:id="formula_14">(I t + ∇ I • u) 2 + α u 2 2 dt dx dy. (<label>11</label></formula><formula xml:id="formula_15">)</formula><p>The choice of the data term being the optical flow constraint Eq. ( <ref type="formula" target="#formula_0">1</ref>) is justified because pixel saturation trivially implies intensity constancy. Also, the NSD is explicitly chosen to be non-smooth since saturated fire blobs are expected to have non-smooth boundary motion. The norm of the flow vector u 2 2 regularizes the flow magnitude, but does not enforce smoothness. This choice, therefore, makes the NSD flow directions purely driven by the data term under the constraint that flow magnitudes are not too large. While this method is not expected to perform well for standard optical flow applications where flow smoothness plays an important role, it proves useful for detecting saturated fire.</p><p>The solution to the minimization problem Eq. ( <ref type="formula" target="#formula_14">11</ref>) is as simple as applying basic arithmetic operations to pre-computed image properties (as opposed to solving a system of equations), which means the NSD optical flow is computationally inexpensive. The Euler-Lagrange equations of Eq. ( <ref type="formula" target="#formula_14">11</ref>) are written as</p><formula xml:id="formula_16">∂ L ∂u = (I t + ∇ I • u) I x + αu = 0 (12a) ∂ L ∂v = (I t + ∇ I • u) I y + αv = 0 (12b)</formula><p>which, after a few manipulations, yield the solution u = -</p><formula xml:id="formula_17">I x I t ∇ I 2 2 + α , v = - I y I t ∇ I 2 2 + α . (<label>13</label></formula><formula xml:id="formula_18">)</formula><p>Again, I x , I y , and I t are pre-computed image derivatives and α is the regularization parameter. Fig. <ref type="figure" target="#fig_5">6</ref> shows NSD flow fields computed for a saturated fire and a rigidly moving hat. Fire motion on the boundary is non-smooth in contrast to the hat's motion-a result from the data-driven formulation of the NSD.</p><p>The following Section III illustrates characteristic fire features that are extracted from the flow fields derived above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OPTICAL FLOW FEATURES</head><p>A naive approach to vision-based detection is to use a supervised machine learning algorithm trained directly on intensity values in the image. This approach will undoubtedly underperform because the classification complexity increases exponentially with the dimensionality of the problem, which in this case is equal to the number of pixels. Further, the computational cost and the amount of training data required become prohibitively large. Instead, feature extraction is employed by incorporating prior information (known physical properties of the problem or human intuition) for the purpose of reducing the problem dimensionality.</p><p>The optical flow computations in Section II do not reduce dimensionality, as the two M × N images determine the values of the M • N, 2D, optical flow vectors. This transformation is an intermediate step that provides a data set from which motion features can be extracted more intuitively than would be possible from the original image. To do so, we pursue a region-based as opposed to a pixel-based approach. Whereas the pixel-wise approach classifies each pixel, the region-wise approach aims to classify a region as a whole by analyzing the set of all pixel values (in this case flow vectors) in that region. In the following section, the proposed feature extraction is defined and motivated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pre-Selection of Essential Pixels</head><p>Static or almost static image regions should be excluded from consideration because our aim is to characterize the type of motion an object is undergoing and they interfere with the motion statistics. For example, the average flow magnitude of a moving object should not depend on the size of the static background, which would be the case if the average was computed over the entire image instead of just the moving region. The set of essential pixels, which are pixels in motion, will therefore be defined as follows. Consider a frame or a subregion of that frame ⊂ R 2 . (Working on subregions is imperative when there are several sources of motion in one frame, as explained in Section IV-A). Assume that the optical flow field u : → R 2 is computed. Then, the set of essential pixels e ⊂ is defined as</p><formula xml:id="formula_19">e = (x, y) ∈ : u(x, y) 2 &gt; c • max u 2 (<label>14</label></formula><formula xml:id="formula_20">)</formula><p>where 0 ≤ c &lt; 1 is chosen such that a sufficiently large number of pixels is retained. In case of extreme outliers, the parameter can be adjusted adaptively. For the data in Section V-A, a value of c = 0.2 provides sufficient motion segmentation across the database. In addition, reliability measures <ref type="bibr" target="#b12">[13]</ref> could be employed to suppress pixels with low motion information. Some of the operations of feature extraction will be performed on this subset e of "sufficiently moving" pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature Extraction</head><p>In <ref type="bibr" target="#b12">[13]</ref> a list of optical flow features is introduced, which is complete in that it considers all possible first-order distortions of a pixel. Those distortions are then averaged within a spatiotemporal block to yield the probability of a characteristic direction or of a characteristic magnitude. Reference <ref type="bibr" target="#b12">[13]</ref> also mentions that the highest discriminating power comes from the characteristic direction and magnitude of the flow vector itself, not considering distortions. This observation matches our experience from early prototyping where flow derivatives seemed to have very little discriminating power and were thus removed from further consideration. Given the flow vectors, we propose four features f i : u → R, i = 1, . . . , 4 defining the four dimensional feature vector</p><formula xml:id="formula_21">F = ( f 1 , f 2 , f 3 , f 4 ) T .</formula><p>These features share the general idea with <ref type="bibr" target="#b12">[13]</ref> to use "characteristic" motion magnitude and direction, yet are not the same. In particular, our magnitude features f 1 and f 2 measure mean magnitude as opposed to relative homogeneity of the magnitudes within a block in <ref type="bibr" target="#b12">[13]</ref>. Our choice is straightforward for fire detection considering the color transformation in Section II-B2, thus biasing the detector to fire-like colored moving objects. The other two features f 3 and f 4 then analyze motion directionality. The OMT feature f 3 takes into account spatial structure of the flow vectors, a strategy not considered in <ref type="bibr" target="#b12">[13]</ref> where all vectors are averaged regardless of their location. Moreover, whereas <ref type="bibr" target="#b12">[13]</ref> identifies the presence of one characteristic direction, feature f 4 is more general in that it can detect multiple characteristic directions. Thus, given an image region and the optical flow fields u OMT and u NSD in that region, the features are chosen as follows. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) OMT Transport Energy:</head><p>This feature measures the mean OMT transport energy per pixel in a subregion</p><formula xml:id="formula_22">f 1 = Mean e I 2 u OMT 2 2 . (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>After the color transformation in Section II-B2, fire and other moving objects in the fire-colored spectrum, are expected to produce high values for this feature.</p><p>2) NSD Flow Magnitude: Similarly, the mean of the regularization term of the NSD optical flow energy Eq. ( <ref type="formula" target="#formula_14">11</ref>) constitutes the second feature</p><formula xml:id="formula_24">f 2 = Mean e 1 2 u NSD 2 2 . (<label>16</label></formula><formula xml:id="formula_25">)</formula><p>The first two features, f 1 , f 2 , will have high values for moving, fire-colored objects. The last two features distinguish turbulent fire motion from rigid motion by comparing flow directionality.</p><p>3) OMT Sink/Source Matching: It is known <ref type="bibr" target="#b29">[30]</ref> that solutions to OMT problems are curl-free mappings. For the turbulent motion of fire, this property tends to create flow fields with vector sinks and sources, a typical case of which is displayed in Fig. <ref type="figure" target="#fig_6">7(b</ref>). For rigid motion, the flow field tends to be comprised of parallel vectors indicating rigid translation of mass.</p><p>The third feature is designed to quantify how well an ideal source flow template matches the computed OMT flow field. The template is defined as</p><formula xml:id="formula_26">u T (x, y) = u T (x, y) v T (x, y) = exp -x 2 + y 2 x y (<label>17</label></formula><formula xml:id="formula_27">)</formula><p>and shown in Fig. <ref type="figure" target="#fig_6">7</ref>(a). Then, the best match is obtained by</p><formula xml:id="formula_28">f 3 = max u T u OMT u OMT 2 + v T v OMT u OMT 2<label>(18)</label></formula><p>where denotes convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) NSD Directional Variance:</head><p>The final feature distinguishes the boundary motion of saturated fire blobs from rigidly moving objects by quantifying the variance of flow directions at moving pixels. A high variance suggests uncorrelated motion in many different directions, whereas a low variance characterizes nearly uni-directional motion. </p><p>using kernel density estimation <ref type="bibr" target="#b30">[31]</ref>. For notational convenience, the histogram h(r, φ) is written in polar coordinates, where r = √ u 2 + v 2 represents the flow vector magnitude and φ = arctan(v/u) the flow vector direction. Fig. <ref type="figure" target="#fig_7">8</ref> shows the motion histograms of the flows from Fig. <ref type="figure" target="#fig_5">6</ref>. As expected, the flow vectors for fire motion are scattered around the origin, whereas the flow vectors for rigid motion point predominantly in one direction (up and to the right, in this case). This correlation is measured by the fourth feature as</p><formula xml:id="formula_30">f 4 = Var{s i , i = 0, . . . , n -1}<label>(20)</label></formula><p>where</p><formula xml:id="formula_31">s i = ∞ 0 2π(i+1)/n 2πi/n h(r, φ) dφ dr ∞ 0 2π 0 h(r, φ) dφ dr . (<label>21</label></formula><formula xml:id="formula_32">)</formula><p>The domain is discretized into wedges s i , and each s i represents the ratio of pixels whose flow vector points in a direction between 2πi /n &lt; φ ≤ 2π(i + 1)/n. For a flow field pointing mainly in one direction, only a few of the s i are large, and consequently, the value for f 4 is high. On the other hand, if the proportion of vectors pointing in each direction is approximately equal, all of the s i will be similar, and f 4 will have a low value. Feature f 4 is more versatile for detecting rigid motion than the probability of a characteristic flow direction in <ref type="bibr" target="#b12">[13]</ref>: For example, consider a flow field where half of the pixels point in one direction, and half in the opposite direction, which we would associate with rigid motion having two characteristic directions. Feature f 4 will still yield a fairly high value indicating rigid motion, whereas in <ref type="bibr" target="#b12">[13]</ref> the probability of a characteristic motion is practically zero since opposite vectors cancel each other out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DETECTION ALGORITHM</head><p>This section touches on two more topics required for an end-to-end detection system. Section IV-A shows how candidate regions, i.e., suspicious subregions of the image, can be created automatically and why this step is important. Then, Section IV-B explains how a candidate region is classified as fire or non-fire based on its feature vector, F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Providing Candidate Regions</head><p>Computing optical flow and extracting features from an entire frame is unfavorable for two reasons. First, computation time increases as the domain grows, and more importantly, the classification fails for scenes with more than one source of motion (e.g., a fire in the corner and a person walking in the center) since the motion statistics of the regions will be averaged and become no longer discriminatory. Therefore, it is essential for the proposed method that a segmentation of different sources of motion be provided, which the detection algorithm then classifies individually.</p><p>A connected region c ⊆ containing a single source of motion is called a candidate region. There may be more than one candidate region in the current image, i c , i = 1, 2, . . . , N and</p><formula xml:id="formula_33">N i=1 i c ⊆</formula><p>for each frame where N varies with time. Many publications construct candidate regions from color values and background estimation. Given a candidate region c , optical flow is computed on c , the features in Section III are computed, and c is classified as a fire or nonfire region. While this approach is sufficient for our purposes, a more powerful method is given by tracking candidate regions using active contours as proposed by <ref type="bibr" target="#b31">[32]</ref>. Briefly, a candidate region is defined by an active contour based on the Chan-Vese energy <ref type="bibr" target="#b32">[33]</ref> that separates the mean of the optical flow magnitude inside and outside of the contour. The main advantage of tracking contours over time is that previous classification results of a candidate region can be incorporated in the current decision.</p><p>This paper focuses on optical flow features. Hence, it is assumed that homogeneous (i.e., belonging to the same moving object or part of the background) candidate regions are available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Region Classification via Neural Network</head><p>The simplest way of classifying a candidate region based on its feature vector F = ( f 1 , f 2 , f 3 , f 4 ) T is to threshold each of the features f i based on heuristically determined cutoff values and make a decision by majority voting. However, better results can be achieved by learning the classification boundary with a machine learning approach such as neural networks. Training the neural network (NN) means performing a non-linear regression in the feature space to best separate the labeled training data into classes. Given a set of feature vectors Fn , n = 1, . . . , N and their respective true class Cn (e.g., Cn = 1 ( Cn = 0) means contour n is a fire (non-fire) blob), the regression model C : R 4 → R used in a NN is</p><formula xml:id="formula_34">C(F) = σ M m=1 w m m (F) . (<label>22</label></formula><formula xml:id="formula_35">)</formula><p>Here, the σ (•) is an activation function and the m (•) are fixed basis functions. The goal is to find the weights w = (w 1 , • • • , w M ) that minimize an error function</p><formula xml:id="formula_36">E(w) = 1 2 N n=1 |C( Fn ) -Cn | 2 . (<label>23</label></formula><formula xml:id="formula_37">)</formula><p>The basis functions used for this paper are</p><formula xml:id="formula_38">m (F) = h 4 n=1</formula><p>w mn f n + w m0 <ref type="bibr" target="#b23">(24)</ref> where features, f n , are the inputs to the NN and h(•) is an activation function (so-called hidden unit), typically taken to be a sigmoid.</p><p>During the training phase, training examples are provided {( Fn , Cn ), n = 1, . . . , N} and the energy in Eq. ( <ref type="formula" target="#formula_36">23</ref>) is optimized over w m and w mn . In the testing phase, a feature vector F is supplied and the output in Eq. ( <ref type="formula" target="#formula_34">22</ref>) is a probability that the feature vector F is associated with a particular class.</p><p>Note that neural networks are just one of many possible choices for learning the class separation boundaries. For instance, support vector machines (SVM) are expected to produce similar results. Selecting discriminatory features, not choosing the classifier, is most important for correct classification. More information on these topics can be found in <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ANALYSIS AND TEST RESULTS</head><p>The results section evaluates the proposed features' performance in three ways. First, the algorithm is tested on a large video database. Since the absolute values of false detection rates are highly dependent on the testing and training data, a qualitative assessment of the results follows. Finally, a novel method of analyzing fire detectors is introduced by using synthetic fire simulations; they allow for a quantitative investigation into how flame saturation, spatial resolution, frame rate and noise influence detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Tests on Video Database</head><p>The algorithm is tested on a database provided by United Technologies Research Center (UTRC), Connecticut, USA. The database features various scenarios including indoor/outdoor, far/close distance, different types of flames (wood, gas, etc.), changing lighting conditions, partial occlusions, etc. The videos have a frame rate of 30 frames/second and spatial dimension of 240 by 360 pixels. From each of the 263 scenarios (containing 169 fire and 94 non-fire sequences), 10 consecutive frames are labeled as ground truth providing a test database of 2630 frames. Note that the non-fire scenarios are chosen to be probable false positives, namely moving and/or fire-colored objects such as cars, people, red leaves, lights and general background clutter. A neural network was trained on frames from 20 of those videos not used in the test database. The test result shows that fire is reliably detected using optical flow features only: the false positive rate (fire is detected when no fire is present) is f p = 3.19% and the false negative rate (no fire detected where there is a fire) is f n = 3.55%. These false detections rates are competitive with recently published, full-fledged fire detection systems, e.g. <ref type="bibr" target="#b1">[2]</ref>: f p = 31.5%, f n = 1.0%, <ref type="bibr" target="#b2">[3]</ref>: f p = 0.68%, f n = 0.028%, <ref type="bibr" target="#b3">[4]</ref>: f p = 0.30%, f n = 12.36%, <ref type="bibr" target="#b11">[12]</ref> (average among test videos): f p = 3.34%, f n = 21.50%. Since this paper explores the discriminatory power of optical flow features only rather than building a complete fire detector, we report the p=1.000 p=1.000 p=1.000 p=0.998 p=0.999 p=0.987 p=1.000 p=0.976 p=1.000  false detection rates as a feasibility check that the proposed features work on a large set of data. More careful analyses of the detector's properties and limitations, are explored in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Analysis on Real Videos</head><p>Example scenarios from the above tests are given in this section to illustrate successful classifications as well as typical false detections and their causes. Fig. <ref type="figure" target="#fig_8">9</ref> illustrates successful fire detections and Fig. <ref type="figure">10</ref> shows two examples of correctly classified non-fire scenes. In Fig. <ref type="figure">10</ref>, the printing on the jacket and the truck entering at the bottom right are moving, fire-like colored (the truck has a reddish tint) objects. Nevertheless, due to their structured motion, these scenarios are correctly classified as not fire.</p><p>Occasional false negative detections were observed in four types of scenarios. First, horizontal lines resulting from structured noise in Fig. <ref type="figure" target="#fig_10">11</ref>(a) introduce structure to the otherwise turbulent fire motion. Structured noise must to be avoided, therefore, when using the proposed features. Second, as in Fig. <ref type="figure" target="#fig_10">11(b)</ref>, the flame color is oddly distorted to a blue tinge which causes the color transformation in Eq. ( <ref type="formula" target="#formula_11">9</ref>) to assign low mass and thus, low motion magnitude. Third, insufficient spatial resolution, as in Fig. <ref type="figure" target="#fig_10">11(c</ref>), leads to very few pixels belonging to fire and motion structure cannot be detected. Lastly, partial occlusions may make the detector fail, as shown  in Fig. <ref type="figure" target="#fig_10">11(d)</ref>, because the occluding object's edge causes motion vectors along that edge to be strongly aligned in the direction of the image gradient. Line detection algorithms could be employed as a preprocessing step to exclude edge pixels from consideration.</p><p>Regarding false positive detections, two deficiencies were observed. In Fig. <ref type="figure" target="#fig_11">12(a)</ref>, random noise on the red exit sign causes the motion field to have significant magnitude and turbulent directionality. Noise-reducing preprocessing can alleviate this problem, but other problems such as loss of texture detail may arise as a result. If spatial resolution is sufficient, multi-resolution techniques could be considered as well. Another false detection was observed for fast rotational motion (in combination with low spatial resolution) as seen in the case of the orange print on the truck in Fig. <ref type="figure" target="#fig_11">12(b)</ref>; the truck makes a sharp turn and undergoes a large perspective change. A change in perspective does not satisfy the intensity constancy assumption Eq. ( <ref type="formula" target="#formula_0">1</ref>) as self-occlusions occur and intensity disappears during the rotation. In order to successfully reject such scenarios, a track-and-detect algorithm as proposed in <ref type="bibr" target="#b31">[32]</ref> can monitor the scene and set off alarm only if the anomaly persists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Analysis on Simulated Videos</head><p>Lastly, an experiment is presented that further illustrates and substantiates the key idea of the paper. Since this experiment is based on synthetic data from fire simulations, there is a controlled test environment that allows for the quantitative analysis of parameter influences such as flame saturation, spatial resolution, frame rate and random noise. The fire simulator and relevant files can be downloaded at https://github.com/pkarasev3/phosphorik.</p><p>Fire Simulation. Fire simulation has become an important tool in modern multi-media applications such as movies, video games, etc. In our application, these simulations serve to generate synthetic ground truth for testing of the vision-based fire detector. Clearly, the physics of fire cannot be modeled exactly, but the basic physical laws are well explored in the field of continuum mechanics <ref type="bibr" target="#b34">[35]</ref>. Furthermore, despite not mimicking real fire videos precisely, simulations pose similar challenges for a fire detector but in a way that can be quantified. The fire simulations used in this paper are inspired by <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> and employ the inviscid Euler equations <ref type="bibr" target="#b26">[27]</ref> to model the physics. From the simulated physical quantities such as heat and pressure, images are rendered using concepts such as light scattering and color models.</p><p>Experiment. We create a sequence of images where fire is simulated (fire sequence), as shown in Fig. <ref type="figure" target="#fig_12">13(a)</ref>. We then create another sequence of images made up of identical fire frames shifted horizontally left to right (rigid sequence). The size of the shift is chosen such that the energy features f 1 and f 2 in Eq. ( <ref type="formula" target="#formula_22">15</ref>) and ( <ref type="formula" target="#formula_24">16</ref>) are similar to the ones in the fire sequence. Frames from the rigid sequence are shown in Fig. <ref type="figure" target="#fig_12">13(b</ref>). The same process is repeated for four more backgrounds other than black, as shown in Fig. <ref type="figure" target="#fig_12">13(c</ref>). Note that the background affects the rendering of the fire flame; so, different backgrounds introduce some variability in the appearance of the flame. Overall, there are five fire and five rigid sequences. The goal is to distinguish between a fire and a moving image of a fire in a video based on the directional features f 3 and f 4 in Eq. ( <ref type="formula" target="#formula_28">18</ref>) and <ref type="bibr" target="#b19">(20)</ref>. We observe that just about any static feature used in the literature will typically fail this experiment, as will many temporal features if they only quantify the magnitude of the change.</p><p>Analysis. Fig. <ref type="figure" target="#fig_13">14</ref> plots the feature values f 3 and f 4 for the frames of the experiment described above. Intuitively, if the data points from the rigid sequences are well separated from the points belonging to the fire sequences, the features effectively discriminate between the two classes (rigid and fire). How well the features discriminate will be measured quantitatively by fitting a line L in the f 3 -f 4 space such that the two classes are best separated i.e,</p><formula xml:id="formula_39">E = i = frames w(i ) (dist(L, f i )) 2<label>(25)</label></formula><p>is minimized. Here, f i = ( f i 3 , f i 4 ) is the feature point for frame i in the f 3 -f 4 space. The weighting function w(i ) is defined as</p><formula xml:id="formula_40">w(i ) =</formula><p>1, if i is fire (rigid) and below (above) L 100, if i is fire (rigid) and above (below) L ( <ref type="formula">26</ref>) and assures that points on the wrong side of L are penalized more to achieve the best possible separation. The separation line L is also plotted in Fig. <ref type="figure" target="#fig_13">14</ref>. As expected, the directional features are capable of separating the feature points with only a few points lying on the wrong side, but still close to the separation line.</p><p>The resulting minimal error E min for Eq. ( <ref type="formula" target="#formula_39">25</ref>) quantifies how well the directional features perform in the experiment. Moreover, the slope s of L indicates which feature is having a bigger impact; for instance, if the slope is large, the OMT feature f 3 explains the difference between classes to a large degree, and if the slope is small, the NSD feature f 4 is most discriminatory. Based on these two quantities, the following experiments analyze the influence of video parameter changes.</p><p>1) Flame Saturation: The first experiment was performed using a flame saturation value of 15. In the fire simulations, it is possible to vary the saturation value that changes flames from totally transparent (value 0) to totally white or fully saturated (value 100). Frames for several different saturation values are shown in Fig. <ref type="figure" target="#fig_12">13(d</ref>). Repeating the above experiment for different saturation values, the analysis of the result in terms of E min and the slope of the separation line s, yields Fig. <ref type="figure" target="#fig_14">15(a)</ref>. It is observed that the error first decreases  for increasing saturation and then increases for saturations of higher than 10. This result shows that the detector performs best when the flame is not too dim and not too saturated. Moreover the slope s tends to decrease with higher saturation, which is expected since the OMT feature f 3 was designed for dynamic textures, and the NSD feature f 4 dominates when the flame is saturated and appears more like a rigid object.</p><p>2) Spatial Resolution: In this experiment, we vary the size of the images. The images shown so far have been 128 × 128 pixels. By repeating the experiments for different image sizes with the flame size changing proportionally, Fig. <ref type="figure" target="#fig_14">15(b</ref>) is obtained. The key observation is that there is a certain minimum size of about 70 × 70 pixels needed to guarantee sufficient resolution to compute apparent motion. Above this minimum size, performance is almost invariant under spatial resolution. The slope of the separation line (apart from the first two stems where the detector breaks) seems positively related to the image size, i.e. the OMT feature relies more on a high spatial resolution than the NSD feature does. fire sequence (red markers) rigid sequence (blue markers) Fig. <ref type="figure" target="#fig_5">16</ref>. From OpenCV Horn-Schunck (CVHS) computed on the simulation experiment in Section V-C, the probability of characteristic direction φ <ref type="bibr">[v]</ref> and the probability of characteristic magnitude [v] as defined in <ref type="bibr" target="#b12">[13]</ref> are extracted. This figure stands in direct comparison to Fig. <ref type="figure" target="#fig_13">14</ref> (asterisk -black, circle -city, cross -tree, diamond -valley, square -sky). It is observed that CVHS works well only for black background, whereas for non-trivial backgrounds, the features become unreliable.</p><p>3) Frame Rate: It is known that fire flickers at a rate of roughly 10 H z. To relate simulation time to real time, the flicker rate in the simulation is compared to this rate. Thus, the time step in the simulation can be chosen to correspond to a particular frame rate in real time. A slightly decreasing trend is observed for the error in Fig. <ref type="figure" target="#fig_14">15</ref>(c), when increasing the frame rate from initially 30 frames/sec. The absolute change is small compared to the size changes seen above; this means that the features are robust with respect to temporal resolution. No correlation between the slope of the separation line and the frame rate is observed. 4) Gaussian Noise: Finally, the influence of random noise is studied. Each channel of the RGB-images (scaled between 0 and 255) is perturbed by additive, zero mean, i.i.d. Gaussian noise while the standard deviation is varied. As seen in Fig. <ref type="figure" target="#fig_14">15</ref>(c), up to a standard deviation of 7, the detector is separating the two types of motion increasingly worse. For standard deviations higher than 7, the detector can no longer discriminate between classes. Random noise should, therefore, be avoided when applying the proposed features. The slope is increasing with the noise level as well; this occurs because the NSD optical flow is error-prone to local noise. The OMT optical flow, on the other hand, connects neighboring pixels in its system matrix, thus allowing for some smoothing effect to take place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison to OpenCV Horn-Schunck</head><p>This last section intends to show the inadequacy of standard Horn-Schunck for fire videos, a claim made earlier in the paper. For that purpose, the simulation experiment from Section V-C resulting in Fig. <ref type="figure" target="#fig_13">14</ref> is repeated for the OpenCV implementation of Horn-Schunck (CVHS), which is shown to work best among the methods considered in <ref type="bibr" target="#b12">[13]</ref> for dynamic texture recognition. In our tests, different choices for the regularization parameter (λ = 0.25 and λ = 0.5) do not result in qualitative differences. In order to be able to compare the experiment to Fig. <ref type="figure" target="#fig_13">14</ref>, the best two features from <ref type="bibr" target="#b12">[13]</ref> are extracted from the CVHS flow fields, which are the probability of having a characteristic direction φ[v] and of having a characteristic magnitude <ref type="bibr">[v]</ref>. Fig. <ref type="figure" target="#fig_5">16</ref> shows the result for the simulated fire (red) and the rigid sequence (blue). Whereas the CVHS features work well for a black background, scene variability in form of non-trivial backgrounds make the features unreliable: The feature points in the bottom-left corner strongly overlap for fire and rigid scenes. Moreover, observe that for the rigid sequences, the CVHS features form separated clusters for each background, whereas for the fire sequences, the features are all mixed (except for black background). This shows that standard Horn-Schunck yields consistent results for different backgrounds in the case of rigid motion, but becomes unpredictable for fire content since fire does not satisfy the brightness constancy plus smoothness assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>The very interesting dynamics of flames have motivated the use of motion estimators to distinguish fire from other types of motion. Two novel optical flow estimators, OMT and NSD, have been presented that overcome insufficiencies of classical optical flow models when applied to fire content. The obtained motion fields provide useful space on which to define motion features. These features reliably detect fire and reject non-fire motion, as demonstrated on a large dataset of real videos. Few false detections are observed in the presence of significant noise, partial occlusions, and rapid angle change. In an experiment using fire simulations, the discriminatory power of the selected features is demonstrated to separate fire motion from rigid motion. The controlled nature of this experiment allows for the quantitative evaluation of parameter changes. Key results are the need for a minimum spatial resolution, robustness to changes in the frame rate, and maximum allowable bounds on the additive noise level. Future work includes the development of optical flow estimators with improved robustness to noise that take into account more than two frames at a time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The Proposed fire detection algorithm. The paper's focus is put on the feature extraction block, where two optical flow fields (OMT and NSD) are computed in parallel from which the 4D feature vector is built.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Hue term f (min{|H c -H |, 1 -|H c -H |}) in Eq. (9).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Two examples for the generalized mass transformation Eq. (9). (a) and (c): Original images. (b) and (d): Respective generalized mass (black -0, white -1). Fire texture is preserved, saturated regions are assigned as low mass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 4.Two examples for the generalized mass transformation Eq. (9). (a) and (c): Original images. (b) and (d): Respective generalized mass (black -0, white -1). Fire texture is preserved, saturated regions are assigned as low mass.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. OMT flow fields: fire with dynamic texture (left) and a white hat (right) moving up/right. The red box indicates the area for which the flow field is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. NSD flow fields: saturated fire (left) and a white hat (right) moving up/right. The red box indicates the area for which the flow field is shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. (a) Ideal source flow template and (b) OMT flow field for the fire image in Fig. 5. The source matching feature is obtained by the maximum absolute value of the convolution between (a) and (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Motion histograms for flows in Fig. 6. (a) (fire) has a multi-directional distribution, whereas (b) (hat) is dominantly moving up and right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Examples of detected fire scenes with the resulting probabilities.</figDesc><graphic coords="8,312.47,180.53,81.86,54.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig.</head><label></label><figDesc>Fig. Examples of potential positives classified as non-fire scenes with the resulting probabilities.</figDesc><graphic coords="8,439.31,267.77,123.26,92.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Examples of false negative detections (Fire scene falsely classified as non-fire scene) with the resulting probabilities.</figDesc><graphic coords="9,49.43,326.69,123.26,92.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12.Examples of false positive detections (Non-fire scene falsely classified as fire scene) with the resulting probabilities.</figDesc><graphic coords="9,176.27,326.69,123.26,92.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Sample frames from the fire simulation experiment.</figDesc><graphic coords="10,52.91,340.01,59.06,58.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Directional features f 3 and f 4 for frames from the simulation experiment. Each marker corresponds to one frame. Different markers indicate different backgrounds: asterisk -black, circle -city, cross -tree, diamondvalley, and square -sky. The separation line L best separates the two clusters according to Eq. (25).</figDesc><graphic coords="10,240.47,237.77,59.06,59.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. Influence of parameter changes on the error Eq. (25) and the slope of the separation line L in Fig. 14.</figDesc><graphic coords="10,240.47,340.01,59.06,58.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>φ</head><label></label><figDesc>[v] (characteristic direction) Λ [v] (characteristic magnitude)</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported by the Office of Naval Research under Contract N00014-10-C-0204, Grants from AFOSR and ARO, Grants from the National Center for Research Resources under Grant P41-RR-013218, the National Institute of Biomedical Imaging and Bioengineering under Grant P41-EB-015902 of the National Institutes of Health, and the National Alliance for Medical Image Computing, funded by the National Institutes of Health through the NIH Roadmap for Medical Research, under Grant U54 EB005149.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Bartas</surname></persName>
		</author>
		<title level="m">La Sepmaine ou Creation du Monde</title>
		<editor>
			<persName><forename type="first">Michel</forename><surname>Gadoulleau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jean</forename><surname>Febvrier</surname></persName>
		</editor>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page">1578</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fire detection in video sequences using a generic color model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Çelik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Demirel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fire Safety J</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="158" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A probabilistic approach for vision-based fire detection in videos</title>
		<author>
			<persName><forename type="first">P</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Izquierdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="721" to="731" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Machine vision-based real-time early flame and smoke detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Meas. Sci. Technol</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">45502</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An image processing technique for fire detection in video images</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loepfe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brupbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fire Safety J</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="285" to="289" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vision based fire detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
		<meeting>Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="134" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SVM based forest fire detection using static and dynamic features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Sci. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="821" to="841" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Computer vision based method for real-time fire and flame detection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Toreyin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dedeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Gudukbay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fire detection based on vision sensor and support vector machines</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fire Safety J</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="329" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-sensor fire detection by fusing visual and nonvisual flame features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verstockt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vanoosthuyse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Van Hoecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van De Walle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Int. Conf. Image Signal Process</title>
		<meeting>4th Int. Conf. Image Signal ess</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="333" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Flame recognition in video</title>
		<author>
			<persName><forename type="first">W</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitoria</forename><surname>Lobo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="319" to="327" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Covariance matrix-based fire and flame detection method in video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Habiboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Günay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Çetin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis and performance evaluation of optical flow features for dynamic texture recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process., Image Commun</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="680" to="691" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A brief survey of dynamic texture description and recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Péteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Recognit. Syst</title>
		<meeting>Int. Conf. Comput. Recognit. Syst</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="17" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic texture detection based on motion analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Amiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="63" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic texture as foreground and background</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fazekas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haindl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="741" to="750" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Video fire smoke detection using motion and color features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chunyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jinjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yongming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fire Technol</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="651" to="663" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf</title>
		<meeting>Int. Joint Conf</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Saisan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="58" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dynamic textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chiuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="109" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial segmentation of temporal texture using mixture linear models</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Dyn. Vis</title>
		<meeting>Int. Conf. Dyn. Vis</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling, clustering, and segmenting video with mixtures of dynamic textures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="909" to="926" />
			<date type="published" when="2008-05">May 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Optical flow estimation &amp; segmentation of multiple moving dynamic textures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="516" to="521" />
			<date type="published" when="2005-06">Jun. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fire and smoke detection in video with optimal mass transport based optical flow and neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kolesov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Image Process</title>
		<meeting>IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2010-09">Sep. 2010</date>
			<biblScope unit="page" from="761" to="764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Introduction to the Mechanics of a Continuous Medium</title>
		<author>
			<persName><forename type="first">L</forename><surname>Malvern</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1969">1969</date>
			<publisher>Prentice-Hall</publisher>
			<pubPlace>Upper Saddle River, NJ, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Benamou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Brenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numer. Math</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="393" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the translocation of masses</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Sci</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1381" to="1382" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The geometry of optimal transportation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Gangbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Math</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="161" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Kernel density estimation via diffusion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Botev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grotowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kroese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2916" to="2957" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A video analytics framework for amorphous and unstructured anomaly detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kolesov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tannenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int. Conf. Image Process</title>
		<meeting>18th IEEE Int. Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<biblScope unit="page" from="2945" to="2948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Active contours without edges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Fundamentals of Fire Phenomena</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quintiere</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Physically based modeling and animation of fire</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fedkiw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="721" to="728" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Visual simulation of smoke</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fedkiw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf</title>
		<meeting>Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="15" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">His current research interests include the application of control-theoretic methods to medical image processing and computer vision. Ivan Kolesov received the Ph.D. degree in electrical and computer engineering from the Georgia Institute of Technology</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009, and the Diplom-Ingenieur degree in engineering cybernetics from the University of Stuttgart</title>
		<meeting><address><addrLine>Atlanta, GA, USA; Stuttgart, Germany; Atlanta, GA, USA; Birmingham, AL, USA; Atlanta, GA, USA; Birmingham, AL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2010. 2008. 2010, and 2013. 2012</date>
		</imprint>
		<respStmt>
			<orgName>Georgia Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a Postdoctoral Researcher with the Department of Electrical and Computer Engineering at the University of Alabama at Birmingham. His current research interests include the fields of computer vision and machine learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
