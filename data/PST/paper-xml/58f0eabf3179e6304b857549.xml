<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Learning Based Binaural Speech Separation in Reverberant Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xueliang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Deliang</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Deep Learning Based Binaural Speech Separation in Reverberant Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EEC525F97A4B4BE0C93887AF4D00D6BD</idno>
					<idno type="DOI">10.1109/TASLP.2017.2687104</idno>
					<note type="submission">This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2017.2687104, IEEE/ACM Transactions on Audio, Speech, and Language Processing This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TASLP.2017.2687104, IEEE/ACM Transactions on Audio, Speech, and Language Processing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Binaural speech separation</term>
					<term>computational auditory scene analysis (CASA)</term>
					<term>room reverberation</term>
					<term>deep neural network (DNN)</term>
					<term>Beamforming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech signal is usually degraded by room reverberation and additive noises in real environments. This paper focuses on separating target speech signal in reverberant conditions from binaural inputs. Binaural separation is formulated as a supervised learning problem, and we employ deep learning to map from both spatial and spectral features to a training target. With binaural inputs, we first apply a fixed beamformer and then extract several spectral features. A new spatial feature is proposed and extracted to complement the spectral features. The training target is the recently suggested ideal ratio mask. Systematic evaluations and comparisons show that the proposed system achieves very good separation performance and substantially outperforms related algorithms under challenging multi-source and reverberant environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>E VERYDAY listening scenarios are complex, with multiple concurrent sound sources and their reflections from the surfaces in physical space. Separating the target speech in such an environment is called the "cocktail party problem" <ref type="bibr" target="#b5">[6]</ref>. A solution to the cocktail party problem, also known as the speech separation problem, is important to many applications such as hearing aid design, robust automatic speech recognition (ASR) and mobile communication. However, speech separation remains a technical challenge despite extensive research over decades.</p><p>Since the target speech and background noise usually overlap in time and frequency, it is hard to remove the noise without speech distortion in monaural separation. However, the speech and interfering sources are often located at different positions of the physical space, one can exploit the spatial information for speech separation by using two or more microphones.</p><p>Fixed and adaptive beamformers are common multimicrophone speech separation techniques <ref type="bibr" target="#b28">[29]</ref>. The delayand-sum beamformer is the simplest and most widely used fixed beamformer, which can be steered to a specified direction by adjusting phases for each microphone and adds</p><p>The work was conducted while X. Zhang was visiting The Ohio State University. This research was supported in part by a China National Nature Science Foundation grant (No. 61365006), an AFOSR grant (No. FA9550-12-1-0130) and an NIDCD grant (No. R01DC012048).</p><p>X. Zhang is with the Department of Computer Science, Inner Mongolia University, Hohhot, China (e-mail: cszxl@imu.edu.cn). D.L. the signals from different microphones. One limitation of a fixed beamformer is that it needs a large array to achieve high-fidelity separation. Compared with fixed beamformers, adaptive beamformers provide better performance in certain conditions, like strong and relatively few interfering sources.</p><p>The minimized variance distortionless response (MVDR) <ref type="bibr" target="#b9">[10]</ref> beamformer is a representative adaptive beamformer, which minimizes the output energy while imposing linear constraints to maintain energies from the direction of the target speech. Adaptive beamforming can be converted into an unconstrained optimization problem by using a Generalized Sidelobe Canceller <ref type="bibr" target="#b11">[12]</ref>. However, adaptive beamformers are more sensitive than fixed beamformers to microphone array errors such as sensor mismatch and mis-steering, and to correlated reflections arriving from outside the look direction <ref type="bibr" target="#b0">[1]</ref>. The performance of both fixed and adaptive beamformers diminishes in the presence of room reverberation, particularly when target source is outside the critical distance at which direct-sound energy equals reverberation energy.</p><p>A different class of multi-microphone speech separation is based on Multichannel Wiener Filtering (MWF), which estimates the speech signal of the reference microphone in the minimum-mean-square-error sense by utilizing the correlation matrices of speech and noise. In contrast to beamforming, no assumption of target speech direction and microphone array structure needs to be made, while exhibiting a degree of robustness. The challenge for MWF is to estimate the correlation matrices of speech and noise, especially in nonstationary noise scenarios <ref type="bibr" target="#b25">[26]</ref>.</p><p>Another popular class of binaural separation methods is localization-based clustering <ref type="bibr" target="#b21">[22]</ref>  <ref type="bibr" target="#b37">[38]</ref>. In general, two steps are taken. The localization step is to build the relationship between source locations and interaural parameters, such as interaural difference (ITD) and interaural level difference (ILD), in individual time-frequency (T-F) units. The separation step is to assign each T-F unit into a different sound source by clustering or histogram picking. In <ref type="bibr" target="#b21">[22]</ref>, these two steps are jointly estimated by using an expectation-maximization algorithm.</p><p>Although studied for many years, binaural speech separation is still a challenging problem, especially in multi-source and reverberant conditions. In contrast, the human auditory system is capable of extracting an individual sound source from a complex mixture with two ears. Such perceptual organization is called auditory scene analysis (ASA) <ref type="bibr" target="#b1">[2]</ref>.</p><p>Inspired by ASA, computational auditory scene analysis (CASA) <ref type="bibr" target="#b30">[31]</ref> aims to achieve source separation based on perceptual principles. In CASA, target speech is typically separated by applying a T-F mask to the noisy input. The values of this mask indicate how much energy of a corresponding T-F unit should be retained. The value of the ideal binary mask (IBM) <ref type="bibr" target="#b29">[30]</ref> is 1 or 0, where 1 indicates that the target signal dominates the T-F unit and unit energy is entirely kept, and 0 indicates otherwise. Speech perception research shows that IBM separation produces dramatic improvements of speech intelligibility in noise for both normal-hearing listeners and hearing-impaired listeners. In this context, it is natural to formulate the separation task as a supervised, binary classification problem where the IBM is aimed as the computational goal <ref type="bibr" target="#b29">[30]</ref>. In the binaural domain, this kind of formulation is first done by Roman et al. <ref type="bibr" target="#b23">[24]</ref>, in which a kernel density estimation method is used to model the distribution of the ITD and ILD features and classification is done in accordance with the maximum a posterior (MAP) decision rule.</p><p>Treating speech separation as a supervised learning problem has become popular in recent years, particularly since deep neural networks (DNNs) were introduced for supervised speech separation <ref type="bibr" target="#b31">[32]</ref>. Extensive studies have been done on features <ref type="bibr" target="#b32">[33]</ref>, training targets <ref type="bibr">[15] [34-37]</ref> and deep models <ref type="bibr" target="#b14">[15]</ref> [32] <ref type="bibr">[36] [39]</ref> in the monaural domain. Compared with the rapid progress in monaural separation, the studies on supervised binaural separation are few. Recently, however, Jiang et al. <ref type="bibr" target="#b16">[17]</ref> extract binaural and monaural features and train a DNN for each frequency band to perform binary classification. Their results show that even a single monaural feature can improve separation performance in reverberant conditions when interference and target are very close to each other.</p><p>In this study, we address the problem of binaural speech separation in reverberant environments. In particular, we aim to separate reverberant target speech from spatially diffuse background interference; such a task is also known as speech enhancement. The proposed system is supervised in nature, and employs DNN. Both spatial and spectral features are extracted to provide complementary information for speech separation. As in any supervised learning algorithm, discriminative features play a key role. For spectral feature extraction, we incorporate a fixed beamformer as a preprocessing step and use a complementary monaural feature set. In addition, we propose a two-dimensional ITD feature and combine it with the ILD feature to provide spatial information. Motivated by recent analysis of training targets, our DNN training aims to estimate the ideal ratio mask (IRM), which is shown to produce better separated speech than the IBM, especially for speech quality <ref type="bibr" target="#b33">[34]</ref>. In addition, we conduct feature extraction on fullband signals and train only one DNN to predict the IRM across all frequencies. In other words, the prediction of the IRM is at the frame level, which is much more efficient than subband classification in <ref type="bibr" target="#b16">[17]</ref>.</p><p>In the following section, we present an overview of our DNN-based binaural speech separation system and the extraction of spectral and spatial features. In Section III, we describe the training target and DNN training methodology. The evaluation, including a description of comparison methods, is provided in Section IV. We present the experimental results and comparison in Section V. We conclude the paper in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. SYSTEM OVERVIEW AND FEATURE EXTRACTION</head><p>The proposed speech separation system is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. Binaural input signals are generated by placing the target speaker in a reverberant space with many other simultaneously interfering talkers forming a spatially diffuse, speech babble. In such an environment, the background noise is non-stationary and diffuse. To separate the target speech from the background noise, the left-ear and right-ear signals are first fed into two modules to extract the spectral and spatial features separately. In the upper module, a beamformer is employed to preprocess the two-ear signals to produce a single signal for spectral feature extraction. In the lower module, the left-ear and right-ear signals are each first decomposed into T-F units independently. Then, cross correlation function (CCF) and ILD are extracted in each pair of corresponding left-ear and right-ear units, and regarded as spatial features. The spectral and spatial features are then combined to form the final input feature. Our computational goal is to estimate the IRM. We train a DNN to map from the final input feature to the IRM. After obtaining a ratio mask from the trained DNN, the waveform signal of the target speech is synthesized from the sound mixture and the mask <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spectral Features</head><p>We employ the delay-and-sum (DAS) beamformer to process the left-ear and right-ear signals into a single signal before extracting monaural spectral features. Beamforming is a commonly used spatial filter for microphone array processing. As sounds coming from different directions reach the two ears with different delays, this fixed beamformer is steered to the direction of the target sound by properly shifting the signal of each ear and then sums them together. As the noises coming from other directions are not aligned, the sum will reduce their amplitudes relative to the target signal, hence enhancing the target. The rationale for proposing beamforming before spectral feature extraction is twofold. First, beamforming enhances the target signal, and second, it avoids an adhoc decision of having to choose one side for monaural feature extraction, as done in <ref type="bibr" target="#b16">[17]</ref> for instance.</p><p>After beamforming, we extract amplitude modulation spectrum (AMS), relative spectral transform and perceptual linear prediction (RASTA-PLP) and mel-frequency cepstral coefficients (MFCC). In <ref type="bibr" target="#b32">[33]</ref>, these features are shown to be complementary and have been successfully used in DNNbased monaural separation. It should be mentioned that the complementary feature set originally proposed in <ref type="bibr" target="#b32">[33]</ref> is extracted at the unit level, i.e. within each T-F unit. We extract the complementary feature set at the frame level as done in <ref type="bibr" target="#b33">[34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Features</head><p>We first decompose both the left-ear and right-ear signals into cochleagrams <ref type="bibr" target="#b30">[31]</ref>. Specifically, the input mixture is </p><formula xml:id="formula_0">CCF (c, m, τ ) = k x cm,l (k)x cm,r (k -τ ) k x 2 cm,l (k) k x 2 cm,r (k -τ )<label>(1)</label></formula><p>In the above formula, τ varies between -1 ms and 1 ms, x cm,l and x cm,r represent the left-and right-ear signals of the unit at channel c and frame m, respectively, and k indexes a signal sample of a T-F unit. For the 16 kHz sampling rate, the dimension of CCF is 33. In <ref type="bibr" target="#b16">[17]</ref>, CCF values are directly used as a feature vector to distinguish the signals coming from different locations.</p><p>Here, we propose a new 2-dimensional (2D) ITD feature. The first dimension is the CCF value at an estimated time lag τ , corresponding to the direction of the target speech. The second dimension is the maximum value of CCF, which reflects the coherence of the left and right ear signals, and has been used for selecting binaural cues for sound localization <ref type="bibr" target="#b8">[9]</ref>. The reasons for proposing these two features are as follows. The maximum CCF value is used to distinguish directional sources from diffuse sounds. For a directional source, the maximum CCF value should be close to 1, whereas for a diffuse sound it is close to 0. The CCF value at the estimated target direction is to differentiate the target speech and the interfering sounds that come from different directions. Specifically, we have</p><formula xml:id="formula_1">IT D(c, m) =   CCF (c, m, τ ) max τ CCF (c, m, τ )   (2)</formula><p>ILD corresponds to the energy ratio in dB, and it is calculated for each unit pair as below</p><formula xml:id="formula_2">ILD(c, m) = 10log 10 k x 2 cm,l (k) k x 2 cm,r (k)<label>(3)</label></formula><p>To sum up, the spatial features in each T-F unit pair are composed of 2D ITD and 1D ILD. We concatenate all the unitlevel features at a frame to form the frame-level spatial feature vector. For 64-channel cochleagrams, the total dimension is 192 for each time frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DNN-BASED SPEECH SEPARATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Targets</head><p>The ideal ratio mask (IRM) is defined as <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_3">IRM (c, m) = S 2 (c, m) S 2 (c, m) + N 2 (c, m)<label>(4)</label></formula><p>where S 2 (c, m) and N 2 (c, m) denote the speech and noise energy, respectively, in a given T-F unit. This mask is essentially the square-root of the classical Wiener filter, which is the optimal estimator in the power spectrum <ref type="bibr" target="#b19">[20]</ref>. The IRM is obtained using a 64-channel gammatone filterbank.</p><p>As discussed in Sect. I, the IRM is shown to be preferable to the IBM <ref type="bibr" target="#b33">[34]</ref>. Therefore, we employ the IRM in a frame as the training target, which provides the desired signal at the frame level for supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. DNN Training</head><p>A DNN is trained to estimate the IRM using the frame-level features described in Section II. The DNN includes 2 hidden layers, each with 1000 units. We find that this relatively simple DNN architecture is effective for our task. Recent development in deep learning has resulted in new activation functions <ref type="bibr" target="#b6">[7]</ref> [13] <ref type="bibr" target="#b22">[23]</ref> and optimizers <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b7">[8]</ref>. Here, the rectified linear unit (ReLU) activation function <ref type="bibr" target="#b22">[23]</ref> is used for the hidden layers and the sigmoid activation function is used for the output layer. The cost function is mean square error (MSE). Weights of the DNN are randomly initialized. The adaptive gradient algorithm (AdaGrad) <ref type="bibr" target="#b7">[8]</ref> is utilized for back propagation, which is an enhanced version of stochastic gradient descent (SGD) that automatically determines a per-parameter learning rate. We also employ the dropout technique <ref type="bibr" target="#b26">[27]</ref> on hidden units to avoid overfitting. The dropout rate is 0.5. The total number of training epochs is 100. The batch size is 512.To incorporate temporal context, we use an input window that spans 9 frames (4 before and 4 after) to predict one frame of the IRM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>For both training and test datasets, we generate binaural mixtures by placing the target speaker in a reverberant space with many interfering speech sources simultaneously. A reverberant signal is generated by convolving a speech signal with a binaural room impulse response (BRIR). In this study, we use two sets of BRIRs. One is simulated by software, called BRIR Sim Set. The other is measured in real rooms, called BRIR Real Set. These sets were generated or recorded at the University of Surrey 1 .</p><p>The BRIR Sim Set is obtained from a room simulated using CATT-Acoustics modeling software <ref type="bibr" target="#b3">[4]</ref>. The simulated room is shoebox-shaped with dimensions of 6m × 4m × 3m (length, width, height). The reverberation time (T60) was varied between 0 and 1 second with 0.1s increments by changing the absorption coefficient of all six surfaces. The impulse responses are calculated with the receiver located at the center of the room at a height of 2 m and the source at a distance of 1.5 m from the receiver. The sound source was placed at the head height with azimuth between -90 • and 90 • spaced by 5 • .</p><p>The BRIR Real Set is recorded in four rooms with different sizes and reflective characteristics, and their reverberation times are 0.32s, 0.47s, 0.68s and 0.89s. The responses are captured using ahead and torso simulator (HATS) and a loudspeaker. The loudspeaker was placed around the HATS on an arc in the median plane with a 1.5 m radius between ±90 • and measured at 5 • intervals.</p><p>To generate a diffuse multitalker babble (see <ref type="bibr" target="#b20">[21]</ref>), we use the TIMIT corpus <ref type="bibr" target="#b10">[11]</ref> which contains 6300 sentences, with 10 sentences spoken by each of 630 speakers. Specifically, 10 sentences of each speaker in the TIMIT corpus are first concatenated. Then, we randomly choose 37 speakers, one for each source location as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. A random slice of each speaker is cut and convolved with the BRIR corresponding to its location. Finally, we sum the convolved signals to form the diffuse babble, which is also non-stationary. The IEEE corpus <ref type="bibr" target="#b15">[16]</ref> is employed to generate reverberant binaural target utterances, and it contains 720 utterances spoken by a female speaker. The target source is fixed at azimuth 0 c irc, in front of the dummy head (see Fig. <ref type="figure" target="#fig_0">1</ref>). To generate a reverberant target signal, we convolve an IEEE utterance with the BRIR at 0 c irc. Finally, the reverberant target speech and background noise are summed to yield two binaural mixtures.</p><p>1 http://iosr.uk/software/index.php For the training and development sets, we respectively select 500 and 70 sentences from the IEEE corpus and generate binaural mixtures using BRIR Sim Set with 4 T60 values of 0s, 0.3s, 0.6s and 0.9s; T60 = 0s corresponds to the anechoic condition. The development set is used to determine the DNN parameters. So, the training set includes 2000 mixtures. The remaining 150 IEEE sentences are used to generate the test set. To evaluate the proposed method, we use three sets of BRIRs to build test sets called simulated matched room, simulated unmatched room and real room. For the simulated matchedroom test set, we use the same simulated BRIRs as the ones in the training stage. For the simulated unmatched-room test set, the BRIR Sim Set with T60's of 0.2s, 0.4s, 0.8s and 1.0s are used. The real-room test set is generated by using BRIR Real Set. The SNR of the mixtures for training and test is set to -5dB, which is the average at the two ears. It means that the SNR at a given ear may vary around -5dB due to the randomly generated background noise and different reverberation times. In SNR calculations, the reverberant target speech, not its anechoic version, is used as the signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>We quantitatively evaluate the performance of speech separation by two metrics, which are conventional SNR and short-time objective intelligibility (STOI) <ref type="bibr" target="#b27">[28]</ref>. SNR is calculated as</p><formula xml:id="formula_4">SN R = 10log 10 t S 2 (t) t (S(t) -O(t)) 2<label>(5)</label></formula><p>Here, S(t) and O(t) denote the target signal and the synthesized one from an estimated IRM, respectively. STOI measures objective intelligibility by computing the correlation of short-time temporal envelopes between target and separated speech, resulting in a score in the range of [0, 1], which can be roughly interpreted as the percent-correct predicted intelligibility. STOI is widely used to evaluate speech separation algorithms aiming for speech intelligibility in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison Methods</head><p>We compare the performance of the proposed method with several other prominent and related methods for binaural speech separation. The first kind is beamforming and we choose DAS and MVDR beamformers for comparison. As described earlier, the DAS beamformer is employed as a preprocessor in our system. The MVDR beamformer minimizes the output energy while imposing linear constraints to maintain the energy from the direction of the target speech. Both the DAS and MVDR beamformer need the target DOA (direction of arrival), which should be estimated in general. Because the location of the target speaker is fixed in our evaluation, we provide the target direction to the beamformers, which facilitates the implementation.</p><p>The second method is MWF <ref type="bibr" target="#b24">[25]</ref>. For this method, the correlation matrices of the speech and noises need to be estimated by using voice activity detection (VAD) and speech detection errors will degrade its performance. To avoid the VAD errors, we calculate the noise correlation matrix from the background noise directly. The same is done for MVDR, which also needs to calculate the noise correlation matrix. Therefore, the actual results for MWF and MVDR are expected to be somewhat lower.</p><p>The next one is MESSL <ref type="bibr" target="#b21">[22]</ref> that uses spatial clustering for source localization. Given the number of sources, MESSL iteratively modifies Gaussian mixture models (GMMs) of interaural phase difference and ILD to fit the observed data. Across frequency integration is handled by linking the GMMs models in individual frequency bands to a principal ITD.</p><p>The fourth comparison method employs DNN to estimate the IBM <ref type="bibr" target="#b16">[17]</ref>. First, input binaural mixtures are decomposed into 64-channel subband signals. At each frequency channel, CCF, ILD and monaural GFCC (gammatone frequency cepstral coefficient) features are extracted and used to train a DNN for subband classification. Each DNN has two hidden layers each containing 200 sigmoidal units, which is the same as in <ref type="bibr" target="#b16">[17]</ref>. Weights of DNNs are pre-trained with restricted Boltzmann machines. The subband binaural classification algorithm is referred as SBC in the following. It should be mentioned that, even though each DNN is small, SBC uses 64 DNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EVALUATION AND COMPARISON A. Simulated-room Conditions</head><p>In this test condition, we intend to evaluate the performance of the proposed algorithm in the simulated rooms, which are divided into two parts: matched and unmatched conditions. As mentioned earlier, for matched-room conditions, test reverberated mixtures are generated by using the same BRIRs as in the training stage, where the T60s are 0.3s, 0.6s and 0.9s. For the unmatched-room conditions, the BRIRs for generating reverberated mixtures are still simulated ones, but the T60s are different from those in training conditions and take the values of 0.2s, 0.4s, 0.8s and 1.0s.The results of STOI and SNR are shown in Table <ref type="table" target="#tab_1">I</ref> and Fig. <ref type="figure" target="#fig_1">2</ref>, respectively. The "MIX L " and "MIX R " refer to the unprocessed mixtures at the left and right ear respectively.</p><p>Compared the unprocessed mixtures, the proposed system obtains the absolute STOI gain about 22% on average in the As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the proposed algorithm also obtains the largest SNR gains in all conditions. It can be seen that SBC outperforms MWF in the matched-room and less reverberant unmatched-room conditions. The SNR gains obtained by MESSL are much larger than those of DAS and MVDR, while these three methods have similar STOI scores. The main reason is that SNR does not distinguish noise distortion and speech distortion, which affect speech intelligibility in different ways.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-room Conditions</head><p>In this test condition, we use the BRIR Real Set to evaluate the proposed separation system and compare it with other methods. The STOI and SNR results are given in Table <ref type="table" target="#tab_2">II</ref> and Fig. <ref type="figure" target="#fig_2">3</ref>, respectively. The proposed system achieves the best results in all four room conditions. Compared with unprocessed mixtures, the average STOI gain is about 20% (i.e. from 43% to 63%), which is consistent with that in simulated room conditions.</p><p>From the above experimental results, we can see that the proposed algorithm outperforms SBC which is also a DNNbased separation algorithm. One of the differences is that the proposed algorithm employs ratio masking for separation, while SBC utilizes binary masking. As described earlier, binary masking is not as preferable as ratio masking. A simple way to turn a binary mask to a ratio mask in the context of DNN is to directly use the outputs of the subband DNNs, which can be interpreted as posterior probabilities with values ranging from 0 to 1. With such soft masks, SBC's average STOI scores are 63.25% for matched-room conditions, 61.96% for unmatched-room conditions and 55.80% for real-room conditions. These results represent significant improvements over binary masks, but they are still not as high as those of the proposed algorithm.</p><p>Fig. <ref type="figure">4</ref> illustrates the spectrograms of separated speech using different methods on a test utterance mixed with the multitalker babble noise at -5dB in a highly reverberant condition with T60=0.89s. As shown in the figure, the spectrogram of the separated speech using the proposed method is close to that of clean reverberant speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Further Analysis</head><p>Our binaural speech separation system uses both spectral and spatial features. For spectral features, the DAS beamformer is employed as a preprocessor. The spatial features are formed by combining the proposed 2D ITD and ILD. Previous work <ref type="bibr" target="#b16">[17]</ref> shows that binaural separation can benefit from joint spectral and spatial features. In fact, several reasonable spectral and spatial features could be constructed. In this subsection, we further analyze several alternatives. Also we compare with alternative training targets.</p><p>1) Spectral features: One simple way to combine spectral and spatial analyses is to directly concatenate the left-and right-ear monaural features. In this case, we extract the complementary feature set from the left-and right-ear signals independently and concatenate them to form the input feature vector for DNN. We compare this feature vector with the proposed beamformed features and also single-ear monaural features (left-ear as in <ref type="bibr" target="#b16">[17]</ref>). The interaural features are excluded here. The same DNN configuration and training procedure are used (see Section III-B). The test datasets are also the same. Average STOI results are shown in Fig. <ref type="figure">5</ref>. From the figure, we can see that extracting the spectral features on the output signal of the beamformer is better than concatenating the spectral features of the left-and right-ear signals. The beamformed and concatenated features are more effective than the single-ear feature.</p><p>2) Spatial features: ITD and ILD are the most commonly used cues for binaural separation. While ILD is typically calculated in the same way (see Eq. 3), the representation of ITD information varies in different algorithms. In <ref type="bibr" target="#b23">[24]</ref>, ITD was estimated as the lag corresponding to the maximum of CCF. Jiang et al. <ref type="bibr" target="#b16">[17]</ref> directly used CCF to characterize the interaural time difference. They also show that the CCF is more effective than ITD as a unit-level feature. In contrast, only two values of CCF in each T-F unit are selected in our system.</p><p>We compare the performances of using conventional ITD <ref type="bibr" target="#b23">[24]</ref>, CCF and the proposed 2D ITD as the spatial features. To make the comparison, the frame-level features are formed by concatenating ITD, CCF and 2D ITD in each T-F unit. Since concatenating unit-level CCF vectors directly leads to a very high dimension, we perform principal component analysis (PCA) to reduce the dimension to 128, equal to the size of 2D ITD frame-level feature. Three DNNs with the same configuration are trained using these different spatial features. The STOI results are shown in Fig. <ref type="figure">6</ref>. We can see that the results with the conventional ITD are much worse than CCF plus PCA and the proposed 2D ITD. This indicates that the conventional ITD is not discriminative in reverberant conditions. While proposed 2D ITD yields essentially the same efficiency. As CCF changes with target speech direction, the DNN has to be trained for multiple target directions as done in <ref type="bibr" target="#b16">[17]</ref>. On the other hand, our 2D ITD feature requires target direction to be estimated.</p><p>3) Fullband vs. subband separation: Early supervised speech separation algorithms <ref type="bibr">[17] [18]</ref> typically perform subband separation. In contrast, the proposed algorithm employs fullband separation. Earlier in this section, the proposed algorithm has been demonstrated to perform much better than the SBC algorithm of Jiang et al. <ref type="bibr" target="#b16">[17]</ref>. To what extent can the better performance be attributed to fullband separation? This question is not addressed in earlier comparisons since the features and the training target of the SBC algorithm are different from ours, and also the DNN for each frequency channel is relatively small in <ref type="bibr" target="#b16">[17]</ref>. Here, we make a comparison between subband and fullband separation by using the same features and the same training target.</p><p>For subband separation, DAS beamforming is first applied to convert the left-and right-ear signals into a single-channel signal. Then, we decompose the signal into 64 channels by using the gammatone filterbank. For each frequency band, we extract the complementary feature set <ref type="bibr" target="#b32">[33]</ref>, 2D ITD and ILD. The same temporal context is utilized by incorporating 9 frames (4 before and 4 after). The training target is the IRM. The configuration of DNN for each frequency channel is the same as described in section III-B.</p><p>The STOI results are shown in Fig. <ref type="figure" target="#fig_4">7</ref>. We can see that fullband separation still performs better with the same features and training target. Of course, another disadvantage of subband separation is its computational inefficiency with a multitude of DNNs to be trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Training targets:</head><p>This study uses the IRM as the training target, and a more direct target is the spectral magnitude of the target speech <ref type="bibr" target="#b36">[37]</ref>. However, such spectral mapping is many-to-one and more difficult to estimate than the IRM <ref type="bibr" target="#b33">[34]</ref>. Signal approximation (SA) <ref type="bibr" target="#b14">[15]</ref> [36] is a training target that can be viewed as a combination of ratio masking and spectral mapping. SA-based speech separation has been shown to yield higher signal-to-distortion ratio compared to masking-based or mapping-based methods. The difference between Huang et al. <ref type="bibr" target="#b14">[15]</ref> and Weninger et al. <ref type="bibr" target="#b35">[36]</ref> is that the former makes of both target and interference signals.</p><p>In this subsection, compare IRM estimation and the two SA-based methods mentioned above. For this comparison, the input features, DNN configurations and training procedures (seen in Section II-B) are the same for all the three methods. The STOI scores are shown in Fig. <ref type="figure">8</ref> We close this section by discussing computational complexity. Compared to the training-based algorithms of SBC and MESSL, the proposed algorithm is faster, as SBC uses a DNN for each of 64 subbands and MESSL utilizes the slow expectation maximization algorithm. The DAS, MVDR and MWF beamformers have much lower computational complexities than the proposed algorithm with the given target direction, because feature extraction in our algorithm is time consuming, especially the CCF calculation. On the other hand, the beamformers need DOA estimation when target direction is unknown, and CCF-based DOA estimation <ref type="bibr" target="#b18">[19]</ref> is a representative method. In other words, the beamforming techniques and the proposed algorithm have the same level computational complexity when DOA estimation is performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUDING REMARKS</head><p>In this work, we have proposed a DNN-based binaural speech separation algorithm which combines spectral and spatial features. DNN-based speech separation has shown its ability to improve speech intelligibility <ref type="bibr" target="#b31">[32]</ref> [14] even with just monaural spectral features. As demonstrated in previous work <ref type="bibr" target="#b16">[17]</ref>, binaural speech separation by incorporating monaural features represents a promising direction to further elevate separation performance.</p><p>For supervised speech separation, input features and training targets are both important. In this study, we make a novel use of beamforming to combine left-ear and right-ear monaural signals before extracting spectral features. In addition, we have proposed a new 2D ITD feature. With the IRM as the training target, the proposed system outperforms representative multichannel speech enhancement algorithms and also a DNNbased subband classification algorithm <ref type="bibr" target="#b16">[17]</ref> in non-stationary background noise and reverberant environments.</p><p>A major issue of supervised speech separation is generalization to untrained environments. Our algorithm shows consistent results in unseen reverberant noisy conditions. This strong generalization ability is partly due to the use of effective features. Although only one noisy situation is considered, the noise problem can be addressed by involving large-scale training data <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the present study, the target speaker is fixed to the front direction and sound localization is not addressed. For the proposed algorithm, two parts need the target direction. One is DAS beamforming and the other is calculation of 2D ITD. Sound localization is a well-studied problem <ref type="bibr" target="#b30">[31]</ref>. Recently, DNN is also used for sound localization <ref type="bibr" target="#b20">[21]</ref>, although only spatial features are considered. We believe that incorporating monaural separation is a good direction to improve the robustness of sound localization in adverse environments with both background noise and room reverberation. One way to incorporate monaural separation is to employ spectral features for initial separation, from which reliable T-F units are selected for sound localization. Moreover, separation and localization could be done iteratively as in <ref type="bibr" target="#b38">[39]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Schematic diagram of the proposed binaural separation system.</figDesc><graphic coords="3,269.67,150.20,143.49,52.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Average SNRs of different methods in simulated room conditions. (a) SNR results in simulated matched-room conditions. (b) SNR results in simulated unmatched-room conditions.</figDesc><graphic coords="6,48.49,268.25,252.00,197.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Average SNRs of different methods in real room conditions.</figDesc><graphic coords="6,311.51,56.36,251.91,197.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .Fig. 5 .Fig. 6 .</head><label>456</label><figDesc>Fig. 4. Spectrograms of separated speech using different algorithms in a recorded room condition (Room D with T60=0.89s). The input SNR is -5 dB. (a) Clean speech. (b) Mixture at the left ear. (c) Mixture at the right ear. (d) Result of DAS. (e) Result of MVDR. (f) Result of MWF. (g) Result of MESSL. (h) Result of SBC. (i) Result of the proposed algorithm.</figDesc><graphic coords="7,126.57,537.53,100.57,90.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Comparison of DNN-based speech separation using different spatial features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I AVERAGE</head><label>I</label><figDesc>STOI SCORES (%) OF DIFFERENT METHODS IN SIMULATED MATCHED-ROOM AND UNMATCHED-ROOM CONDITIONS. T60 MIX L MIX R DAS MVDR MWF MESSL SBC Pro.</figDesc><table><row><cell>Matched room</cell><cell>0.0s 58.00 58.04 63.56 63.75 66.86 65.92 63.65 74.66 0.3s 53.13 52.64 58.61 58.78 64.06 58.66 62.79 74.88 0.6s 44.08 41.00 50.82 50.84 57.72 51.89 55.08 68.53 0.9s 44.58 43.31 48.20 48.15 57.38 48.46 53.37 65.39 Avg. 49.05 49.65 55.30 55.38 61.51 56.23 58.72 70.87</cell></row><row><cell>Unmatched room</cell><cell>0.2s 55.28 57.20 61.80 61.91 65.35 60.52 64.68 74.95 0.4s 47.98 48.82 54.46 54.64 61.21 55.91 59.02 70.40 0.8s 39.99 41.59 47.06 47.01 56.86 47.12 54.27 65.92 1.0s 39.05 40.95 45.21 45.01 55.55 46.05 51.64 62.82 Avg. 45.58 47.14 52.13 52.14 59.74 52.40 57.40 68.52</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II AVERAGE</head><label>II</label><figDesc>STOI SCORES (%) OF DIFFERENT METHODS IN REAL ROOM CONDITIONS. Room MIX L MIX R DAS MVDR MWF MESSL SBC Pro.</figDesc><table><row><cell cols="2">A (0.32s) 47.49 49.02 53.71 53.84 59.50 54.39 53.37 66.70</cell></row><row><cell cols="2">B (0.47s) 41.29 42.55 48.10 48.08 55.01 48.61 42.95 61.96</cell></row><row><cell cols="2">C (0.68s) 44.33 45.06 51.31 50.86 58.39 52.11 54.13 64.78</cell></row><row><cell cols="2">D (0.89s) 39.61 39.18 45.48 45.58 55.22 45.35 48.52 60.57</cell></row><row><cell>Avg.</cell><cell>43.18 43.95 49.65 49.59 57.03 50.12 49.74 63.50</cell></row><row><cell cols="2">simulated matched-room conditions and 23% in the simulated</cell></row><row><cell cols="2">unmatched-room conditions. From Table I, we can see that the</cell></row><row><cell cols="2">proposed system outperforms the other comparison methods</cell></row><row><cell cols="2">in anechoic and all reverberation conditions. The second-</cell></row><row><cell cols="2">best system is MWF. DAS and MVDR have similar results,</cell></row><row><cell cols="2">because the background noise is quite diffuse; it can be proven</cell></row><row><cell cols="2">that MVDR and DAS become identical when noise is truly</cell></row><row><cell cols="2">diffuse. For the supervised learning algorithms, both SBC</cell></row><row><cell cols="2">and the proposed algorithm exhibit good generalization in the</cell></row><row><cell cols="2">unmatched-room conditions.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. We can see that ratio masking produces the highest scores. Due to its inclusion of interference signal, Huang et al.'s method outperforms The SNR results are given in Fig.9. It can be seen that the SA-based methods obtain higher SNR, particularly Huang et al.'s version. Higher SNRs are expected as signal approximation aims to maximize output SNR<ref type="bibr" target="#b35">[36]</ref>. Similar results are obtained with different DNN configurations (with larger or smaller hidden layers, and one more hidden layer).</figDesc><table><row><cell cols="2">0.75</cell><cell></cell><cell cols="2">Ratio Masking</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SA [15]</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell cols="2">SA [36]</cell></row><row><cell cols="2">0.65</cell><cell></cell><cell></cell></row><row><cell>STOI</cell><cell>0.6</cell><cell></cell><cell></cell></row><row><cell cols="2">0.55</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell></row><row><cell cols="2">0.45</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell>Matched room</cell><cell>Unmatched room</cell><cell>Real room</cell></row><row><cell cols="4">Fig. 8. Average STOI scores of using different targets.</cell></row><row><cell></cell><cell>4.5</cell><cell></cell><cell cols="2">Ratio Masking</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SA [15]</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SA [36]</cell></row><row><cell></cell><cell>4</cell><cell></cell><cell></cell></row><row><cell>SNR (dB)</cell><cell>3 3.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.5</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>Matched room</cell><cell>Unmatched room</cell><cell>Real room</cell></row><row><cell cols="4">Fig. 9. Average SNR of using different targets.</cell></row></table><note><p>Weninger et al.'s.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. , NO. ,</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Yuxuan Wang for providing his DNN code, Yi Jiang for assistance in using his SBC code and the Ohio Supercomputing Center for providing computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Microphone Arrays: Signal Processing Techniques and Applications</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Brandstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Ward</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Auditory Scene Analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Bregman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><surname>Catt-Acoustic</surname></persName>
		</author>
		<ptr target="http://www.catt.se/CATT-Acoustic.htm" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale training to increase speech intelligibility for hearing-impaired listeners in novel noises</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="2604" to="2612" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On Human Communication</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1957">1957</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ource localization in complex listening situations: Selection of binaural cues based on interaural coherence</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merimaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="3075" to="3089" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An algorithm for linearly constrained adaptive array processing</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE</title>
		<meeting>of the IEEE</meeting>
		<imprint>
			<date type="published" when="1972">1972</date>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="926" to="935" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DARPA TIMIT acoustic phonetic continuous speech corpus</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<ptr target="http://www.ldc.upenn.edu/Catalog/LDC93S1.html" />
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An alternative approach to linearly constrained adaptive beamforming</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Ant. Prop</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="27" to="34" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV 2015</title>
		<meeting>ICCV 2015</meeting>
		<imprint>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An algorithm to improve speech recognition in noise for hearing-impaired listeners</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Yoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="3029" to="3038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint optimization of masks and deep recurrent neural networks for monaural source separation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="2136" to="2147" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">IEEE recommended practice for speech quality measurements</title>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Electroacoust</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="225" to="246" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binaural classification for reverberant speech segregation using deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="2112" to="2121" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An algorithm that improves speech intelligibility in noise for normal-hearing listeners</title>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1486" to="1494" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The generalized correlation method for estimation of time delay</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust. Speech Signal Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Speech Enhancement: Theory and Practice</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CRC</publisher>
			<pubPlace>Boca Raton, FL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploiting deep neural networks and head movements for binaural localisation of multiple speakers in reverberant conditions</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>May</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3302" to="3306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-based expectationmaximization source separation and localization</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="382" to="394" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML 2010</title>
		<meeting>ICML 2010</meeting>
		<imprint>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Speech segregation based on sound localization</title>
		<author>
			<persName><forename type="first">N</forename><surname>Roman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="2236" to="2252" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On optimal frequency-domain multichannel linear filtering for noise reduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Souden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Affes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="260" to="276" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Robustness analysis of multichannel Wiener filtering and generalized sidelobe cancellation for multimicrophone noise reduction in hearing aid applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Spriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wouters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="487" to="503" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beamforming: A versatile approach to spatial filtering</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Buckley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On ideal binary mask as the computational goal of auditory scene analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Separation by Humans and Machines</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Divenyi</surname></persName>
		</editor>
		<meeting><address><addrLine>Norwell, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="181" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Brown</surname></persName>
		</author>
		<title level="m">Computational Auditory Scene Analysis: Principles, Algorithms, and Applications</title>
		<meeting><address><addrLine>Hobboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley-IEEE Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards scaling up classification-based speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1381" to="1390" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploring monaural features for classification-based speech segregation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="270" to="279" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On training targets for supervised speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1849" to="1858" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Oracle performance investigation of the ideal masks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWAENC 2016</title>
		<meeting>IWAENC 2016</meeting>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminatively trained recurrent neural for single-channel speech separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Global Conf. Signal and Information Process</title>
		<meeting>IEEE Global Conf. Signal and Information ess</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="577" to="581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An experimental study on speech enhancement based on deep neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="65" to="68" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Blind separation of speech mixtures via time-frequency masking</title>
		<author>
			<persName><forename type="first">O</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="1830" to="1847" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">His research interests include speech separation, computational auditory scene analysis and speech signal processing</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeLiang Wang received the NSF Research Initiation Award in 1992 and the ONR Young Investigator Award in 1996. He received the OSU College of Engineering Lumley Research Award in 1996, 2000, 2005, and 2010. His 2005 paper</title>
		<meeting><address><addrLine>Hohhot, China; Beijing, China; Los Angeles; Columbus, OH; Cambridge, MA; Copenhagen, Denmark; Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computational Intelligence Society</publisher>
			<date type="published" when="1998-09">2016. 2003. 2010. August 2015 to September 2016. 1998 to September 1999. October 2006 to June 2007</date>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1066" to="1078" />
		</imprint>
		<respStmt>
			<orgName>Harbin Institute of Technology, Harbin ; Computer Science and Engineering, Ohio State University (OSU) ; Science, Inner Mongolia University ; Department of Computer Science and Engineering and the Center for Cognitive Science at The Ohio State University ; Department of Psychology at Harvard University</orgName>
		</respStmt>
	</monogr>
	<note>The time dimension for scene analysis. He also received the 2008 Helmholtz Award from the International Neural Network Society, and was named a University Distinguished Scholar in 2014. He was an IEEE Distinguished Lecturer (2010-2012. and is an IEEE Fellow</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
