<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSPatch: Dual Spatial Paern Prefetcher</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rahul</forename><surname>Bera</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Processor Architecture Research Lab</orgName>
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anant</forename><forename type="middle">V</forename><surname>Nori</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Processor Architecture Research Lab</orgName>
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sreenivas</forename><surname>Subramoney</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Processor Architecture Research Lab</orgName>
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DSPatch: Dual Spatial Paern Prefetcher</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data prefetching</term>
					<term>microarchitecture</term>
					<term>memory latency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>High main memory latency continues to limit performance of modern high-performance out-of-order cores. While DRAM latency has remained nearly the same over many generations, DRAM bandwidth has grown signicantly due to higher frequencies, newer architectures (DDR4, LPDDR4, GDDR5) and 3D-stacked memory packaging (HBM). Current state-of-the-art prefetchers do not do well in extracting higher performance when higher DRAM bandwidth is available. Prefetchers need the ability to dynamically adapt to available bandwidth, boosting prefetch count and prefetch coverage when headroom exists and throttling down to achieve high accuracy when the bandwidth utilization is close to peak.</p><p>To this end, we present the Dual Spatial Pattern Prefetcher (DSPatch) that can be used as a standalone prefetcher or as a lightweight adjunct spatial prefetcher to the state-of-the-art delta-based Signature Pattern Prefetcher (SPP). DSPatch builds on a novel and intuitive use of modulated spatial bit-patterns. The key idea is to:</p><p>(1) represent program accesses on a physical page as a bit-pattern anchored to the rst "trigger" access, (2) learn two spatial access bit-patterns: one biased towards coverage and another biased towards accuracy, and (3) select one bit-pattern at run-time based on the DRAM bandwidth utilization to generate prefetches. Across a diverse set of workloads, using only 3.6KB of storage, DSPatch improves performance over an aggressive baseline with a PC-based stride prefetcher at the L1 cache and the SPP prefetcher at the L2 cache by 6% (9% in memory-intensive workloads and up to 26%). Moreover, the performance of DSPatch+SPP scales with increasing DRAM bandwidth, growing from 6% over SPP to 10% when DRAM bandwidth is doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computer systems organization ? Processors and memory architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>High main memory latency continues to limit the performance of modern high-performance out-of-order (OOO) cores. Prefetching is a well-studied approach to mitigate the performance impact of the high memory latency <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b35">38,</ref><ref type="bibr" target="#b40">43,</ref><ref type="bibr" target="#b42">45,</ref><ref type="bibr" target="#b46">49,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b59">62,</ref><ref type="bibr" target="#b62">65,</ref><ref type="bibr" target="#b66">69,</ref><ref type="bibr" target="#b69">[72]</ref><ref type="bibr" target="#b70">[73]</ref><ref type="bibr" target="#b71">[74]</ref><ref type="bibr" target="#b74">77]</ref>. A primary metric to improve performance using prefetchers is coverage, which is the fraction of program loads to memory that are removed by the prefetcher. At odds with coverage, but still very important, is prefetcher accuracy, which is the fraction of issued prefetches that are actually needed by the program loads. Inaccurate prefetches can pollute the small on-die caches and can cause excessive pressure on memory bandwidth, which in turn can increase the latency of responses from memory.</p><p>While DRAM latency has remained nearly the same over decades <ref type="bibr" target="#b24">[27,</ref><ref type="bibr" target="#b56">59]</ref>, DRAM bandwidth has grown signicantly <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b55">58]</ref>. Higher DRAM core frequencies and new DRAM architectures (e.g., DDR4 <ref type="bibr" target="#b8">[10]</ref>, LPDDR4 <ref type="bibr" target="#b9">[12]</ref>, GDDR5 <ref type="bibr">[11]</ref>) boost memory bandwidth at the same memory interface width. Newer 3D-stacked memory packages <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b21">24,</ref><ref type="bibr" target="#b55">58]</ref> enable higher bandwidth by increasing the memory interface width. When higher DRAM bandwidth headroom is available, the negative impact due to inaccurate prefetches is lower. While the DRAM bandwidth is shared among multiple cores in a system, several prior studies across mobile, client and server systems <ref type="bibr" target="#b37">[40,</ref><ref type="bibr" target="#b44">47,</ref><ref type="bibr" target="#b49">52,</ref><ref type="bibr" target="#b58">61,</ref><ref type="bibr" target="#b65">68,</ref><ref type="bibr" target="#b72">75]</ref> have observed that the available memory bandwidth is not heavily utilized. Latency is often a bigger bottleneck than bandwidth, since either <ref type="bibr" target="#b0">(1)</ref> there are very few active threads running in the system, (2) not all threads are memory sensitive, or (3) there is not enough memory parallelism present in the program to fully utilize the memory bandwidth <ref type="bibr" target="#b37">[40]</ref>. Yet, as we demonstrate in Figure <ref type="figure" target="#fig_1">1</ref>, current state-of-the-art prefetchers (Signature Pattern Prefetcher (SPP) <ref type="bibr" target="#b51">[54]</ref>, Best Oset Prefetcher (BOP) <ref type="bibr" target="#b59">[62]</ref> and Spatial Memory Streaming (SMS) <ref type="bibr" target="#b70">[73]</ref>) do not scale well in performance with increasing peak DRAM bandwidth. Prefetching  techniques need to evolve to make the best use of this critical DRAM bandwidth resource when power budget is available. Specically, prefetchers need to possess the ability to dynamically adapt to available DRAM bandwidth, boosting predictions and coverage when headroom exists and throttling down to achieve high accuracy when the bandwidth utilization is close to peak.</p><p>Prefetching is a speculation mechanism to predict future addresses to be accessed by the program. Address access patterns can be represented in various forms, including full addresses, osets in a spatial region (typically a 4 KB page), or address deltas between accesses. Choosing an address access representation that has the best chance of exposing repeating patterns can help to boost prefetch coverage and performance.</p><p>Patterns in address accesses that are readily apparent when taking a global or accumulative view of accesses may not be visible when taking a restricted view of deltas between recent consecutive accesses. Figure <ref type="figure" target="#fig_2">2</ref> illustrates an example of multiple streams of accesses within a single spatial region and their representation in various formats. The rst access to the region is called the "trigger" access. Access streams B through E have the same trigger oset in the spatial region and touch all the same osets but in dierent temporal order. Such variations are typically an artifact of reordering due to out-of-order scheduling in the core and the cache/memory sub-systems. The longer the access sequence, the higher the probability of variations <ref type="bibr" target="#b40">[43]</ref>. These access streams all have dierent representations when successive address deltas are used to represent the access patterns. Yet, we realize that they can actually be represented by a single spatial bit-pattern. For example, access streams B and C with trigger oset 1 have two dierent delta representations (+4,-1,+7,+1 and +4,+6,-7,+8) but the same (i.e., single) bit-pattern representation BP2 (0100 1100 0001 1000). Crucially, we observe that when bit-patterns are anchored to the "trigger" oset (rotated left in this case), all access streams in the example can be represented by a single anchored bit-pattern. Such an anchored bit-pattern essentially represents two views of the delta stream: the deltas between consecutive accesses (we call this the local view of deltas) and the deltas relative to the trigger access (we call this the global view of deltas). While the use of anchored spatial bit-patterns can boost coverage, it does not provide the ability to adapt and scale prefetch coverage based on the available resources and DRAM bandwidth in the system. Multiple access streams in a spatial region can have anchored bit-patterns that are very similar (i.e., have some common set bits) but not exactly the same. We propose a novel and intuitive approach to simultaneously optimize coverage and accuracy by learning two bit-patterns: one biased towards coverage and another towards accuracy. A bitwise OR operation on the recently-observed anchored bit-patterns in a given memory region adds bits into a resultant bit-pattern (called the coverage-biased bit-pattern) and thus modulates the resultant bit-pattern for higher coverage. Similarly, a bitwise AND operation on the recently-observed anchored bit-patterns in a given memory region subtracts bits away from a resultant bit-pattern (called the accuracy-biased bit-pattern) and thus modulates the second resultant bit-pattern for higher accuracy. Figure <ref type="figure" target="#fig_3">3</ref> shows an example of how multiple dierent address patterns to the same memory region that map to three dierent anchored bit-patterns can be modulated into a coverage-biased bitpattern (shown in green) and an accuracy-biased bit-pattern (shown in red). As we will show, dynamic modulation of these bit-patterns enables simultaneous optimization for both coverage and accuracy, even though these metrics are at odds with each other. The available memory bandwidth headroom, coupled with a quantied measure of accuracy and coverage can be used to select between the two two modulated bit-patterns dynamically at run-time. We make the following key contributions in this work:</p><p>? We observe that even though peak DRAM bandwidth is growing with newer DRAM architectures and packages, state-ofthe-art prefetcher performance does not scale well with the growing DRAM bandwidth. ? We show that a spatial bit-pattern representation anchored around a trigger access to a region can eectively capture all deltas (local and global) from the trigger. This transformation exposes similar patterns that are otherwise obfuscated to look dierent due to memory access reordering in the processor and the memory subsystem. ? We introduce a new prefetching algorithm that learns two modulated bit-patterns to prefetch in a given memory region by using simple logical OR and AND operations. One bitpattern is biased towards coverage and the other is biased towards accuracy. ? We propose a simple method to track DRAM bandwidth utilization and to measure the coverage and accuracy of modulated bit-patterns. We show that this method enables eective dynamic selection of a single bit-pattern to generate prefetch candidates at run-time.</p><p>Across a diverse set of 75 workloads, with only 3.6KB of storage, DSPatch improves performance over an aggressive baseline that employs a PC-based stride prefetcher at the L1 cache and the SPP at the L2 cache by 6% (9% in memory-intensive workloads and up to 26%). As a standalone prefetcher, DSPatch has slightly (1%) higher performance than the state-of-the-art SPP with only 2/3 rd of the storage requirements of SPP. We nd that, the use of SPP and DSPatch together combines the benets of both the state-of-theart ne-grained delta-based prefetching and the state-of-the-art bit-pattern-based prefetching. We show that, by simultaneously optimizing for both coverage and accuracy, every 2% increase in coverage with DSPatch comes at only a 1% increase in mispredictions. Finally, the performance of DSPatch+SPP scales well with increasing memory bandwidth, growing from 6% over SPP to 10% when DRAM bandwidth is doubled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND MOTIVATION</head><p>Prefetching is a speculation technique that predicts the addresses of high-latency accesses in the program and brings the associated data into low-latency on-die caches for use by the program. Highlatency accesses, typically to DRAM main memory, often stall the retirement of instructions in a core <ref type="bibr" target="#b62">[65,</ref><ref type="bibr" target="#b63">66]</ref>. They also reduce the look-ahead for instruction-level-parallelism (ILP) extraction, since lling up of the re-order buer (ROB) due to memory access related stalls prevents allocation of younger independent instructions into processor structures <ref type="bibr" target="#b62">[65,</ref><ref type="bibr" target="#b63">66]</ref>.</p><p>Multiple mechanisms to represent address access patterns in a program have been studied over the years. Address access patterns can be represented via various means, including (1) full addresses, (2) osets in a spatial region (typically a 4 KB page) or (3) address deltas between consecutive accesses. In order to identify such patterns, prefetchers typically examine a subset of memory accesses ltered by certain program context. For example, a PC-based stride prefetcher tries to learn a constant stride between consecutive cacheline addresses referenced by a program counter (PC) value. Here, the PC acts as a program context that lters out accesses, in order to easily discover the access pattern. We call this program context that is used for ltering accesses as signature. A signature can be constructed using memory access information of a program like physical addresses, osets or deltas between consecutive accesses, and can potentially be augmented with program control ow information like the PC. Prefetchers typically learn the repeating program address access pattern by correlating it with a signature and predict that the learnt pattern will be needed again when the signature is seen again. The design choice a prefetcher makes on what to use as a signature and as the address access pattern representation determines its eectiveness at optimizing the four main metrics of prefetching:</p><p>? Coverage: The fraction of high-latency memory accesses of the program that are saved by the prefetcher (the higher the better) ? Timeliness: The fraction of the latency of the high-latency accesses hidden by the prefetcher (the higher the better)</p><p>? Accuracy: The fraction of prefetched addresses that are later needed by the program (the higher the better) ? Storage: The hardware storage requirements of the prefetcher (the smaller the better)</p><p>In the rest of this section, we comprehensively examine three state-of-the-art prefetchers (SPP <ref type="bibr" target="#b51">[54]</ref>, BOP <ref type="bibr" target="#b59">[62]</ref> and SMS <ref type="bibr" target="#b70">[73]</ref>) with respect to their choices of signature and address access pattern representations. By analyzing a wide range of workloads, we compare the performance and identify the merits of the three types of prefetching. Finally, for each of the prefetchers, wherever possible, we evaluate potential dynamic bandwidth-aware tuning opportunities (similar to <ref type="bibr" target="#b32">[35,</ref><ref type="bibr" target="#b71">74]</ref>) for higher coverage to arrive at the prefetcher's best possible scalability in the presence of memory bandwidth headroom.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Signature Pattern Prefetcher (SPP)</head><p>SPP <ref type="bibr" target="#b51">[54]</ref> is the state-of-the-art delta-based prefetcher. It uses a signature comprised solely of up to four recent consecutive address deltas (which we call "local" deltas) observed in a 4KB page. Each signature tracks at most four possible next deltas as prefetch candidates, along with a condence value associated with each candidate. This allows SPP to track complex but repeating address delta patterns at low cost. SPP uses a recursive look-ahead mechanism to boost prefetch distance and timeliness. SPP appends each prefetch candidate delta recursively to the candidate delta's signature to generate further prefetch candidates. The condence of each new prefetch candidate is a cascaded product of condences leading to the candidate's level and the candidate's stored condence value. A prefetch delta candidate whose cascaded condence value is above a threshold value triggers a prefetch.</p><p>A big advantage of a delta-based prefetcher is that every access can participate in generating prefetches. This eectively allows multiple "bites" at the "apple" (coverage) to boost performance. With low storage requirements and through the use of cascaded condence values, SPP has ne-grained control over coverage, timeliness and accuracy. As seen in Figure <ref type="figure" target="#fig_4">4</ref>, SPP outperforms both BOP and SMS in six out of our nine workload categories, as well as on average. However, there are scenarios where SPP loses out on coverage or timeliness. In pages with sparse and highly irregular access patterns, SPP cannot track all possible deltas, losing out on coverage. The deltas it tracks have low condence values, limiting the recursive prefetch distance and hence timeliness. Due to these shortcomings, SPP performs worse than either or both of the other two prefetchers in ISPEC17, Cloud and SYSMark workload categories.</p><p>Bandwidth-aware tuning opportunity. SPP uses a static condence threshold value of 25% to allow the prefetching of a candidate delta. With a simple dynamic scheme that monitors the available DRAM bandwidth headroom, we could modulate this threshold to lower values when DRAM bandwidth utilization is low. In Section 2.5 we evaluate an enhanced version of SPP (eSPP) that has the ability to lower its condence threshold value to 12.5% if more than half of the DRAM bandwidth is not utilized. We observe that even eSPP shows poor performance scaling with higher memory bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Best Oset Prefetcher (BOP)</head><p>BOP <ref type="bibr" target="#b59">[62]</ref> is a delta-based prefetcher that aims to determine the set of optimal "global" deltas between accesses within a memory region (e.g., 4KB). For example, if a program experiences a repeating series of successive local deltas (1,2,1,2,1,2...), BOP identies a single global delta of 3 (or its multiples) as an eective representation of address access patterns in the program. Further, BOP tracks and utilizes the most appropriate global delta to achieve timeliness (a multiple of 3 in this example).</p><p>Unlike SPP, which constructs a local view of accesses, BOP constructs a global view of accesses that helps BOP in two ways. First, the global view exposes more patterns in a memory region than a restricted local view of consecutive accesses. This especially helps BOP to predict future accesses in workloads with irregular access patterns with only few accesses per page. Second, the global view is robust against program access reordering, which can further disrupt the pattern learning based on a restricted local view of accesses. As seen in Figure <ref type="figure" target="#fig_4">4</ref>, BOP, at a prefetch degree of two, has the highest performance among all prefetchers in the ISPEC17 workload category. However, BOP learns only a limited set of global deltas for all access streams in the program in a statically-dened epoch (dened by the number of accesses). This severely limits BOP's coverage and timeliness in HPC and Server workload categories.</p><p>Bandwidth-aware tuning opportunity. The original BOP proposal tracks only a limited set of possible global deltas (in a 4KB page, 126 possible deltas from -63 to +63 exist) and statically picks a single best global delta per epoch (for a prefetch degree of one). In Section 2.5, we evaluate an enhanced bandwidth-aware version of BOP (called eBOP) that can adapt to DRAM bandwidth headroom. eBOP has a default prefetch degree of one, but can dynamically increase its degree to two and four if the bandwidth headroom is more than 25% and 50%, respectively. We nd that neither BOP's nor eBOP's performance improvement scales well with additional memory bandwidth. This is because of two reasons. First, BOP's predictions suer from poor accuracy since BOP does not use any program context information as a signature to generate prefetches. Second, any limit on prefetch degree hurts BOP's coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Spatial Memory Streaming (SMS)</head><p>SMS <ref type="bibr" target="#b70">[73]</ref> tracks address accesses within a spatial region (e.g., 2KB or 4KB) as a spatial bit-pattern. It maps each region's address access pattern to a signature comprising the trigger access PC and trigger oset in the region. The trigger access is dened as the rst access to the region that adds the region to the structure that tracks recently-accessed regions. Doing so, SMS eectively exploits spatial correlations between an access from a PC and other accesses in the region. A bit-pattern representation inherently captures a global view of accesses in the region. Used in conjunction with a trigger PC based signature, SMS performs better than SPP in ISPEC17, Cloud and SYSmark workload categories (Figure <ref type="figure" target="#fig_4">4</ref>).</p><p>However, SMS makes a number of static decisions that negatively aect its overall performance. Based on a study of available workloads <ref type="bibr" target="#b70">[73]</ref>, it statically decides to track 2KB regions (rather than 4KB) for accuracy reasons, thereby limiting prefetch distance and timeliness opportunities compared to SPP and BOP. With no explicit mechanism to track the accuracy of the stored bit-patterns, SMS relies on a high degree of access ltering through the use of sophisticated signature (PC+Oset). Therefore, to increase overall coverage, SMS relies on tracking a large number of signatures, increasing its storage requirements to tens of KB. Figure <ref type="figure">5</ref> shows that reducing the number of entries of pattern history table (i.e., the table that stores the correlation between signature and bit-pattern) in SMS from a baseline 16K entries (16-way associative) at 88KB storage down to 256 entries at 3.5KB storage approximately halves SMS's average performance improvement across all of our evaluated workloads. Furthermore, the current SMS design provides no clear opportunities to dynamically tune performance based on increase in DRAM bandwidth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Cache Pollution</head><p>Inaccurate prefetches can cause pollution in on-die caches by evicting useful cache blocks. The impact of pollution can be mitigated via the use of dead-block prediction <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b45">48,</ref><ref type="bibr" target="#b50">53]</ref> and prefetch-aware replacement and insertion policies <ref type="bibr">[33-36, 46, 74, 79]</ref>. In fact, multiple generations of last-level cache replacement policies have specically targeted identifying and replacing dead blocks <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b75">78]</ref>. We do not observe signicant pollution impact of inaccurate prefetches.</p><p>We nd that the pressure on the memory bandwidth resource is the primary negative impact of inaccurate prefetches in the stateof-the-art prefetchers we examine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Performance Scaling of Prefetchers with Memory Bandwidth Scaling</head><p>Figure <ref type="figure" target="#fig_5">6</ref> shows how the performance improvement of each prefetcher scales with increased memory bandwidth. We draw three major conclusions from the gure. First, the performance improvement of none of the three prefetchers scales well with increased memory bandwidth. In other words, the benet of each prefetcher saturates as memory bandwidth increases. Second, the rate of the increase in performance improvement with increase in memory bandwidth is higher in SMS. In fact, the performance improvement of SMS matches that of SPP at higher memory bandwidth points. This shows the benet of spatial bit-pattern prefetching in the presence of higher memory bandwidth. Third, eBOP enjoys the best performance scaling with memory bandwidth due to its dynamic modulation of the prefetch degree. However, the lack of program context information and the limited prefetching degree constrains eBOP coverage and leaves signicant performance on the table. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Takeaways and Our Goal</head><p>In summary, our analysis of state-of-the-art prefetchers lead to three major takeaways:</p><p>? None of the state-of-the-art prefetchers we examine scale well in performance when higher DRAM bandwidth is available. SMS inherently lacks the ability to use available memory bandwidth in its algorithm to ne tune prefetch aggressiveness. SPP and BOP can be made bandwidth aware, yet they scale poorly in performance (as we see for eSPP and eBOP Figure <ref type="figure" target="#fig_5">6</ref>). ? A spatial bit-pattern representation anchored around a trigger access to a memory region eectively captures all deltas in the region: local (deltas between consecutive accesses) and global (deltas with respect to the trigger access). This representation exposes patterns that are otherwise obfuscated by reordering in the processor and the memory subsystem. ? Using simple bit operations like OR and AND on the recently seen access bit-patterns in a memory region, we can learn two modulated bit-patterns, one biased towards coverage and the other biased towards accuracy. We can dynamically select the appropriate bit-pattern to generate prefetches to increase prefetch coverage when memory bandwidth utilization is low, or to increase prefetch accuracy when memory bandwidth utilization is high.</p><p>Our goal is to design a spatial bit-pattern prefetcher that integrates the memory bandwidth inherently into its learning algorithm to dynamically adjust its notion of aggressiveness so that it can scale its performance improvement with higher memory bandwidth. To this end, we present the Dual Spatial Pattern Prefetcher (DSPatch), which makes use of the dual modulated spatial bit-patterns we introduced earlier to simultaneously optimize prefetch coverage and accuracy based on the memory bandwidth utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DUAL SPATIAL PATTERN PREFETCHER</head><p>In this section, we describe the Dual Spatial Pattern Prefetcher (DSPatch), a spatial bit-pattern prefetcher that learns two bit-patterns per memory region (i.e., a physical page) and associates them with a program counter (PC) based signature:</p><p>? One bit-pattern (called Co P) is biased towards higher coverage. It is calculated as a simple OR of the recently observed spatial program access bit-patterns to the physical page. The OR operation adds bits to the learnt bit-pattern and grows the bit-pattern for higher coverage, up to a certain threshold. ? The other bit-pattern (called AccP ) is biased towards higher accuracy. It is calculated as a simple AND of the coveragebiased bit-pattern (Co P) and the currently observed program access bit-pattern to the physical page. The AND operation reduces the set bits to maximize accuracy but since AccP is derived from Co P, coverage is kept in check. DSPatch mainly comprises of two hardware structures: Page Buer (PB) and Signature Pattern Table (SPT ). The purpose of PB is to record the observed spatial bit-patterns as the program accesses a physical page. The purpose of SPT is to store the two modulated spatial bit-patterns (Co P and AccP ), derived from previously-observed bit-patterns, by associating them with the trigger PC into the page. Thus, DSPatch observes program accesses per physical page, but learns overall program access patterns in a page-agnostic way by associating the spatial bit-patterns with the triggering PC signature.</p><p>SPT is looked up with the triggering PC when a new physical page is accessed, to retrieve the two modulated bit-patterns: Co P and AccP . The key goal of DSPatch is to dynamically adapt prefetching for either higher coverage or higher accuracy depending on the DRAM bandwidth utilization. Using a simple 2-bit bandwidth utilization signal broadcast from the memory controller to all the cores, DSPatch selects either the Co P (when memory bandwidth utilization is low) or the AccP (when memory bandwidth utilization is high) bit-pattern to drive the prefetching. Section 3.1 shows a high-level view of DSPatch. Section 3.2 discusses how DSPatch tracks the overall bandwidth utilization across all the cores. The subsequent sections describe the algorithm to modulate, learn and predict the spatial bit-patterns. retrieves the two Co P and AccP bit-patterns and the measure of their goodness (step 3 ). Selection logic, detailed in Section 3.6, uses the memory bandwidth utilization measure to select a bit-pattern to generate prefetch candidates (step 4 ). The selected bit-pattern is anchored (i.e., rotated) to align to the trigger access oset before issuing prefetches. On eviction from the PB (step 5 ), for each trigger (per 2KB segment), the stored bit-pattern is rst anchored (i.e., rotated) to trigger oset. Then, SPT is looked up using the stored trigger PC and the stored bit-patterns and the counters are updated as described in Section 3.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tracking Bandwidth Utilization</head><p>DSPatch tracks memory bandwidth utilization with a simple counter at the memory controller that counts the number of issued DRAM column access (CAS) commands in a time window of (4 ? tRC) cycles (where tRC is the minimum allowed time between two DRAM row activations). To include hysteresis in tracking, the counter is halved after every window. The number of channels and the width of each channel determines the peak DRAM bandwidth, as well as the peak possible number of CAS commands in each tRC window. We further bucket this counter into quartiles (25%, 50% and 75%) of peak bandwidth. Every tRC cycle, the value of the counter is compared to each of the three quartile thresholds, resulting in a two bit (2b) quantized value representing which quartile the current bandwidth utilization falls into (e.g., 3 indicates more than 75% bandwidth utilization, whereas 0 indicates less than 25% bandwidth utilization). This 2-bit quantized bandwidth utilization value is broadcast to all cores and is used as a representative of current memory bandwidth utilization in the DSPatch algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Anchored Spatial Bit-patterns</head><p>To maximize the possibility of exposing data access patterns, DSPatch uses a program access representation that is robust against reordering of accesses in the processor and the memory hierarchy. The choice of signature has a signicant impact on the design of a prefetcher. The more information a signature encodes, the more prefetch ltering is achieved and hence the higher the expected accuracy. Prior bit-pattern prefetchers use the PC of the trigger access along with the oset in the page <ref type="bibr" target="#b70">[73]</ref> or the actual page address <ref type="bibr" target="#b23">[26]</ref>. They implicitly expect high prediction accuracy and hence just store and use the last occurring bit-pattern per signature without explicitly tracking accuracy. However, this comes at the cost of extra storage since the prefetcher needs to track a large enough set of frequently occurring signatures to achieve high coverage. DSPatch uses just the PC of the trigger access to a physical page as the signature. It learns two modulated bit-patterns from the recently seen accesses to a physical page and stores the bit-patterns by associating them with a PC signature in the Signature-Pattern Table (SPT ). Upon encountering the same signature, DSPatch looks up the SPT and selects a bit-pattern to generate prefetch candidates associated with that signature. DSPatch organizes the SPT as a 256entry tagless direct-mapped structure. A simple folded-XOR hash of the PC is used to index into this structure. While this indexing can reduce storage requirements, there are associated trade-os in accuracy and coverage. The use of only PC as a program signature can result in lower accuracy while aliasing of multiple PCs into a single entry can have an unpredictable impact if we only store and use the last occurring bit-pattern. Therefore, DSPatch uses simple mechanisms (with bitwise AND and PopCount operations) to track coverage and accuracy of stored bit-patterns (as we describe in Section 3.5). Crucially, DSPatch uses two modulated bit-patterns, one biased towards coverage (through OR operations) and the other biased towards accuracy (through AND operations). This allows DSPatch to simultaneously optimize for both coverage and accuracy (see Section 3.6). We describe these components of DSPatch in the subsequent sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Quantifying Accuracy and Coverage</head><p>Figure <ref type="figure" target="#fig_8">8</ref> depicts a simple scheme for quantifying the accuracy and coverage of bit-pattern predictions for a given physical page. Pop-Count of the predicted bit-pattern gives the prefetch count (C pr ed ), whereas the PopCount of the access bit-pattern generated by the program (called the program bit-pattern) gives the total number of accesses (C r eal ). Similarly, PopCount of the bitwise AND operation between the program bit-pattern and the predicted bit-pattern gives the accurate prefetch count (C acc ). Prediction accuracy is computed as the ratio C acc /C pr ed whereas prediction coverage is computed as C acc /C r eal . Instead of computing the exact fractional value, we quantize our measure of accuracy and coverage into four quartiles via simple shift and compare operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Modulated Dual Bit-patterns:</head><p>Coverage-biased and Accuracy-biased  Coverage-biased Bit-pattern (Co P). Since an anchored bitpattern eectively captures all deltas from a trigger access, adding more deltas to increase predictions and coverage is a simple matter of setting the appropriate bits in the bit-pattern. This can be achieved via simple bitwise OR operations on the predicted bitpattern with the program bit-pattern. However, since too many repeated ORs could eventually set all bits in a bit-pattern, we limit updates to at most three OR operations. DSPatch uses a 2b saturating counter named OrCount for each Co P to track the number of OR operations. OrCount is incremented every time the OR operation adds any bits to the predicted bit-pattern. DSPatch also employs a 2b saturating counter called Measure Co P to quantify the goodness of Co P. Measure Co P is incremented in two cases: (1) if the Co P prediction accuracy is less than a threshold value (called AccT hr ) or (2) if the prefetch coverage from Co P is less than a threshold value (called Co Thr). Thus, a saturated Measure Co P value essentially indicates that prefetching with the Co P bit-pattern would either lack in prefetch accuracy or prefetch coverage, and hence Co P needs to be relearnt from scratch. DSPatch resets Co P to the current program bit-pattern when Measure Co P is saturated and either of the two following conditions are satised: (1) current memory bandwidth utilization is in the highest quartile or (2) prefetch coverage is less than 50%. We use the 50% quartile threshold value for both AccT hr and Co Thr.</p><p>Accuracy-biased Bit-pattern (AccP ). The accuracy-biased bitpattern requires retaining recurring bits in the bit-pattern, which can be achieved by an AND operation. Rather than recursive AND operations on AccP , on every update, AccP is replaced by a bitwise AND operation of the program bit-pattern and the Co P. Similar to the Measure Co P , DSPatch also uses a 2b saturating counter called Measure Acc P to quantify the goodness of AccP . Measure Acc P is incremented if AccP prediction accuracy is less than 50%, and is decremented otherwise. Thus, a saturated Measure Acc P counter value essentially indicates that prefetching with the AccP bit-pattern would lack in prefetch accuracy. DSPatch uses Measure Acc P to completely throttle down predictions when memory bandwidth utilization is high.</p><p>Bit-pattern Selection for Prefetch Generation. Figure <ref type="figure" target="#fig_10">10</ref> shows the algorithm DSPatch uses to choose between Co P and AccP for prefetch generation. When DRAM bandwidth utilization is in the highest quartile (75%), we select AccP for prefetching if Measure Acc P is not saturated. When bandwidth utilization is in the second highest quartile (between 50% and 75%), we select AccP for prefetching if Measure Co P is saturated (indicating that Co P is inaccurate) and Co P otherwise. When bandwidth utilization is less than 50%, we simply select Co P for prefetching. To minimize any pollution eect when bandwidth utilization is less than 50%, we ll the prefetched blocks at low priority in the on-die L2 cache and LLC, if Measure Co P is saturated (indicating that Co P is inaccurate). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">2KB (32b) vs 4KB (64b) Predictions and Multiple Triggers</head><p>Prior bit-pattern based prefetching proposals <ref type="bibr" target="#b34">[37,</ref><ref type="bibr" target="#b70">73]</ref> do not explicitly track accuracy and hence statically limit themselves to 2KB (32b) bit-patterns. Since DSPatch incorporates measures to track accuracy and to throttle its predictions, it can dynamically make predictions at both 2KB and 4KB memory region. Instead of using 64b bit-patterns for Co P and AccP , we split them into two 32b bit-patterns. The 2b Measure Co P and Measure Acc P counters track 2KB (32b) bit-patterns and the prefetch generation is done per 2KB (32b) segment of a 4KB page. Splitting a 64b bit-pattern into two 32b bit-patterns also enables a further benet for DSPatch: two prefetch triggers per 4KB page (one per 2KB segment). The rst (trigger) access to each 2KB segment in the 4KB page can attempt to trigger prefetches. The trigger to the rst 2KB segment is allowed to predict both 32b bit-patterns (i.e., for the full 4KB page) while the trigger to the second 2KB segment is only allowed to predict a single 32b bit-pattern (for the 2KB region relative to the trigger).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Compressing Bit-patterns to Further Reduce Storage Requirements</head><p>DSPatch uses one nal optimization to further reduce the storage overhead of the bit-patterns. We see that deltas +1 and -1 are the two most frequently occurring deltas in programs. As shown in Figure <ref type="figure" target="#fig_11">11</ref>(a), these two deltas together appear more than 50% of the time on average. Therefore, instead of storing bit-patterns with each bit representing a 64B cacheline, we store a compressed bitpattern where each bit represents two adjacent 64B cachelines. We call this compression technique 128B-granularity compression and the resultant compressed bit-pattern 128B-granularity bit-pattern.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Storage Requirements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>We evaluate DSPatch using an in-house cycle accurate simulator that models dynamically-scheduled x86 cores clocked at 4 GHz. The core micro-architectural parameters are taken from the latest Intel Skylake processor <ref type="bibr" target="#b0">[1]</ref> and are listed in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prefetchers</head><p>Our baseline conguration has a PC-based stride prefetcher in the L1 cache. We evaluate prior prefetching proposals SMS <ref type="bibr" target="#b70">[73]</ref>, BOP <ref type="bibr" target="#b59">[62]</ref> and SPP <ref type="bibr" target="#b51">[54]</ref>, as well as DSPatch, as the L2 prefetcher. Each L2 prefetcher is trained on L1 misses (both demand and prefetch misses from L1) and lls prefetched lines into the L2 cache and the LLC. We ne tune each prefetcher individually in our simulation environment to produce the best possible result. Table <ref type="table">3</ref> shows the prefetcher congurations. We also evaluate the AMPM prefetcher <ref type="bibr" target="#b40">[43]</ref> but do not show its results as it under-performs all other prefetchers in single-thread simulations.</p><p>BOP <ref type="bibr" target="#b59">[62]</ref> 256-entry RR, MaxRound=100, MaxScore=31, BadScore=1, Degree=2 (for ST), 1 (for MT) 1.3 KB SMS <ref type="bibr" target="#b70">[73]</ref> 2KB page region, 64-entry AT, 32-entry FT, 16K-entry PHT 88 KB SPP <ref type="bibr" target="#b51">[54]</ref> 256-entry ST, 512-entry PT, 8-entry GHR, 12b compressed delta path, 10b feedback 6.2KB</p><p>Table <ref type="table">3</ref>: Parameters of each evaluated prefetcher</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Workloads</head><p>We evaluate a diverse set of 75 workloads, including all benchmarks from the SPEC CPU2017 <ref type="bibr" target="#b14">[17]</ref> and the SPEC CPU2006 <ref type="bibr" target="#b13">[16]</ref> suites. These workloads span various types of real-world applications, and we categorize them into 9 classes. Table <ref type="table">4</ref> shows the workload classes along with example workloads from each class. We present the average performance of each of these classes as well as the geometric mean performance across all 75 workloads. We use both homogeneous and heterogeneous workload mixes to simulate a multi-programmed system. To construct the homogeneous workload mixes, we select each of the 42 high-MPKI workloads from our full workload set and run four copies of it, one in each core of the simulator. To construct the heterogeneous workload mixes, we randomly select four workloads from the 42 high-MPKI workloads and generate 75 heterogeneous workload mixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class</head><p>Example Workloads Client 7-zip compression and decompression [2], vp9-encoding/decoding <ref type="bibr" target="#b20">[23]</ref> Server TPC-C <ref type="bibr" target="#b19">[22]</ref>, SPECjbb 2015 <ref type="bibr" target="#b16">[19]</ref>, SPECjEnterprise2010 <ref type="bibr" target="#b17">[20]</ref>, Spark pagerank <ref type="bibr" target="#b3">[5]</ref> HPC linpack <ref type="bibr" target="#b7">[9]</ref>, NAS Parallel Benchmarks <ref type="bibr" target="#b10">[13]</ref>, PARSEC <ref type="bibr" target="#b11">[14]</ref>, SPEC-ACCEL <ref type="bibr" target="#b12">[15]</ref>, SPEC MPI <ref type="bibr" target="#b15">[18]</ref> FSPEC06 All benchmarks. e.g., sphinx3, soplex, GemsFDTD ISPEC06 All benchmarks. e.g., gcc, mcf, omnetpp FSPEC17 All benchmarks. e.g., namd, povray, lbm ISPEC17 All benchmarks. e.g., omnetpp, xalancbmk, leela Cloud Bigbench <ref type="bibr" target="#b4">[6]</ref>, Cassandra <ref type="bibr" target="#b1">[3]</ref>, Hadoop-hbase, kmeans, streaming <ref type="bibr" target="#b2">[4]</ref> SYSMark SYSmark-excel, photoshop, word, sketchup <ref type="bibr" target="#b18">[21]</ref> Table <ref type="table">4</ref>: Evaluated workload categories</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Single-thread Performance</head><p>Figure <ref type="figure" target="#fig_12">12</ref> shows the performance comparison of prior prefetchers and DSPatch, both as a standalone prefetcher and as an adjunct prefetcher to SPP. We make three major observations from by 3% on average across all workloads while requiring only less than 1/20 th of the storage requirements of SMS. The use of dual modulated bit-patterns and multiple triggers helps DSPatch outperform the traditional bit-pattern prefetching employed by SMS. Second, DSPatch performs 1% better than SPP on average while requiring only less than 2/3 rd of the storage of SPP. DSPatch's gains over SPP mainly come from the SYSmark, Cloud and ISPEC workload categories. All other workload categories showcase the benets of the ne-grained delta-based prefetching paradigm employed by SPP. Third, the combination of ne-grained delta-based prefetching of SPP and dual modulated spatial bit-pattern-based prefetching of DSPatch achieves 6% performance improvement over standalone SPP on average, outperforming every standalone prefetcher in all workload categories. This clearly makes the case for using DSPatch as a lightweight adjunct prefetcher to SPP so that we can extract the benets of both prefetching paradigms.</p><p>Figure <ref type="figure" target="#fig_13">13</ref> shows the performance line graph for the set of 42 memory-intensive single-thread workloads. On average, the combined DSPatch+SPP prefetcher outperforms the standalone SPP by 9%. DSPatch+SPP performs worse than SMS in only one TPC-C workload, which has very large code footprint. With more than 4000 trigger PCs per kilo instructions, SMS benets from its large signature storage capability of 16K entries. DSPatch+SPP outperforms standalone SPP by 26% in NPB, by 20% in BigBench and by 16% in SYSMark-excel and mcf (ISPEC06) workloads.</p><p>Figure <ref type="figure" target="#fig_15">14</ref> shows the performance of BOP and a 256-entry SMS (iso-storage with DSPatch) as adjunct prefetchers to SPP. We make two observations from Figure <ref type="figure" target="#fig_15">14</ref>. First, as an adjunct prefetcher to SPP, DSPatch provides much higher performance than BOP and SMS with similar storage requirements. Second, DSPatch+SPP outperforms BOP+SPP by 2.1%, mainly because DSPatch+SPP has higher prefetch coverage than BOP+SPP (60% vs 55%).  For comprehensiveness, we also evaluate DSPatch in conjunction with SPP and BOP together. DSPatch improves average performance by 2.6% on top of the SPP+BOP combination prefetcher on average (not shown here). This clearly indicates non-overlapping coverage opportunities between BOP and DSPatch, encouraging further optimization and research in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Scaling with Memory Bandwidth</head><p>Figure <ref type="figure" target="#fig_16">15</ref> shows how the performance improvements of dierent prefetchers scale as we scale the DRAM bandwidth from a single channel DDR4-1600 (with 12.5GBps bandwidth) to dual channel DDR4-2400 (with 38 GBps bandwidth).</p><p>Two key takeaways emerge from the data. First, the performance of DSPatch+SPP scales well with increasing memory bandwidth, growing from 6% over SPP to 10% when the memory bandwidth is doubled, when going from the single channel DDR4-2133 system to the dual channel DDR4-2133 system. Second, as an adjunct prefetcher to SPP, the performance gap between eBOP+SPP and DSPatch+SPP increases with increase in memory bandwidth headroom, growing from 2.1% in the single channel DDR4-2133 system to 5% in the dual channel DDR4-2400 system. We conclude that, DSPatch, via its fundamental design choices, is best suited to extract higher performance from higher memory bandwidth. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Impact On Coverage And Accuracy</head><p>Figure <ref type="figure" target="#fig_5">16</ref> quanties the coverage and misprediction rates of the evaluated prefetchers. On average, DSPatch+SPP has 15% higher coverage than the standalone SPP prefetcher. This comes at a 6.5% increase in the rate of mispredictions. Since DSPatch uses dual modulated patterns simultaneously optimized for both coverage and accuracy, we achieve a 2:1 ratio in the impact on coverage and accuracy: a 2% increase in coverage comes at only a 1% increase in mispredictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Multi-programmed Performance</head><p>Multiple cores competing for the DRAM bandwidth resource reduces the headroom for prefetchers to boost coverage and performance. The use of the accuracy-biased bit-pattern in DSPatch plays a crucial role in such scenarios to generate highly accurate prefetches to make the best use of the scarce DRAM bandwidth.</p><p>Figure <ref type="figure" target="#fig_1">17</ref> shows the performance improvement of all prefetchers on 42 homogeneous workload mixes. We make two observations. First, as an adjunct prefetcher to SPP, DSPatch improves performance by 5.9% over the standalone SPP. Second, even though SMS outperforms the standalone SPP by 3.2% and 2.5% in the Cloud and SYSmark workload categories, DSPatch+SPP outperforms SMS by 4% and 7.3% in these workload categories.</p><p>Figure <ref type="figure" target="#fig_17">18</ref> compares the performance improvement of all prefetchers on homogeneous and heterogeneous workload mixes for two dierent DRAM bandwidth congurations: dual channel DDR4 at  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Contribution Of Accuracy-biased Patterns</head><p>We study the performance impact and contribution of the accuracybiased predictions in the DSPatch design. In DSPatch, accuracybiased bit-patterns provide highly accurate predictions when memory bandwidth utilization is close to peak (greater than 75%  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Prefetching is an extensively studied approach to hide high memory latency with a large body of work over decades covering a wide range of algorithms and implementations. To our knowledge, this is the rst work to use two spatial bit-patterns and a memorybandwidth-driven dynamic selection of bit-patterns to achieve better scalability in performance with scaling in memory bandwidth.</p><p>We divide past prefetching solutions into three major categories: pre-computation, temporal, and non-temporal prefetchers. We compare DSPatch with each of these categories, as well as prior prefetchthrottling mechanisms and highlight how DSPatch fundamentally diers from them.</p><p>Pre-computation Prefetchers. One avor of prefetching relies on pre-computation to hide latency. Examples include runahead execution <ref type="bibr" target="#b29">[32,</ref><ref type="bibr" target="#b38">41,</ref><ref type="bibr" target="#b48">51,</ref><ref type="bibr" target="#b60">[63]</ref><ref type="bibr" target="#b61">[64]</ref><ref type="bibr" target="#b62">[65]</ref> and helper thread prefetching <ref type="bibr" target="#b25">[28,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b57">60,</ref><ref type="bibr" target="#b73">76,</ref><ref type="bibr" target="#b77">80]</ref> proposals. The use of pre-computation makes these proposals highly accurate with the ability to provide coverage even when no patterns exist in address accesses. However, pre-computation prefetchers are higher in complexity compared to light-weight hardware prefetchers that capture access patterns. DSPatch, being a traditional prefetching proposal, diers completely from pre-computation prefetchers and predicts future accesses only by learning patterns in past accesses.</p><p>Temporal Prefetchers. Temporal prefetchers including STeMS <ref type="bibr" target="#b74">[77]</ref>, ISB <ref type="bibr" target="#b42">[45]</ref> and the Domino prefetcher <ref type="bibr" target="#b22">[25]</ref> are built on the Markov prefetching <ref type="bibr" target="#b46">[49]</ref> model by tracking the temporal order of full cache-line address accesses rather than address deltas or cacheline osets in spatial regions. While tracking repeating patterns of full cacheline addresses can be quite accurate, it has multi-megabyte storage requirements, which necessitates storing meta-data in memory. DSPatch requires only 3.6 KB, which can easily t inside a core.</p><p>Non-temporal Prefetchers. Prefetchers that predict deltas or bit-patterns in a spatial region (like a 2KB or 4KB page) have significantly lower storage requirements and generally lower complexity than pre-computation or temporal prefetchers. Stream <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b47">50]</ref> and stride <ref type="bibr" target="#b35">[38]</ref> prefetchers capture simple repeating deltas. More recently, prefetching proposals that can capture more complex delta patterns like a repeating series of deltas have emerged. We further categorize these prefetchers into two broad groups.</p><p>(1) Delta-based Prefetchers. Delta-based prefetchers like VLDP <ref type="bibr" target="#b69">[72]</ref> and SPP <ref type="bibr" target="#b51">[54]</ref> use a history of address deltas (inspired by the TAGE <ref type="bibr" target="#b68">[71]</ref> branch predictor) to predict future deltas . As we discussed and evaluated, SPP uses its prefetch condence values to recursively prefetch further ahead to improve timeliness. BOP <ref type="bibr" target="#b59">[62]</ref> uses a set of global deltas to capture a repeating series of smaller deltas. We have already comprehensively compared DSPatch to both SPP and BOP proposals in this work. Another prior proposal, Kill-the-PC <ref type="bibr" target="#b52">[55]</ref> co-designs both the prefetching and the cache replacement policy to be aware of each other. However, the prefetching component of KPC (called KPC-P) is identical to SPP. Our evaluation of the prefetching component of KPC showed no signicant improvement over SPP.</p><p>(2) Bit-pattern-based Prefetchers. Bit-pattern-based prefetchers, exemplied by SMS <ref type="bibr" target="#b70">[73]</ref>, use the PC as part of their signature to predict bit-patterns. These prefetchers have higher storage requirements (of the order of many tens of KB) than delta-based prefetchers. Rotated bit-patterns were rst used by Ferdman et al. <ref type="bibr" target="#b34">[37]</ref> to eliminate the cacheline oset in the spatial region (2KB) from the signature and reduce storage requirements to around 40KB. DSPatch further reduces storage by compressing the bit-pattern where each bit represents one 128B block, instead of a single 64B cacheline. A recent work, Bingo <ref type="bibr" target="#b23">[26]</ref>, extends bit-pattern-based prefetching to use both long and short history events (again inspired by the TAGE <ref type="bibr" target="#b68">[71]</ref> branch predictor). In addition to the oset in the region along with the PC as part of the signature (a short event in their terminology), Bingo also supports a long event signature utilizing the full cacheline address. Bingo fuses these signatures into the same prediction table, enable multiple predictions from a single entry for higher coverage than SMS. However, Bingo still consumes over 100KB of area. DSPatch signicantly simplies bitpattern prefetching using a mere 3.6KB of storage with anchored bit-patterns along with mechanisms to track and boost coverage and accuracy for higher performance. DSPatch's design choices fundamentally enable good performance scaling with higher memory bandwidth as well.</p><p>Prefetch-throttling Mechanisms. Prefetcher throttling mechanisms play a crucial role in any aggressive prefetcher design. Multiple prior proposals <ref type="bibr">[31, 34-36, 42, 56, 57, 67, 69, 70, 74, 79]</ref> take prefetching metrics like coverage, accuracy and bandwidth consumption into consideration to selectively throttle or drop prefetch requests in an attempt to reduce prefetcher-induced pollution in cache capacity as well as in available memory bandwidth. DSPatch also has inherently simple mechanisms to track prefetch accuracy and coverage, along with the ability to scale performance with increase in memory bandwidth. Even so, prior prefetch-throttling proposals can be orthogonally applied to DSPatch as well to further adjust its prefetch aggressiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY</head><p>We introduce DSPatch, a new spatial bit-pattern prefetcher that uses memory bandwidth utilization inherently in its algorithm to adjust prefetch aggressiveness and provide better scaling in performance improvement with increase in memory bandwidth. DSPatch exploits two key ideas. First, it learns two spatial bit-patterns to generate prefetches in a given memory region (i.e., a physical page) by using simple logical OR and AND operations. One bit-pattern is biased towards coverage and the other bit-pattern is biased towards accuracy. Second, DSPatch dynamically selects any one bit-pattern to generate prefetches at run-time based on the memory bandwidth utilization and the coverage and accuracy of each bit-pattern. These two ideas in unison help DSPatch to achieve better scaling in performance with increase in memory bandwidth than state-of-the-art prefetchers. Our evaluations show that, using only 3.6 KB of hardware storage, DSPatch improves performance by 6%, on average across 75 single-thread workloads, over an aggressive baseline with a PC-based stride prefetcher at the L1 cache and the SPP prefetcher at the L2 cache. DSPatch's performance improvement grows from 6% to 10% when DRAM bandwidth is doubled. As memory bandwidth continues to increase with improvements in DRAM architecture and packaging, we believe that the next-generation processors will signicantly benet from DSPatch's ability to extract higher performance in the presence of higher memory bandwidth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: EFFECT OF INACCURATE PREFETCHES ON CACHE POLLUTION</head><p>To study the impact of pollution caused by inaccurate prefetches in the last level cache (LLC), we examine all load, store and prefetch requests (where prefetches are generated by an aggressive but fairly inaccurate streaming prefetcher <ref type="bibr" target="#b26">[29]</ref>) for all of our simulated workloads, using the methodology we describe in Section 4. We capture request type (load, store or prefetch), address of request, and for every level of cache that a request lls into, the victim address it evicts from that level. Inaccurate prefetches are those prefetches that do not see use by a demand request (load or store) before their eviction from on-die caches. We use the LLC victim addresses evicted by inaccurate prefetches to quantify the cache pollution caused by the prefetcher. We categorize the LLC victim addresses into three classes. Figure <ref type="figure" target="#fig_19">20</ref> illustrates the breakdown of these classes across all of our 75 single-threaded workloads for three dierent LLC sizes.</p><p>? NoReuse: LLC victim addresses that see no use by any demand within 10 million instructions of their eviction from the LLC. Such addresses are eectively already dead in the LLC at their time of eviction. Therefore, their eviction by inaccurate prefetches does not cause pollution. This class comprises a dominant 84% of all LLC victim addresses even for a small 2MB LLC. ? PrefetchedBeforeUse: LLC victim addresses that are prefetched into on-die caches before their next access by a demand.</p><p>The eviction of such LLC victim addresses by inaccurate prefetches does increase memory trac but does not not cause cache pollution. This class comprises nearly 13% of all LLC victim addresses for a 2MB LLC.</p><p>? BadPollution: LLC victim addresses whose next demand access misses in on-die caches and goes to main memory. These addresses are the true victims of pollution by inaccurate prefetches. However, this class comprises only 3% of LLC victim addresses even for a small 2MB LLC.  We conclude that the pollution impact of inaccurate prefetches is relatively small in the congurations and the workloads we examine.</p><p>Multiple prior works <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b41">44,</ref><ref type="bibr" target="#b43">46,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b67">70,</ref><ref type="bibr" target="#b71">74,</ref><ref type="bibr" target="#b76">79</ref>] also report similar observations on the abundance of dead cache lines in LLC in the presence of an aggressive prefetcher. These works either determine the ll/replacement priority of a prefetched cache line based on prefetcher accuracy or design dead-block predictors to identify dead lines as high-priority replacement candidates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Prefetcher performance scaling with DRAM bandwidth (points correspond to single and dual channels of DDR4-1600, 2133 and 2400)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Multiple dierent address access streams in a single memory region can be represented by a single spatial bitpattern, anchored to their respective trigger accesses.</figDesc><graphic url="image-1.png" coords="2,62.36,415.69,221.73,85.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of two modulated bit-patterns that DSPatch takes advantage ofWe present the Dual Spatial Pattern Prefetcher (DSPatch), a lightweight spatial prefetcher that can be used as a standalone prefetcher or as a light-weight adjunct spatial prefetcher to the state-of-theart delta-based Signature Pattern Prefetcher (SPP). DSPatch builds on an intuitive use of simple logical OR and AND operations to learn two modulated spatial bit-pattern representations of accesses to a given memory region (i.e., a physical page). One bit-pattern is biased towards coverage and the other bit-pattern is biased towards accuracy. DSPatch employs a simple but eective method to track coverage and accuracy characteristics of each modulated bitpattern as well as the overall DRAM bandwidth utilization. Based on this tracking information, DSPatch dynamically selects one bitpattern to generate prefetches. If the DRAM bandwidth utilization is high, DSPatch selects the accuracy-biased bit-pattern for prefetching. If the DRAM bandwidth utilization is low, DSPatch selects the coverage-biased bit-pattern if the bit-pattern has good enough accuracy, or the accuracy-biased bit-pattern otherwise.We make the following key contributions in this work:? We observe that even though peak DRAM bandwidth is growing with newer DRAM architectures and packages, state-ofthe-art prefetcher performance does not scale well with the growing DRAM bandwidth. ? We show that a spatial bit-pattern representation anchored around a trigger access to a region can eectively capture all deltas (local and global) from the trigger. This transformation exposes similar patterns that are otherwise obfuscated to look dierent due to memory access reordering in the processor and the memory subsystem. ? We introduce a new prefetching algorithm that learns two modulated bit-patterns to prefetch in a given memory region by using simple logical OR and AND operations. One bitpattern is biased towards coverage and the other is biased towards accuracy. ? We propose a simple method to track DRAM bandwidth utilization and to measure the coverage and accuracy of</figDesc><graphic url="image-2.png" coords="2,309.89,228.34,221.68,87.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of BOP, SMS and SPP L2 prefetchers over a baseline with an L1 PC-stride prefetcher and a single channel of DDR4-2133</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: None of the ve state-of-the-art prefetchers we examine, including eSPP and eBOP, scale well in performance with higher DRAM bandwidth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 Figure 7 :</head><label>77</label><figDesc>Figure 7 depicts the overall architecture of DSPatch. Page Buer (PB) and Signature Prediction Table (SPT ) are the two prime structures of DSPatch. Each PB entry tracks accesses in a 4KB physical page and accumulates L1 misses in the page's stored bit-pattern (step 1 ).The rst access (step 2 ) to each 2KB segment in the 4KB physical page is eligible to trigger prefetches. The PC of this trigger access is stored in the PB entry and used to index into the SPT , which</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 2</head><label>2</label><figDesc>motivates the use of spatial bit-patterns anchored to the trigger (i.e., rst) access to a memory region to capture all local and global deltas from the trigger. DSPatch uses such anchored bitpatterns to represent and predict program address access patterns to a given memory region. Our implementation of DSPatch employs a Page Buer (PB) that tracks the 64 most-recently-accessed 4KB physical pages at the L2 cache level. Each PB entry stores a 64b bitpattern that accumulates the L2 cache block addresses referenced by the program loads and stores in the page.3.4 The Choice of Signature andSignature-Pattern Mapping</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>AFigure 8 :</head><label>8</label><figDesc>Figure 8: Prediction accuracy and coverage can be measured by simple bitwise AND and PopCount operations bandwidth utilization. To this end, DSPatch stores two modulated bit-patterns per SPT entry, one is biased towards coverage (Co P) and the other is biased towards accuracy (AccP ), as shown in Figure 9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Two modulated spatial bit-patterns that can simultaneously optimize for both coverage and accuracy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Selection of Co P versus AccP bit-patterns for prefetching based on DRAM bandwidth utilization and measures of the goodness of predictions</figDesc><graphic url="image-3.png" coords="7,63.36,329.56,221.59,52.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: (a) +1 and -1 are the two most frequently occurring deltas, occurring more than 60% of time across all workloads. (b) 128B-granularity compression induces no mispredictions 42% of the time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure12shows the performance comparison of prior prefetchers and DSPatch, both as a standalone prefetcher and as an adjunct prefetcher to SPP. We make three major observations from Figure12. First, as a standalone prefetcher, DSPatch outperforms SMS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Performance line graph for 42 memory-intensive single-thread workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Performance of BOP, 256 entry SMS and DSPatch as adjunct prefetchers to SPP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Performance scaling with DRAM bandwidth</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Performance improvement of prefetchers on homogeneous and heterogeneous multi-programmed workload mixes for two dierent DRAM bandwidths</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Performance improvement of the full-blown DSPatch versus two other DSPatch variants that do not use the accuracy-biased bit-pattern. The y-axis starts at 10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: Breakdown of the types of cache pollution caused by inaccurate prefetches. The y-axis starts at 75%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>128B-granularity compression halves DSPatch's pattern storage requirements. While this compression technique could theoretically have up-to 50% inaccuracy in predictions, we observe less than one misprediction for every ve cacheline predictions (20% inaccuracy). Figure11(b)shows the distribution of misprediction rate caused by 128B-granularity compression across all of our evaluated single-threaded workloads. As we can see from the gure, 128Bgranularity compression incurs no mispredictions 42% of the time, across all workloads. This also means, 128B-granularity compression is able to represent the exact bit-pattern by consuming only half of the storage in 42% of the time. In fact, for 70% of the time, the misprediction rate caused by 128B-granularity compression is lower than 25%.</figDesc><table><row><cell></cell><cell>100%</cell><cell></cell><cell></cell></row><row><cell></cell><cell>90%</cell><cell></cell><cell cols="2">Distribution of misprediction rate due to</cell></row><row><cell>Delta Occurrence Distribution</cell><cell>20% 30% 40% 50% 60% 70% 80%</cell><cell>+1 -1 +2,+3</cell><cell cols="2">128B-granularity compression 42% 12% 16% 9% 7% 14%</cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell>Exactly 0%</cell><cell>0%-12.5%</cell><cell>12.5%-25%</cell></row><row><cell></cell><cell>0%</cell><cell></cell><cell>25%-37%</cell><cell>37%-50%</cell><cell>Exactly 50%</cell></row><row><cell></cell><cell></cell><cell>Deltas</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell>(b)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>shows that DSPatch requires only 3.6KB of storage for the conguration we evaluate in Section 5.</figDesc><table><row><cell>Structure</cell><cell>Field (#bits in each entry)</cell><cell>Entries</cell><cell>#Bits</cell></row><row><cell>PB</cell><cell>Page number (36) + Bit-pattern (64) + 2x[PC (8) + Oset (6)] = 158 bits</cell><cell>64</cell><cell>10112</cell></row><row><cell>SPT</cell><cell>CovP (32) + 2*Measur e Co P (2) + 2*ORCount (2) + AccP (32) + 2*Measur e Acc P (2) = 76 bits</cell><cell>256</cell><cell>19456</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell>3.6 KB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>DSPatch storage overhead</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>. Single-thread</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Simulation parameters</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank the anonymous reviewers for their useful feedback. We also thank all the members of <rs type="institution">Intel Processor Architecture Research Lab</rs>, especially <rs type="person">Shankar Balachandran</rs> for his immense help and constructive feedback.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">6th Generation Intel?? Processor Family</title>
		<ptr target="https://www.intel.com/content/www/us/en/processors/core/desktop-6th-gen-core-family-spec-update.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Apache Cassandra</title>
		<ptr target="https://cassandra.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Apache Hadoop</title>
		<ptr target="https://hadoop.apache.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Apache Spark TM</title>
		<ptr target="https://www.cloudera.com/products/open-source/apache-hadoop/apache-spark.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">BigBench</title>
		<ptr target="https://blog.cloudera.com/blog/2014/11/bigbench-toward-an-industry-standard-benchmark-for-big-data-analytics/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">HBM Specication</title>
		<ptr target="https://www.amd.com/Documents/High-Bandwidth-Memory-HBM.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">HMC Specication v2.1</title>
		<ptr target="http://www.hybridmemorycube.org/les/SiteDownloads/HMC-30G-VSR_HMCC_Specication_Rev2.1_20151105.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">HP-LINPACK</title>
		<ptr target="https://www.netlib.org/benchmark/hpl/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">JEDEC-DDR4</title>
		<ptr target="https://www.jedec.org/sites/default/les/docs/JESD79-4.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">LPDDR4 Specication</title>
		<ptr target="https://www.jedec.org/sites/default/les/docs/JESD209-4.pdf" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">NAS Parallel Benchmark</title>
		<ptr target="https://github.com/benchmark-subsetting/NPB3.0-omp-C" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">PARSEC</title>
		<ptr target="http://parsec.cs.princeton.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">SPEC ACCEL ?</title>
		<ptr target="https://www.spec.org/accel/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SPEC CPU</title>
		<ptr target="https://www.spec.org/cpu2006/" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">SPEC CPU 2017</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">SPEC MPI ? 2007</title>
		<ptr target="https://www.spec.org/mpi2007/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<ptr target="https://www.spec.org/jbb2015/" />
	</analytic>
	<monogr>
		<title level="j">SPECjbb ?</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">?</forename><surname>Specjenterprise</surname></persName>
		</author>
		<ptr target="https://www.spec.org/jEnterprise2010/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">SYSmark</title>
		<ptr target="https://bapco.com/products/sysmark-2014/" />
		<imprint>
			<date type="published" when="2005">2014 ver 1.5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">"</forename><surname>Tpc-C</surname></persName>
		</author>
		<ptr target="http://www.tpc.org/tpcc/detail.asp" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">VP9 Encoding</title>
		<ptr target="https://trac.mpeg.org/wiki/Encode/VP9" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Scalable Processing-in-memory Accelerator for Parallel Graph Processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domino Temporal Data Prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lot-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bingo spatial data prefetcher</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bakhshalipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakerinava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lot-Kamran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sarbazi-Azad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Understanding Latency Variation in Modern DRAM Chips: Experimental Characterization, Analysis, and Optimization</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Chappell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<title level="m">Simultaneous Subordinate Microthreading (SSMT), &quot; in ISCA</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Eective hardware-based data prefetching for highperformance processors</title>
		<author>
			<persName><forename type="first">T.-F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dynamic Speculative Precomputation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Sequential Hardware Prefetching in Shared-Memory Multiprocessors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Dahlgren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenstr?m</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPDS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving Data Cache Performance by Pre-executing Instructions Under a Cache Miss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dundas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mudge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fairness via Source Throttling: A Congurable and High-performance Fairness Substrate for Multi-core Memory Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Prefetch-aware Shared Resource Management for Multi-core Systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Coordinated control of multiple prefetchers in multi-core systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Techniques for bandwidth-ecient prefetching of linked data structures in hybrid prefetching systems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatial memory streaming with rotated patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JILP Data Prefetching Championship</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>in In 1st</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Stride Directed Prefetching in Scalar Processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Janssens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bypass and insertion algorithms for exclusive last-level caches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Subramoney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Demystifying Complex Workload-DRAM Interactions: An Experimental Study</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hajinazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Senol</forename><surname>Cali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMETRICS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Continuous runahead: Transparent hardware acceleration for memory intensive workloads</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Memory Prefetching Using Adaptive Stream Detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Access map pattern matching for data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISC</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unied Memory Optimizing Architecture: Memory Subsystem Control with a Unied Predictor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rethinking belady&apos;s algorithm to accommodate prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Web Search Using Mobile Cores: Quantifying and Mitigating the Price of Eciency</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">Janapa</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dead block replacement and bypass with a sampling predictor</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JWAC</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Prefetching using Markov predictors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grunwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Improving Direct-mapped Cache Performance by the Addition of a Small Fully-associative Cache and Prefetch Buers</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">B-fetch: Branch prediction directed prefetching for chip-multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Proling a Warehouse-scale Computer</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kanev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Darago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Moseley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Sampling dead block prediction for last-level caches</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jimenez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Path condence based lookahead prefetching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kill the Program Counter: Reconstructing Program Behavior in the Processor Cache Hierarchy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Teran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Prefetch-aware DRAM controllers</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Improving Memory Bank-level Parallelism in the Presence of Prefetching</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Narasiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Simultaneous multilayer access: Improving 3D-stacked memory bandwidth at low cost</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adaptive-latency DRAM: Optimizing DRAM timing for the common-case</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">HPCA</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tolerating Memory Latency Through Software-controlled Preexecution in Simultaneous Multithreading Processors</title>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Luk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards energy-proportional datacenter memory with mobile DRAM</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">T</forename><surname>Malladi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Nothaft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Periyathambi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kozyrakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Best-oset hardware prefetching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Techniques for ecient processing in runahead execution engines</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Ecient runahead execution: Power-ecient memory latency tolerance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Runahead Execution: An Eective Alternative to Large Instruction Windows</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>IEEE Micro</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Expert Prefetch Prediction: An Expert Predicting the Usefulness of Hardware Prefetchers</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Balachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE CAL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Performance, Energy Characterizations and Architectural Implications of An Emerging Mobile Platform Benchmark Suite-MobileBench</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pandiyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<editor>IISWC</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-F. Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Mitigating prefetcher-caused pollution using informed caching policies for prefetched blocks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yedkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kozuch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mowry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACO</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">A new case for the TAGE branch predictor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seznec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Eciently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-eciency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">The Impact of Memory Subsystem Resource Sharing on Datacenter Applications</title>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vachharajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Soa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Helper Threads via Virtual Multithreading on an Experimental Itanium?2 Processor-based Platform</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-M</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Yunus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Shen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Practical o-chip meta-data for temporal memory streaming</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">SHiP: Signature-based hit predictor for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hasenplaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">PACMan: prefetchaware cache management for high performance caching</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Steely</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Emer</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Execution-based Prediction Using Speculative Slices</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
