<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Compression With Edge-Based Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dong</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Xiaoyan</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Feng</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shipeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Ya-Qin</forename><surname>Zhang</surname></persName>
						</author>
						<title level="a" type="main">Image Compression With Edge-Based Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">05C475252F9CCC888A83137076A83083</idno>
					<idno type="DOI">10.1109/TCSVT.2007.903663</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T17:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Edge extraction</term>
					<term>image compression</term>
					<term>image inpainting</term>
					<term>structure propagation</term>
					<term>texture synthesis</term>
					<term>visual redundancy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, image compression utilizing visual redundancy is investigated. Inspired by recent advancements in image inpainting techniques, we propose an image compression framework towards visual quality rather than pixel-wise fidelity. In this framework, an original image is analyzed at the encoder side so that portions of the image are intentionally and automatically skipped. Instead, some information is extracted from these skipped regions and delivered to the decoder as assistant information in the compressed fashion. The delivered assistant information plays a key role in the proposed framework because it guides image inpainting to accurately restore these regions at the decoder side. Moreover, to fully take advantage of the assistant information, a compression-oriented edge-based inpainting algorithm is proposed for image restoration, integrating pixel-wise structure propagation and patch-wise texture synthesis. We also construct a practical system to verify the effectiveness of the compression approach in which edge map serves as assistant information and the edge extraction and region removal approaches are developed accordingly. Evaluations have been made in comparison with baseline JPEG and standard MPEG-4 AVC/H.264 intra-picture coding. Experimental results show that our system achieves up to 44% and 33% bits-savings, respectively, at similar visual quality levels. Our proposed framework is a promising exploration towards future image and video compression.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O VER THE LAST two decades, great improvements have been made in image and video compression techniques driven by a growing demand for storage and transmission of visual information. State-of-the-art JPEG2000 and MPEG-4 AVC/H.264 are two examples that significantly outperform their previous rivals in terms of coding efficiency. However, these mainstream signal-processing-based compression schemes share a common architecture, namely transform followed by entropy coding, where only the statistical redundancy among pixels is considered as the adversary of coding. Through two decades of development, it has been becoming difficult to continuously improve coding performance under such architecture. Specifically, to achieve high compression performance, more and more modes are introduced to deal with regions of different Manuscript received <ref type="bibr">August 25, 2006</ref>; revised <ref type="bibr">December 11, 2006</ref>. This paper was recommended by Associate Editor T. Nguyen.</p><p>D. Liu was with Microsoft Research Asia, Beijing 100081, China. He is now with the University of Science and Technology of China, Hefei 230027, China (e-mail: liud@mail.ustc.edu.cn).</p><p>X. Sun, F. Wu, S. Li, and Y.-Q. Zhang are with Microsoft Research Asia, Beijing 100081, China (e-mail: xysun@microsoft.com; fengwu@microsoft.com; spli@microsoft.com; yzhang@microsoft.com).</p><p>Color versions of one or more of the figures in this paper are available online at http://ieeexplore.ieee.org.</p><p>Digital Object Identifier 10.1109/TCSVT.2007.903663</p><p>properties in image and video coding. Consequently, intensive computational efforts are required to perform mode selection subject to the principle of rate-distortion optimization. At the same time, more and more memory-cost context models are utilized in entropy coding to adapt to different kinds of correlations. As a result, small improvements in coding efficiency are accomplished with great pain of increased complexity in both encoder and decoder.</p><p>Besides statistical redundancy, visual redundancy in videos and images has also been considered in several works. They are motivated by the generally accepted fact that minimizing overall pixel-wise distortion, such as mean square error (MSE), is not able to guarantee good perceptual quality of reconstructed visual objects, especially in low bit-rate scenarios. Thus, the human vision system (HVS) has been incorporated into compression schemes in <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref>, trying to remove some visual redundancy and to improve coding efficiency as well as visual quality. Moreover, attempts have been made to develop compression techniques by identifying and utilizing features within images to achieve high coding efficiency. These kinds of coding approaches are categorized as "second-generation" techniques in <ref type="bibr" target="#b2">[3]</ref>, and have raised a lot of interest due to the potential of high compression performance. Nevertheless, taking the segmentation-based coding method as an example, the development of these coding schemes is greatly influenced by the availability as well as effectiveness of appropriate image analysis algorithms, such as edge detection, segmentation, and texture modeling tools.</p><p>Recently, technologies in computer vision as well as computer graphics have shown remarkable progress in hallucinating pictures of good perceptual quality. Indeed, advancements in structure/texture analysis <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and synthesis are leading to promising efforts to exploit visual redundancy. So far, attractive results have been achieved by newly presented texture synthesis techniques to generate regions of homogeneous textures from their surroundings <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Furthermore, various image inpainting methods have been presented, aiming to fill-in missing data in more general regions of an image in a visually plausible way. In fact, the word inpainting was initially invented by museum or art restoration workers. It is first introduced into digital image processing by Bertalmio et al. <ref type="bibr" target="#b14">[15]</ref>, where a third order partial differential equation (PDE) model is used to recover missing regions by smoothly propagating information from the surrounding areas in isophote directions. Subsequently, more models are introduced and investigated in image inpainting, e.g., total variation (TV) model <ref type="bibr" target="#b15">[16]</ref>, coupled second order PDE model taking into account the gradient orientations <ref type="bibr" target="#b16">[17]</ref>, curvature-driven diffusion (CDD) model <ref type="bibr" target="#b17">[18]</ref>, and so on. All these approaches work at pixel level and are good at recovering small flaws and thin structures. Additionally, exemplar-based approaches have been proposed to generate textural coarseness; by augmenting texture synthesis with certain automatic guidance, edge sharpness and structure continuity can also be preserved <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. Combining PDE diffusion and exemplar-based synthesis presents more encouraging inpainting results in <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Moreover, inpainting capability is further improved by simple human interactions when human knowledge is borrowed to imagine what unknown regions should be, so that the restoration results look natural to viewers <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>.</p><p>Due to its potential in image recovery, image inpainting likewise provides current transform-based coding schemes another way to utilize visual redundancy in addition to those that have been done in <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b2">[3]</ref>. This inference has been successfully exemplified in error concealment when compressed visual data is transmitted over error-prone channels <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Moreover, it has been reported that improvement is achieved by employing image inpainting techniques in image compression even though in a straightforward fashion <ref type="bibr" target="#b25">[26]</ref>. Besides, image compression also brings new opportunities to image inpainting, as we have pointed out in <ref type="bibr" target="#b31">[32]</ref>. Since the complete source images are available, many kinds of assistant information can be extracted to help inpainting deal with complex regions that contain structures or other features and which are unable to be properly inferred from the surroundings. Thus, inpainting here becomes a guided optimization for visual quality instead of a blind optimization for image restoration. Accordingly, new inpainting techniques may be developed to better serve image compression.</p><p>When image inpainting and image compression are jointly considered in an integrated coding system, two main problems need to be addressed. The first: What should be extracted from a source image as assistant information to represent important visual information? The second: How to reconstruct an image with this assistant information? On the one hand, it has been reported that using different image analyzers, various kinds of assistant information can be extracted, including edge, object, sketch <ref type="bibr" target="#b4">[5]</ref>, epitome <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, and so on, to represent an image or portion of an image. Then, given a specific kind of assistant information, the corresponding restoration method should be developed to complete a desired reconstruction by making full use of it. On the other, from the compression point of view, the effectiveness of restoration methods as well as the efficiency of the compression of assistant information would also influence the choice of assistant information. Such dependency makes the problems more complicated.</p><p>In this paper, we propose an image coding framework in which currently developed vision techniques are incorporated with traditional transform-based coding methods to exploit visual redundancy in images. In this scheme, some regions are intentionally and automatically removed at the encoder and are restored naturally by image inpainting at the decoder. In addition, binary edge information consisting of lines of one-pixel width is extracted at the encoder and delivered to the decoder to help restoration. Techniques, including edge thinning and exemplar selection are proposed, and an edge-based inpainting method is presented in which distance-related structure propagation is proposed to recover salient structures, followed by texture synthesis. The basic idea of this paper has been discussed in our conference papers <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b31">[32]</ref>. However, some problems have not been investigated carefully in those papers, including questions such as why the edges of image are selected as assistant information, or how to select the exemplar blocks automatically, and so on.</p><p>The remainder of this paper is organized as follows. In Section II, we introduce the framework of our proposed coding scheme. We also discuss on the necessity and importance of the assistant edge information via image inpainting models. The key techniques proposed for our coding approach are described in Sections III and IV. Specifically, Section III shows the edge extraction and exemplar selection methods, and the edge-based image inpainting is proposed in Section IV. Section V presents experimental results in terms of bit-rate and visual quality. In Section VI, we conclude this paper and discuss future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. FRAMEWORK OF OUR PROPOSED IMAGE COMPRESSION SCHEME</head><p>As the basic idea of "encoder removes whereas decoder restores" has been mentioned in literature for image compression <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b27">[28]</ref>, we would like to point out the novelties of our proposed method here. First, in our approach, the original image is not simply partitioned into two parts: one is coded by conventional transform-based approach, and the other is skipped during encoding and restored during decoding. Instead, techniques for image partition, block removal, and restoration in our proposed scheme are carefully designed towards compression rather than straightforward adoption. Furthermore, skipped regions will not be completely dropped at the encoder side if they contain portion of information that is difficult to be properly recovered by conventional image inpainting methods. In fact, assistant information is extracted from the skipped regions to guide the restoration process and further induce new inpainting techniques.</p><p>The framework of our proposed compression scheme is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. In this scheme, an original image is first analyzed at the encoder side. The "image analysis" module automatically preserves partial image regions as exemplars and sends them to the "exemplar encoder" module for compression using conventional approaches. Meanwhile, it extracts designated information from skipped regions as assistant information and sends it to the "assistant info encoder" module. Then, the coded exemplars and coded assistant information are banded together to form final compressed data of this image. Correspondingly, at the decoder side, exemplars and assistant information are first decoded and reconstructed. Then, the regions skipped at the encoder are restored by image inpainting based on the twofold information. At the end, the restored regions are combined with the decoded exemplar regions to present the entire reconstructed image.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows a general framework of the proposed compression scheme that does not constrain which kind of assistant information should be used there. Since source image is always available at the encoder side, there are many choices of assistant information extracted from the skipped regions, e.g., semantic object, visual pattern, complete structure, simple edges, and so on. Here we start from the mathematical models in image  inpainting to discuss what on earth the assistant information should be.</p><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, suppose that we are given an image function , , where is a square region in . , depicted as the gray region in Fig. <ref type="figure" target="#fig_1">2</ref>, is an open bounded subset of with Lipschitz continuous boundary. It is just the region to be restored by image compression, image inpainting, or a combination of them. This restoration problem can be generalized as <ref type="bibr" target="#b0">(1)</ref> Here, is the original image function in , where it should satisfy for any . is a reconstruction of at decoder. is a Lagrange factor. Clearly, (1) is to find the optimal function by minimizing the joint cost consisting of reconstructed distortion and coding bits for . Thus, image compression and image inpainting can be viewed as two extreme cases of (1). Specifically, in traditional image compression, is directly coded and sent to the decoder, where many bits may be needed to represent ; whereas in image inpainting, there is no bit to represent since is inferred from . However, our proposed method, which is quite different from compression or inpainting, can be granted as a combination of them.</p><p>In typical inpainting scenarios, the restoration of is usually an ill-posed problem because information in is totally unknown. Fortunately, an image is a 2-D projection of the 3-D real world. The lost region often has similar statistic, geometric and surface reflectivity regularities as those in the surroundings. It makes the above ill-posed problem possible to be solved. Therefore, some models are introduced in image inpainting to characterize statistic, geometric and surface regularities. These models should employ generic regularities, rather than rely on a specific class of images so that model-based inpainting can be applied in generic images.</p><p>One such model, TV model, is presented in <ref type="bibr" target="#b15">[16]</ref> for image inpainting, in which the variation regularity is first introduced. Since local statistical correlation usually plays an more important role than the global one, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>, instead of is used to infer the regularities in , where is a band around . Then, the TV model is to find a function on the extended inpainting region such that it minimizes the following energy function:</p><p>(2) The first term in (2) is to measure local homogeneity of image function in the region , and the second term, called as fidelity term, is the sum of squared difference (SSD) between the reconstructed in and the original in . Equation (2) can be solved by the Euler-Lagrange method described in <ref type="bibr" target="#b15">[16]</ref>. Accordingly, TV inpainting is good at restoring homogenous regions. But, if the lost region contains rich structures, it does not work well, especially when structures are separated far apart by the lost region.</p><p>To solve it, another parameter is introduced in the inpainting model <ref type="bibr" target="#b16">[17]</ref>. Let be the vector field of normalized gradient of . is the corresponding parameter to be restored on . With the new parameter of gradient directions, the inpainting problem is posed as extending the pair of functions on to a pair of functions on . It is completed by minimizing the following function:</p><p>(3)</p><p>The first term presents smooth continuation demand on , where and are positive constants, and is a smoothing kernel. It is the integral of the divergence (in function space) of the vector field , with respect to the gradients of the smoothed . The second term is an constraint between and , where is a positive weighing factor. should be related to by trying to impose . The use of the vector field is the main point of the model given in (3). Thus, it enables image inpainting to restore missing regions by continuing both the geometric and photometric regularities of images.</p><p>However, the model in ( <ref type="formula">3</ref>) assumes that the parameter can be inferred from under a certain smooth constraint. But this assumption is not always true for nature images. Taking Fig. <ref type="figure" target="#fig_1">2</ref> as an example, the area to be restored consists of two homogenous regions divided by an edge denoted by the solid curve. The dashed curve is the inferred edge in according to (3), which is quite different from the actual one. This problem is hard to be solved in conventional inpainting scenarios even using human intelligence as proposed in <ref type="bibr" target="#b22">[23]</ref>. Therefore, in our proposed coding framework, assistant information should be used to correctly infer on . As we have discussed, is the vector field of normalized gradient and is independent from the absolute magnitudes of gradients. It contains two parts of information: where exists and what its direction is. Commonly, it can be simply represented by binary edges of one-pixel width for the purpose of efficient compression.</p><p>Consequently, edge information is selected as assistant information for image inpainting in this paper. With assistant information, we could remove more regions in an image. Thus, it greatly enhances the compression power of our method. Since edges are low-level features in image, there are some mature tools available to automatically track them in an image. Moreover, edge information is concise and easy to describe in compressed fashion. Therefore, the employment of edge information can, on the one hand, help preserving good visual quality of the reconstructed image. On the other, it enables high compression performance by removing some structural regions and efficiently coding edge information.</p><p>Accordingly, an overview of our approach is exemplified in Fig. will be coded into bit-stream. Then, at the decoder side, the edge information is utilized to guide the structure propagation for the recovery of edge-related regions. The corresponding result is given in (d). The remainder unknown regions will be restored by texture synthesis. The final reconstructed image after region combination is given in (e).</p><p>In the following two sections, we will explain the modules in our framework in detail, especially on the two most important modules, namely image analysis and assisted image inpainting. Here, we would like to emphasize that the introduction of assistant edge information raises different demands on both the encoder and decoder. We deal with them comprehensively in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EDGE EXTRACTION AND EXEMPLAR SELECTION</head><p>The image analysis module at the encoder side consists of two sub-modules: The first is to extract edge information from image and the second is to select exemplar and skipped regions at block level according to available edge information. They are discussed in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Edge Extraction</head><p>As discussed in Section II, edge information plays an important role in the proposed coding scheme. It assists the encoder to select exemplar and skipped regions and the decoder to restore skipped regions with our proposed edge-based inpainting. Extracted edges do not need to represent complete and continuous topological properties of an image because our purpose is not to segment or restore an object. Discontinuous edges can likewise play the role of assistant information in the proposed scheme. But taking the topological properties into account in edge extraction will make edges more meaningful in terms of low-level vision.</p><p>Therefore, though there are many mature tools available to extract edges from images, the topology-based algorithm presented in <ref type="bibr" target="#b32">[33]</ref> is adopted in our system to extract assistant information. The algorithm presents good results especially on extracting intersection edges. According to this method, an input image is first smoothed by a two-dimensional isotropic Gaussian filter so as to avoid noise. Second, and are calculated on the filtered image for each pixel . If is the local maximum gradient along the direction and larger than a threshold, then pixel belongs to an edge. At last, the pixels with non-maximum gradients are checked by spatially-adapted thresholds to prevent missing edges caused by the unreliable estimation of .</p><p>As shown in Fig. <ref type="figure" target="#fig_4">4</ref>(a) by blue curves, edges extracted with the above algorithm (or most of the existing methods as well) are often of more than one-pixel width. This causes ambiguous directions in guiding the restoration at the decoder side and also increases the number of bits to code the edge information. Although <ref type="bibr" target="#b32">[33]</ref> also proposes a thinning method, it does not satisfy the special requirement in our proposed edge-based inpainting. It is because that pixel values on edges are not coded but rather inferred from connected surrounding edges in our proposed scheme. Thus, a new thinning method is proposed here by taking into account the consistence of pixel values on edges as well as the smoothness of edges.</p><p>Here, we present the details of our proposed thinning method. Given the detected edge pixels, we first group them into eight connective links and each edge-link (also known as a connected component in the graph that is made up by edge pixels) is thinned independently. Complying with the terminologies defined in Section II, our goal is to find a one-pixel-width line which contains pixels, i.e., for , yielding the minimal energy <ref type="bibr" target="#b3">(4)</ref> where , and are positive weighting factors. The energy function (4) consists of three terms. The first term is the Laplacian of each edge pixel. The second term is the constraint on pixel values of all edge pixels. After thinning, remaining edge pixels should have similar values. To make this constraint as simple as possible, only the difference among eight neighboring pixels are considered, and the function is defined as if otherwise (5) denotes the 8-neighbor of . The last term of (4) evaluates the curvature of the edge at each pixel. Similar to <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, is defined as</p><formula xml:id="formula_0">(6)</formula><p>In addition, we want to emphasize that the thinning process should not shorten the edge, thus only redundant pixels on the edge can be removed.</p><p>The optimal thinning solution for each edge-link is obtained through dynamic programming algorithm. Given a start point of each edge-link, the energies of all possible paths, linked in eight connective manner, are calculated according to (4). Referring to the width of the initial edge-link, several paths with smaller energies are recorded in the dynamic programming. Then, each recorded path is extended consequently by adding one neighbor pixel which results in the minimal energy. Note that the thinning algorithm can be performed in parallel manner for all edge-links in an image, because they are independent in terms of thinning process. Fig. <ref type="figure" target="#fig_4">4</ref>(b) presents the corresponding thinning results using our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Exemplar Selection</head><p>After edges are extracted, exemplar selection is performed based on these available edges. Here, for simplicity, the exemplar selection process is performed at block level. Specifically, an input image is first partitioned into non-overlapped 8 8 blocks, and each block is classified as structural or textural according to its distance from edges. In detail, if more than one-fourth of pixels in a block are within a short distance (e.g., five-pixel) from edges, it is regarded as a structural block, otherwise a textural one. Then, different mechanisms are used to select the exemplars for textural blocks and structural blocks. Blocks that are not selected as exemplars will be skipped during encoding. Moreover, exemplar blocks are further classified into two types, the necessary ones and the additional ones, based on their impacts on inpainting as well as on visual fidelity. Generally, one image can not be properly restored without necessary exemplar blocks, whereas additional blocks help to further improve visual quality.</p><p>1) Textural Exemplar Selection: Fig. <ref type="figure" target="#fig_5">5</ref>(a) illustrates the process of exemplar selection for textural blocks. In this figure, edge information is denoted by thickened lines, based on which the image is separated into structural regions (indicated by gray blocks) and textural regions (indicated by white and black blocks).</p><p>It is generally accepted that pure textures can be satisfactorily generated even given a small sample. However, in practice, image regions are often not pure textures, but rather contain kinds of local variations, such as lighting, shading, and gradual changing. Furthermore, exemplar-based texture synthesis is sensitive to the chosen samples. In image inpainting, a common solution to unknown textural regions is to synthesize them from samples in their neighborhood.</p><p>In our scheme, the necessary textural exemplars are selected in the border of textural regions. That is, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>(a), denoted by white blocks, if a textural block is next to a structural one, along either horizontal or vertical direction, it is considered as necessary. Such blocks are selected because they contain the information of transitions between different textural regions, which are hard to be restored by inner samples. Besides, propagation of these blocks, from outer to inner, can reconstruct the related textural regions.</p><p>To further improve visual quality of reconstructed images, additional blocks can be progressively selected to enrich exemplars. In this process, we consider additional blocks as representatives of local variations. On the one hand, if a block contains obvious variation, it should be preserved in advance. On the other, because the variation is a local feature, removing large-scale regions should be avoided in exemplar selection. Thus, each non-necessary textural block is related to a variation parameter defined as <ref type="bibr" target="#b6">(7)</ref> Here and are positive weighting factors. indicates 4-neighbor of a certain block. The functions and are the variance and mean value of the pixel values in a block, respectively. In our system, according to an input ratio, the blocks with higher variation parameters will be selected, during which we also check the connective degree of each block so that the removed blocks do not constitute a large region.</p><p>2) Structural Exemplar Selection: Fig. <ref type="figure" target="#fig_5">5</ref>(b) shows the exemplar selection method for structural blocks. In this figure, edges are represented by lines with indicated different types, and structural regions are indicated in white and black blocks, whereas all textural regions in gray.</p><p>As we have discussed, besides many textural blocks, some structural blocks are also skipped at the encoder side and restored at the decoder side by the guidance of edge information. Therefore, necessary and additional structural exemplars are also selected based on available edges. To better introduce the method, edges are categorized into four types according to their topological properties, as indicated in Fig. <ref type="figure" target="#fig_5">5</ref>(b): "isolated" edge traces from a free end (i.e., an edge pixel connected with only one other edge pixel) to another free end; "branch" edge traces from a free end to a conjunction (i.e., an edge pixel connected with more than three other edge pixels); "bridge" edge connects two conjunctions; and, "circle" edge gives a loop trace.</p><p>Commonly, edge acts as the boundary of different region partitions. For the sake of visual quality, in image inpainting, two textural partitions along both sides of an edge should be restored independently. The tough problem here is how to restore the transition between two partitions. We may use a model to interpolate the transition from textures of two partitions, but usually the results look very artificial and unnatural. Therefore, the blocks containing the neighborhood of free ends should be selected as exemplar so that the transitions of textural partitions can be restored by propagating information in these blocks along the edges. Conjunction blocks of edges are also selected as exemplar for similar reason because there are transitions among more than three textural regions. For circle edges, a circle completely divides the image into two partitions-inner part and outer part-so we choose two blocks as necessary exemplars, which contain the most pixels belonging to inner region and outer region of a circle edge, respectively. In a few words, by necessary exemplars, we provide not only samples for different textures separated by an edge, but also the information of the transitions between these textures, and thus the decoder is able to restore the structural regions.</p><p>Additional structural blocks can also be selected as exemplars to further improve visual quality. Given an input ratio, the process is quite similar to that for textural blocks. Each non-necessary structural block is also related to a variation parameter, which can be calculated by <ref type="bibr" target="#b6">(7)</ref>. Here, the different partitions separated by the edges are independently considered in calculating the mean value as well as the variance, and resulting parameters of different partitions are summed up to get the total variation parameter of a block. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EDGE-BASED IMAGE INPAINTING</head><p>Based on the received edges and exemplars, we propose an edge-based image inpainting method to recover the non-exemplar regions at the decoder side. Different from the encoder, the inpainting algorithm is not block-wise but rather designed to deal with arbitrary-shaped regions. Still, the non-exemplar regions are classified into structures and textures according to their distances to the edge as the encoder. Generally, structures are propagated first, followed by texture synthesis [as shown in Fig. <ref type="figure" target="#fig_2">3(d)</ref> and<ref type="figure">(e)</ref>]. A confidence map, similar to that in <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, is constructed to guide the order of structure well as texture synthesis. Specifically, at the very beginning, known pixels (pixels in decoded exemplars) are marked with confidence 1 and unknown pixels (pixels in removed blocks) are marked with confidence 0. Afterwards, each generated pixel is related with a confidence value between 0 and 1 during the inpainting process. Besides, known pixels as well as generated pixels are all called "available" ones in this section.</p><p>In the literature, exemplar-based inpainting methods can be roughly classified into two types, i.e., pixel-wise schemes and patch-wise schemes. Pixel-wise methods are suitable for restoration of small gaps, but may introduce blurring effects or ruin texture pattern while dealing with large areas. Patch-wise methods, on the contrary, are good at keeping texture pattern, but may introduce seams between different patches, which are quite annoying. In our scheme, these two strategies are adapted for different circumstances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Structure Propagation</head><p>A sketch map of structure propagation is shown in Fig. <ref type="figure" target="#fig_6">6</ref>. The gray block in Fig. <ref type="figure" target="#fig_6">6</ref> indicates an unknown structural block; the black curve with circle points represents an edge piece and related pixels; and four dash-dot lines restrict a region, namely influencing region, including unknown pixels within a short distance (e.g., ten-pixel) from the edge. Notice that it is the edge piece together with the influencing region, rather than a structural block, is treated as a basic unit in the structure propagation. Since the free ends and conjunctions of edges are all selected as exemplars, the textural regions along an edge can be readily divided and independently generated in inpainting process.</p><p>To recover a basic unit, the unknown pixels belonging to the edge piece are firstly generated. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a), the unknown pixels (denoted by black points) are generated from the known pixels (indicated by white points) using linear interpolation, i.e., (8a)</p><formula xml:id="formula_1">where if is known otherwise (8b)</formula><p>where, similar to (4), gives the number of pixels in this edge piece and and index different pixels.</p><p>After the edge restoration, neighboring structure as well as texture within the influencing region will be filled-in with regard to the recovered edge. The inpainting method for completion of influencing region is designed concerning the following facts. First, pixel-wise approach is preferred since narrow regions along edge pieces are to be handled. Second, edges are expressed by one-pixel-width curves, which can be quite different in geometric shapes among exemplar and non-exemplar regions, so we have to wrap the edges to reconstruct the unknown structure. Finally, the widths of structures are local variant, which means that it is hard to tell the exact boundary between structure and texture in an influencing region. Therefore, in our scheme, each pixel in the influencing region will have two candidates: one is treated as a structural pixel to be propagated parallel along the edge; the other is regarded as a textural pixel to be generated from the neighboring available pixels. Then, the one that makes a smooth transition from structure to texture will be selected to fill-in the unknown pixel. Moreover, as the decision making on candidate pixels is highly relevant to its available neighbors, the order for pixel completion is another important issue that should be considered. Thus, we also construct a confidence map, as mentioned at the beginning of this section, to control the generation order. For the unknown pixel, the higher the neighboring confidence is, the earlier it will be generated.</p><p>Accordingly, the recovery of influencing region is performed as follows. Here, unknown pixels to be recovered in the influencing region are called target pixels. They are denoted by black points in Fig. <ref type="figure" target="#fig_6">6(b)</ref>. For each target pixel, two candidate pixels are searched out from the surrounding available pixels. The structural candidate (S-candidate) of the target pixel, which lies within the influencing region, is indicated by horizontal striped point in Fig. <ref type="figure" target="#fig_6">6(b)</ref>; whereas the textural candidate (T-candidate) of the target pixel is denoted by vertical striped point, which locates within a short distance from the target pixel spite whether it is within the influencing region or not.</p><p>A pair method, similar to that in <ref type="bibr" target="#b7">[8]</ref>, is utilized to generate both the S-candidate and the T-candidate. As illustrated in Fig. <ref type="figure" target="#fig_7">7</ref>, for each assistant pixel, also known as any available pixel belonging to the 8-adjacent neighborhood of the target pixel, we will search for its match pixel(s) with the most similar value to it. Then, complying with the spatial relation between the assistant pixel and the target one, a pixel adjacent to a match pixel in the same relative spatial position is selected as a source pixel. As indicated in Fig. <ref type="figure" target="#fig_7">7</ref>, an assistant pixel may correspond to several match pixels and gives several source pixels; meanwhile, several assistant pixels in 8-adjacent neighborhood may generate the same source pixel, as well.</p><p>After obtaining several source pixels, we propose to use a weighted-SSD (sum of squared difference) criterion to choose the S-candidate, as given in <ref type="bibr" target="#b8">(9)</ref> where and are corresponding, the th pixel in the neighborhood of the S-candidate and the target pixel, respectively, and indicates the distance from each pixel to the edge, , as used before, is the reconstructed image. By minimizing (9), we can find the S-candidate from the obtained source pixels, which is situated in a similar relative position to that of the target pixel with respect to the edge, thus ensure the parallel diffusion of structural information.</p><p>Differently, since no direction information involved in textural region, only the ordinary SSD between the neighborhood of source pixels and target pixel is considered as the criterion to choose the T-candidate, <ref type="bibr" target="#b9">(10)</ref> Similar to that in <ref type="bibr" target="#b8">(9)</ref>, here represents the th pixel in the neighborhood of the T-candidate. Thus, the source pixel that has the most similar neighboring values to the target one will be selected as the T-candidate.</p><p>In fact, the two diffusions, or S-candidate selection and T-candidate selection, are simultaneous and competitive. These two candidates have to compete with each other and only one of them will be chosen to fill-in the target pixel. Normally, if target pixel nears edge, the choice will bias to the S-candidate. In addition, it can be observed that long-distant parallel diffusion of structural information often leads to blurring artifacts. Thus, the determination is made by comparing and which are defined in <ref type="bibr" target="#b9">(10)</ref> and <ref type="bibr" target="#b10">(11)</ref>, respectively <ref type="bibr" target="#b10">(11)</ref> Here and are constants and stands for the distance from the target pixel to the edge and indicates the distance from the target pixel to the S-candidate, as shown in Fig. <ref type="figure" target="#fig_6">6(b)</ref>. If is less than , then the T-candidate is chosen to fill-in the target pixel, otherwise the S-candidate is selected. In this way, all unknown pixels within the influencing region of an edge are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Texture Synthesis</head><p>The edges as well as their influencing regions are readily restored by structure propagation. Then, in this subsection, the remainder unknown regions are treated as textural regions, so texture synthesis is employed to fill-in these holes.</p><p>For textural regions, we prefer patch-wise algorithms because they are good at preserving large-scale texture pattern. We choose square patches as the fundamental elements while a confidence map is introduced to guide the order of synthesis. Unknown textural regions are progressively restored during texture synthesis by first reconstructing the prior patches and then the others that remain. The priority of a patch is determined by calculation of confidence and the distance from the edge. As shown in Fig. <ref type="figure" target="#fig_8">8</ref>, for each patch centered at a marginal pixel of unknown regions (denoted by target patch), we calculate the average confidence value of all pixels in this patch, as well as the average distance of all pixels from the edge. Then the patch with the highest confidence rating and the greatest distance from the edge will be synthesized first.</p><p>Afterwards, a source patch, which is most similar to the target patch, will be searched out from the neighborhood of the target patch. Here, the similarity of two patches is measured by the SSD of pixel values between overlapped available pixels of two patches. A patch that results in the least SSD will be chosen as the source patch. Notice that the filling-in process is not as simple as copy-paste work, we have to deal with overlapped regions as well as seams. In our algorithm, the graph-cut method proposed in <ref type="bibr" target="#b11">[12]</ref> is used to merge the source patch into the existing image, and the Poisson editing <ref type="bibr" target="#b33">[34]</ref> is utilized to erase the seams. After one patch is restored, the confidence map is updated. All newly recovered pixels are treated as available pixels in the following synthesis steps. Then, the next target patch is searched and processed until no unknown pixel exists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation</head><p>Our presented approach can be integrated with the state-ofthe-art coding schemes to enhance compression performance. In our experiments, two compression standards, JPEG and MPEG-4 AVC/H.264 (referred to simply as H.264 hereafter), are adopted. Thus, two fully automatic image coding systems, based on JPEG and H.264 respectively, have been constructed to evaluate the effectiveness of our proposed compression approach. In this subsection, we would like to clarify several implementation details of the systems.</p><p>First, in both systems, the one-pixel-width edge information is coded using JBIG method. Note that the edge information coded into final bit-stream is only a subset of the entire map. In other words, the edges that are fully covered by exemplar regions will not be coded [it can be observed by comparing the edges in Fig. <ref type="figure" target="#fig_4">4(b)</ref> and<ref type="figure">(f)</ref>]. Second, in the JPEG-based system, the exemplar locations are denoted at block level by a binary map, in which 1 stands for a removed block and 0 for an exemplar block, and the map is coded by an arithmetic coder. The original image is then coded by JPEG coding method, during which the removed blocks will be skipped in encoding but to be filled with the DC values copied from previous blocks, so that the DC prediction in JPEG can still be performed in exemplar block compression. Third, in the H.264-based system, since the basic coding unit is macro-block <ref type="bibr">16 16</ref>, we consider two instances: if a macro-block is totally removed, then a new macroblock type I_SKIP is coded; otherwise, the macro-block has a new element called block removal pattern (BRP) for indicating which of the four 8 8 blocks is removed, and the BRP is later coded by the arithmetic coder, too. Similar to the JPEG-based method, the exemplar blocks are coded using H.264 scheme and DC values from previous blocks are filled to the removed blocks to enable the intra prediction of H.264 scheme.</p><p>In addition, there are some predefined parameters in both encoder and decoder. To test the robustness of our system, we fix these parameters as follows for all test images. At the encoder side, the weighting factors are defined as , , and</p><p>(suggested by <ref type="bibr" target="#b16">[17]</ref>) for (4) in edge thinning, while for <ref type="bibr" target="#b6">(7)</ref> in exemplar selection. At the decoder side, structure propagation works on pixels that have less than ten-pixel distances from edges. The search range for T-candidate is 9 9, and the S-candidate is found in the entire influencing region. The search range and patch size for texture synthesis are 11 11 and 7 7, respectively. The parameters and in <ref type="bibr" target="#b10">(11)</ref> are set to 5. We would like to remark that the weighting factors for edge thinning have been carefully tuned using our test images, while the other parameters are just empirically selected with consulting the existing systems (e.g., <ref type="bibr" target="#b22">[23]</ref>). However, it should be noticed that the parameters can greatly influence the computational complexity of both encoder and decoder, which will be further analyzed in the following. At last, the only two flexible parameters in our experiments are the additional block ratios for structural exemplar and textural exemplar; actually, they act as quality control parameters in our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Test Results</head><p>We test our compression systems on a number of standard color images from the USC-SIPI image database <ref type="foot" target="#foot_0">1</ref> and the Kodak image library. <ref type="foot" target="#foot_1">2</ref> Some results are presented here to evaluate the compression ratio as well as reconstructed quality of our scheme. In all tests, the quality parameter (QP) of JPEG coding method is set to 75, while the QP of H.264 intra coding is set to 24. Bit-rate savings are listed in Table <ref type="table" target="#tab_0">I</ref>.</p><p>Fig. <ref type="figure" target="#fig_2">3</ref> shows test image Lena and corresponding results of our JPEG-based system. As mentioned before, the coded exemplars and the edge information are denoted in (c). In this test, 10% additional structural blocks as well as 50% additional textural blocks are preserved. Based on the preserved blocks and assistant edge information, our presented structure propagation gives inpainting results in (d), and the final reconstructed image after texture synthesis is shown in (e). Compared with the reconstructed image shown in (f) by baseline JPEG, our scheme saves 20% bits (as given in Table <ref type="table" target="#tab_0">I</ref>) but presents similar visual quality. More comparison results in visual quality concerning standard images can be found in Fig. <ref type="figure">9</ref>. It shows that up to 44% bits-saving (shown in Table <ref type="table" target="#tab_0">I</ref>) is achieved by our scheme at the similar visual quality levels, compared to baseline JPEG.</p><p>In Fig. <ref type="figure" target="#fig_0">10</ref>, our proposed structure propagation method is evaluated. In this test, we remove only structural blocks and use different approaches to recover them. The details of partial images together with the assistant edge information are given in the first column. Then, results generated by the PDE-based diffusion <ref type="bibr" target="#b34">[35]</ref>, which is the traditional solution to structural regions, are shown in the second column. This method works well only  <ref type="bibr" target="#b23">24)</ref> Fig. <ref type="figure">9</ref>. Comparisons with baseline JPEG on test images Jet, Milk, and Peppers. From left to right: removed blocks (black blocks) and assistant edge information (blue curves); reconstructed image by our scheme; reconstructed image by baseline JPEG. Fig. <ref type="figure" target="#fig_0">10</ref>. Comparisons of different structure propagation approaches, zoomed-in partitions. From left to right: removed blocks (black blocks) with assistant edge information (blue curves); inpainting result by PDE-diffusion <ref type="bibr" target="#b34">[35]</ref>; inpainting result by patch-wise synthesis <ref type="bibr" target="#b22">[23]</ref>; inpainting result by our scheme. Note that only our scheme takes advantage of the edges.</p><p>for smooth regions and certain simple structures. In addition, image completion method presented in <ref type="bibr" target="#b22">[23]</ref> is also tested in the third column, only the user interaction is omitted. Since no edge information is utilized, these two methods result in annoying distortion in most of the structural regions. In the last column, we give our results that are accomplished by the help of edge information. It is clearly demonstrated that the assistant edges help greatly in structure restoration, and thus empower the encoder to remove more blocks.</p><p>Furthermore, our JPEG-based system is also tested using the images in the Kodak image library, which contains photographs of natural scenes at high resolution. The comparison results on visual quality are shown in Fig. <ref type="figure" target="#fig_9">11</ref> in which the top row shows our results whereas the bottom row presents baseline JPEG results. It can be observed that the visual quality of our resulting image is very similar to that of JPEG. The bits-saving of our JPEG-based system is indicated in Table <ref type="table" target="#tab_0">I</ref>. Our method averagely saves 27% bits for the five images shown in Fig. <ref type="figure" target="#fig_9">11</ref> at the similar visual quality levels.</p><p>To investigate the detailed bit-rate cost in our scheme, we list the percentage of different coded elements in Table <ref type="table" target="#tab_1">II</ref>, from which we notice that even different images will lead to different allocations of coded elements, the exemplar location information as well as the edge information still costs only a little overhead. Commonly, the bits used to code the exemplar blocks occupy more than 90% of total bits cost. However, it is still possible to further reduce the bits cost on edge information, taking into account the exemplar locations, or skipping those edges that can be inferred from the exemplar blocks.</p><p>In Fig. <ref type="figure" target="#fig_10">12</ref> we show the reconstructed images by our H.264based system in comparison with standard H.264 intra coding. Both results show similar visual quality to each other, as in the JPEG comparisons. The bit-rate saving is also noticeable, shown in Table <ref type="table" target="#tab_0">I</ref>, but not as much as the comparison with JPEG. This is caused by two reasons. On the one hand, the H.264 intra coding is more efficient than JPEG in coding performance, so the nonexemplar blocks, especially the textural ones, will cost fewer bits in standard H.264 than in baseline JPEG, but in our scheme the edge information still cost the same bits in either realization. On the other, due to the complicated spatial predictions performed in the H.264 intra coding, the filling of only DC values for removed blocks is proved not good enough, since it breaks the original spatial relations between neighboring blocks, but for JPEG this filling of DC values seems enough since JPEG only conduct DC prediction. Nevertheless, our scheme can still acquire up to 33% bit-rate saving compared to the state-of-the-art H.264 intra coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Discussion</head><p>It can be observed that the ratio of additional textural exemplar has a big effect on visual quality of the reconstructed images. As given in Table <ref type="table" target="#tab_0">I</ref>, for homogeneous textural regions, such as the red wooden door in kodim02 [Fig. <ref type="figure" target="#fig_9">11(a)</ref>], low exemplar ratio is used to pursue high compression ratio; whereas for complex and irregular textural regions, e.g., flowers and leaves in kodim07 [Fig. <ref type="figure" target="#fig_9">11(d)]</ref>, high ratio is preferred to ensure good visual quality. However, thanks to the given edge information,  the reconstructed quality of structural regions is less sensitive to the additional structural exemplar ratio.</p><p>In addition, the improvement of our scheme in terms of compression ratio is various for different images. Commonly, the more complicated the image is, the less gain we can provide. It is because that when coding images with lots of details [such as kodim05, Fig. <ref type="figure" target="#fig_9">11(c)</ref>], the extracted edge map usually contains miscellaneous edges which makes many blocks as necessary exemplars. Thus, only a limited number of regions can be removed at encoder. However, in this case, 15% bits-saving is still provided by our JPEG-based system without noticeable visual loss, as shown in Table <ref type="table" target="#tab_0">I</ref>.</p><p>The computational complexity of our scheme is relatively higher than that of the traditional coding schemes, since at the encoder side we perform extra edge extraction and exemplar selection, and at the decoder side we add the inpainting process. In particular, the computation of the decoder is greatly related with the parameters used in the inpainting, such as search range and patch size, which determine the necessary number of SSD calculations. There are several previous work proposed to reduce the computations of SSD, so as to accelerate the image synthesis <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and those methods can be adopted in our system as well.</p><p>The visual quality assessment is highly related to our work, that is, if we have a good metric used to measure visual quality, we are able to not only better evaluate our scheme, but also further improve the performance by rate-"distortion" optimization, where "distortion" measures the perceptual quality in addition to the statistical fidelity. Unfortunately, we have not yet found such a good metric for our purposes. Thus, for visual quality comparisons in our experiments, we always set the same quality parameters for both the standard compression scheme and our inpainting-based scheme. Thus, the exemplar regions will have the same quality (both subjectively and objectively). Additionally, the restored regions still have acceptable visual quality. Accordingly, in this paper, the "comparable quality" or "similar visual quality levels" indicates visually similar qualities, which are examined by human observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we present an image compression framework that adopts inpainting techniques to remove visual redundancy inherent in natural images. In this framework, some kinds of distinctive features are extracted from images at the encoder side. Based on the obtained features, some regions of an image are skipped during encoding, only to be recovered by the assisted inpainting method at the decoder side. Due to the delivered assistant information, our presented framework is able to remove enough regions so that the compression ratio can be greatly increased. Our presented inpainting method is capable in effectively restoring the removed regions for good visual quality, as well.</p><p>Moreover, we present an automatic image compression system, in which edge information is selected as the assistant information because of its importance in preserving good visual quality. The main techniques we proposed for this compression system, i.e., edge thinning, exemplar selection and edge-based inpainting, are also addressed in this paper. Experimental results using many standard color images validate the ability of our proposed scheme in achieving higher compression ratio while preserving good visual quality. Compared to JPEG and H.264, at the similar visual quality levels, up to 44% and 33% bits-saving can be acquired by our approach, respectively. Further improvements of current scheme are still promising. First, the assistant information as well as the selected exemplars can be described and compressed into bit-stream in more compact fashion. Second, extraction of the distinctive features can be more flexible and adaptable. Besides edge information, there are other candidates, such as sketch <ref type="bibr" target="#b4">[5]</ref> and epitome <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, which could be derived from source images to assist the vision technologies and the compression methods as well. Furthermore, image inpainting is still a challenging problem when some kinds of assistant information are provided, into which we need to put more effort in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors acknowledge all anonymous reviewers for their constructive opinions that contribute to the improvement of this paper. Special thanks go to H.-Y. Shum and J. Sun for their helpful discussions, and to D. Daniels for his editing. Also, thanks go to C. Wang for his previous work that has been integrated into our current system. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The framework of our proposed image compression scheme.</figDesc><graphic coords="3,103.26,66.74,383.00,112.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of image inpainting, where the gray region is to be restored.</figDesc><graphic coords="3,78.06,220.60,171.00,92.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison with baseline JPEG on test image Lena. (a) Original image. (b) Edge map (blue curves). (c) Removed blocks (black blocks) and assistant edge information (blue curves), note that the assistant edge information is a subset of the entire edge map. (d) Reconstructed image after structure propagation. (e) Reconstructed image after texture synthesis. (f) Reconstructed image by baseline JPEG.</figDesc><graphic coords="4,155.64,66.38,282.00,205.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3. In this figure, (a) is the input original image Lena. After image analysis, an edge map [denoted by blue curves in (b)] is generated, based on which the exemplars [denoted by the nonblack blocks in (c)] are selected. Consequently, the exemplars and the needed edge information [shown as blue curves in (c)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Step-wise results of our scheme on test image Lena, zoomed-in partition. (a) Original image with detected edge pixels (blue curves). (b) Thinned edges (blue curves). (c) Chosen necessary structural blocks. (d) Chosen additional structural blocks. (e) Chosen necessary textural blocks. (f) Chosen additional textural blocks. (g) Structure propagation result. (h) Texture synthesis result.</figDesc><graphic coords="5,73.02,66.26,181.00,190.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. An example of necessary exemplar selection in which curves denote edges and black blocks denote skipped regions. (a) Textural exemplar selection in which white blocks are necessary textural exemplars; (b) structural exemplar selection in which white blocks are necessary structural exemplars, four types of edges are also distinguished in (b). Also see Fig. 4 for a practical example.</figDesc><graphic coords="6,157.08,66.46,279.00,143.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Pixel-wise structure propagation. (a) A piece of edge and its influencing region, with arrowed dash-dot lines and dash lines showing the propagation directions. (b) Restoration of influencing region in which each generated pixel is copied from one of two candidate pixels.</figDesc><graphic coords="7,121.98,67.02,346.00,183.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Pair matching in our structure propagation algorithm.</figDesc><graphic coords="8,82.44,66.98,165.00,142.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Patch-wise texture synthesis in our scheme.</figDesc><graphic coords="9,101.82,67.22,123.00,139.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Comparisons with baseline JPEG on the Kodak Image Library. (a) kodim02; (b) kodim03; (c) kodim05; (d) kodim07; (e) kodim19. The top row shows the reconstructed images by our scheme and the bottom row shows the reconstructed images by baseline JPEG.</figDesc><graphic coords="12,114.84,66.30,363.00,345.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Comparisons with standard H.264 intra picture coding. (a) Jet. (b) Lena. (c) Milk. (d) Peppers. (e) kodim11. (f) kodim20. (g) kodim23. The top row shows the reconstructed images by our scheme and the bottom row shows the reconstructed images by standard H.264 intra coding.</figDesc><graphic coords="13,119.82,66.58,350.00,344.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Dong</head><label></label><figDesc>Liu received the B.S. degree in electrical engineering from the University of Science and Technology of China (USTC), Hefei, China, in 2004. He is now pursuing the Ph.D. degree at USTC. He has been a visiting student in Microsoft Research Asia since 2005, where his research concentrates on image/video compression and compressionoriented vision technologies. He is also interested in image segmentation and image representation. Xiaoyan Sun (M'04) received the B.S., M.S., and Ph.D. degrees in computer science from Harbin Institute of Technology, Harbin, China, in 1997, 1999, and 2003, respectively. She joined Microsoft Research Asia, Beijing, China, as an Associate Researcher in 2003 and has been a Researcher since 2006. She has authored or co-authored over 30 conference and journal papers and submitted several proposals and contributed techniques to MPEG-4 and H.264. Her research interests include video/image compression, video streaming, and multimedia processing. Feng Wu (M'99-SM'06) received the B.S. degree in electrical engineering from the University of Xi'an Electrical Science and Technology, Xi'an, China, in 1992, and the M.S. and Ph.D. degrees in computer science from Harbin Institute of Technology, Harbin, China, in 1996 and in 1999, respectively. He joined Microsoft Research Asia, Beijing, China, as an Associate Researcher in 1999 and was promoted to Researcher in 2001. He has played a major role in Internet Media Group to develop scalable video coding and streaming technologies. He has authored or co-authored over 60 papers in video compression and contributed some technologies to MPEG-4 and H.264. His research interests include video and audio compression, multimedia transmission, and video segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,80.88,105.62,431.00,145.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,70.32,279.30,452.00,450.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,63.06,67.10,464.00,277.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I BIT</head><label>I</label><figDesc>-RATE SAVINGS OF OUR SCHEME COMPARED TO JPEG (QP IS SET TO 75) AND H.264 (QP IS SET TO</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERCENTAGE</head><label>II</label><figDesc>OF DIFFERENT CODED ELEMENTS IN OUR JPEG-BASED SYSTEM (QP IS SET TO 75)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://sipi.usc.edu/services/database/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://r0k.us/graphics/kodak/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Signal compression based on models of human perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Safranek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1993-10">Oct. 1993</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1385" to="1422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Locally adaptive perceptual image coding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Höntsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Karam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1472" to="1483" />
			<date type="published" when="2000-09">Sep. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Second-generation image coding: An overview</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Millar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="29" />
			<date type="published" when="1997-03">Mar. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Texture analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tuceryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Pattern Recognition and Computer Vision</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Pau</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S P</forename><surname>Wang</surname></persName>
		</editor>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="207" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a mathematical theory of primal sketch and sketchability</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV&apos;03)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1228" to="1235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV&apos;99)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast texture synthesis using tree-structured vector quantization</title>
		<author>
			<persName><forename type="first">L.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="479" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Synthesizing natural textures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ashikhmin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Symp. Interactive 3D Graphics (SI3D)</title>
		<meeting>ACM Symp. Interactive 3D Graphics (SI3D)</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time texture synthesis by patch-based sampling</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="127" to="150" />
			<date type="published" when="2001-07">Jul. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graphcut textures: Image and video synthesis using graph cuts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schödl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIG-GRAPH</title>
		<meeting>ACM SIG-GRAPH</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="277" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Parallel controllable texture synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lefebvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="777" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Texture optimization for example-based synthesis</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="795" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mathematical models for local nontexture inpaintings</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1019" to="1043" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Non-texture inpainting by curvature-driven diffusions (CDD)</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Visual Commun. Image Represen</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="436" to="449" />
			<date type="published" when="2001-12">Dec. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image repairing: Robust image synthesis by adaptive ND tensor voting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR&apos;03)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fragment-based image completion</title>
		<author>
			<persName><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">PatchWorks: Example-based region tiling for image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<idno>MSR-TR-2004-04</idno>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Microsoft Research</publisher>
			<pubPlace>Redmond, WA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image completion with structure propagation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="861" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003-08">Aug. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A combined PDE and texture synthesis approach to inpainting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Grossauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV&apos;04)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV&apos;04)</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="214" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structure and texture filling-in of missing image blocks in wireless transmission and compression applications</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="296" to="303" />
			<date type="published" when="2003-03">Mar. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Error concealment in video transmission over packet networks by a sketch-based approach</title>
		<author>
			<persName><forename type="first">L</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G B</forename><surname>De Natale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Process.: Image Commun</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="57" to="76" />
			<date type="published" when="1999-09">Sep. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Geometric-structure-based error concealment with novel applications in block-based low-bit-rate coding</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="648" to="665" />
			<date type="published" when="1999-06">Jun. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Epitomic analysis of appearance and shape</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV&apos;03)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="34" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video epitomes</title>
		<author>
			<persName><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR&apos;05)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="42" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Image compression with structure-aware inpainting</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst (ISCAS&apos;06)</title>
		<meeting>IEEE Int. Symp. Circuits Syst (ISCAS&apos;06)</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1816" to="1819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compression with vision technologies</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">presented at the Picture Coding Symp. (PCS)</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-04">Apr. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Driving vision by topology</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Rothwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mundy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V.-D</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Symp. Comput. Vis</title>
		<meeting>Int. Symp. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="395" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vector-valued image regularization with PDEs: A common framework for different applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tschumperlé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="506" to="517" />
			<date type="published" when="2005-04">Apr. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
