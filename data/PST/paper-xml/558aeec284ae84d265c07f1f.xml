<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Multi-View Reconstruction of Large-Scale Scenes using Interest Points, Delaunay Triangulation and Graph Cuts</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Labatut</surname></persName>
							<email>labatut@di.ens.fr</email>
							<affiliation key="aff0">
								<orgName type="department">CERTIS École normale supérieure École des ponts</orgName>
								<orgName type="institution">Marne-la-Vallée</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Philippe</forename><surname>Pons</surname></persName>
							<email>pons@certis.enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">CERTIS École normale supérieure École des ponts</orgName>
								<orgName type="institution">Marne-la-Vallée</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Renaud</forename><surname>Keriven</surname></persName>
							<email>keriven@certis.enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">CERTIS École normale supérieure École des ponts</orgName>
								<orgName type="institution">Marne-la-Vallée</orgName>
								<address>
									<settlement>Paris</settlement>
									<country>France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Multi-View Reconstruction of Large-Scale Scenes using Interest Points, Delaunay Triangulation and Graph Cuts</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4B738F90C69EA6FFBE4497898BC485AD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel method to reconstruct the 3D shape of a scene from several calibrated images. Our motivation is that most existing multi-view stereovision approaches require some knowledge of the scene extent and often even of its approximate geometry (e.g. visual hull). This makes these approaches mainly suited to compact objects admitting a tight enclosing box, imaged on a simple or a known background. In contrast, our approach focuses on largescale cluttered scenes under uncontrolled imaging conditions. It first generates a quasi-dense 3D point cloud of the scene by matching keypoints across images in a lenient manner, thus possibly retaining many false matches. Then it builds an adaptive tetrahedral decomposition of space by computing the 3D Delaunay triangulation of the 3D point set. Finally, it reconstructs the scene by labeling Delaunay tetrahedra as empty or occupied, thus generating a triangular mesh of the scene. A globally optimal label assignment, as regards photo-consistency of the output mesh and compatibility with the visibility of keypoints in input images, is efficiently found as a minimum cut solution in a graph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction 1.Motivation</head><p>As pointed out in the review by Seitz et al. <ref type="bibr" target="#b32">[33]</ref>, most top-performing algorithms for dense multi-view stereo reconstruction require significant knowledge of the geometry of the scene. This ranges from a tight bounding box to a closer approximation by the visual hull.</p><p>The visual hull is defined as the intersection of cones generated by the silhouettes of the objects in the input views <ref type="bibr" target="#b25">[26]</ref>. This technique requires an accurate segmentation of input images. In real-life examples, however, such segmentation is not available or even feasible. In practice, vi-sual hull computation only applies to datasets obtained under controlled imaging conditions, namely on a simple or a known background.</p><p>Despite this serious limitation, in the last few years, a number of multi-view stereovision algorithms exploiting visual hull have been proposed. They rely on visual hull either as an initial guess for further optimization <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b43">44]</ref>, as a soft constraint <ref type="bibr" target="#b11">[12]</ref> or even as a hard constraint <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> to be fulfilled by the reconstructed shape.</p><p>While the unavailability of silhouette information discards many of the top-performing multi-view stereovision algorithms, the requirement for the ability to handle largescale scenes discards most of the others, and in particular volumetric methods, i.e. methods based on a regular decomposition of the domain into elementary cells, typically voxels. Obviously, this approach is mainly suited to compact objects admitting a tight enclosing box, as its computational and memory cost quickly becomes prohibitive when the size of the domain increases.</p><p>Volumetric multi-view stereovision methods include space carving <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43]</ref>, level sets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32]</ref>, and volumetric graph cuts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. Actually, what distinguishes these three categories is the type of optimization they rely on: a greedy occupancy assignment in space carving, a surface deformation driven by a gradient descent in level sets, and a global combinatorial optimization in graph cuts.</p><p>Large-scale cluttered scenes for which no reliable initial guess of geometry is available also disqualify the deformable model framework <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b31">32]</ref>. Indeed, it is based on a local optimization by gradient descent. As a result, it is highly sensitive to initial conditions.</p><p>The multi-view stereovision methods which have proven more adapted to reconstruct large-scale scenes (e.g. outdor architectural scenes) are those representing geometry by several depth maps <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>. However, their 978-1-4244-1631-8/07/$25.00 ©2007 IEEE performance for complete reconstruction seems to be lower than previously discussed approaches, either as regards accuracy or completeness of the obtained model. This may be due to the difficulty to handle visibility globally and consistently in this approach. Moreover, in the complete reconstruction case, the several partial models of the scene have to be fused at post-processing using a volumetric technique <ref type="bibr" target="#b9">[10]</ref>.</p><p>From the above discussion, we draw the conclusion that, although very impressive progress has been made in the last few years in the multi-view stereovision problem as regards reconstruction accuracy, novel algorithms that can handle more general scenes are still needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Novelty of our approach</head><p>In this paper, we propose a novel multi-view reconstruction approach adapted to large-scale cluttered scenes under uncontrolled imaging conditions. Our method first generates a quasi-dense 3D point cloud of the scene by matching keypoints across images in a lenient manner, thus possibly retaining many false matches. Then it builds an adaptive tetrahedral decomposition of space by computing the 3D Delaunay triangulation of the 3D point set. Finally, it reconstructs the scene by labeling Delaunay tetrahedra as empty or occupied and in this way generates a triangular mesh of the scene. A globally optimal label assignment, as regards photo-consistency of the output mesh and compatibility with the visibility of keypoints in input images, is efficiently found as a minimum cut solution in a graph.</p><p>Our method shares with existing multi-view graph cuts approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref> the desirable property of yielding an exact global optimum of an energy functional. Compared to these methods, however, our approach enjoys a unique combination of desirable features:</p><p>1. It uses a fully adaptive unstructured tetrahedral decomposition of space, namely the Delaunay triangulation of a quasi-dense point sample of the surface of the scene, in constrast with a regular subdivision used in volumetric graph cuts <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b41">42]</ref>. This yields several significant benefits:</p><p>• with a dense enough sample of the surface, it alleviates quantization artifacts, namely the staircasing effect.</p><p>• like other complex-based method <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>, it allows to directly output a quality triangular mesh of the scene, free of self-intersections.</p><p>• without any knowledge of the scene geometry, it keeps the computation and memory cost sustainable on large-scale scenes, since empty space regions are represented by few large tetrahedra.</p><p>2. It exploits visibility information coming from keypoints to guide the position of the surface. As a result, it avoids the mininum cut solution from being an empty surface. Hence it exonerates from the different techniques proposed in the literature so far to solve this problem: a heuristic ballooning term <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b41">42]</ref>, a restriction of the feasible set using silhouette information <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref>, or a maximization of photo-flux <ref type="bibr" target="#b7">[8]</ref>. Moreover, this visibility information is not enforced as a hard constraint but integrated in the very optimization framework, hence yielding robustness to outliers.</p><p>3. It can handle closed as well as open scenes. For example, it can simultaneously recover the walls of an indoor scene and a complete reconstruction of objects seen from all sides in the input images.</p><p>The remainder of this paper is organized as follows. Section 2 gives some background on the different techniques needed in our approach: interest point detectors, Delaunay triangulation and graph cuts. In Section 3, we describe in detail the different steps of our multi-view stereo reconstruction algorithm. Section 4 discusses implementation aspects and presents some numerical experiments that demonstrate the potential of our approach for reconstructing largescale cluttered scenes from real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Keypoint extraction and description</head><p>Our method relies on the extraction of robust keypoints that can be matched across different viewpoints: we use the keypoint extraction and description method of Lowe <ref type="bibr" target="#b29">[30]</ref>. The first stage of the Scale-invariant feature transform (SIFT) searches for scale-space extrema in the differenceof-Gaussian function convolved with the image in order to find interest points <ref type="bibr" target="#b28">[29]</ref>. The second stage associates a descriptor (a high dimension vector) to each keypoint localization: this descriptor represents the distributions of smaller scale features in the neighbourhood of the detected point, it is invariant to scale and rotation and is robust to small affine or projective deformations and illumination changes. It has also been shown to perform among the very best descriptors <ref type="bibr" target="#b30">[31]</ref> and has become one of the most widely used descriptor in practice nowadays, justifying our choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Delaunay triangulation</head><p>The following definitions are taken from a computational geometry textbook <ref type="bibr" target="#b5">[6]</ref>. Let P = {p 1 , . . . , p n } be a set of points in R d . The Voronoi cell associated to a point p i , denoted by V (p i ), is the region of space that is closer from p i than from all other points in P:</p><formula xml:id="formula_0">V (p i ) = { p ∈ R d : ∀ j = i, p -p i ≤ p -p j } V (p i )</formula><p>is the intersection of n-1 half-spaces bounded by the bisector planes of segments [p i p j ], j = i. V (p i ) is therefore a convex polytope, possibly unbounded. The Voronoi diagram of P, denoted by Vor(P), is the partition of space induced by the Voronoi cells V (p i ).</p><p>The Delaunay triangulation Del(P) of P is defined as the geometric dual of the Voronoi diagram: there is an edge between two points p i and p j in the Delaunay triangulation if and only if their Voronoi cells V (p i ) and V (p j ) have a non-empty intersection. It yields a triangulation of P, that is to say a partition of the convex hull of P into d-dimensional simplices (i.e. into triangles in 2D, into tetrahedra in 3D, and so on). Figure <ref type="figure" target="#fig_0">1</ref> displays an example of a Voronoi diagram and its associated Delaunay triangulation in the plane.</p><p>The algorithmic complexity of the Delaunay triangulation of n points is O(n log n) in 2D, and O(n 2 ) in 3D. However, as was recently proven in <ref type="bibr" target="#b1">[2]</ref>, the complexity in 3D drops to O(n log n) when the points are distributed on a smooth surface, which is the case of interest here.</p><p>Our choice of Delaunay triangulation as a space subdivision for multi-view stereo reconstruction is motivated by the following remarkable property: under some assumptions, and especially if P is a "sufficiently dense" sample of a surface, in some sense defined in <ref type="bibr" target="#b0">[1]</ref>, then a good approximation of the surface is "contained" in Del(P), in the sense that the surface can be accurately reconstructed by selecting an adequate subset of the triangular facets of the Delaunay triangulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Energy minimization by graph cuts</head><p>Given a finite directed graph G = (V, E) with nodes V and edges E with non-negative weights (capacities), and two special vertices, the source s and the sink t, an s-t-cut C = (S, T ) is a partition of V into two disjoints sets S and T such that s ∈ S and t ∈ T . The cost of the cut is the sum of the capacity of all the edges going from S to T : c(S, T ) = (p,q)∈S×T |p→q∈E w pq . The minimum st-cut problem consists in finding a cut C with the smallest cost: the Ford-Fulkerson theorem <ref type="bibr" target="#b13">[14]</ref> states that this problem is equivalent to computing the maximum flow from the source s to the sink t and many classical algorithms exist to efficiently solve this problem. Such a cut can be viewed as a binary labeling of the nodes: by building an appropriate graph, many segmentation problems in computer vision can be solved very efficiently <ref type="bibr" target="#b18">[19]</ref>. More generally, global minimization of a whole class of energy is achievable by graph cuts <ref type="bibr" target="#b23">[24]</ref>.</p><p>Kirsanov and Gortler <ref type="bibr" target="#b21">[22]</ref> first proposed to use graph cut on complexes to globally optimize surface functionals and also developed the idea of using random sparse complexes for their flexibility over regular subdivisions: this differs from the graphs commonly used in computer vision, which are often regular grids in the input images or in the bounding volume of the scene. Our approach similarly relies on a sparse complex-based graph: this graph however directly derives from an adaptive space decomposition efficiently provided by the Delaunay triangulation. Moreover the specifics of our graph construction are quite different and tailored to the multi-view reconstruction problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Reconstruction method</head><p>Our algorithm can be decomposed in four main steps: the first step is straightforward as it reduces to extracting features from the input views. The keypoints are then matched pair-wise between different views by taking epipolar geometry into account: these matches enable the generation a quasi-dense 3D point cloud, which is later refined and structured by incrementally building a Delaunay triangulation and merging 3D points that are close enough. Finally a graph cut optimization is used to extract the surface of the scene from this triangulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quasi-dense 3D point cloud generation</head><p>The first step in our method is the generation of a quasidense 3D point cloud. To this end, pairs of keypoints are matched across different views. The usual way of obtaining robust matches is, given one keypoint, to find the best match in the other image, and to keep it provided its matching score is significantly better than the second best matching score. Here, however, as the global optimization in the final step is able to cope with false matches, we favor density over robustness and we admit a lot of false positives. To achieve this, given one keypoint, we always keep the best match along the epipolar line, plus we keep all other matches along the epipolar line whose matching scores are not significantly lower than the best match. This step out-  puts a 3D point cloud by computing the 3D position associated to each match.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Match aggregation and Delaunay triangulation</head><p>The next step in our method consists in adding some structure to the previous 3D point cloud, while efficiently aggregating matches in tuples. This is accomplished by incrementally building a Delaunay triangulation of the 3D point set. Each vertex of the triangulation does not only store its position, it also maintains the list of keypoints it originates from. Each time a candidate point from the original 3D point cloud is to be added, its nearest neighbour in the triangulation is found (this query is very efficient in a Delaunay triangulation <ref type="bibr" target="#b4">[5]</ref>) and the maximum reprojection error between the two 3D points is computed.</p><p>As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, two different cases can occur. If the maximum reprojection error is above some threshold, the candidate point is regarded as a distinct point and is inserted in the Delaunay triangulation. If the error is below the threshold, the candidate point is not inserted in the Delaunay triangulation. Instead, the nearest vertex is updated: first, the list of keypoints it originates from is complemented with the two keypoints from which the candidate point was generated, then its position is recomputed using its updated keypoint list, and the Delaunay triangulation is modified accordingly, if needed.</p><p>This step outputs a Delaunay triangulation, whose vertices store a keypoint tuple and the best-fit corresponding 3D position. Note that the size of keypoint tuples is related to the confidence of 3D points, since false matches are unlikely to aggregate into large tuples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Surface extraction</head><p>The final step in our method consists in labeling each tetrahedron of the Delaunay triangulation as inside or outside of the scene. The output triangular mesh is then obtained by taking the triangular facets between adajacent tetrahedra having different labels. This constrains the reconstructed surface to be included in the Delaunay triangulation. This is not a limitation, however, as soon as the point cloud is sufficiently dense, as discussed in Section 2.2.</p><p>A globally optimal label assignment is efficiently found using graph cuts. To this end, we consider the dual graph to the Delaunay triangulation, in other words, the graph whose vertices correspond to Delaunay tetrahedra, and whose edges correspond to the triangular facets between adajacent tetrahedra. Actually, this coincides with the vertices and edges of the Voronoi diagram of the point set. In addition, there are links between each vertex of the graph (i.e. each Delaunay tetrahedron) and the sink and the source.</p><p>In the sequel, we note S the surface to be reconstructed. As discussed above, S is a union of Delaunay triangles. We wish to minimize an energy functional composed of three terms, one dealing with visibility, one dealing with photoconsistency and one dealing with surface smoothness:</p><formula xml:id="formula_1">E(S) = E vis (S) + λ photo E photo (S) + λ area E area (S) (1)</formula><p>where λ photo and λ area are positive weights. In the rest of this section, we give the exact definition of each energy term and we describe how it can be implemented in the graph cuts framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Surface visibility</head><p>Each vertex in the triangulation keeps some visibility information: the keypoint tuple from which it was reconstructed (this tuple can contain as little as two keypoints or as many tuples as the total number of input views if the point was the result of multiple merges). This information is decisive to design the E vis (S) term: if some vertex belongs to the final surface then it should be visible in the views it comes from. Consequently, all the tetrahedra intersected by a ray emanating from the vertex to the camera center of one of these views should be labelled as outside (and the tetrahedron behind the vertex should be labelled as inside): in Figure <ref type="figure" target="#fig_3">3</ref>, according to the blue ray and vertex, the q 2 tetrahedron should be labelled as inside and all the other shown tetrahedra as outside.</p><p>The following term: E vis (S) = λ vis #{ray conflicts}, where a ray from a vertex to a camera center is in conflict if it intersects a tetrahedron labelled as inside, naturally comes to mind. Unfortunately, such energy term is not suitable for graph cut optimization, as it would require complex interactions between more than two nodes in the graph <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Instead, the number of intersections of the ray with the oriented surface will be used (only ray crossings of a triangle from the inside to the outside are to be penalized). The surface should also go through the vertex originating the ray and the last tetrahedron traversed by the ray should be labelled as outside. The construction of the corresponding visibility term for one ray is detailled in Figure <ref type="figure" target="#fig_3">3</ref> with the usual notation D p (.) for data terms and V pq (., .) for neighbouring terms. Note that the subterm V p2q2 cannot be translated to weights in the graph because the tetrahedra whose nodes are p and q do not share a triangle and the nodes p and q are thus not linked in the graph. Fortunately this term only amounts to a link to the sink with weight w q2t = λ in . The positive weights λ in , λ out and λ ∞ take into account the confidence in the reconstructed vertex yielding the ray.</p><p>The global visibility term sums all the contributions of the rays cast by all the vertices of the triangulations (the corresponding weights of the edges of the graph are accumulated the same way): it gives a complex distribution of "votes" for each tetrahedron to be inside or outside the surface and only the tetrahedra containing the cameras get a non 0-weighted link to the source.</p><p>Note that the edges for the origin and for the end of the ray (with weights λ in and λ ∞ respectively) can straightforwardly be adjusted (thanks to the Delaunay triangulation) to allow the reconstruction of, for instance, the walls of an indoor scene: not only finite tetrahedra can be used as nodes in the graph but also infinite tetrahedra (which have three vertices on the convex hull of the 3D point cloud and share an infinite vertex). . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Surface photo-consistency</head><p>The photo-consistency term E photo (S) of our energy measures how well the given surface S matches the different input images in which it is seen. It is defined as the sum over the whole surface of some photo-consistency measure ρ ≥ 0 (in our case, every triangle of the surface has a uniform photo-consistency):</p><formula xml:id="formula_2">E photo (S) = S ρ dS = T ∈S ρ(T ) A(T )<label>(2)</label></formula><p>The photo-consistency of each triangle is computed only in the views from which its three vertices were reconstructed. Furthermore, as a triangle of the surface S lies by definition on the interface between the inside and the outside of the reconstructed object(s), its orientation needs to be taken into account: an "oriented photo-consistency" is used, which means that the two possible orientations of a given triangle get different photo-consistencies, each computed only in the subset of the considered views compatible with the given orientation of the triangle. This maps pretty easily onto the graph cuts framework: for each directed pair of tetrahedra (represented by nodes p and q in the graph) which shares a triangle T with normal n (pointing from tetrahedron p to tetrahedron q), an edge p → q is added with a weight w pq = ρ {Πi| di. n&gt;0} (T ), where d i is the direction from the center of the triangle to the center of the i-th camera Π i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Surface smoothness</head><p>Surface smoothness is encouraged by minimizing the area of the surface. Hence it is the simplest term of our energy:</p><formula xml:id="formula_3">E area (S) = A(S) = S dS = T ∈S A(T )<label>(3)</label></formula><p>This is also trivially minimized in the graph cuts framework: for each pair of tetrahedra (sharing a triangle T ) represented by nodes p and q in our graph, an edge p → q is added with a weight w pq = A(T ) and, similarly, an opposite edge q → p with the same weight w qp = w pq is also added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation aspects</head><p>First, keypoints are extracted from the images with the SIFT keypoint detector <ref type="foot" target="#foot_0">1</ref> .</p><p>Then, the Delaunay triangulation is computed using the Computational Geometry Algorithms Library (CGAL)<ref type="foot" target="#foot_1">2</ref>  <ref type="bibr" target="#b4">[5]</ref>. CGAL defines all the needed geometric primitives and provides an excellent algorithm to compute the Delaunay triangulation in 3D: it is robust to degenerate configurations and floating-point error, thanks to the use of exact geometric predicates, while being able to process millions of points per minute on a standard workstation. It provides all the elementary operations needed in our algorithm: vertex insertion, vertex move, nearest vertex query and various traversals of the triangulation.</p><p>The photo-consistency is evaluated with a software rasterizer with sweeps the projection of each triangle of the Delaunay triangulation in the chosen views and computes the mean of the color variance of the pixels in this triangle.</p><p>Finally we compute the minimum s-t-cut of the graph we designed using the software<ref type="foot" target="#foot_2">3</ref> described in <ref type="bibr" target="#b6">[7]</ref> which is in fact better suited for regular grid-based graphs more commonly found in computer vision.</p><p>Our implementation currently leaves some room for improvement in term of computational speed: leaving aside the time required to extract the keypoints, it can take (on an Intel R Core TM 2 Duo 2.13 GHz PC) as little as a minute and a half to reconstruct a scene from a ∼ 50 images dataset to a few dozens minutes from a ∼ 300 images dataset depending on the number of input keypoints to match. Fortunately our method is versatile: we can use any type of features as input and we could switch to lighter features such as SURF <ref type="bibr" target="#b2">[3]</ref>. The matching of keypoints is done by brute force so most of the above computational time is actually spent on feature matching alone: this could be improved by   resorting to a more adapted nearest neighbour search <ref type="bibr" target="#b3">[4]</ref>. Lastly the photo-consistency computation may take advantage of modern graphics hardware.</p><formula xml:id="formula_4">p 0 p 2 q 2 q 2 p 0 p 1 q 1 p 2 p 1 q 1 q 2 su rf</formula><formula xml:id="formula_5">D p 0 (0) = 0 V p 1 q 1 (0, 1) = λ out V p 2 q 2 (0, 0) = V p 2 q 2 (1, 0) = λ in D q 2 (0) = λ in λ in λ out λ ∞ D p 0 (1) = λ ∞ V p 1 q 1 (0, 0) = V p 1 q 1 (1, 0) = V p 1 q 1 (1, 1) = 0 V p 2 q 2 (0, 1) = V p 2 q 2 (1, 1) = 0 D q 2 (1) = 0</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Temple</head><p>The first experiment (shown in Figure <ref type="figure" target="#fig_4">4</ref>.2) uses the 312 views temple dataset from the review of Seitz et al. <ref type="bibr" target="#b32">[33]</ref>.</p><p>It shows that our approach is flexible and while able to reconstruct large-scale scenes, it can still cope with more traditional multi-view stereo without using any of the usual clues that most high-precision algorithms would require. Also recall that the output surface depends on the 3D point cloud reconstructed from matched features, so regions without many matched keypoints are reconstructed as large triangles whereas densily sampled regions are more detailed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Toys</head><p>The data for the second experiment (shown in Figure <ref type="figure" target="#fig_4">4</ref>.3) was acquired with a consumer-grade handheld DV camcorder shooting soft toys laid on a table; one frame out of ten was extracted from the video sequence resulting in a 237 views dataset and calibration was done with a tracking software. The imaging conditions were absolutely not controlled, many of the images show large specular highlights on the tablecloth. No additional stabilizer was used and besides motion blur, many important color subsampling and aliasing artifacts due to video compression requirements are clearly noticeable. Despite such a hard dataset, our algorithm was able to reconstruct the table and the soft toys showing its robustness and its ability to cope with a large-scale cluttered scene without any additional information about its extent. Note that some small details compared the global scale of the scene are still recovered (the antennas, the ears or the tail of some of the soft toys, for instance) but areas that lack matchable features are less accurately reconstructed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and future work</head><p>We have presented a new multi-view reconstruction method adapted to large-scale cluttered scenes under uncontrolled imaging conditions. First a quasi-dense 3D point cloud of the scene is generated by matching keypoints across different views. An adaptive tetrahedral decomposition of the space is then built by means of a Delaunay triangulation of the 3D point set. Finally the scene is reconstructed by labeling the tetrahedra as empty or occupied using an assignement globally optimal as to photo-consistency of the output mesh and compatibility with the visibility of the matched keypoints. This new approach is free from numerous restrictions of previous reconstruction algorithms: it does not require any knowledge of the extent of the scene, it can deal with large-scale scenes at a reasonable computational cost, it exploits visibility information from keypoints to guide the position of the surface in a robust way, lastly, it can handle closed and open scenes.</p><p>We have demonstrated our method on real data: a classical dataset acquired in a controlled setup and a new realworld data set showing the efficiency of our method in handling difficult imaging conditions. The experimental results shown are quite promising and we are looking forward to evaluating our approach on other challenging data sets. We also expect to greatly improve the computation time of our implementation. Ultimately, our method could be incorporated into a full reconstruction system in which the feature extraction and matching step would be shared between calibration and reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The Voronoi diagram (gray edges) of a set of 2D points (red dots) and its associated Delaunay triangulation (black edges).</figDesc><graphic coords="3,61.93,72.00,212.62,134.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>th e ne ar es t ve rte x a n e w v e rt e x up da te in s e rt</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A candidate point (blue cross) updates the Delaunay triangulation depending on the maximum reprojection error between it and the nearest vertex: either it is inserted as a new vertex, or it updates the position of the nearest vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure3. A ray emanating from a vertex to a camera center (and the putative surface), the corresponding visibility-related energy term that penalizes the number of intersections with the ray (the label 0 means s / "outside" and the label 1 means t / "inside") and the edge weights of the crossed tetrahedra in the graph.</figDesc><graphic coords="6,84.30,354.75,82.68,110.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Some images of the temple dataset and our results.</figDesc><graphic coords="6,84.30,468.58,82.68,110.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Some images of the toys dataset, examples showing the realistic imaging conditions / acquisition artifacts (shadows and specularities / motion blur, aliasing and color subsampling) and our results.</figDesc><graphic coords="7,85.55,342.26,165.38,124.03" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.cs.ubc.ca/˜lowe/keypoints/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.cgal.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.adastral.ucl.ac.uk/˜vladkolm/ software.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Surface reconstruction by Voronoi filtering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Amenta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete and Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="481" to="504" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Complexity of the Delaunay triangulation of points on surfaces: the smooth case</title>
		<author>
			<persName><forename type="first">D</forename><surname>Attali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Boissonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lieutier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SURF: Speeded up robust features</title>
		<author>
			<persName><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shape indexing using approximate nearest-neighbour search in high-dimensional spaces</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Beis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1000" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Triangulations in CGAL</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Boissonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Devillers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teillaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yvinec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium on Computational Geometry</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Algorithmic Geometry, chapter Voronoi diagrams: Euclidian metric, Delaunay complexes</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Boissonnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yvinec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="435" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An experimental comparison of min-cut/max-flow algorithms for energy minimization in vision</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1124" to="1137" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From photohulls to photoflux optimization</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1149" to="1158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A probabilistic framework for space carving</title>
		<author>
			<persName><forename type="first">A</forename><surname>Broadhurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="388" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A volumetric approach for building complex models from range images</title>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shape reconstruction from 3D and 2D data using PDE-based deformable surfaces</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="238" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Silhouette and stereo fusion for 3D object modeling. Computer Vision and Image Understanding</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Esteban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schmitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="367" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variational principles, surface evolution, PDE&apos;s, level set methods and the stereo problem</title>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="336" to="344" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Fulkerson</surname></persName>
		</author>
		<title level="m">Flows in Networks</title>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Energy minimization via graph cuts: Settling what is possible</title>
		<author>
			<persName><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Drineas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Carved visual hulls for imagebased modeling</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="564" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bayesian 3D modeling from images using multiple depth maps</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gargallo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="885" to="891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view stereo revisited</title>
		<author>
			<persName><forename type="first">M</forename><surname>Goesele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2402" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exact maximum a posteriori estimation for binary images</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Greig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">T P A H</forename><surname>Seheult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="271" to="279" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
	<note>Methodological</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical volumetric multiview stereo reconstruction of manifold surfaces based on dual graph embedding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="503" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view stereo reconstruction of dense shape and complex appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="175" to="189" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A discrete global minimization algorithm for continuous variational problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kirsanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Gortler</surname></persName>
		</author>
		<idno>TR-14-04</idno>
		<imprint>
			<date type="published" when="2004-07">jul 2004</date>
		</imprint>
		<respStmt>
			<orgName>Harvard Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-camera scene reconstruction via graph cuts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="82" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What energy functions can be minimized via graph cuts?</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="147" to="159" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A theory of shape by space carving</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Kutulakos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="218" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The visual hull concept for silhouette-based image understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Laurentini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="162" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Oriented visibility for multiview reconstruction</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ivanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="225" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A quasi-dense approach to surface reconstruction from uncalibrated images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lhuillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="418" to="433" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A performance evaluation of local descriptors</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1615" to="1630" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-view stereo reconstruction and scene flow estimation with a global image-based matching score</title>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Keriven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Faugeras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="193" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A comparison and evaluation of multi-view stereo reconstruction algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Diebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="519" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Photorealistic scene reconstruction by voxel coloring</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="151" to="173" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-view reconstruction using photo-consistency and exact silhouette constraints: A maximum-flow formulation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Volumetric stereo with silhouette and feature constraints</title>
		<author>
			<persName><forename type="first">J</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wide-baseline stereo from multiple views: a probabilistic account</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="552" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Combined depth and outlier estimation in multi-view stereo</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fransens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2394" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dense matching of multiple wide-baseline views</title>
		<author>
			<persName><forename type="first">C</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1194" to="1201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3D surface reconstruction using graph cuts with surface constraints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="219" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Example-based stereo with general BRDFs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Treuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="457" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-view stereo via volumetric graph-cuts</title>
		<author>
			<persName><forename type="first">G</forename><surname>Vogiatzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dealing with textureless regions and specular highlights: A progressive space carving scheme using a novel photo-consistency measure</title>
		<author>
			<persName><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Welch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SDG cut: 3D reconstruction of non-lambertian objects using graph cuts on surface distance grid</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2269" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
