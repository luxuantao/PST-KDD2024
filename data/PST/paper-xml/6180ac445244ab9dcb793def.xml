<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Latent Structures Mining with Contrastive Modality Fusion for Multimedia Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinghao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Student Member, IEEE</roleName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<email>qiang.liu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mengqi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Shu</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institute of Automation</orgName>
								<orgName type="department" key="dep3">School of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Latent Structures Mining with Contrastive Modality Fusion for Multimedia Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Multimedia Recommendation</term>
					<term>Graph Structure Learning</term>
					<term>Contrastive Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimedia content is of predominance in the modern Web era. Recent years have witnessed growing research interests in multimedia recommendation, which aims to predict whether a user will interact with an item with multimodal contents. Most previous studies focus on modeling user-item interactions with multimodal features included as side information. However, this scheme is not well-designed for multimedia recommendation. Firstly, only collaborative item-item relationships are implicitly modeled through high-order item-user-item co-occurrences. Considering that items are associated with rich contents in multiple modalities, we argue that the latent semantic item-item structures underlying these multimodal contents could be beneficial for learning better item representations and assist the recommender models to comprehensively discover candidate items. Secondly, previous studies disregard the fine-grained multimodal fusion. Although having access to multiple modalities might allow us to capture rich information, we argue that the simple coarse-grained fusion by linear combination or concatenation in previous work is insufficient to fully understand content information of items and item relationships.To this end, we propose a latent structure MIning with ContRastive mOdality fusion method, which we term MICRO for brevity. To be specific, in the proposed MICRO model, we devise a novel modality-aware structure learning module, which learns item-item relationships for each modality. Based on the learned modality-aware latent item relationships, we perform graph convolutions which explicitly inject item affinities to modality-aware item representations. Additionally, we design a novel multi-modal contrastive framework to facilitate fine-grained multimodal fusion by forcing the modality-aware representation and multimodal fused representation to be close. Finally, these enriched item representations can be plugged into existing collaborative filtering methods to make more accurate recommendations. Extensive experiments on three real-world datasets demonstrate the superiority of our method over state-of-the-art multimedia recommendation methods and ablation studies validate the efficacy of mining latent item-item relationships and the contrastive multimodal fusion framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>W ITH the rapid development of Internet, information overload has become an increasingly crucial challenge. Personalized recommender systems act as an indispensable tool to help users find their preferred information from massive irrelevant contents. Nowadays, users are easily accessible to large amounts of online information represented in multiple modalities, including images, texts, videos, etc. For example, the visual appearance and textual descriptions play important roles when users selecting products online; the visual cover and textual tags allow users to find interesting items from a large amount of instant videos. Recent years have witnessed growing research interests in multimedia recommendation, which aims to predict whether a user will interact with an item with multimodal contents. It has This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Collaborative relation Semantic relation</p><formula xml:id="formula_0">u 1 u 2</formula><p>Fig. <ref type="figure">1</ref>. A toy example of recommendation with two types of item relations.</p><p>In this paper, we argue that semantic structures mined from multimodal features are helpful for comprehensively discovering candidate items supplementary to collaborative signals in traditional work (Best viewed in color).</p><p>been successfully applied to many online applications, such as e-commerce, instant video platforms and social media platforms.</p><p>Collaborative Filtering (CF), as one of the most prevalent techniques in personalized recommendation, have been widely studied. Focusing on exploiting abundant user-item interactions, CF methods group users according to their arXiv:2111.00678v1 [cs.IR] 1 Nov 2021 historical interactions, by encoding users and items into lowdimensional dense vectors and making recommendations based on these embeddings <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. Following traditional CF framework, early work on multimedia recommendation like VBPR <ref type="bibr" target="#b4">[5]</ref>, DeepStyle <ref type="bibr" target="#b5">[6]</ref>, and ACF <ref type="bibr" target="#b6">[7]</ref> incorporates multimodal features as side information in addition to the learned dense vectors of items, so as to group users based on both historical interactions and item contents. However, since these methods only model direct user-item interactions, their expressiveness is confined.</p><p>Inspired by the recent surge of graph neural networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, Wang et al. <ref type="bibr" target="#b9">[10]</ref> propose to model user-item relationships as bipartite graphs. The first-order connectivities in user-item graphs indicate the interaction history. And the second-order connectivities reveal collaborative relations that similar users (or items) who have co-interacted with the same items (or users). These graph-based recommender systems <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> inject high-order connectivities into the embedding process to learn better representations and achieve great success. Recently, many attempts have been made to integrate multimodal contents into graphbased recommendation systems. MMGCN <ref type="bibr" target="#b12">[13]</ref> constructs modality-specific user-item interaction graphs to model user preferences specific to each modality. Following MMGCN, GRCN <ref type="bibr" target="#b13">[14]</ref> utilizes multimodal features to refine user-item interaction graphs by identifying false-positive feedbacks and prunes the corresponding noisy edges.</p><p>Despite their effectiveness, previous attempts suffer from two limitations. Firstly, existing work fails to explicitly model item-item relationships, which have been proved to be important in recommender systems <ref type="bibr" target="#b14">[15]</ref>. Specifically, only collaborative relations are implicitly considered through high-order item-user-item co-occurrences. However, semantic relations, reflecting the content information of items which also helps inferring users' preferences, are not explicitly modeled. Taking Figure <ref type="figure">1</ref> as an example, existing methods will recommend the shirt ( ) for u 2 according to collaborative relations, since shirts ( ), hats ( ), and pants ( ) all interacted with u 1 . However, previous work may not be able to recommend coats ( ) to u 2 , which are semantically (visually in this example) similar to shirts. Considering that items are associated with rich multimodal content features in multimedia recommendation, there exist a wealth of semantic relations underlying multimodal contents, which would assist the recommender models to comprehensively discover candidate items.</p><p>Secondly, previous attempts disregard the fine-grained multimodal fusion: early work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> only focuses on unimodal information; other work on multimedia recommendation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> conducts multimodal fusion by simple linear combination or concatenation. We argue that finegrained multimodal fusion is important to learn better item representations by exploiting the correlation and interactions between modalities. Based on the hypothesis that a powerful representation is one that models modality-invariant factors <ref type="bibr" target="#b16">[17]</ref> which has exhibited remarkable benefits in many multimodal tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>, we propose to conduct fine-grained multimodal fusion by capturing shared item relationships from multiple modalities, which would facilitate learning better item representations and making more accurate recommendations.</p><p>In this paper, we propose a novel method to mine latent semantic item-item relationships underlying multimodal features of items, and conduct fine-grained multimodal fusion based on the learned structures to inject shared item-item relationships from multiple modalities into the item representations. As shown in Figure <ref type="figure">2</ref>, the proposed MICRO consists of four key components. Firstly, we develop a novel modality-aware structure learning layer, which learns modality-aware item structures from content features of each modality. Secondly, we perform graph convolutions on the learned modality-aware latent graphs to explicitly consider item relationships of each modality individually. Thirdly, we devise a novel multimodal contrastive framework and construct self-supervision signals by maximizing the agreement between item representations under individual modalities and the multimodal fused representations, and thus the fused multimodal representations can adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. Finally, the resulting enhanced item representations are infused with item relationships in multiple modalities, which will be added into the output item embeddings of CF models to make recommendations.</p><p>Our work enjoys two additional benefits. Firstly, MICRO can alleviate the cold-start problem. Previous graph-based multimedia recommendation methods face cold-start problems where long-tailed items are only interacted with few users or even never interacted with users. Since previous methods utilize multimodal content features based on useritem interaction graph, those long-tailed items will become isolated nodes in graph, which will invalidate their usage of multimodal information. Our work, on the contrary, can alleviate the cold-start problem in two ways: firstly, we mine latent item-item structures and the long-tailed items will get similar user feedbacks from their learned neighbors; secondly, the multimodal contrastive framework serves as a self-supervised auxiliary task. The external self-supervision signals are introduced to learn better item representations involved with relation information, which would further alleviate the cold-start problem.</p><p>Secondly, MICRO can serve as a flexible play-and-plug module. Unlike previous attempts which utilize multimodal features based on dedicated user-item aggregation strategies, MICRO separates the usage of multimodal features with the usage of user-item interactions and is agnostic to downstream CF methods.</p><p>In summary, the main contribution of this work is threefold.</p><p>• We highlight the importance of explicitly exploiting item relationships and considering fine-grained multimodal fusion in multimedia recommendation.</p><p>• We propose a novel method to mine latent item relations and conduct fine-grained multimodal fusion based on the mined structures.</p><p>• We perform extensive experiments on three public datasets. Notably, our method outperforms the state-ofthe-art methods by 20% on average in terms of different metrics, validating the effectiveness of our proposed model.</p><p>To foster reproducible research, our code is made publicly available at https://github.com/CRIPAC-DIG/MICRO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Latent Structure Learning Graph Convolutions Contrastive Fusion</head><p>Visual features   </p><formula xml:id="formula_1">E = " &gt; A A A C 0 n i c b V F L b x M x E H a W Q k t 4 p X D g w G V F h M Q h i n Z R 1 X I s 4 s K x S E 1 b K Q n R 2 O v d W P F L 9 m x p s t o D 4 s q / 6 B V + E P 8 G b 5 I D m 3 Y k y 5 + / m f E 8 P m q l 8 J g k f z v R g 7 2 H j / Y P H n e f P H 3 2 / E X v 8 O W F N 6 V j f M S M N O 6 K g u d S a D 5 C g Z J f W c d B U c k v 6 e J z 4 7 + 8 5 s 4 L o 8 9 x a f l U Q a F F L h h g o G a 9 1 x N q Z O a X K l z V p / r b B P k N V l j P e v 1 k m K w t v g v S L e i T r Z 3 N D j u 3 k 8 y w U n G N T I L 3 4 z S x O K 3 A o W C S 1 9 1 J 6 b k F t o C C j w P U o L i f V u s J 6 v h d Y L I 4 N y 4 c j f G a / T + j A u W b J k O k A p z 7 X V 9 D 3 u c b l 5 h / n F Z C 2 x K 5 Z p t C e S l j N H G z j j g T j j O U y w C A O R F 6 j d k c H D A M S 2 t V o a p u v 4 1 Z I F D f Z l U p U T j z v c 0 W D u x c s J s 2 i 2 K x 2 j A N k o I 6 c M v K G i 8 a b Y Q u B h l n x q 2 V 8 k M b Z l P G h X + C g 4 F k A z 8 H y 3 f q + 5 L m o t j p K W R a W J m 6 2 w 2 y p r s i 3 g U X H 4 b p 8 f D o 6 1 H / d L A V + I C 8 I W / J e 5 K S E 3 J K v p A z M i K M 1 O S W / C Z / o v N o F f 2 I f m 5 C o 8 4 2 5 x V p W f T r H 0 7 j 6 a U = &lt; / l a t e x i t &gt; A t Visual structure &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " X S G t Q x B u r 8 x u f 0 O d g F 7 F f N 4 c 1 w w = " &gt; A A A C 0 n i c b V F L b x M x E H a W V w m v t B w 4 c F k R I X G I o t 2 q A o 5 F v f R Y p K a t l I R</formula><formula xml:id="formula_2">M v N L F a 7 q S / 1 t g v w G q + t 6 1 u s n w 2 R t 8 X 2 Q b k G f b O 1 s t t + 5 m 2 S G l Y p r Z B K 8 H 6 e J x W k F D g W T v O 5 O S s 8 t s A U U f B y g B s X 9 t F p P U M f v A 5 P F u X H h a I z X 7 L 8 Z F S j f N B k i F e D c 7 / o a 8 n + + c Y n 5 5 2 k l t C 2 R a 7 Y p l J c y R h M 3 6 4 g z 4 T h D u Q w A m B O h 1 5 j N w Q H D s L R W F a r q 9 t u Y B Q L 1 b V a V E o U z 3 9 t s 4 c D O B b t p s y g W q w 3 T I C m o A 7 e s r P G i 0 U b o Y p B x Z t x a K T + 0 Y T Z l X P g n O B h I N v B z s H y n v i 9 p L o q d n k K m h Z W p u</formula><formula xml:id="formula_3">= " &gt; A A A C 0 n i c b V F L b x M x E H a W A i W 8 0 n L g w G V F h M Q h i n a r C j h W 4 t J j k Z q 2 U h K i s d e 7 s e K X 7 N n S Z L U H 1 C v / o l f 4 Q f w b v E k O b N q R L H / + Z s b z + K i V w m O S / O 1 E j / Y e P 3 m 6 / 6 z 7 / M X L V 6 9 7 B 4 c X 3 p S O 8 R E z 0 r g r C p 5 L o f k I B U p + Z R 0 H R S W / p I u v j f / y m j s v j D 7 H p e V T B Y U W u W C A g Z r 1 3 k 6 o k Z l f q n B V p / X 3 C f I b r K 7 r W a + f D J O 1 x f d B u g V 9 s r W z 2 U H n b p I Z V i q u k U n w f p w m F q c V O B R M 8 r o 7 K T 2 3 w B Z Q 8 H G A G h T 3 0 2 o 9 Q R 1 / C E w W 5 8 a F o z F e s / 9 n V K B 8 0 2 S I V I B z v + t r y I d 8 4 x L z L 9 N K a F s i 1 2 x T K C 9 l j C Z u 1 h F n w n G G c h k A M C d C r z G b g w O G Y W m t K l T V 7 b c x C w T q 2 6 w q J Q p n f r T Z w o G d C 3 b T Z l E s V h u m Q V J Q B 2 5 Z W e N F o 4 3 Q x S D j z L i 1 U n 5 o w 2 z K u P B P c D C Q b O D n Y P l O f V / S X B Q 7 P Y V M C y t T d 7 t B 1 n R X x P v g 4 m i Y f h o e f z v u n w y 2 A u + T d + Q 9 + U h S 8 p m c k F N y R k a E k Z r c k d / k T 3 Q e r a K f 0 e 0 m N O p s c 9 6 Q l k W / / g F k M e m u &lt; / l a t e x i t &gt; H v</formula><p>&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T n o K w / h 7 w j Y i / r 9 M S 7 r 5 p O U q g P k</p><formula xml:id="formula_4">= " &gt; A A A C 0 n i c b V F L b x M x E H a W Q k t 4 p X D g w G V F h M Q h i n Z R R T l W 4 t J j k Z q 2 U h K i s d e 7 s e K X 7 F l o s t o D 4 s q / 6 L X 9 Q f w b v E k O b N q R L H / + Z s b z + K i V w m O S / O 1 E j / Y e P 9 k / e N p 9 9 v z F y 1 e 9 w 9 c X 3 p S O 8 R E z 0 r g r C p 5 L o f k I B U p + Z R 0 H R S W / p I u v j f / y B 3 d e G H 2 O S 8 u n C g o t c s E A A z X r v Z 1 Q I z O / V O G q T u v v E + T X W G E 9 6 / W T Y b K 2 + D 5 I t 6 B P t n Y 2 O + z c T D L D S s U 1 M g n e j 9 P E 4 r Q C h 4 J J X n c n p e c W 2 A I K P g 5 Q g + J + W q 0 n q O M P g c n i 3 L h w N M Z r 9 v + M C p R v m g y R C n D u d 3 0 N + Z B v X G L + Z V o J b U v k m m 0 K 5 a W M 0 c T N O u J M O M 5 Q L g M A 5 k T o N W Z z c M A w L K 1 V h a q 6 / T Z m g U B 9 m 1 W l R O H M z z Z b O L B z w a 7 b L I r F a s M 0 S A r q w C 0 r a 7 x o t B G 6 G G S c G b d W y g 9 t m E 0 Z F / 4 J D g a S D f w c L N + p 7 0 u a i 2 K n p 5 B p Y W X q b j f I m u 6 K e B 9 c f B q m n 4 d H 3 4 7 6 J 4 O t w A f k H X l P P p K U H J M T c k r O y I g w U p M b c k v u o v N o F f 2 K f m 9 C o 8 4 2 5 w 1 p W f T n H 1 + D 6 a w = &lt; / l a t e x i t &gt; H t &lt; l a t e x i t s h a _ b a s e = " W O w / k B v + i y E + / d y X L i c = " &gt; A A A C X i c b V H L b h M x F H W G V w m v F C Q b E Z E S C y i a A Z V w L J q N w C I + l Z I o u n Y E y t + y b D n U H Q m z C j C D / E e J I s m J Q r W T o + x f c a i V w m O S / O l E N e u n n v H D x / h + f e l M x s f M S O P O K X g u h e Z j F C j u X U c F J X j K O G / s M d e G P J S t n C n I t M s E A A z X v P Z q w C U D W b r P k F g d j T U / G S b r i K + D d A v Z B u j + X n R h W K G R i b B + m a W J x V F A w y e v u t P D c A l t B z i c B a l D c z r A H X I j C L O D M u H I x m v o w L l f a l o e N m e h v y f N i k w e z u r h L Y F c s h b J C x m j i Z h v x Q j j O U J Y B A H M i B q z J T h g G H b W q k J V b b s K g v s q Q q J w k u b z R Y p W A X b R b F n L D N E g K s C V l T V e N N Y I n Q W n B m N s o P b Z h N G R f + C U L w i w E i z f q e L m o l p e Q a e H S N u s D X d N f E O H T F P D f A H W P y D P y n L w k K X l D D s k J G Z E x Y e S K / C C / y O / o Y R G X N v m d R Z v z h L Q i + v X E D P o u g = = &lt; / l a t e x i t &gt; L BPR Contrastive loss &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L v r w g P y 2 Y 5 A f N M n u M D 2 a b M I S F i s = " &gt; A A A C z 3 i c b V F N b 9 N A E N 2 Y r x K + U u D G x S J C 4 h B F d l U B x 0 q 9 c O D Q S q S t l E T R e D N 2 V t k v 7 Y 5 p E 8 u I K / 8 C i R P 8 I / 4 N 6 y Q H n D L S S m / f z O y 8 n Z d Z K T w l y Z 9 O d O f u v f s P D h 5 2 H z 1 + 8 v R Z 7 / D 5 h T e l 4 z j i R h p 3 l Y F H K T S O S J D E K + s Q V C b x M l u e N v n L L + i 8 M P o z r S x O F R R a 5 I I D B W r W e z l R Q A s O s v p U z y a E N 1 S d 1 r N e P x k m m 4 h v g 3 Q H + m w X Z 7 P D z s / J 3 P B S o S Y u w f t x m l i a V u B I c I l 1 d 1 J 6 t M C X U O A 4 Q A 0 K / b T a y K / j N 4 G Z x 7 l x 4 W i K N + y / H R U o 7 1 c q C 5 W N W L + f a 8 j / 5 c Y l 5 R + m l d C 2 J N R 8 O y g v Z U w m b n Y R z 4 V D T n I V A H A n g t a Y L 8 A B p 7 C x 1 p R M 1 e 2 7 M U u C z L d Z V U o S z l y 3 2 c K B X Q h + 0 2 Z J L N d b p k F S Z A 7 c q r L G i 8 Y Y o Y v B H L l x G 5 v 8 0 I a / K e P C O y E R 3 O I D v w C L e / N 9 m e W i 2 N M U O i 2 s T d 3 t B l v T f R N v g 4 u j Y f p u e H x + 3 D 8 Z 7 A w + Y K / Y a / a W p e w 9 O 2 E f 2 R k b M c 7 W 7 A f 7 x X 5 H 5 9 F 1 9 D X 6 t i 2 N O r u e F 6 w V 0 f e / O 6 j o B Q = = &lt; / l a t e x i t &gt; L C</formula><p>Fig. <ref type="figure">2</ref>. The overall framework of our proposed MICRO model. Firstly, we develop a novel modality-aware structure learning layer to mine the modalityaware latent item-item semantic relationships from multimodal features. Secondly, we employ graph convolutions on the learned modality-aware graphs to explicitly model item relationships of each modality individually. Thirdly, we devise a novel contrastive multimodal fusion framework to force the fused multimodal representations to adaptively capture item relationships shared between multiple modalities in a self-supervised manner. Finally, the resulting item representations are infused with item relationships in multiple modalities, which will be added into the output item embeddings of CF models to make recommendations. The contrastive loss and recommendation (BPR) loss will be optimized together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE PROPOSED METHOD</head><p>In this section, we first formulate the multimedia recommendation problem and introduce our model in detail. As illustrated in Figure <ref type="figure">2</ref>, there are four main components in our proposed framework: (1) a modality-aware graph structure learning layer that learns item graph structures from content features of each modality, (2) graph convolutional layers that learn the modality-aware item embeddings by injecting item-item affinities based on the learned graph structures, (3) an attentive multimodal fusion framework with contrastive auxiliary task to promote fine-grained multimodal fusion, and (4) downstream CF methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminary</head><p>Let U, I(|I| = N ) denote the set of users and items, respectively. Each user u ∈ U is associated with a set of items I u with positive feedbacks which indicate the preference score y ui = 1 for i ∈ I u . x u , x i ∈ R d is the input ID embedding of u and i, respectively, where d is the embedding dimension. Besides user-item interactions, multimodal features are offered as content information of items. We denote the modality features of item i as e m i ∈ R dm , where d m denotes the dimension of the features, m ∈ M is the modality, and M is the set of modalities. The purpose of multimedia recommendation is to accurately predict users' preferences by ranking items for each user according to predicted preference scores ŷui . In this paper, we consider visual and textual modalities denoted by M = {v, t}. Please kindly note that our method is not fixed to the two modalities and multiple modalities can be involved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modality-aware Latent Structure Mining</head><p>Multimodal features provide rich and meaningful content information of items, while existing methods only utilize multimodal features as side information for each item, ignoring the important semantic relationships of items underlying features. In this section, we introduce how to discover the underlying latent graph structure of item graphs in order to learn better item representations.</p><p>To be specific, we first construct initial k-Nearest-Neighbor (kNN) modality-aware item graphs S m by utilizing raw multimodal features. After that, we learn the latent graph structures A m from transformed multimodal features. Finally, we combine the learned structures with the initial structures by a skip connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Constructing Initial Modality-aware Graphs</head><p>We first construct initial kNN modality-aware graph S m by using raw features for each modality m. Based on the hypothesis that similar items are more likely to interact than dissimilar items <ref type="bibr" target="#b20">[21]</ref>, we quantify the semantic relationship between two items by their similarity. Common options for node similarity measurement include cosine similarity <ref type="bibr" target="#b21">[22]</ref>, kernel-based functions <ref type="bibr" target="#b22">[23]</ref>, and attention mechanisms <ref type="bibr" target="#b23">[24]</ref>. Our method is agnostic to similarity measurements, and we opt to the simple and parameter-free cosine similarity in this paper. The similarity matrix S m ∈ R N ×N is computed by</p><formula xml:id="formula_5">S m ij = (e m i ) e m j e m i e m j .<label>(1)</label></formula><p>Typically, the graph adjacency matrix is supposed to be nonnegative but S ij ranges between [−1, 1]. Thus, we suppress its negative entries to zeros. Moreover, common graph structures are much sparser other than a fully-connected graph, which is computationally demanding and might introduce noisy, unimportant edges <ref type="bibr" target="#b23">[24]</ref>. We conduct kNN sparsification <ref type="bibr" target="#b24">[25]</ref> on the dense graph: for each item i, we only keep edges with the top-k confidence scores:</p><formula xml:id="formula_6">S m ij = S m ij , S m ij ∈ top-k(S m i,: ), 0, otherwise,<label>(2)</label></formula><p>where S m i,: denotes the i-row of S, and S m is the resulting sparsified, directed graph adjacency matrix. To alleviate the exploding or vanishing gradient problem <ref type="bibr" target="#b7">[8]</ref>, we normalize the adjacency matrix as:</p><formula xml:id="formula_7">S m = (D m ) − 1 2 S m (D m ) − 1 2 ,<label>(3)</label></formula><p>where D m ∈ R N ×N is the diagonal degree matrix of S m and D m ii = j S m ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Learning Latent Modality-aware Graphs</head><p>Although we have obtained the modality-aware initial graph structures S m by utilizing raw multimodal features, they may not be ideal for the recommendation task. This is because the raw multimodal features are often noisy or even incomplete due to the inevitably error-prone data measurement or collection. To this end, we propose to dynamically learn the graph structures by the transformed multimodal features and combine the learned structures with initial ones. Firstly, we transform raw modality features into highlevel features e m i :</p><formula xml:id="formula_8">e m i = W m e m i + b m ,<label>(4)</label></formula><p>where W m ∈ R d×dm and b m ∈ R d denote the trainable transformation matrix and the bias vector, respectively. We dynamically infer the graph structures utilizing e m i , repeat the graph learning process described in Eqs. (1, 2, 3) and obtain the adjacency matrix A m .</p><p>Although the initial graph could be noisy, it still carries rich and useful information regarding item graph structures. Also, drastic change of adjacency matrix will lead to unstable training. To keep rich information of initial item graph and stabilize the training process, we add a skip connection that combines the learned graph with the initial graph:</p><formula xml:id="formula_9">A m = λ S m + (1 − λ) A m ,<label>(5)</label></formula><p>where λ ∈ (0, 1) is the coefficient of skip connection that controls the amount of information from the initial structure. The obtained A m is the final graph adjacency matrix representing latent structures for modality m.</p><p>It is worth mentioning that both S m and A m are sparsified and normalized matrices, thus the final adjacency matrix A m is also sparsified and normalized, which is computationally efficient and stabilizes gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Item Affinities Learning with Graph Convolutions</head><p>After obtaining the modality-aware latent structures, we perform graph convolution operations to learn better item representations by injecting item-item affinities into the embedding process. Graph convolutions can be treated as message propagation and aggregation. Through propagating the item representations from its neighbors, one item can aggregate information within the first-order neighborhood.</p><p>Furthermore, by stacking multiple graph convolutional layers, the high-order item-item relationships can be captured.</p><p>Following Wu et al. <ref type="bibr" target="#b25">[26]</ref> and He et al. <ref type="bibr" target="#b11">[12]</ref>, we employ simple message propagation and aggregation without feature transformation and non-linear activations which is effective and computationally efficient. In the l-th layer, the message passing and aggregation could be formulated as:</p><formula xml:id="formula_10">H m (l) = A m H m (l−1) ,<label>(6)</label></formula><p>where H m (l) ∈ R N ×d is the l-th layer item embedding matrix of modality m, the i-th row of which denotes the embedding vector of item i. For all modalities m ∈ M, we use the same item ID embedding matrix to initialize the input embedding matrix H m (0) . We utilize ID embedding vector of items as input representations rather than multimodal features, since we employ graph convolutions to directly capture itemitem affinities and multimodal features are used to bridge semantic relationships. After stacking L layers, H m (L) encodes the high-order item-item relationships of modality m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multimodal Fusion with Contrastive Auxiliary Task</head><p>Multiple modalities convey comprehensive information <ref type="bibr" target="#b26">[27]</ref>. Item relations shared between modalities are important to learn better item representations based on the hypothesis that a powerful representation is one that models modalityinvariant factors <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. To this end, we first utilize an attention mechanism to fuse item embeddings H m (L) of different modalities, and then devise a self-supervised contrastive loss to promote multimodal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Aggregating Multiple Modalities</head><p>We omit the subscript (L) and use h m i to denote the i-th row of H m (L) , which is the output embedding of graph convolutions corresponding to item i. The importance of each modality corresponding to item i can be formulated as follows:</p><formula xml:id="formula_11">w m i = q tanh (W h m i + b) ,<label>(7)</label></formula><p>where q ∈ R d denotes attention vector and W ∈ R d×d , b ∈ R d denote the weight matrix and bias vector, respectively. These parameters are shared for all modalities. After obtaining the importance of different modalities, we normalize them to get the weight coefficients:</p><formula xml:id="formula_12">α m i = exp (w m i ) |M| m=1 exp (w m i ) ,<label>(8)</label></formula><p>Then, the multimodal fused embedding of item i can be represented as:</p><formula xml:id="formula_13">h i = |M| m=1 α m i h m i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Contrastive Auxiliary Task</head><p>After obtaining the multimodal fused item embeddings, we devise a novel self-supervised auxiliary task to further force the fused item embeddings to adaptively distill the shared information from multiple modalities. Existing contrastive learning frameworks <ref type="bibr" target="#b27">[28]</ref> seek to maximize the agreement among differently augmented views of the same data examples, which has been proven to be effective in multiview representation learning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29]</ref> and multimodal tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. In this work, since multiple modality-aware graphs are involved, we propose to construct self-supervision signals by maximizing the agreement between item representations under individual modalities and the fused multimodal representations.</p><p>In this way, the fused multimodal representations can adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. The resulting contrastive loss can be mathematically expressed as:</p><formula xml:id="formula_14">L C = − 1 |I| i∈I 1 |M| m∈M 1 2 (I (h m i , hi) + I (hi, h m i )) ,<label>(10)</label></formula><p>where I(•, •) denotes the mutual information which quantifies the agreement between two representations, which is implemented by the InfoNCE estimator <ref type="bibr" target="#b27">[28]</ref>. Specifically, we set (h m i , h i ) as positive samples, while all other item embeddings in an individual modality (h m i , h m j ) and the fused multimodal embeddings (h m i , h j ) are considered as negatives:</p><formula xml:id="formula_15">I(h m i , hi) = log e θ(h m i ,h i) /τ e θ(h m i ,h i) /τ + j =i e θ(h m i ,h j )/τ + e θ(h m i ,h m j )/τ ,<label>(11)</label></formula><p>where τ ∈ R is a temperature parameter and θ(•, •) is the critic function which is implemented by a simple cosine similarity.</p><p>The proposed objective also conceptually relates to contrastive knowledge distillation <ref type="bibr" target="#b31">[32]</ref>, where several teacher models (representations under different individual modalities) and one student model (the fused representations) are employed. By forcing the embeddings between several teachers and a student to be the same, these fused representations adaptively collect information from all modality-aware item relations. Additionally, the multimodal contrastive framework serves as a self-supervised auxiliary task, where the external self-supervision signals are introduced to learn better item representations involved with relation information from multiple modalities, which would further alleviate the cold-start problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Incorporating with Collaborative Filtering Methods</head><p>Unlike previous attempts which utilize multimodal features based on sophisticated user-item aggregation strategies, MICRO separates the usage of multimodal features with the usage of user-item interactions and is agnostic to downstream CF methods. Specifically, we learn item representations from mined item relations and then combine them with downstream CF methods that model user-item interactions. It is flexible and could be served as a play-and-plug module for any CF methods.</p><p>We denote the output user and item embeddings from CF methods as x u , x i ∈ R d and simply enhance item embeddings by adding normalized multimodal fused item embeddings h i : We then compute the user-item preference score by taking inner product of user embeddings and enhanced item embeddings:</p><formula xml:id="formula_16">x i = x i + h i h i 2 . (<label>12</label></formula><formula xml:id="formula_17">)</formula><formula xml:id="formula_18">ŷui = x u x i .<label>(13)</label></formula><p>Additionally, the play-and-plug paradigm separates the usage of multimodal features with user-item interactions, thus alleviating the cold-start problem, where the long-tailed items are only interacted with few users or even never interacted with users. We learn latent structures for items and the tailed items will get similar feedbacks from relevant neighbors through neighborhood aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Optimization</head><p>We adopt the Bayesian Personalized Ranking (BPR) loss <ref type="bibr" target="#b32">[33]</ref> to compute the pair-wise ranking, which encourages the prediction of an observed entry to be higher than its unobserved counterparts:</p><formula xml:id="formula_19">L BPR = − u∈U i∈Iu j / ∈Iu ln σ (ŷ ui − ŷuj ) ,<label>(14)</label></formula><p>where I u indicates the observed items associated with user u and (u, i, j) denotes the pairwise training triples where i ∈ I u is the positive item and j / ∈ I u is the negative item sampled from unobserved interactions. σ(•) is the sigmoid function.</p><p>The overall loss function can be formulated as:</p><formula xml:id="formula_20">L = L BPR + βL C (<label>15</label></formula><formula xml:id="formula_21">)</formula><p>where β ∈ R is a hyper-parameter to control the effect of the contrastive auxiliary task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>In this section, we conduct experiments on three widely used real-world datasets to answer the following research questions:</p><p>• RQ1: How does our model perform compared with the state-of-the-art multimedia recommendation methods and other CF methods in both warm-start and cold-start settings?</p><p>• RQ2: How do the structure mining and contrastive learning modules contribute to the model performance?</p><p>• RQ3: How sensitive is our model under the perturbation of several key hyper-parameters?</p><p>• RQ4: What is the contribution of each modality?</p><p>TABLE 2 Performance comparison of our MICRO with different baselines in terms of Recall@20 (R@20), Precision@20 (P@20), and NDCG@20. The best performance is highlighted in bold and the second is highlighted by underlines. Improv. indicates relative improvements over the best baseline in percentage. All improvements are significant with p-value ≤ 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Clothing Sports Baby R@20 P@20 NDCG@20 R@20 P@20 NDCG@20 R@20 P@20 NDCG@20 MF 0.  <ref type="table" target="#tab_0">1</ref>. The three datasets include both visual and textual modalities. We use the 4,096-dimensional visual features that have been extracted and published. For the textual modality, we extract textual embeddings by concatenating the title, descriptions, categories, and brand of each item and utilize sentence-transformers <ref type="bibr" target="#b34">[35]</ref> to obtain 1,024-dimensional sentence embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Baselines</head><p>To evaluate the effectiveness of our proposed model, we compare it with several state-of-the-art recommendation models. These baselines fall into two groups: CF methods (i.e., MF, NGCF, LightGCN) and deep content-aware recommendation models (i.e., VBPR, MMGCN, GRCN).</p><p>• MF <ref type="bibr" target="#b32">[33]</ref> optimizes Matrix Factorization using the Bayesian personalized ranking (BPR) loss, which exploits the user-item direct interactions only as the target value of interaction function.</p><p>• NGCF <ref type="bibr" target="#b9">[10]</ref> explicitly models user-item interactions by a bipartite graph. By leveraging graph convolutional operations, it allows the embeddings of users and items to interact with each other to harvest the collaborative signals as well as high-order connectivity signals.</p><p>• LightGCN <ref type="bibr" target="#b11">[12]</ref> argues the unnecessarily complicated design of GCNs (i.e., feature transformation and nonlinear activation) for recommendation systems and proposes a light model which only consists of two essential components: light graph convolution and layer combination.</p><p>• VBPR <ref type="bibr" target="#b4">[5]</ref>: Based upon the BPR model, it integrates the visual features and ID embeddings of each item as its representation and feeds them into the Matrix Factorization framework. In our experiments, we concatenate multi-modal features as content information to predict the interactions between users and items.</p><p>• MMGCN <ref type="bibr" target="#b12">[13]</ref> is one of the state-of-the-art multimodal recommendation methods, which constructs modal-specific graphs and refines modal-specific representations for users and items. It aggregates all model-specific representations to obtain the representations of users or items for prediction.</p><p>• GRCN <ref type="bibr" target="#b13">[14]</ref> is also one of the state-of-the-arts multimodal recommendation methods. It refines user-item interaction graph by identifying the false-positive feedback and prunes the corresponding noisy edges in the interaction graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Evaluation Protocols</head><p>We conduct experiments in both warm-start and cold-start settings.</p><p>Warm-start settings. For each dataset, we select 80% of historical interactions of each user to constitute the training set, 10% for validation set, and the remaining 10% for testing set. For each observed user-item interaction, we treat it as a positive pair, and then conduct the negative sampling strategy to pair them with one negative item that the user does not interact before.</p><p>Cold-start settings. We remove all user-item interaction pairs associated with a randomly selected 20% item set from the training set. We further divide the half of the items (10%) into the validation set and half (10%) into the testing set. In other words, these items are entirely unseen in the training set.</p><p>We adopt three widely-used metrics to evaluate the performance of preference ranking: Recall@k, NDCG@k, and Precision@k. By default, we set k = 20 and report the averaged metrics for all users in the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Implementation Details</head><p>We implemente our method in PyTorch <ref type="bibr" target="#b35">[36]</ref> and set the embedding dimension d fixed to 64 for all models to ensure fair comparison. We optimize all models with the Adam <ref type="bibr" target="#b36">[37]</ref> optimizer, where the batch size is fixed at 1024. We use the Xavier initializer <ref type="bibr" target="#b37">[38]</ref> to initialize the model parameters. The optimal hyper-parameters are determined via grid search on the validation set: the learning rate is set to 0.0005, the coefficient of 2 normalization is set to 10 −4 . The k of kNN sparsification is set to 10, the λ of skip connection is set to 0.7, the temperature parameter τ is set to 0.5, the coefficient β used to control the effect of contrastive auxiliary task is set to 0.03. Besides, we stop training if Recall@20 on the validation set does not increase for 10 successive epochs to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Performance Comparison (RQ1)</head><p>We start by comparing the performance of all methods, and then explore how the our method alleviate the coldstart problem. In this subsection, we combine MICRO with LightGCN as a downstream CF method, and will also conduct experiments with different CF methods in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overall Performances</head><p>Table <ref type="table">2</ref> reports the performance comparison results, from which we can observe:</p><p>• Our method MICRO significantly outperforms both CF methods and content-aware methods, verifying the effectiveness of our methods. Specifically, MICRO improves over the strongest baselines in terms of Recall@20 by 24.1%, 18.6%, and 18.3% in Clothing, Sports, and Baby, respectively. This indicates that our proposed method is well-designed for multimedia recommendation by discovering underlying item-item relationships and conducting fine-grained multimodal fusion through the contrastive auxiliary task.</p><p>• Compared with CF methods, content-aware methods yield better performance overall, which indicates that multimodal features provide rich content information about items, and can boost recommendation accuracies. GRCN outperforms other baselines in three datasets since it discovers and prunes false-positive edges in user-item interaction graphs.</p><p>• Additionally, existing content-aware recommendation models are highly dependent on the representativeness of multimodal features and thus obtain fluctuating performances over different datasets. For Clothing dataset where visual features are very important in revealing item attributes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, VBPR, MMGCN, and GRCN outperform all CF methods. For the other two datasets where multimodal features may not directly reveal item attributes, content-aware methods obtain relatively small improvements. The performances of VBPR and MMGCN are even inferior to the CF method LightGCN. Different from existing content-aware methods, we discover the latent item relationships underlying multimodal features instead of directly using them as side information. The latent item relationships are less dependent on the representativeness of multimodal features, and thus we are able to obtain robust performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Performances in Cold-start Settings</head><p>The cold-start problem remains a prominent challenge in recommendation systems <ref type="bibr" target="#b38">[39]</ref>. Multimodal features of items provide rich content information, which can be exploited to alleviate the cold-start problem. We conduct cold-start experiments and compare with representative baselines. MICRO w/o. contrast is the simplified variant of MICRO, which discards the contrastive auxiliary task in Section 2.4.2 and only utilizes the BPR loss in Eq. ( <ref type="formula" target="#formula_19">14</ref>). Figure <ref type="figure" target="#fig_3">3</ref> reports the results of performance comparison, from which we can observe: • Both MICRO w/o. contrast and MICRO can alleviate the cold-start problem and outperform all baselines on three datasets. They learn item graphs from multimodal features, along which cold-start items will get similar feedbacks from relevant neighbors through neighborhood aggregation of graph convolutions.</p><p>• Additionally, MICRO outperforms MICRO w/o. contrast on three datasets. In MICRO, the multimodal contrastive framework serves as a self-supervised auxiliary task. The self-supervision signals are constructed by maximizing the agreement between item representations under individual modalities and the multimodal fused representations to learn better item representations which encode item relationships from multiple modalities. In this way, the cold-start problem would be further alleviated.</p><p>• CF methods MF and LightGCN obtain poor performances under cold-start settings in general, primarily because they only leverage users' feedbacks to predict the interactions between users and items. Although these methods may work well for items with sufficient feedbacks, they cannot help in cold-start settings, since no user-item interaction is available to update the representations of cold-start items.</p><p>• Content-aware model VBPR outperforms CF methods in general, which indicates that the content information provided by multimodal features benefits recommendation for cold-start items. In particular, content information can help bridge the gap between the existing items to cold-start items. However, some graph-based contentaware methods such as GRCN, although perform well in warm-start settings, obtain poor performance in coldstart settings. GRCN utilizes multimodal features on user-item interaction bipartite graphs, which is also heavily dependent on user-item interactions. For coldstart items, they never interact with users and become isolated nodes in the user-item graphs, leading to inferior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Studies (RQ2)</head><p>In this subsection, we combine MICRO with three commonused CF methods, i.e., MF, NGCF, and LightGCN to validate the effectiveness and flexibility of our proposed method.</p><p>For each CF method, we have three other variants: the first one is CF+feats, which does not consider latent itemitem relationships and directly uses transformed multimodal features to replace the item representations learned from item graphs in Eq. ( <ref type="formula" target="#formula_16">12</ref>). The second one is named as MICRO/feats, which uses multimodal features as the input initial item embeddings of graph convolutions instead of ID embeddings. The third is MICRO w/o. contrast, which discards the contrastive auxiliary task in Section 2.4.2 and only utilizes the BPR loss in Eq. ( <ref type="formula" target="#formula_19">14</ref>). Table <ref type="table" target="#tab_2">3</ref> summarizes the performance and the relative improvements gained by MICRO over CF+feats, from which we have the following observations:</p><p>• MICRO significantly and consistently outperforms all original CF methods and CF+feats variants on three datasets, obtaining up to 68.8% improvements over the CF+feats variants, verifying the flexibility of our plug-in paradigm.</p><p>• Even without the contrastive auxiliary task, MICRO w/o. contrast obtains significant improvements over CF+feats, indicating the effectiveness of discovering latent item-item relationships from multimodal features. Furthermore, the improvements between MICRO and MICRO w/o. contrast show the importance of finegrained multimodal fusion, through which we can capture item relationships shared between modalities adaptively.</p><p>• Based on the learned item graph structures, MI-CRO/feats employs graph convolutions on multimodal features. Our proposed method MICRO utilizes the same learned structures but employ graph convolutions on item ID embeddings, which aims to directly model item affinities. The improvements between them validate the effectiveness of explicitly modeling item affinities where multimodal features are only used to bridge semantic relationships between items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Sensitivity Analysis (RQ3)</head><p>Since the graph structure learning layer and the contrastive auxiliary task play pivotal roles in our method, in this subsection, we conduct sensitivity analysis with different  hyper-parameters on graph structure learning layers and the contrastive auxiliary task. Firstly, we investigate performance of MICRO-LightGCN with respect to different k value of the k-NN sparsification operation since k is important which determines the number of neighbors of each item, and controls the amount of information propagated from neighbors. Secondly, we discuss how the skip connection coefficient λ affects the performance which controls the amount of information from the initial graph structures. Finally, we explore how the contrastive auxiliary task magnitude β affects the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Impact of Varied k Values</head><p>Figure <ref type="figure">4</ref> reports the results of performance comparison. k = 0 means no item relationships are included and the model is degenerated to LightGCN. We have the following observations:</p><p>• Our method gains significant improvement between k = 0 and k = 5, which validates the rationality of item relationships mined from multimodal features. Even if only a small part of the neighbors are included, we can obtain better item representations by aggregating meaningful and important information from the neighbors, which boost the recommendation performance.</p><p>• Furthermore, the performance first improves as k increases, which verifies the effectiveness of information aggregation along item-item graphs since more neighbors bring more meaningful information that helps to make more accurate recommendations.</p><p>• The trend, however, declines when k continues to increase, since there may exist many unimportant neighbors that will inevitably introduce noise to information propagation. This demonstrates the necessity of conducting kNN sparsification on the learned dense graph. We have the following observations:</p><p>• When we set λ = 0, the model obtains poor performance. It only learns graph structure from the transformed features, completely updating the adjacency matrix every time, ignoring the rich and useful information of raw features and resulting in fluctuated training process.</p><p>• The performance first grows as λ becomes larger, validating the importance of initial structures constructed by raw multimodal features. However, it begins to deteriorate when λ continues to increase, since raw features are often noisy due to the inevitably error-prone data measurement or collection process. Learning the graph structures dynamically can reduce noise.</p><p>• Overall, there are no apparent sharp rises and falls, indicating that our method is not that sensitive to the selection of λ. Notably, all models surpass the baselines (c.f. Table <ref type="table">2</ref>), proving the effectiveness of item graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">Impact of Varied Coefficients β</head><p>We investigate how the contrastive auxiliary task magnitude β affects the performance. Figure <ref type="figure" target="#fig_5">5</ref> reports the performance comparison. β = 0 denotes MICRO w/o. contrast, which discards the contrastive auxiliary task. We can observe that</p><p>• With the increase of β, the performances on all datasets first rise and is always better than β = 0. The primary recommendation task achieves decent gains when jointly optimized with the self-supervised auxiliary task even with a small β.</p><p>• However, it begins to decline when β continues to increase. A small β can promote the primary task, while a larger one would mislead it. The benefits brought by the self-supervised task could be easily neutralized and the recommendation task is sensitive to the magnitude of the self-supervised task, which is also observed in other recommendation works with contrastive learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">The Contribution of Each Modality (RQ4)</head><p>In this subsection, we aim to explore the contribution of each modality. Table <ref type="table" target="#tab_3">4</ref> reports the performance comparison over different modalities. We observe that the performances of utilizing multiple modalities are better than that of ones within the single modality, demonstrating that incorporating the information from multiple modalities facilitates comprehensive understanding of items. Additionally, textual modality contributes more than visual modality in general. It is reasonable since textual modality provide more fine-grained information which directly reveals the titles, categories and descriptions of items while visual modality only provides coarse-grained visual appearances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multimodal Recommendation</head><p>Collaborative filtering (CF) has achieved great success in recommendation systems, which leverage users' feedbacks (such as clicks and purchases) to predict the preference of users and make recommendations. However, CF-based methods suffer from sparse data with limited user-item interactions and rarely accessed items. To address the problem of data sparsity, it is important to exploit other information besides user-item interactions. Multimodal recommendation systems consider massive multimedia content information of items, which have been successfully applied to many applications, such as e-commerce, instant video platforms and social media platforms <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>. For example, VBPR <ref type="bibr" target="#b4">[5]</ref> extends matrix factorization by incorporating visual features extracted from product images  <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b47">48]</ref> and especially multimodal recommendation systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b48">49]</ref>. MMGCN <ref type="bibr" target="#b12">[13]</ref> constructs modal-specific graph and conducts graph convolutional operations, to capture the modal-specific user preference and distills the item representations simultaneously. In this way, the learned user representation can reflect the users' specific interests on items. Following MMGCN, GRCN <ref type="bibr" target="#b13">[14]</ref> focuses on adaptively refining the structure of interaction graph to discover and prune potential false-positive edges.</p><p>The above methods directly utilize multimodal features as side information of each item and disregard fine-grained multimodal fusion. In our model, we step further by discovering item-item relationships from multimodal features, and conduct fine-grained multimodal fusion to inject complementary item-item relationships from multiple modalities into the item representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Deep Graph Structure Learning</head><p>GNNs have shown great power on analyzing graphstructured data and have been widely employed for graph analytical tasks across a variety of domains, including node classification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b49">50]</ref>, link prediction <ref type="bibr" target="#b50">[51]</ref>, information retrieval <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b52">53]</ref>, etc. However, most GNN methods are highly sensitive to the quality of graph structures and usually require a perfect graph structure that are hard to construct in real-world applications <ref type="bibr" target="#b53">[54]</ref>. Since GNNs recursively aggregate information from neighborhoods of one node to compute its node embedding, such an iterative mechanism has cascading effects -small noise in a graph will be propagated to neighboring nodes, affecting the embeddings of many others. Additionally, there also exist many realworld applications where initial graph structures are not available. Recently, considerable literature has arisen around the central theme of Graph Structure Learning (GSL), which targets at jointly learning an optimized graph structure and corresponding representations. There are three categories of GSL methods: metric learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, probabilistic modeling <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56]</ref>, and direct optimization approaches <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>For example, IDGL <ref type="bibr" target="#b23">[24]</ref> casts the graph learning problem into a similarity metric learning problem and leverage adaptive graph regularization for controlling the quality of the learned graph; DGM <ref type="bibr" target="#b59">[60]</ref> predicts a probabilistic graph, allowing a discrete graph to be sampled accordingly in order to be used in any graph convolutional operator. NeuralSparse <ref type="bibr" target="#b54">[55]</ref> considers the graph sparsification task by removing taskirrelevant edges. It utilizes a deep neural network to learn k-neighbor subgraphs by selecting at most k neighbors for each node in the graph. We kindly refer to <ref type="bibr" target="#b60">[61]</ref> for a recent overview of approaches for graph structure learning.</p><p>In personalized recommendation, although user-item interactions can be formulated as a bipartite graph naturally, item-item relations remain rarely explored. To model item relationships explicitly, we employ metric learning approaches to represent edge weights as a distance measure between two end nodes, which fits for multimedia recommendation since rich content information can be included to measure the semantic relationship between two items.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Contrastive Learning</head><p>Self-supervised learning is an emerging technique to learn representations by self-defined supervision signals generated from raw data without relying on annotated labels. Contrastive learning (CL) has become a dominant branch of self-supervised learning, which targets at obtaining robust and discriminative representations by grouping positive samples closer and negative samples far from each other. For visual data, negative samples can be generated using a multiple-stage augmentation pipeline <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>, consisting of color jitter, random flip, cropping, resizing, rotation, color distortion, etc. The latest advances extend self-supervised learning to graph representation learning <ref type="bibr" target="#b63">[64]</ref>. Velickovic et al. <ref type="bibr" target="#b64">[65]</ref> introduce an objective function measuring the Mutual Information (MI) between global graph embeddings and local node embeddings. GraphCL <ref type="bibr" target="#b65">[66]</ref> and GRACE <ref type="bibr" target="#b49">[50]</ref> propose a node-level contrastive objective to simplify previous work. Furthermore, Zhu et al. <ref type="bibr" target="#b66">[67]</ref> propose a contrastive method with adaptive augmentation that incorporates various priors for topological and semantic aspects of the graph. Generally, most CL work differs from each other in terms of the generation of negative samples and contrastive objectives.</p><p>There also exist several works combining self-supervised learning with session-based recommendation <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b68">69]</ref>, social recommendation <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b68">69]</ref> and multimedia recommendation <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71]</ref>. Zhou et al. <ref type="bibr" target="#b68">[69]</ref> utilize contrastive learning to learn the correlations among attribute, item, subsequence, and sequence. Yu et al. <ref type="bibr" target="#b39">[40]</ref> employ self-supervised learning to regain the connectivity information with hierarchical mutual information maximization. Wei et al. <ref type="bibr" target="#b70">[71]</ref> aim to maximize the mutual information between item content and collaborative signals to alleviate the cold-start problem.</p><p>In this work, since multiple modality-aware graphs are involved, the individual modality-aware item representations and multimodal fused representations are natural positive pairs. We utilize contrastive learning to maximize the agreement between item representations under an individual modality and the multimodal fused representations. In this way, the fused multimodal representations can adaptively capture itemitem relationships shared between multiple modalities in a self-supervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we have proposed the latent structure mining method (MICRO) for multimodal recommendation, which leverages graph structure learning to discover latent item relationships underlying multimodal features and devises a novel contrastive framework to fuse multimodal item relationships. In particular, we first develop a modality-aware structure learning layer and graph convolutions to inject modality-aware item relationships into item representations. Furthermore, we propose a novel multimodal contrastive framework to adaptively capture item-item relationships shared between multiple modalities in a self-supervised manner. Finally, the resulting enhanced item representations are infused with item relationships in multiple modalities, which will be added into the output item embeddings of CF models to make recommendations. Empirical results on three public datasets have demonstrated the effectiveness of our proposed model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( a )</head><label>a</label><figDesc>Sports fleece hoodie… (b) Casual summer T-shirt… (c) Loose-fit summer shorts… (d) Shirt for business, wedding… (e) Summer casual tank top… (f) Business pants…Textual features t e x i t s ha 1 _ b a s e 6 4 = " Z H 5 M P s I H Y S g B 6 n d s N N m G h e 7 6 G d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>o 7 P V u r P g l e 7 Y 0 W e 0 B c e 2 / 6 B V + E P 8 G b 5 I D m z K S 5 c / f z H g e H 7 V S e E y S P 5 3 o w c N H j 5 / s P e 0 + e / 7 i 5 a v e / s G F N 6 V j f M S M N O 6 K g u d S a D 5 C g Z J f W c d B U c k v 6 e K k 8 V 9 e c + e F 0 e e 4 t H y q o N A i F w w w U L P e m w k 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>9 0 g a 7 o r 4 n 1 w c T h M P w 6 P v h 7 1 j w d b g f f I W / K O f C A p + U S O y S k 5 I y P C S E 3 u y C / y O z q P V t G P 6 O c m N O p s c 1 6 T l k W 3 f w F T k e m n &lt; / l a t e x i t &gt; A v &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c 7 r P W N S D z 5 e V 2 p 7 f g H c 4 r R Y / / 4 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Performances of our method with different baselines in cold-start settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fig. 4. Performance comparison over different hyperparameters settings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Performance comparison over different β values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 4 . 2</head><label>42</label><figDesc>Impact of Varied Coefficients λ Figure 4 reports the performance comparison. λ = 0 means only consider the graph structure learned by the transformed multimodal features, and λ = 1 means we only consider the initial structure generated by the raw multimodal features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Liang</head><label></label><figDesc>Wang received both the BEng and MEng degrees from Anhui University in 1997 and 2000, respectively, and the PhD degree from the Institute of Automation, Chinese Academy of Sciences (CASIA) in 2004. From 2004 to 2010, he was a research assistant at Imperial College London, United Kingdom, and Monash University, Australia, a research fellow at the University of Melbourne, Australia, and a lecturer at the University of Bath, United Kingdom, respectively. Currently, he is a full professor of the Hundred Talents Program at the National Lab of Pattern Recognition, CASIA. His major research interests include machine learning, pattern recognition, and computer vision. He has widely published in highly ranked international journals such as IEEE TPAMI and IEEE TIP, and leading international conferences such as CVPR, ICCV, and ECCV. He has served as an Associate Editor of IEEE TPAMI, IEEE TIP, and PR. He is an IEEE Fellow and an IAPR Fellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Datasets can be accessed at http://jmcauley.ucsd.edu/data/ amazon/links.html.</figDesc><table><row><cell></cell><cell cols="3">Statistics of the datasets</cell><cell></cell></row><row><cell cols="5">Dataset 1 # Users # Items # Interactions Density</cell></row><row><cell>Clothing Sports Baby</cell><cell>39,387 35,598 19,445</cell><cell>23,033 18,357 7,050</cell><cell>237,488 256,308 139,110</cell><cell>0.00026 0.00039 0.00101</cell></row><row><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Performance of our proposed MICRO on top of different downstream collaborative filtering (CF) methods. Improv. indicates relative improvements in percentage over the base CF model with multimodal features (CF+feats).</figDesc><table><row><cell>Model</cell><cell>R@20</cell><cell cols="2">Clothing P@20 NDCG@20</cell><cell>R@20</cell><cell>Sports P@20</cell><cell>NDCG@20</cell><cell>R@20</cell><cell>Baby P@20</cell><cell>NDCG@20</cell></row><row><cell cols="3">MF MF+feats MICRO/feats MICRO w/o. contrast 0.0664 0.0034 0.0191 0.0010 0.0456 0.0023 0.0627 0.0032 MICRO 0.0715 0.0036 Improv. 56.8% 56.5%</cell><cell>0.0088 0.0197 0.0276 0.0301 0.0319 61.9%</cell><cell cols="2">0.0430 0.0023 0.0674 0.0036 0.0823 0.0043 0.0853 0.0045 0.0875 0.0046 29.8% 27.8%</cell><cell>0.0202 0.0304 0.0372 0.0397 0.0402 32.3%</cell><cell cols="2">0.0440 0.0024 0.0701 0.0037 0.0766 0.0040 0.0701 0.0036 0.0758 0.0040 8.1% 8.1%</cell><cell>0.0200 0.0306 0.0334 0.0309 0.0337 10.1%</cell></row><row><cell cols="3">NGCF NGCF+feats MICRO/feats MICRO w/o. contrast 0.0486 0.0025 0.0387 0.0020 0.0436 0.0022 0.0539 0.0027 MICRO 0.0637 0.0032 Improv. 46.1% 45.5%</cell><cell>0.0168 0.0190 0.0235 0.0213 0.0279 46.8%</cell><cell cols="2">0.0728 0.0038 0.0748 0.0040 0.0796 0.0042 0.0804 0.0043 0.0874 0.0046 16.8% 15.0%</cell><cell>0.0332 0.0344 0.0359 0.0360 0.0396 15.1%</cell><cell cols="2">0.0591 0.0032 0.0660 0.0035 0.0751 0.0040 0.0700 0.0037 0.0800 0.0042 21.2% 20.0%</cell><cell>0.0261 0.0295 0.0324 0.0307 0.0351 19.0%</cell></row><row><cell cols="3">LightGCN LightGCN+feats MICRO/feats MICRO w/o. contrast 0.0703 0.0036 0.0470 0.0024 0.0477 0.0024 0.0659 0.0033 MICRO 0.0782 0.0040 Improv. 63.9% 66.7%</cell><cell>0.0215 0.0208 0.0289 0.0321 0.0351 68.8%</cell><cell cols="2">0.0803 0.0042 0.0754 0.0040 0.0889 0.0047 0.0925 0.0049 0.0988 0.0052 31.0% 30.0%</cell><cell>0.0377 0.0350 0.0406 0.0428 0.0457 30.6%</cell><cell cols="2">0.0698 0.0037 0.0793 0.0042 0.0871 0.0046 0.0830 0.0044 0.0892 0.0047 12.5% 11.9%</cell><cell>0.0319 0.0344 0.0388 0.0359 0.0402 16.9%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Performances comparison over different modalities.</figDesc><table><row><cell>Dataset</cell><cell>Model</cell><cell>R@20</cell><cell>P@20</cell><cell>NDCG@20</cell></row><row><cell>Clothing</cell><cell cols="3">Visual Textual 0.0699 0.0035 0.0613 0.0031 Both 0.0782 0.0040</cell><cell>0.0282 0.0321 0.0351</cell></row><row><cell>Sports</cell><cell cols="3">Visual Textual 0.0928 0.0048 0.0879 0.0046 Both 0.0988 0.0052</cell><cell>0.0408 0.0430 0.0457</cell></row><row><cell>Baby</cell><cell cols="3">Visual Textual 0.0825 0.0043 0.0772 0.0040 Both 0.0892 0.0047</cell><cell>0.0348 0.0373 0.0402</cell></row><row><cell cols="5">to improve the performance. DVBPR [45] attempts to jointly train the image representation as well as the parameters in a recommender model. Sherlock [16] incorporates categorical information for recommendation based on visual features. DeepStyle [6] disentangles category information from visual representations for learning style features of items and sensing preferences of users. ACF [7] introduces an item-level and component-level attention model for inferring the underlying users' preferences encoded in the implicit user feedbacks. VECF [46] models users' various attentions on different image regions and reviews. MV-RNN [47] uses multimodal features for sequential recommendation in a recurrent framework. Recently, Graph Neural Networks (GNNs) have been introduced into recommendation systems</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by National Key Research and Development Program (2018YFB1402600), National Natural Science Foundation of China (61772528), Beijing National Natural Science Foundation (4182066), and Shandong Provincial Key Research and Development Program (2019JZZY010119).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shu Wu is an Associate Professor in Center for</head><p>Research on Intelligent Perception and Computing (CRIPAC). He has published more than 50 papers in the areas of data mining and information retrieval at international journals and conferences, such as IEEE TKDE, WWW, AAAI, SIGIR, ICDM.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mining latent structures for multimedia recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3872" to="3880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recommender Systems: The Textbook</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Survey of Collaborative Filtering Techniques</title>
		<author>
			<persName><forename type="first">X</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page" from="1" to="421" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="144" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepStyle: Learning User Preferences for Visual Recommendation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="841" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attentive Collaborative Filtering: Multimedia Recommendation with Itemand Component-Level Attention</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Session-based Recommendation with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="346" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Light-GCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MMGCN: Multi-modal Graph Convolution Network for Personalized Recommendation of Micro-video</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1437" to="1445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph-Refined Convolutional Network for Multimedia Recommendation with Implicit Feedback</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3451" to="3459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Item-based Collaborative Filtering Recommendation Algorithms</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sarwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Riedl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="285" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sherlock: Sparse hierarchical embeddings for visually-aware one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The development of embodied cognition: Six lessons from babies</title>
		<author>
			<persName><forename type="first">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial life</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="13" to="29" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="776" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName><forename type="first">H</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Audio-visual instance discrimination with cross-modal agreement</title>
		<author>
			<persName><forename type="first">P</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">486</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Birds of a Feather: Homophily in Social Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mcpherson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Smith-Lovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Sociol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="415" to="444" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AM-GCN: Adaptive Multi-channel Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1243" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive Graph Convolutional Neural Networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3546" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Iterative Deep Graph Learning for Graph Neural Networks: Better and Robust Node Embeddings</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaki</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">326</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast Approximate kNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>-R. Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Saad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">69</biblScope>
			<biblScope unit="page" from="1989" to="2012" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>De Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName><forename type="first">T</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">719</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised learning of remote sensing scene representations using contrastive multiview coding</title>
		<author>
			<persName><forename type="first">V</forename><surname>Stojnic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Risojevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1182" to="1191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contrastive multimodal fusion with tupleinfonce</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Separating skills and concepts for novel visual question answering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Whitehead</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5632" to="5641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">BPR: Bayesian Personalized Ranking from Implicit Feedback</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image-Based Recommendations on Styles and Substitutes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3980" to="3990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the Difficulty of Training Deep Feedforward Neural Networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Methods and Metrics for Cold-Start Recommendations</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Schein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Popescul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Pennock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="253" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Selfsupervised multi-channel hypergraph convolutional network for social recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Q V</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="413" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Selfsupervised hypergraph convolutional networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4503" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning visual clothing style with heterogeneous dyadic cooccurrences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4642" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Disentangled item representation for recommender systems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIST</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021-02">Feb. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visually-Aware Fashion Recommendation and Design with Generative Image Models</title>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Personalized Fashion Recommendation with Visual Explanations based on Multimodal Attention Network: Towards Visually Explainable Recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="765" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mv-rnn: A multiview recurrent neural network for sequential recommendation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="331" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Personalized graph neural networks with attention mechanism for session-aware recommendation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Hierarchical fashion graph network for personalized outfit recommendation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="159" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Graph Contrastive Representation Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GRL+@ICML</title>
				<imprint>
			<date type="published" when="2020-06">Jun. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A Graph-based Relevance Matching Model for Ad-hoc Retrieval</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Graph-based Hierarchical Relevance Matching Signals for Ad-hoc Retrieval</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>WWW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning Discrete Structures for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust Graph Representation Learning via Neural Sparsification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">468</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning to Drop: Robust Graph Neural Network via Topological Denoising</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="779" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Topology Optimization based Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4054" to="4061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph Structure Learning for Robust Graph Neural Networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Exploring Structure-Adaptive Graph Learning for Robust Semi-Supervised Classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Differentiable Graph Module (DGM) for Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<title level="m">Deep Graph Structure Learning for Robust Representations: A Survey</title>
				<imprint>
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning Representations by Maximizing Mutual Information Across Views</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">519</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A Framework For Contrastive Self-Supervised Learning and Designing A New Approach</title>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>arXiv.org</idno>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">An Empirical Study of Graph Contrastive Learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning with Adaptive Augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2021-04">Apr. 2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Selfsupervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">S3-rec: Self-supervised learning for sequential recommendation with mutual information maximization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Pre-training graph transformer with multimodal side information for recommendation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Jinghao Zhang is currently pursuing his Ph.D. degree of Computer Science at Center for Research on Intelligent Perception and Computing (CRIPAC) at National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>CRIPAC) at National Laboratory of Pattern Recognition (NLPR</orgName>
		</respStmt>
	</monogr>
	<note>His current research interests mainly include graph representation learning and recommender systems. Yanqiao Zhu is currently pursuing his master&apos;s degree of. Institute of Automation, Chinese Academy of Sciences (CASIA. Previously</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
