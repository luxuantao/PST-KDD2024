<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Signal Processing: Image Communication</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-12-12">12 December 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhigang</forename><surname>Tu</surname></persName>
							<email>tuzhigang@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory of Information Engineering in Surveying, Mapping and Remote sensing</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430079</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Central China Normal University</orgName>
								<address>
									<addrLine>LuoyuRoad 152</addrLine>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dejun</forename><surname>Zhang</surname></persName>
							<email>zhangdejun@cug.edu.cnd.zhang</email>
							<affiliation key="aff2">
								<orgName type="department">School of Information Engineering</orgName>
								<orgName type="institution">China University of Geosciences</orgName>
								<address>
									<postCode>30074</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ronald</forename><surname>Poppe</surname></persName>
							<email>r.w.poppe@uu.nl</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<addrLine>Princetonplein 5</addrLine>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Remco</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
							<email>r.c.veltkamp@uu.nl</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Information and Computing Sciences</orgName>
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<addrLine>Princetonplein 5</addrLine>
									<settlement>Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baoxin</forename><surname>Li</surname></persName>
							<email>baoxin.li@asu.edu</email>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">School of Computing, Informatics</orgName>
								<orgName type="department" key="dep2">Decision System Engineering</orgName>
								<orgName type="institution">Arizona State University</orgName>
								<address>
									<postCode>85287</postCode>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<email>jsyuan@buffalo.edu</email>
							<affiliation key="aff5">
								<orgName type="department">Computer Science and Engineering department</orgName>
								<orgName type="institution">State University of New York at Buffalo</orgName>
								<address>
									<postCode>14260-2500</postCode>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>T I C L E I N F O</surname></persName>
						</author>
						<title level="a" type="main">Signal Processing: Image Communication</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-12-12">12 December 2018</date>
						</imprint>
					</monogr>
					<idno type="MD5">E9CE8FE924E88B75F35B5BAD9EC43D17</idno>
					<idno type="DOI">10.1016/j.image.2018.12.002</idno>
					<note type="submission">Received 17 March 2018; Received in revised form 27 October 2018; Accepted 3 December 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Optical flow Variational method CNN-based method Evaluation measures Challenges</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dense motion estimations obtained from optical flow techniques play a significant role in many image processing and computer vision tasks. Remarkable progress has been made in both theory and its application in practice. In this paper, we provide a systematic review of recent optical flow techniques with a focus on the variational method and approaches based on Convolutional Neural Networks (CNNs). These two categories have led to state-of-the-art performance. We discuss recent modifications and extensions of the original model, and highlight remaining challenges. For the first time, we provide an overview of recent CNN-based optical flow methods and discuss their potential and current limitations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion provides essential information for a wide variety of visual tasks, and directly affects the subsequent image processing. Consequently, the computation of motion information from image sequences is an important issue in computer vision and image processing. Optical flow, one of the most successful motion estimation methods, has been widely investigated. Since Horn and Schunck (HS) <ref type="bibr" target="#b0">[1]</ref> as well as Lucas and Kanade (LK) <ref type="bibr" target="#b1">[2]</ref> proposed the differential method to calculate optical flow in 1981, a great deal of extensions and modifications have been proposed. More recently, a new line of optical flow algorithms considers the use of Convolutional Neural Networks (CNNs). The two classes of algorithm have become the predominant ways to calculate optical flow <ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref>. In this survey, we review both and discuss challenges and opportunities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Optical flow</head><p>The relative movement of a 3D scene observed with a camera leads to changes in the 2D projected images. The estimation of this motion comes down to the computation of a projection of the actual motion onto the image plane <ref type="bibr" target="#b5">[6]</ref>. To this end, algorithms have relied on the analysis of brightness variations of pixels in pairs of subsequent images. The 2D displacement field that describes apparent motion of brightness patterns between two successive images is called the optical flow <ref type="bibr" target="#b0">[1]</ref>.</p><p>The optical flow field is often considered as the projected scene flow field, which is the true motion of the objects in the scene as viewed from the image plane <ref type="bibr" target="#b3">[4]</ref>. The optical flow field is ideally a dense field of displacement vectors (see Fig. <ref type="figure" target="#fig_0">1</ref>), which maps all points of the first image onto their corresponding locations in the second image.</p><p>The concept of optical flow was proposed by Gibson <ref type="bibr" target="#b6">[7]</ref>. Poggio and Reiehardt <ref type="bibr" target="#b7">[8]</ref> presented an approach to compute the motion of each pixel in an image, which can be considered a rough flow method. A first practical optical flow model was established by the classical work of Horn and Schunck (HS) <ref type="bibr" target="#b0">[1]</ref>. It is based on the assumption that the brightness of a pixel keeps constant during a short time interval, which is known as the brightness constancy assumption (BCA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Applications of optical flow</head><p>Although the optical flow field is an approximate projection of the true motion of the scene, it provides valuable information about the spatial arrangement of the viewed objects and the change rate of the arrangement. We discuss several domains where optical flow is used.</p><p>Visual Surveillance. Visual surveillance systems are often designed as modular systems of functional modules such as motion detection, depth estimation, object tracking, and object behavioral analysis <ref type="bibr" target="#b8">[9]</ref>. Optical flow is effective in separating the foreground and background, and to identify moving objects <ref type="bibr" target="#b9">[10]</ref>. This allows for the detection and tracking of objects through the scene. Optical flow is extensively used for visual surveillance tasks including tracking <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, action recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> and anomaly detection <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Robot Navigation. Navigation can be roughly described as the process of determining a suitable path between a robot's start and goal location <ref type="bibr" target="#b18">[19]</ref>. As optical flow is the apparent motion of the brightness patterns of image sequences, it not only provides important information about the unknown environment, but it is also helpful to determine the direction and the speed of the robot. Due to this characteristic, optical flow is one of the two primary techniques for mapless robot navigation <ref type="bibr" target="#b19">[20]</ref>. Using optical flow for robot navigation is inspired by emulating the flying behavior of bees <ref type="bibr" target="#b20">[21]</ref>. Optical flow is widely used for obstacle detection <ref type="bibr" target="#b21">[22]</ref> and collision avoidance <ref type="bibr" target="#b22">[23]</ref>.</p><p>Image Interpolation and Super-Resolution. High definition TVs (HDTVs) have significantly improved our visual experience <ref type="bibr" target="#b23">[24]</ref>. Image sequence interpolation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref> is the process of creating intermediate images between two given consecutive images, to increase the frame rate. It has broad applications in the fields of video post-processing and restoration. Image sequence super-resolution <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> aims at combining the available information from a sequence of low-resolution (LR) images to produce a high-resolution (HR) image with more details. Both image interpolation and super-resolution require accurate and detailed alignment of pixels in which the dense optical flow field plays a crucial role.</p><p>Physical and Metrological Applications. Optical flow can be applied as a measurement in modeling and simulation, including weather forecasting <ref type="bibr" target="#b28">[29]</ref> and fluid dynamics <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Optical flow techniques</head><p>In this section, we introduce the primary optical flow techniques and give a detailed analysis of their characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.1.">Differential technique</head><p>The differential technique also refers to the gradient-based approach. It computes the velocity from spatial and temporal derivatives of the image brightness <ref type="bibr" target="#b3">[4]</ref>. The constraint relies purely on the BCA. This leads to an aperture problem: one equation cannot uniquely determine two unknown components (the horizontal and vertical pixel displacement) of the flow field. An additional constraint encoding a priori information on the flow field needs to be incorporated with the BCA to make the problem well-posed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Normally, the prior takes the form of spatial coherence imposed by either local or global constraints. Generally, we identify two approaches <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>:</p><p>â€¢ Global Differential Method introduces a global smoothness constraint (also called regularization term) to solve the aperture problem. The assumption is that neighboring pixels come from the same object and consequently undergo a similar motion. As a result, the displacement field of the brightness patterns varies smoothly. The variational method proposed by Horn and Schunck <ref type="bibr" target="#b0">[1]</ref> is generally considered as the typical global method. It is one of the most successful techniques to compute optical flow <ref type="bibr" target="#b5">[6]</ref>. Solving the optical flow estimation requires the minimization of the associated Euler-Lagrange equations. These form a system of partial differential equations that can be derived analytically using the calculus of variations <ref type="bibr" target="#b33">[34]</ref>. In practice, the system can be solved with numerical methods such as Successive Over-Relaxation (SOR) <ref type="bibr" target="#b34">[35]</ref>, or efficient multi-grid methods <ref type="bibr" target="#b35">[36]</ref>.</p><p>In general, there are two main ways to minimize the global energy function. One is a continuous optimization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b36">37]</ref> such as gradient descent <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref> and the variational method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>. The second approach is a discrete optimization <ref type="bibr" target="#b39">[40]</ref>, for example using graph cuts algorithms <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> or message-passing algorithms <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>The discrete optimization method approximates the continuous space of solutions and enables a more thorough and complete search of the state space. Since discrete methods do not require differentiation of the energy, they can handle a wider variety of data and regularization terms than continuous methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>On the other hand, discrete optimization methods are generally limited in terms of accuracy and efficiency by the number of labels and the size of the label space <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref>. For optical flow, subpixel accuracy is often required <ref type="bibr" target="#b39">[40]</ref>, and real-time computation is necessary for many applications <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>â€¢ Local Differential Method assumes that the motion within a local neighborhood can be described by a parametric model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>For each pixel in a local neighborhood, an equation relating brightness and flow can be derived. The set of equations for all the pixels within the local neighborhood is then solved to estimate the optical flow by performing least-squares minimization. Setting the proper size of the neighborhood is a challenge. A small-sized neighborhood might not contain sufficient information to handle the aperture problem. In contrast, a large-sized neighborhood allows the integration of information over many pixels but may include pixels from other motion surfaces. Moreover, the local method experiences difficulties in homogeneous regions and regions is motion discontinuities <ref type="bibr" target="#b50">[51]</ref>.</p><p>The global and local methods differ in the interaction of neighboring pixels due to the constraints. Global methods constrain a pixel's flow by its neighboring pixels' flow vectors, whereas local methods constrain a pixel's flow by its neighboring pixels' intensity values. Also, the two methods are computated differently. The global method aggregates the constraint residual over the whole field. Therefore, the flow recovery at one pixel relies on the flow recovery at other pixels. In contrast, the local method integrates the flow constraint of one pixel into a linear system, and solves each pixel's flow independently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.2.">Region-based technique</head><p>The region-based technique relies on searching matching patches between two consecutive images. When two corresponding patches have the largest correlation, the optical flow is defined as the shift of the patches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b51">52]</ref>. Popular measures include the sum of absolute differences (SAD), the sum of squared differences (SSD) <ref type="bibr" target="#b0">[1]</ref>, and crosscorrelation metrics <ref type="bibr" target="#b52">[53]</ref>. The region-based technique is more robust to noise than differential techniques. In addition, the region-based technique works well even when images are interlaced or decimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.3.">Feature-based technique</head><p>Feature-based techniques attempt to link sparse but discriminative image features in successive images over time <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b53">54]</ref>. This technique ignores ambiguous areas and, as a result, the calculated flow field is sparse but robust. Discriminative features, in particular corners and edges, as well as low-contrast features such as flat regions, can be matched to determine the optical flow <ref type="bibr" target="#b54">[55]</ref>. Feature-based methods consists of two steps: feature detection and correspondence matching. Currently, feature-based optical flow methods are widely used, especially for large displacement matching <ref type="bibr" target="#b55">[56]</ref>. The technique has two main drawbacks. First, the optical flow is very sparse when the background or the objects contain features that are not discriminative. Second, the selected features may not be reliable and disappear in subsequent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.4.">Frequency-based technique</head><p>The frequency-based technique, or velocity tuned filters, calculates optical flow using velocity-tuned filters in the Fourier domain. The advantage is that motion-sensitive mechanisms operating on spatiotemporally oriented energy in the Fourier domain can estimate motion that cannot be estimated using matching approaches <ref type="bibr" target="#b56">[57]</ref>. For example, the technique can deal with the motion of random dot patterns. According to the output of the velocity-tuned filters, the frequency-based technique can be classified into two groups:</p><p>â€¢ Energy-Based Method. This method is based on the output energy of the filters <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b57">58]</ref>. The energy of a continuous translation in space is concentrated on a plane in the spatiotemporal frequency domain with the orientation related to the velocity <ref type="bibr" target="#b58">[59]</ref>. The first computational model for the perception of motion <ref type="bibr" target="#b59">[60]</ref> consists of a quadrature Gabor filter pair. Later, the model was extended to compute optical flow <ref type="bibr" target="#b57">[58]</ref>. The optical flow is formulated as the least-squares fit of spatiotemporal energy to a plane in frequency space.</p><p>â€¢ Phase-Based Method. It defines the velocity component in the output of band pass velocity tuned Gabor filters <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b60">61]</ref>. The method is based on the decomposition of the original image into band-pass channels, similar to those produced by quadrature-pair filters in steerable pyramids <ref type="bibr" target="#b61">[62]</ref>. Multi-scale representations are typically used for flow computation. A further decomposition into orientation bands yields more local constraints with a generally higher Signal-Noise-Ratio (SNR). Phase-based methods can be considered superior to the energy-based method in three aspects. First, subpixel accuracy can be achieved. Second, the velocity resolution is preserved by taking responses from neighboring filters. Finally, the technique is more robust to changes in viewpoint and illumination, as phase information is insensitive to changes in speed and contrast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.5.">CNN-based technique</head><p>CNNs have achieved impressive success in a wide variety of image processing tasks, including optical flow estimation. CNNs are increasing used to replace hand-crafted features by learned features <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>. The CNN is applied to extract deep features of the input images. These features are then integrated into common optimization algorithms to calculate optical flow <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66]</ref>. End-to-end methods are one class of approach where the CNN is not only used for learning the image features, but also for matching these features in the images <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b66">67]</ref>. In this way, the whole optical flow process is performed by the CNN (see Fig. <ref type="figure" target="#fig_1">2</ref>). While CNNS are typically trained in a supervised way by providing pairs of subsequent frames and the resulting flow field, recent work has also addressed the unsupervised training of CNNs for optical flow <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Evaluation measures</head><p>Evaluation measures are aimed at revealing properties of optical flow algorithms and helps to better understand the relative strengths and weaknesses of each. They not only provide information to improve the algorithms but also supply insight to select more suitable approaches to handle special challenges such as dealing with noise, large displacements or motion boundaries. In addition, evaluation measures support researchers to establish more realistic and complex benchmarks, and to develop ways to ensure continuous progress. The measurement of the performance of optical flow algorithms remains the subject of scientific debate, and have led to the introduction of various evaluation measures. We discuss the most relevant ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.1.">Mathematical measures</head><p>We define the ground truth (GT) optical flow as ğ‘”(ğ‘¥, ğ‘¦) = (ğ‘¢ ğ‘‡ , ğ‘£ ğ‘‡ ) and the estimated flow as ğ‘’(ğ‘¥, ğ‘¦) = (ğ‘¢ ğ¸ , ğ‘£ ğ¸ ). Some mathematical criteria are proposed to evaluate the performance quantitatively.</p><p>â€¢ Average Angular Error (AAE):</p><formula xml:id="formula_0">ğ´ğ´ğ¸ = 1 ğ‘€ğ‘ âˆ‘ arccos( ğ‘¢ ğ‘‡ ğ‘¢ ğ¸ + ğ‘£ ğ‘‡ ğ‘£ ğ¸ + 1 âˆš (ğ‘¢ 2 ğ‘‡ + ğ‘£ 2 ğ‘‡ + 1)(ğ‘¢ 2 ğ¸ + ğ‘£ 2 ğ¸ + 1) )<label>(1)</label></formula><p>where ğ‘€ and ğ‘ are the numbers of columns and rows of the image, and ğ‘€ğ‘ is the number of pixels. The AAE measure is defined as the flow error deviation of angle between the GT flow and the estimated flow <ref type="bibr" target="#b60">[61]</ref>. The drawback of this measure is that errors in small flows are penalized relatively severely.</p><p>â€¢ Average Endpoint Error (AEE):</p><formula xml:id="formula_1">ğ´ğ¸ğ¸ = 1 ğ‘€ğ‘ âˆ‘ âˆš (ğ‘¢ ğ¸ -ğ‘¢ ğ‘‡ ) 2 + (ğ‘£ ğ¸ -ğ‘£ ğ‘‡ ) 2<label>(2)</label></formula><p>This error measure calculates the Euclidean distance between the endpoints of the GT flow and the estimated flow <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b4">5]</ref>. The AEE discounts errors in regions of small flow while strongly penalizing large estimation errors.</p><p>â€¢ Average Magnitude Error (AME):</p><formula xml:id="formula_2">ğ´ğ‘€ğ¸ = 1 ğ‘€ğ‘ âˆ‘ | âˆš ğ‘¢ 2 ğ¸ + ğ‘£ 2 ğ¸ - âˆš ğ‘¢ 2 ğ‘‡ + ğ‘£ 2 ğ‘‡ |<label>(3)</label></formula><p>The AME is defined as the average magnitude of the velocity difference between the GT flow and the estimated flow <ref type="bibr" target="#b68">[69]</ref>. Since it does not account for errors in direction, it is usually used together with the AAE.</p><p>â€¢ Normalized Average Magnitude Error (NAME):</p><formula xml:id="formula_3">ğ‘ğ´ğ‘€ğ¸ = 1 ğ‘€ğ‘ âˆ‘ | âˆš ğ‘¢ 2 ğ¸ + ğ‘£ 2 ğ¸ - âˆš ğ‘¢ 2 ğ‘‡ + ğ‘£ 2 ğ‘‡ | âˆš ğ‘¢ 2 ğ‘‡ + ğ‘£ 2 ğ‘‡ (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>This measure is defined as the average ratio between the magnitude of the velocity difference and the magnitude of the ground truth. It is somewhat unreliable for small GT vectors <ref type="bibr" target="#b69">[70]</ref>.</p><p>â€¢ Error Normal to the Gradient (ENG):</p><formula xml:id="formula_5">ğ¸ğ‘ğº âŠ¥ ((ğ‘¢ ğ¸ , ğ‘£ ğ¸ ), (ğ‘¢ ğ‘‡ , ğ‘£ ğ‘‡ )) = â€–((ğ‘¢ ğ¸ -ğ‘¢ ğ‘‡ ), (ğ‘£ ğ¸ -ğ‘£ ğ‘‡ ))ğ‘“ âŠ¥ (ğ‘¥, ğ‘¦)â€– (<label>5</label></formula><formula xml:id="formula_6">)</formula><p>where ğ‘“ âŠ¥ (ğ‘¥, ğ‘¦) = (-ğœ• ğ‘¦ ğ¼(ğ‘¥, ğ‘¦), ğœ• ğ‘¥ ğ¼(ğ‘¥, ğ‘¦)) ğ‘‡ . This error measure aims to examine the effectiveness of an algorithm to compensate for the aperture problem <ref type="bibr" target="#b68">[69]</ref>. Recently, it was pointed out that the residual error between the GT and the estimated warped interpolation (ğ¼ ğ‘¤ğ‘ğ‘Ÿğ‘ ), which is interpolated by the estimated optical flow, is also a good way to evaluate the performance <ref type="bibr" target="#b4">[5]</ref>.</p><p>â€¢ Root-Mean-Square (RMS) error:  This error measure is defined as the root-mean-square (RMS) difference between the warped interpolation image ğ¼ ğ‘¤ğ‘ğ‘Ÿğ‘ (ğ‘¥, ğ‘¦) and the GT image ğ¼(ğ‘¥, ğ‘¦) <ref type="bibr" target="#b4">[5]</ref>.</p><formula xml:id="formula_7">ğ‘…ğ‘€ğ‘† = [ 1 ğ‘€ğ‘ âˆ‘ (ğ‘¥,ğ‘¦) (ğ¼ ğ‘¤ğ‘ğ‘Ÿğ‘ (ğ‘¥, ğ‘¦) -ğ¼(ğ‘¥, ğ‘¦)) 2 ] 1âˆ•2<label>(6)</label></formula><p>â€¢ Normalized Interpolation Error (NIE): The NIE between a warped interpolation image ğ¼ ğ‘¤ğ‘ğ‘Ÿğ‘ (ğ‘¥, ğ‘¦) and a GT image ğ¼(ğ‘¥, ğ‘¦) is given by <ref type="bibr" target="#b70">[71]</ref>:</p><formula xml:id="formula_8">ğ‘…ğ‘€ğ‘† = [ 1 ğ‘€ğ‘ âˆ‘ (ğ‘¥,ğ‘¦) (ğ¼ ğ‘¤ğ‘ğ‘Ÿğ‘ (ğ‘¥, ğ‘¦) -ğ¼(ğ‘¥, ğ‘¦)) 2 â€–âˆ‡ğ¼(ğ‘¥, ğ‘¦)â€– 2 + ğœ€ ] 1âˆ•2<label>(7)</label></formula><p>Here, ğœ€ is an arbitrary scaling constant, typically set to 1. Interpolation error measures (Eqs. ( <ref type="formula" target="#formula_7">6</ref>) and ( <ref type="formula" target="#formula_8">7</ref>)) require a robust interpolation algorithm. In addition, they rely on the homogeneity of the scene, as any incorrect flow vector pointing to a location with an identical brightness value is considered incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.2.">Color-coded visualization</head><p>To more vividly perceive the performance of optical flow, color coding approaches were introduced to visualize the optical flow and the error map. Fig. <ref type="figure" target="#fig_0">1(b</ref>) and (c) show two types of flow field. Each provides a qualitative insight of the accuracy of the estimation. The color-coded flow field (Fig. <ref type="figure" target="#fig_0">1(b)</ref>) is a dense visualization of the optical flow field. A color hue is associated to each direction and the saturation of the color increases with the magnitude of the flow vector (see Fig. <ref type="figure" target="#fig_2">3</ref>). The vectorplot flow field (Fig. <ref type="figure" target="#fig_0">1(c</ref>)) directly represents the displacement vectors and provides a good intuitive perception of physical motion.</p><p>Errors in the estimation of the optical flow can also be visualized <ref type="bibr" target="#b71">[72]</ref>. Fig. <ref type="figure" target="#fig_3">4</ref> shows the correct pixels are colored in blue while the error pixels are colored in red. Such visualizations provide a readily understood means of understanding the performance of an optical flow algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Evaluation datasets</head><p>Publicly available datasets with ground truth optical flow allow researchers to compare their novel algorithms to existing work, and understand relative strengths and weaknesses of the methods. There is a steady progression in the size and realism of these benchmark datasets. We briefly discuss here the four most common optical flow datasets. The characteristics of the datasets are compared in Table <ref type="table" target="#tab_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.1.">Middlebury</head><p>The Middlebury benchmark dataset <ref type="bibr" target="#b4">[5]</ref> provides ground truth for both real and synthetic sequences with complex conditions such as motion discontinuities, non-rigid motion, motion blur, and multiple independent motion. It does not contain very large displacements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.2.">MPI-Sintel</head><p>This benchmark contains long photo-realistic sequences with extremely difficult cases <ref type="bibr" target="#b72">[73]</ref>. It includes large motion, specular reflections, motion blur, defocus blur, and atmospheric effects. For evaluation, the basic EPE and some variants derived from EPE are used. The MPI-Sintel protocol measures the error distribution with respect to occlusion and large motion by different thresholds, see Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.3.">Flying Chairs</head><p>The Flying Chairs datset <ref type="bibr" target="#b62">[63]</ref> is sufficiently large to train CNNs for optical flow computation. The dataset consists of 22,872 image pairs with corresponding optical flow fields. Images are collected from Flickr on which the segmented images of chairs from <ref type="bibr" target="#b73">[74]</ref> are overlaid. Random affine transformations are used to chairs and background to acquire the second image and ground truth optical flow fields.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.4.">KITTI</head><p>The KITTI benchmark <ref type="bibr" target="#b74">[75]</ref> consists of real sequences taken from a driving platform in an uncontrolled environment. The scene is much more challenging compared to other benchmarks because it includes non-Lambertian surfaces, different illumination conditions, a large variety of materials and large motion. Recently, background and foreground annotations were provided for the dataset <ref type="bibr" target="#b71">[72]</ref>. A specific evaluation protocol was introduced for the KITTI benchmark. It uses an EPE threshold of ğœ pixels (ğœ âˆˆ (2, â€¦ , 5)), and computes the percentage of pixels whose EPE is above the threshold. With occlusion ground truth, the metric is shown in Table <ref type="table" target="#tab_1">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Variational optical flow technique</head><p>In 1981, Horn and Schunck (HS) <ref type="bibr" target="#b0">[1]</ref> proposed to compute the displacement vector of every pixel by minimizing a global energy function that is a combination of a BCA-based data term and a smoothness term. A smoothness parameter ğœ† was to balance the two terms. Many extensions and modifications of this variational optical flow technique have been proposed <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b76">77]</ref>. According to the most recent survey <ref type="bibr" target="#b4">[5]</ref>, the variational method plays a dominant role in flow computation since almost all top-performing flow algorithms are based on the variational framework <ref type="bibr" target="#b76">[77]</ref>.</p><p>There are three primary advantages of the variational optical flow method when compared to other kinds of optical flow techniques <ref type="bibr" target="#b5">[6]</ref>:</p><p>â€¢ It can integrate different assumptions into a single minimization framework.</p><p>â€¢ It has the filling-in effect which yields a dense flow field, whereas many other techniques require post-processing to interpolate the sparse flow field.</p><p>â€¢ Its energy function can be formulated in such a way that it is invariant under rotations in most cases.</p><p>Additionally, the variational method can generally be implemented to run in real-time using modern numerical approaches <ref type="bibr" target="#b47">[48]</ref>, or advanced computation techniques (FPGA <ref type="bibr" target="#b77">[78]</ref> or GPU <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b78">79]</ref>). We will now introduce the classical variational method and its most important improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The classical variational method</head><p>The fundamental principle behind the variational optical flow method is the BCA, which assumes that the brightness of a moving pixel remains constant over time. Mathematically, it can be formulated as:</p><formula xml:id="formula_9">ğ¼(ğ‘¥, ğ‘¦, ğ‘¡) = ğ¼(ğ‘¥ + ğ‘¢, ğ‘¦ + ğ‘£, ğ‘¡ + 1)<label>(8)</label></formula><p>Where ğ¼ âˆ¶ (ğ›º Ã— ğ‘‡ â†’ â„œ) is a image sequence, ğ¼(ğ‘¥, ğ‘¦, ğ‘¡) is the current frame at time ğ‘¡, ğ¼(ğ‘¥ + ğ‘¢, ğ‘¦ + ğ‘£, ğ‘¡ + 1) is the next frame at time ğ‘¡ + 1.</p><p>(ğ›º â†’ â„œ 2 ) denotes the image domain and ğ‘‡ is the sampled time interval of the sequence. ğ± = (ğ‘¥, ğ‘¦) represents a point in the image domain ğ›º. ğ° = (ğ‘¢, ğ‘£) denotes the flow field, with ğ‘¢ and ğ‘£ the horizontal and vertical flow component, respectively. Eq. ( <ref type="formula" target="#formula_9">8</ref>) produces a difficult optimization problem. HS applied a firstorder Taylor expansion to linearize the right-hand side, which yields the approximation:</p><formula xml:id="formula_10">ğ‘¢ ğœ•ğ¼ ğœ•ğ‘¥ + ğ‘£ ğœ•ğ¼ ğœ•ğ‘¦ + ğœ•ğ¼ ğœ•ğ‘¡ = 0<label>(9)</label></formula><p>This is the well-known optical flow constraint (OFC). Eq. ( <ref type="formula" target="#formula_10">9</ref>) can be reformed as:</p><formula xml:id="formula_11">âˆ‡ğ¼(ğ±)ğ°(ğ±) + ğ¼ ğ‘¡ (ğ±) = 0<label>(10)</label></formula><p>Based on the BCA, the data term is formulated as:</p><formula xml:id="formula_12">ğ¸ ğ· (ğ‘¢, ğ‘£) = âˆ« ğ›º (ğ¼(ğ‘¥, ğ‘¦, ğ‘¡) -ğ¼(ğ‘¥ + ğ‘¢, ğ‘¦ + ğ‘£, ğ‘¡ + 1)) 2 ğ‘‘ğ›º (11)</formula><p>Only the BCA based constraint leads to an undetermined equation system. To overcome the ambiguity, HS proposed a smoothness constraint which dictates that the flow field should vary smoothly as neighboring pixels tend to have similar motion. In other words, the flow variation between a point (ğ‘¥, ğ‘¦) and its neighbors are close to zero. Mathematically, it can be expressed as:</p><formula xml:id="formula_13">|âˆ‡ğ‘¢| 2 + |âˆ‡ğ‘£| 2 = ( ğœ•ğ‘¢ ğœ•ğ‘¥ ) 2 + ( ğœ•ğ‘¢ ğœ•ğ‘¦ ) 2 + ( ğœ•ğ‘£ ğœ•ğ‘¥ ) 2 + ( ğœ•ğ‘£ ğœ•ğ‘¦ ) 2<label>(12)</label></formula><p>According to the smoothness assumption, the smoothness term is defined as:</p><formula xml:id="formula_14">ğ¸ ğ‘† (ğ‘¢, ğ‘£) = âˆ« ğ›º (|âˆ‡ğ‘¢| 2 + |âˆ‡ğ‘£| 2 )ğ‘‘ğ›º<label>(13)</label></formula><p>A global energy function for optical flow is formulated by combining the smoothness term with the data term:</p><formula xml:id="formula_15">ğ¸(ğ‘¢, ğ‘£) = âˆ« ğ›º (ğ¼(ğ‘¥, ğ‘¦, ğ‘¡) -ğ¼(ğ‘¥ + ğ‘¢, ğ‘¦ + ğ‘£, ğ‘¡ + 1)) 2 âŸâââââââââââââââââââââââââŸâââââââââââââââââââââââââŸ ğ·ğ‘ğ‘¡ğ‘ ğ‘‡ ğ‘’ğ‘Ÿğ‘š ğ‘‘ğ›º + ğœ† âˆ« ğ›º (|âˆ‡ğ‘¢| 2 + |âˆ‡ğ‘£| 2 ) âŸââââââââŸââââââââŸ ğ‘†ğ‘šğ‘œğ‘œğ‘¡â„ğ‘›ğ‘’ğ‘ ğ‘  ğ‘‡ ğ‘’ğ‘Ÿğ‘š ğ‘‘ğ›º (14)</formula><p>Here, ğœ† is the parameter that controls the balance between the two terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Modifications and extensions</head><p>The effectiveness of the HS model is constrained to constant illumination, and Lambertian surfaces with full visibility. Realistic videos typically do not meet these requirements. For example, HS is not capable to handle occlusions causes by multiple moving objects <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b80">81]</ref> to deal with motion discontinuities <ref type="bibr" target="#b37">[38]</ref>. Also, illumination changes and motion blur violate the BCA and smoothness assumption <ref type="bibr" target="#b81">[82]</ref><ref type="bibr" target="#b82">[83]</ref><ref type="bibr" target="#b83">[84]</ref>. When displacements are larger than the object structure, matching typically fails due to the limitations of the Taylor expansion and the coarse-to-fine strategy <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b85">86]</ref>. Finally, different types of video frames might have different qualities and a fixed smoothness parameter is, therefore, an unsuitable factor to balance the data term and the smoothness term <ref type="bibr" target="#b86">[87]</ref>.</p><p>Subsequent improvements have been dedicated to remedy the drawbacks of the classical variational method. We discuss the three main categories of improvements subsequently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Data term improvement</head><p>HS utilized a quadratic penalization on the BCA to model the data fidelity. In practice, the BCA can be easily violated due to noise, illumination changes and occlusions. The quadratic penalization function lacks the capability to handle these situations. Various kinds of methods have been proposed to improve the performance of the data term.</p><p>1. Gaussian filtering has been introduced as a preprocessing step for variational methods to improve the performance of the data term with respect to noise <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b81">82]</ref>.</p><p>2. To address illumination changes, three schemes have been proposed:</p><p>â€¢ Robust constancy constraint. Brox et al. <ref type="bibr" target="#b34">[35]</ref> imposed a gradient constancy term into the data term. Papenberg et al. <ref type="bibr" target="#b84">[85]</ref> found that other high-order constancy constraints such as the constancy of the Hessian and the constancy of the Laplacian, are also useful. Zimmer et al. <ref type="bibr" target="#b87">[88]</ref> used the gradient constancy together with the brightness constancy in the HSV color space to handle illumination changes. Xu et al. <ref type="bibr" target="#b76">[77]</ref> pointed out that using either gradient constancy or brightness constancy was more suitable than using both. They presented a binary weight map to switch between the two terms. Generally, higher-order constancy constraints are invariant under additive illumination changes but not invariant to multiplicative illumination changes. Thus, they may not be effective with respect to realistic illumination changes that usually contain a multiplicative part <ref type="bibr" target="#b88">[89]</ref>. Alternatively, Mohamed et al. <ref type="bibr" target="#b89">[90]</ref> proposed an illumination-robust constancy based on local texture features to form a texture constancy constraint for the data term.</p><p>â€¢ Structure-texture decomposition. Inspired by the idea of <ref type="bibr" target="#b90">[91]</ref>, to gain robustness against illumination changes, Wedel et al. <ref type="bibr" target="#b91">[92]</ref> applied the Rudin-Osher-Fatemi (ROF) technique <ref type="bibr" target="#b92">[93]</ref> to decompose the input image into ''structure" and ''texture" components, and to reconstruct the new image as a linear combination of both with emphasis on the ''texture" component. Additive illumination changes are sensitive to structures while the texture part is less affected. A main defect of this method is the potential loss of valid information. Especially when no brightness variations occur, this method removes useful information of the structure component.</p><p>â€¢ Color space. This kind of method aims at handling illumination changes using photometric invariant color spaces. The robustness during brightness variations is achieved by introducing a color channel transformation. The input color images are transformed to a color space where the channels are invariant to illumination changes. The photometric invariants of the HSI color space <ref type="bibr" target="#b88">[89,</ref><ref type="bibr" target="#b93">94]</ref>, the normalized RGB channels <ref type="bibr" target="#b93">[94]</ref>, the spherical space <ref type="bibr" target="#b88">[89]</ref>, and the HSV space <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b82">83]</ref> have been investigated.</p><p>3. Robust penalty functions. The quadratic penalty function used by HS severely penalizes larger displacement errors. To reduce the impact of such local errors that are considered as outliers, robust penalty functions were proposed <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b94">95]</ref>. Common used alternatives are the Charbonnier <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b95">96]</ref>, generalized Charbonnier <ref type="bibr" target="#b75">[76]</ref>, Tukey <ref type="bibr" target="#b96">[97]</ref>, Lorentzian <ref type="bibr" target="#b37">[38]</ref>, Leclerc <ref type="bibr" target="#b97">[98]</ref>, and modified Hampel penalty function <ref type="bibr" target="#b98">[99]</ref>. In a recent survey paper, Sun et al. <ref type="bibr" target="#b99">[100]</ref> suggest to use the Charbonnier or slightly non-convex penalty function. On the other hand, the robust penalty function causes nonlinearity to the data term. The graduated non-convexity (GNC) strategy <ref type="bibr" target="#b100">[101]</ref> aids in improving the performance in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Smoothness term improvement</head><p>The smoothness term aims at smoothing the motion field in regions of coherent motion while preserving discontinuities at motion boundaries. To improve the performance of the smoothness term in discarding outliers, preserving discontinuities and propagating information in low texture areas, numerous improvements have been proposed. A taxonomy of smoothness terms appears in <ref type="bibr" target="#b101">[102]</ref>.</p><p>â€¢ Image-and flow-based terms. The homogeneous smoothness term proposed in the original HS model does not consider flow discontinuities and over-smoothes motion boundaries. Isotropic and anisotropic image-driven smoothness terms to suppress smoothing across image boundaries are introduced in Alvarez et al. <ref type="bibr" target="#b85">[86]</ref> and <ref type="bibr" target="#b102">[103]</ref>, respectively. Schnorr <ref type="bibr" target="#b103">[104]</ref> designed an isotropic flowdriven smoothness term that also avoids over-segmentation of strongly textured structures. Weickert et al. <ref type="bibr" target="#b101">[102]</ref> constructed an anisotropic flow-driven smoothness term that achieves smoother effects along flow discontinuities while producing fewer fluctuations. Zimmer et al. <ref type="bibr" target="#b87">[88]</ref> exploited a joint image-and flowdriven smoothness term to avoid artifacts of over-smoothing and over-segmentation. Recently, the advanced motion and structureadaptive regularization term is widely used for preserving edges <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b87">88]</ref>. This term favors motion discontinuities to coincide with discontinuities of the image structures.</p><p>â€¢ Non-local regularization. The gradient of the flow can only supply a local constraint on the interaction between pixels. Largerange interactions can capture the form of the displacement field more accurately. According to this assumption, non-local smoothness terms have been recently studied in <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b105">106]</ref> by describing the structure of the flow in an extended neighborhood.</p><p>â€¢ Spatio-temporal regularization. Assuming the flow field varies smoothly and gradually over time, the spatial-smoothness term can be extended to the temporal domain. Similar to the spatial case, temporal smoothness can be achieved locally either in terms of the temporal flow gradient <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b94">95]</ref>, or by taking into account temporal coherence by modeling along the trajectory of the moving object <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b107">108]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Smoothness parameter improvement</head><p>Nagel and Enkelmann <ref type="bibr" target="#b108">[109]</ref> suggested to make ğœ† a function of the iteration number, but this proved ineffective. An approach with the blurring operator for choosing ğœ† in ill-conditioned inverse problems has been described in <ref type="bibr" target="#b109">[110]</ref>. Ng and Solo <ref type="bibr" target="#b110">[111]</ref> replaced the blurring operator by the Euclidean distance. These brute-force methods that search for the smoothness parameter are computationally expensive. Zimmer et al. <ref type="bibr" target="#b81">[82]</ref> presented an optimal prediction principle (OPP) to automatically determine the optimal ğœ†, but it is limited to constant speed and linear motion. Different from these methods which aim to find a globally optimal ğœ†, are local fusion methods that estimate the appropriate ğœ† for each point independently. Raket <ref type="bibr" target="#b111">[112]</ref> fuse flows of different smoothness parameters. The local evaluation in terms of the best data fit on the gradient image is easily affected by illumination changes, and is sensitive to noise and discontinuities. Tu et al. <ref type="bibr" target="#b112">[113]</ref> address this issue evaluating the local interpolation error in terms of weighted L1 block match on the corresponding set of intensity images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenges</head><p>Despite considerable progress over the past decades, there are still unsolved challenges for optical flow methods. These include dealing with occlusions, preserving motion boundaries, dealing with large displacements, removing outliers, handling textureless areas and rotation. We discuss these challenges below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Occlusions</head><p>Occlusion is a common phenomenon when dealing with realistic scenes. One view is that occlusion is produced due to layers that ordered in depth <ref type="bibr" target="#b113">[114]</ref>. A point is then occluded if it switches from one motion layer to another between consecutive frames <ref type="bibr" target="#b114">[115]</ref>. Often, occlusions are an explanation of optical flow mismatching <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b114">115]</ref>. Violations of the BCA of optical flow accumulate in occluded areas <ref type="bibr" target="#b115">[116]</ref>. A brightness change between corresponding points in subsequent images denotes an occlusion of the point under the assumptions of Lambertian reflection and constant illumination <ref type="bibr" target="#b116">[117,</ref><ref type="bibr" target="#b80">81]</ref>.</p><p>Occlusion can be classified into three categories. First, self occlusion occurs when one part of an object occludes another. Second, inter-object occlusion occurs when two moving objects occlude each other. Finally, background occlusion occurs when a structure in the background occludes moving objects.</p><p>To increase the robustness of the data term to occlusions, Black and Anandan <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b94">95]</ref> replaced the quadratic function with a robust penalty function. Also, some more effective regularizers have been presented <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b103">104,</ref><ref type="bibr" target="#b117">118]</ref>. These methods aim to reduce the errors caused by occlusions, but still fail when pixels fully disappear. Recently, researchers suggested to explicitly deal with occlusions in a two-steps manner. First, to detect occlusions and then to process the occluded areas <ref type="bibr" target="#b75">[76]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Occlusion detection</head><p>Occlusion detection is crucial in the estimation of optical flow as it is easier to compute optical flow if the occluded regions were known. Many methods have been proposed to detect occlusions. Some approaches analyze the mismatch between the forward and the backward flow to detect occlusions <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b118">119,</ref><ref type="bibr" target="#b119">120]</ref>. Others use the residual from optical flow estimation to determine whether a region is occluded <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b120">121,</ref><ref type="bibr" target="#b116">117]</ref>. Some approaches exploit features of the optical flow field, such as edges <ref type="bibr" target="#b121">[122,</ref><ref type="bibr" target="#b122">123]</ref>, motion cues <ref type="bibr" target="#b123">[124,</ref><ref type="bibr" target="#b124">125]</ref> and depth <ref type="bibr" target="#b124">[125]</ref>. Confidence measures have also been proposed to detect occlusions <ref type="bibr" target="#b125">[126,</ref><ref type="bibr" target="#b54">55]</ref>. A promising avenue for improvement is in the combination of optical flow features such as motion constancy <ref type="bibr" target="#b126">[127]</ref> with image features (e.g. brightness, color, texture, depth) <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b123">124,</ref><ref type="bibr" target="#b126">[127]</ref><ref type="bibr" target="#b127">[128]</ref><ref type="bibr" target="#b128">[129]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Detailed introduction and analysis</head><p>Thompson <ref type="bibr" target="#b129">[130]</ref> applied a boundary detection scheme to classify boundaries into an occluding group and an occluded group. This distinction is then used to alleviate the impact of occlusions by a flow projection strategy. Alvarez et al. <ref type="bibr" target="#b79">[80]</ref> checked the consistency between the forward and backward flow with a threshold to detect occlusions, and then ignored the occlusions by resetting the detected pixels to zero in the post-processing phase. Xiao et al. <ref type="bibr" target="#b116">[117]</ref> modified <ref type="bibr" target="#b79">[80]</ref> by reformulating the energy function by explicitly introducing an occlusion term to balance the energy loss due to occlusion. A multi-cue driven bilateral filter was exploited to substitute the original anisotropic filter. This novel filter can adaptively control the diffusion process according to the detected occlusion information and can thus avoid diffusing flow across occlusion boundaries. Sand and Teller <ref type="bibr" target="#b120">[121]</ref> improved <ref type="bibr" target="#b116">[117]</ref> by utilizing an occlusion-aware bilateral filter, which combines the flow divergence and pixel projection difference to detect occlusions. Occlusions were tackled by deleting and creating particles in disoccluded areas. Ince et al. <ref type="bibr" target="#b119">[120]</ref> proposed a variational formulation to jointly compute optical flow, implicitly detect occlusions and extrapolate optical flow in occluded areas. This approach permits interaction between optical flow estimation and occlusion detection. Ayvaci et al. <ref type="bibr" target="#b80">[81]</ref> treated residuals of occlusion and non-occlusion separately. Rua <ref type="bibr" target="#b114">[115]</ref> exploited a local spatio-temporal image-reconstruction model that only needs the information of a plausible motion captured from the image pair for occlusion detection. In this manner, occlusions can be more directly identified without the requirement of a precomputed dense motion field. Zhang et al. <ref type="bibr" target="#b130">[131]</ref> proposed to detect occlusions by comparing the brightness changes of the triangulation and embedded pixels between successive frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Over-smoothing</head><p>Imposing smoothness is one approach to cope with the aperture problem but comes at the cost of smoothing motion boundaries. Techniques have been presented to allow piecewise smoothing while preserving discontinuities by improving the regularization term to better fit the flow field. In addition to the measures discussed in Section 2.2.2, we summarize several other effective approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Statistical learning-based approaches</head><p>Learning the statistics of the flow field and utilizing it as a prior to regularize the flow field can improve the overall flow accuracy because prior information is useful to reduce smoothness in textured regions and discontinuities. Roth and Black <ref type="bibr" target="#b44">[45]</ref> learned spatial smoothness by using Field-of-Experts. Sun et al. <ref type="bibr" target="#b117">[118]</ref> learned statistical models of both data constancy error and image structure-adaptive flow derivatives. Shen and Wu <ref type="bibr" target="#b131">[132]</ref> introduced a sparsity prior to calculate optical flow. This novel measure is effective in preserving motion discontinuities because these can be accurately estimated by finding the sparsest representation of the flow field. Jia et al. <ref type="bibr" target="#b132">[133]</ref> extended this approach into a learned sparse model (LSM), which obtains combines first-order spatial regularity and the learned, higher-order, sparse model. Moreover, it does not need separate regularization of smooth motions and motion discontinuities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Segmentation-based approaches</head><p>Coupling optical flow estimation with segmentation can offer better support for the preservation of flow discontinuities. Decomposing the image sequence into coherently moving objects or different layers would greatly simplify the optical flow solution. This is because it is able to work in regions that do not contain discontinuities, and also motion boundaries are avoided. Memin et al. <ref type="bibr" target="#b133">[134]</ref> presented a simple mechanism that enables interaction between a dense discontinuitypreserving estimation process and a segmentation process. The joint estimation-segmentation model introduces a mixture between local smoothness and region-wise parameterization, which is beneficial for flow field discontinuity preservation. Amiaz and Kiryati <ref type="bibr" target="#b134">[135]</ref> proposed an algorithm that embeds the energy function of <ref type="bibr" target="#b84">[85]</ref> within an active contour segmentation framework. It obtains the accurate performance as <ref type="bibr" target="#b84">[85]</ref> while producing sharper flow boundaries. Xu et al. <ref type="bibr" target="#b135">[136]</ref> introduced a modified segmentation-embedded optical flow framework that accommodates the parametric and segmented motion estimation in a variational model to handle discontinuities. Lara et al. <ref type="bibr" target="#b136">[137]</ref> designed a novel model of localized layers to enhance the performance of optical flow computation with respect to semantic image segmentation. By using layered optical flow in constrained regions around objects of interest, the localized layer model overcomes the drawback of the typically global layered model which is unable to represent complex motion boundary relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Large displacements</head><p>For the variational algorithm, the data term is usually linearized by Taylor expansion. This approximation performs badly when dealing with large non-linear displacements because the linearization is only valid for velocities of limited magnitude <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b137">138]</ref>. Additionally, there is the increased risk of getting trapped in a local minimum when the displacement increases. We introduce techniques to handle this problem in three aspects.</p><p>One common way to deal with large displacements is to embed the estimation in a coarse-to-fine or warping framework <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b138">139]</ref>. In such a framework, downsampled pyramid images are produced. A flow increment is estimated between the first image and the warped second image at the current scale. Then the accumulated flow field is propagated to the next finer level until the finest level is reached <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b139">140]</ref>. Anandan <ref type="bibr" target="#b140">[141]</ref> introduced the unidirectional coarse-to-fine scheme to the variational model to calculate dense flow fields. Starting with Black and Anandan <ref type="bibr" target="#b37">[38]</ref>, the unidirectional coarse-to-fine strategy combined with other techniques became more widely used in variational algorithms <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b91">92]</ref>. Bruhn et al. <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b141">142]</ref> and Zimmer et al. <ref type="bibr" target="#b81">[82]</ref> proposed an advanced coarse-to-fine warping technique: bidirectional multigrid scheme. This strategy is useful but also introduces some issues:</p><p>â€¢ The interpolation to resize image and flow introduces errors.</p><p>Larger artifacts are produced on the next finer scale when upsampling flows at occlusion boundaries <ref type="bibr" target="#b33">[34]</ref>.</p><p>â€¢ Selecting a suitable downsampling factor is non-trivial but affects the overall-performance of the algorithm to a large extent <ref type="bibr" target="#b85">[86]</ref>.</p><p>â€¢ Oversmoothing of fine structures and failure to capture small-scale and fast-moving objects <ref type="bibr" target="#b142">[143]</ref> is a significant risk. Downsampling reduces the size of the image and the displacements. If the object size is smaller than its displacement, it is likely to be smoothed out at coarse level and will not be recovered accurately (see Fig. <ref type="figure" target="#fig_4">5</ref>).</p><p>To remedy these drawbacks, Alvarez et al. <ref type="bibr" target="#b102">[103]</ref> did not conduct linearization to allow for larger displacement. A linear scale-space focusing scheme was exploited for avoiding convergence to incorrect local minima. This method only partially handles the large displacement problem as it still depends on sub-sample warping. Steinbruecker et al. <ref type="bibr" target="#b137">[138]</ref> presented a strategy to avoid both linearization and warping. Their algorithm requires an exhaustive search for pixel-level candidate matching, which is computationally expensive and makes the estimation intractable.</p><p>Recently, there has been a great interest in using feature matching to address the issue of large displacements. Two main directions have been exploited in literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">Integrating feature matching into the energy function</head><p>The SIFT-flow method <ref type="bibr" target="#b143">[144]</ref> employed SIFT <ref type="bibr" target="#b144">[145]</ref> descriptors to compute a scene flow field. Since this method fully depends on SIFT descriptors, it has difficulty estimating small-motion regions <ref type="bibr" target="#b145">[146]</ref>.</p><p>Inspired by <ref type="bibr" target="#b146">[147]</ref>, Brox et al. <ref type="bibr" target="#b2">[3]</ref> added a feature matching constraint into the classical energy function. In this way, correspondences from descriptor matching are obtained, which are helpful to support the coarse-to-fine strategy in avoiding local minima. This method has some defects, for example, the local descriptors are reliable only at salient locations, and false matches are typically generated. To handle these issues, subsequent modifications of <ref type="bibr" target="#b2">[3]</ref> have been proposed. Stoll et al. <ref type="bibr" target="#b147">[148]</ref> presented an adaptive integration strategy to reduce false point matches. Zin <ref type="bibr" target="#b148">[149]</ref> applied segment matching <ref type="bibr" target="#b149">[150]</ref> to modify the matching component. The matching term is modified to deal with weakly localized line features. Weinzaepfel <ref type="bibr" target="#b145">[146]</ref> proposed a DeepFlow algorithm which involved dense and deformable matching, to gain performance for fast motion. Revaud et al. <ref type="bibr" target="#b150">[151]</ref> improved the performance of <ref type="bibr" target="#b145">[146]</ref> by replacing the HOG descriptor in the matching term of <ref type="bibr" target="#b2">[3]</ref>. Their DeepMatching algorithm is able to handle non-rigid deformations explicitly. In addition, it is robust to repetitive or weak textures by incorporating a multiscale scoring of the matches. Since DeepMatching is based on gradient histograms, it is insensitive to appearance changes that arise from illumination and color variations. Kroeger et al. <ref type="bibr" target="#b151">[152]</ref> applied inverse search for fast patch correspondences. This produces a dense displacement field in terms of patch aggregation along multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Fusing matching information with variational flow</head><p>Lempitsky et al. <ref type="bibr" target="#b45">[46]</ref> is among the first to apply fusion to improve optical flow estimations. They proposed to fuse flow candidates obtained from different flow methods or a single flow method with different parameter settings. Derived from this idea, sparse matches and approximate nearest neighbor fields (ANNF) have become popular.</p><p>Xu et al. <ref type="bibr" target="#b76">[77]</ref> applied sparse SIFT matching and PatchMatch <ref type="bibr" target="#b152">[153]</ref> to compute flow candidates. When fusing them with the initial subpixel dense optical flow, much more accurate results can be obtained. Revaud et al. <ref type="bibr" target="#b153">[154]</ref> used a sparse set of matches to construct a dense correspondence field by performing a sparse-to-dense interpolation. This method, EpicFlow, is not only fast but it is also robust to motion boundaries and large displacements. However, it is vulnerable to input matching noise. To deal with this issue, Hu et al. <ref type="bibr" target="#b154">[155]</ref> proposed a robust interpolation method. By assuming the flow in each superpixel satisfies a piecewise affine model, the optical flow can be robustly estimated. Li <ref type="bibr" target="#b155">[156]</ref> exploited a pyramid gradient matching method to produce robust dense matches for optical flow computation. Due to the efficient and scalable nature of the gradient image, the method is fast and accurate.</p><p>ANNF has been used by Chen et al. <ref type="bibr" target="#b138">[139]</ref> to compute an initial motion field which contains different types of correspondence information, and then fuse these with variational flow candidates for refinement. The use of ANNF comes with two issues. First, there are usually many outliers that are hard to identify because there is no regularization. Second, ANNF is computational inefficient. There have been several modifications to remedy these issues.</p><p>Lu et al. <ref type="bibr" target="#b156">[157]</ref> exploited a generic PatchMatch filter method to handle multi-labeling problems efficiently using fast superpixel-based search strategies. Not only the computational cost is significantly reduced but also accurate optical flow is obtained as the generated ANNF is edge-aware. Bailer et al. <ref type="bibr" target="#b157">[158]</ref> presented a novel purely data based hierarchical correspondence field search strategy which is able to not only find most inliers but also to reject outliers effectively. Bao et al. <ref type="bibr" target="#b78">[79]</ref> exploited a fast randomized edge preserving ANNF. By propagating self-similarity patterns similar to <ref type="bibr" target="#b158">[159]</ref>, the computational time is significantly decreased. Hu et al. <ref type="bibr" target="#b159">[160]</ref> proposed CPM (Coarse-to-fine PatchMatch) to speed up the computation by combining an efficient random search strategy with a coarse-to-fine scheme. As a propagation step with constrained random search radius between successive levels on the pyramid architecture was used, outliers are filtered and the correspondences are smoothed.</p><p>How to optimally fuse the matching information with the variational flow is the topic of ongoing research. The common fusion method, quadratic pseudo-boolean optimization (QPBO) <ref type="bibr" target="#b160">[161,</ref><ref type="bibr" target="#b161">162]</ref>, has some drawbacks. The submodularity constraint imposed on the pairwise terms is hard to fully meet, and the constraint is not suitable for variational optical flow models. Tu et al. <ref type="bibr" target="#b112">[113]</ref> proposed a weighted local intensity fusion (WLIF) approach to evaluate the local interpolation error with respect to L1 block match on the corresponding set of images. An adaptive weight based on selective gradient magnitude is exploited to handle outliers and occlusions. However, WLIF is not robust to occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Outliers</head><p>Outliers, in both the images (e.g. image noise) and the intermediate flow fields (e.g. flow errors <ref type="bibr" target="#b37">[38]</ref>) directly affect the accuracy of the final optical flow. One flow outlier is able to influence its neighboring flow vectors due to the smoothness operation and the derivative computation. Even worse, these errors will be accumulated and diffused during numerical iteration. There are three major types of techniques aim to deal with this problem. One technique that uses penalty functions to increase the robustness to outliers, has be introduced in Section 2.2.1. In the following, we focus on the other two techniques: denoising and joint flow computation and outlier removing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Denoising</head><p>Variational algorithms rely on spatiotemporal derivatives, but this process enhances noise. Fermuller et al. <ref type="bibr" target="#b162">[163]</ref> pointed out that noise causes a bias to underestimate not only the lengths but also the directions of flow vectors. Therefore, denoising plays a crucial role in optical flow estimation <ref type="bibr" target="#b163">[164]</ref>. We divide denoising approaches into three categories.</p><p>â€¢ Image denoising. Filters are applied to the input images to remove noise and other high frequencies, while favorable structures of the images are enhanced. Generally, denoising can be achieved by averaging pixels locally or non-locally. Gaussian filtering is a basic local linear filtering scheme. The image is convolved with spatial or spatiotemporal Gaussian kernels with a predefined standard deviation <ref type="bibr" target="#b0">[1]</ref>. Weber et al. <ref type="bibr" target="#b164">[165]</ref> modified the method to convolve the image with a set of linear, separable spatiotemporal kernels. The Laplacian filter was used by Lempitsky et al. <ref type="bibr" target="#b165">[166]</ref>. Song et al. <ref type="bibr" target="#b166">[167]</ref> proposed a two-step denoising approach. In the first step, the Gaussian filter is employed to reduce high-frequency noise and in the second step, a generalized regularization method is applied for deblurring. Due to the high efficiency and edge-aware smoothing of guided image filtering (GIF) <ref type="bibr" target="#b167">[168]</ref>, Tu et al. <ref type="bibr" target="#b168">[169]</ref> exploited a novel adaptive guided image filter to correct errors caused by outliers in the warped interpolation image during numerical computation to boost the accuracy of optical flow estimation. Buades et al. <ref type="bibr" target="#b169">[170]</ref> proposed a non-local filtering approach to remove noise by averaging pixels in a non-local area. Liu and Freeman <ref type="bibr" target="#b170">[171]</ref> designed an adaptive denoising framework that incorporates robust optical flow into a non-local form to remove incidental and structured noise introduced by digital cameras.</p><p>â€¢ Flow denoising. Denoising the flow field is an effective way to improve the performance of optical flow computation in three different ways. First, it removes noise and outliers in the flow field. Second, it preserves flow edges without over-smoothing. And finally, it propagates valid flow information into low texture areas. We discuss several techniques.</p><p>-Median filtering (MF). With MF, outliers can be well discarded <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b171">172,</ref><ref type="bibr" target="#b172">173]</ref>. Sun et al. <ref type="bibr" target="#b75">[76]</ref> formulated the MF heuristic as a non-local term <ref type="bibr" target="#b169">[170]</ref> in the objective function, and modified the performance by integrating information about image structure and flow boundaries into a weighted non-local term (a weighted median filter (WMF)) to prevent over-smoothing. The color similarity measure, the most powerful cue of the WMF, can be easily perturbed by noisy pixels. To increase the robustness of the WMF to noise, Tu et al. <ref type="bibr" target="#b172">[173]</ref> presented a PatchWMF method, which introduces the idea of non-local patch denoising to compute the color similarity in terms of patch difference. In addition, they designed an improved color patch similarity measure.</p><p>-Bilateral filtering (BF). BF is a non-linear, edge-preserving and noise-reducing smoothing filter. The output of each pixel is calculated as a weighted average of intensity values in a local neighborhood <ref type="bibr" target="#b173">[174]</ref>. Xiao et al. <ref type="bibr" target="#b116">[117]</ref> replaced the traditional anisotropic diffusion process with a multi-cue driven BF and separated the variational method into two update stages. Similar to MF, <ref type="bibr" target="#b116">[117]</ref> is able to adaptively control the diffusion process according to the detected occlusion information, image intensity and motion dissimilarity. However, this method is computationally expensive. Sand and Teller <ref type="bibr" target="#b120">[121]</ref> improved its efficiency by applying the BF only near flow boundaries. Tu et al. <ref type="bibr" target="#b139">[140]</ref> proposed a combined post-filtering method that first classifies the flow field into edges, occlusions, and flat regions. Each region is smoothed using a weighted median filter (WMF), bilateral filter (BF) and fast median filter (MF), respectively. A hybrid gradient bilateral filter and Gaussian filter (HGBGF) are exploited to further solve the low efficiency issue of BF.</p><p>-Kalman filtering (KF). KF allows for the estimation to be incrementally updated with each measurement. The velocity measurements of each frame can be integrated with the previous ones and uncertain measurements can be attached to the velocity of each point. This information is useful to reduce outliers. Additionally, more accurate estimation can be achieved by repeating the measurements <ref type="bibr" target="#b174">[175]</ref>. Recently, Rabe <ref type="bibr" target="#b175">[176]</ref> combined KF with the variational approach to establish a temporal smoothness of the dense 3D motion field.</p><p>â€¢ Joint flow computation and outliers removing. In this technique, outliers are explicitly considered in the energy function and handled during optimization. By parameterizing the appearance of each frame as a function of both the pixel motion and the motioninduced blur, Portz et al. <ref type="bibr" target="#b176">[177]</ref> improved the data term to compute optical flow in the presence of spatially varying motion blur. Tu et al. <ref type="bibr" target="#b83">[84]</ref> modified the efficiency of <ref type="bibr" target="#b176">[177]</ref> by designing a downsample interpolation technique during the blur detection, and applied an edge-preserving regularization approach. Recently, Tu et al. <ref type="bibr" target="#b177">[178]</ref> proposed a new hybrid objective energy function that jointly computes optical flow and restores images with preserved edges by alternately minimizing an optical flow module and a denoising restoration module. These methods face one common issue that deblurring or denoising the images during optimization is not always beneficial for boosting the performance as some useful details would be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Textureless</head><p>The homogeneous regularization term of the HS model leads to flow vectors with constant smoothing over the flow field. However, if the images contain regions with weak textures, the variational methods fails to produce correct optical flow due to the poor image derivatives information <ref type="bibr" target="#b178">[179]</ref>.</p><p>Two kinds of techniques are introduced to handle the issue of textureless. First, the regularization term can be improved. In <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b101">102]</ref>, the regularization term was modified by integrating temporal information. Trobin et al. <ref type="bibr" target="#b179">[180]</ref> proposed a regularization term based on the decorrelated second-order derivatives to avoid the total variation (TV) induced staircase effects, especially on weakly textured regions. Xu et al. <ref type="bibr" target="#b135">[136]</ref> regularized affine parameters and the segmentation information into a variational model. Muller et al. <ref type="bibr" target="#b180">[181]</ref> exploited a modified TV regularization term with stereo and feature information. Palomares et al. <ref type="bibr" target="#b181">[182]</ref> introduced a Non-Local Total Variation (NLTV) regularizer to incorporate low-level image segmentation in a unified variational architecture. In this manner, motion from textured areas is propagated to the untextured regions.</p><p>The second approach to improve optical flow estimation in weakly textured regions is by integrating feature term. Revaud et al. <ref type="bibr" target="#b150">[151]</ref> introduced DeepMatching into the matching term of the variational energy function. DeepMatching is robust to weak textures because it uses a multiscale scoring of the matches. By considering patches at multiple scales, the lack of distinctiveness that affects small patches can be overcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Rotation</head><p>Image rotation is common but is a challenging problem in optical flow estimation. Three kinds of methods deal with this problem:</p><p>â€¢ Modified Derivatives. Numerical methods to calculate image derivatives lack rotational invariancy. Freeman and Adelson <ref type="bibr" target="#b61">[62]</ref> proposed a steerable filters technique to compute the directional derivatives in a straightforward way. Niu <ref type="bibr" target="#b182">[183]</ref> presented a modified strategy to compute the directional derivatives based on their original definition. Aubert and Kornprobst <ref type="bibr" target="#b183">[184]</ref> computed the gradient with neighbors and applied a weighted factor to control the significance of the directional derivatives.</p><p>â€¢ Modified coordinate systems. Smoothness constraints regularize each pixel's flow by its neighboring flow vectors. However, the horizontal-vertical coordinate system often fails to represent the intrinsic image structures. For example, a pixel and its horizontal and vertical neighbors have a different velocity during rotation. Various coordinate systems were constructed to handle this problem. Ho and Goecke <ref type="bibr" target="#b184">[185]</ref> transferred the horizontalvertical coordinate system into the log-polar coordinate system. Niu et al. <ref type="bibr" target="#b185">[186]</ref> designed an adaptive local structure-oriented coordinate system.</p><p>â€¢ Modified constraints. Since the traditional data and smoothing terms do not consider the rotation, additional constraints can be integrated into the function. Liu et al. <ref type="bibr" target="#b143">[144]</ref> used SIFT, which is rotation and scale invariant, into the data term to handle rotation. Other scale invariant descriptors <ref type="bibr" target="#b186">[187,</ref><ref type="bibr" target="#b187">188]</ref> are also exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CNN-based technique</head><p>CNN-based Technique learns to extract deep features from input images. For a novel test image pair, optical flow is then calculated by an optimization of the learned features. Inspired by the recent success of deep convolutional neural networks (CNNs) in many computer vision and image processing tasks, Dosovitskiy et al. <ref type="bibr" target="#b62">[63]</ref> were among the first to present a CNN architecture called FlowNet to learn optical flow directly from synthetic annotated data. Its efficiency is several orders of magnitude higher than the variational method. This landmark achievement encouraged subsequent supervised, unsupervised, and semi-supervised CNN-based methods for optical flow estimation. Currently, CNN-based algorithms provide a promising alternative to the variational method. Because its very different nature, we first outline the major differences between the two classes of technique. We then discuss supervised, unsupervised and semi-supervised methods in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Differences between CNN-based and variational methods</head><p>We compare CNN-based and variational methods by discussing the relative advantages and disadvantages of CNN-based methods.</p><p>One of the main advantages of CNN-based approaches is that they are more flexible in the image features they use for optical flow estimation. Multi-layer and hierarchical architectures enable CNNs to extract more abstract, deeper, and multiscale features. Also, CNNs can model complex, non-linear transformations between the input images and the estimated flow field. Overall, the stochastic minimization of the loss across an entire training dataset avoids some of the pitfalls of optimizing a complex energy-function on individual inputs in variational methods <ref type="bibr" target="#b188">[189]</ref>. As a result, fewer assumptions on the input data need to be introduced. Importantly, CNN-based optical flow algorithms are must faster than variational methods.</p><p>CNN-based methods also have several drawbacks. Given that the parameters of CNNs are learned from training data, the performance of the CNN-based technique heavily depends on the quality and size of this labeled training dataset. It is difficult to obtain dense ground truth labelings for real scenes. Therefore, datasets with synthetic, rendered, sequences have been introduced. While these contain realistic motions, they do not reflect the complexity of realistic photometric effects such as image noise, illumination changes, shading and motion blur.</p><p>CNNs contain many parameters, typically in the order of millions. This means that there is a significant risk of overfitting, especially given the difficulty of obtaining suitable training data. Also, this causes CNNs to have a large memory footprint. The large number of parameters also hinders intuitive understanding of the inner workings of the network <ref type="bibr" target="#b189">[190]</ref>. Finally, several hyperparameters significantly affect the efficacy and efficiently of the learning process. For example, it is hard to set a good loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Supervised methods</head><p>Supervised methods are the predominant way of training CNNs capable of optical flow estimation. Their performance, both in terms of accuracy and efficiency, is typically good. However, supervised methods require a large amount of ground-truth optical flow to train the parameters of the CNNs. Also, there is no guarantee that the trained models generalize to different scenarios. Here, we distinguish between end-to-end methods that perform both feature extraction and matching, and methods that only perform one of these tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">End-to-end</head><p>Dosovitskiy et al. <ref type="bibr" target="#b62">[63]</ref> exploited an end-to-end CNN architecture, FlowNet, to compute optical flow. Two CNN networks, FlowNetS and FlowNetC, are constructed based on the U-Net denoising autoencoder. The CNNs are trained in a supervised way on the FlyingChairs dataset they published. The optical flow estimation can be performed efficiently, but the quality of the results are low and the network is constrained for a specific image resolution. Moreover, the EPE loss function that is used in the training procedure ignores the relation between image pairs and optical flow. Based on the architecture of <ref type="bibr" target="#b62">[63]</ref>, Mayer et al. <ref type="bibr" target="#b190">[191]</ref> constructed a more sophisticated synthetic dataset with 3D scene information to train CNNs for optical flow estimation.</p><p>Many adaptations and extensions of FlowNet <ref type="bibr" target="#b62">[63]</ref> have introduced to reduce the size of the trained CNN model, improve the running efficiency, and enhance the accuracy of the optical flow estimation. By taking into account the constraint relationship between image pairs and optical flow during training, Xiang et al. <ref type="bibr" target="#b191">[192]</ref> exploited an additional supervised term in their loss function. Ranjan and Black <ref type="bibr" target="#b192">[193]</ref> designed a Spatial Pyramid Network (SPyNet) following the coarse-to-fine idea of the variational method to deal with large displacements. At each pyramid level, a convolutional network is learned to compute optical flow, which is then up-sampled to the next level to guide the warping of the second image towards the first. Compared to FlowNet, there are significantly less model parameters and the results are more accurate. However, its accuracy is lower than FlowNetS <ref type="bibr" target="#b62">[63]</ref>. Extending FlowNet by stacking multiple encoder-decoder networks resulted in FlowNet 2.0 <ref type="bibr" target="#b66">[67]</ref>. This network reduces the estimation error by more than 50%. The success is partly due to the introduction of a subnetwork focused on small, subpixel motion. FlowNet 2.0 has three limitations. First, its subnetworks need to be trained sequentially to reduce over-fitting. Second, the model size is large (over 160 M parameters) which is not suitable for mobile and embedded devices. Third, FlowNet 2.0 is slower than FlowNet.</p><p>Inspired by FlowNet <ref type="bibr" target="#b62">[63]</ref> and the coarse-to-fine pyramid strategy, several more effective methods are proposed <ref type="bibr" target="#b193">[194,</ref><ref type="bibr" target="#b194">195]</ref>. Sun et al. <ref type="bibr" target="#b193">[194]</ref> proposed PWC-Net, which is based on simple and well-established principles such as pyramidal processing, warping, and cost volume. In contrast to FlowNet 2.0, the model size of PWC-Net is about 17 times smaller, leading to an application real-time. Hui et al. <ref type="bibr" target="#b194">[195]</ref> improved the current CNN frameworks in other two aspects. First, they exploit a more effective cascaded flow inference strategy. Second, a flow regularization layer is proposed to reduce outliers and compress vague flow boundaries. Zhao et al. <ref type="bibr" target="#b195">[196]</ref> presented a Multi-Scale Correspondence Structure Learning (MSCSL) approach to jointly model the multi-scale correspondence structure in a flexible architecture using a Spatial Conv-GRU neural network based on multi-level deep features.</p><p>To handle the black-box nature of the CNN-based methods, and to understand the trained network is/is not suit to work on which cases, Ilg et al. <ref type="bibr" target="#b189">[190]</ref> studied methods to estimate the uncertainty of deep regression networks for optical flow. On contribution is that they try to answer an open question which approaches can be used for uncertainty estimation. The other contribution is that they exploit a multi-hypotheses network produces multiple hypotheses in one singlenetwork without the need of sampling or ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Not end-to-end</head><p>We distinguish between CNNs used to learn deep features from the input images and CNNs employed to predict optical flow with the acquirements of pre-processing and/or post-processing. In the first category, Gadot and Wolf <ref type="bibr" target="#b196">[197]</ref> used a Siamese CNN to compute the descriptors per pixel of each image independently. The PatchMatch method <ref type="bibr" target="#b152">[153]</ref> is used on top of the abstracted descriptors to produce an ANNF. In contrast to the previous approaches that employed Neural Network layers to calculate the matching score, they applied the L2 metric to simplify the ANNF computation. The sparse optical flow obtained from the matches is used as input for EpicFlow <ref type="bibr" target="#b153">[154]</ref> to obtain dense optical flow. Schuster et al. <ref type="bibr" target="#b197">[198]</ref> improved <ref type="bibr" target="#b152">[153]</ref> by introducing a Hinge loss to replace the DrLIM loss. Guney and Geiger <ref type="bibr" target="#b64">[65]</ref> applied the Siamese CNN to learn per-pixel context-aware features for solving optical flow though discrete optimization. Bai et al. <ref type="bibr" target="#b198">[199]</ref> applied two branches of a Siamese network to process subsequent images to learn features. Bailer et al. <ref type="bibr" target="#b199">[200]</ref> proposed a multi-scale feature creation method tailored to CNNs to estimate optical flow. One significant contribution of this method is that they exploited a novel thresholded loss for Siamese networks that allows to speed up training by about two times.</p><p>Zweigand and Wolf <ref type="bibr" target="#b65">[66]</ref> use CNNs to predict the dense optical flow. They presented a data-driven sparse-to-dense interpolation method based on a CNN without pooling to compute the optical flow. A set of sparse and noisy matches, a binary mask and an edge map are used as the network input. Hu et al. <ref type="bibr" target="#b200">[201]</ref> proposed a RecSPy network consisting of a Siamese network to learn deep features and a Recurrent CNN that formulates the spatial pyramid as a recurrent process and is used to update and refine the optical flow at each scale of the pyramid. The refinement is based on an energy function that encodes structure and constancy constraints to improve the optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Unsupervised methods</head><p>The dependency of supervised method on large labeled datasets has recently motivated the study of unsupervised method, which is able to train CNN models on unlabeled image pairs or videos to predict optical flow. In contrast to supervised methods, the performance of the unsupervised method still falls behind but is improving.</p><p>Based on the classical optical flow constraint, Ahmadi and Patras <ref type="bibr" target="#b201">[202]</ref> designed a loss function without regularization that is differentiable with respect to the unknown flow field. It allows the backpropagation of the error to previous layers. Yu et al. <ref type="bibr" target="#b202">[203]</ref> replaced the supervised loss function of FlowNet by a proxy loss function, which combines a data term that imposes brightness constancy over time with a smoothness term that models the expected variation of optical flow across images, to learn optical flow end-to-end in an unsupervised way. The network is able to optimize the brightness constancy and motion smoothness assumptions explicitly by making use of differentiable warping. Since the proxy loss is too simplistic, its accuracy is much lower than FlowNet. To address this issue, Zhu et al. <ref type="bibr" target="#b203">[204]</ref> proposed a guided optical flow learning method by combining the unsupervised proxy loss with proxy ground truth flow produced via a classical optical flow method. They applied the classical methods to produce proxy ground truth optical flow to guide the CNN training. The learned models are subsequently fine-tuned in an unsupervised manner by minimizing an image reconstruction loss. Ren et al. <ref type="bibr" target="#b63">[64]</ref> exploited network similar to the Spatial Transformer Network <ref type="bibr" target="#b204">[205]</ref>, utilizing the photometric consistency for end-to-end learning without supervision. The photometric error between the warped feature map from the reference image and the target image is treated as the loss, measured by the objective function used in the variational method <ref type="bibr" target="#b84">[85]</ref>.</p><p>Fan et al. <ref type="bibr" target="#b205">[206]</ref> designed TVNet, which is formulated by imitating and unfolding the iterations in the classical variational method, TV-L1 <ref type="bibr" target="#b48">[49]</ref>, to customized neural layers. TVNet is well initialized as a particular TV-L1 approach and can obtain desirable results without additional training on ground-truth optical flows. It incorporates the strengths of both the classical variational method and CNNs. Wang et al. <ref type="bibr" target="#b206">[207]</ref> designed a deep neural architecture to boost the performance of unsupervised flow estimation with the emphasis on addressing the issues of occlusion and large motion. First, they explicitly model the occlusion map that is caused by motion and combine it with the loss function. The occlusion problem of the common loss function, which prefers to compensate the occluded regions by moving other pixels, is handled. Second, they exploited three strategies to deal with large displacements: a new warping method to facilitate the learning of large displacements, introducing additional warped inputs during the decoder stage, and applying histogram equalization and channel representation to the flow computation. Meister et al. <ref type="bibr" target="#b188">[189]</ref> extended the FlowNetSbased UnsupFlownet <ref type="bibr" target="#b202">[203]</ref>. They introduced an unsupervised loss that relies on occlusion-aware bidirectional optical flow estimation. They also applied the comprehensive unsupervised loss to train FlowNetC to learn bidirectional flow. Finally, iterative refinement is conducted by stacking multiple networks of FlowNet. These methods <ref type="bibr" target="#b206">[207,</ref><ref type="bibr" target="#b188">189]</ref> rely on the accuracy of the estimated optical flow, and require to set a heuristics to infer occlusions. By extending the two-frame architecture of <ref type="bibr" target="#b193">[194]</ref> to multiple frames, Caron et al. <ref type="bibr" target="#b207">[208]</ref> proposed a novel unsupervised formulation to jointly estimate optical flow and occlusions over multiple frames. The temporal constraints are beneficial to predict more accurate optical flow in occluded regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-supervised methods</head><p>To benefit from both synthetic data with labeled ground truth flow and realistic data without labeled ground truth flow, semi-supervised methods have been explored <ref type="bibr" target="#b208">[209]</ref>. One strategy is to minimize the End Point Error (EPE) loss for data with ground truth flow and the loss functions that measure the brightness constancy and spatial smoothness for unlabeled input <ref type="bibr" target="#b201">[202]</ref>. Such a method is usually sensitive to the selection of parameters which may decrease the performance of optical flow.</p><p>Lai et al. <ref type="bibr" target="#b208">[209]</ref> presented a generative adversarial Network (GAN) to learn optical flow with both the labeled and the unlabeled data in a semi-supervised learning framework. The adversarial loss, which plays as a regularizer for both types of data, is able to discover structural patterns of flow warp errors without making explicit assumptions on the brightness constancy and motion smoothness. Yang et al. <ref type="bibr" target="#b209">[210]</ref> exploited a Conditional Prior Network (CPN) to predict optical flow. It consists of a prior that imposes a bias on the possible solutions and that is learned in a supervised way and a simple network that is trained to map pairs of images to optical flow in a completely unsupervised manner. The regularizer in the form of the conditional prior is integrated into the loss function for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Limitations of the CNN-based methods</head><p>â€¢ The supervised method requires large amount of ground-truth optical flow to train the parameters of the flow network to obtain reasonable accuracy, it is costly and challengingly to label the data. Besides, there is no guarantee that the trained model can still work in different scenarios with unknown challenges.</p><p>â€¢ The performance of the unsupervised method still has a relatively large distance compared to their supervised counterparts. Specially, the widely used surrogate losses, which depend on the fundamental assumptions of brightness constancy and spatial smoothness priors, greatly restrain the performance.</p><p>â€¢ The semi-supervised method requires both labeled and unlabeled data for training, thus it is easy to fall between supervised learning and unsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Optical flow is essential for a host of computer vision tasks. Its underspecified nature and unpractical constraints in the input introduce a number of challenges in the estimation. In this survey, we have discussed these challenges and the related methods that have been introduced to address them. We have also described the measures and datasets to benchmark optical flow estimation algorithms.</p><p>We have focused on the dominant variational algorithm and its modifications and extensions. We have outlined the main directions of research and discussed their relative advantages and disadvantages. For the first time, we have presented an in-depth overview of methods that are based on convolutional neural networks (CNNs). This novel class of approach has distinct advantages in terms of efficiency and accuracy.</p><p>The thorough understanding of the challenges in the automated estimation of optical flow and the characteristics of the described approaches will guide future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of optical flow: (a), (d) frames 10 and 11 of the RubberWhale sequence on the Middlebury benchmark [5], respectively; (b) color-coded ground truth flow (black regions indicate unknown flow vectors); (c) vector plot ground truth flow.</figDesc><graphic coords="2,75.37,55.79,444.76,84.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. End-to-end CNN learning model for optical flow estimation.</figDesc><graphic coords="4,141.32,55.79,312.80,208.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. HSV color space: Direction is coded by hue, length is coded by saturation.</figDesc><graphic coords="5,207.34,55.79,180.60,179.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. A flow error map colorization in KITTI 2015 dataset.</figDesc><graphic coords="5,75.37,269.80,444.76,134.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) The coarse-to-fine strategy. (b) Original image. (c) Third level image. (d) Fourth level image. (e) Estimated flow. Note that the neck is not recovered. Image taken from [77].</figDesc><graphic coords="9,75.37,55.79,444.76,220.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>MPI-Sintel special evaluation protocol.</figDesc><table><row><cell>Method</cell><cell>EPE all</cell><cell>EPE matched</cell><cell>EPE unmatched</cell><cell>d0-10</cell><cell>d10-60</cell><cell>d60-140</cell><cell>s0-10</cell><cell>s10-40</cell><cell>s40+</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>KITTI special evaluation protocol.</figDesc><table><row><cell>Method</cell><cell>Setting</cell><cell>Out-Noc</cell><cell>Out-All</cell><cell>Avg-Noc</cell><cell>Avg-All</cell><cell>Density</cell><cell>Runtime</cell><cell>Environment</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Characteristic comparison of the available public datasets.</figDesc><table><row><cell>Datasets</cell><cell>Middlebury</cell><cell>KITTI</cell><cell>MPI-Sintel</cell><cell>Flying Chairs</cell></row><row><cell>Image pairs</cell><cell>72</cell><cell>194</cell><cell>1041</cell><cell>22872</cell></row><row><cell>Ground truth (GT)</cell><cell>08</cell><cell>194</cell><cell>1041</cell><cell>22872</cell></row><row><cell>GT density per image (%)</cell><cell>100</cell><cell>âˆ¼50</cell><cell>100</cell><cell>100</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The work is supported by the funding CXFW-18-413100063 of Wuhan University. It is also supported by the National Natural Science Foundation of China (61501198), Natural Science Foundation of Hubei Province, China (2014CFB461) and (2017CFB598).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dejun Zhang received the bachelor in communication engineering at the School of Information Science and Technology, Southwest Jiaotong University, China, 2006. In 2011, he received the Master degree in electronic engineering at the School of Manufacturing Science and Engineering, Southwest University of Science and Technology, China. In 2015, he received the Ph.D. degree in Computer Science from Wuhan University, China. He is currently a lecturer with the faculty of college of information and engineering, Sichuan agricultural university, China. Since 2015, he has been serving as a senior member of the China Society for Industrial and Applied Mathematics (CSIAM) and a committee member of the geometric design and computing of CSIAM. His research areas include machine learning, bioinformatics and computer graphics. His research interests include digital geometric processing, computer graphic, action recognition and localization.</p><p>Ronald Poppe received a Ph.D. in Computer Science from the University of Twente, The Netherlands. He was a visiting researcher at the Delft University of Technology, Stanford University and University of Lancaster. He is currently an assistant professor at the Information and Computing Sciences department of Utrecht University. His research interests include the analysis of human behavior from videos and other sensors, the understanding and modeling of human (communicative) behavior and the applications of both in real-life settings. In 2012 and 2013, he received the most cited paper award from the ''Image and Vision Computing" journal, published by Elsevier.</p><p>Remco C. Veltkamp is full professor of Multimedia at Utrecht University, Netherlands. His research interests are the analysis, recognition and retrieval of, and interaction with, music, images, and 3D objects and scenes, in particular the algorithm and experimentation aspects. He has written over 150 refereed papers in reviewed journals and conferences, and supervised 15 PhD theses. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: descriptor matching in variational motion estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Performance of optical flow techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="77" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<title level="m">Variational Optic Flow Computation: Accurate Modelling and Efficient Numerics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Department of Mathematics and Computer Science, Saarland University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Perception of the Visual World</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950">1950</date>
			<publisher>Houghton Mifflin Company</publisher>
			<pubPlace>Boston</pubPlace>
		</imprint>
	</monogr>
	<note>first ed.</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Visual control orientation behavior in the fly: Part II. Towards underlying neural interactions</title>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Reiehardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q. Rev. Biophys</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="377" to="438" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An evaluation of optical flow algorithms for crowd analytics in surveillance system</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kajo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kamel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Intelligent and Advanced Systems</title>
		<meeting>Int. Conf. Intelligent and Advanced Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object tracking: a survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Text detection, tracking and recognition in video: A comprehensive survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2752" to="2773" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Track and segment: An iterative unsupervised approach for video object proposals</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="933" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3899" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fusing disparate object signatures for salient object detection in video</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="285" to="299" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MSR-CNN: Applying motion salient region based descriptors for action Recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3524" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Histograms of optical flow orientation and magnitude and entropy to detect anomalous events in videos</title>
		<author>
			<persName><forename type="first">R</forename><surname>Colque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="673" to="682" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Detecting anomalous events in videos by learning deep representations of appearance and motion</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual navigation for mobile robots: A survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Font</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Robot. Syst</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="296" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Vision for mobile robot navigation: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Desouza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="267" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Divergence stereo for robot navigation: Learning from bees</title>
		<author>
			<persName><forename type="first">J</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Curotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garibaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="434" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optical flow for self-supervised learning of obstacle appearance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wagter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Remes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Croon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Intell. Robots and Systems</title>
		<meeting>Int. Conf. Intell. Robots and Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3098" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Barry</surname></persName>
		</author>
		<title level="m">High-Speed Autonomous Obstacle Avoidance with Pushbroom Stereo</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ultra-high-throughput VLSI architecture of H.265/HEVC CABAC encoder for UHDTV applications</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image sequence interpolation based on optical flow, segmentation, and optimal control</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lorenz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1020" to="1030" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video frame interpolation via adaptive convolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A bayesian approach to adaptive video super resolution</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="209" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">End-to-end learning of video super-resolution with motion compensation</title>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optical flow for verification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sandgathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Weather Forecast</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1479" to="1494" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D motion estimation of atmospheric layers from image sequences</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Memin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2385" to="2396" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rainbow particle imaging velocimetry for dense 3D fluid velocity imaging</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Idoughi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aguirre-Pablo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aljedaani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thoroddsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Heidrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on variational optic flow methods for small displacements</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Models Regist. Appl. Med. Imaging</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="103" to="136" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Local, semi-Global, and Global Optimization for Motion Estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Austria</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Institute for Computer Graphics and Vision, Graz University of Technology</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<title level="m">Variational Optical Flow Algorithms for Motion Estimation</title>
		<meeting><address><addrLine>Utrecht University</addrLine></address></meeting>
		<imprint>
			<publisher>Netherlands</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
	<note>Department of Information and Computing Sciences</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High accuracy optical flow estimation based on a theory for warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="25" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Kanade meets Horn/Schunck: Combining local and global optic flow methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">/</forename><surname>Lucas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="231" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Computing optical flow via variational techniques</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Appl. Math</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="156" to="182" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The robust estimation of multiple motions: Parametric and piecewise-smooth flow fields</title>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="104" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Variational optic flow computation with a spatio-temporal smoothness constraint</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="255" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Optical flow modeling and computation: A survey</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fortun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kervrann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Markov random fields with efficient approximations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="648" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optical flow estimation using Laplacian mesh energy</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2435" to="2442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Constrained optical flow estimation as a matching problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mozerov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2044" to="2055" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Highly over parameterized optical flow using patchmatch belief propagation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hornaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="220" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the spatial statistics of optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="50" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fusion flow: Discrete continuous optimization for optical flow estimation</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Discrete optimization for optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A multigrid platform for realtime motion computation with discontinuity-preserving variational methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="277" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<editor>DAGM conf. PR</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extended lucas-kanade tracking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="142" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kanade 20 years on: A unifying framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">-</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A computational framework and an algorithm for the measurement of visual motion</title>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="310" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast Normalized Cross-Correlation, Canadian Image Process</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Society</title>
		<imprint>
			<biblScope unit="page" from="120" to="123" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A feature-based approach for determining dense long range correspondences</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="170" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a confidence measure for optical flow</title>
		<author>
			<persName><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1107" to="1120" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A feature-based approach for dense segmentation and estimation of large disparity motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="143" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The computation of optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beauchemin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="433" to="467" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Optical flow using spatiotemporal filters</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="302" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A Look at Motion in the Frequency Domain, National Aeronautics and Space Administration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahumada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Ames Research Center</publisher>
			<biblScope unit="page" from="1" to="10" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Computation of component image velocity from local phase information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jepson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="104" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="891" to="906" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>HazÄ±rbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep discrete flow</title>
		<author>
			<persName><forename type="first">F</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="207" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">InterpoNet, a brain inspired neural network for optical flow dense interpolation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zweigand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4563" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of Optical Flow Estimation with Deep Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1647" to="1655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Optical flow estimation: advances and comparisons</title>
		<author>
			<persName><forename type="first">M</forename><surname>Otte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="51" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Recovering motion fields: An Evaluation of eight optical flow algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Galvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mccane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Novins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">On benchmarking optical flow</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mccane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Novins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crannitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="126" to="143" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Prediction error as a quality metric for motion and stereo</title>
		<author>
			<persName><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="781" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W G</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Seeing 3D chairs: Exemplar part-based 2D-3D alignment using a large dataset of CAD models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3762" to="3769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">An evaluation of data costs for optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. GCPR</title>
		<meeting>GCPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="343" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Secrets of optical flow estimation and their principles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Motion detail preserving optical flow estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">FPGA-based real-time optical flow system</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pelayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ortigosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="279" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Fast edge-preserving patchmatch for large displacement optical flow</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3534" to="3541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Symmetrical dense optical flow estimation with occlusions detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Deriche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Papad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="371" to="385" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Sparse occlusion detection with optical flow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ayvaci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="322" to="338" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Optic flow in harmony</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="368" to="388" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Illumination-robust variational optical flow with photometric invariants</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Mileva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM PR Symposium</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Estimating accurate optical flow in the presence of motion blur</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. EI</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Highly accurate optic flow computation with theoretically justified warping</title>
		<author>
			<persName><forename type="first">N</forename><surname>Papenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Didas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="141" to="158" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A scale-space approach to nonlocal optical flow calculations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Scale-Space Theories in Comput. Vis.</title>
		<imprint>
			<biblScope unit="volume">1682</biblScope>
			<biblScope unit="page" from="235" to="246" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Weighted root mean square approach to select the optimal smoothness parameter of the variational optical flow algorithms</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Opt. Eng</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMMCVPR</title>
		<meeting>EMMCVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="207" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Robust optical flow from photometric invariants</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1835" to="1838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Illumination-Robust optical flow using a local directional pattern</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rashwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mertsching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Puig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1499" to="1508" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Structure-texture image decompositionmodeling, algorithms, and parameter selection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gilboa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="136" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">An improved algorithm for tv-l1 optical flow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sta. and Geometrical Appl. to Vis. Motion Anal</title>
		<imprint>
			<biblScope unit="volume">5064</biblScope>
			<biblScope unit="page" from="23" to="45" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Nonlinear total variation based noise removal algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fatemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Motion from color</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="1991">1991</date>
			<biblScope unit="page" from="292" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Two deterministic halfquadratic regularization algorithms for computed imaging</title>
		<author>
			<persName><forename type="first">P</forename><surname>Charbonnier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Blanc-Feraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barlaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Robust multiresolution estimation of parametric motion models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Odobez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="348" to="365" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Dense estimation and object-based segmentation of the optical flow with robust techniques</title>
		<author>
			<persName><forename type="first">E</forename><surname>Memin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="703" to="719" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Robust local optical flow for feature tracking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Senst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1377" to="1387" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">A quantitative analysis of current practices in optical flow estimation and the principles behind them</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="137" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Visual Reconstruction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A theoretical framework for convex regularizers in PDEbased computation of image motion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Reliable estimation of dense optical flow fields with large displacements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="56" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Segmentation of visual motion by minimizing convex non-quadratic functionals</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICPR</title>
		<meeting>ICPR</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="661" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Motion estimation with non-local total variation regularization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Werlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2464" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Efficient nonlocal regularization for optical flow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Modeling temporal coherence for optical flow</title>
		<author>
			<persName><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Valgaerts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zimmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1116" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Robust trajectory-space TV-L1 optical flow for non-rigid sequences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. EMMCVPR</title>
		<meeting>EMMCVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="300" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">An investigation of smoothness constraints for the estimation of displacement vector fields from image sequences</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nagel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Enkelmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="565" to="593" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">A sure-fired way to choose smoothing parameters in ill-conditioned inverse problems</title>
		<author>
			<persName><forename type="first">V</forename><surname>Solo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="89" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">A data-driven method for choosing smoothing parameters in optical flow problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Solo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="360" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Local smoothness for global optical flow</title>
		<author>
			<persName><forename type="first">L</forename><surname>Raket</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Weighted local intensity fusion method for variational optical flow estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="223" to="232" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Layered RGBD scene flow estimation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="548" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Determining occlusions from space and time image reconstructions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Crivelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bouthemy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1382" to="1391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Detecting occlusions as an inverse problem</title>
		<author>
			<persName><forename type="first">V</forename><surname>Estellers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="198" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Bilateral filtering-based optical flow estimation with occlusion detection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isnardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="211" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Learning optical flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A cooperative algorithm for stereo matching and occlusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="675" to="684" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Occlusion-aware optical flow estimation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1443" to="1451" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Particle video: Long-range motion estimation using point trajectories</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="91" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Layered motion segmentation and depth ordering by tracking edges</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="479" to="493" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Occlusion boundaries from motion: Low-level detection and mid-level reasoning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="325" to="357" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Occlusion boundary detection and figure/ground assignment from optical flow</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2233" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Learning to find occlusion regions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Humayun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2161" to="2168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">A statistical confidence measure for optical flows</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garbe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="290" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Occlusion-aware motion layer extraction under large interframe motions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2615" to="2626" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Layered segmentation and optical flow estimation over time</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1768" to="1775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Local occlusion detection under deformations using topological invariants</title>
		<author>
			<persName><forename type="first">E</forename><surname>Lobaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alterovitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="101" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Exploiting discontinuities in optical flow</title>
		<author>
			<persName><forename type="first">W</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="163" to="173" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Robust non-local TV-L1 optical flow estimation with occlusion detection</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4055" to="4066" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Sparsity model for robust optical flow estimation at motion discontinuities</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2456" to="2463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Optical flow estimation using learned sparse model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2391" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Hierarchical estimation and segmentation of dense motion fields</title>
		<author>
			<persName><forename type="first">E</forename><surname>Memin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="155" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Piecewise-smooth dense optical flow via level sets</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amiaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="124" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A segmentation based variational model for accurate optical flow estimation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="671" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Large displacement optical flow computation without warping</title>
		<author>
			<persName><forename type="first">F</forename><surname>Steinbrucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1069" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Large displacement optical flow from nearest neighbor fields</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2443" to="2450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A combined post-filtering method to improve accuracy of variational optical flow estimation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Van Der Aa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Gemeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1926" to="1940" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
		<title level="m">Measuring Vision Motion from Image Sequence</title>
		<imprint>
			<publisher>University of Massachusetts, US</publisher>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Variational optical flow computation in real time</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Feddern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schnorr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="608" to="615" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<author>
			<persName><surname>S2f</surname></persName>
		</author>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2087" to="2096" />
		</imprint>
	</monogr>
	<note type="report_type">Slow-To-Fast Interpolator Flow</note>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">SIFT flow: Dense correspondence across different scenes and its applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="978" to="994" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Large displacement optical flow with deep matching</title>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepflow</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Layered estimation of atmospheric mesoscale dynamics from satellite imagery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Heas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Memin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szantai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4087" to="4104" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Adaptive integration of feature matches into variational optical flow methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bruhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">A general dense image matching framework combining direct and feature-based costs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">MSLD: A robust descriptor for line matching</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="941" to="953" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">DeepMatching: hierarchical deformable dense matching</title>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="323" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="471" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">The generalized patchmatch correspondence algorithm</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="29" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Epicflow</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1164" to="1172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Robust interpolation of correspondences for large displacement optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="481" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<monogr>
		<title level="m" type="main">Pyramidal gradient matching for optical flow estimation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03217</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">PatchMatch filter: Edge-aware filtering meets randomized search for visual correspondence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1866" to="1879" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="4015" to="4023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">PatchMatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Efficient coarse-to-fine patchmatch for large displacement optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5704" to="5712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Roof duality, complementation and persistency in quadratic 0-1 optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Simeone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="121" to="155" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Optimizing binary MRFs via extended roof duality</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Szummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">The statistics of optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Fermuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Quantification of Smoothing Requirement for 3D optic flow calculation of volumetric images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hadiashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tennakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bruijne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2128" to="2137" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Robust computation of optical flow in a multi-scale differential framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="81" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Fusion moves for Markov random field optimization</title>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1392" to="1405" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">A Kalman filter-integrated optical flow method for velocity sensing of mobile robots</title>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Seneviratne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Althoefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Mechatronics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="563" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Adaptive guided image filter for warping in variational optical flow</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Proc</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="253" to="265" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">A high-quality video denoising algorithm based on reliable motion estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="706" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Structure-and motion-adaptive regularization for high accuracy optic flow</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1663" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Improved color patch similarity measure based weighted median filter</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Gemeren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Bilateral filtering for gray and color images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manduchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="839" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Recursive optical flow estimation-adaptive filtering approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feuer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="138" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Dense, robust and accurate motion field estimation from stereo image sequences in real-time</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="582" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Optical flow in the presence of spatially-varying motion blur</title>
		<author>
			<persName><forename type="first">T</forename><surname>Portz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1752" to="1759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Variational method for joint optical flow estimation and edge-aware image restoration</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="11" to="25" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">Accurate optical flow via direct cost volume processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1289" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">An unbiased second-order prior for high-accuracy motion estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Trobin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAGM Symposium on PR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="396" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Feature-and depth-supported modified total variation optical flow for 3D motion field estimation in real scenes</title>
		<author>
			<persName><forename type="first">T</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rannacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1193" to="1200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">A new minimization strategy for large displacement variational optical flow</title>
		<author>
			<persName><forename type="first">R</forename><surname>Palomares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Llopis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Math. Imaging Vision</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="27" to="46" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<analytic>
		<title level="a" type="main">OPtical Flow Estimation in the Presence of Fast or Discontinuous Motion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">School of Computer Science</title>
		<meeting><address><addrLine>University of Adelaide, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Mathematical problems in image processing: Partial differential equations and the calculus of variations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kornprobst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied mathematical sciences</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Optical flow estimation using Fourier Mellin Transform</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">Locally oriented optical flow computation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1573" to="1586" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Scale-space SIFT flow</title>
		<author>
			<persName><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. WACV</title>
		<meeting>WACV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Dense descriptors for optical flow estimation: A comparative study</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baghaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Imaging</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Unflow: unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<analytic>
		<title level="a" type="main">Uncertainty estimates and multi-hypotheses networks for optical flow</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Galesso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="652" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<analytic>
		<title level="a" type="main">Deep optical flow supervised learning with prior assumptions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Saddi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="43222" to="43232" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4161" to="4170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8934" to="8943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Liteflownet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8981" to="8989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Deep optical flow estimation via multi-scale correspondence structure learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">X</forename></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bourahla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">PatchBatch: A batch augmented loss for optical flow</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4236" to="4245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gadot</surname></persName>
		</author>
		<title level="m">Optical flow requires multiple strategies (but only one network), in: Proc. CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4950" to="4959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<analytic>
		<title level="a" type="main">Exploiting semantic information and deep matching for optical flow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="154" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">CNN-based patch matching for optical flow with thresholded hinge embedding loss</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bailer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3250" to="3259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Recurrent spatial pyramid cnn for optical flow estimation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2814" to="2823" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Unsupervised convolutional neural networks for motion estimation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1629" to="1633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<analytic>
		<title level="a" type="main">Back to basics: unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCVW</title>
		<meeting>ECCVW</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Guided optical flow learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Newsamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">End-to-end learning of motion representation for video understanding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6016" to="6025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b206">
	<analytic>
		<title level="a" type="main">Occlusion aware unsupervised learning of optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4884" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Semi-supervised learning for optical flow with generative adversarial networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Conditional prior networks for optical flow</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="271" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">He is currently a professor at the State Key Laboratory of Information Engineering in Surveying, Mapping and Remote sensing, Wuhan University. His research interests include computer vision, image processing, video analytics, and machine learning. Specially for motion estimation</title>
	</analytic>
	<monogr>
		<title level="m">2015, he received the Ph.D. degree in</title>
		<meeting><address><addrLine>China; Netherlands; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>Wuhan University ; Computer Science from Utrecht University ; Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
	<note>From 2015 to 2016, he was a postdoctoral researcher at Arizona State University, US. Then from 2016 to 2018, he was a research fellow at the School of Electrical and Electronic Engineering. object segmentation, object tracking, action recognition and localization, and anomaly detection</note>
</biblStruct>

<biblStruct xml:id="b211">
	<monogr>
		<title level="m" type="main">He received his BE degree in electronic information engineering and PhD degree in communication and information system from Wuhan University</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004 and 2010</date>
			<pubPlace>China; China; China</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer School of Central China Normal University ; Computer School of Wuhan University</orgName>
		</respStmt>
	</monogr>
	<note>His research interests include motion estimation, superresolution reconstruction, image fusion and image enhancement. respectively. Then, from 2010 to 2013, he served as an assistant professor at</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
