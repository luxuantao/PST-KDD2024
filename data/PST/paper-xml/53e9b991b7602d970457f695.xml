<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Machine Learning Attacks Against the Asirra CAPTCHA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Philippe</forename><surname>Golle</surname></persName>
							<email>pgolle@parc.com</email>
							<affiliation key="aff0">
								<orgName type="department">Palo Alto Research Center</orgName>
								<address>
									<postCode>94304</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Management of Computing and Information Systems-Security and Protection</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<postCode>2008</postCode>
									<settlement>Alexandria</settlement>
									<region>Virginia</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Machine Learning Attacks Against the Asirra CAPTCHA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CAPTCHA</term>
					<term>reverse Turing test</term>
					<term>machine learning</term>
					<term>support vector machine</term>
					<term>classifier</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Asirra CAPTCHA <ref type="bibr" target="#b6">[7]</ref>, proposed at ACM CCS 2007, relies on the problem of distinguishing images of cats and dogs (a task that humans are very good at). The security of Asirra is based on the presumed difficulty of classifying these images automatically.</p><p>In this paper, we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier is a combination of support-vector machine classifiers trained on color and texture features extracted from images. Our classifier allows us to solve a 12-image Asirra challenge automatically with probability 10.3%. This probability of success is significantly higher than the estimate of 0.2% given in <ref type="bibr" target="#b6">[7]</ref> for machine vision attacks. Our results suggest caution against deploying Asirra without safeguards.</p><p>We also investigate the impact of our attacks on the partial credit and token bucket algorithms proposed in <ref type="bibr" target="#b6">[7]</ref>. The partial credit algorithm weakens Asirra considerably and we recommend against its use. The token bucket algorithm helps mitigate the impact of our attacks and allows Asirra to be deployed in a way that maintains an appealing balance between usability and security. One contribution of our work is to inform the choice of safeguard parameters in Asirra deployments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The Asirra CAPTCHA <ref type="bibr" target="#b6">[7]</ref>, proposed at ACM CCS 2007, relies on the problem of distinguishing images of cats and dogs (Asirra stands for "Animal Species Image Recognition for Restricting Access"). An Asirra challenge consists of 12 images, each of which is of either a cat or a dog. To solve the CAPTCHA, the user must select all the cat images, and none of the dog images. This is a task that humans are very good at. According to <ref type="bibr" target="#b6">[7]</ref>, Asirra "can be solved by humans 99.6% of the time in under 30 seconds". The usability of Asirra is a significant advantage compared to CAPTCHAs <ref type="bibr" target="#b8">[9]</ref> based on recognizing distorted strings of letters and numbers.</p><p>The security of Asirra is based on the presumed difficulty of classifying images of cats and dogs automatically. As reported in <ref type="bibr" target="#b6">[7]</ref>, evidence from the 2006 PASCAL Visual Object Classes Challenge suggests that cats and dogs are particularly difficult to tell apart algorithmically. A classifier based on color features, described in <ref type="bibr" target="#b6">[7]</ref>, is only 56.9% accurate. The authors of <ref type="bibr" target="#b6">[7]</ref> conjecture that "based on a survey of machine vision literature and vision experts at Microsoft Research, we believe classification accuracy of better than 60% will be difficult without a significant advance in the state of the art". With a 60% accurate classifier, the probability of solving a 12-image Asirra challenge is only about 0.2%.</p><p>In this paper, we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier allows us to solve a 12-image Asirra challenge with probability 10.3%. This success probability is significantly higher than the 0.2% estimate given in <ref type="bibr" target="#b6">[7]</ref> for machine vision attacks. While our success rate may appear low in absolute terms, it nevertheless poses a serious threat to Asirra if additional safeguards are not deployed to prevent machine adversaries from requesting, and attempting to solve (at virtually no cost), too many CAPTCHAs.</p><p>Our classifier is a combination of two support-vector machine <ref type="bibr" target="#b4">[5]</ref> (SVM) classifiers trained on color and texture features of images. The classifier is entirely automatic, and requires no manual input other than the one-time labelling of training images. Using 15,760 color features, and 5,000 texture features per image, our classifier is 82.7% accurate. The classifier was trained on a commodity PC, using 13,000 labelled images of cats and dogs downloaded from the Asirra website <ref type="bibr" target="#b0">[1]</ref>.</p><p>We also investigate the impact of our attacks on the partial credit and token bucket algorithms proposed in <ref type="bibr" target="#b6">[7]</ref>. The partial credit algorithm weakens Asirra considerably and we recommend against its use. The token bucket algorithm helps mitigate the impact of our attacks and allows Asirra to be deployed in a way that maintains an appealing balance between usability and security. One contribution of our work is to inform the choice of safeguard parameters in Asirra deployments.</p><p>Beyond this immediate contribution, we also hope that our paper will contribute to the popularization of machine learning techniques, for both offensive and defensive purposes, in the security community. Machine learning and other artificial intelligence techniques have not so far been widely used in cryptographic attacks. Yet recent work suggests that these techniques are powerful tools for the cryptanalyst's arsenal. SAT solvers, for example, have been used to find collisions in hash functions <ref type="bibr" target="#b12">[13]</ref> and defeat authentication schemes <ref type="bibr" target="#b7">[8]</ref>. Object recognition algorithms <ref type="bibr" target="#b13">[14]</ref> were used in very successful breaks of the text-based Gimpy and EZ-Gimpy CAPTCHAs. The machine learning classifiers of the type used in this paper, likewise, will hopefully find broader applications in computer security.</p><p>Organization. We describe our SVM classifiers in section 2, using color features (section 2.1), texture features (section 2.2) and both in combination (section 2.3). We discuss the use of these classifiers in attacking Asirra in section 3. Section 3.1 investigates the impact of our attacks on the partial credit and token bucket algorithms of <ref type="bibr" target="#b6">[7]</ref>. We briefly discuss other counter-measures that may help mitigate our attack in section 3.2. Finally, we review related work in section 4 and conclude in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">SUPPORT VECTOR MACHINE CLASSI-FIERS FOR ASIRRA IMAGES</head><p>Asirra relies on a large and growing database of some 3,000,000 images of cats and dogs licensed from the adoption service Petfinder.com. The images displayed by Asirra are 250-by-250 pixels. In the majority of images, there is either a single cat or a single dog. Some images contain multiple cats or multiple dogs. In a very few images, there is no recognizable animal, or else there is both a cat and a dog (these images cannot be classified according to the rules of Asirra).</p><p>Image collection. We collected 13,000 distinct images from the Asirra implementation publicly available on the Asirra website <ref type="bibr" target="#b0">[1]</ref>. The website serves Asirra CAPTCHAs that consist of 12 images selected at random (according to <ref type="bibr" target="#b5">[6]</ref>) from the entire Asirra image database. We wrote a script to automatically refresh the website and download the 12 images in the new Asirra CAPTCHA obtained after each refresh. Over the course of a night, our script refreshed the website approximately 1,100 times and downloaded just over 13,000 images. To avoid duplicates, every image was saved in a file named after a hash of its pixels (we detected and discarded 6 duplicate images). Other than duplicates, no images were deleted, filtered or otherwise selected.</p><p>The collection of 13,000 images thus obtained is a representative, unbiased sample of Asirra images, since "the Asirra service selects images randomly from [Asirra's] entire image database for each challenge" <ref type="bibr" target="#b5">[6]</ref>. The Asirra authors conjecture that "Photos have a wide variety of backgrounds, angles, poses, lighting, and so forth -factors that make accurate automatic classification difficult" <ref type="bibr" target="#b6">[7]</ref>. We have every reason to believe that our subset of 13,000 images offers a similar diversity of factors.</p><p>Manual classification. The next step was to manually classify the 13,000 images in our collection into 3 classes: Cat, Dog and Other. The Cat and Dog classes are selfexplanatory. The Other class was for images which either contained no recognizable animal, or contained both a cat and a dog. Manual classification was followed by a manual verification step, in which 159 misclassified images (1.2% of the total) were detected and moved to the correct category. After verification, we obtained 6,403 images of cats (49.3%), 6,466 images of dogs (49.7%) and 131 other images (1.0% of the total). In the rest of our work, we kept only the images of cats and dogs and discarded the other images.</p><p>Building a classifier. We experimented with different color and texture features computed from images. These features are described in the rest of this section. We trained a support vector machine (SVM) classifier <ref type="bibr" target="#b4">[5]</ref> with each set of features. SVM classifiers were selected for their ability to extract linear combination of features, their predictive power, and their computational scalability. We refer the reader to <ref type="bibr" target="#b9">[10]</ref> for an excellent introduction to SVM (chapter 12), and a comparison of the characteristics of SVMs and other learning methods (page 313). In short, a SVM is a supervised learning method which constructs an optimal linear boundary (or separating hyperplane) between two classes. This hyperplane is optimal in the sense that it maximizes the distance, or margins, between the hyperplane and the two classes on each side of it (an error penalty accounts for misclassified points, when the two classes are not perfectly linearly separable). The power of SVM classifiers comes from the fact that the linear boundary is not computed directly in feature space, but in a transformed, higher-dimensional version of the feature space. The transformation is represented, loosely speaking, by a kernel function. Linear boundaries in the transformed space produce non-linear boundaries when mapped back to the original feature space.</p><p>Measuring accuracy. We measured the accuracy of our SVM classifiers using 5-fold cross-validation on random subsets of our image collection. Cross-validation operates by dividing a subset of images into 5 randomly chosen partitions; 4 of these partitions are used for training while the remaining one is used for validation. We report results using subsets of various sizes (5,000 and 10,000 images), to show the influence of the size of the training sample on the accuracy of our classifier. The accuracy reported for our classifiers in the following sections is the average accuracy (and its standard deviation) over the 5 experiments of 5-fold cross-validation. We note that all our subsets of images, and all the partitions used for cross-validation were generated at random to avoid any bias that might affect our results. SVM implementation. We trained our SVM with a radial basis kernel. This kernel defines the inner product of two feature vectors v and v as</p><formula xml:id="formula_0">K(v, v ) = exp (−γ|v − v | 2 ).</formula><p>The parameter γ was tuned with 5-fold cross-validation to approximately achieve the best test error performance. We found that γ = 10 −3 worked well for color features and γ = 10 −1 worked well for texture features. We used the LIB-SVM <ref type="bibr" target="#b2">[3]</ref>  the LIBSVM library to make more economical use of memory for vectors of boolean features. All computations were done on a commodity desktop PC running Windows XP with dual 3.40 GHZ CPUs and 3.00 GB of RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Color Features</head><p>Recall that an Asirra image is 250-by-250 pixels. We divide the image into N vertical and N horizontal strips of equal width. We thus obtain a division of the image into a grid of N 2 cells. Each cell is a square of width and height 250/N (rounded to the nearest integer).</p><p>We also partition the color space. We use the HSV (hue, saturation, value) model of color, since it is closer to human perception of color, and thus easier to interpret, than the RGB (red, green, blue) model. We subdivide the hue channel of the color spectrum into C h bands of equal width, the saturation channel into Cs bands of equal width and the value channel into Cv bands of equal width. Altogether, this gives us a partition of the color space into C h CsCv color regions.</p><p>Informally, the feature vector associated with an image indicates, for every cell in the image and every color region, whether there is at least one pixel in the cell which belongs to the color region. Note that these features are boolean. Unlike color histograms, they do not indicate how many pixels in a given cell fall within a certain color region, but only whether one or more pixel in the cell falls in the color region. Our experiments show that these boolean features yield more accurate classifiers than color histograms, in addition to being more efficient.</p><p>More precisely, the feature vector</p><formula xml:id="formula_1">F(N, C h , Cs, Cv) is a boolean vector of length N 2 C h CsCv. The boolean feature associated with cell (x, y) ∈ [1, . . . , N ] × [1, . . . , N ] and color region (h, s, v) ∈ [1, . . . , C h ] × [1, . . . , Cs] × [1, . . . ,</formula><p>Cv] takes the value 1 (or true) if there is one or more pixel in cell (x, y) of a color that belongs to color region (h, s, v). Otherwise, the feature takes the value 0 (false).</p><p>We trained SVM classifiers with these color features, and measured their accuracy using 5-fold cross-validation, as explained above. Our results are given in Table <ref type="table" target="#tab_0">1</ref> for various values of the parameters N, C h , Cs, Cv and for training sets of various sizes. For example, the feature set F3 = F(5, 10, 6, 6) consists of 9,000 color features obtained with a division of images into 25 cells (N = 5) and a division of the color space into 360 color regions (C h = 10, Cs = 6 and Cv = 6). With 4,000 training images, an SVM classifier using the feature set F3 is on average 74.6% accurate (over the 5 experiments of 5-fold cross-validation). With 8,000 training images, the accuracy of this classifier increases to 75.7 %.</p><p>Combining color features computed on cells of various sizes further improves the accuracy of our classifier. We experimented with the union of the three feature sets F1 = F(1, 10, 10, 10), F2 = F(3, 10, 8, 8) and F3 = F(5, 10, 6, 6) of Table <ref type="table" target="#tab_0">1</ref>. The total number of color features in F1 ∪ F2 ∪ F3 is 15,760. The accuracy of a classifier using these features is shown in Table <ref type="table" target="#tab_1">2</ref> Color features and classifier accuracy. We observed in our experiments that SVM classifiers trained on boolean color features are more accurate than those trained on color histograms. This finding runs counter to intuition: boolean features, which record only the presence or absence in an image of pixels belonging to a certain region of the color spectrum, encode strictly less information than color histograms, which also record the number of such pixels. We advance two hypotheses for why boolean features outperform color histograms. The first is that boolean color features, unlike color histograms, are scale-independent: they record the presence or the absence of, say, the green of a cat's eye or the pink of a dog's tongue regardless of the size of the eye or tongue in the picture. Our second hypothesis is that the distribution of boolean features is much more regular (only two values are possible) than the distribution of real-valued color histograms (in which the range of possible values may cover several orders of magnitude). The regularity of boolean features facilitates the tuning of SVM parameters (notably γ), which results in superior accuracy.</p><p>Predictive power of individual color features. We observe that individual boolean color features, in isolation, fail to distinguish cats from dogs with any accuracy. The graph in Figure <ref type="figure" target="#fig_0">1</ref> shows the 1,000 boolean color features in F1 = F(1, 10, 10, 10), plotted according to the fraction of cats (horizontal axis) and the fraction of dogs (vertical axis) for which the feature evaluates to "True". The features are clustered along the diagonal, which indicates that the ability of any given color feature to distinguish between cats and dogs is very weak. Nevertheless, an SVM classifier can harness the aggregate power of the color features to produce more accurate predictions. The success of our classifier does not come from the careful selection of a few colors with high predictive values, but rather from the combination of a large number of weakly predictive features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Texture Features</head><p>Approaches to texture recognition can be broadly divided into two categories. The first category takes a statistical approach to texture computation. Texture is defined via quantitative measurements of intensity in different regions of the image (e.g. by convolution with a Gabor filter <ref type="bibr" target="#b10">[11]</ref>). The second category takes a structural approach, and defines texture as a set of texture tiles (also called texels) in repeated patterns. We experimented with both approaches, and found that structural measurements of texture produced more accurate classifiers.</p><p>We start with an informal presentation of our structural approach to texture recognition. We extract small subimages (5-by-5 pixels) from training images of cats and dogs. We call these sub-images texture tiles. Figure <ref type="figure" target="#fig_1">2</ref> shows examples of texture tiles extracted from Asirra images. We collect a set T of texture tiles of size t = |T |, such that the distance between any two tiles in T is above a certain threshold (we define below our measure of distance between tiles). This ensures that the tiles in T are sufficiently diverse, and that there are no duplicate tiles. The feature vector associated with an image is the vector of distances between the image and each texture tile in T (we define below our measure of distance between an image and a tile). Finally, we train an SVM classifier with these feature vectors. More precisely, we proceed as follows. Selection of texture tiles. We select random images of cats and dogs from the set of training images. We divide each image into vertical and horizontal strips of equal width (5 pixels). We thus obtain a division of each image into (250/5) 2 = 2500 feature tiles. Each feature tile is a square of 5-by-5 pixels. Let us denote T0 this initial set of candidate tiles. We define the distance between two tiles as the average Euclidian distance between the pixels of the tiles in RGB color space. From T0, we then compute a subset T of texture tiles iteratively as follows. Initially, T is empty. We consider in turn each tile T ∈ T0. If there already exists a tile in T whose distance to T is below a certain threshold δ, we discard T . Otherwise, we add the tile T to T . We repeat this computation for all candidate tiles in T0 until we obtain a set T of size t. Note that the initial set T0 must be chosen of sufficiently large size to ensure the existence of a subset T of size t.</p><p>Feature vector. The feature vector associated with an image is the vector of distances between the image and each of the t texture tiles in T . The distance between an image A and a texture tile T ∈ T is defined as follows. For 0 ≤ i, j ≤ (250 − 5), let us denote Ai,j the square sub-image of A, of width 5 pixels and height 5 pixels, whose top left corner is the pixel of A in row i and column j. We define the distance d(Ai,j, T ) between a sub-image Ai,j and a texture tile T as the maximum of the Euclidean distance between their pixels in RGB space. The distance between A and T is defined as d(A, T ) = mini,j d(Ai,j, T ). Distances are normalized to the range [0, 1].</p><p>Results. We trained an SVM classifier with these texture features, and measured its accuracy using 5-fold crossvalidation. Our results are given in Table <ref type="table" target="#tab_3">3</ref>. The feature set G1 consists of 1,000 features which record the distance of an image to 1,000 texture tiles. The texture tiles are selected such that the distance between any two of them is at least 40.0. With 4,000 training images, an SVM classifier using the feature set G1 is 74.5 % accurate. We define a feature set G2 which consists of 5,000 features which record the distance of an image to 5,000 texture tiles similarly selected.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Combination of Color and Texture Features</head><p>The SVM classifiers of sections 2.1 and 2.2 produce for each image a real-valued estimate of whether the image is of a cat or of a dog. We mapped the "cat" class to the value 1.0 and the "dog" class to the value −1.0. Thus, an image which produces a positive output is labelled "cat" and an image which produces a negative output is labelled "dog". The output of different SVM classifiers can be combined simply by a weighted average of the estimates they produce. We combined in this way an SVM classifier which uses the set of color features F1 ∪ F2 ∪ F3 (with weight 1/3) and a second SVM classifier which uses the set of texture features G2 (with weight 2/3). The accuracy of this combination is given in Table <ref type="table" target="#tab_4">4</ref>. With a training set of 8,000 images, we obtain a classifier which is 82.7 % accurate. The confusion matrix of this classifier is in Table <ref type="table">5</ref>. Table <ref type="table">5</ref>: Confusion matrix for the combined color and texture classifier of section 2.3.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the probability distribution of the combined outputs of the color and texture SVM classifiers, for the "cat" and "dog" classes. Figure <ref type="figure" target="#fig_3">4</ref> shows the cats and dogs in our sample of 13, 000 pets that are most cat-like and most dog-like, according to the combined classifier.</p><p>Accuracy versus completeness. We can achieve lower error rates if we allow the classifier to assign some images a "don't know" label. The quality of this 3-class classifier is measured by completeness (the fraction of images classified as either "cat" or "dog") and accuracy (the fraction of images in the "cat" and "dog" classes which are accurately classified). We turn our combined color and texture classifier into a 3class classifier as follows. The classifier is parameterized by a real-valued parameter ≥ 0. Images which produce an output smaller than − are labelled "dog". Images which produce an output between − and are labelled "don't know". Finally, images which produce an output larger than are labelled "cat".</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows a plot of the accuracy versus completeness of this classifier obtained for different values of the parameter . The base accuracy of the combined color and texture classifier, when classifying all images (completeness of 100 %), is 82.7 %. If the classifier can ignore half the images (completeness of 50 %), its accuracy rises to 94.8 %. For 20 % completeness, accuracy rises to 98.5 %.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ATTACKING ASIRRA</head><p>In this section, we describe the application of the machine classifiers of section 2 to attacking Asirra. Recall that an Asirra challenge consists of 12 images of cats and dogs. To solve the challenge, one must identify correctly the subset of cat images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generic attack.</head><p>A classifier with a success probability 0 &lt; p &lt; 1 of correctly classifying a single Asirra image succeeds in solving a 12-image Asirra challenge with probability p 12 . Our best classifier is 82.74 % accurate, which implies that we can solve an Asirra challenge completely automatically with probability 10.3 %. Note that this success probability is independent of the probability distribution according to which Asirra challenges are generated.</p><p>Leveraging prior information. This success probability can be improved, if the attacker has knowledge of the prior probability distribution of Asirra challenges over the space {Cat, Dog} 12 (whatever that distribution may be). Let C = {I1, . . . , I12} denote a 12-image Asirra challenge and let A = {a1, . . . , a12}, where ai ∈ {Cat, Dog} denote an assignment of the images I1, . . . , I12 to the "cat" and "dog" classes. According to Bayes' rule,</p><formula xml:id="formula_2">Pr[A|C] = Pr[A] Pr[C|A]/ Pr[C].</formula><p>The attacker's goal is to compute maxA Pr[A|C], or equivalently maxA(Pr[A] Pr[C|A]). The first term, Pr[A] is the prior probability distribution that we are assuming is known to the attacker. The second term, Pr[C|A] can be estimated by the attacker as follows. Let us assume that the attacker uses a classifier Cl which produces real-valued outputs, as in section 2.3. Let δ cat denote the probability distribution of the output of Cl over images of cats, and δ dog denote the probability distribution of the output of Cl over images of dogs. With this classifier, the attacker can estimate</p><formula xml:id="formula_3">Pr[C|A] = i | a i =cat δ cat (Cl(Ii)) i | a i =dog δ dog (Cl(Ii)).</formula><p>Example. The exact rules for creating Asirra challenges are not specified precisely in <ref type="bibr" target="#b6">[7]</ref>. The basic rule, however, seems to be that the 12 images of an Asirra challenge are drawn uniformly at random, either from the full Asirra database of more than 3,000,000 images, or from a subset of images of pets located in the geographic vicinity of the user. If this assumption is correct, it implies that each image in an Asirra challenge is drawn independently at random from the "cat" class with probability q and from the "dog" class with probability 1 − q. An attacker can learn the value of the parameter q by observing Asirra challenges. Our own measurements suggest q ≈ 0.5 since we found approximately the same number of cats and dogs in our sample of the Asirra database. As explained above, we can leverage this information to compute the most likely assignment A for a given challenge: maxA(Pr[A] Pr[C|A]). In this example, Pr[A] = q w (1 − q) 12−w , where w is the number of cats in A. Using the combined color and texture classifier of section 2.3 to estimate Pr[C|A], we solve an Asirra challenge with probability 10.4 %. This probability of success is only barely higher than that of the generic attack (10.3%). The reason is that, with the classifier of section 2.3, the generic attack already produces assignments that nearly follow a binomial distribution of cats and dogs.</p><p>Hypothetical example. Consider an hypothetical variant of Asirra in which every challenge contains exactly 6 images of cats and 6 images of dogs (this variant is not proposed in <ref type="bibr" target="#b6">[7]</ref>). We now have Pr[A] = 0 if the number of cats and dogs in A are not equal, and Pr[A] = 1/ 12   6   otherwise. Using the Bayesian formula above, and the classifier of section 2.3, we can solve these variant Asirra challenges automatically with probability 23.8 %. While this variant may be attractive from a usability point-of-view (users may solve Asirra challenges faster if they know they must find exactly 6 cats), our analysis shows that it is insecure and should be avoided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Partial Credit Algorithm and Token Bucket Scheme</head><p>Two enhancements to Asirra are proposed in <ref type="bibr" target="#b6">[7]</ref>. The first is a partial credit algorithm designed to improve the usability of Asirra for human users. The second is a token bucket scheme designed to harden Asirra against automated attacks. In this section, we study the impact of these enhancements on the classifier of section 2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Partial credit algorithm (PCA).</head><p>A user who correctly classifies 11 of the 12 images in an Asirra challenge is considered to have "nearly" solved the challenge. The user is placed in an intermediate state and presented with a second challenge. If the user solves or nearly solves the second challenge (i.e. identifies 11 or 12 images correctly), the user passes. Otherwise, the user fails and is returned to the default (non intermediate) state.  <ref type="table">7</ref>: Impact of the token bucket scheme on the success of the classifier of section 2.3. In <ref type="bibr" target="#b6">[7]</ref>, the parameter TB-refill is set to 3.</p><p>the partial credit algorithm on the success of the classifier of section 2.3 (for comparison, the table also includes the figures for the human success rate, taken from <ref type="bibr" target="#b6">[7]</ref>). With PCA, the success rate of our automatic classifier is 38.0 % after 3 challenges. This is unacceptably high, and leads us to recommend that the partial credit algorithm should not be deployed with Asirra.</p><p>Token bucket scheme. A full description of the token bucket scheme can be found in <ref type="bibr" target="#b6">[7]</ref>. In essence, the token bucket scheme punishes users who fail a lot of Asirra challenges. These users must solve correctly two Asirra challenges in close succession to be considered successful. The token bucket scheme is parameterized by a parameter TBrefill, which specifies how many chances the user is given to correctly solve a second CAPTCHA after solving the first one. A value TB-refill = 1 means that a user who has failed "too many" Asirra challenges must then solve two successive CAPTCHAs correctly to be considered successful. In <ref type="bibr" target="#b6">[7]</ref>, the value TB-refill =3 is suggested, which means the user is allowed 3 trials to solve a second CAPTCHA correctly. Table <ref type="table">7</ref> shows the impact of the token bucket scheme on the success of the classifier of section 2.3. Our results suggest that PCA leads to weak security, even in combination with the token bucket scheme. On the other hand, Asirra appears reasonably secure with the parameter TB-refill =1, since our attack in that case is only 1.1% successful (of course, this parameter is bound to also significantly decrease the human success rate, and thus negatively impact the usability of Asirra).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Defenses</head><p>The best defenses against our machine learning attacks are IP monitoring schemes, which prevent an adversary from requesting, and attempting to solve, too many Asirra challenges. The token bucket scheme proposed in <ref type="bibr" target="#b6">[7]</ref>, and summarized above in section 3.1, is a clever instantiation of an IP monitoring scheme. The strictest version of the token bucket scheme reduces the probability of solving an Asirra challenge automatically to 1.1% (with, however, a parallel reduction in the usability of Asirra for humans). The token bucket scheme could be further strengthened by requiring users to correctly solve more than two Asirra challenges in a row. Unfortunately, this would also negatively affect the ability of humans to pass Asirra challenges. Another approach to improving the security of Asirra is to increase the number of images used in challenges.</p><p>Distorting, warping or degrading the quality of the images is unlikely to do much to lower the accuracy of SVM classifiers based on color and texture features, since these features are largely unaffected by global image distortions. Using greyscale images, instead of color images, may decrease the accuracy of the color classifiers of section 2.1, but would likely have little effect on the texture classifiers of section 2.2. These techniques do not appear promising: they are unlikely to dent the effectiveness of automatic classifiers without also significantly reducing the usability advantage that is Asirra's greatest strength. They would amount to "the arms race found in text CAPTCHAs that [Asirra is] trying to avoid" <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RELATED WORK</head><p>A number of attacks have been reported against textbased CAPTCHAs. Mori and Malik <ref type="bibr" target="#b13">[14]</ref> proposed object recognition algorithms that succeeded in recognizing words in the EZ-Gimpy CAPTCHA with probability 92% and in the Gimpy CAPTCHA with probability 33%. More recently, attacks have been reported in the popular press against the CAPTCHAs used by Yahoo! <ref type="bibr" target="#b14">[15]</ref> and Google <ref type="bibr" target="#b15">[16]</ref>. Unfortunately, few details are available and it is difficult to ascertain the validity of these attacks. Very recent work <ref type="bibr" target="#b16">[17]</ref> gives a detailed description of character segmentation attacks against Microsoft and Yahoo! CAPTCHAs.</p><p>Beyond Asirra, there have been other proposals for userfriendly, clickable CAPTCHAs. For example, Lopresti <ref type="bibr" target="#b11">[12]</ref> proposes asking users to select the right orientation of a page through a click. BotBarrier <ref type="bibr" target="#b1">[2]</ref> asks users to click on a specified location in an image. The security of these proposals relies on new and relatively untested assumptions. It is not clear whether these assumptions will withstand the test of time.</p><p>Another approach to clickable CAPTCHAs was recently proposed by Chow et al. <ref type="bibr" target="#b3">[4]</ref>. The approach consists of combining several textual CAPTCHAs into a grid of clickable CAPTCHAs (e.g. a 3-by-4 grid). The solution to the grid is the determination (e.g. by clicking) of the grid elements which satisfy some given requirement. For example, the user may be asked to identify in the grid the subset of CAPTCHAs which embed English words (assuming some, but not all, do). One advantage of this approach is that it relies on existing security assumptions about text-based CAPTCHAs that have been in use for a long time and have been the object of intense scrutiny.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier allows us to solve a 12-image Asirra challenge with probability 10.3%. The weakness we have exposed in the current implementation of Asirra cautions against deploying Asirra without additional safeguards. With appropriate safeguards, notably the token bucket scheme described in <ref type="bibr" target="#b6">[7]</ref>, we believe that Asirra continues to offer an appealing balance between security and usability. We hope that this work will contribute to the secure deployment of Asirra.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: This graph shows the 1,000 boolean color features in F1 = F(1, 10, 10, 10), plotted according to the fraction of cats (horizontal axis) and the fraction of dogs (vertical axis) for which the feature evaluates to "True". Note that features are clustered along the diagonal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of texture tiles (5-by-5 pixels) extracted from images of cats and dogs. The classifier of section 2.2 relies on such texture tiles.</figDesc><graphic url="image-1.png" coords="4,344.24,67.97,184.25,144.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Probability distribution of the combined output of the color and texture classifiers of section 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The cats and dogs in our sample that are most cat-like and most dog-like, according to the classifier of section 2.3. Classified as cat Classified as dog Cats 4,271 729 Dogs 997 4,003</figDesc><graphic url="image-6.png" coords="5,372.54,360.83,64.60,64.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy versus completeness of the combined color and texture classifier of section 2.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Java implementation of SVM. We rewrote parts of Accuracy of SVM classifiers trained on color features extracted from Asirra images. The color features are described in section 2.1. The accuracy of the classifier is the fraction of cat and dog images classified correctly in the test set.</figDesc><table><row><cell></cell><cell cols="3">Color features</cell><cell></cell><cell></cell><cell></cell><cell cols="2"># Images</cell><cell></cell><cell>Classifier accuracy</cell></row><row><cell>Feature set</cell><cell>N</cell><cell cols="3">C h Cs Cv</cell><cell cols="2"># features</cell><cell>Total</cell><cell cols="2">Training set</cell><cell>mean</cell><cell>stdev</cell></row><row><cell>F1</cell><cell>1</cell><cell cols="3">10 10 10</cell><cell></cell><cell>1,000</cell><cell>5,000</cell><cell>4,000</cell><cell></cell><cell>67.3 %</cell><cell>1.6</cell></row><row><cell>F2</cell><cell>3</cell><cell>10</cell><cell>8</cell><cell>8</cell><cell></cell><cell>5,760</cell><cell>5,000</cell><cell>4,000</cell><cell></cell><cell>74.6 %</cell><cell>1.1</cell></row><row><cell>F3</cell><cell>5</cell><cell>10</cell><cell>6</cell><cell>6</cell><cell></cell><cell>9,000</cell><cell>5,000</cell><cell>4,000</cell><cell></cell><cell>74.6 %</cell><cell>0.6</cell></row><row><cell>F3</cell><cell>5</cell><cell>10</cell><cell>6</cell><cell>6</cell><cell></cell><cell>9,000</cell><cell>10,000</cell><cell>8,000</cell><cell></cell><cell>75.7 %</cell><cell>0.7</cell></row><row><cell></cell><cell cols="3">Color features</cell><cell></cell><cell></cell><cell cols="2"># Images</cell><cell cols="3">Classifier accuracy</cell></row><row><cell></cell><cell cols="2">Feature set</cell><cell cols="3"># features</cell><cell>Total</cell><cell cols="2">Training set</cell><cell>mean</cell><cell>stdev</cell></row><row><cell cols="3">F1 ∪ F2 ∪ F3</cell><cell cols="3">15,760</cell><cell>5,000</cell><cell>4,000</cell><cell cols="2">76.3 %</cell><cell>0.9</cell></row><row><cell cols="3">F1 ∪ F2 ∪ F3</cell><cell cols="3">15,760</cell><cell>10,000</cell><cell>8,000</cell><cell cols="2">77.1 %</cell><cell>0.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of SVM classifiers trained on a combination of color features.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. With 4,000 training images, the classifier is 76.3 % accurate. With 8,000 training images, it is 77.1 % accurate.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of SVM classifiers trained on texture features extracted from Asirra images. The texture features are described in section 2.2. The accuracy of the classifier is the fraction of cat and dog images classified correctly in the test set.</figDesc><table><row><cell cols="2">Texture features</cell><cell></cell><cell></cell><cell cols="2"># Images</cell><cell cols="2">Classifier accuracy</cell></row><row><cell>Feature set</cell><cell># tiles</cell><cell>δ</cell><cell></cell><cell>Total</cell><cell cols="2">Training set</cell><cell>mean</cell><cell>stdev</cell></row><row><cell>G1</cell><cell>1,000</cell><cell>40.0</cell><cell></cell><cell>5,000</cell><cell>4,000</cell><cell cols="2">74.5 %</cell><cell>2.0</cell></row><row><cell>G2</cell><cell>5,000</cell><cell>40.0</cell><cell></cell><cell>5,000</cell><cell>4,000</cell><cell cols="2">78.0 %</cell><cell>1.9</cell></row><row><cell>G2</cell><cell>5,000</cell><cell>40.0</cell><cell cols="2">10,000</cell><cell>8,000</cell><cell cols="2">80.4 %</cell><cell>0.9</cell></row><row><cell cols="2">Features</cell><cell></cell><cell cols="3"># Images</cell><cell cols="2">Classifier accuracy</cell></row><row><cell></cell><cell></cell><cell cols="2">Total</cell><cell cols="2">Training set</cell><cell>mean</cell><cell>stdev</cell></row><row><cell cols="2">(F1 ∪ F2 ∪ F3) + G2</cell><cell cols="2">5,000</cell><cell></cell><cell>4,000</cell><cell>80.3 %</cell><cell>1.4</cell></row><row><cell cols="4">(F1 ∪ F2 ∪ F3) + G2 10,000</cell><cell></cell><cell>8,000</cell><cell>82.7 %</cell><cell>0.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy of the combined outputs of the color classifier of section 2.1 and the texture classifier of section 2.2. The color classifier is given half the weight of the texture classifier (see section 2.3).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>shows the impact of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Impact of the partial credit algorithm on the success of the classifier of section 2.3. For comparison, the table also includes the human success rates reported in<ref type="bibr" target="#b6">[7]</ref>.</figDesc><table><row><cell>Enhancement</cell><cell>TB-refill</cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell>success rate</cell></row><row><cell>Token bucket</cell><cell>3</cell><cell>2.9 %</cell></row><row><cell>Token bucket</cell><cell>2</cell><cell>2.0 %</cell></row><row><cell>Token bucket</cell><cell>1</cell><cell>1.1 %</cell></row><row><cell>Token bucket + PCA</cell><cell>3</cell><cell>19.0 %</cell></row><row><cell>Token bucket + PCA</cell><cell>2</cell><cell>15.3 %</cell></row><row><cell>Token bucket + PCA</cell><cell>1</cell><cell>13.0 %</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>I would like to thank David Goldberg, Maurice Chu, Richard Chow, Glenn Durfee and Kurt Partridge for valuable feedback on this project. Glenn and Richard also helped manually classify training images, for which I am very grateful. I would like to thank Jeremy Elson, John Douceur and Jon Howell for generously answering my questions about ASIRRA and offering additional labelled images. Finally, I would like to thank the anonymous reviewers whose comments helped improve this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A Human Interactive Proof</title>
		<author>
			<persName><surname>Msr Asirra</surname></persName>
		</author>
		<ptr target="http://research.microsoft.com/asirra/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Botbarrier</surname></persName>
		</author>
		<ptr target="http://www.botbarrier.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">LIBSVM : a library for support vector machines</title>
		<author>
			<persName><forename type="first">Chih-Chung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://www.csie.ntu.edu.tw/~cjlin/libsvm" />
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Making CAPTCHAs Clickable</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jakobsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of HotMobile</title>
				<meeting>of HotMobile</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Support-vector network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Private communication</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Asirra: a CAPTCHA that exploits interest-aligned manual image categorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Douceur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Howell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM CCS 2007</title>
				<meeting>of ACM CCS 2007</meeting>
		<imprint>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cryptanalysis of a Cognitive Authentication Scheme</title>
		<author>
			<persName><forename type="first">P</forename><surname>Golle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2007 IEEE Symposium on Security and Privacy</title>
				<meeting>of the 2007 IEEE Symposium on Security and Privacy</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Captcha</forename><surname>Google</surname></persName>
		</author>
		<ptr target="https://www.google.com/accounts/DisplayUnlockCaptcha" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The Elements of Statistical Learning (Data Mining, Inference, and Prediction)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer Series in Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of texture features based on Gabor filters</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kruizinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Grigorescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 10th International Conference on Image Analysis and Processing</title>
				<meeting>of the 10th International Conference on Image Analysis and essing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Leveraging the CAPTCHA problem</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lopresti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Second International Workshop on Human Interactive Proofs</title>
				<meeting>of the Second International Workshop on Human Interactive Proofs</meeting>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="97" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Applications of SAT Solvers to Cryptanalysis of Hash Functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory and Applications of Satisfiability Testing -SAT 2006</title>
				<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="102" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recognizing objects in adversarial clutter: Breaking a visual CAPTCHA</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2003 Conference on Computer Vision and Pattern Recognition</title>
				<meeting>of the 2003 Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="134" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Slashdot</surname></persName>
		</author>
		<author>
			<persName><surname>Yahoo</surname></persName>
		</author>
		<author>
			<persName><surname>Hacked</surname></persName>
		</author>
		<ptr target="http://it.slashdot.org/it/08/01/30/0037254.shtml" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
	<note>posted Jan</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Google&apos;s CAPTCHA busted in recent spammer tactics</title>
		<author>
			<persName><forename type="first">Websense</forename><surname>Blog</surname></persName>
		</author>
		<ptr target="http://securitylabs.websense.com/content/Blogs/2919.aspx" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
	<note>posted Feb</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Low-cost Attack on a Microsoft CAPTCHA. To appear in</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Ahmad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM CCS</title>
				<meeting>of ACM CCS</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
