<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MLPINIT: EMBARRASSINGLY SIMPLE GNN TRAIN-ING ACCELERATION WITH MLP INITIALIZATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-09-30">30 Sep 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
							<email>han@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp;M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
							<email>tzhao@snap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
							<email>xia.hu@rice.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
							<email>nshah@snap.com</email>
							<affiliation key="aff1">
								<orgName type="department">Snap Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 40 45 50 Epoch 0.45 0.50 0.55 0</addrLine>
									<postCode>60 0.65</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<addrLine>0 5 10 15 20 25 30 35 40 45 50 Epoch 0.625 0.650 0</addrLine>
									<postCode>675 0.700</postCode>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<address>
									<addrLine>GraphSAGE) 0 5 10 15 20 25 30 35 40 45 50 Epoch 0.4 0.5 0.6</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MLPINIT: EMBARRASSINGLY SIMPLE GNN TRAIN-ING ACCELERATION WITH MLP INITIALIZATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-09-30">30 Sep 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2210.00102v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training graph neural networks (GNNs) on large graphs is complex and extremely time consuming. This is attributed to overheads caused by sparse matrix multiplication, which are sidestepped when training multi-layer perceptrons (MLPs) with only node features. MLPs, by ignoring graph context, are simple and faster for graph data, however they usually sacrifice prediction accuracy, limiting their applications for graph data. We observe that for most message passing-based GNNs, we can trivially derive an analog MLP (we call this a PeerMLP) whose weights can be made identical, making us curious about how do GNNs using weights from a fully trained PeerMLP perform? Surprisingly, we find that GNNs initialized with such weights significantly outperform their PeerMLPs for graph data, motivating us to use PeerMLP training as a precursor, initialization step to GNN training. To this end, we propose an embarrassingly simple, yet hugely effective initialization method for GNN training acceleration, called MLPInit. Our extensive experiments on multiple large-scale graph datasets with diverse GNN architectures validate that MLPInit can accelerate the training of GNNs (up to 33? speedup on OGB-products) and often improve prediction performance (e.g., up to 7.97% improvement for GraphSAGE across 7 datasets for node classification, and up to 17.81% improvement across 4 datasets for link prediction on metric Hits@10). Most importantly, MLPInit is extremely simple to implement and can be flexibly used as a plug-and-play initialization method for message passing-based GNNs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNNs) <ref type="bibr" target="#b66">(Zhang et al., 2018;</ref><ref type="bibr" target="#b69">Zhou et al., 2020;</ref><ref type="bibr" target="#b52">Wu et al., 2020)</ref> have attracted considerable attention from both academic and industrial researchers and have shown promising results on various practical tasks, e.g., recommendation <ref type="bibr" target="#b9">(Fan et al., 2019;</ref><ref type="bibr" target="#b39">Sankar et al., 2021;</ref><ref type="bibr" target="#b55">Ying et al., 2018;</ref><ref type="bibr" target="#b44">Tang et al., 2022)</ref>, knowledge graph analysis <ref type="bibr" target="#b0">(Arora, 2020;</ref><ref type="bibr" target="#b36">Park et al., 2019;</ref><ref type="bibr" target="#b49">Wang et al., 2021)</ref>, forecasting <ref type="bibr">(Jiang &amp; Luo, 2022;</ref><ref type="bibr" target="#b43">Tang et al., 2020)</ref> and chemistry analysis <ref type="bibr" target="#b31">(Li et al., 2018b;</ref><ref type="bibr" target="#b56">You et al., 2018;</ref><ref type="bibr" target="#b6">De Cao &amp; Kipf, 2018;</ref><ref type="bibr" target="#b33">Liu et al., 2022)</ref> However, training GNN on large-scale graphs is extremely time-consuming and costly in practice, thus spurring considerable work dedicated to scaling up the training of GNNs, even necessitating new massive graph learning libraries <ref type="bibr" target="#b64">(Zhang et al., 2020;</ref><ref type="bibr" target="#b10">Ferludin et al., 2022)</ref> for large-scale graphs.</p><p>Recently, several approaches for more efficient GNNs training have been proposed, including novel architecture design <ref type="bibr" target="#b51">(Wu et al., 2019;</ref><ref type="bibr">You et al., 2020d;</ref><ref type="bibr" target="#b29">Li et al., 2021)</ref>, data reuse and partitioning paradigms <ref type="bibr" target="#b48">(Wan et al., 2022;</ref><ref type="bibr" target="#b12">Fey et al., 2021;</ref><ref type="bibr" target="#b62">Yu et al., 2022)</ref> and graph sparsification <ref type="bibr" target="#b4">(Cai et al., 2020;</ref><ref type="bibr">Jin et al., 2021b)</ref>. However, these kinds of methods often sacrifice prediction accuracy and increase modeling complexity, while meriting sometimes significant additional engineering efforts.</p><p>MLPs are used to accelerate GNNs <ref type="bibr" target="#b65">(Zhang et al., 2021;</ref><ref type="bibr" target="#b14">Frasca et al., 2020;</ref><ref type="bibr" target="#b20">Hu et al., 2021)</ref> by decoupling GNNs to node features learning and graph structure learning. Our work also leverages MLPs, but adopts a distinct perspective. Notably, we observe that the weight space of MLPs and GNNs can be identical, which enable us to transfer weights between MLP and GNN models. With that MLPs train faster than GNNs in mind, this observation inspired us to raise the following question:</p><p>Can we train GNNs more efficiently by leveraging the weights of converged MLPs? indicates the best performance that GNNs with random initialization can achieve. indicates the comparable performance of the GNN with MLPInit. Reduced Time indicates the training time reduced by our proposed MLPInit compared to random initialization. This experimental result shows that MLPInit is able to accelerate the training of GNNs significantly.</p><p>To answer this question, we first pioneer a thorough investigation to reveal the relationship between the MLPs and GNNs in terms of trainable weight space. For ease of presentation, we define the PeerMLP of a GNN<ref type="foot" target="#foot_0">1</ref> so that GNN and its PeerMLP share the same weights<ref type="foot" target="#foot_1">2</ref> . We find that interestingly, GNNs can be optimized by training the weights of their PeerMLP. Based on this observation, we adopt weights of converged PeerMLP as the weights of corresponding GNNs and find that these GNNs perform even better that converged PeerMLP on node classification tasks (results in Table <ref type="table" target="#tab_2">2</ref>).</p><p>Motivated by this, we propose an embarrassingly simple, yet remarkably effective method to accelerate GNNs training by initializing GNN with the weights of its converged PeerMLP. Specifically, to train a target GNN, we first train its PeerMLP and then initialize the GNN with the optimal weights of converged PeerMLP. We present the experimental results in Figure <ref type="figure" target="#fig_0">1</ref> to show the training speed comparison of GNNs with random initialization and with MLPInit. In Figure <ref type="figure" target="#fig_0">1</ref>, Reduced Time shows the training time reduced by our proposed MLPInit compared to random initialized GNN, while achieving the same test performance. This experimental result shows that MLPInit is able the accelerate the training of GNNs significantly: for example, we speed up the training of commonly used Graph-SAGE, GraphSAINT, ClusterGCN, GCN by 2.48?, 3.94?, 2.06?, 1.91? on OGB-arXiv dataset, indicating the superiority of our method in GNNs training acceleration. And moreover, we speed up the training of GraphSAGE more than 14? on OGB-products dataset.</p><p>We highlight our contributions as follows:</p><p>? We pioneer a thorough investigation to reveal the relationship between MLPs and GNNs in terms of the trainable weight space through the following observations: (i) GNNs and MLPs have the same weight space. (ii) GNNs can be optimized by training the weights of their PeerMLPs. (iii) GNN with weights from its converged PeerMLP surprisingly performs better than the performance of its converged PeerMLP on node classification tasks. while often improving the model performance<ref type="foot" target="#foot_2">3</ref> (e.g., 7.97% improvement for node classification on GraphSAGE and 17.81% improvement for link prediction on Hits@10 ). ? MLPInit is extremely easy to implement and has virtually negligible computational overhead compared to the conventional GNN training schemes. In addition, it is orthogonal to other GNN acceleration methods, such as weight quantization and graph coarsening, further increasing headroom for GNN training acceleration in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>Notations. We denote an attributed graph G = (X, A), where</p><formula xml:id="formula_0">X = {x 1 , x 2 , ? ? ? , x N } ? R N ?D</formula><p>is the node feature matrix and A = {0, 1} N ?N is the binary adjacency matrix. N is the number of nodes and D is the dimension of node feature. For the node classification task, we denote the prediction targets denoted by Y ? {0, 1, . . . , C -1} N , where C is the number of classes. We denote a GNN model as f gnn (X, A; w gnn ), and an MLP as f mlp (X; w mlp ), where w gnn and w mlp denote the trainable weights in the GNN and MLP, respectively. Moreover, w * gnn and w * mlp denote the fixed weights of optimal (or converged) GNN and MLP, respectively.</p><p>Graph Neural Networks. Although various forms of graph neural networks (GNNs) exist, our work refers to the conventional message passing flavor <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>. These models work by learning a node's representation by aggregating information from the node's neighbors recursively.</p><p>One simple yet widely popular GNN instantiation is the graph convolutional network (GCN), whose multi-layer from can be written concisely: the representation vectors of all nodes at the l-th layer is H l = ?(AW l H l-1 ), where ?(?) denotes activation function, W l is the trainable weights of the l-th layer, and H l-1 is the node representations output by the previous layer. Denoting the output of the last layer of GNN by H, for a node classification task, the prediction of node label is ? = softmax(H). For a link prediction task, one can predict the edge probabilities with any suitable decoder, e.g., commonly used inner-product decoder as</p><formula xml:id="formula_1">? = sigmoid(H ? H T ) (Kipf &amp; Welling, 2016b).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MOTIVATING ANALYSES</head><p>In this section, we reveal that MLPs and GNNs share the same weight space, which facilitates the transferability of weights between the two architectures. Through this section, we use GCN (Kipf &amp; Welling, 2016a) as a prototypical example for GNNs for notational simplicity, but we note that our discussion is generalizable to other message passing GNN architectures.</p><p>Motivation 1: GNNs share the same weight space with MLPs. To show the weight space of GNNs and MLPs, we present the mathematical expression of one layer of <ref type="bibr">MLP and GCN (Kipf &amp; Welling, 2016a)</ref> as follows:</p><p>GNN: H l = ?(AW l gnn H l-1 ), MLP:</p><formula xml:id="formula_2">H l = ?(W l mlp H l-1 ),<label>(1)</label></formula><p>where W l gnn and W l mlp are the trainable weights of l-th layer of MLP and GCN, respectively. If we set the hidden layer dimensions of GNNs and MLPs to be the same, then W l mlp and W l gnn will naturally have the same size. Thus, although the GNN and MLP are different models, their weight spaces can be identical. Moreover, for any GNN model, we can trivially derive a corresponding MLP whose weight space can be made identical. For brevity, and when context of a GNN model is made clear, we can write such an MLP which shares the same weight space as a PeerMLP, i.e., their trainable weights can be transferred to each other.</p><p>Motivation 2: MLPs train faster than GNNs. GNNs train slower than MLPs, owing to their non-trivial relational data dependency. We empirically validate that training MLPs is much faster than training GNNs in Table <ref type="table" target="#tab_1">1</ref>. Specifically, this is because MLPs do not involve the sparse matrix multiplication for neighbor aggregation. A GNN layer (here we consider a simple GCN layer, as defined in Equation ( <ref type="formula" target="#formula_2">1</ref>)) can be broken down into two operations: feature transformation (Z = W l H l-1 ) and neighbor aggregation (H l = AZ) <ref type="bibr" target="#b34">(Ma et al., 2021)</ref>. The neighbor aggregation and feature transformation are typically a sparse and dense matrix multiplications, respectively. Table <ref type="table" target="#tab_1">1</ref> shows the time usage for these different operations on several real-world graphs. As expected, neighbor aggregation in GNNs consumes the large majority of computation time. For example, on the Yelp dataset, the neighbor aggregation operation induces a 3199? time overhead.</p><p>Given that the weights of GNNs and their PeerMLP can be transferred to each other, but the PeerMLP can be trained much faster, we raise the following questions:</p><p>1. What will happen if we directly adopt the weights of a converged PeerMLP to GNN? 2. To what extent can PeerMLP speed up GNN training and improve GNN performance?</p><p>In this paper, we try to answer these questions with comprehensive empirical analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WHAT WILL HAPPEN IF WE DIRECTLY ADOPT THE WEIGHTS OF A CONVERGED PEERMLP TO GNN?</head><p>To answer this question, we conducted comprehensive preliminary experiments to investigate weight transferability between MLPs and GNNs. We made the following interesting and inspiring findings:</p><p>Observation 1: The training loss of GNN will decrease by optimizing the weights of its PeerMLP. We conducted a verification experiment to investigate the loss changes of the GNNs with the weights trained from its PeerMLP and present the results in Figure <ref type="figure">2</ref>. In this experiment, we have two models, a GNN and its corresponding PeerMLP, who share the same weights w mlp That is, the PeerMLP is f mlp (X; w mlp ) and the GNN is f gnn (X, A; w mlp ). We optimize the weights w mlp by training the PeerMLP, and the loss curve of f mlp (X; w mlp ) is the blue line in the left figure in Figure 2. We also compute the loss of GNN f gnn (X, A; w mlp ) with the weights from PeerMLP. The loss curve of f gnn (X, A; w mlp ) is shown in the red line. Figure <ref type="figure">2</ref> shows the surprising phenomenon that the training loss of GNN with weights trained from PeerMLP decreases consistently. Impressively, these weights (w mlp ) were derived without employing neighbor aggregation in training. Observation 2: Converged weights from PeerMLP provide a good GNN initialization.</p><p>As PeerMLP and GNN have the same weight spaces, a natural follow-up question is whether GNN can directly adopt the weights of the converged PeerMLP and perform well. We next aim to understand this question empirically. Specifically, we first trained a PeerMLP for a target GNN and obtain the optimal weights w * mlp . Next, we run inference on test data using a GNN with w * mlp of PeerMLP, i.e., applying f gnn (X, A; w * mlp ). Table <ref type="table" target="#tab_2">2</ref> shows the results of f mlp (X; w * mlp ) and f gnn (X, A; w * mlp ). We can observe that the GNNs with the optimal weights of PeerMLP consistently outperform PeerMLP, indicating that the weights from converged PeerMLP can serve as good enough initialization of the weights of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">THE PROPOSED METHOD: MLPINIT</head><p>The above findings show that MLPs can help the training of GNNs. In this section, we formally present our method MLPInit, which is an embarrassingly simple, yet extremely effective approach to accelerate GNN training.</p><p>The basic idea of MLPInit is straightforward: we adopt the weights of a converged PeerMLP to initialize GNN, and subsequently fine-tune the GNN. Specifically, for a target GNN (f gnn (X, A; w gnn )), we first construct a PeerMLP (f mlp (X, A; w mlp )), with matching target weights. Next, we optimize the weight of the PeerMLP model by training the PeerMLP solely with the node features X for m epochs. Upon training the PeerMLP to convergence and obtaining the optimal weights (w * mlp ), we initialize the GNN with w * mlp and then fine-tune the GNN with n epochs. We present PyTorch-style pseudo-code of MLPInit in node classification setting in Algorithm 1.  <ref type="table" target="#tab_4">3</ref> shows that N is generally much larger than n.</p><p>Ease of Implementation. MLPInit is extremely easy to implement as shown in Algorithm 1. First, we construct a MLP (PeerMLP), which has the same weights with the target GNN.</p><p>Next, we use the node features X and node labels Y to train the PeerMLP to converge. Then, we adopt the weights of converged PeerMLP to the GNN, and fine-tune the GNN while additionally leveraging the adjacency A. In addition, our method can also directly serve as the final, or deployed GNN model, in resource constrained settings: assuming n = 0, we can simply train the PeerMLP and adopt w * mlp directly. This reduces training cost further, while enabling us to serve a likely higher performance model in deployment or test settings, as Table <ref type="table" target="#tab_2">2</ref> shows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DISCUSSION</head><p>In the section, we discuss the relation between MLPInit and existing methods. Since we position MLPInit as an acceleration method involve MLP, we first compare it with MLP-based GNN Acceleration Methods. Additionally, we also compare it with GNN Pre-training methods.</p><p>Comparison to MLP-based GNN Acceleration Methods. Recently, several works aim simplify the GNN to MLP-based constructs during training or inference <ref type="bibr" target="#b67">(Zhang et al., 2022;</ref><ref type="bibr" target="#b51">Wu et al., 2019;</ref><ref type="bibr" target="#b14">Frasca et al., 2020;</ref><ref type="bibr" target="#b41">Sun et al., 2021;</ref><ref type="bibr" target="#b21">Huang et al., 2020;</ref><ref type="bibr" target="#b20">Hu et al., 2021)</ref>. Our method is proposed to accelerate the message passing based GNN for large-scale graphs. Thus, MLP-based GNN acceleration is a completely different line of work compared to ours since it removes the message passing in the GNNs and use MLP to model graph structure instead. Thus, MLP-based GNN acceleration methods are out of scope of the discussion in this work.</p><p>Comparison to GNN Pre-training Methods. Our proposed MLPInit are orthogonal to the GNN pre-training methods <ref type="bibr">(You et al., 2020b;</ref><ref type="bibr">Zhu et al., 2020b;</ref><ref type="bibr">Veli?kovi? et al., 2018b;</ref><ref type="bibr" target="#b61">You et al., 2021;</ref><ref type="bibr" target="#b38">Qiu et al., 2020;</ref><ref type="bibr" target="#b72">Zhu et al., 2021;</ref><ref type="bibr" target="#b18">Hu et al., 2019)</ref>. GNN pretraining typically leverages graph augmentation to pretrain weights of GNNs or obtain the node representation downstream task usage. Comparing with the pre-training methods, MLPInit has two main differences (or advantages) that significantly contribute to the speed up: (i) the training of PeerMLP does not involve using the graph structure data, while pre-training methods rely on it. (ii) Pre-training methods usually involves graph data augmentation <ref type="bibr" target="#b68">(Zhao et al., 2022;</ref><ref type="bibr" target="#b38">Qiu et al., 2020)</ref>, which requires additional training time.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In the next subsections, we conduct and discuss experimental results to understand MLPInit from the following aspects: (i) training speedup, (ii) performance improvements, (iii) hyperparameter sensitivity, (iv) robustness and loss landscape. For node classification, we consider Flickr, Yelp, Reddit, Reddit2, A-products, and two OGB datasets <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>, OGB-arXiv and OGB-products as benchmark dataset. We adopt GCN (w/ mini-batch) (Kipf &amp; Welling, 2016a), GraphSAGE <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref>, GraphSAINT <ref type="bibr" target="#b63">(Zeng et al., 2019)</ref> and ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref>. The details of datasets and baselines are in Appendices B.1 and B.2, respectively. For link prediction task, we consider Cora, CiteSeer, PubMed, CoraFull, CS, Physics, A-Photo and A-Computers as our datasets. Our link prediction setup is using as GCN as encode which transform a graph to node representation H and then use an inner-product decoder ? = sigmoid(H ? H T ) to predict the probability of the link existence, which is discussed in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">HOW MUCH CAN MLPINIT ACCELERATE GNN TRAINING?</head><p>In this section, we compare the training of GNNs with random initialization and MLPInit. We compute training epochs needed by GNNs with random initialization to achieve the best test performance. We also compute the running epochs needed by GNNs with MLPInit to achieve comparable test performance. We present the comparison in Table <ref type="table" target="#tab_4">3</ref>. We also plotted the loss and accuracy curves of different GNNs on OGB-arXiv in Figure <ref type="figure" target="#fig_1">3</ref>. We made the following major observations:  In this section, we conducted experiments to show the superiority of the proposed method in terms of final, converged GNN model performance on node classification and link prediction tasks. The reported test performance of both random initialization and MLPInit are selected based on the validation data. We present the performance improvement of MLPInit compared to random initialization in Tables <ref type="table" target="#tab_5">4</ref> and<ref type="table" target="#tab_6">5</ref> for node classification and link prediction, respectively.</p><p>Observation 4: MLPInit improves the prediction performance for both node classification and link prediction task in most cases. Table <ref type="table" target="#tab_5">4</ref> shows our proposed method gains 7.97%, 7.00%, 6.61%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Init MLPInit</head><p>OGB-products OGB-arXiv and 14.00% improvements for GraphSAGE, GraphSAINT, ClusterGCN and GCN on average cross all the datasets for node classification task. The results in Table <ref type="table" target="#tab_6">5</ref> and Table <ref type="table" target="#tab_7">6</ref> show our proposed method gains 1.05%, 1.10%, 17.81%, 20.97%, 14.88%,10.46% on average cross various metrics for link prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">IS MLPINIT ROBUST UNDER DIFFERENT HYPERPARAMETERS?</head><p>In practice, one of the most time-consuming parts of training large-scale GNNs is hyperparameter tuning <ref type="bibr">(You et al., 2020a)</ref>. Here, we perform experiments to investigate the sensitivity of MLPInit to various hyperparameters, including the architecture hyperparameters and training hyperparameters. Observation 5: MLPInit makes GNNs less sensitive to hyperparameters and improves the overall performance across various hyperparameters. In this experiment, we trained PeerMLP and GNN with different "Training HP" (Learning rate, weight decay and batch size) and "Architecture HP" (i.e., layers, number of hidden neurons, ) and we presented the learning curves of GNN with different hyperparameters in Figure <ref type="figure" target="#fig_3">4</ref>. One can see from the results that GNNs trained with MLPInit have much smaller standard deviation than that with random initialization. Moreover, MLPInit consistently outperforms random initialization in task performance. This advantage allows our approach to save time in searching for architectural hyperparameters. In practice, different datasets require different model parameters. Using our proposed method, we can generally choose random hyperparameters and obtain reasonable and relatively stable performance owing to the PeerMLP's lower sensitivity to hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">WILL MLPINIT FACILITATE BETTER CONVERGENCE FOR GNNS?</head><p>In this experiment, we performed experiments to analyze the convergence of a fine-tuned GNN model. In other words, does this pre-training actually help find better local minima?</p><p>Observation 6: MLPInit actually helps find better local minima for GNNs. The geometry of loss surfaces (e.g., flatness) reflects the properties of the learned model, which provides various insights to assess the generalization ability <ref type="bibr" target="#b13">(Fort et al., 2019;</ref><ref type="bibr" target="#b22">Huang et al., 2019;</ref><ref type="bibr" target="#b50">Wen et al., 2018)</ref> and robustness <ref type="bibr" target="#b13">(Fort et al., 2019;</ref><ref type="bibr" target="#b32">Liu et al., 2020;</ref><ref type="bibr" target="#b8">Engstrom et al., 2019)</ref> of a neural network. In this experiment, we plot the loss landscape of GNN (GraphSAGE) with random weight initialization and MLPInit using the visualization tool introduced in <ref type="bibr">Li et al. (2018a)</ref>. The loss landscape are plotted based on the training loss and with OGB-arXiv and OGB-products datasets. The loss landscapes are shown in Figure <ref type="figure">5</ref>. For the fair comparison, two random direction of the loss landscape are the same and the lowest values of losses in the landscape are the same within one dataset, 1.360 for OGB-arXiv and 0.340 for OGB-products. From the loss landscape, we can see that the basin with converged minimum has larger area in terms of the same level of loss in each dataset's (row's) plots, indicating the loss landscape of the model trained with MLPInit are more flat than the model with random initialization. In summary, MLPInit helps find better local minima for GNNs.</p><p>Observation 7: MLPInit actually helps find better local minima for graph neural networks. To further understand the training procedure of MLPInit, we visualize the training trajectories along with loss landscapes through tools <ref type="bibr">(Li et al., 2018a)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we present several lines of related work.</p><p>Massage passing-based GNNs. Graph neural networks typically follows the massage passing mechanism, which aggregates the information form node's neighbors and learns node representation for downstream tasks. Following with the pioneer work GCN (Kipf &amp; Welling, 2016a), lots of work <ref type="bibr">(Veli?kovi? et al., 2018a;</ref><ref type="bibr" target="#b53">Xu et al., 2018;</ref><ref type="bibr">Balcilar et al., 2021;</ref><ref type="bibr" target="#b45">Thorpe et al., 2021;</ref><ref type="bibr" target="#b3">Brody et al., 2021;</ref><ref type="bibr" target="#b42">Tailor et al., 2021)</ref> are proposed to improve or understand massage passing-based GNNs.</p><p>Efficient GNNs. In order to scale GNNs to large graphs, the efficiency of GNNs attracted lots of attentions recently. subgraph sampling techniques <ref type="bibr" target="#b16">(Hamilton et al., 2017;</ref><ref type="bibr" target="#b5">Chiang et al., 2019;</ref><ref type="bibr" target="#b63">Zeng et al., 2019)</ref> have been proposed for efficient mini-batch training for large-scale graphs. All of this kind of methods follow the message passing mechanism for efficient GNN training. There is another line of work on scalable GNNs is to use MLP to simplify GNNs <ref type="bibr" target="#b67">(Zhang et al., 2022;</ref><ref type="bibr" target="#b51">Wu et al., 2019;</ref><ref type="bibr" target="#b14">Frasca et al., 2020;</ref><ref type="bibr" target="#b41">Sun et al., 2021;</ref><ref type="bibr" target="#b21">Huang et al., 2020;</ref><ref type="bibr" target="#b20">Hu et al., 2021)</ref>, which decouples the feature aggregation and transformation operations to avoid feature aggregation operation. We compared our work with this line of work in Section 4.2. There are also other works such as weights quantization and graph sa and graph sparsification <ref type="bibr" target="#b4">(Cai et al., 2020)</ref>. However, these kinds of methods often sacrifice prediction accuracy and increase modeling complexity, while meriting sometimes significant additional engineering efforts.</p><p>Pre-training GNNs. The recent GNN pretraining methods mainly adopt contrastive learning <ref type="bibr" target="#b17">(Hassani &amp; Khasahmadi, 2020;</ref><ref type="bibr" target="#b38">Qiu et al., 2020;</ref><ref type="bibr">Zhu et al., 2020b;</ref><ref type="bibr">2021;</ref><ref type="bibr" target="#b61">You et al., 2021;</ref><ref type="bibr">2020c;</ref><ref type="bibr">Jin et al., 2021a;</ref><ref type="bibr" target="#b18">Hu et al., 2019;</ref><ref type="bibr">Zhu et al., 2020a)</ref>. GNN pretraining typically leverages graph augmentation to pretrain weights of GNNs or obtain the node representation downstream task usage or generative objectives. For example, Grace <ref type="bibr">Zhu et al. 2020b</ref> maximizes the agreement between two view of one graph. This line of work usually involves graph data augmentation, which requires additional training time and engineering effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>This work presents a simple yet effective initialization method, MLPInit, to accelerate the training of GNNs, which adopts the weights form their converged PeerMLP initialize GNN and then finetune GNNs. With comprehensive experimental evidence, we demonstrate that superiority of our proposed method on training speedup (up to 33? speedup on OGB-products), downstream task performance improvements(up to 7.97% performance improvement for GraphSAGE) and robustness improvements (larger minimal area in loss landscape) on the resulting GNNs. Notably, our proposed method is easy to implement and employ in real applications to speed up the training of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL EXPERIMENTS</head><p>In this appendix, we present additional experiments to show the superiority of our proposed method, MLPInit, including additional result for link prediction and training curves. We also present more analysis on the weights distribution changes of MLPInit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 ADDITIONAL EXPERIMENTAL LINK PREDICTION</head><p>Here, we present the additional experiment results on link prediction task in Table <ref type="table" target="#tab_7">6</ref>, which is similar to our results in Table <ref type="table" target="#tab_6">5</ref>, but on more datasets. In general, we observe that GNN with MLPInit outperforms that with random initialization on link prediction task on these additional datasets.</p><p>MLPInit does not gain better performance on A-products dataset. We conjecture that the reason for this is that node features may contain less task-relevant information on A-products. Prior work <ref type="bibr">(Li et al., 2018a)</ref> suggests that "small weights still appear more sensitive to perturbations, and produce sharper looking minimizers." To this end, we explore the distribution of weights of GNNs with both random initialization and MLPInit, and present the results in Figure <ref type="figure">7</ref>. We can observe that with the same number of training epochs, the weights of GraphSAGE with MLPInit produce more high-magnitude (both positive and negative) weights, indicating the MLPInit can help the optimization of GNN. This difference stems from a straightforward reason: MLPInit provides a good initialization for GNNs since the weight are trained by the PeerMLP before (also aligning with our observations in Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ADDITIONAL HYPERPARAMETER SENSITIVITY</head><p>In this appendix, we present the additional results to explore more on the sensitivity to the various hyperparameters. The results are the full version of Figure <ref type="figure">8</ref>. In the appendix, we present the details of datasets and baselines for node classification and link prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 DATASETS FOR NODE CLASSIFICATION</head><p>The details of datasets used for node classification are listed as follows:</p><p>? Yelp <ref type="bibr" target="#b63">(Zeng et al., 2019)</ref> contains customer reviewers as nodes and their friendship as edges.</p><p>The node feature are the low-dimensional review representation for their reviews.</p><p>? Flickr <ref type="bibr" target="#b63">(Zeng et al., 2019)</ref> contains customer reviewers as nodes and their common properties as edges. The node features the 500-dimensional bag-of-word representation of the images.</p><p>? Reddit, Reddit2 <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref> is constructed by Reddit posts. The node in this dataset is post and belonging to different communities. Reddit2 is the sparser version of Reddit by deleting some edges.</p><p>? A-products <ref type="bibr" target="#b63">(Zeng et al., 2019)</ref> contains products and its categories.</p><p>? OGB-arXiv <ref type="bibr" target="#b19">(Hu et al., 2020)</ref> is the citation network between all arXiv papers. Each node denotes an paper and each edge denotes citation between two papers. The node features are the average 128-dimensional word vector of its title and abstract.</p><p>? OGB-products <ref type="bibr" target="#b19">(Hu et al., 2020;</ref><ref type="bibr" target="#b5">Chiang et al., 2019)</ref> is Amazon product co-purchasing network. Nodes represent products in Amazon, and edges between two products indicate that the products are purchased together. Node features in this dataset are low-dimensional representation of the product description text.</p><p>We present the statistic of datasets used for node classification task in Table <ref type="table" target="#tab_8">7</ref>. We present the details of GNN models as follows:</p><p>? GCN (Kipf &amp; Welling, 2016a; <ref type="bibr" target="#b16">Hamilton et al., 2017)</ref> is the original graph convolution network, which aggregates neighbor's information to obtain node representation. In our experiment, we train GCN in a mini-batch fashion by adopting a subgraph sampler from <ref type="bibr" target="#b16">Hamilton et al. (2017)</ref>.</p><p>? GraphSAGE <ref type="bibr" target="#b16">(Hamilton et al., 2017)</ref> proposes a graph sampling based training strategy to scale up graph neural networks. It samples a fixed number of neighbors per node and training the GNNs in a mini-batch fashion.</p><p>? GraphSAINT <ref type="bibr" target="#b63">(Zeng et al., 2019</ref>) is a graph sampling-based method to train GNNs on large-scale graphs, which proposes a set of graph sampling algorithms to partition graph data into subgraphs. This method also present normalization techniques to eliminate biases during the graph sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preprint</head><p>? ClusterGCN <ref type="bibr" target="#b5">(Chiang et al., 2019)</ref> is proposed to train GCN algorithm in small batches by using the graph clustering structure. This approach samples the nodes associated with the dense subgraphs identified by the graph clustering algorithm. Then the GCN is trained by the subgraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 DATASETS FOR LINK PREDICITON</head><p>For link prediction task, we consider Cora, CiteSeer, PubMed, CoraFull, CS, Physics, A-Photo and A-Computers as our baselines. The details of the datasets used for node classification are listed as follows:</p><p>? Cora, CiteSeer, PubMed <ref type="bibr" target="#b54">(Yang et al., 2016)</ref> are representative citation network datasets. These datasets contains a number of research papers, where nodes and edges denote documents and citation relationship, respectively. Node features are low-dimension representation for papers. Labels indicate the research field of documents.</p><p>? CoraFull <ref type="bibr">(Bojchevski &amp; G?nnemann, 2018</ref>) is a citation network that contains paper and their citation relationship. Labels are generated based on topics. This dataset is the original data of the entire network of Cora and Cora dataset in Planetoid is its subset.</p><p>? CS, Physics <ref type="bibr" target="#b40">(Shchur et al., 2018)</ref>  ? A-Photo, A-Computers <ref type="bibr" target="#b40">(Shchur et al., 2018)</ref> are two datasets from Amazon co-purchase dataset <ref type="bibr" target="#b35">McAuley et al. (2015)</ref>. Nodes in this dataset represent products while edges represent co-purchase relationship between two products. Node features are low-dimension representation of product reviews. Labels are categories of products.</p><p>We also present the statistic of dataset used for link prediction task in Table <ref type="table" target="#tab_10">8</ref>. Our link prediction setup is consistent with our discussion in Section 2, in which we use an innerproduct decoder ? = sigmoid(H?H T ) to predict the probability of the link existence. We presented the results in Tables <ref type="table" target="#tab_6">5</ref> and<ref type="table" target="#tab_7">6</ref>. Following standard benchmarks <ref type="bibr" target="#b19">(Hu et al., 2020)</ref>, the evaluation metrics adopted are AUC, Average Precision (AP), are hits ratio (Hit@#). The details of link predction are presented in Appendix C.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS</head><p>In this appendix, we present the hyperparameters used for node classification task and link prediction task for all models and dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The training speed comparison of the GNNs with Random initialization and MLPInit.indicates the best performance that GNNs with random initialization can achieve. indicates the comparable performance of the GNN with MLPInit. Reduced Time indicates the training time reduced by our proposed MLPInit compared to random initialization. This experimental result shows that MLPInit is able to accelerate the training of GNNs significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The training curves of different GNNs on OGB-arXiv. GNN with MLPInit generally obtain lower loss and higher accuracy that those with random initialization, and converge faster. The training curves are depicted based on ten runs. More experiment results are presented in Appendix A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: The loss landscape of GNN trained with random initialization (left) and MLPInit (right). The basin with converged minimum of MLPInit has larger area than that of random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training curves of GNNs with various hyperparameters. GNNs with MLPInit consistently outperforms random initialization and have smaller standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>in Figure 6. In this experiment, we use GraphSAGE on OGB-arXiv and OGB-products datasets, we first train a PeerMLP and then use the weights of the converged PeerMLP to initialize the GraphSAGE and fine-tune it. The we plot the training trajectories of PeerMLP and GraphSAGE on the loss landscape of GraphSAGE. The red line indicate the training trajectories of the training of PeerMLP and the blue line indicates the fine-tuning of GNNs. We can see that the end point of the training of MLP (red line) is close to the minima area in the loss landscape. The training trajectory clearly shows the reason why MLPInit works, i.e., the first-phase training of GNNs can be taken over by lightweight MLPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :Figure 9 :Figure 10 :</head><label>78910</label><figDesc>Figure 7: Weight histograms of GraphSAGE model weights with random initialization (blue) and MLPInit (red) on OGB-arXiv (left) and OGB-products (right). The results are plotted based on 20 epochs after training GraphSAGE with zero weight decay. Note that MLPInit produces more weights of higher magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? Based on the above observations, we proposed a embarrassingly simple yet surprisingly effective initialization method to accelerate the GNNs training. Our method, called MLPInit, initializes the weights of GNNs with the weight of its converged PeerMLP. After initialization, we observe that GNN training takes less than half epochs to converge than those with random initialization. Thus, MLPInit is able to accelerate the training of GNNs since the training MLPs is cheaper and faster than training GNNs. ? Comprehensive experimental results on multiple large-scale graphs with diverse GNNs validate that MLPInit is able to accelerate the training of GNNs (up to 33? speedup on OGB-products)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the running time of different operations in GNNs. We compute the time used for forward and backward of two operation. The time unit is milliseconds (ms). The relation of GNN and MLP during the training of PeerMLP. Left: Cross-Entropy Loss of f gnn (X, A; w mlp ) (GNN) and f mlp (X; w mlp ) (PeerMLP) over training epochs of PeerMLP. In this experiment, GNN and PeerMLP share the same weight w mlp , which are trained by the PeerMLP. Middle: training trajectory of PeerMLP on its own loss landscape. Right: training trajectory of GNN with weights from PeerMLP on GNN's loss landcape. The figures show that training loss of GNN with weights trained from MLP will decrease. The details are presented in Appendix C.2.</figDesc><table><row><cell>Operation</cell><cell></cell><cell>OGB-arXiv</cell><cell></cell><cell></cell><cell>Flickr</cell><cell></cell><cell></cell><cell>Yelp</cell><cell></cell></row><row><cell>#Nodes</cell><cell></cell><cell>169343</cell><cell></cell><cell></cell><cell>89250</cell><cell></cell><cell></cell><cell>716847</cell><cell></cell></row><row><cell>#Edges</cell><cell></cell><cell>1166243</cell><cell></cell><cell></cell><cell>899756</cell><cell></cell><cell></cell><cell>13954819</cell><cell></cell></row><row><cell></cell><cell>Forward</cell><cell>Backward</cell><cell>Total</cell><cell>Forward</cell><cell>Backward</cell><cell>Total</cell><cell>Forward</cell><cell>Backward</cell><cell>Total</cell></row><row><cell>Z = W X</cell><cell>0.32</cell><cell>1.09</cell><cell>1.42</cell><cell>0.28</cell><cell>0.97</cell><cell>1.26</cell><cell>1.58</cell><cell>4.41</cell><cell>5.99</cell></row><row><cell>H = AZ</cell><cell>1.09</cell><cell>1028.08</cell><cell>1029.17</cell><cell>1.01</cell><cell>836.95</cell><cell>837.97</cell><cell>9.74</cell><cell>19157.17</cell><cell>19166.90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>724?</cell><cell></cell><cell></cell><cell>665?</cell><cell></cell><cell></cell><cell>3199?</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">w  *  mlp</cell><cell></cell><cell></cell><cell>w  *  gnn</cell><cell></cell><cell></cell></row><row><cell>Figure 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of GNNs and its PeerMLP with the weights of a converged PeerMLP on test data.</figDesc><table><row><cell></cell><cell>Methods PeerMLP</cell><cell>GNN</cell><cell>Improv.</cell></row><row><cell>OGB-arXiv</cell><cell cols="3">GraphSAGE 56.04?0.27 62.87?0.95 ? 12.18% GraphSAINT 53.88?0.41 63.26?0.71 ? 17.41% ClusterGCN 54.47?0.41 60.81?1.30 ? 11.63% GCN 56.31?0.21 56.28?0.89 ? 0.04%</cell></row><row><cell>OGB-products</cell><cell cols="3">GraphSAGE 63.43?0.14 74.32?1.04 ? 17.16% GraphSAINT 57.29?0.32 69.00?1.54 ? 20.44% ClusterGCN 59.53?0.46 71.74?0.70 ? 20.51% GCN 62.63?0.15 71.11?0.10 ? 13.55%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 PyTorch-style Pseudocode of MLPInit Since training of the PeerMLP is comparatively cheap, and the weights of the converged PeerMLP can provide a good initialization for the corresponding GNN, the end result is that we can significantly reduce the training time of the GNN. Assum-</figDesc><table><row><cell># f_gnn: graph neural network model</cell></row><row><cell># f_mlp: PeerMLP of f_gnn</cell></row><row><cell># Train PeerMLP for N epochs</cell></row><row><cell>for X, Y in dataloader_mlp:</cell></row><row><cell>P = f_mlp(X)</cell></row><row><cell>loss = nn.CrossEntropyLoss(P, Y)</cell></row><row><cell>loss.backward()</cell></row><row><cell>optimizer_mlp.step()</cell></row></table><note><p># Initialize GNN with MLPInit torch.save(f_mlp.state_dict(), "w_mlp.pt") f_gnn.load_state_dict("w_mlp.pt") # Train GNN for n epochs for X, A, Y in dataloader_gnn: P = f_gnn(X, A) loss = nn.CrossEntropyLoss(P, Y) loss.backward() optimizer_gnn.step() Training Acceleration. ing that the training of GNN from a random initialization needs N epochs to converge, and N &gt;&gt; n, the total training time can be largely reduced given that MLP training time is negligible compared to GNN training time. The experimental results in Table</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Speed improvement when MLPInit achieves comparable performance with random initialized GNN. The number reported is the training epochs needed. (-) means our method can not reach comparable performance. The epoch used by Random/MLPInit is denoted as / in Figure1and N /n in Section 4.1. The detailed speedup computation method are presented in Appendix C.3.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Flickr</cell><cell cols="6">Yelp Reddit Reddit2 A-products OGB-arXiv OGB-products</cell><cell>Avg.</cell></row><row><cell>SAGE</cell><cell>Random( , N ) MLPInit ( , n) Improv.</cell><cell cols="4">45.6 39.9 1.14? 2.20? 4.93? 6.23? 44.7 36.0 48.0 20.3 7.3 7.7</cell><cell>48.9 40.8 1.20?</cell><cell>46.7 22.7 2.06?</cell><cell>43.0 2.9 20.22 44.7 14.83? 2.21?</cell></row><row><cell>SAINT</cell><cell>Random MLPInit Improv.</cell><cell>31.0 14.1 2.20?</cell><cell cols="3">35.8 0.0 -1.86? 4.64? 40.6 28.3 21.8 6.1</cell><cell>50.0 9.1 5.49?</cell><cell>48.3 19.5 2.48?</cell><cell>44.9 16.9 2.66? 3.18? 39.8 12.5</cell></row><row><cell>C-GCN</cell><cell>Random MLPInit Improv.</cell><cell cols="4">15.7 7.3 2.15? 2.24? 3.61? 2.76? 40.3 46.2 47.0 18.0 12.8 17.0</cell><cell>37.4 1.0 37.40?</cell><cell>42.9 10.9 3.94?</cell><cell>42.8 15.0 2.85? 3.32? 38.9 11.7</cell></row><row><cell>GCN</cell><cell>Random MLPInit Improv.</cell><cell cols="2">46.4 30.5 1.52? 1.91? 44.5 23.3</cell><cell>42.4 0.0 -</cell><cell>2.4 0.0 -</cell><cell>47.7 0.0 -</cell><cell>46.7 24.5 1.91?</cell><cell>43.8 1.3 33.69? 3.46? 39.1 11.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance improvement when GNN with random initialization and with MLPInit achieve best test performance, respectively. Mean and standard deviation are calculated based on ten runs. The best test performance for two methods are independently selected based on validation data.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>Flickr</cell><cell>Yelp</cell><cell>Reddit</cell><cell>Reddit2 A-products OGB-arXiv OGB-products</cell><cell>Avg.</cell></row><row><cell>SAGE</cell><cell cols="6">Random 53.72?0.16 63.03?0.20 96.50?0.03 51.76?2.53 77.58?0.05 72.00?0.16 80.05?0.35 MLPInit 53.82?0.13 63.93?0.23 96.66?0.04 89.60?1.60 77.74?0.06 72.25?0.30 80.04?0.62 Improv. ? 0.19% ? 1.43% ? 0.16% ? 73.09% ? 0.21% ? 0.36% ? 0.01% ? 7.97% 70.66 76.29</cell></row><row><cell>SAINT</cell><cell cols="6">Random 51.37?0.21 29.42?1.32 95.58?0.07 36.45?4.09 59.31?0.12 67.95?0.24 73.80?0.58 MLPInit 51.35?0.10 43.10?1.13 95.64?0.06 41.71?1.25 68.24?0.17 68.80?0.20 74.02?0.19 Improv. ? 0.05% ? 46.47% ? 0.06% ? 14.45% ? 15.06% ? 1.25% ? 0.30% ? 7.00% 59.12 63.26</cell></row><row><cell>C-GCN</cell><cell cols="6">Random 49.95?0.15 56.39?0.64 95.70?0.06 53.79?2.48 52.74?0.28 68.00?0.59 78.71?0.59 MLPInit 49.96?0.20 58.05?0.56 96.02?0.04 77.77?1.93 55.61?0.17 69.53?0.50 78.48?0.64 Improv. ? 0.02% ? 2.94% ? 0.33% ? 44.60% ? 5.45% ? 2.26% ? 0.30% ? 6.61% 65.04 69.34</cell></row><row><cell>GCN</cell><cell cols="6">Random 50.90?0.12 40.08?0.15 92.78?0.11 27.87?3.45 36.35?0.15 70.25?0.22 77.08?0.26 MLPInit 51.16?0.20 40.83?0.27 91.40?0.20 80.37?2.61 39.70?0.11 70.35?0.34 76.85?0.34 Improv. ? 0.51% ? 1.87% ? 1.49% ? 188.42% ? 9.22% ? 0.14% ? 0.29% ? 14.00% 56.47 64.38</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The performance of link prediction task. The results are based on ten runs. The experiments on other datasets are presented in Table6. More experiments are presented in Appendix A.1. MLPInit can significantly reduce the training time of GNNs. In this experiment, we summarize the epochs needed by GNN with random initialization to obtain the best performance, and then we calculate the epochs needed by GNN with MLPInit to reach the comparable performance on par with the random initialized GNN. We present the time speedup of MLPInit in Table3. Table3shows MLPInit speed up the training of GNNs by 2 -5 times generally and in some cases even more than 30 times. The consistent reduction of training epochs on different datasets demonstrates that MLPInit can generally speed up GNN training quite significantly.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>AUC</cell><cell>AP</cell><cell>Hits@10</cell><cell>Hits@20</cell><cell>Hits@50 Hits@100</cell></row><row><cell>PubMed</cell><cell>MLP random GNN random GNN mlpinit Improvement MLP random</cell><cell cols="5">94.76?0.30 94.28?0.36 14.68?2.60 24.01?3.04 40.02?2.75 96.66?0.29 96.78?0.31 28.38?6.11 42.55?4.83 60.62?4.29 97.31?0.19 97.53?0.21 37.58?7.52 51.83?7.62 70.57?3.12 ? 0.68% ? 0.77% ? 32.43% ? 21.80% ? 16.42% 95.20?0.18 95.53?0.25 28.70?3.73 39.22?4.13 53.36?3.81</cell><cell>54.85?2.03 75.14?3.00 81.42?1.52 ? 8.36% 64.83?1.95</cell></row><row><cell>DBLP A-Photo Physics</cell><cell>GNN random GNN mlpinit Improvement MLP random GNN random GNN mlpinit Improvement MLP random GNN random GNN mlpinit Improvement Avg.</cell><cell cols="5">96.29?0.20 96.64?0.23 36.55?4.08 43.13?2.85 59.98?2.43 96.67?0.13 97.09?0.14 40.84?7.34 53.72?4.25 67.99?2.85 ? 0.39% ? 0.47% ? 11.73% ? 24.57% ? 13.34% 86.18?1.41 85.37?1.24 4.36?1.14 6.96?1.28 12.20?1.24 92.07?2.14 91.52?2.08 9.63?1.58 12.82?1.72 20.90?1.90 93.99?0.58 93.32?0.60 9.17?2.12 13.12?2.11 22.93?2.56 ? 2.08% ? 1.97% ? 4.75% ? 2.28% ? 9.73% ? 11.32% 71.57?1.00 77.76?1.20 ? 8.65% 17.91?1.26 29.08?2.53 32.37?1.89 96.26?0.11 95.63?0.15 5.38?1.32 8.76?1.37 15.86?0.81 24.70?1.11 95.84?0.13 95.38?0.15 6.62?1.00 10.39?1.04 18.55?1.60 26.88?1.95 96.89?0.07 96.55?0.11 8.05?1.44 13.06?1.94 22.38?1.94 32.31?1.43 ? 1.10% ? 1.22% ? 21.63% ? 25.76% ? 20.63% ? 20.20% ? 1.05% ? 1.10% ? 17.81% ? 20.97% ? 14.88% ? 10.46%</cell></row><row><cell cols="7">Observation 3: 5.2 HOW WELL DOES MLPINIT PERFORM ON NODE CLASSIFICATION AND LINK PREDICTION</cell></row><row><cell></cell><cell>TASKS?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The performance of link prediction task. The results are based on ten runs.</figDesc><table><row><cell></cell><cell>Methods</cell><cell>AUC</cell><cell>AP</cell><cell cols="2">Hits@10 Hits@20</cell><cell>Hits@50 Hits@100</cell></row><row><cell></cell><cell>MLP random</cell><cell cols="5">91.87?1.25 92.16?0.99 46.98?4.33 58.05?6.44 76.03?4.17 86.28?3.05</cell></row><row><cell>Cora</cell><cell>GNN random GNN mlpinit</cell><cell cols="5">91.80?1.39 92.68?1.28 51.48?7.57 63.04?6.12 78.12?4.06 86.07?2.77 92.93?0.88 93.36?0.86 53.72?7.43 68.65?4.22 80.30?2.35 89.37?2.21</cell></row><row><cell></cell><cell cols="2">Improvement ? 1.23%</cell><cell>? 0.74%</cell><cell>? 4.35%</cell><cell>? 8.91%</cell><cell>? 2.79%</cell><cell>? 3.84%</cell></row><row><cell>CiteSeer</cell><cell cols="6">MLP random GNN random GNN mlpinit Improvement ? 0.73% 90.10?0.99 90.65?0.98 46.64?5.11 57.21?4.84 71.45?3.15 81.93?2.24 89.86?1.18 90.88?0.99 49.98?5.54 58.02?5.00 71.65?3.50 82.11?2.84 90.51?1.07 90.96?0.80 50.02?3.09 58.77?4.48 71.78?4.12 82.66?2.53 ? 0.08% ? 0.09% ? 1.29% ? 0.18% ? 0.67%</cell></row><row><cell></cell><cell>MLP random</cell><cell cols="5">96.29?0.12 95.79?0.13 13.36?1.49 19.67?2.21 33.46?2.17 46.82?1.91</cell></row><row><cell>CS</cell><cell>GNN random GNN mlpinit</cell><cell cols="5">96.11?0.08 95.75?0.10 14.27?2.77 22.57?2.52 35.40?2.01 48.21?2.00 96.72?0.10 96.49?0.14 16.96?3.37 25.44?3.00 40.69?2.99 53.78?2.00</cell></row><row><cell></cell><cell cols="2">Improvement ? 0.63%</cell><cell cols="4">? 0.77% ? 18.81% ? 12.70% ? 14.96% ? 11.55%</cell></row><row><cell>A-Computers</cell><cell cols="6">MLP random GNN random GNN mlpinit Improvement ? 1.11% 81.85?0.79 82.41?0.69 91.78?0.48 91.94?0.42 90.76?1.61 91.06?1.47 ? 0.96% ? 11.04% 2.10?0.48 7.60?1.47 11.10?1.74 18.64?1.94 25.42?2.15 4.13?0.86 7.83?0.95 12.18?1.01 6.76?3.27 11.11?1.82 17.40?2.58 24.59?2.56 ? 0.16% ? 6.65% ? 3.26%</cell></row><row><cell>CoraFull</cell><cell cols="6">MLP random GNN random GNN mlpinit Improvement ? 0.87% 95.72?0.18 95.55?0.23 19.38?4.71 27.83?3.27 42.98?2.01 57.20?1.27 95.87?0.36 95.77?0.42 21.33?4.77 30.57?3.49 45.08?3.46 59.58?2.53 96.71?0.16 96.73?0.22 25.78?4.92 36.68?5.36 53.81?2.34 66.73?1.96 ? 1.01% ? 20.87% ? 19.98% ? 19.37% ? 12.01%</cell></row><row><cell cols="7">A.2 WEIGHT DIFFERENCE OF GNNS WITH RANDOM INITIALIZATION AND MLPINIT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Statistics for datasets used for node classification.</figDesc><table><row><cell>Dataset</cell><cell># Nodes.</cell><cell cols="4"># Edges # Classes # Feat Density</cell></row><row><cell>Flickr</cell><cell>89,250</cell><cell>899,756</cell><cell>7</cell><cell>500</cell><cell>0.11?</cell></row><row><cell>Yelp</cell><cell>716,847</cell><cell>13,954,819</cell><cell>100</cell><cell>300</cell><cell>0.03?</cell></row><row><cell>Reddit</cell><cell cols="2">232,965 114,615,892</cell><cell>41</cell><cell>602</cell><cell>2.11?</cell></row><row><cell>Reddit2</cell><cell>232,965</cell><cell>23,213,838</cell><cell>41</cell><cell>602</cell><cell>0.43?</cell></row><row><cell>A-products</cell><cell cols="2">1,569,960 264,339,468</cell><cell>107</cell><cell>200</cell><cell>0.11?</cell></row><row><cell>OGB-arXiv</cell><cell>169,343</cell><cell>1,166,243</cell><cell>40</cell><cell>128</cell><cell>0.04?</cell></row><row><cell cols="2">OGB-products 2,449,029</cell><cell>61,859,140</cell><cell>47</cell><cell>218</cell><cell>0.01?</cell></row><row><cell cols="3">B.2 BASELINES FOR NODE CLASSIFICATION</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>are from Co-author dataset, which is co-authorship graph based on the Microsoft Academic Graph from the KDD Cup 2016 challenge. Nodes in this dataset are authors and edges indicate co-author relationship. Node features represent paper keywords. Labels indicate the research field of authors.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Statistics for datasets used for link prediction.</figDesc><table><row><cell>Dataset</cell><cell cols="4"># Nodes # Edges # Feat Density</cell></row><row><cell>Cora</cell><cell>2,708</cell><cell cols="2">5,278 1,433</cell><cell>0.72?</cell></row><row><cell>CiteSeer</cell><cell>3,327</cell><cell cols="2">4,552 3,703</cell><cell>0.41?</cell></row><row><cell>PubMed</cell><cell>19,717</cell><cell>44,324</cell><cell>500</cell><cell>0.11?</cell></row><row><cell>DBLP</cell><cell cols="3">17,716 105,734 1,639</cell><cell>0.34?</cell></row><row><cell>CoraFull</cell><cell cols="3">19,793 126,842 8,710</cell><cell>0.32?</cell></row><row><cell>A-Photo</cell><cell cols="2">7,650 238,162</cell><cell>745</cell><cell>4.07?</cell></row><row><cell>A-Computers</cell><cell cols="2">13,752 491,722</cell><cell>767</cell><cell>2.60?</cell></row><row><cell>CS</cell><cell cols="3">18,333 163,788 6,805</cell><cell>0.49?</cell></row><row><cell>Physics</cell><cell cols="3">34,493 495,924 8,415</cell><cell>0.14?</cell></row><row><cell cols="2">B.4 BASELINES FOR LINK PREDICITON</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The formal definition of PeerMLP is in Section 3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>By share the same weight, we mean that the trainable weights of GNN and its PeerMLP are the same in terms of size, dimension and values.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>By performance, we refer to the model prediction quality metric of the downstream task on the corresponding test data throughout the discussion.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/pyg-team/pytorch_geometric/blob/2.0.4/examples/link_pred.py</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We run our experiments on the machine with NVIDIA Tesla T4 GPU (16GB memory) and 60GB DDR4 memory to train the models. For A-products and OGB-products datasets, we run the experiments with one NVIDIA A100 GPU (40GB memory). The code is implemented based on PyTorch 1.9.0 <ref type="bibr" target="#b37">(Paszke et al., 2019)</ref> and PyTorch Geometric 2.0.4 <ref type="bibr" target="#b11">(Fey &amp; Lenssen, 2019)</ref>. The optimizer is Adam <ref type="bibr" target="#b26">(Kingma &amp; Ba, 2015)</ref> to train all GNNs and their PeerMLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 EXPERIMENT SETTING FOR FIGURE 2</head><p>In this experiment, we use GraphSAGE as GNN on OGB-arXiv dataset. We construct that PeerMLP(f mlp (X; w mlp ) ) for GraphSAGE (f gnn (X, A; w mlp )) and train it for 50 epochs. From the mathematical expression, GraphSAGE and its PeerMLP share the same weights w mlp and the weights w mlp are only trained by PeerMLP. We use the trained weights for GraphSAGE to do inference along the training procedure. For the landscape, suppose we have two optimal weight w * gnn and w * mlp for GraphSAGE and its PeerMLP, the middle one is the loss landscape based the PeerMLP with optimal with w * mlp while the the right one is the loss landscape based the GraphSAGE with optimal weights w * gnn .</p><p>C.3 EXPERIMENT SETTING FOR FIGURES 3, 9 AND 10 AND TABLES 3 AND 4</p><p>In this appendix, we present the detailed experiment setting for our main result Figures 3, 9 and 10 and Tables <ref type="table">3</ref> and<ref type="table">4</ref>. We construct the PeerMLP for each GNNs. And the we first train the PeerMLP Preprint for 50 epochs and save the best model with the best validation performance. And then we use the weight trained by PeerMLP to initialize the GNNs, then fine-tune the GNNs. To investigate the performance of GNNs, we fine-tune the GNNs for 50 epochs. We list the hyperparameters used in our experiments. We borrow the optimal hyperparameters from paper <ref type="bibr" target="#b7">(Duan et al., 2022)</ref>. And our code is also based on the official code 4 of paper <ref type="bibr" target="#b7">(Duan et al., 2022)</ref>. For datasets not included in paper <ref type="bibr" target="#b7">(Duan et al., 2022)</ref>, we use the heuristic hyperparameter setting for them.</p><p>C.4 EXPERIMENT SETTING FOR TABLE <ref type="table">2</ref> The GNN used in Table <ref type="table">2</ref> is GraphSAGE. We construct the PeerMLP for GraphSAGE on OGB-arXiv and OGB-products datasets. And the we first train the PeerMLP for 50 epochs and save the best model with the best validation performance. And we do the infer the test performance with PeerMLP and GraphSAGE with the weight trained by PeerMLP and we report the test performance in Table <ref type="table">2</ref>.</p><p>C.5 EXPERIMENT SETTING FOR TABLES 5 AND 6</p><p>In this appendix, we present the detailed experiment setting for link prediction task. We adopt the default setting for the official examples 5 of PyTorch Geometric 2.0.4. The GNN used for link prediction task is a 2-layer GCN, and the decoder is the commonly used inner-product decoder as</p><p>We explore two kinds of hyperparameters "Training HP" (Learning rate, weight decay, batch size and dropout) and "Architecture HP" (i.e., layers, number of hidden neurons) in this experiments.</p><p>? Training Hyperparameter ( Training HP ), total combinations: 2 ? 2 ? 2 ? 2 = 16 -Learning rate: {0.001, 0.0001} -Weight decay: {1e -4, 4e -4} -Batch size: {500, 1000} -Dropout: {0.2, 0.5}</p><p>? Architecture Hyperparameter ( Architecture HP ), total combinations: 3 ? 5 = 15 -Number of layers: {2, 3, 4} number of hidden neurons: {32, 64, 128, 256, 512}</p><p>In Figures <ref type="figure">4</ref> and<ref type="figure">8</ref>, we plotted the learning curves based on the mean and standard deviation over all the hyperparameters combinations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A survey on graph neural networks for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Arora</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12374</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">Muhammet</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renton</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>H?roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Ga?z?re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?bastien</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks?</title>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph coarsening with neural networks</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="257" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Molgan: An implicit generative model for small molecular graphs</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11973</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive study on large-scale graph training: Benchmarking and rethinking</title>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Keyu Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peihao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName><forename type="first">Logan</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph neural networks for social recommendation</title>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The world wide web conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Arno</forename><surname>Oleksandr Ferludin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Eigenwillig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Blais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Zelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sibon</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neslihan</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><surname>Bulut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03522</idno>
		<title level="m">Tf-gnn: Graph neural networks in tensorflow</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3294" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Fort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02757</idno>
		<title level="m">Deep ensembles: A loss landscape perspective</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Emanuele</forename><surname>Fabrizio Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><surname>Monti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11198</idno>
		<title level="m">Sign: Scalable inception graph neural networks</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Contrastive multi-view representation learning on graphs</title>
		<author>
			<persName><forename type="first">Kaveh</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hosein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khasahmadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4116" to="4126" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Graph-mlp: node classification without message passing in graph</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhecan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04051</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Combining label propagation and simple models out-performs graph neural networks</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Austin</forename><forename type="middle">R</forename><surname>Benson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13993</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding generalization through visualizations</title>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyad</forename><surname>Emam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Fowl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><forename type="middle">K</forename><surname>Terry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03291</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graph neural network for traffic forecasting: A survey. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">117921</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selfsupervised learning on graphs: Deep insights and new direction</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW Workshop</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph condensation for graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Variational graph auto-encoders</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training graph neural networks with 1000 layers</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6437" to="6449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03324</idno>
		<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On the loss landscape of adversarial training: Identifying challenges and how to overcome them</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21476" to="21487" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph rationalization with environment-based augmentations</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 28th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A unified view on graph neural networks as graph signal denoising</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1202" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating node importance in knowledge graphs using graph neural networks</title>
		<author>
			<persName><forename type="first">Namyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="596" to="606" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gcc: Graph contrastive coding for graph neural network pre-training</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1150" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph neural networks for friend ranking in large-scale social platforms</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2535" to="2546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Scalable and adaptive graph neural networks with selflabel-enhanced training</title>
		<author>
			<persName><forename type="first">Chuxiong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09376</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Do we need anisotropic graph neural networks?</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Shyam A Tailor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Opolka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lane</forename><surname>Donald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Knowing your fate: Friendship, action and temporal explanations for user engagement prediction on social apps</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2269" to="2279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Friend story ranking with edge-contextual local graph convolutions</title>
		<author>
			<persName><forename type="first">Xianfeng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1007" to="1015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grand++: Graph neural diffusion with a source term</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Thorpe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><forename type="middle">Minh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedi</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Strohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bns-gcn: Efficient full-graph training of graph convolutional networks with partition-parallelism and random boundary node sampling</title>
		<author>
			<persName><forename type="first">Youjie</forename><surname>Cheng Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">Sung</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingyan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="673" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mixed-curvature multi-relational graph neural network for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Shen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokai</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><forename type="middle">F</forename><surname>Cruz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1761" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Smoothout: Smoothing out sharp minima to improve generalization in deep learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yandan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunpeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.07898</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd ICML</title>
		<meeting>the 33rd ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph convolutional neural networks for web-scale recommender systems</title>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><surname>Eksombatchai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="974" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Design space for graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17009" to="17021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">When does self-supervision help graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10871" to="10880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">L2-gcn: Layer-wise and learned efficient training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2127" to="2135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph contrastive learning automated</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12121" to="12132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graphfm: Improving large-scale gnn training via feature momentum</title>
		<author>
			<persName><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="25684" to="25701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Agl: a scalable system for industrial-purpose graph machine learning</title>
		<author>
			<persName><forename type="first">Dalong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibang</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shuang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02454</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph-less neural networks: Teaching old mlps new tricks via distillation</title>
		<author>
			<persName><forename type="first">Shichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Graph convolutional networks: Algorithms, applications and open challenges</title>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Social Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="79" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Graph attention multi-layer perceptron</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeang</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.04355</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Graph data augmentation for graph machine learning: A survey</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08871</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Self-supervised training of graph convolutional networks</title>
		<author>
			<persName><forename type="first">Qikui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pingkun</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02380</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04131</idno>
		<title level="m">Deep graph contrastive representation learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
