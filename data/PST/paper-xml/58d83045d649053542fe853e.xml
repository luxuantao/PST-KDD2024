<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Path Confidence based Lookahead Prefetching</title>
				<funder ref="#_cF6qqgb">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_U7CPJKJ">
					<orgName type="full">Intel Corp</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jinchun</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp; M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seth</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
							<email>seth.h.pugsley@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Gratz</surname></persName>
							<email>pgratz@gratz1.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp; M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><forename type="middle">L Narasimha</forename><surname>Reddy</surname></persName>
							<email>reddy@tamu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Texas A&amp; M University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
							<email>chris.wilkerson@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
							<email>zeshan.a.chishti@intel.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Intel Labs</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Path Confidence based Lookahead Prefetching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Designing prefetchers to maximize system performance often requires a delicate balance between coverage and accuracy. Achieving both high coverage and accuracy is particularly challenging in workloads with complex address patterns, which may require large amounts of history to accurately predict future addresses. This paper describes the Signature Path Prefetcher (SPP), which offers effective solutions for three classic challenges in prefetcher design. First, SPP uses a compressed history based scheme that accurately predicts complex address patterns. Second, unlike other history based algorithms, which miss out on many prefetching opportunities when address patterns make a transition between physical pages, SPP tracks complex patterns across physical page boundaries and continues prefetching as soon as they move to new pages. Finally, SPP uses the confidence it has in its predictions to adaptively throttle itself on a perprefetch stream basis. In our analysis, we find that SPP improves performance by 27.2% over a no-prefetching baseline, and outperforms the state-of-the-art Best Offset prefetcher by 6.4%. SPP does this with minimal overhead, operating strictly in the physical address space, and without requiring any additional processor core state, such as the PC.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The "Memory Wall" <ref type="bibr" target="#b0">[1]</ref>, the vast gulf between processor execution speed and memory latency, has led to the development of large and deep cache hierarchies over the last twenty years. Although processor frequency is no-longer on the exponential growth curve, the drive towards ever greater DRAM capacities and off-chip bandwidth constraints have kept this gap from closing significantly. Cache hierarchies can improve average access times for data references with good temporal locality, however, they do not help to decrease the latency of the first reference to a particular memory address.</p><p>Prefetching is a well-studied technique which can provide an efficient means to improve the performance of modern microprocessors. The aim of prefetching is to proactively fetch useful cache lines from further down in the memory hierarchy, ahead of their first demand reference. Typically, prefetching techniques predict future access patterns based on past memory accesses. In essence, prefetching hardware speculates on the spatial and temporal locality of memory references, based on past program behavior.</p><p>In some earlier proposed prefetching techniques, the prefetching opportunity is limited to waiting until a cache miss occurs, and then prefetching either a set of lines sequentially following the current miss <ref type="bibr" target="#b1">[2]</ref>, a set of lines following a strided pattern with respect to the current miss <ref type="bibr" target="#b2">[3]</ref>, or a set of blocks spatially around the miss <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>. More recent prefetchers d bwaves lbm Fig. <ref type="figure">1</ref>: Impact of prefetching depth on a simple PC-based delta prefetcher.</p><p>attempt to predict complex, irregular access patterns <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. While these methods show significant benefit, because they are inherently reactive, the depth of their speculation is limited, which can lead to untimely prefetches.</p><p>Increasing prefetch depth is one way to speculate deeper. For example, if a processor has a simple next line prefetcher that prefetches a (+1) delta ahead of the current demand cache line, we can build a more aggressive next d line prefetcher that prefetches d * (+1) deltas ahead of the current miss (e.g., +1 1 , +1 2 , ..., +1 d ). To generalize, a stride prefetcher whose delta distance (stride) is N and whose prefetching depth is d can be represented by (N 1 , N 2 , ..., N d ).</p><p>Naively increasing the prefetching depth, however, does not always improve overall system performance, because often the predicted reference stream and the actual reference stream will eventually diverge. Figure <ref type="figure">1</ref> shows the effect of prefetching depth on two different SPEC CPU 2006 benchmarks using a simple Program Counter (PC) based delta prefetcher <ref type="bibr" target="#b10">[11]</ref>. As the prefetching depth d grows, the PC delta prefetcher brings more cache lines that are dN distance away from base address. In this figure, bwaves benefits from deeper prefetching until d = 7, because its memory access pattern is a predictable series of (+1) or (-1) deltas, up to seven steps ahead of the current demand access. After that point, the performance benefit drops as further speculative requests serve only to consume memory bandwidth, without contributing additional cache hits. On the other hand, lbm suffers from performance degradation as the prefetching depth grows. Since lbm has a variety of memory access patterns <ref type="bibr" target="#b8">[9]</ref>, deeper prefetching with a simple delta predictor wastes bandwidth, and pollutes the cache. While deeper speculation is useful for bwaves, lbm shows the greatest benefit from a speculative depth of only d = 1. Thus, as Figure <ref type="figure">1</ref> illustrates, achieving performance across many workloads requires adapting speculation relative to its accuracy.</p><p>To address both prefetching coverage and accuracy, prior work has adopted lookahead mechanisms <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. These studies, however, suffer from high hardware complexity <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or do not implement adaptive throttling <ref type="bibr" target="#b8">[9]</ref>. Moreover, most hardware prefetchers work in the physical address space <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, where the mapping between virtual and physical memory is not known. As a result, it is often difficult to predict patterns across 4KB physical page boundaries.</p><p>In this work, we propose a simple but powerful path confidence-based lookahead prefetcher, the Signature Path Prefetcher (SPP). The contributions of this work are:</p><p>? We introduce a signature mechanism that stores memory access patterns in a compressed form and initiates the lookahead prefetching process. Up to four small deltas can be compressed in this 12-bit signature without aliasing. By correlating the signature with future likely delta patterns, SPP learns both simple and complicated memory access patterns quickly and accurately. The signature can be also used to detect the locality between two physical pages, and continue the same prefetching pattern off the end of one physical page and onto the next. ? We develop a path confidence-based prefetch throttling mechanism. As lookahead prefetching goes deeper, a series of signatures builds a signature path. Each signature path has a different confidence value based on its previous delta history, prefetching accuracy, and the depth of prefetching. The path confidence value is used to throttle prefetching depth dynamically in order to balance prefetch coverage with accuracy. ? Unlike prior lookahead based prefetchers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, SPP does not require deep hooks into the core microarchitecture and is purely based on the physical memory access stream. We evaluate SPP with a combination of SPEC CPU 2006 and commercial workloads and find it achieves an average 27.2% performance improvement compared to a baseline without prefetching. Moreover, SPP outperforms recent, best of class, lookahead and non-lookahead prefetchers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, including the winner of the most recent data prefetching competition, by 6.4% on average. The remaining sections are organized as follows. Section II discusses the motivation for path confidence based prefetching. Section III describes the detailed hardware implementation of SPP. A detailed performance evaluation is presented in Section IV. Finally, we conclude the paper in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MOTIVATION AND PRIOR WORK</head><p>In this section, we examine previously proposed prefetchers with an eye toward improvement. In particular, we note that complex data access patterns are difficult to predict without the use of recursive lookahead mechanisms, while existing lookahead-based prefetchers do not consider path confidence, instead prefetching to an arbitrary degree. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Prefetching complex access patterns</head><p>An optimal prefetching algorithm should cover a wide range of memory access patterns. Simple stride prefetching techniques only detect sequences of addresses that differ by a constant value and fail to capture diverse delta patterns <ref type="bibr" target="#b13">[14]</ref>. For example, Figure <ref type="figure" target="#fig_0">2</ref> shows two examples of complex memory access patterns, taken from GemsFDTD and mcf, which cannot be captured by a simple prefetcher. Though both show complicated patterns, GemsFDTD (Figure <ref type="figure" target="#fig_0">2a</ref>) has a repeating sequence of strides (+7, -6, +12, +6, -5, -6, -6), that should be predictable assuming the prefetcher can store a long delta history. Concatenating such a long sequence in a simple pattern table, however, could result in huge storage overhead.</p><p>On the other hand, Figure <ref type="figure" target="#fig_0">2b</ref> shows that mcf has a random (though biased) access pattern that is difficult to predict. In this particular case, it is better to use a simple next-line prefetcher, because (+1) is the most commonly seen delta pattern. Offset based prefetchers such as the Best Offset prefetcher <ref type="bibr" target="#b9">[10]</ref> and the Sandbox prefetcher <ref type="bibr" target="#b14">[15]</ref> evaluate multiple offsets at runtime and issue prefetches with an offset that maximizes likelihood of use. These offset prefetchers do not, however, account for temporal ordering between delta patterns, and suffer from low accuracy on complex, yet predictable address patterns. In addition, if there are multiple offsets that are commonly observed during program execution, offset prefetchers take longer to train or fail to select the optimal offset. Lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> efficiently encode the relationship between accesses to yield future predictions, enabling further speculative lookahead accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Adapting Aggressiveness</head><p>Lookahead prefetchers learn patterns by collecting histories of observed data access patterns, and correlating these with the P 0 % P 1 % Fig. <ref type="figure">3</ref>: A case of runaway lookahead prefetching caused by an infinite loop in the prediction pattern table . 

next expected delta in the pattern. Figure <ref type="figure">3</ref> shows an example lookahead prefetcher that recursively refers to a pattern table to generate future prefetches. In this example, the prefetcher indexes into the pattern table to find the next predicted delta for prefetching. Once this prefetch is issued, the prefetcher recursively uses that prediction to again index into the pattern table and generate further predictions. This recursion allows lookahead prefetchers <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> to prefetch far ahead of the current program execution, and generate timely prefetches for as long as their predictions remain accurate.</p><p>In principle, the lookahead process can be repeated as long as the predicted pattern is found in the pattern table. As shown in Figure <ref type="figure">3</ref>, delta (+1) predicts the 3rd index (+3), and delta (+3) predicts the 1st index (+1), forming a loop. While the loop here may persist for many iterations, it is unlikely to persist forever, thus the true desired prefetching depth must be limited. To avoid over-prefetching, existing lookahead prefetchers <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b8">[9]</ref> globally and statically limit the depth to which lookahead is pursued ahead of the current demand access stream. Unfortunately, it is often the case that the ideal prefetch depth varies from application to application, as shown in Figure <ref type="figure">1</ref>, and even varies between prefetch streams within the same application. Thus a per-prefetch stream throttling mechanism is critical to adapt prefetch aggressiveness to the stream's prediction confidence <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Prefetching and Page Boundaries</head><p>Virtual memory is a vital tool in the operation of modern computers, but it creates unique challenges when designing data prefetchers which work in the lower-levels of the cache. Two addresses which are contiguous in the virtual address space might be separated by great distances in the physical address space. By extension, patterns which are trivially easy to detect in the virtual address space might be nearly impossible to fully detect in the physical address space.</p><p>Prefetchers are often located alongside caches where they have no access to address translation hardware, and hence they do not have knowledge of the relationship between physical pages in the virtual address space. Thus, data access patterns which cross physical page boundaries are difficult to exploit. Physical address prefetchers often try to learn intra-page or global patterns, applying these patterns to new pages that are encountered, or they discover simple patterns, like streams or strides, by considering each new page in a vacuum. This has the effect of either ignoring complex patterns altogether, as in the case of the Best Offset prefetcher <ref type="bibr" target="#b9">[10]</ref>, or requiring long, per-page, warmup times, as in Access Map Pattern Matching <ref type="bibr" target="#b7">[8]</ref> and conventional stream prefetchers. All of these prefetchers must stop prefetching once a page boundary is reached, because it is impossible to know the physical mapping of the next page in the virtual address space. Although challenging to implement, an effective prefetcher should be able to seamlessly continue complex patterns detected in one page as they cross page boundaries, without having re-detect the pattern from scratch in the new page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Other Prior Prefetchers</head><p>Somogyi et al. proposed Spatial Memory Streaming (SMS) <ref type="bibr" target="#b3">[4]</ref> which leverages the correlation between memory request instruction addresses (PC) and access patterns spatial near the current memory request. This purely spatial pattern ignores the temporal ordering between future demand accesses. Later efforts extended this approach to detect the temporal order between delta patterns <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. While these prefetchers achieve good performance, they require megabytes of hardware state storage, which is orders of magnitude more than the other prefetchers considered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DESIGN</head><p>To address the deficiencies outlined in Section II, we propose the Signature Path Prefetcher (SPP), a novel, lowoverhead and accurate lookahead prefetcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Design Overview</head><p>SPP uses a speculative mechanism we refer to as "lookahead" to increases its prefetching degree and improve timeliness. Once a delta prediction has been made by SPP, and a prefetch request has been issued, the accumulated history used to make the prediction(i.e., the signature) is speculatively extended to include the predicted delta, thereby generating a new lookahead signature (the mechanism is discussed in detail in Section III-B). This signature can then be used to make a new delta prediction, which leads to producing yet another signature. Thus, we use predicted deltas, with no confirmation of their accuracy, to recursively speculate down a "signature path." These speculative signatures resemble techniques used in branch prediction to deeply speculate beyond unresolved branches. Relying on this mechanism, SPP can continue to speculate much further than the most recently confirmed delta. In fact, with this mechanism in place, the challenge shifts from generating new delta predictions to deciding when to stop.</p><p>From the discussion in Section II-B of Figure <ref type="figure">3</ref>, we can conclude that lookahead prefetches will tend to be less accurate than initial prefetches. In general, as the number of speculative deltas increases, our confidence in the prefetch requests should decrease as the product of the confidence of each speculative prefetch along the path. To illustrate this, assume the probability of the first delta 0 prediction being accurate is p 0 and the probability of the second delta 1 is p 1 . During speculative lookahead, the prefetch resulting from delta 1 has a probability of being accurate of p 0 * p 1 . This is because the path used to select delta 1 , contains delta 0 which itself is only accurate with probability p 0 . In fact, the probability that the delta predicted at any lookahead depth of d will produce a useful prefetch is a product of the probabilities of all prior speculative deltas ( p = p 0 * p 1 * p 2 * ... * p d ). We label this path probability P d as the product of the probability of all constituent deltas to depth d such that P d = p. Further, the probability of any path of depth d can be expressed as a function of the probability of the most recent delta and the probability of the previous path, as shown in Equation <ref type="formula">1</ref>:</p><formula xml:id="formula_0">P d = p d ? P d-1</formula><p>(1) P d can be compared against a given confidence threshold to adaptively determine when prefetching for this particular lookahead stream should be stopped. Further, this confidence, P d , can also be used to determine which cache level to insert prefetched lines, with more accurate/confident prefetches sent to a higher cache level and less accurate/confident prefetches sent to a lower cache level.</p><p>To gracefully handle the transition between physical pages, SPP shares learning across page boundaries. As discussed in Section II-C, SPP operates in the physical address space, without any access to address translation. Thus, the spatial relationships between physical pages are largely unknown and assumed to be random. Despite this, SPP leverages learning from one physical page to make timely predictions in other pages. SPP does this in two ways, first, while delta history signatures are maintained on a per-physical page basis, as will be described in Section III-B, those signatures index into a global table for predicting deltas, which is shared by all pages. Second, as described in Section III-D, when the first demand access is made to a new physical page, the delta signature patterns from a previous physical page whose delta predictions crossed a page boundary can be inherited and used to bootstrap predictions in this new page. This gives SPP the advantage of not requiring long, per-page warmup periods to start prefetching complex patterns, which leads to higher prefetch coverage.</p><p>The high level design of the SPP engine is illustrated in Figure <ref type="figure" target="#fig_1">4</ref>. The SPP module consists of three main structures  The PT also estimates the path confidence that a given delta pattern will yield a useful prefetch. If the delta generated by the PT is found to have sufficient confidence (above a configured threshold), then it is passed as a prefetch candidate to the Prefetch Filter (PF), which checks for redundant prefetches. If the predicted delta crosses a 4KB physical page boundary, SPP does not issue the prefetch but instead redirects the request to the GHR for page boundary learning. In the remainder of this section we describe each stage of the SPP in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Memory Access Patterns</head><p>The ST, shown in Figure <ref type="figure">5a</ref>, is designed to capture memory access patterns within 4KB physical pages, and to compress the previous deltas in that page into a 12-bit history signature. The ST tracks the 256 most recently accessed pages, and stores the last block accessed in each of those pages in order to calculate the delta signature of the current memory access, which is then used to access and update the PT. Whenever there is an L2 cache access, its physical address is passed to the ST to find a matching entry for the corresponding physical page. Figure <ref type="figure">5a</ref> shows an example of accessing the ST with a physical page number of 3 and block offset 3 from the beginning of the page. In this case, the ST finds a matching entry for this page and is able to read a stored signature 0x1. This signature is a compressed representation of a previous access pattern to that page, which was generated via a series of XORs and shifts as shown in Equation <ref type="formula" target="#formula_1">2</ref>. In this case, the signature 0x1 represents a single previous delta access to Page 3, which was (+1).</p><formula xml:id="formula_1">New Signature =(Old Signature &lt;&lt; 3-Bit) XOR (Delta)<label>(2)</label></formula><p>Since the ST stores the last block offset 1 accessed in Page 3, we know that the current delta in Page 3 is (3 -1) = (+2). This delta is non-speculative, because it is based on a demand request to the L2. Therefore, we can infer that a given set of accesses (signature 0x1 in this instance) will lead to a delta of (+2). Figure <ref type="figure">5b</ref> shows how this correlation is used to update the delta pattern in the PT.</p><p>The probability of each delta occurring is approximated using a per-delta confidence value C d , which is derived from counters stored in the PT. Since a matching delta (+2) that corresponds to 0x1 is found in the PT, the corresponding C delta counter increases by one. In order to estimate the prefetch accuracy probability for each delta, we also maintain an separate counter C sig , which tracks the total occurrences of the signature itself. If either C delta or C sig saturates, all counters associated with that signature are right shifted by 1. In doing so, SPP is able to continue updating its counters with the most recent information without ever completely losing all previously collected history. If the PT contains no matching delta, we simply replace the existing delta with the lowest C delta value.</p><p>Unlike the ST, whose entries correspond to individual physical pages, each PT entry is shared globally by all pages. If Page A and Page B, for example, share the same access pattern, they will generate the same signature, which indexes to the same entry in the PT, and updates the same delta pattern in the PT. Sharing patterns between pages in the PT minimizes SPP training time as well as the number of entries needed to store predictions. Each entry in the PT can hold up to four different deltas so that multiple different deltas can be prefetched by a single signature. Each of the deltas in a PT entry can be prefetched if its corresponding probability (C d = C delta /C sig ) is above the given prefetching threshold T P .</p><p>After updating the PT, the ST is also updated with a new signature based on the current delta (+2). Equation <ref type="formula" target="#formula_1">2</ref>shows how the SPP generates a new history signature. The old signature 0x1 is left-shifted 3-bits and XORed with the current delta (+2). In this way, a 12-bit signature can represent the last four memory accesses in Page 3. Note that we refer to this signature as being "compressed" because deltas greater than 7 have the potential to overlap and cause aliasing with existing history. At this point, the new signature (0x1&lt;&lt;3) XOR (+2) = 0xA represents the current access pattern, (+1,+2), in Page 3. Assuming 4KB pages with 64B cache lines, all possible deltas fall in the range of (-63) to (+63). We use a 7-bit sign+magnitude representation for both positive and negative deltas. Thus, negative and positive deltas produce different signatures, pointing to different entries in the pattern table, and ultimately different prefetch targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Path Confidence-based Prefetching</head><p>After updating the ST, as in Figure <ref type="figure">5a</ref>, SPP accesses the PT so that it can predict the next delta following signature 0xA. As shown in Figure <ref type="figure" target="#fig_3">6a</ref>, the path confidence P d is calculated for deltas associated with signature 0xA. The initial path confidence P 0 is simply set by (C delta / C sig ) since there is no prior path confidence value. In this example, (+2) delta has P 0 = 0.8 which is greater than the prefetching threshold T P . Therefore, the PT adds the delta (+2) to the current cache line's base address and issues a prefetch request.</p><p>In addition, the PT initiates the lookahead process by building a speculative lookahead signature. As shown in Figure <ref type="figure" target="#fig_3">6a</ref>, the lookahead signature 0x52 is generated from 0xA and the predicted delta (+2) using Equation <ref type="formula" target="#formula_1">2</ref>. While there could be multiple candidates for prefetching, SPP only generates a single lookahead signature, choosing the candidate with the highest confidence. The lookahead signature is used to index the PT again so that SPP can search for further prefetch and lookahead candidates down the signature path. If the lookahead signature 0x52 finds more prefetch candidates in the PT (Figure <ref type="figure" target="#fig_3">6b</ref>), the process will be repeated and prefetch requests will be issued. The lookahead mechanism is used recursively until the signature path confidence P d falls below the prefetching threshold T P . Note, a separate, greater confidence threshold T f determines which level of the cache a given prefetch will be fill into, either the L2 if P d is greater than T f or the LLC if P d is less than T f . SPP also uses a global accuracy scaling factor ? based on the observed global prefetching accuracy to further throttle down or increase the aggressiveness of the lookahead process. Thus, Equation 1 is modified to include ? according to Equation <ref type="formula" target="#formula_2">3</ref>:</p><formula xml:id="formula_2">P d = ? ? C d ? P d-1 (? &lt; 1)<label>(3)</label></formula><p>If the prefetching accuracy is generally high across all prefetches, ? will decrease the path confidence P d very slowly, allowing deeper lookahead prefetching. On the other hand, if global prefetching accuracy is low, ? will quickly throttle down lookahead prefetching. The prefetching accuracy is tracked by the PF described in section III-E. The global ? and the local, per-delta C d work together to provide a throttling mechanism which globally adapts to the general prefetching confidence of a given program phase, while simultaneously favoring some signature paths over others based on their relative confidence. Note that for simplicity, in the discussion above we describe path confidence calculations in the context of floating point numbers. However, in a real implementation (and in our simulator), we use 7-bit fixed point numbers to represent path confidence values between 0?100, and perform multiplication and division on those fixed point numbers. In addition, since C delta and C sig are 4-bit saturating counters, we can use a simple 16x16=256 entry lookup table that stores all possible division results, which allows us to completely remove the expensive divider modules. Moreover, the extra computational latency can be hidden, because SPP can calculate the path confidence in the background while the L2 cache is waiting for the DRAM to service demand misses.</p><p>In addition to throttling when P d falls below the prefetch threshold T p , SPP also stops prefetching if there are not enough L2 read queue resources. Reserving L2 read queue entries is desirable because even accurate, useful prefetches can take resources away from even more performance-critical demand misses from the L1 cache. Therefore, SPP does not issue prefetches when the number of empty L2 read queue entry becomes less than the number of L1 MSHRs. To summarize, SPP stops prefetching if the prefetcher observes one of following conditions:</p><p>1) Low path confidence P d .</p><p>2) Too few L2 read queue resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Page Boundary Learning</head><p>One of the challenges for history-based prefetchers like SPP is the loss of history information that occurs during transitions to new physical pages. Pages that are adjacent in the virtual address space may not be adjacent in the physical address space. As a result, patterns that are very easy to learn and follow in the virtual address space may be very hard to detect and predict when they are broken up by a physical page transition. Previously proposed history-based techniques <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref> address this by making predictions using the first offset in a page, or with very short delta histories. Although these predictions may increase coverage, they do so at the expense of accuracy because they are made with little or no information about what happened in the previously accessed page.</p><p>SPP addresses this problem with a novel mechanism that allows histories to be maintained and tracked across physical page transitions. It does this by augmenting the per-page ST with a global history that is updated when the prefetcher makes a prediction off the end of a page, and is checked against when accessing new pages for the first time.</p><p>Figure <ref type="figure" target="#fig_4">7</ref> shows how SPP connects a signature path across physical page boundaries without the need to relearn any delta history patterns, or do any other warmup in the new page. As shown in Figure <ref type="figure" target="#fig_4">7a</ref>, when there is a prefetch request that goes beyond the current Page A, a conventional streaming prefetcher must stop prefetching, because it is impossible for the prefetcher to predict the next physical page number. However, this boundary-crossing prediction can still be useful when the next page is accessed for the first time, and we find that the page offset of that first access matches the out-ofbounds offset previously predicted by SPP.</p><p>In order to track this behavior, the boundary crossing prediction is stored in a small 8-entry GHR. The GHR stores the current signature, path confidence, last offset, and delta used for the out-of-page prefetch request, which is everything necessary to bootstrap SPP prefetching in a new page. If we access a new page that is not currently tracked (i.e., a miss in the ST), SPP searches for a GHR entry whose last offset and delta match the current offset value in the new page. Figure <ref type="figure" target="#fig_4">7b</ref> shows that Page B is accessed with an initial offset of 1. SPP checks if any GHR entry's last offset and delta value match the current offset of 1. In this case, the signature 0x52 has the last offset and delta whose sum (62 + 3 = 65) matches the offset of 1 in Page B, since there are only 64 64B blocks in 4KB physical page. We can now predict that Page B will produce a delta pattern that is a continuation of the signature 0x52.</p><p>Since the pattern predicted by signature 0x52 is now continuing in a new page, we need to connect the signature 0x52 with delta (+3) by generating a new signature. Using the same Equation <ref type="formula" target="#formula_1">2</ref>, we generate a new signature 0x293 for Page B that can begin prefetching immediately, without needing to learn any additional delta history. The new signature 0x293 is entered into the ST for future use by Page B. Thus, SPP does not suffer from long, per-page warmup periods, and it can prefetch complex patterns in new physical pages sooner, resulting in higher prefetch coverage. Unlike the Global History Buffer <ref type="bibr" target="#b16">[17]</ref>, which records all observed delta patterns, the GHR only stores delta patterns that cross page boundaries. Note that SPP does not stop looking even further ahead for more prefetch candidates after coming to the end of a page and updating the GHR. As shown in Figure <ref type="figure" target="#fig_4">7a</ref>, if the (+3) delta, which lands in Page B, is predicted to be followed by a (-5) delta, which comes back to Page A, then SPP can still exploit this behavior and prefetch an offset of 60 in Page A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Prefetch Filter</head><p>The main objectives of the PF, shown in Figure <ref type="figure" target="#fig_5">8</ref>, are to decrease redundant prefetch requests, and to track prefetching accuracy. The PF is a direct-mapped filter that records prefetched cache lines. SPP always checks the PF first, before it issues prefetches. If the PF already contains a cache line, this means that line has already been prefetched, and SPP drops the redundant prefetch request. Entries in the filter get cleared by resetting a Valid bit when the corresponding cache line is evicted from the L2 cache.</p><p>Due to collisions, a filter entry may already be occupied by another prefetched cache line. In this case, SPP simply replaces the old cache line, stores the current prefetch request in the filter, and issues the current prefetch. Note that this simple replacement policy might erase cache lines from the filter before they get evicted from the L2 cache, which could lead to re-prefetching, but we find in practice that this happens very infrequently.</p><p>By adding a Useful bit to each filter entry, the PF can also approximate prefetching accuracy. SPP has two global counters, one which tracks the total number of prefetch requests (C total ), and the other which tracks the number of useful prefetches (C usef ul ). The C total counter increases whenever SPP issues a prefetch that is not dropped by the filter. Useful prefetches are detected by actual L2 cache demand requests hitting in the PF, which increments the C usef ul counter. To avoid increasing C usef ul more than once per useful prefetched line, we set a used bit in the PF entry which keeps it from being double counted. The global prefetching accuracy tracked by this filter is used for ? in Equation 3 to throttle the path confidence value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>In this section, we evaluate the SPP prefetch engine. We first present the evaluation methodology, followed by single core and multi-core performance. Finally, we present a sensitivity study of SPP's design parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methodology</head><p>We evaluate SPP using the ChampSim simulator, which is an updated version of the simulation infrastructure used in the 2nd Data Prefetching Championship (DPC-2) <ref type="bibr" target="#b17">[18]</ref>. We model 1-4 out-of-order cores, whose parameters can be found in Table <ref type="table" target="#tab_2">I</ref>. ChampSim is a trace-based simulator, and we collect SimPoint <ref type="bibr" target="#b18">[19]</ref> traces from 18 memory intensive SPEC CPU2006 <ref type="bibr" target="#b19">[20]</ref> applications using PinPlay <ref type="bibr" target="#b20">[21]</ref>. We also collect single thread traces from 3 server workloads (Data Caching, Graph Analytics, SAT Solver) from CloudSuite <ref type="bibr" target="#b21">[22]</ref>. Since our SimPoint methodology does not work with the server workloads, we instead collect the server workload traces after fast-forwarding at least 30B instructions to pass through the benchmark's initialization phase. For performance evaluation, we warm up each core for 200M instructions and collect results over an additional 1B instructions.</p><p>Single core simulations use a single DRAM channel. In multi-core simulations, all cores share a single L3 cache and main memory system, with two DRAM channels. Instruction caching effects are not modeled in this simulation infrastructure. In ChampSim, all prefetching actions are initiated by an L2 access, but prefetches can be directed to fill in either the L2 or the L3 cache. Our simulation infrastructure uses a 4KB page size when mapping virtual to physical addresses. In ChampSim, virtual to physical page mappings are arbitrarily randomized. All of the prefetchers we evaluate in this work were designed to operate strictly in the physical address space with no knowledge of the relationship between physical and virtual address spaces.</p><p>We compare SPP against three top performing recently proposed prefetching algorithms: the Variable Length Delta Prefetcher (VLDP) <ref type="bibr" target="#b8">[9]</ref>, the Best Offset Prefetcher (BOP) <ref type="bibr" target="#b9">[10]</ref>, and the DRAM-Aware Access Map Pattern Matching (DA-AMPM) <ref type="bibr" target="#b22">[23]</ref> prefetcher <ref type="foot" target="#foot_1">1</ref> . We use the original code for    each of these prefetchers submitted to DPC-2. In each case, their design parameters have been re-tuned to attain their highest performance in ChampSim running these traces. SPP does not use any feature of ChampSim that was not also available to the designers of the other evaluated prefetchers. SPP's threshold configurations were empirically derived. The prefetching threshold T P = 0.25 and fill level threshold T F = 0.9 were found to provide good performance improvement and accuracy, and they are used throughout these results except where otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Single Core Performance</head><p>Figure <ref type="figure" target="#fig_7">9</ref> shows the IPC speedup of all four evaluated prefetchers over a no-prefetching baseline. Overall, SPP out-performs or matches all other prefetchers on nearly every benchmark. On average, SPP achieves a 27.2% geometric mean speedup, which is 6.4% and 5.6% more than BOP and DA-AMPM respectively. Also, SPP outperforms VLDP (a recent lookahead prefetcher) by 13.2%. SPP shows particularly significant improvement in benchmarks that have complex access patterns. For example, both GemsFDTD and lbm show substantial performance improvement with SPP. Further, SPP shows remarkable performance gains in bwaves, cactusADM, leslie3d, and libquantum. These applications are highly predictable by SPP, and gain significantly from SPP's ability to adaptively look ahead very deeply.</p><p>As Ferdman et al. <ref type="bibr" target="#b21">[22]</ref> noted in a prior study, data prefetching for large scale workloads is not as effective as prefetching for general purpose workloads. Performance with SPP only improves by 2?4% for this class of benchmark. However, SPP and VLDP show the best performance for Data Caching and Graph Analytics, because both prefetchers are able to capture the complex memory access patterns found in those workloads. BOP achieves a 13.8% performance gain in SAT solver by aggressively prefetching on every L2 cache access. The impact of aggressive prefetching by BOP will be discussed further in the next section. We also modeled and compared the performance of the SMS <ref type="bibr" target="#b3">[4]</ref> prefetcher originally designed for server workloads, however, the overall performance of SMS on both SPEC and CloudSuite was less than that of DA-AMPM. To simplify the performance analysis, we only include the results from DA-AMPM.</p><p>Figure <ref type="figure" target="#fig_7">9b</ref> shows the performance improvement under different memory resource constraints. The baseline configuration used for Figure <ref type="figure" target="#fig_7">9a</ref> has 12.8GB/s of DRAM bandwidth and 2MB LLC. The low bandwidth test is configured with only 3.2GB/s memory bandwidth, and the small LLC configuration has only 512KB of last level cache. Along with a geomean across all benchmarks, Figure 9b also shows performance for three benchmarks where SPP has lower performance than other prefetchers. As shown in Figure <ref type="figure" target="#fig_7">9b</ref>, due to aggressive prefetching, VLDP and BOP show similar or worse performance improvement for mcf and xalandbmk under the low bandwidth configuration. Meanwhile, BOP always shows the best performance for mcf and SAT solver regardless of resource constraints. We find that these benchmarks have random access patterns (i.e., pointer chasing) that cannot be easily captured in the physical address space. Therefore, offset-based prefetchers <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b14">[15]</ref> work better than SPP for these, because they are trained by individual offset occurrence frequency, and do not rely on longer delta patterns repeating. However, the benefit of aggressive offset prefetching without pattern matching can be nullified by resource constraints when multiple workloads fight for limited shared LLC and DRAM bandwidth <ref type="bibr" target="#b23">[24]</ref>. The performance degradation of BOP with mcf and SAT solver is discussed in the multi-core analysis section.</p><p>1) Prefetching Coverage and Accuracy: The substantial performance benefit of SPP versus the other prefetchers shown in Figure <ref type="figure" target="#fig_7">9</ref> is a direct result of SPP's prefetching accuracy and coverage. Figure <ref type="figure" target="#fig_8">10</ref> shows the prefetching coverage for each benchmark. In this figure, each prefetcher is indicated  by its first letter (e.g., "V" for VLDP, "D" for DA-AMPM, etc.). Prefetching coverage is measured by the number of useful prefetches divided by the number of cache misses without prefetching. Because prefetching can be useful in different ways, we further break down the useful prefetches into four different categories. Useful represents the portion of prefetched cache lines that are filled into the cache before their first demand access, eliminating all cache miss penalty. On the other hand, Late represents the portion of prefetched cache lines that were issued to DRAM but were not filled before their first demand access. Thus, Late prefetches have the effect of accelerating the demand fetch compared to a regular cache miss. Useful and Late are measured for both the L2 cache and the LLC. We also plot the fraction of useless prefetches on the bottom of the figure, showing the effect of over-aggressive, low-confidence prefetching. On average, BOP shows the highest prefetching coverage, however, it also generates a large number of useless prefetches (mostly to the L2 cache), which consume additional DRAM bandwidth and energy. Alhough SPP has slightly lower coverage compared to BOP, it has the largest number of L2 Useful prefetches due to its highly accurate dynamic path confidence-based throttling.</p><p>Because prefetching coverage alone does not show the aggressiveness of each prefetcher, we also plot the raw number of prefetch requests in Figure <ref type="figure" target="#fig_9">11</ref>. As discussed in Section IV-B and shown in Figure <ref type="figure" target="#fig_7">9a</ref>, BOP and VLDP outperform SPP in three workloads (mcf, xalancbmk and SAT solver) in a singlecore environment. Figure <ref type="figure" target="#fig_9">11</ref> shows that for these workloads, BOP and VLDP generate a huge number of useless prefetches to achieve this performance advantage, aggressively consuming DRAM bandwidth and LLC capacity. In a single core environment, this over-prefetching does not negatively impact performance, however as we will show, performance does degrade when the LLC and DRAM bandwidth are shared by multiple cores.</p><p>2) Average Lookahead Depth: Because the length of memory access patterns is different in each application, the optimal lookahead prefetching depth also varies. Figure <ref type="figure" target="#fig_10">12</ref> shows the diversity of lookahead depths SPP uses across the 21 benchmarks we examined. For example, libquantum is well known to have very stable and long delta patterns. confirms that SPP uses a very deep lookahead depth for libquantum. Meanwhile, milc also uses a deep lookahead depth, but this case is very different from libquantum. We find that many delta patterns in milc do not fit within a single 4KB page. In these cases, SPP will predict that the access pattern will leave the current page, and then, several deltas later, return to the current page, where prefetching can continue. SPP will continue looking further ahead until its confidence falls below the threshold. On the other hand, SPP does not prefetch deeply on xalancbmk, because this benchmark is not amenable to prefetching, due to a low degree of spatial and temporal locality in each page. In this case, SPP detects that prefetching is inaccurate by using the PF, and lowers the global scaling factor ?, which serves to limit the lookahead depth. The average lookahead depth of SPP across all traces is 6.9.</p><note type="other">Figure 12</note><p>3) Contribution to Performance Improvement: Figure <ref type="figure" target="#fig_11">13</ref> shows the relative contribution of lookahead and page boundary learning (via the GHR), compared to a basic version of SPP without either of these features. To highlight the impact of each feature, we select two benchmarks (libquantum and GemsFDTD), and break down the performance benefit. A basic SPP algorithm without lookahead or the GHR shows a performance improvement of 11.8% on libquantum and 14.0% on GemsFDTD. Adding lookahead prefetching achieves a significant additional speedup of 21.4% for libquantum, because the benchmark has simple memory access patterns that can be accurately covered with very deep lookahead prefetching. On the top of that, page boundary learning provides little benefit for libquantum, because each page takes very little time to train, even withouth the GHR. For GemsFDTD, lookahead prefetching improves performance by 25.9%. However, page boundary learning provides a substantial boost of 4.5% to GemsFDTD because many delta patterns do not fit inside a single 4KB page. In this application, a common delta pattern is (+30) followed by (+1), thus many of the deltas cross page boundaries. The GHR structure captures this behavior in GemsFDTD and provides a substantial performance improvement. On average, basic SPP, without lookahead provides a 12.9% speedup; adding lookahead prefetching provides 12.3% more speedup, and finally adding the page boundary learning gains another 2.0% improvement.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multi-programmed Mix Performance</head><p>To show results for a multi-core system, we generate 20 multi-programmed mixes consisting of traces from different benchmarks, and assign each trace to a different core. The mixes are randomly generated in order to fairly represent the characteristics of multi-programmed workloads. Figure <ref type="figure" target="#fig_13">14a</ref> shows the performance improvement of four-workload mixes, measured by normalized weighted speedup. The graph is sorted by speedup order. Out of the 20 random mixes, SPP achieves the best performance on 19 mixes. For the remaining mix, DA-AMPM beats SPP by less than 0.5%. On average, SPP achieves a 21.7% speedup compared to the baseline.</p><p>Note that the BOP prefetcher, which nearly ties for second place on single core benchmarks, shows particularly poor performance in multicore systems. As shown in Figure <ref type="figure" target="#fig_8">10</ref>, although BOP has good coverage, it also fetches a high number of useless lines. In a single core system this does not hurt performance, because the LLC is not over-committed. In a multicore system, however, the LLC pressure is greater, which leads to performance loss. Figure <ref type="figure" target="#fig_13">14b</ref> shows two examples of multi-programmed mixes that suffer from BOP's aggressive prefetching. Each mix contains benchmarks in which BOP outperforms SPP in the single core environment. In MIX10, VLDP and BOP show the least performance degradation on mcf. However, this performance in mcf comes from aggressive prefetching, which prevents other workloads from getting better performance. In particular, BOP shows performance degradation on bzip2 while the other prefetchers   show performance improvement. In MIX18, all prefetchers gain performance in wrf and zeusmp at the cost of degrading SAT solver and xalancbmk. Similarly, BOP's advantage is lower than SPP's due to its aggressive prefetching on SAT solver and xalancbmk. Note that in single core experiments, BOP showed performance improvement in xalancbmk and SAT solver. However, when resources are shared among multiple cores, BOP's aggressive prefetching hurts overall system performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Storage Sensitivity</head><p>The total storage used by SPP in the preceding experiments is 5.37KB, with the storage requirement for each individual component shown in Table <ref type="table" target="#tab_4">II</ref>. As the table shows, SPP's largest component is the PT (3KB). This table has multiple delta predictions and counters for each tracked signature. Figure <ref type="figure" target="#fig_14">15</ref> shows a performance sensitivity analysis between BOP, DA-AMPM, and SPP when scaling storage size. Each point represents the storage configuration used for the performance evaluation. As expected, the overall performance of SPP and DA-AMPM does not increase with greater storage capacity. Generally, SPP always outperforms the other prefetchers at a given storage budget. Note that, counterintuitively, BOP does not benefit from greater storage, because it takes a longer time to find the best offset value when more cache lines are in its recent request table <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Signature Sensitivity</head><p>SPP uses a 12-bit history signature built by a series of 3bit shifts and XORs. This shifting and XORing represents a form of lossy information compression. The more bits that are shifted, the less compression, and vice-versa. Here we examine the performance impact of this compression. Figure <ref type="figure" target="#fig_15">16</ref> shows the performance sensitivity to the number of shifted bits. Since each delta pattern within a 4KB page can be represented with 7 bits (-63 ? 63), there is no value in shifting more than 7 bits. The figure shows that a 12-bit signature prefers 3bit shifting. The goal of the history signature is to track as many useful deltas as possible within a small 12-bit signature. This is best achieved by giving small deltas, which are the most common, all the precision they need, while still keeping some information from larger deltas. By shifting 3-bits prior to XORing, SPP compresses four deltas into 12-bits. Deltas from 0-7 are represented at full precision. Larger deltas (&gt;7) cause some aliasing, but still contribute information to the final signature.</p><p>V. CONCLUSION Signature Path Prefetching offers compelling solutions for three major prefetching challenges. First, it is able to learn and prefetch complex data access patterns by using a compressed history signature. Second, it is able to detect when a data access pattern crosses a page boundary, and quickly resume prefetching on the new page. Third, it is able to balance aggressive prefetching with accuracy by using path confidence. SPP is able to do all this without using the program counter or other core registers, and while operating strictly in the physical address space. SPP improves performance versus a no-prefetching baseline by 27.2%, and improves performance by 6.4% versus the next best competing technique.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Complex memory access pattern.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Overall SPP architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 5: SPP table update operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Path confidence-based lookahead prefetching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Learning delta patterns across page boundaries.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 :</head><label>8</label><figDesc>Fig. 8: Prefetching Filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) IPC speedup versus no prefetching. (b) IPC speedup for Low BW and small LLC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: Single-core IPC speedup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 :</head><label>10</label><figDesc>Fig. 10: Prefetching coverage and useless prefetches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 :</head><label>11</label><figDesc>Fig. 11: Raw prefetch request breakdown. VLDP and BOP both generate significant numbers of useless prefetches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 :</head><label>12</label><figDesc>Fig. 12: Average Lookahead Depth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 13 :</head><label>13</label><figDesc>Fig. 13: Contribution of SPP components. Stacked graph represents accumulated speedup from each component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(a) Workloads sorted by normalized weighted speedup (b) Performance alaysis for MIX10 and MIX18.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 :</head><label>14</label><figDesc>Fig. 14: Normalized speedup for mixes of 4 workloads.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 15 :</head><label>15</label><figDesc>Fig. 15: SPP storage sensitivity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 16 :</head><label>16</label><figDesc>Fig. 16: Performance sensitivity with respect to the number of bits to shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Simulator parameters.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>+ 8192 + 264 + 20 = 44060 bits ? 5.37 KB</figDesc><table><row><cell>Structure</cell><cell>Entry</cell><cell>Component</cell><cell>Storage</cell></row><row><cell></cell><cell></cell><cell>Valid (1 bit)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tag (16 bit)</cell><cell></cell></row><row><cell>Signature Table</cell><cell>256</cell><cell cols="2">Last offset (6 bit) 11008 bits</cell></row><row><cell></cell><cell></cell><cell>Signature (12 bit)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>LRU (6 bit)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Csig (4 bit)</cell><cell></cell></row><row><cell>Pattern Table</cell><cell>512</cell><cell>C delta (4*4 bit)</cell><cell>24576 bits</cell></row><row><cell></cell><cell></cell><cell>Delta (4*7 bit)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Valid (1 bit)</cell><cell></cell></row><row><cell>Prefetch Filter</cell><cell>1024</cell><cell>Tag (6 bit)</cell><cell>8192 bits</cell></row><row><cell></cell><cell></cell><cell>Useful (1 bit)</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Signature (12 bit)</cell><cell></cell></row><row><cell>Global History Register</cell><cell>8</cell><cell>Confidence (8 bit) Last offset (6 bit)</cell><cell>264 bits</cell></row><row><cell></cell><cell></cell><cell>Delta (7 bit)</cell><cell></cell></row><row><cell>Accuracy Counter</cell><cell>1 1</cell><cell>C total (10 bit) C usef ul (10 bit)</cell><cell>10 bits 10 bits</cell></row><row><cell>11008 + 24576</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>SPP storage overhead.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>978-1-5090-3508-3/16/$31.00 c 2016 IEEE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>Note: DA-AMPM is an extended version of AMPM<ref type="bibr" target="#b7">[8]</ref> which accounts for DRAM row buffer locality.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENT</head><p>We thank the <rs type="funder">National Science Foundation</rs>, which partially supported this work through grants <rs type="grantNumber">CCF-1320074</rs> and <rs type="grantNumber">I/UCRC-1439722</rs>, and <rs type="funder">Intel Corp</rs>. for their generous support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cF6qqgb">
					<idno type="grant-number">CCF-1320074</idno>
				</org>
				<org type="funding" xml:id="_U7CPJKJ">
					<idno type="grant-number">I/UCRC-1439722</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hitting the memory wall: implications of the obvious</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Wulf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGARCH Comp. Arch. News</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="20" to="24" />
			<date type="published" when="1995-03">March 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential program prefetching in memory hierarchies</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="7" to="21" />
			<date type="published" when="1978-12">December 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective hardware-based data prefetching for high-performance processors</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="609" to="623" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatial memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moshovos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="252" to="263" />
			<date type="published" when="2006">2006</date>
			<publisher>IEEE Computer Society</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Memory prefetching using adaptive stream detection</title>
		<author>
			<persName><forename type="first">I</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 39th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="397" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatiotemporal memory streaming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Somogyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Linearizing irregular memory accesses for improved correlated prefetching</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="247" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Access map pattern matching for high performance data cache prefetch</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Instruction-Level Parallelism</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficiently prefetching complex address patterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shevgoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koladiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 48th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A best-offset prefetcher</title>
		<author>
			<persName><forename type="first">P</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA), 2016 IEEE 20th International Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stride directed prefetching in scalar processors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Janssens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMICRO Newsletter</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="102" to="110" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bfetch: Branch prediction directed prefetching for chip-multiprocessors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kadjo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gratz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="623" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Runahead execution: An alternative to very large instruction windows for out-of-order processors</title>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Symposium on High Performance Computer Architecture (HPCA)</title>
		<meeting>the 9th International Symposium on High Performance Computer Architecture (HPCA)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="129" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ac/dc: An adaptive data cache prefetcher</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques</title>
		<meeting>the 13th International Conference on Parallel Architectures and Compilation Techniques</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="135" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sandbox prefetching: Safe run-time evaluation of aggressive prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>-F. Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computer Architecture (HPCA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="626" to="637" />
		</imprint>
	</monogr>
	<note>IEEE 20th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feedback directed prefetching: Improving the performance and bandwidth-efficiency of hardware prefetchers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Srinath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPCA 2007. IEEE 13th International Symposium on</title>
		<title level="s">High Performance Computer Architecture</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="63" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data cache prefetching using a global history buffer</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Nesbit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software, IEE Proceedings</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="96" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The 2nd Data Prefetching Championship (DPC-2)</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Pugsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using simpoint for accurate and efficient simulation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Biesbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMETRICS Performance Evaluation Review</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="318" to="319" />
			<date type="published" when="2003">2003</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Standard Performance Evaluation Corporation CPU2006 Benchmark Suite</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pinplay: a framework for deterministic replay and reproducible analysis of parallel programs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stallcup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lueck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cownie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization</title>
		<meeting>the 8th annual IEEE/ACM international symposium on Code generation and optimization</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clearing the clouds: a study of emerging scale-out workloads on modern hardware</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ferdman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adileh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kocberber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Volos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alisafaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jevdjic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kaynak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ailamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Falsafi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unified memory optimizing architecture: memory subsystem control with a unified predictor</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Inaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hiraki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM International Conference on Supercomputing</title>
		<meeting>the 26th ACM International Conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="267" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Friendly fire: understanding the effects of multiprocessor prefetches</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Enright Jerger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Lipasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Performance Analysis of Systems and Software (ISPASS)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="177" to="188" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
