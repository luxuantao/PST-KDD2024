<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-03-22">22 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jing</forename><surname>Gu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eliana</forename><surname>Stefani</surname></persName>
							<email>estefani@ucsc.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
							<email>jessetho@usc.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Southern California</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><forename type="middle">Eric</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Cruz</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-22">22 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2203.12667v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Comm Complexity Task Objective Fine-grained Navigation Coarse-grained Navigation Nav + Object Interaction Initial Instruction(s)</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks. Visionand-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer vision, robotics, and machine learning communities. In this paper, we review contemporary studies in the emerging field of VLN, covering tasks, evaluation metrics, methods, etc. Through structured analysis of current progress and challenges, we highlight the limitations of current VLN and opportunities for future work. This paper serves as a thorough reference for the VLN research community. 1  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans communicate with each other using natural language to issue tasks and request help. An agent that can understand human language and navigate intelligently would significantly benefit human society, both personally and professionally. Such an agent can be spoken to in natural language, and would autonomously execute tasks such as household chores indoors, repetitive delivery work outdoors, or work in hazardous conditions following human commands (bridge inspection; fire-fighting). Scientifically, developing such an agent explores how an artificial agent interprets natural language from humans, perceives its visual environment, and utilizes that information to navigate to complete a task successfully.</p><p>Vision-and-Language Navigation (VLN) <ref type="bibr">(Anderson et al., 2018b;</ref><ref type="bibr" target="#b21">Chen et al., 2019;</ref><ref type="bibr" target="#b4">Thomason et al., 2019b)</ref> is an emerging research field that aims to build such an embodied agent that can communicate with humans in natural language and navigate in real 3D environments. VLN extends visual navigation in both simulated <ref type="bibr" target="#b124">(Zhu et al., 2017;</ref><ref type="bibr">Mirowski, 2019)</ref> and real environments <ref type="bibr" target="#b72">(Mirowski et al., 2018)</ref> with natural language communication. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, VLN is a task that involves the oracle (frequently a human), the agent, and the environment. The agent and the oracle communicate in natural language. The agent may ask for guidance and the oracle could respond. The agent navigates and interacts with the environment to complete the task according to the instructions received and the environment observed. Meanwhile, the oracle observes the environment and agent status, and may interact with the environment to help the agent.</p><p>Since the development and release of works such as Room-to-Room (R2R) <ref type="bibr">(Anderson et al., 2018b)</ref>, many VLN datasets have been introduced. Regarding the degree of communication, researchers create benchmarks where the agent is required to passively understand one instruction before navigation, to benchmarks where agents converse with the oracle in free-form dialog. Regarding the task objective, the requirements for the agent range from strictly following the route described in the ini-tial instruction to actively exploring the environment and interacting with objects. In a slight abuse of terminology, we refer to benchmarks that involve object interaction together with substantial sub-problems of navigation and localization, such as ALFRED <ref type="bibr" target="#b88">(Shridhar et al., 2020)</ref>, as VLN benchmarks.</p><p>Many challenges exist in VLN tasks. First, VLN faces a complex environment and requires effective understanding and alignment of information from different modalities. Second, VLN agents require a reasoning strategy for the navigation process. Data scarcity is also an obstacle. Lastly, the generalization of a model trained in seen environments to unseen environments is also essential. We categorize the solutions according to the respective challenges. (1) Representation learning methods help understand information from different modalities. (2) Action strategy learning aims to make reasonable decisions based on gathered information. (3) Data-centric learning methods effectively utilize the data and address data challenges such as data scarcity. (4) Prior exploration helps the model familiarize itself with the test environment, improving its ability to generalize.</p><p>We make three primary contributions. (1) We systematically categorize current VLN benchmarks from communication complexity and task objective perspectives, with each category focusing on a different type of VLN task. (2) We hierarchically classify current solutions and the papers within the scope.</p><p>(3) We discuss potential opportunities and identify future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks and Datasets</head><p>The ability for an agent to interpret natural language instructions (and in some instances, request feedback during navigation) is what makes VLN unique from visual navigation <ref type="bibr" target="#b17">(Bonin-Font et al., 2008)</ref>. In Table <ref type="table">2</ref>, we mainly categorize current datasets on two axes, Communication Complexity and Task Objective.</p><p>Communication Complexity defines the level at which the agent may converse with the oracle, and we differentiate three levels: In the first level, the agent is only required to understand an Initial Instruction before navigation starts. In the second level, the agent sends a signal for help whenever it is unsure, utilizing the Guidance from the oracle. In the third level, the agent with Dialogue ability asks questions in the form of natural language during the navigation and understands further oracle guidance.</p><p>Task Objective defines how the agent attains its goal based on the initial instructions from the oracle. In the first objective type, Fine-grained Navigation, the agent can find the target according to a detailed step-by-step route description. In the second type, Coarse-grained Navigation, the agent is required to find a distant target goal with a coarse navigation description, requiring the agent to reason a path in a navigable environment and possibly elicit additional oracle help. Tasks in the previous two types only require the agent to navigate to complete the mission. In the third type, Navigation and Object Interaction, besides reasoning a path, the agent also needs to interact with objects in the environment to achieve the goal since the object might be hidden or need to change physical states. <ref type="foot" target="#foot_1">2</ref>As with coarse-grained navigation, some object interaction tasks can require additional supervision via dialogue with the oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Initial Instruction</head><p>In many VLN benchmarks, the agent is given a natural language instruction for the whole navigation process, such as "Go upstairs and pass the table in the living room. Turn left and go through the door in the middle." Fine-grained Navigation An agent needs to strictly follow the natural language instruction to reach the target goal. <ref type="bibr">Anderson et al. (2018b)</ref> create the R2R dataset based on the Matterport3D simulator <ref type="bibr" target="#b18">(Chang et al., 2017</ref>). An embodied agent in R2R moves through a house in the simulator traversing edges on a navigation graph, jumping to adjacent nodes containing panoramic views. R2R is extended to create other VLN benchmarks. Roomfor-Room joins paths in R2R to longer trajectories <ref type="bibr" target="#b50">(Jain et al., 2019)</ref>. <ref type="bibr" target="#b112">Yan et al. (2020)</ref> collect XL-R2R to extend R2R with Chinese instructions. RxR <ref type="bibr" target="#b57">(Ku et al., 2020)</ref> contains instructions from English, Hindi, and Telegu. The dataset has more samples and the instructions in it are time-aligned to the virtual poses of the instruction. The English split of RxR is further extended to build Landmark-RxR <ref type="bibr" target="#b40">(He et al., 2021)</ref> by incorporating landmark information.</p><p>In most current datasets, agents traverse a navigation graph at predefined viewpoints. To facil- Outdoor environments are usually more complex and contain more objects than indoor environments. In TOUCHDOWN <ref type="bibr" target="#b21">(Chen et al., 2019)</ref>, an agent follows instructions to navigate a streetview rendered simulation of New York City to find a hidden object. Most photo-realistic outdoor VLN datasets including TOUCHDOWN <ref type="bibr" target="#b21">(Chen et al., 2019)</ref>, StreetLearn <ref type="bibr" target="#b71">(Mirowski et al., 2019;</ref><ref type="bibr" target="#b69">Mehta et al., 2020</ref><ref type="bibr" target="#b0">), StreetNav(Hermann et al., 2020</ref><ref type="bibr">), and Talk2Nav (Vasudevan et al., 2021)</ref> are proposed based on Google Street View. Some work uses natural language to guide drones. LANI <ref type="bibr" target="#b73">(Misra et al., 2018</ref>) is a 3D synthetic navigation environment, where an agent navigates between landmarks following natural language instructions. Current datasets on drone navigation usually fall in a synthetic environment such as Unity3D <ref type="bibr" target="#b14">(Blukis et al., 2018</ref><ref type="bibr" target="#b15">(Blukis et al., , 2019))</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-grained Navigation</head><p>In real life, detailed information about the route may not be available since it may be unknown to the human instructor (oracle). Usually, instructions are more concise and contain merely information of the target goal.</p><p>RoomNav <ref type="bibr" target="#b109">(Wu et al., 2018)</ref> requires agent navigate according to instruction "go to X", where X is a predefined room or object.</p><p>In Embodied QA <ref type="bibr" target="#b25">(Das et al., 2018)</ref>, the agent navigates through the environment to find answer for a given question. The instructions in REVERIE <ref type="bibr">(Qi et al., 2020b)</ref> are annotated by humans, and thus more complicated and diverse. The agent navigates through the rooms and differentiates the object against multiple competing candidates. In SOON <ref type="bibr">(Zhu et al., 2021a)</ref>, an agent receives a long, complex coarse-to-fine instruction which gradually narrows down the search scope.</p><p>Navigation+Object Interaction For some tasks, the target object might be hidden (e.g., the spoon in a drawer), or need to change status (e.g., a sliced apple is requested but only a whole apple is available). In these scenarios, it is necessary to interact with the objects to accomplish the task (e.g., opening the drawer or cutting the apple). Interactive Question Answering (IQA) requires the agent to navigate and sometimes to interact with objects to answer a given question. Based on indoor scenes in AI2-THOR <ref type="bibr" target="#b53">(Kolve et al., 2017)</ref>, <ref type="bibr" target="#b88">Shridhar et al. (2020)</ref> propose the ALFRED dataset, where agents are provided with both coarse-grained and fine-grained instructions complete household tasks in an interactive visual environment. CHAI <ref type="bibr" target="#b73">(Misra et al., 2018)</ref> requires the agent to navigate and simply interact with the environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Oracle Guidance</head><p>Agents in Guidance VLN tasks may receive further natural language guidance from the oracle during navigation. For example, if the agent is unsure of the next step (e.g., entering the kitchen), it can send a [help] signal, and the oracle would assist by responding "go left" <ref type="bibr">(Nguyen et al., 2019)</ref>. Fine-grained Navigation The initial fine-grained navigation instruction may still be ambiguous in a complex environment. Guidance from the oracle could clarify possible confusion. <ref type="bibr" target="#b24">Chi et al. (2020)</ref> introduce Just Ask-a task where an agent could ask oracle for help during navigation. Coarse-grained Navigation With only a coarsegrained instruction given at the beginning, the agent tends to be more confused and spends more time exploring. Further guidance resolves this ambiguity. VNLA <ref type="bibr">(Nguyen et al., 2019)</ref> and HANNA <ref type="bibr" target="#b77">(Nguyen and Daum? III, 2019)</ref> both train an agent to navigate indoors to find objects. The agent could request help from the oracle, which responds by providing a subtask which helps the agent make progress. While oracle in VNLA uses predefined script to respond, the oracle in HANNA uses a neural network to generate natural language responses. CEREALBAR <ref type="bibr" target="#b91">(Suhr et al., 2019)</ref> is a collaborative task between a leader and a follower. Both agents move in a virtual game environment to collect valid sets of cards. Navigation+Object Interaction While VLN is still in its youth, there are no VLN datasets in support of Guidance and Object Interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Human Dialogue</head><p>It is human-friendly to use natural language to request help <ref type="bibr" target="#b12">(Banerjee et al., 2020;</ref><ref type="bibr" target="#b4">Thomason et al., 2019b)</ref>. For example, when the agent is not sure about what fruit the human wants, it could ask "What fruit do you want, the banana in the refrigerator or the apple on the table?", and the human response would provide clear navigation direction. Fine-grained Navigation No datasets are in the scope of this category. Currently, route-detailed instruction with possible guidance could help the agent achieve relatively good performance in most simulated environments. We expect datasets to be developed for this category for super long horizon navigation tasks in complex environments especially with rich dynamics where dialog is necessary to clear confusions. Coarse-grained Navigation CVDN <ref type="bibr" target="#b4">(Thomason et al., 2019b</ref>) is a dataset of human-human dialogues. Besides interpreting a natural language instruction and deciding on the following action, the VLN agent also needs to ask questions in natural language for guidance. The oracle, with knowledge of the best next steps, needs to understand and correctly answer said questions.</p><p>Dialogue is important in complex outdoor environments. de Vries et al. ( <ref type="formula">2018</ref>) introduce the Talk the Walk dataset, where the guide has knowledge from a map and guides the tourist to a destination, but does not know the tourist's location; while the tourist navigates a 2D grid via discrete actions. Navigation+Object Interaction Minecraft Collaborative Building (Narayan- <ref type="bibr" target="#b21">Chen et al., 2019)</ref> studies how an agent places blocks into a building by communicating with the oracle. TEACh <ref type="bibr" target="#b80">(Padmakumar et al., 2021)</ref> is a dataset that studies object interaction and navigation with free-form dialog. The follower converses with the commander and interacts with the environment to complete various house tasks such as making coffee. Dial-FRED <ref type="bibr" target="#b35">(Gao et al., 2022)</ref> extends ALFRED <ref type="bibr" target="#b88">(Shridhar et al., 2020)</ref> dataset by allowing the agent to actively ask questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Goal-oriented Metrics mainly consider the agent's proximity to the goal. The most intuitive is Success Rate (SR), which measures how frequently an agent completes the task within a certain distance of the goal. Goal Progress <ref type="bibr" target="#b4">(Thomason et al., 2019b)</ref> measures the reduction in remaining distance to the target goal. Path Length (PL) measures the total length of the navigation path. Shortest-Path Distance (SPD) measures the mean distance between the agent's final location and the goal. Since a longer path length is undesirable (increases duration and wear-and-tear on actual robots), Success weighted by Path Length (SPL) <ref type="bibr">(Anderson et al., 2018a)</ref> balances both Success Rate and Path Length. Similarly, Success weighted by Edit Distance (SED) <ref type="bibr" target="#b21">(Chen et al., 2019)</ref> compares the expert's actions/trajectory to the agent's actions/trajectory, also balancing SR and PL. Oracle Navigation Error (ONE) takes the shortest dis-tance from any node in the path rather than just the last node, and Oracle Success Rate (OSR) measures whether any node in the path is within a threshold from the target location. Path-fidelity Metrics evaluate to what extent an agent follows the desired path. Some tasks require the agent not only to find the goal location but also to follow specific path. Fidelity measures the matches between the action sequence in the expert demonstration and the action sequence in the agent trajectory. Coverage weighted by LS (CLS) <ref type="bibr" target="#b50">(Jain et al., 2019)</ref> is the product of the Path Coverage (PC) and Length Score (LS) with respect to the reference path. It measures how closely an agent's trajectory follows the reference path. Normalized Dynamic Time Warping (nDTW) <ref type="bibr" target="#b47">(Ilharco et al., 2019)</ref> softly penalizes deviations from the reference path to calculate the match between two paths. Success weighted by normalized Dynamic Time Warping (SDTW) <ref type="bibr" target="#b47">(Ilharco et al., 2019)</ref> further constrains nDTW to only successful episodes to capture both success and fidelity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">VLN Methods</head><p>As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we categorize existing methods into Representation Learning, Action Strategy Learning, Data-centric Learning, and Prior Exploration. Representation learning methods help agent understand relations between these modalities since VLN involves multiple modalities, including vision, language, and action. Moreover, VLN is a complex reasoning task where mission results depend on the accumulating steps, and better action strategies help the decision-making process. Additionally, VLN tasks face challenges within their training data. One severe problem is scarcity.</p><p>Collecting training data for VLN is expensive and time-consuming, and the existing VLN datasets are relatively small with respect to the complexity of VLN tasks. Therefore, data-centric methods help to utilize the existing data and create more training data. Prior exploration helps adapt agents to previously unseen environments, improving their ability to generalize, decreasing the performance gap between seen versus unseen environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Learning</head><p>Representation learning helps the agent understand how the words in the instruction relate to the perceived features in the environment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pretraining</head><p>Vision or Language Using a pretrained model to initialize a vision or text encoder provides agents with single-modality knowledge. pretrained vision models may use a ResNet <ref type="bibr" target="#b39">(He et al., 2016)</ref> or Vision Transformers <ref type="bibr" target="#b29">(Dosovitskiy et al., 2020)</ref>. Other navigation tasks <ref type="bibr">(Wijmans et al., 2019b)</ref> may also provide visual initialization <ref type="bibr" target="#b56">(Krantz et al., 2020)</ref>. Large pretrained language models such as BERT <ref type="bibr" target="#b28">(Devlin et al., 2019)</ref> and GPT <ref type="bibr" target="#b86">(Radford et al., 2019)</ref> can encode language and improve instruction understanding <ref type="bibr" target="#b51">(Li et al., 2019)</ref>, which can be further pretrained with VLN instructions <ref type="bibr" target="#b82">(Pashevich et al., 2021)</ref> before fine-tuning in VLN task. Vision and Language Vision-and-language pretrained models provide good joint representation for text and vision. A common practice is to initialize the VLN agent with a pretrained model such as ViLBERT <ref type="bibr" target="#b63">(Lu et al., 2019)</ref>. The agent may be further trained with VLN-specific features such as objects and rooms <ref type="bibr">(Qi et al., 2021)</ref>. VLN Downstream tasks benefit from being closely related to the pretraining task. Researchers also explored pretraining on the VLN domain directly. VLN-BERT <ref type="bibr" target="#b67">(Majumdar et al., 2020)</ref> pretrains navigation models to measure the compatibility between paths and instructions, which formats VLN as a path selection problem. PREVALENT <ref type="bibr" target="#b38">(Hao et al., 2020)</ref> is trained from scratch on image-textaction triplets to learn textual representations in VLN tasks. The output embedding from the [CLS] token in BERT-based pretraining models could be leveraged in a recurrent fashion to represent history state <ref type="bibr" target="#b44">(Hong et al., 2021;</ref><ref type="bibr" target="#b74">Moudgil et al., 2021)</ref>.</p><p>Airbert <ref type="bibr" target="#b37">(Guhur et al., 2021)</ref> achieve good performance on few-shot setting after pretraining on a large-scale in-domain dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Semantic Understanding</head><p>Semantic understanding of VLN tasks incorporates knowledge about important features in VLN. In addition to the raw features, high-level semantic representations also improve performance in unseen environments.</p><p>Intra-Modality Visual or textual modalities can be decomposed into many features, which matter differently in VLN. The overall visual features extracted by a neural model may actually hurt the performance in some cases <ref type="bibr">(Thomason et al., 2019a;</ref><ref type="bibr" target="#b45">Hu et al., 2019;</ref><ref type="bibr">Zhang et al., 2020b)</ref>. Therefore, it is important to find the feature(s) that best improve performance. High-level features such as visual appearance, route structure, and detected objects outperform the low level visual features extracted by CNN <ref type="bibr" target="#b45">(Hu et al., 2019)</ref>. Different types of tokens within the instruction also function differently <ref type="bibr">(Zhu et al., 2021b)</ref>. Extracting these tokens and encoding the object tokens and directions tokens are crucial <ref type="bibr">(Qi et al., 2020a;</ref><ref type="bibr">Zhu et al., 2021b)</ref>.</p><p>Inter-Modality Semantic connections between different modalities: actions, scenes, observed objects, direction clues, and objects mentioned in instructions can be extracted and then softly aligned with attention mechanism <ref type="bibr">(Qi et al., 2020a;</ref><ref type="bibr" target="#b34">Gao et al., 2021)</ref>. The soft alignment also highlights relevant parts of the instruction with respect to the current step <ref type="bibr" target="#b59">(Landi et al., 2019;</ref><ref type="bibr">Zhang et al., 2020a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Graph Representation</head><p>Building graph to incorporate structured information from instruction and environment observation provides explicit semantic relation to guide the navigation. The graph neural network may encode the relation between text and vision to better interpret the context information <ref type="bibr">(Hong et al., 2020a;</ref><ref type="bibr" target="#b27">Deng et al., 2020)</ref>. The graph could record the location information during the navigation, which can used to predict the most likely trajectory <ref type="bibr">(Anderson et al., 2019a)</ref> or probability distribution over action space <ref type="bibr" target="#b27">(Deng et al., 2020)</ref>. When connected with prior exploration, an overview graph about the navigable environment <ref type="bibr">(Chen et al., 2021a</ref>) can be built to improve navigation interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Memory-augmented Model</head><p>Information accumulates as the agent navigates, which is not efficient to utilize directly. Memory structure helps the agent effectively leverage the navigation history. Some solutions leverage memory modules such as LSTMs or recurrently utilize informative states <ref type="bibr" target="#b44">(Hong et al., 2021)</ref>, which can be relatively easily implemented, but may struggle to remember features at the beginning of the path as path length increases. Another solution is to build a separate memory model to store the relevant information <ref type="bibr">(Zhu et al., 2020c;</ref><ref type="bibr" target="#b61">Lin et al., 2021;</ref><ref type="bibr" target="#b77">Nguyen and Daum? III, 2019)</ref>. Notably, by hierarchically encoding a single view, a panorama, and then all panoramas in history, HAMT <ref type="bibr">(Chen et al., 2021b)</ref> successfully utilized the full navigation history for decision-making.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Auxiliary Tasks</head><p>Auxiliary tasks help the agent better understand the environment and its own status without extra labels. From the machine learning perspective, an auxiliary task is usually achieved in the form of an additional loss function. The auxiliary task could, for example, explain its previous actions, or predict information about future decisions <ref type="bibr">(Zhu et al., 2020a)</ref>. Auxiliary tasks could also involve the current mission such as current task accomplishment, and vision &amp; instruction alignment <ref type="bibr">(Ma et al., 2019a;</ref><ref type="bibr">Zhu et al., 2020a)</ref>. Notably, auxiliary tasks are effective when adapting pretrained representations for VLN <ref type="bibr" target="#b46">(Huang et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Action Strategy Learning</head><p>With many possible action choices and complicated environment, action strategy learning provides a variety of methods to help the agent decide on those best actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Reinforcement Learning</head><p>VLN is a sequential decision-making problem and can naturally be modeled as a Markov decision process. So Reinforcement Learning (RL) methods are proposed to learn better policy for VLN tasks. A critical challenge for RL methods is that VLN agents only receive the success signal at the end of the episode, so it is difficult to know which actions to attribute success to, and which to penalize. To address the ill-posed feedback issue, <ref type="bibr" target="#b104">Wang et al. (2019)</ref> propose RCM model to enforces cross-modal grounding both locally and globally, with goal-oriented extrinsic reward and instructionfidelity intrinsic reward. <ref type="bibr" target="#b40">He et al. (2021)</ref> propose to utilize the local alignment between the instruction and critical landmarks as the reward. Evaluation metrics such as CLS <ref type="bibr" target="#b50">(Jain et al., 2019)</ref> or nDTW <ref type="bibr" target="#b47">(Ilharco et al., 2019)</ref> can also provide informative reward signal <ref type="bibr" target="#b58">(Landi et al., 2020)</ref>, and natural language may also provide suggestions for reward <ref type="bibr" target="#b32">(Fu et al., 2019)</ref>.</p><p>To model the dynamics in the environment, <ref type="bibr" target="#b105">Wang et al. (2018)</ref> leverage model-based reinforcement learning to predict the next state and improve the generalization in unseen environment. <ref type="bibr">Zhang et al. (2020a)</ref> find recursively alternating the learning schemes of imitation and reinforcement learning improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Exploration during Navigation</head><p>Exploring and gathering environmental information while navigating provides a better understanding of the state space. Student-forcing is a frequently used strategy, where the agent keeps navigating based on sampled actions and is supervised by the shortest-path action <ref type="bibr">(Anderson et al., 2018b)</ref>.</p><p>There is a tradeoff between exploration versus exploitation: with more exploration, the agent sees better performance at the cost of a longer path and longer duration, so the model needs to determine when and how deep to explore <ref type="bibr">(Wang et al., 2020a)</ref>. After having gathered the local information, the agent needs to decide which step to choose, or whether to backtrack <ref type="bibr" target="#b51">(Ke et al., 2019)</ref>. <ref type="bibr">Notably, Koh et al. (2021)</ref> designed Pathdreamer, a visual world model to synthesize visual observation future viewpoints without actually looking ahead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Navigation Planning</head><p>Planing future navigation steps leads to a better action strategy. From the visual side, predicting the waypoints <ref type="bibr" target="#b55">(Krantz et al., 2021)</ref>, next state and reward <ref type="bibr" target="#b105">(Wang et al., 2018)</ref>, generate future observation <ref type="bibr" target="#b52">(Koh et al., 2021)</ref> or incorporating neighbor views <ref type="bibr" target="#b6">(An et al., 2021)</ref> has proven effective. The natural language instruction also contains landmarks and direction clues to plan detailed steps. <ref type="bibr">Anderson et al. (2019b)</ref> predict the forthcoming events based on the instruction, which is used to predict actions with a semantic spatial map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Asking for Help</head><p>An intelligent agent asks for help when uncertain about the next action. Action probabilities or a separately trained model <ref type="bibr" target="#b24">(Chi et al., 2020;</ref><ref type="bibr">Zhu et al., 2021c;</ref><ref type="bibr">Nguyen et al., 2021a)</ref> can be leveraged to decide whether to ask for help. Using natural language to converse with the oracle covers a wider problem scope than sending a signal. Both rule-based methods <ref type="bibr" target="#b80">(Padmakumar et al., 2021)</ref> and neural-based methods <ref type="bibr" target="#b87">(Roman et al., 2020;</ref><ref type="bibr">Nguyen et al., 2021a)</ref> have been developed to build navigation agents with dialog ability. Meanwhile, for tasks <ref type="bibr" target="#b4">(Thomason et al., 2019b;</ref><ref type="bibr" target="#b80">Padmakumar et al., 2021)</ref> that do not provide an oracle agent to answer question in natural language, researchers also need to build a rule-based <ref type="bibr" target="#b80">(Padmakumar et al., 2021</ref><ref type="bibr">) or neural-based (Roman et al., 2020)</ref> oracle. Dial-FRED <ref type="bibr" target="#b35">(Gao et al., 2022</ref>) uses a language model as an oracle to answer questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data-centric Learning</head><p>Compared with previously discussed works that focus on building a better VLN agent structure, data-centric methods most effectively utilize the existing data, or create synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Data Augmentation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trajectory-Instruction Augmentation</head><p>Augmented path-instruction pairs could be used in VLN directly. Currently the common practice is to train a speaker module to generate instructions given a navigation path <ref type="bibr" target="#b31">(Fried et al., 2018)</ref>. This generated data have varying quality <ref type="bibr" target="#b116">(Zhao et al., 2021)</ref>. Therefore an alignment scorer <ref type="bibr" target="#b46">(Huang et al., 2019)</ref> or adversarial discriminator <ref type="bibr" target="#b33">(Fu et al., 2020)</ref> can select high-quality pairs for augmentation. Environment Augmentation Generating more environment data not only helps generate more trajectories, but also alleviates the problem of overfitting in seen environments. Randomly masking the same visual feature across different viewpoints <ref type="bibr" target="#b94">(Tan et al., 2019)</ref> or simply splitting the house scenes and re-mixing them <ref type="bibr" target="#b62">(Liu et al., 2021)</ref> could create new environments, which could further be used to generate more trajectory-instruction pairs <ref type="bibr" target="#b31">(Fried et al., 2018)</ref>. Training data may also be augmented by replacing some visual features with counterfactual ones <ref type="bibr" target="#b81">(Parvaneh et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Curriculum Learning</head><p>Curriculum learning <ref type="bibr" target="#b13">(Bengio et al., 2009)</ref> gradually increases the task's difficulty during the training process. The instruction length could be a metric for task difficulty. BabyWalk <ref type="bibr">(Zhu et al., 2020b)</ref> keep increasing training samples' instruction length during the training process. Attributes from the trajectory may also be used to rank task difficulty. <ref type="bibr" target="#b34">Zhang et al. (2021)</ref> rearrange the R2R dataset using the number of rooms each path traverses. They found curriculum learning helps smooth the loss landscape and find a better local optima.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Multitask Learning</head><p>Different VLN tasks can benefit from each other by cross-task knowledge transfer. <ref type="bibr">Wang et al. (2020c)</ref> propose an environment-agnostic multitask navigation model for both VLN and Navigation from Dialog History tasks <ref type="bibr" target="#b4">(Thomason et al., 2019b)</ref>. <ref type="bibr" target="#b19">Chaplot et al. (2020)</ref> propose an attention module to train a multitask navigation agent to follow instructions and answer questions <ref type="bibr">(Wijmans et al., 2019a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Instruction Interpretation</head><p>A trajectory instruction interpreted multiple times in different ways may help the agent better understand its objective. LEO <ref type="bibr" target="#b111">(Xia et al., 2020)</ref> leverages and encodes all the instructions with a shared set of parameters to enhance the textual understanding. LWIT <ref type="bibr">(Nguyen et al., 2021b)</ref> interprets the instructions to make it clear to interact with what class of objects. Shorter, and more concise instructions provide clearer guidance for the agent compared to longer, semantically entangled instructions, thus <ref type="bibr">Hong et al. (2020b)</ref> breaks long instructions into shorter ones, allowing the agent to track progress and focus on each atomic instruction individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prior Exploration</head><p>Good performance in seen environments often cannot generalize to unseen environments <ref type="bibr" target="#b45">(Hu et al., 2019;</ref><ref type="bibr" target="#b81">Parvaneh et al., 2020;</ref><ref type="bibr" target="#b94">Tan et al., 2019)</ref>. Prior exploration methods allow the agent to observe and adapt to unseen environments,<ref type="foot" target="#foot_2">3</ref> bridging the performance gap between seen and unseen environments. <ref type="bibr" target="#b104">Wang et al. (2019)</ref> introduce a self-supervised imitation learning to learn from the agent's own past, good behaviors. The best navigation path determined to align the instruction the best by a matching critic will be used to update the agent. <ref type="bibr" target="#b94">Tan et al. (2019)</ref> leverage the testing environments to sample and augment paths for adaptation. <ref type="bibr" target="#b33">Fu et al. (2020)</ref> propose environment-based prior exploration, where the agent can only explore a particular environment where it is deployed. When utilizing graph, prior exploration may construct a map or overview about the unseen environment to provide explicit guidance for navigation <ref type="bibr">(Chen et al., 2021a;</ref><ref type="bibr" target="#b117">Zhou et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Visual-and-Language Tasks</head><p>This paper focuses on Vision-and-Language Navigation tasks with an emphasis on photo-realistic environments. 2D map may also be a uesful virtual environment for navigation tasks <ref type="bibr" target="#b100">(Vogel and Jurafsky, 2010;</ref><ref type="bibr" target="#b20">Chen and Mooney, 2011;</ref><ref type="bibr" target="#b83">Paz-Argaman and Tsarfaty, 2019)</ref>. Synthetic environments may also be a substitute for realistic environment <ref type="bibr" target="#b66">(MacMahon et al., 2006;</ref><ref type="bibr" target="#b16">Blukis et al., 2020)</ref>. <ref type="bibr" target="#b96">Tellex et al. (2011)</ref> propose to instantiate a probabilistic graphical model for natural language commands in robotic navigation and mobile manipulation process.</p><p>In VLN, an agent needs to follow the given instruction and even ask for assistants in human language. An agent in Visual Navigation tasks is usually not required to understand information from textual modality. Visual Navigation is a problem of navigating an agent from the current location to find the goal target. Researchers have achieved success in both simulated environments <ref type="bibr" target="#b124">(Zhu et al., 2017;</ref><ref type="bibr">Mirowski, 2019)</ref> and real environments <ref type="bibr" target="#b72">(Mirowski et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Directions</head><p>In this paper, we discuss the importance of VLN agents as a part of society, how their tasks vary as a function of communication level versus task objective, and how different agents may be evaluated. We broadly review VLN methodologies and categorize them. This paper only discusses these issues broadly at an introductory level. In reviewing these papers, we can see the immense progress that has already been made, as well as directions that this research topic can be expanded on.</p><p>Current methods usually do not explicitly utilize external knowledge such as objects and general house descriptions in Wikipedia. Incorporating knowledge also improves the interpretability and trust of embodied AI. Moreover, currently several navigation agents learn which direction to move and with what to interact, but there is a last-mile problem of VLN-how to interact with objects. <ref type="bibr">An-derson et al. (2018b)</ref> asked whether a robot could learn to "Bring me a spoon"; new research may ask how a robot can learn to "Pick up a spoon". The environments also lack diversity: most interior terrestrial VLN data consists of American houses, but never warehouses or hospitals: the places where these agents may be of most use.</p><p>Below we detail additional future directions: Collaborative VLN Current VLN benchmarks and methods predominantly focus on tasks where only one agent navigates, yet complicated realworld scenarios may require several robots collaborating. Multi-agent VLN tasks require development in swarm intelligence, information communication, and performance evaluation. MeetUp! <ref type="bibr" target="#b48">(Ilinykh et al., 2019</ref>) is a two-player coordination game where players move in a visual environment to find each other. VLN studies the relationship between the human and the environment in Figure <ref type="figure" target="#fig_0">1</ref>, yet here humans are oracles simply observing (but not acting on) the environment. Collaboration between humans and robots is crucial for them to work together as teams (e.g., as personal assistants or helping in construction). Future work may target at collaborative VLN between multiple agents or between human and agents. Simulation to Reality There is a performance loss when transferred to real-life robot navigation <ref type="bibr" target="#b10">(Anderson et al., 2020)</ref>. Real robots function in continuous space, but most simulators only allow agents to "hop" through a pre-defined navigation graph which is unrealistic for three reasons <ref type="bibr" target="#b56">(Krantz et al., 2020)</ref>. Navigation graphs assume: (1) perfect localization-in the real world it is a noisy estimate;</p><p>(2) oracle navigation-real robots cannot "teleport" to a new node; (3) known topology-in reality an agent may not have access to a preset list of navigable nodes. Continuous implementations of realistic environments may contain patches of the images, be blurred, or have parallax errors, making them unrealistic. A simulation that is based on both a 3D model and realistic imagery could improve the match between virtual sensors (in simulation) and real sensors. Lastly, most simulators assume a static environment only changed by the agent. This does not account for other dynamics such as people walking or objects moving, nor does it account for lighting conditions through the day. VLN environments with probabilistic transition functions may also narrow the gap between simulation and reality.</p><p>Ethics &amp; Privacy During both training and in-ference, VLN agents may observe and store sensitive information that can get leaked or misused. Effective navigation with privacy protection is crucially important. Relevant areas such as federated learning <ref type="bibr" target="#b54">(Kone?n? et al., 2016)</ref> or differential privacy <ref type="bibr" target="#b30">(Dwork et al., 2006)</ref>   <ref type="bibr" target="#b112">(Yan et al., 2020;</ref><ref type="bibr" target="#b57">Ku et al., 2020)</ref> could be good resources to study multicultural differences from the linguistic perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>Here in Table <ref type="table">2</ref>, we introduce more information about the datasets. Compared with the number of the datasets, the simulators are limited. More specifically, most indoor datasets are based on Mat-terport3D and most outdoor datasets are based on Google Street View. Also, more datasets are about indoor environments rather than outdoor environments. Outdoor environments are usually more complex and contain more objects compared with indoor environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Simulator</head><p>The virtual features of the dataset are deeply connected with the simulator in which datasets are built. Here we summarize simulators frequently used during the VLN dataset creation process.</p><p>House3D <ref type="bibr" target="#b109">(Wu et al., 2018)</ref> is a realistic virtual 3D environment built based on the SUNCG <ref type="bibr" target="#b89">(Song et al., 2017)</ref> dataset. An agent in the environment has access to first-person view RGB images, together with semantic/instance masks and depth information.</p><p>Matterport3D <ref type="bibr">(Anderson et al., 2018b)</ref> simulator is a large-scale visual reinforcement learning simulation environment for research on embodied AI based on the Matterport3D dataset <ref type="bibr" target="#b18">(Chang et al., 2017)</ref>. Matterport3D contains various indoor scenes, including houses, apartments, hotels, offices, and churches. An agent can navigate between viewpoints along a pre-defined graph. Most indoors VLN datasets such as R2R and its variants are based on the Matterport3D simulator.</p><p>Habitat <ref type="bibr" target="#b68">(Manolis Savva* et al., 2019;</ref><ref type="bibr">Szot et al., 2021)</ref> is a 3D simulation platform for training embodied AI in 3D physics-enabled scenarios. Compared with other simulation environments, Habitat 2.0 <ref type="bibr">(Szot et al., 2021)</ref> shows strength in system response speed. Habitat has the following datasets built-in: Matterport3D <ref type="bibr" target="#b18">(Chang et al., 2017)</ref>, Gibson <ref type="bibr" target="#b110">(Xia et al., 2018)</ref>, and Replica <ref type="bibr" target="#b90">(Straub et al., 2019)</ref>. AI2-THOR <ref type="bibr" target="#b53">(Kolve et al., 2017</ref>) is a near photo-realistic 3D indoor simulation environment, where agents could navigate and interact with objects. Based on the object interaction function, it helps to build a dataset that requires object interaction, such as ALFRED <ref type="bibr" target="#b88">(Shridhar et al., 2020)</ref>.</p><p>Gibson <ref type="bibr" target="#b110">(Xia et al., 2018</ref>) is a real-world perception interactive environment with complex semantics. Each viewpoint has a set of RGB panoramas with global camera poses and reconstructed 3D meshes. Matterport3D dataset <ref type="bibr" target="#b18">(Chang et al., 2017)</ref> is also integrated into the Gibson simulator.</p><p>House3D <ref type="bibr" target="#b109">(Wu et al., 2018</ref>) converts SUNCG's static environment into a virtual environment, where the agent can navigate with physical constraints (e.g. it cannot pass through walls or objects).</p><p>LANI <ref type="bibr" target="#b73">(Misra et al., 2018)</ref> is a 3D simulator built in Unity3D platform. The environment in LANI is a fenced, square, grass field containing randomly placed landmarks. An agent needs to navigate between landmarks following the natural language instruction. Drone navigation tasks <ref type="bibr" target="#b14">(Blukis et al., 2018</ref><ref type="bibr" target="#b15">(Blukis et al., , 2019) )</ref> are also built based on LANI.</p><p>Currently, most datasets and simulators focus on indoors navigable scenes partly because of the difficulty of building an outdoor photo-realistic 3D simulator out of the increased complexity. Google Street View<ref type="foot" target="#foot_3">4</ref> , an online API that is integrated with Google Maps, is composed of billions of realistic street-level panoramas. It has been frequently used to create outdoor VLN tasks since the development of TOUCHDOWN <ref type="bibr" target="#b21">(Chen et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Room-to-Room Leaderboard</head><p>Room-to-Room (R2R) <ref type="bibr">(Anderson et al., 2018b)</ref> is the benchmark used most frequently for evaluating different methods. Here we collect all the reported performance metrics in the corresponding papers and the official R2R leaderboard<ref type="foot" target="#foot_4">5</ref> . Since beam search explores more routes, and since prior exploration has additional observations in the test environment, their performance can not be directly compared with other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The agent and oracle discuss the VLN task in natural language. Both observe and interact with the navigable environment to accomplish a task.</figDesc><graphic url="image-2.png" coords="1,314.56,283.44,54.85,59.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Categories of VLN methods. Methods may not be mutually exclusive to an individual category.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Vision-and-Language Navigation benchmarks organized by Communication Complexity versus Task Objective. Please refer to Appendix for more details about the datasets and the commonly used underlying simulators.</figDesc><table><row><cell>itate transfer learning to real agents, VLN tasks</cell></row><row><cell>should provide a continuous action space and a</cell></row><row><cell>freely navigable environment. To this end, Krantz</cell></row><row><cell>et al. (2020) reconstruct the navigation graph based</cell></row><row><cell>R2R trajectories in continuous environments and</cell></row><row><cell>create VLNCE. Irshad et al. (2021) propose Robo-</cell></row><row><cell>VLN task where the agent operates in a continuous</cell></row><row><cell>action space over long-horizon trajectories.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We also release a Github repo to keep track of advances in VLN: https://github.com/eric-ai-lab/ awesome-vision-language-navigation</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Navigation and Object Interaction includes both finegrained and coarse-grained instructions, which ideally should be split further. But given that there are only few datasets in this category, we keep the current categorization in Table2.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Thus prior exploration methods are not directly comparable with other VLN methods.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://developers.google.com/maps/ documentation/streetview/overview</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://eval.ai/web/challenges/ challenge-page/97/leaderboard/270</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulator</head><p>Language-Active Environment Room-to-Room <ref type="bibr">(Anderson et al., 2018b)</ref> Matterport3D Indoor Room-for-Room <ref type="bibr" target="#b50">(Jain et al., 2019)</ref> Matterport3D Indoor Room-Across-Room <ref type="bibr" target="#b57">(Ku et al., 2020)</ref> Matterport3D Indoor Landmark-RxR <ref type="bibr" target="#b40">(He et al., 2021)</ref> Matterport3D Indoor XL-R2R <ref type="bibr" target="#b112">(Yan et al., 2020)</ref> Matterport3D Indoor VLNCE <ref type="bibr" target="#b56">(Krantz et al., 2020)</ref> Habitat Indoor StreetLearn <ref type="bibr" target="#b71">(Mirowski et al., 2019)</ref> Google Street View Outdoor StreetNav <ref type="bibr">(Hermann et al., 2020)</ref> Google Street View Outdoor TOUCHDOWN <ref type="bibr" target="#b21">(Chen et al., 2019)</ref> Google Street View Outdoor Talk2Nav <ref type="bibr" target="#b99">(Vasudevan et al., 2021)</ref> Google Street View Outdoor LANI <ref type="bibr" target="#b73">(Misra et al., 2018)</ref> -Outdoor RoomNav <ref type="bibr" target="#b109">(Wu et al., 2018)</ref> House3D Indoor EmbodiedQA <ref type="bibr" target="#b25">(Das et al., 2018)</ref> House3D Indoor REVERIE <ref type="bibr">(Qi et al., 2020b)</ref> Matterport3D Indoor SOON <ref type="bibr">(Zhu et al., 2021a)</ref> Matterport3D Indoor IQA <ref type="bibr" target="#b36">(Gordon et al., 2018)</ref> AI2-THOR Indoor CHAI <ref type="bibr" target="#b73">(Misra et al., 2018)</ref> CHALET Indoor ALFRED <ref type="bibr" target="#b88">(Shridhar et al., 2020)</ref> AI2-THOR Indoor VNLA <ref type="bibr">(Nguyen et al., 2019)</ref> Matterport3D Indoor HANNA <ref type="bibr" target="#b77">(Nguyen and Daum? III, 2019)</ref> Matterport3D Indoor CEREALBAR <ref type="bibr" target="#b91">(Suhr et al., 2019)</ref> -Indoor Just Ask <ref type="bibr" target="#b24">(Chi et al., 2020)</ref> Matterport3D Indoor CVDN <ref type="bibr" target="#b4">(Thomason et al., 2019b)</ref> Matterport3D Indoor RobotSlang <ref type="bibr" target="#b12">(Banerjee et al., 2020)</ref> -Indoor Talk the Walk (de <ref type="bibr" target="#b26">Vries et al., 2018)</ref> -Outdoor MC Collab (Narayan- <ref type="bibr" target="#b21">Chen et al., 2019)</ref> Minecraft Outdoor TEACh <ref type="bibr" target="#b80">(Padmakumar et al., 2021)</ref> AI2-THOR Indoor DialFRED <ref type="bibr" target="#b35">(Gao et al., 2022)</ref> AI2-THOR Indoor  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">XL-R2</title>
		<author>
			<persName><forename type="first">Anderson</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Landmark-RxR (He et al., 2021), VLNCE (Krantz et al., 2020)</title>
		<imprint>
			<date type="published" when="2018">2018. 2019. 2020. 2020. 2019. 2019. 2020. 2018</date>
		</imprint>
	</monogr>
	<note>Talk2Nav. Vasudevan et al., 2021), LANI (Misra et al.</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">EmbodiedQA</title>
		<author>
			<persName><forename type="first">(</forename><surname>Roomnav</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note>REVERIE. Qi et al., 2020b), SOON (Zhu et al., 2021a</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Iqa (gordon</surname></persName>
		</author>
		<editor>ALFRED (Shridhar et al.</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Oracle Guidance Just Ask</title>
		<author>
			<persName><forename type="first">Chi</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CE-REALBAR</title>
		<author>
			<persName><surname>Vnla (nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">None Dialogue None CVDN</title>
		<imprint>
			<publisher>de Vries et al</publisher>
			<date type="published" when="2018">2019. 2019. 2019. 2019b). 2020. 2018</date>
		</imprint>
	</monogr>
	<note>Talk the Walk</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">(</forename><surname>Teach</surname></persName>
		</author>
		<author>
			<persName><surname>Padmakumar</surname></persName>
		</author>
		<title level="m">Minecraft Collaborative Building</title>
		<imprint>
			<date type="published" when="2019">2021. 2019. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Dong</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07201</idno>
		<title level="m">Neighbor-view enhanced model for vision and language navigation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Kosecka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06757</idno>
		<title level="m">Manolis Savva, et al. 2018a. On evaluation of embodied navigation agents</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Roozbeh Mottaghi</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chasing ghosts: Instruction following as bayesian state tracking</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chasing ghosts: Instruction following as bayesian state tracking</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="371" to="381" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer for vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanne</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Visionand-language navigation: Interpreting visuallygrounded navigation instructions in real environments</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The RobotSlang Benchmark: Dialog-guided robot localization and navigation</title>
		<author>
			<persName><forename type="first">Shurjo</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Following high-level navigation instructions on a simulated quadcopter with imitation learning</title>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nataly</forename><surname>Brukhim</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bennett</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
			<affiliation>
				<orgName type="collaboration">RSS</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to map natural language instructions to physical quadcopter control using simulated flight</title>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Terme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyvind</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to map natural language instructions to physical quadcopter control using simulated flight</title>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Terme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyvind</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">A</forename><surname>Knepper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Robot Learning</title>
		<meeting>the Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="1415" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual navigation for mobile robots: A survey</title>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Bonin-Font</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Oliver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of intelligent and robotic systems</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="296" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3D: Learning from RGB-D data in indoor environments. International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Embodied multimodal multitask learning</title>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to interpret natural language navigation instructions from observations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Touchdown: Natural language navigation and spatial reasoning in visual street environments</title>
		<author>
			<persName><forename type="first">Howard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.01282</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12530" to="12539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Marynel V?zquez, and Silvio Savarese. 2021a. Topological planning with transformers for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo</forename><surname>Junshen K Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="11276" to="11286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">2021b. History aware multimodal transformer for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Louis</forename><surname>Guhur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13309</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Just ask: An interactive learning framework for vision and language navigation</title>
		<author>
			<persName><forename type="first">Ta-Chung</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minmin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihail</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seokhwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Embodied question answering</title>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><surname>Kiela</surname></persName>
		</author>
		<title level="m">Talk the walk: Navigating new york city through grounded dialogue</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolving graphical planner: Contextual global planning for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020-12">2020. 2020-December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theory of cryptography conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Speaker-follower models for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Justin</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anoop</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07742</idno>
		<title level="m">From language to goals: Inverse reinforcement learning for visionbased instruction following</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Counterfactual vision-and-language navigation via adversarial path sampler</title>
		<author>
			<persName><forename type="first">Tsu-Jui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Room-and-object aware knowledge reasoning for remote embodied referring expression</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luting</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3064" to="3073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Govind</forename><surname>Thattai</surname></persName>
		</author>
		<author>
			<persName><surname>Gaurav</surname></persName>
		</author>
		<author>
			<persName><surname>Sukhatme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.13330</idno>
		<title level="m">Dialfred: Dialogue-enabled agents for embodied instruction following</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Iqa: Visual question answering in interactive environments</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4089" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Airbert: In-domain pretraining for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Pierre-Louis</forename><surname>Guhur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1634" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards learning a generic agent for vision-and-language navigation via pretraining</title>
		<author>
			<persName><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Landmarkrxr: Solving vision-and-language navigation with fine-grained alignment supervision</title>
		<author>
			<persName><forename type="first">Keji</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuanglin</forename><surname>Sima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to follow directions in street view</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Moritz Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Language and visual entity relationship graph for agent navigation</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7685" to="7696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sub-instruction aware vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.271</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3360" to="3376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Vln bert: A recurrent vision-and-language bert for navigation</title>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Rodriguez-Opazo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1643" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are you looking? grounding to multiple modalities in vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1655</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6551" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Transferable representation learning in vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Haoshuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05446</idno>
		<title level="m">General evaluation for instruction conditioned navigation using dynamic time warping</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sina</forename><surname>Zarrie?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Schlangen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05084</idno>
		<title level="m">Meetup! a corpus of joint activity dialogues in a visual environment</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Zubair Irshad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10674</idno>
		<title level="m">Hierarchical cross-modal agent for robotics vision-and-language navigation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stay on the path: Instruction fidelity in vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Magalhaes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1181</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1862" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Tactical rewind: Self-correction via backtracking in visionand-language navigation</title>
		<author>
			<persName><forename type="first">Liyiming</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pathdreamer: A world model for indoor navigation</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yu Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="14738" to="14748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winson</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Van-Derbilt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alvaro</forename><surname>Herrasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">AI2-THOR: An Interactive 3D Environment for Visual AI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Kone?n?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Richt?rik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Waypoint models for instruction-guided navigation in continuous environments</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15162" to="15171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond the nav-graph: Vision-and-language navigation in continuous environments</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Room-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roma</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods for Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Perceive, transform, and act: Multi-modal attention networks for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Landi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Embodied vision-and-language navigation with dynamic convolutional filters</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Landi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Robust navigation with language pretraining and stochastic sampling</title>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sceneintuitive agent for remote embodied visual grounding</title>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Vision-language navigation with random environmental mixup</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Dong</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1644" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">2019a. Self-monitoring navigation agent via auxiliary progress estimation</title>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghassan</forename><surname>Al-Regib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The regretful agent: Heuristic-aided navigation through progress estimation</title>
		<author>
			<persName><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Walk the talk: Connecting language, knowledge, and action in route instructions</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Stankiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Kuipers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Def</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improving vision-and-language navigation with imagetext pairs from the web</title>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Habitat: A Platform for Embodied AI Research</title>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhavana</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Retouchdown: Releasing touchdown on StreetLearn as a public resource for language grounding tasks in street view</title>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.splu-1.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Spatial Language Understanding</title>
		<meeting>the Third International Workshop on Spatial Language Understanding</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="56" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to navigate</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<idno type="DOI">10.1145/3347450.3357659</idno>
	</analytic>
	<monogr>
		<title level="m">1st International Workshop on Multimodal Understanding and Learning for Embodied Applications, MULEA &apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">Koichi</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.01292</idno>
		<title level="m">The streetlearn environment and dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Learning to navigate in cities without a map</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">Koichi</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><forename type="middle">Moritz</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems, NIPS&apos;18<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2424" to="2435" />
		</imprint>
	</monogr>
	<note>and Raia Hadsell</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mapping instructions to actions in 3d environments with visual goal prediction</title>
		<author>
			<persName><forename type="first">Dipendra</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valts</forename><surname>Blukis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyvind</forename><surname>Niklasson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Shatkhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2667" to="2678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Soat: A scene-and object-aware transformer for vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Moudgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harsh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Collaborative dialogue in Minecraft</title>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Narayan-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Jayannavar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Yonatan Bisk, and Hal Daum? III au2. 2021a. Learning when and what to ask: a hierarchical reinforcement learning framework</title>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Help, anna! visual navigation with natural multimodal assistance via retrospective curiosity-encouraging imitation learning</title>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1063</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="684" to="695" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Vision-based navigation with language-based assistance via imitation learning with indirect intervention</title>
		<author>
			<persName><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Van-Quang Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayuki</forename><surname>Suganuma</surname></persName>
		</author>
		<author>
			<persName><surname>Okatani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00596</idno>
		<title level="m">Look wide and interpret twice: Improving performance on interactive instruction-following tasks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Padmakumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjali</forename><surname>Narayan-Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Spandana</forename><surname>Gella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robinson</forename><surname>Piramithu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gokhan</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilek</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Teach: Task-driven embodied agents that chat</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Counterfactual vision-and-language navigation: Unravelling the unseen</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Parvaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Qinfeng Shi, and Anton van den Hengel</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Episodic transformer for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Pashevich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15942" to="15952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Run through the streets: A new dataset and baseline models for realistic urban navigation</title>
		<author>
			<persName><forename type="first">Tzuf</forename><surname>Paz-Argaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reut</forename><surname>Tsarfaty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08970</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Qi Wu. 2021. The road to know-where: An object-and-room informed sequential bert for indoor vision-language navigation</title>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="1655" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Reverie: Remote embodied visual referring expression in real indoor environments</title>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<editor>
			<persName><forename type="middle">Yuankai</forename><surname>Springer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Qi</forename><surname>Qi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Peter</forename><surname>Wu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xin</forename><surname>Anderson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chunhua</forename><surname>Wang</surname></persName>
		</editor>
		<editor>
			<persName><surname>Shen</surname></persName>
		</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-23">2020. August 23-28, 2020</date>
			<biblScope unit="page" from="303" to="317" />
		</imprint>
	</monogr>
	<note>Computer Vision-ECCV 2020: 16th European Conference</note>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">RMM: A recursive mental model for dialog navigation</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Homero Roman Roman</surname></persName>
		</author>
		<author>
			<persName><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of Empirical Methods in Natural Language Processing (EMNLP Findings)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks</title>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Shridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Winson</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<monogr>
		<author>
			<persName><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Whelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><forename type="middle">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shobhit</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Clarkson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingfei</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Budge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaqing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><surname>Yon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuyang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><surname>Briales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Gillingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Mueggler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Pesqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hauke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzo</forename><surname>Strasdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>De Nardi</surname></persName>
		</author>
		<author>
			<persName><surname>Goesele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05797</idno>
		<title level="m">The Replica dataset: A digital replica of indoor spaces</title>
		<meeting><address><addrLine>Steven Lovegrove, and Richard Newcombe</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Executing instructions in situated collaborative interactions</title>
		<author>
			<persName><forename type="first">Alane</forename><surname>Suhr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Schluger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Khader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marwa</forename><surname>Mouallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1218</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2119" to="2130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Depth-guided adain and shift attention network for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME51207.2021.9428422</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Undersander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Maestre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Mukadam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devendra</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Gokaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Vondrus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Dharur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franziska</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Galuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.14405</idno>
		<title level="m">Manolis Savva, and Dhruv Batra. 2021. Habitat 2.0: Training home assistants to rearrange their habitat</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning to navigate unseen environments: Back translation with environmental dropout</title>
		<author>
			<persName><forename type="first">Licheng</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1268</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2610" to="2621" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Self-supervised 3d semantic representation learning for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Sinan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10788</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Understanding natural language commands for robotic navigation and mobile manipulation</title>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashis</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Shifting the baseline: Single modality performance on visual navigation &amp; QA</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1197</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Minnesota. Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1977" to="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">2019b. Vision-and-dialog navigation</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Talk2nav: Long-range vision-andlanguage navigation with dual attention and spatial memory</title>
		<author>
			<persName><forename type="first">Arun</forename><surname>Balajee Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="246" to="266" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Learning to follow navigational directions</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th annual meeting of the association for computational linguistics</title>
		<meeting>the 48th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="806" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Structured scene memory for vision-language navigation</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8455" to="8464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Active visual information gathering for vision-language navigation</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianmin</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Soft expert reward learning for vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Hu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV&apos;20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Reinforced crossmodal matching and self-supervised imitation learning for vision-language navigation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVF/IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the CVF/IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>CVF/IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Look before you leap: Bridging model-free and model-based reinforcement learning for planned-ahead vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Environment-agnostic multitask learning for natural language grounded navigation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV&apos;20)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Embodied Question Answering in Photorealistic Environments with Point Cloud Perception</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Kadian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Building generalizable agents with a re</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Gibson env: Real-world perception for embodied agents</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyang</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<monogr>
		<title level="m" type="main">Multi-view learning for vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<monogr>
		<title level="m" type="main">Cross-lingual visionlanguage navigation</title>
		<author>
			<persName><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<monogr>
		<title level="m" type="main">Jianqing Fan, and Jiajie Peng. 2021. Curriculum learning for vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Language-guided navigation via crossmodal grounding and alternate adversarial learning</title>
		<author>
			<persName><forename type="first">Weixia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Diagnosing the environment bias in vision-and-language navigation</title>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/124</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="890" to="897" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization. Main track</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">On the evaluation of vision-and-language navigation instructions</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16</title>
		<meeting>the 16</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1302" to="1316" />
		</imprint>
	</monogr>
	<note>th Conference of the European Chapter</note>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Rethinking the spatial route prior in vision-andlanguage navigation</title>
		<author>
			<persName><forename type="first">Xinzhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">2021a. Soon: Scenario oriented object navigation with graph-based exploration</title>
		<author>
			<persName><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qizhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="page" from="12689" to="12699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Vision-language navigation with selfsupervised auxiliary reasoning tasks</title>
		<author>
			<persName><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Baby-Walk: Going farther in vision-and-language navigation by taking baby steps</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2539" to="2556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Wanrong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradyumna</forename><surname>Narayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazoo</forename><surname>Sone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugato</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>What really matters</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">2021c. Self-motivated communication agent for real-world vision-dialog navigation</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page" from="1594" to="1603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Vision-dialog navigation by exploring crossmodal memory</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohuan</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10730" to="10739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3357" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
