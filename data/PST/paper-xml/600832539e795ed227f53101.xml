<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2021 GRAPH TRAVERSAL WITH TENSOR FUNCTIONALS: A META-ALGORITHM FOR SCALABLE LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2021 GRAPH TRAVERSAL WITH TENSOR FUNCTIONALS: A META-ALGORITHM FOR SCALABLE LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Representation Learning (GRL) methods have impacted fields from chemistry to social science. However, their algorithmic implementations are specialized to specific use-cases e.g. message passing methods are run differently from node embedding ones. Despite their apparent differences, all these methods utilize the graph structure, and therefore, their learning can be approximated with stochastic graph traversals. We propose Graph Traversal via Tensor Functionals (GTTF), a unifying meta-algorithm framework for easing the implementation of diverse graph algorithms and enabling transparent and efficient scaling to large graphs. GTTF is founded upon a data structure (stored as a sparse tensor) and a stochastic graph traversal algorithm (described using tensor operations). The algorithm is a functional that accept two functions, and can be specialized to obtain a variety of GRL models and objectives, simply by changing those two functions. We show for a wide class of methods, our algorithm learns in an unbiased fashion and, in expectation, approximates the learning as if the specialized implementations were run directly. With these capabilities, we scale otherwise non-scalable methods to set state-of-the-art on large graph datasets while being more efficient than existing GRL libraries -with only a handful of lines of code for each method specialization. GTTF and its various GRL implementations are on: http://anonymous</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning (GRL) has become an invaluable approach for a variety of tasks, such as node classification (e.g., in biological and citation networks; <ref type="bibr" target="#b20">Veličković et al. (2018)</ref>; <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>; <ref type="bibr" target="#b11">Hamilton et al. (2017)</ref>; <ref type="bibr" target="#b24">Xu et al. (2018)</ref>), edge classification (e.g., link prediction for social and protein networks; <ref type="bibr" target="#b17">Perozzi et al. (2014)</ref>; <ref type="bibr" target="#b10">Grover &amp; Leskovec (2016)</ref>), entire graph classification (e.g., for chemistry and drug discovery <ref type="bibr" target="#b9">Gilmer et al. (2017)</ref>; <ref type="bibr" target="#b4">Chen et al. (2018a)</ref>), etc.</p><p>In this work, we propose an algorithmic unification of various GRL methods that allows us to re-implement existing GRL methods and introduce new ones, in merely a handful of code lines per method. Our algorithm (abbreviated GTTF, Section 3.2), receives graphs as input, traverses them using efficient tensor<ref type="foot" target="#foot_0">1</ref> operations, and invokes specializable functions during the traversal. We show function specializations for recovering popular GRL methods (Section 3.3). Moreover, since GTTF is stochastic, these specializations automatically scale to arbitrarily large graphs, without careful derivation per method. Importantly, such specializations, in expectation, recover unbiased gradient estimates of the objective w.r.t. model parameters.</p><p>GTTF uses a data structure A (Compact Adjacency, Section 3.1): a sparse encoding of the adjacency matrix. Node v contains its neighbors in row A[v] A v , notably, in the first degree(v) columns of <ref type="bibr">A[v]</ref>. This encoding allows stochastic graph traversals using standard tensor operations. GTTF is a functional, as it accepts functions ACCUMULATEFN and BIASFN, respectively, to be provided by each GRL specialization to accumulate necessary information for computing the objective, and optionally to parametrize sampling procedure p(v's neighbors | v). The traversal internally constructs a walk forest as part of the computation graph. Figure <ref type="figure" target="#fig_1">1</ref> depicts the data structure and the computation. From a generalization perspective, GTTF shares similarities with Dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref>.  Our contributions are: (i) A stochastic graph traversal algorithm (GTTF) based on tensor operations that inherits the benefits of vectorized computation and libraries such as PyTorch and Tensorflow. (ii) We list specialization functions, allowing GTTF to approximately recover the learning of a broad class of popular GRL methods. (iii) We prove that this learning is unbiased, with controllable variance, for this class of methods (iv) We show that GTTF can scale previously-unscalable GRL algorithms, setting the state-of-the-art on a range of datasets. Finally, <ref type="bibr">(v)</ref> we open-source GTTF along with new stochastic traversal versions of several algorithms, to aid practitioners from various fields in applying and designing state-of-the-art GRL methods for large graphs.</p><formula xml:id="formula_0">0 1 2 3 4 (a) Example graph G      1 1 1 1 1 1 1 1 1 1      (b) Adjacency matrix for graph G      1 0 2 3 4 1 1 4 1 3           1 4 1 2 2     <label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We take a broad standpoint in summarizing related work to motivate our contribution.  <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>, Graph Attention (GAT) <ref type="bibr" target="#b20">(Veličković et al., 2018)</ref>; as well as node embedding (NE) algorithms, including node2vec <ref type="bibr" target="#b10">(Grover &amp; Leskovec, 2016)</ref>, WYS <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018)</ref>; among many others <ref type="bibr" target="#b24">(Xu et al., 2018;</ref><ref type="bibr" target="#b23">Wu et al., 2019;</ref><ref type="bibr" target="#b17">Perozzi et al., 2014)</ref>. The full-batch GCN of <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>, which drew recent attention and has motivated many MP algorithms, was not initially scalable to large graphs, as it processes all graph nodes at every training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To scale MP methods to large graphs, researchers proposed Stochastic Sampling Methods that, at each training step, assemble a batch constituting subgraph(s) of the (large) input graph. Some of these sampling methods yield unbiased gradient estimates (with some variance) including SAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, FastGCN <ref type="bibr" target="#b5">(Chen et al., 2018b)</ref>, LADIES <ref type="bibr" target="#b27">(Zou et al., 2019)</ref>, and GraphSAINT <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref>. On the other hand, ClusterGCN <ref type="bibr" target="#b7">(Chiang et al., 2019)</ref> is a heuristic in the sense that, despite its good performance, it provides no guarantee of unbiased gradient estimates of the full-batch learning. <ref type="bibr" target="#b9">Gilmer et al. (2017)</ref> generalized many GRL models into a Message Passing framework. This framework arguably prompted bundling of GRL methods under Software Libraries, like PyG <ref type="bibr" target="#b8">(Fey &amp; Lenssen, 2019)</ref> and DGL <ref type="bibr" target="#b22">(Wang et al., 2019)</ref>, offering consistent interfaces on data formats.</p><p>We now position our contribution relative to the above. Unlike generalized MP <ref type="bibr" target="#b9">(Gilmer et al., 2017)</ref>, rather than abstracting the model computation, we abstract the learning algorithm. As a result, GTTF can be specialized to recover the learning of MP as well as NE methods. Morever, unlike Software Frameworks, which are re-implementations of many algorithms and therefore inherit the scale and learning of the copied algorithms, we re-write the algorithms themselves, giving them new properties (memory and computation complexity), while maintaining (in expectation) the original algorithm outcomes. Further, while the listed Stochastic Sampling Methods target MP algorithms (such as GCN, GAT, alike), as their initial construction could not scale to large graphs, our learning algorithm applies to a wider class of GRL methods, additionally encapsulating NE methods. Finally, while some NE methods such as node2vec <ref type="bibr" target="#b10">(Grover &amp; Leskovec, 2016)</ref> and DeepWalk <ref type="bibr" target="#b17">(Perozzi et al., 2014)</ref> are scalable in their original form, their scalability stems from their multi-step process: sample many (short) random walks, save them to desk, and then learn node embeddings using positional embedding methods (e.g., word2vec, <ref type="bibr" target="#b16">Mikolov et al. (2013)</ref>) -they are sub-optimal in the sense that their first step (walk sampling) takes considerable time (before training even starts) and also places an artificial limit on the number of training samples (number of simulated walks), whereas our algorithm conducts walks on-the-fly whilst training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH TRAVERSAL VIA TENSOR FUNCTIONALS (GTTF)</head><p>At its core, GTTF is a stochastic algorithm that recursively conducts graph traversals to build representations of the graph. We describe the data structure and traversal algorithm below, using the following notation. G = (V, E) is an unweighted graph with n = |V | nodes and m = |E| edges, described as a sparse adjacency matrix A ∈ {0, 1} n×n . Without loss of generality, let the nodes be zero-based numbered i.e. V = {0, . . . , n − 1}. We denote the out-degree vector δ ∈ Z n -it can be calculated by summing over rows of A as δ u = v∈V A <ref type="bibr">[u, v]</ref>. We assume δ u &gt; 0 for all u ∈ V : pre-processing can add self-connections to orphan nodes. B denotes a batch of nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DATA STRUCTURE</head><p>Internally, GTTF relies on a reformulation of the adjacency matrix, which we term CompactAdj (for "Compact Adjacency", Figure <ref type="figure" target="#fig_5">1c</ref>). It consists of two tensors:</p><p>1. δ ∈ Z n , a dense out-degree vector (figure <ref type="figure" target="#fig_5">1c</ref> </p><formula xml:id="formula_1">= |B|, then B = A[B, R • δ[B]</formula><p>] is a b-sized vector, with B u containing a neighbor of B u , and floor operation . is applied element-wise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">STOCHASTIC TRAVERSAL FUNCTIONAL ALGORITHM</head><p>Our traversal algorithm starts from a batch of nodes. It expands from each into a tree, resulting in a walk forest rooted at the nodes in the batch, as depicted in Figure <ref type="figure" target="#fig_5">1d</ref>. In particular, given a node batch B, the algorithm instantiates |B| seed walkers, placing one at every node in B. Iteratively, each walker first replicates itself a fanout (f ) number of times. Each replica then samples and transitions to a neighbor. This process repeats a depth (h) number of times. Therefore, each seed walker becomes the ancestor of a f -ary tree with height h. Setting f = 1 recovers traditional random walk. In practice, we provide flexibility by allowing a custom fanout value per depth.</p><p>Functional Traverse is listed in Algorithm 1. It accepts: a batch of nodes<ref type="foot" target="#foot_2">2</ref> ; a list of fanout values F (e.g. to F = [3, 5] samples 3 neighbors per u ∈ B, then 5 neighbors for each of those); and more notably, two functions: AC C U M U L A T EFN and BI A SFN. These functions will be called by the functional on every node along the traversal, and will be passed relevant information (e.g. the path taken from root seed node). Custom settings of these functions allow recovering wide classes of graph learning methods. At a high-level, our functional can be used in the following manner: </p><formula xml:id="formula_2">K ← Sample( A [u, :δ u ]] , sample_bias, f ) # Sample f nodes from u's neighbors for k ← 0 to f − 1 do T next ← concatenate(T, [u]) AC C U M U L A T EFN(T next , K[k], f ) Traverse(T next , K[k], f, AC C U M U L A T EFN, BI A SFN) # Recursion def Sample(N , W , f ): C ← tf.cumsum(W ) # Cumulative sum. Last entry must = 1. coin_flips ← tf.random.uniform((f, ), 0, 1) indices ← tf.searchsorted(C, coin_flips) return N [indices]</formula><p>2. Repeat (many rounds):</p><p>i. Reset accumulation information (from previous round) and then sample batch B ⊂ V . ii. Invoke Traverse on (B, AC C U M U L A T EFN, BI A SFN), which invokes the FN's, allowing the first to accumulate information sufficient for running the model and estimating an objective. iii. Use accumulated information to: run model, estimate objective, apply learning rule (e.g. SGD).</p><p>AC C U M U L A T EFN is a function that is used to track necessary information for computing the model and the objective function. For instance, an implementation of DeepWalk <ref type="bibr" target="#b17">(Perozzi et al., 2014)</ref> on top of GTTF, specializes AC C U M U L A T EFN to measure an estimate of the sampled softmax likelihood of nodes' positional distribution, modeled as a dot-prodct of node embeddings. On the other hand, GCN <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref> on top of GTTF uses it to accumulate a sampled adjacency matrix, which it passes to the underlying model (e.g. 2-layer GCN) as if this were the full adjacency matrix. BI A SFN is a function that customizes the sampling procedure for the stochastic transitions. If provided, it must yield a probability distribution over nodes, given the current node and the path that lead to it. If not provided, it defaults to U, transitioning to any neighbor with equal probability. It can be defined to read edge weights, if they denote importance, or more intricately, used to parameterize a second order Markov Chain <ref type="bibr" target="#b10">(Grover &amp; Leskovec, 2016)</ref>, or use neighborhood attention to guide sampling <ref type="bibr" target="#b20">(Veličković et al., 2018)</ref>, as discussed in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SOME SPECIALIZATIONS OF AC C U M U L A T EFN &amp; BI A SFN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">MESSAGE PASSING: GRAPH CONVOLUTIONAL VARIANTS</head><p>These methods, including <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b11">Hamilton et al., 2017;</ref><ref type="bibr" target="#b23">Wu et al., 2019;</ref><ref type="bibr" target="#b1">Abu-El-Haija et al., 2019;</ref><ref type="bibr" target="#b24">Xu et al., 2018)</ref> can be approximated by by initializing A to an empty sparse n × n matrix, then invoking Traverse (Algorithm 1) with u = B; F to list of fanouts with size h; Thus AC C U M U L A T EFN and BI A SFN become:</p><formula xml:id="formula_3">def RO O T E DAD JAC C(T , u, f ): A[u, T −1 ] ← 1;</formula><p>(1)</p><formula xml:id="formula_4">def NORE V I S I TBI A S(T , u): return 1[ A[u].sum() = 0] 1 δu δ u ; (2)</formula><p>where 1 n is an n-dimensional all-ones vector, and negative indexing T −k is the k th last entry of T . If a node has been visited through the stochastic traversal, then it already has fanout number of neighbors and NORE V I S I TBI A S ensures it does not get revisited for efficiency, per line 5 of Algorithm 1. Afterwards, the accumulated stochastic A will be fed<ref type="foot" target="#foot_3">3</ref> into the underlying model e.g. for a 2-layer GCN of <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>:</p><formula xml:id="formula_5">GCN( A,X; W 1 , W 2 ) = softmax( • A ReLu( • AXW 1 )W 2 );<label>(3)</label></formula><p>with</p><formula xml:id="formula_6">• A = D 1/2 D −1 A D −1/2 ; D = diag(δ ); δ = 1 n A ; renorm trick A = I n×n + A</formula><p>Lastly, h should be set to the receptive field required by the model for obtaining output d L -dimensional features at the labeled node batch. In particular, to the number of GC layers multiplied by the number of hops each layers access. E.g. hops=1 for GCN but customizable for MixHop and SimpleGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">NODE EMBEDDINGS</head><p>Given a batch of nodes B ⊆ V , DeepWalk<ref type="foot" target="#foot_4">4</ref> can be implemented in GTTF by first initializing loss L to the contrastive term estimating the partition function of log-softmax:</p><formula xml:id="formula_7">L ← u∈B log E v∼Pn(V ) [exp( Z u , Z v )] ,<label>(4)</label></formula><p>where ., . is dot-product notation, Z ∈ R n×d is the trainable embedding matrix with</p><formula xml:id="formula_8">Z i ∈ R d is d-dimensional embedding for node u ∈ V .</formula><p>In our experiments, we estimate the expectation by taking 5 samples and we set the negative distribution</p><formula xml:id="formula_9">P n (V = v) ∝ δ 3 4 v , following Mikolov et al. (2013). The functional is invoked with no BI A SFN and AC C U M U L A T EFN = def DE E PWA L KAC C(T , u, f ): L ← L − Z i , C T k=1 η [T −k ] C−k+1 C Z T −k ; η [u] ← η [T −1 ] f ;<label>(5)</label></formula><p>where hyperparameter C indicates maximum window size (inherited from word2vec, <ref type="bibr" target="#b16">Mikolov et al., 2013)</ref>, in the summation on k does not access invalid entries of T as C T min(C, T .size), the scalar fraction C−k+1 C is inherited from context sampling of word2vec (Section 3.1 in <ref type="bibr" target="#b15">Levy et al., 2015)</ref>, and rederived for graph context by Abu-El-Haija et al. ( <ref type="formula">2018</ref>), and η [u] stores a scalar per node on the traversal Walk Forest, which defaults to 1 for non-initialized entries, and is used as a correction term. DeepWalk conducts random walks (visualized as a straight line) whereas our walk tree has a branching factor of f . Setting fanout f = 1 recovers DeepWalk's simulation, though we found f &gt; 1 outperforms within fewer iterations e.g. f = 5, within 1 epoch, outperforms DeepWalk's published implementation. Learning can be performed using the accumulated L as: Z ← Z − ∇ Z L;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THEORETICAL ANALYSIS</head><p>Due to space limitations, we include the full proofs of all propositions in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ESTIMATING k TH POWER OF TRANSITION MATRIX</head><p>We show that it is possible with GTTF to accumulate an estimate of transition T matrix to power k.</p><p>Let Ω denote the walk forest generated by GTTF, Ω(u, k, i) as the i th node in the vector of nodes at depth k of the walk tree rooted at u ∈ B, and</p><formula xml:id="formula_10">t u,v,k i as the indicator random variable 1[Ω(u, k, i) = v].</formula><p>Let the estimate of the k th power of the transition matrix be denoted T k . Entry T k u,v should be an unbiased estimate of T k u,v for u ∈ B, with controllable variance. We define:</p><formula xml:id="formula_11">T k u,v = f k i=1 t u,v,k i f k (6)</formula><p>The fraction in Equation 6 counts the number of times the walker starting at u visits v in Ω, divided by the total number of nodes visited at the k th step from u.</p><p>Proposition 1. (UNBIASEDTK) T k u,v as defined in Equation <ref type="formula">6</ref>, is an unbiased estimator of</p><formula xml:id="formula_12">T k u,v Proposition 2. (VARIANCETK) Variance of our estimate is upper-bounded: Var[ T k u,v ] ≤ 1 4f k</formula><p>Naive computation of k th powers of the transition matrix can be efficiently computed via repeated sparse matrix-vector multiplication. Specifically, each column of T k can be computed in O(mk), where m is the number of edges in the graph. Thus, computing T k in its entirety can be accomplished in O(nmk). However, this can still become prohibitively expensive if the graph grows beyond a certain size. GTTF on the other hand can estimate T k in time complexity independent of the size of the graph. (Prop. 7), with low variance. Transition matrix powers are useful for many GRL methods. <ref type="bibr" target="#b18">(Qiu et al., 2018)</ref> 4.2 UNBIASED LEARNING As a consequence of Propositions 1 and 2, GTTF enables unbiased learning with variance control for classes of node embedding methods, and provides a convergence guarantee for graph convolution models under certain simplifying assumptions.</p><formula xml:id="formula_13">Proposition 3. (UNBIASEDLEARNNE) Learning node embeddings Z ∈ R n×d with objective function L, decomposable as L(Z) = u∈V L 1 (Z, u) − u,v∈V k L 2 (T k , u, v)L 3 (Z, u, v)</formula><p>, where L 2 is linear over T k , then using T k yields an unbiased estimate of ∇ Z L.</p><p>Generally, L 1 (and L 3 ) score the similarity between disconnected (and connected) nodes u and v. The above form of L covers a family of contrastive learning objectives that use cross-entropy loss and assume a logistic or (sampled-)softmax distributions. We provide, in the Appendix, the decompositions for the objectives of DeepWalk <ref type="bibr" target="#b17">(Perozzi et al., 2014)</ref>, node2vec <ref type="bibr" target="#b10">(Grover &amp; Leskovec, 2016)</ref> and WYS <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018)</ref>.</p><p>Proposition 4. (UNBIASEDMP) Given input activations, H (l−1) , graph conv layer (l) can use rooted adjacency A accumulated by RO O T E DAD JAC C (1), to provide unbiased pre-activation output, i.e. l) , with A and D defined in (3).</p><formula xml:id="formula_14">E • A k H (l−1) W (l) = D −1/2 A D −1/2 k H (l−1) W (</formula><p>Proposition 5. (UNBIASEDLEARNMP) If objective to a graph convolution model is convex and Lipschitz continous, with minimizer θ * , then utilizing GTTF for graph convolution converges to θ * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">COMPLEXITY ANALYSIS</head><formula xml:id="formula_15">Proposition 6. STORAGE complexity of GTTF is O(m + n).</formula><p>Proposition 7. TIME complexity of GTTF is O(bf h ) for batch size b, fanout f , and depth h.</p><p>Proposition 7 implies the speed of computation is irrespective of graph size. Methods implemented in GTTF inherit this advantage. For instance, the node embedding algorithm WYS (Abu-El-Haija et al., 2018) is O(n 3 ), however, we apply its GTTF implementation on large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We conduct experiments on 10 different graph datasets, listed in in Table <ref type="table" target="#tab_1">1</ref>. We experimentally demonstrate the following. (1) Re-implementing baseline method using GTTF maintains performance.</p><p>(2) Previously-unscalable methods, can be made scalable when implemented in GTTF. (3) GTTF achieves good empirical performance when compared to other sampling-based approaches handdesigned for Message Passing. (4) GTTF consumes less memory and trains faster than other popular Software Frameworks for GRL. To replicate our experimental results, for each cell of the table in our code repository, we provide one shell script to produce the metric, except when we indicate that the metric is copied from another paper. Unless otherwise stated, we used fanout factor of 3 for GTTF implementations. Learning rates and model hyperparameters are included in the Appendix.  <ref type="bibr" target="#b0">-El-Haija et al., 2018)</ref>, (b) = to be released, (c) = <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, (d) = <ref type="bibr" target="#b25">(Yang et al., 2016)</ref>; (e) = <ref type="bibr" target="#b12">(Hu et al., 2020)</ref> In link prediction tasks, a graph is partially obstructed by hiding a portion of its edges. The task is to recover the hidden edges. We follow a popular approach to tackle this task: first learn node embedding Z ∈ R n×d from the observed graph, then predict the link between nodes u and v with score ∝ Z u Z v . We use two ranking metrics for evaluations: ROC-AUC, which is a ranking objective: how well do methods rank the hidden edges above randomly-sampled negative edges and Mean Rank.</p><p>We re-implement Node Embedding methods, DeepWalk <ref type="bibr" target="#b17">(Perozzi et al., 2014)</ref> and WYS (Abu-El-Haija et al., 2018), into GTTF (abbreviated F). Table <ref type="table" target="#tab_3">2</ref> summarizes link prediction test performance.</p><p>LiveJournal and Reddit are large datasets, where original implementation of WYS is unable to scale to. However, scalable F(WYS) sets new state-of-the-art on these datasets. For PPI and HepTh datasets, we copy accuracy numbers for DeepWalk and WYS from <ref type="bibr" target="#b0">(Abu-El-Haija et al., 2018)</ref>. For LiveJournal, we copy accuracy numbers for DeepWalk and PBG from <ref type="bibr" target="#b14">(Lerer et al., 2019)</ref> -note that a well-engineered approach (PBG, <ref type="bibr" target="#b14">(Lerer et al., 2019)</ref>), using a mapreduce-like framework, is under-performing compared to F(WYS), which is a few lines specialization of GTTF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">MESSAGE PASSING FOR NODE CLASSIFICATION</head><p>We implement in GTTF the message passing models: GCN <ref type="bibr" target="#b13">(Kipf &amp; Welling, 2017)</ref>, GraphSAGE <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, MixHop (Abu-El-Haija et al., 2019), SimpleGCN <ref type="bibr" target="#b23">(Wu et al., 2019)</ref>, as their computation is straight-forward. For GAT <ref type="bibr" target="#b20">(Veličković et al., 2018)</ref> and GCNII <ref type="bibr" target="#b6">(Chen et al., 2020)</ref>, as they are more intricate, we download the authors' codes, and wrap them as-is with our functional.</p><p>We show that we are able to run these models in Table <ref type="table" target="#tab_4">3</ref> (left and middle), and that GTTF implementations matches the baselines performance. For the left table, we copy numbers from the published papers. However, we update GAT to work with TensorFlow 2.0 and we use our updated code (GAT*).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">EXPERIMENTS COMPARING AGAINST SAMPLING METHODS FOR MESSAGE PASSING</head><p>We now compare models trained with GTTF (where samples are walk forests) against sampling methods that are especially designed for Message Passing algorithms (GraphSAINT and ClusterGCN), especially since their sampling strategies do not match ours.</p><p>Table <ref type="table" target="#tab_4">3</ref> (right) shows test performance on node classification accuracy on a large dataset: Products. We calculate the accuracy for F(SAGE), but copy from <ref type="bibr" target="#b12">(Hu et al., 2020)</ref> the accuracy for the baselines: GraphSAINT <ref type="bibr" target="#b26">(Zeng et al., 2020)</ref> and ClusterGCN <ref type="bibr" target="#b7">(Chiang et al., 2019)</ref> (both are message passing methods); and also node2vec <ref type="bibr" target="#b10">(Grover &amp; Leskovec, 2016</ref>) (node embedding method).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">RUNTIME AND MEMORY COMPARISON AGAINST OPTIMIZED SOFTWARE FRAMEWORKS</head><p>In addition to the accuracy metrics discussed above, we also care about computational performance. We compare against software frameworks DGL ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2019</head><p>). These software frameworks offer implementations of many methods. Table <ref type="table" target="#tab_5">4</ref> summarizes the following. First (left), we show time-per-epoch on large graphs of their implementation of GraphSAGE, compared with GTTF's, where we make all hyper parameters to be the same (of model architecture, and number of neighbors at message passing layers). Second (middle), we run their GCN implementation on small datasets (Cora, Citeseer, Pubmed) to show peak memory usage. The run times between GTTF, PyG and DGL are similar for these datasets. The comparison can be found in the Appendix. While the aforementioned two comparisons are on popular message passing methods, the third (right) chartshows a popular node embedding method: node2vec's link prediction test ROC-AUC in relation to its training runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We present a new algorithm, Graph Traversal via Tensor Functionals (GTTF) that can be specialized to re-implement the algorithms of various Graph Representation Learning methods. The specialization takes little effort per method, making it straight-forward to port existing methods or introduce new ones. Methods implemented in GTTF run efficiently as GTTF uses tensor operations to traverse graphs. In addition, the traversal is stochastic and therefore automatically makes the implementations scalable to large graphs. We theoretically show that the learning outcome due to the stochastic traversal is in expectation equivalent to the baseline when the graph is observed at-once, for popular GRL methods we analyze. Our thorough experimental evaluation confirms that methods implemented in GTTF maintain their empirical performance, and can be trained faster and using less memory even compared to software frameworks that have been thoroughly optimized.</p><p>f = [15, 10], a fixed learning learning rate of 0.001 and a batch size of 1024, a hidden dimension of 256 and a fixed learning rate of 0.003.</p><p>For GAT (baseline), we follow the authors code and hyperparameters: for Cora and Citeseer, we use Adam with learning rate of 0.005, L2 regularization of 0.0005, 8 attention heads on the first layer and 1 attention head on the output layer. For Pubmed, we use Adam with learning rate of 0.01, L2 regularization of 0.01, 8 attention heads on the first layer and 8 attention heads on the output layer. For F(GAT), we use the same aforementioned hyperparameters, a fanout of 3 and traversal depth of 2 (to cover two layers) i.e. F = <ref type="bibr">[3,</ref><ref type="bibr">3]</ref>. For F(GCN), we use the authors' recommended hyperparameters. Learning rate of 0.005, 0.001 L2 regularization, and F = [3, 3], for all datasets. For both methods, we apply "patience" and stop the training if validation loss does not improve for 100 consecutive epochs, reporting the test accuracy at the best validation loss. For F(MixHop), we wrap the authors' script and use their hyperparameters. For F(GCNII), we use F = <ref type="bibr">[5,</ref><ref type="bibr">5,</ref><ref type="bibr">5,</ref><ref type="bibr">5,</ref><ref type="bibr">5,</ref><ref type="bibr">5]</ref>, as their models are deep (64 layers for Cora). Otherwise, we inherit their network hyperparameters (latent dimensions, number of layers, dropout factor, and their introduced coefficients), as they have tuned them per dataset, but we change the learning rate to 0.005 (half of what they use) and we extend the patience from 100 to 1000, and extend the maximum number of epochs from 1500 to 5000 -this is because we are presenting a subgraph at each epoch, and therefore we intuitively want to slow down the learning per epoch, which is similar to the practice when someone applies Dropout to a neural networks. We re-run their shell scripts, with their code modified to use the Rooted Adjacency rather than the real adjacency, which is sampled at every epoch.</p><p>MLP was trained with 1 layer and a learning rate of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROOFS</head><formula xml:id="formula_16">B.1 PROOF OF PROPOSITION 1 Proof. E[ T k u,v ] = E f k i=1 t u,v,k i f k = f k i=1 E[t u,v,k i ] f k = f k i=1 P [t u,v,k i = 1] f k = f k i=1 T k u,v f k = T k u,v B.2 PROOF OF PROPOSITION 2 Proof. Var[ T k u,v ] = f k i=1 V ar[t u,v,k i ] f 2k = f k T k u,v (1 − T k u,v ) f 2k = T k u,v (1 − T k u,v ) f k Since 0 ≤ T k u,v ≤ 1, then T k u,v (1 − T k u,v ) is maximized with T k u,v = 1 2 . Hence V ar[ T k u,v ] ≤ 1 4f k B.3 PROOF OF PROPOSITION 3 Proof. We want to show that E[∇ Z L( T k , Z)] = ∇ Z L(T k , Z).</formula><p>Since the terms of L 1 are unaffected by T , they are excluded w.l.g. from L in the proof.</p><formula xml:id="formula_17">E[∇ Z L( T k , Z)] = E ∇ Z (− u,v∈V k∈{1..C} L 2 ( T k , u, v)L 3 (Z, u, v)) (by linearity of expectation) = −∇ Z u,v∈V k∈{1..C} L 2 (E[ T k ], u, v)L 3 (Z, u, v) (by Prop 1) = −∇ Z u,v∈V k∈{1..C} L 2 (T k , u, v)L 3 (Z, u, v) = ∇ Z L(T k , Z)</formula><p>The following table gives the decomposition for DeepWalk, node2vec, and Watch Your Step. Node2vec also introduces a biased sampling procedure based on hyperparameters (they name p and q) instead of uniform transition probabilities. We can equivalently bias the transitions in GTTF to match node2vec's. This would then show up as a change in T k in the objective. This effect can also be included in the objective by multiplying Z u , Z v by the probability of such a transition in L 3 . In this format, the p and q variables appear in the objective and can be included in the optimization. For WYS, Q k are also trainable parameters.</p><formula xml:id="formula_18">Method L 1 L 2 L 3 DeepWalk 0 C − k + 1 C T k log( Z u , Z v ) Node2Vec log v∈V exp( Z u , Z v ) C − k + 1 C T k Z u , Z v Watch Your Step log(1 − σ( L u , R v )) Q k T k log(σ( L u , R v ))</formula><p>Table <ref type="table">5</ref>: Decomposition of graph embedding methods to demonstrate unbiased learning. For WYS,</p><formula xml:id="formula_19">Z u = concatenate(L u , R u ).</formula><p>For methods in which the transition distribution is not uniform, such as node2vec, there are two options for incorporating this distribution in the loss. The obvious choice is to sample from a biased transition matrix, T u,v = W u,v , where W is the transition weights. Alternatively, the transition bias can be used as a weight on the objective itself. This approach is still unbiased as</p><formula xml:id="formula_20">E v∼ Wu [L(v, u)] = v∈V P v∼ Wu [v]L(v, u) = v∈V W u,v L(v, u) B.4 PROOF OF PROPOSITION 4</formula><p>Proof. Let A be the neighborhood patch returned by GTTF, and let . indicate a measurement based on the sampled graph, A, such as the degree vector, δ, or diagonal degree matrix, D. For the remainder of this proof, let all notation for adjacency matrices, A or A, and diagonal degree matrices, D or D, and degree vector, δ, refer to the corresponding measure on the graph with self loops e.g. A ← A + I n×n . We now show that the expectation of the layer output is unbiased.</p><formula xml:id="formula_21">E • A k H (l−1) W (l) = E • A k H (l−1) W (l) implies that E • A k H (l−1) W (l) is unbiased if E • A k = D −1/2 AD −1/2 k . E • A k = E D 1/2 D −1 A k D −1/2 = D 1/2 E D −1 A k D −1/2</formula><p>Let P u,v,k be the set of all walks {p = (u, v 1 , ..., v k−1 , v)|v i ∈ V }, and let p∃ A indicate that the path p exists in the graph given by A. Let t u,v,k be the transition probability from u to v in k steps, and let t p be the probability of a random walker traversing the graph along path p.</p><formula xml:id="formula_22">E D −1 A k u,v = E T k u,v = P r t u,v,k = 1 = p∈P u,v,k P r[p∃ A]P r[ t p = 1|p∃ A] = p∈P u,v,k k i=1 1[A[p i , p i+1 ] = 1] f + 1 δ[p i ] k i=1 (f + 1) −1 = p∈P u,v,k k i=1 1[A[p i , p i+1 ] = 1]δ[p i ] −1 = p∈P u,v,k P r[t p = 1] = P r[t u,v,k ] = (T ) k u,v = D −1 A k u,v</formula><p>Thus,</p><formula xml:id="formula_23">E • A k = D −1/2 AD −1/2 k and E • A k H (l−1) W (l) = D −1/2 AD −1/2 k H (l−1) W (l)</formula><p>For writing, we assumed nodes have degree, δ u ≥ f , though the proof still holds if that is not the case as the probability of an outgoing edge being present from u becomes 1 and the transition probability becomes δ −1 u i.e. the same as no estimate at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 PROOF OF PROPOSITION 5</head><p>GTTF can be seen as a way of applying dropout <ref type="bibr" target="#b19">(Srivastava et al., 2014)</ref>, and our proof is contingent on the convergence of dropout, which is shown in <ref type="bibr" target="#b2">Baldi &amp; Sadowski (2014)</ref>. Our dropout is on the adjacency, rather than the features. Denote the output of a graph convolution network<ref type="foot" target="#foot_5">5</ref> with H:</p><formula xml:id="formula_24">H = GCN X (A; W ) = T XW</formula><p>We restrict our analysis to GCNs with linear activations. We are interested in quantifying the change of H as A changes, and therefore the fixed (always visible) features X is placed on the subscript. Let A denote adjacency accumulated by GTTF's RO O T E DAD JAC C (Eq. 1).</p><formula xml:id="formula_25">H c = GCN X ( A c ). Let A = { A c } |A| c=1</formula><p>denote the (countable) set of all adjacency matrices realizable by GTTF. For the analysis, assume the graph is α-regular: the assumption eases the notation though it is not needed. Therefore, degree δ u = α for all u ∈ V . Our analysis depends<ref type="foot" target="#foot_6">6</ref> on 1 |A| A∈A A ∝ A. i.e. the average realizable matrix by GTTF is proportional (entry-wise) to the full adjacency. This is can be shown when considering one-row at a time: given node u with δ u = α outgoing neighbors, each of its neighbors has the same appearance probability = 1 δu . Summing over all combinations δu f , makes each edge appear the same frequency = 1 δu |A|, noting that |A| evenly divides δu f for all u ∈ V . We define a dropout module:</p><formula xml:id="formula_26">d A = |A| c z c A c with z ∼ Categorical |A| of them 1 |A| , 1 |A| , . . . , 1 |A| ,<label>(7)</label></formula><p>where z c acts as Multinoulli selector over the elements of A, with one of its entries set to 1 and all others to zero. With this definitions, GCNs can be seen in the droupout framework as:</p><formula xml:id="formula_27">H = GCN X ( d A)</formula><p>. Nonetheless, in order to inherit the analysis of <ref type="bibr" target="#b2">(Baldi &amp; Sadowski, 2014</ref>, see their equations 140 &amp; 141), we need to satisfy two conditions which their analysis is founded upon: Condition (i) is satisfied due to proof of Proposition 4. To analyze the error signal, i.e. the gradient of the error w.r.t. the network, assume loss function L(H), outputs scalar loss, is λ-Lipschitz continuous.</p><p>The Liptchitz continuity allows us to bound the difference in error signal between L(H) and L( H):</p><formula xml:id="formula_28">||∇ H L(H) − ∇ H L( H)|| 2 2 (a) ≤λ (∇ H L(H) − ∇ H L( H)) (H − H) (8) (b) ≤λ ||∇ H L(H) − ∇ H L( H)|| 2 ||H − H|| 2 (9) w.p. ≥1− 1 Q 2 ≤ λ ||∇ H L(H) − ∇ H L( H)|| 2 W X Q V ar[T ]XW (10) = λQ 2 √ f ||∇ H L(H) − ∇ H L( H)|| 2 ||W || 2 1 ||X|| 2 1 (11) ||∇ H L(H) − ∇ H L( H)|| 2 ≤ λQ 2 √ f ||W || 2 1 ||X|| 2 1 (12)</formula><p>where (a) is by Lipschitz continuity, (b) is by Cauchy-Schwarz inequality, "w.p." means with probability and uses Chebyshev's inequality, with the following equality because the variance of T is shown element-wise in proof for Prop. 2. Finally, we get the last line by dividing both sides over the common term. This shows that one can make the error signal for the different realizations arbitrarily small, for example, by choosing a larger fanout value or putting (convex) norm constraints on W and X e.g. through batchnorm and/or weightnorm. Since we can have</p><formula xml:id="formula_29">∇ H L(H) ≈ ∇ H L( H 1 ) ≈ ∇ H L( H 2 ) ≈ • • • ≈ ∇ H L( H |A| )</formula><p>with high probability, then the analysis of <ref type="bibr" target="#b2">Baldi &amp; Sadowski (2014)</ref> applies. Effectively, it can be thought of as an online learning algorithm where the elements of A are the stochastic training examples and analyzed per <ref type="bibr">(Bottou, 1998;</ref><ref type="bibr" target="#b3">2004)</ref>, as explained by <ref type="bibr" target="#b2">Baldi &amp; Sadowski (2014)</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 PROOF OF PROPOSITION 6</head><p>The storage complexity of</p><formula xml:id="formula_30">CompactAdj is O(sizeof (δ) + sizeof ( A)) = O(n + m).</formula><p>Moreover, for extemely large graphs, the adjacncy can be row-wise partitioned across multiple machines and therefore admitting linear scaling. However, we acknolwedge that choosing which rows to partition to which machines can drastically affect the performance. Balanced partitioning is ideal. It is an NP-hard problem, but many approximations have been proposed. Nonetheless, reducing inter-communication, when distributing the data structure across machines, is outside our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 PROOF OF PROPOSITION 7</head><p>For each step of GTTF, the computational complexity is O(bh f ). This follows trivially from the GTTF functional: each nodes in batch (b of them) builds a tree with depth h and fanout f i.e. with h f tree nodes. This calculation assumes random number generation, AC C U M U L A T EFN and BI A SFN take constant time. The searchsorted function is linear, as it is called on a sorted list: cumulative sum of probabilities. One can implement GAT by following the previous subsection, utilizing AC C U M U L A T EFN and BI A SFN defined in (1) and ( <ref type="formula">2</ref>), but just replacing the model (3) by GAT's:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL GTTF IMPLEMENTATIONS</head><formula xml:id="formula_31">GAT( A, X; A, W 1 , W 2 ) = softmax((A • • A) ReLu((A • • A)XW 1 )W 2 );<label>(13)</label></formula><p>where • is hadamard product and A is an n × n matrix placing a positive scalar (an attention value) on each edge, parametrized by multi-headed attention described in <ref type="bibr" target="#b20">(Veličković et al., 2018)</ref>. However, for some high-degree nodes that put most of the attention weight on a small subset of their neighbors, sampling uniformly (with BI A SFN=NORE V I S I TBI A S) might mostly sample neighbors with entries in A with value ≈ 0, and could require more epochs for convergence. However, our flexible functional allows us to propose a sample-efficient alternative, that is in expectation, equivalent to the above: </p><formula xml:id="formula_32">GAT( A, X; A, W 1 , W 2 ) = softmax(( √ A • • A) ReLu(( √ A • • A)XW 1 )W 2 ); (14) def GA TBI A S(T , u): return NORE V I S I TBI A S(T , u) • A[u, A[u]];<label>(15)</label></formula><formula xml:id="formula_33">= def N2VBI A S(T , u): return p −1[i=T−2] q −1[ A[T−2],A[u] &gt;0] ;<label>(16)</label></formula><p>where 1 denotes indicator function, p, q &gt; 0 are hyperparameters of node2vec assigning (unnormalized) probabilities for transitioning back to the previous node or to node connected to it. A[T −2 ], A[u] counts mutual neighbors between considered node u and previous T −2 .</p><p>An alternative implementation is to not override BI A SFN but rather fold it into AC C U M U L A T EFN, as:</p><formula xml:id="formula_34">def N2VAC C(T , u, f ): DE E PWA L KAC C(T , u, f ); η [u] ← η [u] × N2VBI A S(T , u);<label>(17</label></formula><p>) Both alternatives are equivalent in expectation. However, the latter directly exposes the parameters p and q to the objective L: allowing them to be differentiable w.r.t. L and therefore trainable via gradient descent, rather than by grid-search. Nonetheless, parameterizing p &amp; q is beyond our scope. C.2.2 WATCH YOUR STEP (WYS, ABU-EL-HAIJA ET AL., 2018) First, embedding dictionaries R, L ∈ R n× d 2 can be initialized to random. Then repeatedly over batches B ⊆ V , the loss L can be initialized to estimate the negative part of the objective:    <ref type="bibr" target="#b13">Kipf &amp; Welling (2017)</ref>, and for completeness, we report in Figure <ref type="figure" target="#fig_9">4</ref> the runtime versus test accuracy of GCN on these networks. We compare against PyG, which is optimized for GCN. We find that the methods are somewhat comparable in terms of training time. </p><formula xml:id="formula_35">L ← − u∈B log σ(−E v∈U (V ) [ R u , L v + R v , L u ]),</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>c) CompactAdj for G with sparse A ∈ Z n×n and dense δ ∈ Z n . We store IDs of adjacent nodes in A Walk Forest. GTTF invokes AC C U M U L A T EFN once per (green) instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (c)&amp;(d) Depict our data structure &amp; traversal algorithm on a toy graph in (a)&amp;(b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, right) 2. A ∈ Z n×n , a sparse edge-list matrix in which the row u contains left-aligned δ u non-zero values. The consecutive entries { A[u, 0], A[u, 1], . . . , A[u, δ u − 1]} contain IDs of nodes receiving an edge from node u. The remaining |V | − δ u are left unset, therefore, A only occupies O(m) memory when stored as a sparse matrix (Figure 1c, left). CompactAdj allows us to concisely describe stochastic traversals using standard tensor operations. To uniformly sample a neighbor to node u ∈ V , one can draw r ∼ U[0..(δ u − 1)], then get the neighbor ID with A[u, r]. In vectorized form, given node batch B and access to continuous U[0, 1), we sample neighbors for each node in B as: R ∼ U[0, 1) b , where b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Stochastic Traverse Functional, parametrized by AC C U M U L A T EFN and BI A SFN. input: u (current node); T ← [] (path leading to u, starts empty); F (list of fanouts); AC C U M U L A T EFN (function: with side-effects and no return. It is model-specific and records information for computing model and/or objective, see text); BI A SFN ← U (function mapping u to distribution on u's neighbors, defaults to uniform) def Traverse(T , u, F , AC C U M U L A T EFN, BI A SFN): if F .size() = 0 then return # Base case. Traversed up-to requested depth f ← F .pop() # fanout duplication factor (i.e. breadth) at this depth. sample_bias ← BI A SFN(T , u) if sample_bias.sum() = 0 then return # Special case. No sampling from zero mass sample_bias ← sample_bias / sample_bias.sum() # valid distribution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(i) E[GCN X ( d A)] = GCN X (A): in the usual (feature-wise) dropout, such condition is easily verified. (ii) Backpropagated error signal does not vary too much around around the mean, across all realizations of d A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>C. 1</head><label>1</label><figDesc>MESSAGE PASSING IMPLEMENTATIONS C.1.1 GRAPH ATTENTION NETWORKS (GAT, VELI ČKOVI Ć ET AL., 2018)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Then call GTTF's traverse passing the followingAC C U M U L A T EFN= def WY SAC C(T , u): if T.size() = Q.size(): return; t ← T [0]; U ← T [1 :] ∪ [u]; ctx_weighted_L ← j Q j L Uj ; ctx_weighted_R ← j Q j R Uj ; L ← L − log(σ( R t , ctx_weighted_L + L t , ctx_weighted_R ));show the sensitivity of fanout and walk depth for WYS on the Reddit dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Test AUC score when changing the fanout (left) and random walk length (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Runtime of GTTF versus PyG for training GCN on citation network datasets. Each line averages 10 runs on an Nvidia GTX 1080Ti GPU.D.3 CODE SNAPSHOTS FOR IMPLEMENTING VARIOUS MODELS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Dataset summary. Tasks are LP, SSC, FSC, for link prediction, semi-and fully-supervised classification. Split indicates the train/validate/test paritioning, with (a) = (Abu</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.</figDesc><table><row><cell cols="5">Dataset Split # Nodes # Edges # Classes</cell><cell>Nodes</cell><cell>Edges</cell><cell>Tasks</cell></row><row><cell>PPI</cell><cell>(a)</cell><cell cols="2">3,852 20,881</cell><cell>N/A</cell><cell>proteins</cell><cell>interaction</cell><cell>LP</cell></row><row><cell cols="2">ca-HepTh (a)</cell><cell cols="2">80,638 24,827</cell><cell>N/A</cell><cell cols="2">researchers co-authorship</cell><cell>LP</cell></row><row><cell cols="2">ca-AstroPh (a)</cell><cell cols="2">17,903 197,031</cell><cell>N/A</cell><cell cols="2">researchers co-authorship</cell><cell>LP</cell></row><row><cell cols="2">LiveJournal (b)</cell><cell cols="2">4.85M 68.99M</cell><cell>N/A</cell><cell>users</cell><cell>friendship</cell><cell>LP</cell></row><row><cell>Reddit</cell><cell cols="3">(c) 233,965 11.60M</cell><cell>41</cell><cell>posts</cell><cell cols="2">user co-comment LP/FSC</cell></row><row><cell>Amazon</cell><cell>(b)</cell><cell cols="2">2.6M 48.33M</cell><cell>31</cell><cell>products</cell><cell>co-purchased</cell><cell>FSC</cell></row><row><cell>Cora</cell><cell>(d)</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>articles</cell><cell>citation</cell><cell>SSC</cell></row><row><cell>CiteSeer</cell><cell>(d)</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>articles</cell><cell>citation</cell><cell>SSC</cell></row><row><cell>PubMed</cell><cell>(d)</cell><cell cols="2">19,717 44,338</cell><cell>3</cell><cell>articles</cell><cell>citation</cell><cell>SSC</cell></row><row><cell>Products</cell><cell>(e)</cell><cell cols="2">2.45M 61.86M</cell><cell>47</cell><cell>products</cell><cell>co-purchased</cell><cell>SSC</cell></row><row><cell cols="5">5.1 NODE EMBEDDINGS FOR LINK PREDICTION</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results of node embeddings on Link Prediction. Left: Test ROC-AUC scores. Right: Mean Rank on the right for consistency with<ref type="bibr" target="#b14">Lerer et al. (2019)</ref>. *OOM = Out of Memory.</figDesc><table><row><cell cols="2">PPI HepTh Reddit</cell><cell></cell><cell>LiveJournal</cell></row><row><cell>DeepWalk 70.6 91.8</cell><cell>93.5</cell><cell>DeepWalk</cell><cell>234.6</cell></row><row><cell>F(DeepWalk) 87.9 89.9</cell><cell>95.5</cell><cell>PBG</cell><cell>245.9</cell></row><row><cell cols="2">WYS 89.8 93.6 OOM</cell><cell>WYS</cell><cell>OOM*</cell></row><row><cell>F(WYS) 90.5 93.5</cell><cell>98.6</cell><cell>F(WYS)</cell><cell>185.6</cell></row></table><note><ref type="bibr" target="#b22">Wang et al., 2019)</ref> and PyG(Fey &amp; Lenssen,   </note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Node classification tasks. Left: test accuracy scores on semi-supervised classification (SSC) of citation networks. Middle: test micro-F1 scores for large fully-supervised classification. Right: test accuracy on an SSC task, showing only scalable baselines. We bold the highest value per column.</figDesc><table><row><cell cols="2">Cora Citeseer Pubmeb</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GCN 81.5 70.3</cell><cell>79.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F(GCN) 81.9 69.8</cell><cell>79.4</cell><cell cols="2">Reddit Amazon</cell><cell></cell><cell>Products</cell></row><row><cell>MixHop 81.9 71.4</cell><cell>80.8</cell><cell>SAGE 95.0</cell><cell>83.0</cell><cell>node2vec</cell><cell>72.1</cell></row><row><cell>F(MixHop) 83.1 71.8</cell><cell>80.9</cell><cell>F(SAGE) 95.9</cell><cell>83.2</cell><cell>ClusterGCN</cell><cell>75.2</cell></row><row><cell>GAT* 83.2 72.4</cell><cell>77.7</cell><cell>SimpGCN 94.9</cell><cell>83.4</cell><cell>GraphSAINT</cell><cell>77.3</cell></row><row><cell>F(GAT) 83.3 72.5</cell><cell>77.8</cell><cell>F(SimpGCN) 94.8</cell><cell>83.8</cell><cell>F(SAGE)</cell><cell>77.4</cell></row><row><cell>GCNII 85.5 73.4</cell><cell>80.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F(GCNII) 85.9 73.2</cell><cell>80.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance of GTTF against frameworks DGL and PyG. Left: Speed is the per epoch time in seconds when training GraphSAGE. Memory is the memory in GB used when training GCN. All experiments conducted using an AMD Ryzen 3 1200 Quad-Core CPU and an Nvidia GTX 1080Ti GPU. Right: Training curve for GTTF and PyG implementations of Node2Vec.</figDesc><table><row><cell cols="3">Speed (s) Reddit Products DGL 17.3 13.4 PyG 5.8 9.2</cell><cell cols="2">Memory (GB) Cora Citeseer OOM 1.1 Reddit 1.1 OOM 1.2 1.3</cell><cell>Pubmed 1.1 1.6</cell><cell>ROC-AUC</cell><cell>0.6 0.7 0.8</cell><cell></cell><cell cols="2">framework (node2vec) PyG(node2vec)</cell></row><row><cell>GTTF</cell><cell>4.9</cell><cell>4.4</cell><cell>2.44 0.32</cell><cell>0.40</cell><cell>0.43</cell><cell></cell><cell>0</cell><cell>250</cell><cell>500 Time (s)</cell><cell>750</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>N2VAC C DE E PWA L KAC C; but override BI A SFN</figDesc><table><row><cell>C.2 NODE EMBEDDING IMPLEMENTATIONS</cell></row><row><cell>C.2.1 NODE2VEC (GROVER &amp; LESKOVEC, 2016)</cell></row><row><cell>A simple implementation follows from above:</cell></row><row><cell>C.1.2 DEEP GRAPH INFOMAX (DGI, VELI ČKOVI Ć ET AL., 2019)</cell></row></table><note>DGI implementation on GTTF can use AC C U M U L A T EFN=RO O T E DAD JAC C, defined in (1). To create the positive graph: it can sample some nodes B ⊂ V . It would pass to GTTF's Traverse B, and utilize the accumulated adjacency A for running: GCN( A, X B ) and GCN( A, X permute ), where the second run randomly permutes the order of nodes in X. Finally, the output of those GCNs can then be fed into a readout function which outputs to a descriminator trying to classify if the readout latent vector correspond to the real, or the permuted features.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">To disambiguate: by tensors, we refer to multi-dimensional arrays, as used in Deep Learning literature; and by operations, we refer to routines such as matrix multiplication, advanced indexing, etc</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1">. Construct model &amp; initialize parameters (e.g. to random). Define AC C U M U L A T EFN and BI A SFN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">Our pseudo-code displays the traversal starting from one node rather than a batch only for clarity, as our actual implementation is vectorized e.g. u would be a vector of nodes, T would be a 2D matrix with each row containing transition path preceeding the corresponding entry in u, ... etc. Refer to Appendix and code.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">Before feeding the batch to model, in practice, we find nodes not reached by traversal and remove their corresponding rows (and also columns) from X (and A).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">We present more methods in the Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5">The following definition averages the node features (uses non-symmetric normalization) and appears in multiple GCN's including<ref type="bibr" target="#b11">Hamilton et al. (2017)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6">If not α-regular, it would be 1 |A| A∈A A ∝ D −1 A</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A HYPERPARAMETERS</head><p>For the general link prediction tasks we used a |B| = |V |, C = 5, f = 3, 10 negative samples per edge, Adam optimizer with a learning rate of 0.5, multiplied by a factor of 0.2, every 50 steps, for 200 total iterations. The differences are listed below.</p><p>The Reddit dataset was trained using a starting learning rate of 2.0, decaying 50% every 10 iterations.</p><p>The LiveJournal task was trained using a fixed learning rate of 0.001, |B| = 5000, f = 2, and 50 negative samples per edge.</p><p>For the node classifications tasks:</p><p>For F(SimpleGCN) on Amazon, we use f = [15, 15], a batch size of 1024, and a learning rate of 0.02, decaying by a factor 0f 0.2 after 2 and 6 epochs for a total of 25 epochs. On Reddit, it is the same except f = <ref type="bibr">[25,</ref><ref type="bibr">25]</ref> For F(SAGE) on Amazon we use f = <ref type="bibr">[20,</ref><ref type="bibr">10]</ref>, a two layer model, a batch size of 256, and fixed learning rates of 0.001 and 0.002 respectively. On reddit we use f = [25, 20], a fixed learning rate of 0.001, hidden dimension of 256 and a batch size of 1024. On the Products dataset, we used</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Watch your step: Learning node embeddings via graph attention</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS&apos;18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amol</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hrayr</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML&apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">L. Bottou. Online algorithms and stochastic approximations</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Online Learning and Neural Networks</title>
				<imprint>
			<date type="published" when="1998">2014. 1998</date>
		</imprint>
	</monogr>
	<note>Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stochastic learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Lectures on Machine Learning</title>
		<title level="s">Lecture Notes in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer Verlag</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3176</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The rise of deep learning in drug discovery</title>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Drug discovery today</title>
				<imprint>
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
				<imprint>
			<date type="published" when="2018">2018b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks</title>
		<author>
			<persName><forename type="first">Wei-Lin</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD&apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
				<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pytorch-biggraph: A large-scale graph embedding system</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothee</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Wehrstedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhijit</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Peysakhovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Systems and Machine Learning</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">TACL</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
		<editor>CoRR</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec</title>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Web Search and Data Mining (WSDM&apos;18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The journal of machine learning research</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep graph infomax</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR&apos;19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
				<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Ichi Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML&apos;18)</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph-SAINT: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Few-shot representation learning for out-of-vocabulary words</title>
		<author>
			<persName><forename type="first">Difan</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yewen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
				<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
