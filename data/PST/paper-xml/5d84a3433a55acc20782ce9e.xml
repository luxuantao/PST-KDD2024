<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SELF-TRAINING FOR END-TO-END SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-19">19 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jacob</forename><surname>Kahn</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
						</author>
						<title level="a" type="main">SELF-TRAINING FOR END-TO-END SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-19">19 Sep 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1909.09116v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>semi-supervised</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We revisit self-training in the context of end-to-end speech recognition. We demonstrate that training with pseudo-labels can substantially improve the accuracy of a baseline model by leveraging unlabelled data. Key to our approach are a strong baseline acoustic and language model used to generate the pseudo-labels, a robust and stable beam-search decoder, and a novel ensemble approach used to increase pseudo-label diversity. Experiments on the LibriSpeech corpus show that selftraining with a single model can yield a 21% relative WER improvement on clean data over a baseline trained on 100 hours of labelled data. We also evaluate label filtering approaches to increase pseudo-label quality. With an ensemble of six models in conjunction with label filtering, self-training yields a 26% relative improvement and bridges 55.6% of the gap between the baseline and an oracle model trained with all of the labels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Building automatic speech recognition (ASR) systems requires a large amount of transcribed training data. Compared with hybrid models, the performance of end-to-end models seems to more significantly degrade the amount of available training data decreases <ref type="bibr" target="#b0">[1]</ref>. Transcribing large quantities of audio is both expensive and time-consuming, thus requiring algorithms which can learn more from abundant unpaired audio and text data. Many semi-supervised training approaches have been proposed to take advantage of this unpaired data. One such approach, self-training, uses noisy labels generated from a model trained on a much smaller labelled data set.</p><p>We revisit self-training <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> in the context of sequenceto-sequence models with attention. We show relative gains from self-training on LibriSpeech, a publicly available corpus of read speech, without the use of an externally-trained language model. Using an LM trained on a large text corpus, self-training improves WER by 26% relative on the clean test set and 21% relative on the noisy test set.</p><p>Three key components to our self-training algorithm are (1) a strong baseline acoustic model trained on a small paired data set, (2) a robust and efficient beam search decoder for sequence-to-sequence models which effectively leverages the use of an externally-trained neural language model and (3) a novel ensemble approach for self-training which improves label diversity. Our baseline supervised model, trained on only 100 hours of clean data, gives 8.06% WER on the clean test set, the best reported result in the literature for an end-toend setup. Combined with self-training our model achieves a WER of 5.93% on the clean test set, only 1.7% worse than an oracle experiment trained on all of the available labels from 460 hours of clean speech.</p><p>We also evaluate two methods for pseudo-label filtering <ref type="bibr" target="#b3">[4]</ref> tailored to the mistakes often encountered with sequence-to-sequence models and demonstrate their effect on pseudo-label and model quality. Finally, we give a comprehensive empirical evaluation on the importance of the salient components of our self-training algorithm. In particular, we study the language model used, the mechanisms in the filtering function, and the number of models in the ensemble. In practice, the text available for language model training may not match the distribution of the acoustic transcripts. We give insight into this potential mismatch by observing WER as a function of the perplexity of the language model used to generate the pseudo-labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MODEL</head><p>Our sequence-to-sequence model is an encoder-decoder architecture with attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Let X = [X 1 , . . . , X T ] be the frames of an utterance with corresponding transcription Y = [y 1 , . . . , y U ]. The encoder maps X into a key-value hidden representation:</p><formula xml:id="formula_0">K V = encode(X)<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">K = [K 1 , . . . , K T ] are the keys and V = [V 1 . . . , V T ]</formula><p>are the values. We use a fully convolutional encoder with time-depth separable (TDS) blocks <ref type="bibr" target="#b7">[8]</ref>. The decoder maps the query vectors generated from previous output tokens and the key-value pairs into summary vectors and decodes them into a sequence of predictions. The decoder is given by</p><formula xml:id="formula_2">Q u = g(y u−1 , Q u−1 ) (2) S u = attend(Q u , K, V ) (3) P (y u | X, y &lt;u ) = h(S u , Q u ).<label>(4)</label></formula><p>The function g(•) is a GRU RNN <ref type="bibr" target="#b5">[6]</ref> which encodes the previous token and query vector Q u−1 to produce the next query vector. The attention mechanism, attend(•), produces a summary vector S u , and h(•) computes a distribution over the output tokens. The attention mechanism is a simple innerproduct softmax attention:</p><formula xml:id="formula_3">attend(K, V, Q) = V • softmax 1 √ d K ⊤ Q (5)</formula><p>where d is the is the hidden dimension of the keys (as well as queries and values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Inference</head><p>To perform inference, we search for the most likely hypothesis according to the sequence-to-sequence model (P AM ) and an external language model (P LM ):</p><formula xml:id="formula_4">Ȳ = argmax Y log P AM (Y | X) + α log P LM (Y ) + β|Y |. (<label>6</label></formula><formula xml:id="formula_5">)</formula><p>A candidate is considered complete when an end-of-sentence (EOS) token is proposed. We use several techniques to improve the efficiency and stability of the decoder <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>. One such technique, used to discourage early stopping, is to only propose EOS when the corresponding probability satisfies</p><formula xml:id="formula_6">log P u (EOS | y &lt;u ) &gt; γ • max c =EOS log P u (c | y &lt;u ) (7)</formula><p>where γ is a hyper-parameter which we tune on a development set. We also use a hard attention limit, which does not allow the beam search to propose new hypotheses which attend more than t max frames away from the previous attention peak <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SEMI-SUPERVISED SELF-TRAINING</head><p>In a supervised learning setting, we have access to a paired data set</p><formula xml:id="formula_7">D = {(X 1 , Y 1 ), . . . , (X n , Y n )}.</formula><p>We train a model on the paired data set D by maximizing the likelihood of the ground-truth transcriptions given their corresponding utterances:</p><formula xml:id="formula_8">(X,Y )∈D log P (Y | X).<label>(8)</label></formula><p>In a semi-supervised setting, we have an unpaired data set which consists of unlabelled utterances X and a text data set Y in addition to the paired data set D. We assume |X | ≫ n and similarly |Y| ≫ n.</p><p>To perform self-training, we first bootstrap an acoustic model P AM on the paired data set D by maximizing the objective in Equation <ref type="formula" target="#formula_8">8</ref>. We also train a language model P LM on Y. We then use the acoustic model and the language model to generate a pseudo-label for each unlabelled example X ∈ X by solving Equation <ref type="formula" target="#formula_4">6</ref>. This gives us a pseudo paired data set</p><formula xml:id="formula_9">D = {(X i , Ȳi ) | X i ∈ X }.</formula><p>We then train a new acoustic model on the equally-weighted concatenation of both D and D with the objective</p><formula xml:id="formula_10">(X,Y )∈D log P (Y | X) + (X, Ȳ )∈ D log P ( Ȳ | X). (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Filtering</head><p>The pseudo-labelled data set D contains noisy transcriptions. Achieving the right balance between the size of D (the larger the better) and the noise in the pseudo-labels can make selftraining more effective. We design a simple heuristic-based filtering function specific to sequence-to-sequence models. The filtering function removes the noisiest transcriptions with high recall while retaining the majority of the pseudo-labels.</p><p>Sequence-to-sequence models are known to fail catastrophically at inference in two ways: (1) the attention can loop causing long outputs and (2) the model can predict the EOS token too early leading to an overly short output <ref type="bibr" target="#b9">[10]</ref>.</p><p>We filter for the first failure case by removing examples which contain an n-gram repeated more than c times. Here n and c are hyper-parameters which we tune on a labelled development set. As described in Section 2.1, we attempt to deal with the second failure case by only keeping hypotheses with a EOS probability above a specified threshold. However, on occasion the beam search terminates without finding any hypotheses which end in EOS. We filter all of these examples.</p><p>Additionally, for each pseudo-label, we compute a confidence score based on the conditional likelihood assigned to the label from the acoustic model. For some pseudo-labelled utterance (X i , Ȳi ) ∈ D, we compute the length-normalized log likelihood of that sample as</p><formula xml:id="formula_11">Score( Ȳi ) = log P ( Ȳi | X i ) | Ȳi |</formula><p>where | Ȳi | is the number of tokens in the utterance. The above filtering methods can be combined and tuned collectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ensembles</head><p>We propose and evaluate two approaches to leverage an ensemble of bootstrapped models. We first train M models with different initial weights generated by using different seeds for the random initialization process.</p><p>In the first approach, sample ensemble, we generate a pseudo-labelled data set, Dm , for each model, respectively. We then combine all M sets of pseudo labels with uniform weights and optimize the following objective during training</p><formula xml:id="formula_12">(X,Y )∈D log P (Y | X) + 1 M M m=1 (X, Ȳ )∈ Dm log P ( Ȳ | X).</formula><p>In the implementation, we consider an epoch as a complete pass over the paired data set D and the unpaired audio X . For each X ∈ X we uniformly sample a pseudo-label from one of the M models as the target.</p><p>The second approach, decode ensemble, uses all M models to generate a single pseudo-labelled data set according to the modified objective during inference</p><formula xml:id="formula_13">Ȳ = argmax Y 1 M M m=1 log P m AM (Y | X) + α log P LM + β|Y |.</formula><p>During the beam search we average the scores from all M acoustic models at each step. We obtain a single pseudolabelled data set D and train a model following the objective in Equation <ref type="formula">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data</head><p>All experiments are performed on the publicly available Lib-riSpeech audio book corpus <ref type="bibr" target="#b10">[11]</ref>. We use the "train-clean-100" set as the paired data set, which consists of around 100 hours of clean speech. The unlabelled audio data set consists of 360 hours of clean speech from the utterances in "trainclean-360." We report results on the standard dev and test clean/other (noisy) sets.</p><p>The standard language model training text used with Lib-riSpeech is derived from 14,476 public domain books. The books were selected such that there is no overlap with the dev and test sets <ref type="bibr" target="#b10">[11]</ref>. On the other hand, the training data set transcriptions are almost entirely contained in the LM training text which could result in an unrealistic evaluation of selftraining. To make the learning problem more realistic, we remove all books used to generate the acoustic training data from the language model training data. This results in a removal of 997 books from the LM training corpus.</p><p>We take a few simple steps to pre-process and normalize the resulting text corpus for LM training. First, we detect sentence boundaries using the "punkt" tokenizer <ref type="bibr" target="#b11">[12]</ref> implemented in NLTK <ref type="bibr" target="#b12">[13]</ref>. We normalize the text by converting everything to lower case and removing punctuation except for the apostrophe in contractions (we replace hyphens with a space). Unlike the original LM corpus <ref type="bibr" target="#b10">[11]</ref> we take no steps to replace non-standard words with a canonical verbalized form. However, we find that LMs trained on this new corpus achieve comparable perplexity to LMs trained on the standard text corpus, as measured on the dev clean and other transcriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setting</head><p>Our sequence-to-sequence model consists of nine TDS blocks in groups of three. Before each group we apply a standard 1D convolution with a stride of two in order to reduce the frame-rate of the encoder. The TDS groups contain 10, 14 and 16 channels respectively all with a kernel width of 21. All other architectural details are the same as <ref type="bibr" target="#b7">[8]</ref>. We predict 5,000 sub-word targets generated with the SentencePiece toolkit <ref type="bibr" target="#b13">[14]</ref> using only "train-clean-100" as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head><p>During optimization we pre-train for three epochs with a soft-window (σ = 4) <ref type="bibr" target="#b7">[8]</ref>. Other than 20% dropout, we use 1% uniform target sampling, 10% label smoothing <ref type="bibr" target="#b14">[15]</ref> and 1% word piece sampling <ref type="bibr" target="#b15">[16]</ref> to regularize the models, in addition to teacher-forcing. When training on "train-clean-100," we use a single GPU with a batch size of 16. We use SGD without momentum for 200 epochs with a learning rate of 5e-2 which is annealed by a factor of two every 40 epochs. For all experiments training on a larger pseudo-labelled data set, we use 8 GPUs with a batch size of 16 per GPU and anneal the learning rate by a factor of two every 80 epochs; the architecture and parameters remain the same as in the "trainclean-100" baseline. All experiments are implemented in the wav2letter++ framework <ref type="bibr" target="#b16">[17]</ref>.</p><p>Prior to generating pseudo-labels with any new combination of acoustic and language model, we optimize the beam search hyper-parameters on the dev set including the language model weight and the EOS threshold parameter (Equation <ref type="formula">7</ref>). We train a word piece convolutional LM (ConvLM) <ref type="bibr" target="#b17">[18]</ref> on the text data set described in Section 4.1 using the same model architecture and training recipe as <ref type="bibr" target="#b18">[19]</ref>. In the following experiments, unless specified, we apply our heuristic filtering with c = 2 and n = 4 (Section 3.1).</p><p>When training models on data sets consisting of both paired and pseudo-labelled data, we start from random initialization and train on the combined data set. We observe that this produces better results than starting with a model trained on paired "train-clean-100" utterances and fine-tuning it on pseudo-labelled data. While both techniques produce improvements, starting from a random initialization is consistently better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Supervised Baseline</head><p>A common setup for semi-supervised ASR is to use the "trainclean-100" subset of LibriSpeech as the labelled data set <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the WER from our supervised baseline on </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample Decode</head><p>Fig. <ref type="figure">1</ref>. We compare the two ensemble approaches proposed in Section 3.2. We show WER as a function of the number of models in the ensemble. For each setting we report the average WER over three experiments without an LM.</p><p>"train-clean-100" as well as several other results from the literature. Hayashi et al. <ref type="bibr" target="#b20">[21]</ref> use a sequence-to-sequence model with a BiLSTM-based encoder and location-based attention. They train their model on "train-clean-100" as the baseline for a back-translation style approach. Liu et al. <ref type="bibr" target="#b19">[20]</ref> augment a sequence-to-sequence model with the CTC loss. Compared with the two, our baseline WER on the clean dev and test sets are lower by more than 30% relative. On the other hand, Lüscher et al. <ref type="bibr" target="#b0">[1]</ref> use the sequence-tosequence model proposed in <ref type="bibr" target="#b21">[22]</ref> and to our knowledge, produce the best prior result when limited to "train-clean-100." Compared with this, our TDS baseline model achieves better WER on the dev sets and has similar test WER. We believe our supervised baseline is a challenging yet practical starting point for semi-supervised experiments. This baseline enables us to more meaningfully demonstrate the improvement from adding additional unlabelled audio or text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Evaluating Beam Search</head><p>To study the importance of the stable beam search, we evaluate self-training in two other conditions. First, we compare to pseudo-labels generated from the greedy output of the acoustic model alone. We perform greedy decoding with the supervised baseline model on "train-clean-360" to generate the pseudo-labels. Second, we compare to pseudo-labels generated from a simple beam search with a language model but without the EOS threshold and hard attention limit described in Section 2.1.</p><p>With each setting, we train three models and report the average WER without an external LM in Table <ref type="table">2</ref>. We also compare the pseudo-labels with the ground-truth transcription of "train-clean-360" and compute label WER as an indicator of the label quality. We can see in Table <ref type="table">2</ref> that using an LM in a simple beam search improves the quality of the pseudolabels and hence the resulting trained model. The stable beam search further improves the pseudo-label quality and the resulting WER of a model trained with those labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Labelling</head><p>Label Table <ref type="table">2</ref>. We show the WER from training on pseudo-labels generated using three approaches: (1) only using the acoustic model without a beam search (AM greedy), (2) using both the AM and the LM with a simple beam search (AM+LM simple) and (3) using both the AM and LM with the stable beam search described in Section 2.1 (AM+LM stable). Dev clean and other WERs are reported without an LM averaged over three models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Comparing Ensembles</head><p>Figure <ref type="figure">1</ref> compares the two ensemble approaches on the clean and other dev sets. The sample ensemble results in a larger gain in WER on both sets than the decode ensemble. One possible explanation for this is that since the sample ensemble uses different transcripts for the same sample at training time. This keeps the model from being overly confident in a noisy pseudo-label. In general, the models in the ensemble should tend to disagree more often on incorrect transcriptions and agree more often on correct transcriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Summary</head><p>Table <ref type="table" target="#tab_2">3</ref> summarizes our best results with the strong supervised baseline, the stable beam search, and the sample ensemble. We also decode each model with an LM to demonstrate the full potential of the self-training approach.</p><p>We can see from decoding with an LM. With the sample ensemble approach using six models, we see a further relative improvement of 6.6% on the clean test set.</p><p>To understand the limits of self-training, we also evaluate an oracle model with access to the ground-truth labels from both "train-clean-100" and "train-clean-360." Table <ref type="table" target="#tab_2">3</ref> shows that the best pseudo-label model bridges 55.6% of the gap between the supervised baseline trained only on "train-clean-100" and the oracle model as measured on the clean test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Analyses</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Importance of Filtering</head><p>Table <ref type="table" target="#tab_3">4</ref> shows the results with various filtering functions for pseudo-labels generated on "train-clean-360" by the baseline model. All WER results are the average of three models, all using the same beam-search decoding procedure with a Con-vLM. We evaluate both the "no EOS + n-gram" filters and the acoustic model score threshold excluding samples in the worst 10th percentile.</p><p>In the clean setting, we observe that "no EOS + n-gram" filters improve WER on both the clean and other development sets. The quality of the pseudo-labels is such that removal of the worst 10th percentile of samples based on their acoustic model score improves performance, but removing the nextworst tenth of the data by this criteria removes too much data from the training set. The best-performing combination of these filtering techniques first applies the "no EOS + n-gram" filters and then removes the bottom 10th percentile of samples from the resulting set based on their score. This results in 5% and 8% relative WER reductions on the "clean" and "other" development sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Importance of the LM</head><p>We examine the impact of the LM by training multiple models with pseudo-labels generated from LMs with different perplexity on the dev set. We control for LM perplexity by training the model for a variable number of steps. For each pseudo-label set, we train three models and report the average WER without decoding with an LM.</p><p>In Figure <ref type="figure" target="#fig_1">2</ref> we show the reduction in WER from selftraining on pseudo-labels generated with varying LM perplexities. We can see a clear trend that when the LM perplexity decreases, the WER on the dev set also decreases. In other words, a better LM leads to better model performance for self-training. In Table <ref type="table">2</ref> we show that without using any language model to generate pseudo-labels (AM Greedy), we get a WER of 12.27 on dev clean and 33.42 on dev other. Compared with Figure <ref type="figure" target="#fig_1">2</ref>, it is clear that using an LM even with higher perplexity improves the effectiveness of self-training. We see an upper bound on the LM perplexity of 180, where the quality of the pseudo-labels start to be worse than what we can obtain without an external LM, resulting in worse model performance from self-training.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Filter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>Self-training has been applied to tasks in natural language processing including word-sense disambiguation <ref type="bibr" target="#b1">[2]</ref>, noun identification <ref type="bibr" target="#b22">[23]</ref> and parsing <ref type="bibr" target="#b2">[3]</ref>, in addition to tasks in computer vision such as object detection <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and image classification <ref type="bibr" target="#b25">[26]</ref>.</p><p>In automatic speech recognition, self-training-style approaches have seen some success in hybrid, alignmentbased speech systems. Prior work mainly focuses on different ways of data filtering to improve pseudo-label quality, e.g. confidence-based filtering <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and agreementbased selection <ref type="bibr" target="#b29">[30]</ref> that also takes advantage of outputs from multiple systems, and the data selection process can take place at different levels ranging from frames to utterances <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Using pseudo-labels with a hybrid system can also give an improvement to WER on a large-scale data set <ref type="bibr" target="#b32">[33]</ref> and in another, a student-teacher approach without an external LM gives an improvement when trained on unreleased unpaired audio <ref type="bibr" target="#b33">[34]</ref>. In both of these cases, however, the use of non-public data makes reproduction and direct comparison of methods difficult. As such, one goal of this work is to provide a reproducible recipe for improving ASR performance with pseudo labels, in addition to providing a standard, publicly-available benchmark to which other semi-supervised approaches in automatic speech recognition can compare.</p><p>Recently-proposed semi-supervised approaches for endto-end speech recognition have applied techniques similar to back-translation <ref type="bibr" target="#b34">[35]</ref>. These use unpaired text to generate a synthetic data set, but target hidden state representations instead of acoustic features directly <ref type="bibr" target="#b20">[21]</ref>. Alternatively, both unpaired audio and text can be used by embedding the two in a shared representation <ref type="bibr" target="#b35">[36]</ref>. This prior work in semi-supervised ASR with end-to-end models tends to build from a weak or poorly tuned supervised baseline model, a common issue with semi-supervised learning in general <ref type="bibr" target="#b36">[37]</ref>. In contrast, we compare our self-trained models to a well tuned baseline model which outperforms prior results trained on the same data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>We have shown that self-training can yield substantial improvements for end-to-end systems over a strong baseline model by leveraging a large unlabelled data set. Self-training has been well studied in other application domains but has not been carefully studied in end-to-end speech recognition with deep neural networks. We hypothesize that the noise-robust properties of these models coupled with a strong baseline model trained on a paired data set makes self-training more effective. Furthermore, we show that a filtering mechanism tailored to the types of mistakes encountered with sequenceto-sequence models as well as an ensemble of baseline models can further improve the accuracy gains from self-training.</p><p>One limitation of this study is that LibriSpeech, the data set we perform experiments on, consists entirely of read speech. This is not a completely practical setting as the distribution of books used to train the language model closely matches that of the transcriptions in the acoustic training data. We take considerable care to remove any exact overlap between the two data sets so as to make the problem setting more realistic. Nonetheless, future work should examine noisier and less well-matched unlabelled speech and text.</p><p>One of the main axes for improving semi-supervised learning methods is data scale. We expect that self-training could lead to even more improvements on LibriSpeech and elsewhere by continuing to grow the scale of both the unlabelled audio and text corpora. Aside from demonstrating the effectiveness of self-training on LibriSpeech, we have set forth a strong baseline model and a reproducible semisupervised learning setting for which new and existing approaches can be evaluated. We hope this contributes to the acceleration of this line of research in speech recognition moving forward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The WER on the clean and other development sets as a function of the perplexity of the LM used to generate the pseudo-labels. For each pseudo-labelled data set we train three models and report the average WER without an LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The WER for various end-to-end models trained on the "train-clean-100" subset of LibriSpeech. All numbers are reported without an external LM.</figDesc><table><row><cell></cell><cell></cell><cell>WER</cell><cell cols="2">Test WER</cell></row><row><cell></cell><cell cols="4">clean other clean other</cell></row><row><cell>Liu et al. [20]</cell><cell>21.6</cell><cell>-</cell><cell>21.7</cell><cell>-</cell></row><row><cell cols="2">Hayashi et al. [21] 24.9</cell><cell>-</cell><cell>25.2</cell><cell>-</cell></row><row><cell>Lüscher et al. [1]</cell><cell>14.7</cell><cell>38.5</cell><cell>14.7</cell><cell>40.8</cell></row><row><cell>Our model</cell><cell>14.0</cell><cell>37.0</cell><cell>14.9</cell><cell>40.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Table3that compared with the supervised baseline, even the basic pseudo-labelling approach yields a 21.2% relative improvement on the clean test set and a 20.7% relative improvement on the other test set after Results of semi-supervised self-training on the LibriSpeech dev and test sets. For each model we report WER with and without the use of a convolutional language model. Here, we use the sample ensemble with six models as it gave the best results.</figDesc><table><row><cell>Data set</cell><cell>LM</cell><cell cols="2">Dev WER clean other clean other Test WER</cell></row><row><cell>Baseline Paired 100</cell><cell>None</cell><cell cols="2">14.00 37.02 14.85 39.95</cell></row><row><cell>Oracle Paired 100+360</cell><cell>None</cell><cell>7.20 25.32</cell><cell>7.99 26.59</cell></row><row><cell>Paired 100 + Pseudo 360</cell><cell>None</cell><cell>9.30 28.79</cell><cell>9.84 30.15</cell></row><row><cell cols="2">Paired 100 + Ensemble 360 None</cell><cell>8.60 27.78</cell><cell>9.21 29.29</cell></row><row><cell>Baseline Paired 100</cell><cell>ConvLM</cell><cell>7.78 28.15</cell><cell>8.06 30.44</cell></row><row><cell>Oracle Paired 100+360</cell><cell>ConvLM</cell><cell>3.98 17.00</cell><cell>4.23 17.36</cell></row><row><cell>Paired 100 + Pseudo 360</cell><cell>ConvLM</cell><cell>5.73 22.54</cell><cell>6.35 24.13</cell></row><row><cell cols="2">Paired 100 + Ensemble 360 ConvLM</cell><cell>5.37 22.13</cell><cell>5.93 24.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>For "train-clean-360" pseudo-labels, we show the effect of the filtering mechanism on the amount of training data retained, the oracle WER of the pseudo-labels and the resulting WER on the clean and other development sets from training on the corresponding pseudo-labelled data set. The clean and other WERs are reported with an LM and beam search decoding averaged over three models.</figDesc><table><row><cell></cell><cell>% Hours</cell><cell>Label</cell><cell>Dev</cell><cell>Dev</cell></row><row><cell></cell><cell>Retained</cell><cell>WER</cell><cell>Clean</cell><cell>Other</cell></row><row><cell>None</cell><cell>100</cell><cell>9.57</cell><cell cols="2">6.21 23.66</cell></row><row><cell>no EOS+n-gram</cell><cell>98.3</cell><cell>8.25</cell><cell cols="2">5.98 22.63</cell></row><row><cell>Score</cell><cell>90.0</cell><cell>7.37</cell><cell cols="2">5.92 21.70</cell></row><row><cell>no EOS+n-gram +Score</cell><cell>88.5</cell><cell>7.02</cell><cell cols="2">5.89 21.63</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">ACKNOWLEDGEMENTS</head><p>Thanks to Tatiana Likhomanenko, Qiantong Xu, Ronan Collobert and Gabriel Synnaeve for their help with this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">RWTH ASR systems for librispeech: Hybrid vs attention-w/o data augmentation</title>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03072</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised word sense disambiguation rivaling supervised methods</title>
		<author>
			<persName><forename type="first">David</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">33rd annual meeting of the association for computational linguistics</title>
				<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</title>
				<meeting>the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Setred: Self-training with editing</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="611" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sequence-to-sequence speech recognition with time-depth separable convolutions</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Wav2letter: an end-to-end convnetbased speech recognition system</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03193</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards better decoding and language model integration in sequence to sequence models</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02695</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised multilingual sentence boundary detection</title>
		<author>
			<persName><forename type="first">Tibor</forename><surname>Kiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Strunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="525" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Edward</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bird</surname></persName>
		</author>
		<idno>arXiv preprint cs/0205028</idno>
		<title level="m">Nltk: the natural language toolkit</title>
				<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">wav2letter++: The fastest opensource speech recognition system</title>
		<author>
			<persName><forename type="first">Awni</forename><surname>Vineel Pratap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07625</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adversarial training of end-to-end speech recognition using a criticizing language model</title>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Alexander H Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin-Shan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6176" to="6180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Back-translation-style data augmentation for end-to-end ASR</title>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramon</forename><surname>Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Takeda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Spoken Language Technology Workshop (SLT)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Improved training of end-to-end attention models for speech recognition</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Creating subjective and objective sentence classifiers from unannotated texts</title>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on intelligent text processing and computational linguistics</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="486" to="497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pseudo-labels for supervised learning on dynamic vision sensor data, applied to object detection under ego-motion</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
				<imprint>
			<date type="published" when="2018-06">June 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised self-training of object detection models</title>
		<author>
			<persName><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Schneiderman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cost-effective active learning for deep image classification</title>
		<author>
			<persName><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2591" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised training of a speech recognizer: Recent experiments</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Waibel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth European Conference on Speech Communication and Technology</title>
				<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Confidence-measure-driven unsupervised incremental adaptation for hmm-based speech recognition</title>
		<author>
			<persName><forename type="first">Delphine</forename><surname>Charlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2001 IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<title level="s">Proceedings (Cat. No. 01CH37221</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised training of acoustic models for large vocabulary continuous speech recognition</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="31" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">High quality agreement-based semi-supervised training data for acoustic modeling</title>
		<author>
			<persName><forename type="first">Félix</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaumont</forename><surname>Quitry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Oines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Spoken Language Technology Workshop (SLT</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="592" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semi-supervised training of deep neural networks</title>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Workshop on Automatic Speech Recognition and Understanding</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="267" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semisupervised DNN training with word selection for asr</title>
		<author>
			<persName><forename type="first">Karel</forename><surname>Veselỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukás</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Cernockỳ</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>INTERSPEECH</publisher>
			<biblScope unit="page" from="3687" to="3691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A big data approach to acoustic model training corpus selection</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kapralova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Alex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Weinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Siohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association (Interspeech)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lessons from building acoustic models with a million hours of speech</title>
		<author>
			<persName><forename type="first">Sree</forename><surname>Hari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnan</forename><surname>Parthasarathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikko</forename><surname>Strom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6670" to="6674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06709</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semi-supervised end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoharu</forename><surname>Iwata</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Interspeech</publisher>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
	<note>Atsunori Ogawa, and Marc Delcroix</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
