<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KerGNNs: Interpretable Graph Neural Networks with Graph Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aosong</forename><surname>Feng</surname></persName>
							<email>aosong.feng@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chenyu</forename><surname>You</surname></persName>
							<email>chenyu.you@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shiqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM T. J. Watson Research Center</orgName>
								<address>
									<settlement>Yorktown Heights</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leandros</forename><surname>Tassiulas</surname></persName>
							<email>leandros.tassiulas@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University</orgName>
								<address>
									<settlement>New Haven</settlement>
									<region>CT</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KerGNNs: Interpretable Graph Neural Networks with Graph Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph kernels are historically the most widely-used technique for graph classification tasks. However, these methods suffer from limited performance because of the hand-crafted combinatorial features of graphs. In recent years, graph neural networks (GNNs) have become the state-of-the-art method in downstream graph-related tasks due to their superior performance. Most GNNs are based on Message Passing Neural Network (MPNN) frameworks. However, recent studies show that MPNNs can not exceed the power of the Weisfeiler-Lehman (WL) algorithm in graph isomorphism test. To address the limitations of existing graph kernel and GNN methods, in this paper, we propose a novel GNN framework, termed Kernel Graph Neural Networks (KerGNNs), which integrates graph kernels into the message passing process of GNNs. Inspired by convolution filters in convolutional neural networks (CNNs), KerGNNs adopt trainable hidden graphs as graph filters which are combined with subgraphs to update node embeddings using graph kernels. In addition, we show that MPNNs can be viewed as special cases of KerGNNs. We apply KerGNNs to multiple graph-related tasks and use cross-validation to make fair comparisons with benchmarks. We show that our method achieves competitive performance compared with existing state-of-the-art methods, demonstrating the potential to increase the representation ability of GNNs. We also show that the trained graph filters in KerGNNs can reveal the local graph structures of the dataset, which significantly improves the model interpretability compared with conventional GNN models.</p><p>In recent years, the machine learning research community has devoted substantial energy to applying graph neural networks (GNNs) to numerous downstream graph-related tasks (</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gv</head><p>The yellow shadow represents the subgraph of node v. After interacting with graph filters, the updated node is colored in blue.</p><p>structural information. For graph-level tasks, a permutationinvariant readout function is used to extract feature representations from the entire graph. In fact, MPNNs have been motivated and derived as a continuous and differentiable analog of the Weisfeiler-Lehman (WL) algorithm (Leman and Weisfeiler 1968) which is known to successfully test graph isomorphism for a broad class of graphs. However, recent studies <ref type="bibr" target="#b32">(Xu et al. 2018a;</ref><ref type="bibr" target="#b15">Morris et al. 2019)</ref> show that MPNNs are at most as powerful as the WL kernel <ref type="bibr" target="#b26">(Shervashidze et al. 2011)</ref> and WL algorithm regarding the graph isomorphism tests. This demonstrates theoretical limits in the expressivity of popular GNNs. For example, Figure <ref type="figure" target="#fig_0">1</ref>(a) shows two graphs which cannot be distinguished by 1-WL algorithm, and therefore are also indistinguishable by MPNNs. Before the advent of GNNs, graph kernels were the most widely-used techniques for solving graph classification tasks <ref type="bibr" target="#b14">(Kriege, Johansson, and Morris 2020)</ref>. Graph kernels measure the similarity between graphs, and can be applied into a kernel machine (e.g., support vector machine). Kernel functions remove the need of learning node embeddings in high dimensions, and enable us to operate in a high dimensional feature space by simply computing the kernel value in the low-dimensional feature space, which is more computationally efficient than computing in the highdimensional space directly. Because of the empirical success of kernel-based methods and the increasing availability of graph-structured datasets, numerous graph kernel methods have been proposed, including walks and paths kernels <ref type="bibr" target="#b10">(Gärtner, Flach, and Wrobel 2003;</ref><ref type="bibr" target="#b12">Kashima, Tsuda, and Inokuchi 2003;</ref><ref type="bibr">Borgwardt and Kriegel 2005)</ref>, subgraph kernels <ref type="bibr" target="#b27">(Shervashidze et al. 2009), and</ref><ref type="bibr">WL kernels (Shervashidze et al. 2011)</ref>. However, graph kernels still have limitations due to their hand-crafted features and fixed feature construction scheme, which may not effectively capture high-dimensional information (e.g., complex node interactions) on large graphs.</p><p>In this paper, to address the above-mentioned issues and increase the expressivity of GNNs, we propose a subgraphbased node aggregation algorithm by combining GNNs and graph kernels into one framework, and thus the advantages of both methods can be leveraged. On one hand, for neighborhood aggregation, we apply graph kernels which use the subgraph induced by node neighbors, so that the expressivity will not be limited by 1-WL isomorphism test which uses the multiset of neighboring nodes. An example is shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>, where we note that node 1 in both graphs has the same neighborhood multiset but induce different subgraph topologies with its neighbors which can be distinguished by graph kernels. On the other hand, we make the feature construction scheme of graph kernels trainable following the standard GNN training framework, possibly allowing for greater adaptability.</p><p>Based on the subgraph-based node aggregation, we propose a novel GNN framework, termed KerGNNs. Specifically, we first introduce a set of trainable hidden graphs, named graph filters, in each layer. Each node within the input graph is associated with a subgraph capturing its local topological information. We then adopt graph kernel functions to compare the similarity of graph filters and input subgraphs, and use the computed kernel values to update the respective node's feature representations (as shown in <ref type="bibr">Figure 1(b)</ref>). We show that KerGNNs provide a new kernel perspective to extend the standard CNN structure into the graph domain and generalize most MPNNs. The proposed model is then evaluated with various real-world graph and node classification tasks, and the results show superior performance of KerGNNs compared with many existing state-of-the-art models. To better understand the predictions of GNN-based methods, KerGNNs can further visualize the trained graph filters, similar to visualizing filters in CNNs, and thus provide better human-interpretable explanations for a variety of graph-related tasks, compared to existing GNNs. Our main contributions are summarized as follows:</p><p>1. We use neighborhood subgraph topology combined with kernel methods for GNN neighborhood aggregation, and show with proof that the expressivity of this approach is not limited by the 1-WL algorithm. 2. We provide a new perspective to generalize CNNs into the graph domain, by showing that both 2-D convolution and graph neighborhood aggregation can be interpreted using the language of kernel methods. 3. Besides envisioning the output graphs of the model, KerGNNs can further reveal the local structure of the input graphs by visualizing the topology of trained graph filters, which significantly improves the model interpretability and transparency compared with standard MPNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Expressivity</head><p>Several works have been devoted to improving the expressivity of GNNs by introducing spatial, hierarchical, and higher-order GNN variants. For example, Abu-El-Haija et al. ( <ref type="formula">2019</ref>) proposed the mix-hop structure which can learn a more general class of neighborhood mixing relationships. <ref type="bibr" target="#b23">Sato, Yamada, and Kashima (2019)</ref> proposed to use Consistent Port Numbering GNN to augment the neighborhood aggregation, but port orderings are not unique and different orderings may lead to different expressivity. <ref type="bibr" target="#b14">Klicpera, Groß, and Günnemann (2020)</ref> leveraged the atom coordinate information in the molecular graph to improve the expressivity, but the notion of direction is hard to generalize to more general graphs. Nguyen and Maehara (2020) used the graph homomorphism numbers as updated embeddings and show the expressivity of such graph classifiers with universality property, which unfortunately lacks neural network structure. Higher-order GNN variants have been studied in <ref type="bibr" target="#b15">Morris et al. (2019) and</ref><ref type="bibr">Maron et al. (2019)</ref>, which is more powerful than the 1-WL graph isomorphism test. However, higher-order methods always involve heavy computation and KerGNNs introduce a different way to break this 1-WL limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Combination of Graph Kernel and GNNs</head><p>Graph kernels and GNNs can be combined in the same framework. Some works apply graph kernels and neural networks at different stages <ref type="bibr" target="#b16">(Navarin, Tran, and Sperduti 2018;</ref><ref type="bibr" target="#b19">Nikolentzos et al. 2018)</ref>. There are also works on using GNN architecture to design new kernels. For example, <ref type="bibr" target="#b8">Du et al. (2019)</ref> proposed a graph kernel equivalent to infinitely wide GNNs which can be trained using gradient descent.</p><p>A different line of research focuses on integrating kernel methods into GNNs. <ref type="bibr">Lei et al. (2017)</ref> mapped inputs to RKHS by comparing inputs with reference objects. However, the reference objects they use lack graph structure and may not be able to capture the structural information. <ref type="bibr" target="#b4">Chen, Jacob, and Mairal (2020)</ref> proposed GCKN which maps the input into a subspace of RKHS using walk and path kernel. While GCKN utilizes the local walk and path only starting from the central node, our model considers any walks (up to a maximal length) within the subgraph around the central node, and can thus explore more topological structures. Another recent work by <ref type="bibr" target="#b28">Nikolentzos and Vazirgiannis (2020)</ref> focused on improving model transparency by calculating the graph kernels between trainable hidden graphs and the entire graph. However, the method only supports a single-layer model and lacks theoretical interpretation. Our KerGNN model generalizes their scenario by applying hidden graphs to extract local structural information instead of the entire graph, and therefore constructs a multi-layer structure with better graph classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explainability</head><p>Both graph structures and feature information lead to complex GNN models, making it hard for a human-intelligible explanation of the prediction results. Therefore, the transparency and explainability of GNN models are important issues to address. <ref type="bibr" target="#b1">Baldassarre and Azizpour (2019)</ref> compared two main classes of explainability methods using infection and solubility problems. <ref type="bibr" target="#b21">Pope et al. (2019)</ref> introduced explainability methods for the popular graph convolutional neural networks and demonstrated the extended methods on visual scene graphs and molecular graphs. <ref type="bibr" target="#b34">Ying et al. (2019)</ref> proposed a model-agnostic approach that can identify a compact subgraph that has a crucial role in GNN's prediction. In addition to visualizing output graphs as in regular GNNs, our KerGNN provides trained hidden graphs as a byproduct of training without additional computations, which contain useful structural information showing the common characteristics of the whole dataset instead of one specific graph, and can be helpful for interpreting the predictions of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background: Graph Kernels</head><p>Graph kernels have been proposed to solve the problem of assessing the similarity between graphs, and therefore making it possible to perform classification and regression with graph-structured data. Most graph kernels can be written as the sum of several pair-wise base kernels, following the Rconvolution framework (Haussler 1999):</p><formula xml:id="formula_0">K(G, G ) = v∈V v ∈V k base (v, v ),<label>(1)</label></formula><p>where G = (V, E), G = (V , E ) are two input graphs with node attributes, and k base can be any positive definite kernel defined on the node attributes. In this paper, we mainly consider random walk kernel which will be integrated into our proposed model in the next section.</p><p>Random walk kernels are one of the most studied graph kernels. They count the number of walks that two graphs have in common, and were initially proposed by <ref type="bibr" target="#b10">Gärtner, Flach, and Wrobel (2003)</ref> and <ref type="bibr" target="#b12">Kashima, Tsuda, and Inokuchi (2003)</ref>. Among numerous variations of the random walk kernel, we deploy the P -step random walk kernel which compares random walks up to length P in two graphs.</p><p>Following Equation 1, we can write the base kernel of random walks with length p as</p><formula xml:id="formula_1">k p base (v, v ) =    a(v), a(v ) , if p = 0 a(v), a(v ) • λ u∈N (v) u ∈N (v ) k (p−1) base (u, u ), if p &gt; 0</formula><p>where λ is the coefficient, N (v) denotes neighbors of v, p denotes the length of random walks which we compare in two graphs. If p = 0, the random walk kernel is equivalent to the simple node-pair kernel. To efficiently compute the random walk kernels, we follow the generalized framework of computing walk-based kernel <ref type="bibr" target="#b31">(Vishwanathan et al. 2006)</ref>, and utilize the direct product graph defined as below.</p><p>Definition 1 (Direct Product Graph). For two labeled graphs G = (V, E) and G = (V , E ), the direct product graph is defined as</p><formula xml:id="formula_2">G × = G × G = (V × , E × ), de- fined as V × = {(v, v ) : v ∈ V ∧ v ∈ V } and E × = {{(v, v ), (u, u )} : {v, u} ∈ E ∧ {v , u } ∈ E }.</formula><p>Performing a random walk on the direct product graph G × is equivalent to performing the simultaneous random walks on graphs G and G . The P -step random walk kernel can be calculated as</p><formula xml:id="formula_3">K(G, G ) = P p=0 K p (G, G ) = P p=0 λ p |V×| i,j=1 A p × ij , (2)</formula><p>where A × is the adjacency matrix of G × and λ = (λ 0 , λ 1 , ...) is a sequence of weights. It should be noted that the (i, j)-th element of A p × (i.e., A × to the power of p) represents the number of common walks of length p between the i-th and j-th node in G × .</p><p>To generalize the above formula into the continuous and multi-dimensional scenario, we first define the vertex attributes of the direct product graph G × . Given the node attribute matrix X ∈ R n×d for a graph with n nodes and each node attribute is of dimension d, the node attribute matrix S of the direct product graph</p><formula xml:id="formula_4">G × = G 1 × G 2 is calculated as S = X 1 X T 2 ,</formula><p>where X 1 ∈ R n1×d and X 2 ∈ R n2×d are the node attribute matrices for G 1 and G 2 , respectively, and S ∈ R n1×n2 . The (i, j)-th element of matrix S encodes the similarity between the ith-node of G 1 and the j-th node of G 2 . We flatten S into vector s ∈ R n1n2 for ease of notation, and then integrate the encoded pair-wise similarity into Equation 2</p><formula xml:id="formula_5">K p (G, G ) = |V×| i,j=1 s i s j A p × ij = s T A p × s.<label>(3)</label></formula><p>Based on this equation, we can calculate the kernel value between two input graphs using the similarity of common walks as the metric. The details of calculating Equation 3 are included in Appendix.</p><p>In practice, we also consider a slight variation of Equation 1 by adding trainable weights to each base kernel term, and we call it deep random walk kernel:</p><formula xml:id="formula_6">K(G, G ) = v∈V v ∈V w (v,v ) k base (v, v ),<label>(4)</label></formula><p>where w (v,v ) represents the trainable weight assigned to the base kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Model</head><p>In this section, we first discuss the framework of the proposed KerGNN model. Then we introduce the concept of subgraph-based neighborhood aggregation, and use it to analyze the expressivity of KerGNNs. Next, we show that KerGNNs are inspired by CNNs and compare them from the kernel perspective. Finally we argue that KerGNNs can generalize MPNN architecture and analyze the time complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>KerGNN Framework</head><p>In this subsection, we introduce the KerGNN model which updates each node's embedding according to the subgraph centered at this node instead of the rooted subtree patterns in MPNNs, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b). Unless otherwise specified, we refer to the subgraph as the vertex-induced subgraph formed from a node and all its 1-hop neighbors.</p><p>We first define the embeddings of nodes and subgraphs, which are mapping functions from graphs to the feature space and from nodes to the feature space.</p><p>Definition 2 (Feature mapping). Given a graph G = (V, E), a node feature mapping is a node-wise mapping function φ : V → R d , which maps every node v ∈ V to a point φ(v) in R d , and φ(v) is called the feature map for node v. A graph feature mapping is a function Φ : G → R d , where G is the set of graphs, and Φ(G) is called the feature map for graph G ∈ G.</p><p>For an L-layer neural network, we call the input layer the 0-th layer. At each hidden layer l, the input to this layer is an undirected graph G = (V, E), and each node v ∈ V has a feature map φ l−1 (v) ∈ R d l−1 . The output of layer l is the same graph G, because we do not consider graph pooling here, and each node v ∈ V in the output graph has a feature map φ l (v) ∈ R d l . For example, G can be a graph in the dataset, and φ 0 (v) is the node attributes with dimension d 0 . For graphs with discrete node labels, the attributes can be represented as the one-hot encodings of labels and the dimension of attributes corresponds to the total number of classes. For graphs without node labels, we use the degree of the node as the node attribute.</p><p>Inspired by the filters in CNN, we define a set of graph filters at each KerGNN layer to extract the local structural information around each node in the input graph (see Definition 3 (Graph filter). The i-th graph filter at layer l is a graph</p><formula xml:id="formula_7">H (l) i with n (l) i nodes. It has a trainable adjacency matrix A (l) i ∈ R n (l) i ×n (l) i and node attribute matrix W (l) i ∈ R n (l) i ×d l−1 .</formula><p>At layer l, there are d l graph filters such that the output dimension is also d l , and each node attribute in the graph filter, represented by each row of W (l) i , has the same dimension as the node feature map φ l−1 (v) in the input graph.</p><p>KerGNN Layer. Now we consider a single KerGNN layer. We assume the input is a graph-structured dataset with undirected graph G = (V, E), and each node v ∈ V has the attribute a(v) ∈ R d0 . Then the input node feature map is</p><formula xml:id="formula_8">φ 0 (v) = a(v).</formula><p>Each node v in the graph is equipped with a subgraph</p><formula xml:id="formula_9">G v = (V v , E v ), and feature maps {φ 0 (u) : u ∈ V v } are transformed to φ 1 (v)</formula><p>in a way such that neighbors' local information (topological information and node representations) contained in G v will be aggregated to the central node v. We then rely on the graph filters {H (1) i</p><formula xml:id="formula_10">: i = 1, ..., d 1 } to obtain φ 1 (v). Specifically, we calculate φ 1 (v) by project- ing subgraph feature map Φ 0 (G v ) into the i-th dimension of φ 1 (v) using the kernel function value between graph filter H (1) i and subgraph G v , i.e., φ 1,i (v) = K(G v , H (1) i ),<label>(5)</label></formula><p>where we adopt a random walk kernel as K(•, •), which is introduced in Equation <ref type="formula">2</ref>. After calculating the kernel value of the subgraph G v with respect to every graph filter {H</p><p>(1) i</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>:</head><p>Algorithm 1: Forward pass in l-th KerGNN layer</p><formula xml:id="formula_11">Input: Graph G = (V, E); Input node feature maps {φ l−1 (v) : v ∈ V}; Graph filters {H (l) i : i = 1, ..., d l }; Graph kernel function K Output: Graph G = (V, E); Output node feature maps {φ l (v) : v ∈ V} for v ∈ V do Gv = subgraph({v} ∪ N (v)); for i = 1 to d l do φ l,i (v) = K(Gv, H<label>(l)</label></formula><p>i ); end for end for i = 1, ..., d 1 }, we obtain every dimension of node v's feature map φ 0 (v), which forms the output of the KerGNN layer.</p><p>It should be noted that using graphs G v and H</p><p>(1) i to calculate the kernel value is equivalent to performing inner product of φ 1 (G v ) and φ 1 (H (1) i ) in an implicit high-dimensional space, and using feature map of G v instead of the multiset of neighboring nodes (as used in MPNNs) improves expressivity, which is analyzed in the next subsection. Besides, if we use the output space R d1 to approximate the highdimensional space introduced by the kernel method, the updating rule will correspond to the convolutional kernel network proposed by Mairal et al. ( <ref type="formula">2014</ref>), and we will follow the same idea when we compare KerGNNs with CNNs in the later subsection.</p><p>Multiple-layer Model. Based on the single-layer analysis above, we can construct a multiple-layer KerGNN by stacking KerGNN layers followed by readout layers. Specifically, the input to layer l is the graph G with node feature map i . Then the i-th dimension of the output feature map for node v in G can be explicitly calculated as</p><formula xml:id="formula_12">{φ l−1 (v) : v ∈ G}.</formula><formula xml:id="formula_13">φ l,i (v) = K(G v , H (l) i ).<label>(6)</label></formula><p>The forward pass of the lth-layer of KerGNNs is summarized in Algorithm 1.</p><p>For the graph classification, we then deploy the graphlevel readout layer to generate the embedding for the entire graph. We obtain the graph representation at each layer by summing all the nodes' representations. To leverage information from every layer of the model, we then concatenate the graph representations across all layers:</p><formula xml:id="formula_14">Φ(G) = concat v∈G φ l (v) l = 0, 1, ..., L . (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expressivity of Subgraph-based Aggregation</head><p>In this subsection, we first define the subgraph-based neighborhood aggregation, and discuss the requirements of the subgraph feature map to achieve higher expressivity than 1-WL algorithm, then we show that KerGNN is one of the models that satisfy these requirements.</p><p>To leverage the structural information contained in the subgraph, we aggregate the subgraph information by finding a proper subgraph feature map Φ(G v ), and update the node representation of v combining the subgraph feature map with v's own feature map. Formally, we define this aggregation process as follows.</p><p>Definition 4 (Subgraph-based aggregation). The graph neural network at layer l deploying subgraph-based neighborhood aggregation updates feature mapping φ according to</p><formula xml:id="formula_15">φ l (v) = u (φ l−1 (v), f (Φ l−1 (G v )))</formula><p>, where u and f are update and aggregation functions, respectively.</p><p>GNNs distinguish different graphs by mapping them to different embeddings, which resembles the graph isomorphism test. <ref type="bibr" target="#b32">Xu et al. (2018a)</ref> characterize the representational capacity of MPNNs using the WL graph isomorphism test criterion, and show that MPNNs can be as powerful as 1-WL graph isomorphism test if the node update, aggregation, and graph-level readout function are injective. We follow the similar approach and show in the following that subgraphbased GNNs like KerGNNs can be at least as powerful as the 1-WL graph isomorphism test.</p><p>Because we are comparing the model's expressivity with the 1-WL algorithm which updates node labels based on the multiset of neighboring nodes, to achieve high expressivity, it is natural to think that Φ(G v ) should have a one-to-one relationship with respect to the multiset of nodes that subgraph G v contains. We show in Lemma 1 that the graph feature map induced by the random walk kernel satisfies this condition.</p><p>Lemma 1 if Φ(G) is the feature map of graph G induced by the random walk graph kernel, then Φ(G) is injective with respect to the multiset of all its contained nodes { {a(v) : v ∈ V(G)} }, where { {•} } denotes the multiset and a(v) is the label or attribute of node v.</p><p>The proof follows directly from the random walk kernel definition in <ref type="bibr" target="#b10">Gärtner, Flach, and Wrobel (2003)</ref>, and we notice that the graph feature map induced by the WL graph kernel also satisfies this lemma. Based on this injective relationship between multiset and subgraph feature map, we can compare the expressivity of the subgraph-based GNN and 1-WL graph isomorphism test using the following theorem.</p><p>Theorem 1 Let A : G → R d be a GNN with a sufficient number of GNN layers, if the following conditions hold at layer l: a) A aggregates and updates node features iteratively with</p><formula xml:id="formula_16">φ l (v) = u (φ l−1 (v), f (Φ l−1 (G v )))</formula><p>, where function u and f are injective, and Φ l−1 is the feature mapping induced by the random walk kernel; b) A's graph-level readout, which operates on the multiset of node features { {φ l (v)} }, is injective; then A maps any graphs G and H that 1-WL test decides as non-isomorphic to different embeddings, and there exist graph G and H that 1-WL test decides as isomorphic, but can be mapped to different embeddings by A.</p><p>The proof is shown in Appendix. This theorem shows that subgraph-based GNNs can be more expressive than the 1-WL isomorphism test and thus MPNNs. In the KerGNN model, we do not explicitly calculate the subgraph feature map Φ(G v ) which lives in the high-dimensional space. Instead, we apply the kernel trick and use the subgraph feature map as K(G v , H) = Φ(G v ), Φ(H) . Then, the graph kernel function K(•, H) can be seen as a composition of functions u and f . Therefore, according to Theorem 1, to achieve high representational power, the graph kernel function needs to be injective with respect to the subgraph feature map Φ(G v ), and we introduce the following lemma to show that the KerGNN model satisfies this requirement.</p><p>Lemma 2 There exists a feature map Φ(H) so that</p><formula xml:id="formula_17">K(H, G v ) = Φ(H), Φ(G v ) is unique for different Φ(G v ).</formula><p>The proof is shown in Appendix. Besides, as shown in the definition of graph filters, in the KerGNN model we parameterize the node feature and adjacency matrix of graph filter H instead of directly parameterizing Φ(H).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections to CNNs</head><p>Standard CNN models update the representation of each pixel by convolving filters with the patch centered at it, and in GNNs, a natural analog of the patch in the graph domain is the subgraph. While many MPNNs draw connections with CNNs by extending 2-D convolution to the graph convolution, we show in this subsection that both 2-D image convolution and KerGNN aggregation process can be viewed as applying kernel tricks to the input image or graph, and therefore, the KerGNN model naturally extends the CNN architecture into the graph domain, from a new kernel perspective.</p><p>We first show in Appendix that under suitable assumptions, the 2-D image convolution can be viewed as applying kernel functions between input patches and filters. The basic idea is that we can rethink the 2-D convolution as projecting the input image patch into the kernel-induced Hilbert space. The projection is done by performing inner product between the patch and basis vectors, which can be calculated using the kernel trick, and the projected representation in the output space will be the output of the CNN layer.</p><p>Then we can extend the same philosophy to the graph domain, by introducing subgraphs and topology-aware graph filters as the counterpart of patches and filters in CNNs, and KerGNN will adopt the kernel trick to project the input subgraph representation into the output space (detailed in Appendix). Based on these two observations, we can see that KerGNNs generalize CNNs into the graph domain by replacing the kernel function for vectors with the graph kernel function, which provides a new insight into designing GNN architecture, different from the spatial and spectral convolution perspectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections to Existing GNNs</head><p>As the subgraph of one node can be a more fruitful source of information than just the multiset of its neighbors, we show in this subsection that KerGNNs can generalize the standard MPNNs. From the point of view of KerGNNs, MPNNs deploy a simple graph filter with one node, and an appropriate kernel function can be chosen within KerGNN framework, such that KerGNNs iteratively update nodes' repre-sentations using neighborhood multiset aggregation like in MPNNs. For example, we show in Appendix that the node update rule of Graph Convolutional Network (GCN) (Kipf and Welling 2016) can be treated as using one-node graph filters with properly-defined R-convolution graph kernel. Our model generalizes most MPNN structures by deploying more complex graph filters with multiple nodes and learnable adjacency matrix, and using more expressive and efficient graph kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time Complexity Analysis</head><p>Most MPNNs incur a time complexity of O(n 2 ), or O(m) if the adjacency matrix is sparse containing m non-zero entries, because updating the embedding of node v involves n v neighbors, where n v is the degree of node v. In KerGNNs, we apply graph kernel with the subgraph G v instead of the whole graph, so the computational complexity would be related to the complexity of each subgraph. For the subgraph G v with n v + 1 nodes and adjacency matrix with m v non-zero entries, we update the representation of node v by calculating the random walk kernel with Equation <ref type="formula" target="#formula_22">11</ref>in Appendix. This calculation takes a computation time of</p><formula xml:id="formula_18">O(P d(d n GF (n GF + n v + 1) + m v ))</formula><p>, where P is the maximum length of the random walk, d and d are the node dimensions of the current layer and next layer, n GF is the number of nodes in each graph filter. In an undirected subgraph, m v represents the number of edges and will be greater than n v and smaller than n v (n v − 1)/2. If we sum up the computation time for all the nodes in the entire graph, the time complexity of KerGNNs will range between O(n 2 ) and worst-case scenario (fully-connected graph) O(n 3 ). We experimentally compare the running time of the proposed model with several GNN benchmarks. As shown in Table <ref type="table" target="#tab_2">3</ref> in Appendix, KerGNNs achieve better or similar running time compared to the fastest benchmark method, and much less running time than higher-order GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate the proposed model on graph classification task and node classification task (discussed in Appendix), and we also show the model interpretability by visualizing the graph filters in the trained models as well as the output graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Settings</head><p>Datasets. We evaluate our proposed KerGNN model on 8 publicly available graph classification datasets. Specifically, we use DD <ref type="bibr" target="#b7">(Dobson and Doig 2003)</ref>, PROTEINS <ref type="bibr">(Borgwardt et al. 2005)</ref>, NCI1 <ref type="bibr" target="#b24">(Schomburg et al. 2004</ref>), ENZYMES <ref type="bibr" target="#b24">(Schomburg et al. 2004</ref>) for binary and multiclass classification of biological and chemical compounds, and we also use the social datasets IMDB-BINARY, IMDB-MULTI, REDDIT-BINARY, and COLLAB <ref type="bibr" target="#b33">(Yanardag and Vishwanathan 2015)</ref>.</p><p>Setup. To make a fair comparison with state-of-the-art GNNs, we follow the cross-validation procedure described in <ref type="bibr" target="#b9">Errica et al. (2019)</ref>. We use a 10-fold cross-validation for model assessment and an inner holdout technique with a 90%/10% training/validation split for model selection, following the same dataset index splits as <ref type="bibr" target="#b9">Errica et al. (2019)</ref>. Besides, we use Adam optimizer with an initial learning rate of 0.01 and decay the learning rate by half in every 50 epochs. For the four social datasets, we use node degrees as the input attributes for each node, and for the four bio/chemical datasets, we use node labels or attributes as the input feature for each node. Hyper-parameters. The hyper-parameters that we tune for each dataset include the learning rate, the dropout rate, the number of layers of KerGNNs and MLP, the number of graph filters at each layer, the number of nodes in each graph filter, the number of nodes for each subgraph, and the hidden dimension of each KerGNN layer. For the random walk kernel, we also tune the length of random walks. Baseline Models. We consider the KerGNN model with single and multiple KerGNN layers, namely KerGNN-L, corresponding to KerGNN model with L layers, and KerGNN-L-DRW representing the model deploying the deep random walk kernel. We also compare our models with widely-used GNNs: DGCNN <ref type="bibr" target="#b36">(Zhang et al. 2018</ref>), Diff-Pool <ref type="bibr" target="#b35">(Ying et al. 2018)</ref>, ECC (Simonovsky and Komodakis 2017), GIN <ref type="bibr" target="#b32">(Xu et al. 2018a</ref>), GraphSAGE <ref type="bibr" target="#b11">(Hamilton, Ying, and Leskovec 2017)</ref>, RWGNN (Nikolentzos and Vazirgiannis 2020), GCKN (Chen, Jacob, and Mairal 2020), and two high-order GNNs: 1-2-3 GNN <ref type="bibr" target="#b15">(Morris et al. 2019</ref>) and Powerful <ref type="bibr">GNN (Maron et al. 2019)</ref>. Part of the results for these baseline GNNs are taken from <ref type="bibr" target="#b9">Errica et al. (2019)</ref>, and we run GCKN, 1-2-3 GNN and Powerful GNN using the official implementations. In addition, we also compare the proposed KerGNN model with three popular GNN-unrelated graph kernels: shortest path (SP) kernel <ref type="bibr">(Borgwardt and Kriegel 2005)</ref>, propagation (PK) kernel <ref type="bibr" target="#b17">(Neumann et al. 2016)</ref>, the Weisfeiler-Lehman subtree (WL-sub) kernel <ref type="bibr" target="#b26">(Shervashidze et al. 2011</ref>) and GNN-related GNTK <ref type="bibr" target="#b8">(Du et al. 2019)</ref>. We use the GraKeL library <ref type="bibr" target="#b28">(Siglidis et al. 2020)</ref> to implement these graph kernels and run GNTK using the official implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The graph classification results are shown in Table <ref type="table" target="#tab_0">1</ref>, with the best results highlighted in bold. We can see that the proposed models achieve superior performance than conventional GNNs with 1-WL limits, and achieve similar performance compared with high-order GNNs, with less running time. The single-layer KerGNN model performs well on small graphs like IMDB social datasets. For larger graphs, deeper models with more layers or with deep random walk kernel perform better. We show more experimental results, model parameter studies, and node classification results in Appendix. The optimal parameters of the graph filter are different for different datasets, depending on the local structures of different types of graphs, e.g., the star patterns in graphs of REDDIT-B and the ring and chain patterns in graphs of NCI1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Interpretability</head><p>Visualizing the filters in CNNs gives insights into what features CNNs focus on. Following the same idea, we can also  KerGNNs and other standard GNNs can generate output graphs with updated node attributes, and we can extract important nodes for the classification tasks using relative attribute values, which is shown in output graphs in Figure <ref type="figure" target="#fig_3">2</ref>. We can make several observations from the output MUTAG chemical structures: 1) The carbon atoms at the connection points of rings are more important than those connected with atom groups, which are more important than those at the re-maining positions. 2) The atoms in the atom group are always less important than those carbon atoms in the carbon ring.</p><p>Compared to standard GNN variants, KerGNNs have graph filters as extra information to help explain the predictions of the model. To visualize the graph filters, we extract the adjacency matrix and the attribute matrix for each graph filter from the trained KerGNN layer. We then adopt the ReLU functions to prune the unimportant edges. In Figure <ref type="figure" target="#fig_3">2</ref>, we use different sizes of nodes to denote the relative importance of nodes. For the MUTAG dataset, we can see most of the graph filters have ring structures, similar to the carbon rings at the input graphs, and some graph filters have small connected rings, similar to the concatenated carbon rings. It should be noted that the number of nodes in the rings of graph filters may not be equal to 6 because we limit the total number of nodes to be 8. KerGNN layers utilize these rings in the graph filter to match against the local structural patterns (e.g., carbon rings) in the input graphs, using the graph kernels. This indicates the importance of carbon rings in the mutagenic effect, which also corresponds to our observations in the output graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we have proposed Kernel Graph Neural Networks (KerGNNs), a new graph neural network framework that is not restricted to the theoretical limits of the message passing aggregation. KerGNNs are inspired by several characteristics of CNNs and can be seen as a natural extension of CNNs in the graph domain, from the viewpoint of the kernel methods. KerGNNs achieve competitive performance on a variety of datasets compared with several GNNs and graph kernels, and can provide improved explainability and transparency by visualizing the graph filters and output graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Calculation of Random Walk Kernel</head><p>To calculate the random walk kernel between two graphs, we have to calculate Equation 3 in the main paper:</p><formula xml:id="formula_19">K p (G 1 , G 2 ) = s T A p × s,<label>(8)</label></formula><p>where A × = A 2 ⊗ A 1 , ⊗ denotes the Kronecker product between two matrices, s = vec(S), and vec(•) denotes flattening a matrix into a vector by stacking all the columns. Besides S = X 1 X T 2 ∈ R n1×n2 . In the following, we assume G 1 and G 2 are undirected graphs, which is the case for all the datasets we use in the experiments. According to the properties of Kronecker product</p><formula xml:id="formula_20">A p × = A p 2 ⊗ A p 1 ,<label>(9)</label></formula><p>and</p><formula xml:id="formula_21">vec(AXB) = (B T ⊗ A)vec(X),<label>(10)</label></formula><p>we can calculate Equation <ref type="formula" target="#formula_19">8</ref>as</p><formula xml:id="formula_22">K p (G 1 , G 2 ) = s T A p × s = s T (A p 2 ⊗ A p 1 )s = vec X 1 X T 2 T (A p 2 ⊗ A p 1 )vec X 1 X T 2 = vec X 1 X T 2 T vec (A p 1 ) T X 1 X T 2 A p 2 = vec X 1 X T 2 T vec A p 1 X 1 (A p 2 X 2 ) T = n1 i=1 n2 j=1 X 1 X T 2 A p 1 X 1 (A p 2 X 2 ) T ij ,<label>(11)</label></formula><p>where means Hadamard (element-wise) product, In the KerGNN model, G 1 and G 2 represent the graph filter and the input graph, respectively, and we can use Equation <ref type="formula" target="#formula_22">11</ref>to avoid calculating the direct product graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proofs Proof of Theorem 1</head><p>Let A be a graph neural network which satisfies condition a) and b). We first prove the first part of conclusions that A can map any graphs G and H that 1-WL test decides as non-isomorphic to different embeddings. Suppose starting from iteration l, the 1-WL test decides G and H are non-isomorphic (before that, 1-WL test cannot distinguish two graphs), but graph neural network maps them to the same embeddings A(G) = A(H). This indicates that G and H always have the same labels for iteration i − 1 and i for any i = 1, ..., l − 1 in the 1-WL test. Next we hope to reach a contradiction to this statement. To find this contradiction, we first show on graph G or H, if node features in the graph neural network φ i (v 1 ) = φ i (v 2 ), we always have 1-WL node labels a i (v 1 ) = a i (v 2 ) for any iteration i. This apparently holds for i = 0 because 1-WL and graph neural network start with the same node features. Suppose this holds for iteration j, if for any v 1 and v 2 , φ j+1 (v 1 ) = φ j+1 (v 2 ), then according to the node update rule in Theorem 1, we can get</p><formula xml:id="formula_23">u (φ j (v 1 ), f (Φ j (G v1 ))) = u (φ j (v 2 ), f (Φ j (G v2 ))) . (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>Because u and f are both injective, we then obtain</p><formula xml:id="formula_25">(φ j (v 1 ), Φ j (G v1 )) = (φ j (v 2 ), Φ j (G v2 )) . (<label>13</label></formula><formula xml:id="formula_26">) According to Lemma 1, if Φ j (G v1 ) = Φ j (G v2 ), then { {φ j (w), w ∈ V(G v1 )} } = { {φ j (w), w ∈ V(G v2 )} }, and because φ j (v 1 ) = φ j (v 2 ), we can get (φ j (v 1 ), { {φ j (w), w ∈ N (v 1 )} }) = (φ j (v 2 ), { {φ j (w), w ∈ N (v 2 )} }) . (<label>14</label></formula><formula xml:id="formula_27">)</formula><p>By our assumption at iteration j, we must have</p><formula xml:id="formula_28">(a j (v 1 ), { {a j (w), w ∈ N (v 1 )} }) = (a j (v 2 ), { {a j (w), w ∈ N (v 2 )} }) . (<label>15</label></formula><formula xml:id="formula_29">)</formula><p>Because the mapping in 1-WL test is injective with respect to the node label and the multiset of neighborhood labels, we get a j+1 (v 1 ) = a j+1 (v 2 ). By induction, if node features in the graph neural network φ i (v 1 ) = φ i (v 2 ), we always have 1-WL node labels a i (v 1 ) = a i (v 2 ) for any iteration i. This creates a valid mapping q such that a i (v) = q(φ i (v)) for any node v in the graph.</p><p>Because 1-WL decides graphs G and H as non-isomorphic, which means</p><formula xml:id="formula_30">{ {a l (v), v ∈ V(G)} } = { {a l (v), v ∈ V(H)} }, at layer l. With the mapping between a l (v) and φ l (v), we can get { {φ l (v), v ∈ V(G)} } = { {φ l (v), v ∈ V(H)} }.</formula><p>Because the graph-readout function of graph neural network is injective according to Theorem 1, we should get A(G) = A(H), which contradicts our assumption.</p><p>For the second part of the conclusion that there exist graph G and H that are decided as isomorphic by 1-WL test but nonisomorphic by subgraph-based graph neural network A, to prove it, we can just find an example that satisfies this. The example shown in Figure <ref type="figure" target="#fig_0">1</ref>(a) cannot be distinguished by 1-WL graph isomorphism test, but for the subgraph associated with each node, the random walk graph kernel can embed them to different embeddings, by interacting with an appropriate graph filter (e.g., a graph filter with one node).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 2</head><p>To find at least one feasible Φ(H), assume the length of non-zero vector Φ(G v ) is large but finite Φ(G v ) = [c 0 , c 1 , ..., c N ] with maximum absolute value c, then we can encode each value of Φ(G v ) with the base 2c according to their positions in the vector. Specifically, we can let Φ(H) = [(2c) 0 , (2c) 1 , ..., (2c) N ], and the inner product</p><formula xml:id="formula_31">K(H, G v ) = Φ(H), Φ(G v ) = N i=0 c i (2c) i will be injective with respect to Φ(G v ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections between KerGNNs and CNNs</head><p>In this section, we discuss the claim in the paper that KerGNNs generalize CNNs into the graph domain from the kernel's point of view. We show in the first subsection that 2-D image convolution in CNNs is equivalent to calculating the appropriate kernel function between patches and filters. Then we show in the second subsection that KerGNNs generalize this aggregation approach by introducing the counterparts of patch, filter and convolution in the graph regime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rethinking CNNs from the Kernel's Perspective</head><p>Standard CNN models use image convolution to aggregate the local information around each pixel. In this subsection, we show that the image convolution can be viewed as applying kernel functions between input patches and filters in the convolutional layers.</p><p>Given a convolutional layer in CNN, the input to the layer is an image I ⊂ Ω, where Ω ⊂ [0, 1] 2 is a set of pixel coordinates. Typically, Ω is a two-dimensional grid. We also define a feature mapping function to map every pixel in the image to a finite vector space, φ: I → R d , where d is the dimension of the input feature space. For each pixel q ∈ I with coordinate (x, y), we can find a neighborhood patch x q centered at the pixel q, with patch size r × r. φ(q) is the feature map of the pixel in R d , and with some abuse of notation, φ(x q ) is defined as the concatenation of feature maps of every pixel in the patch x q , i.e., φ(x q ) = [φ(q j )] r×r j=1 for every pixel q j ∈ x q . Thus, φ(x q ) lives in the vector space R d×r 2 . For example, given an RGB image as the input, φ(q) is in the Euclidean space R 3 , and φ(x q ) is the feature vector in R 3×r 2 . Next, we represent the output of the layer as a different image I ⊂ Ω with the feature mapping function φ : I → R d , where d is the dimension of the output feature space. As shown in Figure <ref type="figure" target="#fig_4">3</ref>(a), I and I may not have the same size, but every pixel q ∈ I corresponds to a pixel q ∈ I with an associated patch x q . The goal of the convolutional layer is then to learn this output feature mapping function φ . Specifically, the convolutional layer adopts d filters to perform 2-D convolution operation over the image. The i-th filter performs the dot product over each patch of the image with a fixed stride, and can be parameterized as a vector z i ∈ R d×r 2</p><p>. The output feature is obtained by computing the dot product of z i and φ(x q ) and followed by an element-wise nonlinear function σ. In other words, it is the i-th dimension of the feature representation φ (q ) of the pixel q in the new image I . The process can be written as follows:</p><formula xml:id="formula_32">φ i (q ) = σ(Φ * Z i ) = σ d t=1 r m,n=1 Φ[t, x − m, y − n]Z i [t, m, n] = σ( φ(x q ) , z i ) = σ(z T i φ(x q )),<label>(16)</label></formula><p>where * represents the image convolution, Φ and Z i are tensors with shape (d × r × r) reshaped from vectors φ(x q ) and z i , respectively. We omitted the bias term here for simplicity. Now, we rethink this process from the kernel perspective. According to the theory of kernel methods, each positive definite kernel function K implicitly defines a RKHS H. Next, we try to make this implicit RKHS to be the output feature space of this convolutional layer, by appropriately designing the associated kernel function. Here, we can define a simple dot-product RBF kernel function between the input feature map vector φ(x q ) and the filter vector z i as It is worth noting that RKHS H determined by this RBF kernel is of infinite dimensions, and thus we need to make the assumption here that H can be approximated by a finite output vector space, with a finite set of basis vectors which are defined as trainable filters {z i : i = 1, ..., d } and H = Span(z 1 , ..., z d ). This is similar to the assumption made by <ref type="bibr" target="#b14">(Mairal et al. 2014)</ref>.</p><formula xml:id="formula_33">K(φ(x q ), z i ) = exp(z T i φ(x q )). (<label>17</label></formula><formula xml:id="formula_34">)</formula><formula xml:id="formula_35">z i v Gv Hi v xq (a) (b) graph filter subgraph filter subgraph q' q CNN KerGNN</formula><p>It is noted that the output feature space can also be approximated by a subspace of H using Nyström method as in Mairal ( <ref type="formula">2016</ref>).</p><p>Then we can derive the output feature map of the pixel q by projecting the input feature map vector φ(x q ) into H, and thus the i-th dimension of the output feature map φ (q ) can be computed as the projection of φ(x q ) onto the i-th basis vector z i as</p><formula xml:id="formula_36">φ i (q ) = φ(x q ) , z i H = K(φ, z i ) = exp(z T i φ(x q )),<label>(18)</label></formula><p>where the second equality holds because of the kernel method. Then we obtain the formula to calculate the output feature mapping function from a perspective that is different from image convolution, and the result is similar to Equation <ref type="formula" target="#formula_32">16</ref>. The only difference is in the element-wise nonlinear activation function, and it is worth noticing that the exponential nonlinearity induced by the RBF kernel is very similar to the popular ReLu function σ(•) used in practice. Now we come to the conclusion that under suitable assumptions, the standard image convolution in CNN layers can be approximately interpreted as applying kernel functions between the input patches and the trainable filters, and we can use the computed kernel value to update the feature map of the pixel in the output space. This is our major motivation for the design of KerGNN. In KerGNN, we use the computed kernel values to update the feature map of the node in the output graph. This comparison of convolutional layers and KerGNN layers is shown in Figure <ref type="figure" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generalizing CNNs to KerGNNs</head><p>We explain in this subsection how we derive the update rule shown in Algorithm 1 inspired by CNNs. We consider a single KerGNN layer with an undirected graph G = (V, E) and node feature map of the input graph G can be characterised as φ 0 : V → R d0 as input.</p><p>We rely on the graph filters (as the counterpart of filters) {H</p><p>i , i = 1, ..., d 1 } to aggregation each node's subgraph (as the counterpart of the patch) and obtain φ 1 (v). Specifically, we consider using the R-convolution typed kernel function which implicitly defines an RKHS H 1 . Similar to the analysis of CNN from the kernel perspective (as discussed in the first subsection), we assume that H 1 can be approximated by the finite-dimensional space R d1 with a set of d 1 vectors {Φ(H</p><formula xml:id="formula_38">(1) i ), i = 1, ..., d 1 }, so that H 1 = Span(Φ(H (1) i ), ..., Φ(H (1)<label>d1</label></formula><p>)), Then, we calculate φ 1 (v) by projecting subgraph feature map Φ 0 (G v ) into H 1 , and the i-th dimension of φ 1 (v) can be calculated as the inner product between Φ 0 (G v ) and the basis vector Φ(H</p><formula xml:id="formula_39">(1) i ), i.e., φ 1,i (v) = Φ 0 (G v ), Φ(H (1) i ) H1 ≈ K(G v , H (1) i ),<label>(19)</label></formula><p>where we adopt random walk kernel as K. Similar to our conclusions for CNNs, Equation 19 indicates that we can use the kernel values computed by the subgraph of a node and graph filters as the output feature map of the node. After calculating the kernel value of the subgraph G v with respect to every graph filter {H</p><p>i , i = 1, ..., d 1 }, we obtain every dimension of the feature map of node v in H 1 , which is the output of the KerGNN layer.</p><p>We compare CNNs and KerGNNs in Figure <ref type="figure" target="#fig_4">3</ref>, and summarize the similarities in the following: Sliding over Inputs. In CNN, the filter is systematically applied with each filter-sized patch of the input image, from left to right and top to bottom, with a specified stride. In KerGNNs, we sample a subgraph G v = (V v , E v ) consisting of v and its j-hop neighbors, and the graph filter is applied to G v for ∀v ∈ V (the adjacency of G is preserved in the subgraph). It should be noted that the operations defined below do not require the subgraph and graph filter have the same number of nodes or topology.</p><p>Shared Parameters. In CNN, all the patches share the same filter for convolution to reduce the number of parameters. In KerGNNs, all the subgraphs G v for ∀v ∈ V also share the same graph filter, and thus the parameters of graph filters will not scale up as the input graph becomes larger.</p><p>Local Aggregation. In CNN, the patch consists of a central pixel and neighboring pixels, and the feature map of the patch φ(x p ) contains all the neighbors' information. The filters aggregate the patch and assign the corresponding kernel value to the output feature map of the central pixel. In KerGNNs, we use the graph filters to aggregate the feature map of the subgraph Φ(G v ), and let the kernel value be the output feature map of the central node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connections between KerGNNs and MPNNs</head><p>We show in this section that KerGNNs generalize the standard MPNNs. From the point view of KerGNNs, MPNNs deploy a simple graph filter with one node, and an appropriate kernel function can be chosen with KerGNN framework, such that KerGNNs can iteratively update nodes' representations using neighborhood aggregation like in MPNNs. For example, the vertex update rule of Graph Convolutional Network (GCN) (Kipf and Welling 2016) can be written as</p><formula xml:id="formula_41">φ l (u) = σ   W v∈N (u)∪{u} φ l−1 (v) |N (u)| • |N (v)|   ,<label>(20)</label></formula><p>where N (u) represents all the neighboring nodes of u, σ is the element-wise nonlinear function,</p><formula xml:id="formula_42">φ l−1 (v) ∈ R d l−1 and φ l (v) ∈ R d l .</formula><p>To interpret this updating rule in the KerGNN framework, we can define the subgraph G u containing nodes N (u) ∪ {u}.</p><p>We also define d l graph filters {H (l)</p><p>i , i = 1, ..., d l }, and each graph filter is defined as</p><formula xml:id="formula_43">H (l) i = ({h i }, {}).</formula><p>The attribute of the node h i is parameterized by W (l) i ∈ R 1×d l−1 . Then we define an R-convolution graph kernel as</p><formula xml:id="formula_44">K(G u , H (l) i ) = v∈N (u)∪{u} v ∈{hi} k base (v, v ) = v∈N (u)∪{u} k base (v, h i ),<label>(21)</label></formula><p>and the node-wise kernel k base is defined as</p><formula xml:id="formula_45">k base (v, h i ) = σ   W (l) i T φ l−1 (v) |N (u)| • |N (v)|   . (<label>22</label></formula><formula xml:id="formula_46">)</formula><p>Using the KerGNN framework, the i-th dimension of the output feature map φ l,i (u) can be written as the kernel function between G v and H</p><formula xml:id="formula_47">(l) i : φ l,i (u) = K(G u , H<label>(l)</label></formula><formula xml:id="formula_48">i ) = v∈N (u)∪{u} σ   W (l) i T φ l−1 (v) |N (u)| • |N (v)|   , = σ   W (l) i T v∈N (u)∪{u} φ l−1 (v) |N (u)| • |N (v)|   ,<label>(23)</label></formula><p>which is equivalent to Equation <ref type="formula" target="#formula_41">20</ref>. Therefore, the message aggregation in most MPNNs can be treated as using one-node graph filters in KerGNNs, and our proposed method generalizes MPNNs by deploying more complex graph filters with multiple nodes and learnable adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Implementation Details</head><p>To construct the subgraph for a node, we first calculate all the j-hop neighbors of the node, and extract the subgraph determined by the node and all its neighbors. In implementation, to be compatible with matrix multiplication, we set a maximum size of subgraphs. Any subgraph exceeding this limit is truncated and preserves nearer neighbors. The adjacency matrix of a subgraph that does not reach this limit will be padded with zeros. Therefore, the adjacency matrices of all the subgraphs have the same size.</p><p>For the first layer of the KerGNN model, we optionally add an additional linear mapping to transform node attributes of the input graph to a specified dimension (the dimension of node attributes of graph filter in the first layer), such that we can calculate the graph kernel between input graphs and graph filters at the first layer.</p><p>For simplicity, we define all the graph filters at the same layer to have the same number of nodes. Besides, for comparison, we only use one type of graph kernels within one model, although different graph filters can interact with same subgraphs with different types of graph kernels and thus different graph kernels can be mixed within one model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Details and More Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Classification Task</head><p>Hyper-parameter Search. We conduct the experiment using Intel(R) Core(TM) i7-7700 CPU @ 3.60GHz CPU with NVIDIA GPU (GeForce RTX 2070). We use grid search to select the hyper-parameters for each model during cross-validation, and the hyper-parameter search range is shown in Table <ref type="table" target="#tab_1">2</ref>. We use 4 social datasets: IMDB-BINARY and IMDB-MULTI are movie collaboration datasets. Each graph corresponds to an ego-network for each actor/actress, where nodes correspond to actors/actresses and an edge is drawn between two actors/actresses if they appear in the same movie. Each graph is derived from a pre-specified genre of movies, and the task is to classify the genre that the graph is derived from. IMDB-BINARY considers two genres: Action and Romance. IMDB-MULTI considers three classes: Comedy, Romance, and Sci-Fi. Each graph of REDDIT-BINARY dataset corresponds to an online discussion thread and nodes correspond to users. Two nodes are connected if at least one of them responded to another's comment. The task is to classify each graph to a community or a subreddit it belongs to. COLLAB is a scientific collaboration dataset, derived from 3 public collaboration datasets, namely, High Energy Physics, Condensed Matter Physics, and Astro Physics. Each graph corresponds to an ego-network of different researchers from each field. The task is to classify each graph to a field the corresponding researcher belongs to.</p><p>Model Parameters. We study how the test accuracy is influenced by the number of nodes in the graph filters, which determines the size and complexity of the graph filters. As shown in Figure <ref type="figure" target="#fig_5">4</ref>(a), the optimal size of the graph filter is different for different datasets, depending on the local structures of different types of graphs, e.g., the star patterns in graphs of REDDIT-B and the ring and chain patterns in graphs of NCI1. We also study the influence of the maximum length of random walks as shown in Figure <ref type="figure" target="#fig_5">4</ref>(b), it can be seen that longer walks generally benefit the classification results except for ENZYMES dataset where the model performs the best with walk of length 2. In the code implementation, we specify the maximum number of nodes that the subgraph can contain, which implicitly control the size of the subgraphs, and thus we also study the influence of this threshold of node numbers in Figure <ref type="figure" target="#fig_5">4</ref>(c). DD and ENZYMES achieve higher accuracy with larger subgraphs, because larger subgraph contains more fruitful neighborhood topology information. The remaining datasets are not influenced too much, because we fix the size of graph filters, and the model performance degrades when the subgraph size and graph filter size mismatch, for example, small graph filters cannot handle larger subgraphs.</p><p>Model Running Time. We compare the running time of the proposed model with several GNN benchmarks, as shown in Table <ref type="table" target="#tab_2">3</ref>. We measure the one-epoch running time averaged over 50 epochs, using the same GPU card. All the models are set to have hidden dimension 32 and 1 MLP layer. For KerGNNs, the number of nodes in the graph filter is set to 6 and the maximum subgraph size is set to 10. We observe that KerGNN gives much better running time compared with high-order GNN benchmarks, and achieves similar performance compared with conventional GNN models with 1-WL constraints. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Classification Task</head><p>We further evaluate the proposed KerGNN model for node classification task. we use 4 datasets: Cora, Citeseer, Pubmed <ref type="bibr" target="#b25">(Sen et al. 2008)</ref>, Chameleon <ref type="bibr" target="#b22">(Rozemberczki, Allen, and Sarkar 2021)</ref>. For each dataset, we randomly split nodes of each class into 60%, 20%, and 20% for training, validation and testing, and report the mean accuracy of all models on the test sets over 10 random splits. We compare our model with several popular GNNs including GCN, GAT <ref type="bibr" target="#b30">(Veličković et al. 2017)</ref>, GEOM-GCN <ref type="bibr" target="#b20">(Pei et al. 2020)</ref>, APPNP <ref type="bibr" target="#b14">(Klicpera, Bojchevski, and Günnemann 2018)</ref>, JKNet <ref type="bibr" target="#b32">(Xu et al. 2018b</ref>) and GCNII <ref type="bibr">(Chen et al. 2020)</ref>.</p><p>As shown in Table4, KerGNNs achieve similar or better results compared with SOTA baselines. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Visualizations</head><p>In this section, we show the graph filters trained for MUTAG dataset and REDDIT dataset in Figures <ref type="figure">5 and 6</ref>. We can see that the trained graph filters have different patterns for the two datasets, and each type of patterns reveals the characteristics of corresponding dataset. Specifically, graph filters for MUTAG tend to have ring and circular patterns, while graph filters for REDDIT tend to have star patterns. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) 1-WL graph isomorphism test cannot distinguish one hexagon and two triangles because of same neighborhood multisets, while subgraph-based method can find the difference based on different subgraph topologies. (b)The yellow shadow represents the subgraph of node v. After interacting with graph filters, the updated node is colored in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Layer l is parameterized by d l graph filters {H (l) i : i = 1, ..., d l }. Each graph filter H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Model visualization. Input graphs are drawn from (a) MUTAG and (b) REDDIT-B datasets with different node shapes corresponding to different atom types. In both graph filters and output graphs, node color represents relative attribute value. visualize the trained graph filters, which indicate some key structures of the input dataset. We visualize the graph filters trained with MUTAG 1 (Kersting et al. 2016) and REDDIT-B dataset in Figure 2. The MUTAG dataset consists of 188 chemical compounds divided into two classes according to their mutagenic effect on a bacterium. As shown in the input graphs in Figure 2(a), most of the MUTAG graphs in the dataset consist of ring structures with 6 carbon atoms.KerGNNs and other standard GNNs can generate output graphs with updated node attributes, and we can extract important nodes for the classification tasks using relative attribute values, which is shown in output graphs in Figure2. We can make several observations from the output MUTAG chemical structures: 1) The carbon atoms at the connection points of rings are more important than those connected with atom groups, which are more important than those at the re-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparisons of filters in CNN and KerGNNs. (a) The yellow grids represent the input image, and the pixels in the red box represent the patch around pixel q. The blue grids represent the filter. The position of pixel q in the output grid is denoted by q . (b) The graph denoted in the yellow shadow represents the subgraph of node v. The graph with blue shadow represents the graph filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Test accuracy w.r.t. model parameters. The results are obtained from experiments on 1 fold of datasets, and for all the three graphs only the model parameter on the x-axis is changed with remaining model parameters fixed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Graph filters of MUTAG</figDesc><graphic url="image-7.png" coords="16,104.40,124.90,403.20,403.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-8.png" coords="17,104.40,169.54,403.20,403.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test set classification accuracies (%). The mean accuracy and standard deviation are reported. Best performances are highlighted in bold. OOR means Out of Resources, either time or GPU memory.</figDesc><table><row><cell></cell><cell></cell><cell>DD</cell><cell cols="2">NCI1</cell><cell cols="2">PROTEINS ENZYMES</cell><cell>IMDB-B</cell><cell cols="2">IMDB-M REDDIT-B COLLAB</cell></row><row><cell cols="2"># GRAPHS</cell><cell>1178</cell><cell cols="2">4110</cell><cell>1113</cell><cell>600</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>5000</cell></row><row><cell cols="2"># CLASSES</cell><cell>2</cell><cell>2</cell><cell></cell><cell>2</cell><cell>6</cell><cell>2</cell><cell>3</cell><cell>2</cell><cell>3</cell></row><row><cell cols="2">AVG. # NODES</cell><cell>284</cell><cell cols="2">30</cell><cell>39</cell><cell>33</cell><cell>20</cell><cell>13</cell><cell>430</cell><cell>74</cell></row><row><cell>SP</cell><cell></cell><cell cols="3">78.7±3.8 66.3±2.6</cell><cell>71.9±6.1</cell><cell>25.0±5.6</cell><cell cols="2">57.5±5.4 40.5±2.8</cell><cell>75.5±2.1</cell><cell>58.4±1.3</cell></row><row><cell>PK</cell><cell></cell><cell cols="3">78.0±3.8 72.3±2.8</cell><cell>59.7±0.3</cell><cell>61.0±6.7</cell><cell cols="2">73.9±4.3 51.1±5.8</cell><cell>68.5±2.9</cell><cell>77.3±2.4</cell></row><row><cell>WL-SUB</cell><cell></cell><cell cols="3">77.5±3.5 79.5±3.3</cell><cell>74.8±3.2</cell><cell>51.2±5.3</cell><cell cols="2">72.5±4.6 51.5±5.8</cell><cell>67.2±4.2</cell><cell>77.5±2.4</cell></row><row><cell>GNTK</cell><cell></cell><cell>OOR</cell><cell cols="2">83.5±1.2</cell><cell>75.5±2.2</cell><cell>48.2±2.4</cell><cell cols="2">75.9±3.1 52.2±4.2</cell><cell>OOR</cell><cell>OOR</cell></row><row><cell>DGCNN</cell><cell></cell><cell cols="3">76.6±4.3 76.4±1.7</cell><cell>72.9±3.5</cell><cell>38.9±5.7</cell><cell cols="2">69.2±3.0 45.6±3.4</cell><cell>87.8±2.5</cell><cell>71.2±1.9</cell></row><row><cell cols="2">DIFFPOOL</cell><cell cols="3">75.0±3.5 76.9±1.9</cell><cell>73.7±3.5</cell><cell>59.5±5.6</cell><cell cols="2">68.4±3.3 45.6±3.4</cell><cell>89.1±1.6</cell><cell>68.9±2.0</cell></row><row><cell>ECC</cell><cell></cell><cell cols="3">72.6±4.1 76.2±1.4</cell><cell>72.3±3.4</cell><cell>29.5±8.2</cell><cell cols="2">67.7±2.8 43.5±3.1</cell><cell>OOR</cell><cell>OOR</cell></row><row><cell>GIN</cell><cell></cell><cell cols="3">75.3±2.9 80.0±1.4</cell><cell>73.3±4.0</cell><cell>59.6±4.5</cell><cell cols="2">71.2±3.9 48.5±3.3</cell><cell>89.9±1.9</cell><cell>75.6±2.3</cell></row><row><cell cols="2">GRAPHSAGE</cell><cell cols="3">72.9±2.0 76.0±1.8</cell><cell>73.0±4.5</cell><cell>58.2±6.0</cell><cell cols="2">68.8±4.5 47.6±3.5</cell><cell>84.3±1.9</cell><cell>73.9±1.7</cell></row><row><cell>RWGNN</cell><cell></cell><cell cols="3">77.6±4.7 73.9±1.3</cell><cell>74.7±3.3</cell><cell>57.6±6.3</cell><cell cols="2">70.8±4.8 48.8±2.9</cell><cell>90.4±1.9</cell><cell>71.9±2.5</cell></row><row><cell>GCKN</cell><cell></cell><cell cols="3">77.3±4.0 79.2±1.2</cell><cell>76.1±2.8</cell><cell>59.3±5.6</cell><cell cols="2">74.5±1.2 51.0±3.9</cell><cell>OOR</cell><cell>74.3±2.8</cell></row><row><cell cols="2">1-2-3 GNN</cell><cell>OOR</cell><cell cols="2">72.7±2.9</cell><cell>74.5±5.6</cell><cell>OOR</cell><cell cols="2">70.7±3.4 50.2±2.2</cell><cell>91.1±2.1</cell><cell>OOR</cell></row><row><cell cols="2">POWERFUL GNN</cell><cell>OOR</cell><cell cols="2">83.4±1.8</cell><cell>75.9±3.3</cell><cell>54.8±5.5</cell><cell cols="2">73.0±4.9 50.5±3.2</cell><cell>OOR</cell><cell>75.4±1.4</cell></row><row><cell cols="2">KERGNN-1</cell><cell cols="3">77.6±3.7 74.3±2.2</cell><cell>75.8±3.5</cell><cell>62.1±5.5</cell><cell cols="2">74.4±4.3 51.6±3.1</cell><cell>81.5±1.9</cell><cell>70.5±1.6</cell></row><row><cell cols="2">KERGNN-2</cell><cell cols="3">78.9±3.5 76.3±2.6</cell><cell>75.5±4.6</cell><cell>55.0±5.0</cell><cell cols="2">73.7±4.0 50.9±5.1</cell><cell>82.0±2.5</cell><cell>72.7±2.1</cell></row><row><cell cols="2">KERGNN-3</cell><cell cols="3">75.5±3.1 80.5±1.9</cell><cell>76.5±3.9</cell><cell>54.1±4.3</cell><cell cols="2">72.1±4.6 50.1±4.5</cell><cell>82.0±1.9</cell><cell>71.1±2.0</cell></row><row><cell cols="5">KERGNN-2-DRW 77.0±4.4 82.8±1.8</cell><cell>76.1±4.1</cell><cell>59.5±4.5</cell><cell cols="2">71.1±4.1 50.5±3.1</cell><cell>89.5±1.6</cell><cell>75.1±2.3</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>C N O</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell></row><row><cell>input graphs</cell><cell>graph filters</cell><cell>output graphs</cell><cell>input graphs</cell><cell>graph filters</cell><cell>output graphs</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Hyper-parameter search range. Details of Datasets. We use 5 bioinformatics datasets: MUTAG is a dataset of 188 mutagenic aromatic and heteroaromatic nitro compounds with 7 discrete labels. PROTEINS dataset uses secondary structure elements (SSEs) as nodes and two nodes are connected if they are neighbors in the amino-acid sequence or in 3D space. It has 3 discrete labels, representing helix, sheet, or turn. NCI1 is a dataset made publicly available by the National Cancer Institute (NCI) and is a subset of balanced datasets of chemical compounds screened for ability to suppress or inhibit the growth of a panel of human tumor cell lines. DD is a dataset of 1178 protein X-Ray structures. Two nodes in a protein are connected by an edge if they are less than 6 Angstroms apart. The prediction task is to classify the protein structures into enzymes and non-enzymes.</figDesc><table><row><cell>hidden dimension of the first layer</cell><cell>[8; 16; 32; 64]</cell></row><row><cell>number of graph filter</cell><cell>[16; 32; 64; 128]</cell></row><row><cell>number of nodes of graph filter</cell><cell>[2; 4; 6; 8; 10; 12; 14; 16; 18; 20]</cell></row><row><cell>maximum number of nodes for subgraph</cell><cell>[5; 10; 15; 20; 25; 30]</cell></row><row><cell>j-hop neighborhood</cell><cell>[1; 2; 3]</cell></row><row><cell>maximum step for random walk</cell><cell>[1; 2; 3; 4; 5]</cell></row><row><cell>hidden dimension of linear layer</cell><cell>[8; 16; 32; 48; 64]</cell></row><row><cell>dropout rate</cell><cell>[0.2; 0.4; 0.6; 0.8]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The measured one-epoch running time (s) of different GNN models.</figDesc><table><row><cell></cell><cell>DD</cell><cell>NCI1</cell><cell>PROTEINS</cell><cell>ENZYMES</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>REDDIT-B</cell><cell>COLLAB</cell></row><row><cell>DGCNN</cell><cell>0.271±0.005</cell><cell>0.425±0.009</cell><cell cols="4">0.134±0.006 0.069±0.002 0.113±0.007 0.161±0.008</cell><cell>0.411±0.010</cell><cell>0.824±0.053</cell></row><row><cell>DIFFPOOL</cell><cell>2.887±0.029</cell><cell>4.232±0.035</cell><cell cols="4">1.125±0.006 0.131±0.007 0.210±0.031 0.295±0.013</cell><cell>9.580±0.233</cell><cell>2.296±0.088</cell></row><row><cell>ECC</cell><cell>110.14± 4.31</cell><cell>1.179±0.012</cell><cell cols="4">0.550±0.012 0.233±0.005 0.254±0.007 0.304±0.050</cell><cell>OOR</cell><cell>OOR</cell></row><row><cell>GIN</cell><cell>0.344±0.003</cell><cell>0.443±0.007</cell><cell cols="4">0.133±0.006 0.072±0.004 0.118±0.032 0.160±0.008</cell><cell>0.740±0.053</cell><cell>0.809±0.044</cell></row><row><cell>GRAPHSAGE</cell><cell>0.141±0.002</cell><cell>0.331±0.014</cell><cell cols="4">0.088±0.003 0.047±0.002 0.081±0.004 0.122±0.009</cell><cell>0.198±0.011</cell><cell>0.589±0.026</cell></row><row><cell>1-2-3 GNN</cell><cell>OOR</cell><cell>1.228±0.075</cell><cell>1.056±0.058</cell><cell>OOR</cell><cell cols="3">1.301±0.071 1.754±0.075 0.608±0.025 2</cell><cell>OOR</cell></row><row><cell>POWERFUL GNN</cell><cell>OOR</cell><cell>19.764±0.270</cell><cell>4.24±0.23</cell><cell cols="3">1.997±0.150 2.701±0.084 3.417±0.092</cell><cell>OOR</cell><cell>17.578±0.621</cell></row><row><cell>KERGNN-1</cell><cell>0.732±0.041</cell><cell>0.445±0.033</cell><cell cols="4">0.183±0.034 0.370±0.030 0.092±0.039 0.086±0.026</cell><cell>0.865±0.051</cell><cell>1.336±0.031</cell></row><row><cell>KERGNN-2</cell><cell>1.486±0.078</cell><cell>0.688±0.038</cell><cell cols="4">0.373±0.043 0.085±0.019 0.148±0.044 0.225±0.036</cell><cell>1.677±0.051</cell><cell>2.634±0.047</cell></row><row><cell>KERGNN-3</cell><cell>2.748±0.078</cell><cell>0.876±0.044</cell><cell cols="4">0.553±0.026 0.126±0.024 0.196±0.019 0.343±0.030</cell><cell>2.551±0.065</cell><cell>3.944±0.046</cell></row><row><cell>KERGNN-DRW</cell><cell>1.782±0.051</cell><cell>0.753±0.034</cell><cell cols="4">0.405±0.045 0.090±0.025 0.155±0.036 0.240±0.036</cell><cell>2.078±0.060</cell><cell>2.648±0.039</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Node classification accuracy.</figDesc><table><row><cell></cell><cell cols="4">CORA CITESEER PUBMED CHAMELEON</cell></row><row><cell>GCN</cell><cell>85.77</cell><cell>73.68</cell><cell>88.13</cell><cell>28.18</cell></row><row><cell>GAT</cell><cell>86.37</cell><cell>74.32</cell><cell>87.62</cell><cell>42.93</cell></row><row><cell cols="2">GEOM-GCN 85.27</cell><cell>77.99</cell><cell>90.05</cell><cell>60.90</cell></row><row><cell>APPNP</cell><cell>87.87</cell><cell>76.53</cell><cell>89.40</cell><cell>54.30</cell></row><row><cell>JKNET</cell><cell>87.46</cell><cell>76.83</cell><cell>89.18</cell><cell>62.08</cell></row><row><cell>GCNII</cell><cell>88.49</cell><cell>77.08</cell><cell>89.57</cell><cell>60.61</cell></row><row><cell>KERGNN</cell><cell>87.96</cell><cell>76.61</cell><cell>89.53</cell><cell>62.28</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We use the MUTAG dataset for visualization due to its easily interpretable structure. However, we do not use this dataset in cross-validation because its number of graphs is too small.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was partially supported by the U.S. Office of Naval Research under Grant N00173-21-1-G006, the U.S. National Science Foundation AI Institute Athena under Grant CNS-2112562, and the U.S. Army Research Laboratory and the U.K. Ministry of Defence under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of U.S. Office of Naval Research, the U.S. National Science Foundation, the U.S. Army Research Laboratory, the U.S. Government, the U.K. Ministry of Defence or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixhop: Higher-order graph convolutional architectures via sparsified neighborhood mixing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ver Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Explainability techniques for graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13686</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shortest-path kernels on graphs</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifth IEEE international conference on data mining (ICDM&apos;05</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional kernel networks for graph-structured data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1576" to="1586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Supervised community detection with line graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08415</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13192</idno>
		<title level="m">Graph neural tangent kernel: Fusing graph neural networks with graph kernels</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A fair comparison of graph neural networks for graph classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Podda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.09893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wrobel</surname></persName>
		</author>
		<author>
			<persName><surname>Springer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2003">2003. 2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
	<note>Learning theory and kernel machines</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02216</idno>
	</analytic>
	<monogr>
		<title level="m">Haussler, D. 1999</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of California . . .</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Convolution kernels on discrete structures</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th international conference on machine learning (ICML-03)</title>
				<meeting>the 20th international conference on machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Benchmark Data Sets for Graph Kernels</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Groß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pmlr. Leman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05997</idno>
		<idno>arXiv:1905.11136</idno>
	</analytic>
	<monogr>
		<title level="m">Mairal, J. 2016. End-to-end kernel learning with supervised convolutional kernel networks</title>
				<meeting><address><addrLine>Lei, T; Maron, H</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1968">2018. 2020. 2020. 2017. 1968. 2014. 2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Provably powerful graph networks</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Navarin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sperduti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.06930</idno>
		<title level="m">Pretraining graph neural networks with kernels</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Propagation kernels: efficient graph kernels from propagated information</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bauckhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="209" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph Homomorphism Convolution</title>
		<author>
			<persName><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Maehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7306" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kernel graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meladianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-P</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nikolentzos, G.; and Vazirgiannis, M. 2020. Random Walk Graph Neural Networks. In Conference on Neural Information Processing System</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
	<note>International Conference on Artificial Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">.-C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05287</idno>
		<title level="m">Geom-gcn: Geometric graph convolutional networks</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explainability methods for graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Pope</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rostami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10772" to="10781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiscale attributed node embedding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10261</idno>
		<title level="m">Approximation ratios of graph neural networks for combinatorial problems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BRENDA, the enzyme database: updates and major new developments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Schomburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ebeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gremse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Heldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schomburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic acids research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="D431" to="D433" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>suppl 1</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Weisfeilerlehman graph kernels</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient graphlet kernels for large graph comparison</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">GraKeL: A Graph Kernel Library in Python</title>
		<author>
			<persName><forename type="first">G</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Limnios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skianis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">54</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast computation of graph kernels</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018a. 2018b</date>
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>How powerful are graph neural networks?</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName><forename type="first">P</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
				<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">9240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning with differentiable pooling</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09691</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
				<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Link prediction based on graph neural networks</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
