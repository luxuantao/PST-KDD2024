<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-06-18">18 Jun 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
							<email>shchur@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
							<email>mumme@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
							<email>a.bojchevski@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Günnemann</surname></persName>
							<email>guennemann@in.tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pitfalls of Graph Neural Network Evaluation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-06-18">18 Jun 2019</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1811.05868v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised node classification in graphs is a fundamental problem in graph mining, and the recently proposed graph neural networks (GNNs) have achieved unparalleled results on this task. Due to their massive success, GNNs have attracted a lot of attention, and many novel architectures have been put forward. In this paper we show that existing evaluation strategies for GNN models have serious shortcomings. We show that using the same train/validation/test splits of the same datasets, as well as making significant changes to the training procedure (e.g. early stopping criteria) precludes a fair comparison of different architectures. We perform a thorough empirical evaluation of four prominent GNN models and show that considering different splits of the data leads to dramatically different rankings of models. Even more importantly, our findings suggest that simpler GNN architectures are able to outperform the more sophisticated ones if the hyperparameters and the training procedure are tuned fairly for all models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semi-supervised node classification in graphs is a classic problem in graph mining with applications ranging from e-commerce to computational biology. The recently proposed graph neural network architectures have achieved unprecedented results on this task and significantly advanced the state of the art. Despite their massive success, we cannot accurately judge the progress being made due to certain problematic aspects of the empirical evaluation procedures. We can partially attribute this to the practice of replicating the experimental settings from earlier works, since they are perceived as standard. First, a number of proposed models have all been tested exclusively on the same train/validation/test splits of the same three datasets (CORA, CiteSeer and PubMed) from <ref type="bibr" target="#b15">Yang et al. [2016]</ref>. Such experimental setup favors the model that overfits the most and defeats the main purpose of using a train/validation/test split -finding the model with the best generalization properties <ref type="bibr" target="#b2">[Friedman et al., 2001]</ref>. Second, when evaluating performance of a new model, people often use a training procedure that is rather different from the one used for the baselines. This makes it difficult to identify whether the improved performance comes from (a) a superior architecture of the new model, or (b) a better-tuned training procedure and / or hyperparameter configuration that unfairly benefits the new model <ref type="bibr" target="#b7">[Lipton and Steinhardt, 2018]</ref>.</p><p>In this paper we address these issues and perform a thorough experimental evaluation of four prominent GNN architectures on the transductive semi-supervised node classification task. We implement the four models -GCN <ref type="bibr" target="#b5">[Kipf and Welling, 2017]</ref>, MoNet <ref type="bibr" target="#b11">[Monti et al., 2017]</ref>, GraphSage <ref type="bibr" target="#b3">[Hamilton et al., 2017]</ref> and GAT <ref type="bibr" target="#b14">[Velickovic et al., 2018]</ref> -within the same framework. 1 In our evaluation we focus on two aspects: We use a standardized training and hyperparameter selection procedure for all models. In such a setting, the differences in performance can with high certainty be attributed to the differences in model architectures, not other factors. Second, we perform experiments on four well-known citation network datasets, as well as introduce four new datasets for the node classification problem. For each dataset we use 100 random train/validation/test splits and perform 20 random initializations for each split. This setup allows us to more accurately assess the generalization performance of different models, and does not just select the model that overfits one fixed test set.</p><p>Before we continue, we would like to make a disclaimer, that we do not believe that accuracy on benchmark datasets is the only important characteristic of a machine learning algorithm. Developing and generalizing the theory for existing methods, establishing connections to (and adapting ideas from) other fields are important research directions that move the field forward. However, thorough empirical evaluation is crucial for understanding the strengths and limitations of different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>We consider the problem of semi-supervised transductive node classification in a graph, as defined in <ref type="bibr" target="#b15">Yang et al. [2016]</ref>. In this paper we compare the four following popular graph neural network architectures. Graph Convolutional Network (GCN) <ref type="bibr" target="#b5">[Kipf and Welling, 2017]</ref> is one of the earlier models that works by performing a linear approximation to spectral graph convolutions. Mixture Model Network (MoNet) <ref type="bibr" target="#b11">[Monti et al., 2017]</ref> generalizes the GCN architecture and allows to learn adaptive convolution filters. The authors of Graph Attention Network (GAT) <ref type="bibr" target="#b14">[Velickovic et al., 2018]</ref> propose an attention mechanism that allows to weigh nodes in the neighborhood differently during the aggregation step. Lastly, GraphSAGE <ref type="bibr" target="#b3">[Hamilton et al., 2017]</ref> focuses on inductive node classification, but can also be applied for transductive setting. We consider 3 variants of the GraphSAGE model from the original paper, denoted as GS-mean, GS-meanpool and GS-maxpool.</p><p>The original papers and reference implementations of all above-mentioned models consider different training procedures including different early stopping strategies, learning rate decay, full-batch vs. mini-batch training (a more detailed description is provided in Appendix A). Such diverse experimental setups makes it hard to empirically identify the driver behind the improved performance <ref type="bibr" target="#b7">[Lipton and Steinhardt, 2018]</ref>. Thus, in our experiments we use a standardized training and hyperparameter tuning procedure for all models (more details in Sec. 3) to perform a more fair comparison.</p><p>In addition, we consider four baseline models. Logistic Regression (LogReg) and Multilayer Perceptron (MLP) are attribute-based models that do not consider the graph structure. Label Propagation (LabelProp) and Normalized Laplacian Label Propagation (LabelProp NL) <ref type="bibr" target="#b1">[Chapelle et al., 2009]</ref>, on the other hand, only consider the graph structure and ignore the node attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>Datasets For our experiments, we used the four well-known citation network datasets: PubMed <ref type="bibr" target="#b12">[Namata et al., 2012]</ref>, CiteSeer and CORA from <ref type="bibr" target="#b13">Sen et al. [2008]</ref>, as well as the extended version of CORA from <ref type="bibr" target="#b0">Bojchevski and Günnemann [2018]</ref>, denoted as CORA-Full. We also introduce four new datasets for the node classification task: Coauthor CS, Coauthor Physics, Amazon Computers and Amazon Photo. Descriptions of these new datasets, as well as statistics for all datasets can be found in Appendix B. For all datasests, we treat the graphs as undirected and only consider the largest connected component.</p><p>Setup We keep the model architectures as they are in the original papers / reference implementations. This includes the type and sequence of layers, choice of activation functions, placement of dropout, and choices as to where to apply L 2 regularization. We also fixed the number of attention heads for GAT to 8 and the number of Gaussian kernels for MoNet to 2, as proposed in the respective papers. All the models have 2 layers (input features → hidden layer → output layer).</p><p>For a more balanced comparison, however, we use the same training procedure for all the models. That is, we used the same optimizer (Adam <ref type="bibr" target="#b4">[Kingma and Ba, 2015]</ref> with default parameters), same initialization (weights initialized according to <ref type="bibr">Glorot and Bengio [2010]</ref>, biases initialized with zeros), no learning rate decay, same maximum number of training epochs, early stopping criterion, patience and validation frequency (display step) for all models (Appendix C). We optimize all model parameters (attention weights for GAT, kernel parameters for MoNet, weight matrices for all models) simultaneously. In all cases we use full-batch training (using all nodes in the training set every epoch). For each dataset, the highest accuracy score is marked in bold. N/A stands for the dataset that couldn't be processed by the full-batch version of GS-maxpool because of GPU RAM limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CORA</head><p>Lastly, we used the exact same strategy for hyperparameter selection for every model. We performed an extensive grid search for learning rate, size of the hidden layer, strength of the L 2 regularization, and dropout probability (Appendix C). We restricted the random search space to ensure that every model has at most the same given number of trainable parameters. For every model, we picked the hyperparameter configuration that achieved the best average accuracy on Cora and CiteSeer datasets (averaged over 100 train/validation/test splits and 20 random initializations for each). The chosen best-performing configurations were used for all subsequent experiments and are listed in Table <ref type="table" target="#tab_6">4</ref>. In all cases, we use 20 labeled nodes per class as the training set, 30 nodes per class as the validation set, and the rest as the test set.</p><p>Results Table <ref type="table" target="#tab_0">1</ref> shows mean accuracies (and their standard deviations<ref type="foot" target="#foot_0">2</ref> ) of all models for all 8 datasets averaged over 100 splits and 20 random initializations for each split. There are a few observations to be made. First, the GNN-based approaches (GCN, MoNet, GAT, GraphSAGE) significantly outperform all the baselines (MLP, LogReg, LabelProp, LabelProp NL) across all the datasets. This matches our intuition and confirms the superiority of GNN-based approaches that combine both the structural and attribute information compared to methods considering only the attributes or only the structure.</p><p>Among the GNN approaches, there is no clear winner that dominates across all the datasets. In fact, for 5 out of 8 datasets, scores of the 2nd and 3rd best approaches are less than 1% away from the average score of the best-performing method.  over 20 initializations) as 100%. Then, the score of each model is divided by this number, and the results for each model are averaged over all the datasets and splits. We also rank algorithms by their performance (1 = best performance, 10 = worst), and compute the average rank across all datasets and splits for each algorithm. The final scores are reported in Table <ref type="table" target="#tab_2">2a</ref>. We observe that GCN is able to achieve the best performance across all models. While this result seems surprising, similar findings have been reported in other fields. Simpler models often outperform more sophisticated ones if hyperparameter tuning is performed equally carefully for all methods <ref type="bibr" target="#b10">[Melis et al., 2018</ref><ref type="bibr" target="#b8">, Lucic et al., 2017]</ref>. In future work, we plan to further investigate what are the specific properties of the graphs that lead to the differences in performance of the GNN models.</p><p>Another surprising finding is the relatively lower score and high variance in results obtained by GAT for the Amazon Computers and Amazon Photo datasets. To investigate this phenomenon, we additionally visualize the accuracy scores achieved by different models on the Amazon Photo dataset in Figure <ref type="figure">2</ref> in the appendix. While the median scores for all GNN models are very close to each other, GAT produces extremely low scores (below 40%) for some weight initializations. While these outliers occur rarely (for 138 out of 2000 runs), they significantly lower the average score of GAT.</p><p>Effect of the train/validation/test split To demonstrate the effect of different train/validation/test splits on the performance, we execute the following simple experiment. We run the 4 models on the datasets and respective splits from <ref type="bibr" target="#b15">[Yang et al., 2016]</ref>. As shown in Table <ref type="table" target="#tab_2">2b</ref>, GAT achieves the best scores for the CORA and CiteSeer datasets, and GCN gets the top score for PubMed. If we, however, consider a different random split with the same train/validation/test set sizes the ranking of models is completely different, with GCN being first on CORA and CiteSeer, and MoNet winning on PubMed. This shows how fragile and misleading results obtained on a single split can be. Taking further into account that the predictions of GNNs can greatly change under small data perturbations <ref type="bibr" target="#b16">[Zügner et al., 2018]</ref> clearly confirms the need for evaluation strategies based on multiple splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We have performed an empirical evaluation of four state-of-the-art GNN architectures on the node classification task. We introduced four new attributed graph datasets, as well as open-sourced a framework that enables a fair and reproducible comparison of different GNN models. Our results highlight the fragility of experimental setups that consider only a single train/validation/test split of the data. We also find that, surprisingly, a simple GCN model can outperform the more sophisticated GNN architectures if the same hyperparameter selection and training procedures are used, and the results are averaged over multiple data splits. We hope that these results will encourage future works to use more robust evaluation procedures.</p><p>A Differences in training procedures for GNN models  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datasets description and statistics</head><p>Amazon Computers and Amazon Photo are segments of the Amazon co-purchase graph <ref type="bibr" target="#b9">[McAuley et al., 2015]</ref>, where nodes represent goods, edges indicate that two goods are frequently bought together, node features are bag-of-words encoded product reviews, and class labels are given by the product category.</p><p>Coauthor CS and Coauthor Physics are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge<ref type="foot" target="#foot_1">3</ref> . Here, nodes are authors, that are connected by an edge if they co-authored a paper; node features represent paper keywords for each author's papers, and class labels indicate most active fields of study for each author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classes Features Nodes</head><p>Edges The edge density describes the fraction of all possible edges that is present in the graph and can be computed as #edges / ( 1 2 • #nodes 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyperparameter configurations and Early Stopping</head><p>Grid search was performed over the following search space:  <ref type="table" target="#tab_6">1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1]</ref> We train for a maximum of 100k epochs. However, the actual training time is considerably shorter since we use strict early stopping. Specifically, with our unified early stopping criterion training stops if the total validation loss (loss on the data plus regularization loss) does not improve for 50 epochs.</p><p>Once training has stopped, we reset the state of the weights to the step with the lowest validation loss. GAT has two dropout probabilities (dropout on features / dropout on attention coefficients). All GraphSAGE models have additional weights for the skip connections (which effectively doubles the hidden size). GS-meanpool/GS-maxpool have two hidden sizes (hidden layer size / size of intermediary feature transformation). GAT uses a multi-head architecture with 8 heads and MoNet uses 2 heads, so the hidden state is split over 8 and 2 heads respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effective</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>GAT•</head><label></label><figDesc>Early stopping: stop optimization if neither the validation loss nor the validation accuracy improve for 100 epochs. • Full-batch training. • Maximum number of epochs: 100000. • Train set: 20 per class; validation set: 500 nodes; test set: 1000 (as in the Planetoid split). GraphSAGE • No early stopping. • Mini-batch training with batch size of 512. • Maximum number of epochs (each epoch consists of multiple mini-batches): 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean test set accuracy and standard deviation in percent averaged over 100 random train/validation/test splits with 20 random weight initializations each for all models and all datasets.</figDesc><table><row><cell></cell><cell></cell><cell>CiteSeer</cell><cell>PubMed</cell><cell>CORA Full</cell></row><row><cell>GCN</cell><cell>81.5 ± 1.3</cell><cell>71.9 ± 1.9</cell><cell>77.8 ± 2.9</cell><cell>62.2 ± 0.6</cell></row><row><cell>GAT</cell><cell>81.8 ± 1.3</cell><cell>71.4 ± 1.9</cell><cell>78.7 ± 2.3</cell><cell>51.9 ± 1.5</cell></row><row><cell>MoNet</cell><cell>81.3 ± 1.3</cell><cell>71.2 ± 2.0</cell><cell>78.6 ± 2.3</cell><cell>59.8 ± 0.8</cell></row><row><cell>GS-mean</cell><cell>79.2 ± 7.7</cell><cell>71.6 ± 1.9</cell><cell>77.4 ± 2.2</cell><cell>58.6 ± 1.6</cell></row><row><cell>GS-maxpool</cell><cell>76.6 ± 1.9</cell><cell>67.5 ± 2.3</cell><cell>76.1 ± 2.3</cell><cell>40.7 ± 1.5</cell></row><row><cell>GS-meanpool</cell><cell>77.9 ± 2.4</cell><cell>68.6 ± 2.4</cell><cell>76.5 ± 2.4</cell><cell>40.5 ± 1.5</cell></row><row><cell>MLP</cell><cell>58.2 ± 2.1</cell><cell>59.1 ± 2.3</cell><cell>70.0 ± 2.1</cell><cell>36.8 ± 1.0</cell></row><row><cell>LogReg</cell><cell>57.1 ± 2.3</cell><cell>61.0 ± 2.2</cell><cell>64.1 ± 3.1</cell><cell>40.5 ± 0.8</cell></row><row><cell>LabelProp</cell><cell>74.4 ± 2.6</cell><cell>67.8 ± 2.1</cell><cell>70.5 ± 5.3</cell><cell>50.5 ± 1.5</cell></row><row><cell>LabelProp NL</cell><cell>73.9 ± 1.6</cell><cell>66.7 ± 2.2</cell><cell>72.3 ± 2.9</cell><cell>51.0 ± 1.0</cell></row><row><cell></cell><cell>Coauthor</cell><cell>Coauthor</cell><cell>Amazon</cell><cell>Amazon</cell></row><row><cell></cell><cell>CS</cell><cell>Physics</cell><cell>Computer</cell><cell>Photo</cell></row><row><cell>GCN</cell><cell>91.1 ± 0.5</cell><cell>92.8 ± 1.0</cell><cell>82.6 ± 2.4</cell><cell>91.2 ± 1.2</cell></row><row><cell>GAT</cell><cell>90.5 ± 0.6</cell><cell>92.5 ± 0.9</cell><cell>78.0 ± 19.0</cell><cell>85.7 ± 20.3</cell></row><row><cell>MoNet</cell><cell>90.8 ± 0.6</cell><cell>92.5 ± 0.9</cell><cell>83.5 ± 2.2</cell><cell>91.2 ± 1.3</cell></row><row><cell>GS-mean</cell><cell>91.3 ± 2.8</cell><cell>93.0 ± 0.8</cell><cell>82.4 ± 1.8</cell><cell>91.4 ± 1.3</cell></row><row><cell>GS-maxpool</cell><cell>85.0 ± 1.1</cell><cell>90.3 ± 1.2</cell><cell>N/A</cell><cell>90.4 ± 1.3</cell></row><row><cell>GS-meanpool</cell><cell>89.6 ± 0.9</cell><cell>92.6 ± 1.0</cell><cell>79.9 ± 2.3</cell><cell>90.7 ± 1.6</cell></row><row><cell>MLP</cell><cell>88.3 ± 0.7</cell><cell>88.9 ± 1.1</cell><cell>44.9 ± 5.8</cell><cell>69.6 ± 3.8</cell></row><row><cell>LogReg</cell><cell>86.4 ± 0.9</cell><cell>86.7 ± 1.5</cell><cell>64.1 ± 5.7</cell><cell>73.0 ± 6.5</cell></row><row><cell>LabelProp</cell><cell>73.6 ± 3.9</cell><cell>86.6 ± 2.0</cell><cell>70.8 ± 8.1</cell><cell>72.6 ± 11.1</cell></row><row><cell>LabelProp NL</cell><cell>76.7 ± 1.4</cell><cell>86.8 ± 1.4</cell><cell>75.0 ± 2.9</cell><cell>83.9 ± 2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>If we were interested in comparing one model versus the rest, we could perform pairwise t-tests, as done in<ref type="bibr" target="#b6">Klicpera et al. [2019]</ref>. Since we are interested in comparing all the models to each other, we consider the relative accuracy of each model instead. For this, we take the best accuracy score for each split of each dataset (already averaged</figDesc><table><row><cell></cell><cell>Relative</cell><cell>Avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">accuracy</cell><cell>rank</cell><cell>Planetoid split</cell><cell>CORA</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>GCN</cell><cell>99.4</cell><cell>2.3</cell><cell>GCN</cell><cell cols="3">81.9 ± 0.8 69.5 ± 0.9 79.0 ± 0.5</cell></row><row><cell>MoNet</cell><cell>99.0</cell><cell>2.7</cell><cell>GAT</cell><cell cols="3">82.8 ± 0.5 71.0 ± 0.6 77.0 ± 1.3</cell></row><row><cell>GS-mean</cell><cell>98.3</cell><cell>2.7</cell><cell>MoNet</cell><cell cols="3">82.2 ± 0.7 70.0 ± 0.6 77.7 ± 0.6</cell></row><row><cell>GAT</cell><cell>95.9</cell><cell>3.6</cell><cell>GS-maxpool</cell><cell cols="3">77.4 ± 1.0 67.0 ± 1.0 76.6 ± 0.8</cell></row><row><cell>GS-meanpool</cell><cell>93.0</cell><cell>5.2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GS-maxpool</cell><cell>91.1</cell><cell>6.4</cell><cell>Another split</cell><cell>CORA</cell><cell>CiteSeer</cell><cell>PubMed</cell></row><row><cell>LabelProp NL LabelProp LogReg MLP</cell><cell>89.3 86.6 80.6 77.8</cell><cell>7.4 7.7 8.8 8.8</cell><cell>GCN GAT MoNet GS-maxpool</cell><cell cols="3">79.0 ± 0.7 68.6 ± 1.1 69.5 ± 1.0 77.9 ± 0.7 67.7 ± 1.2 69.5 ± 0.6 77.9 ± 0.7 66.8 ± 1.3 70.7 ± 0.5 74.5 ± 0.6 63.1 ± 1.2 70.3 ± 0.8</cell></row><row><cell cols="3">(a) Relative accuracy and average rank.</cell><cell cols="4">(b) Different split leads to a completely different ranking of models.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>(a)  Relative accuracy scores and ranks averaged over all datasets. See text for the definition. (b) Model accuracy on the Planetoid split from<ref type="bibr" target="#b15">Yang et al. [2016]</ref> and another split on the same datasets. Different splits lead to a completely different ranking of models.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Early stopping: stop optimization if the validation loss is larger than the mean of validation losses of the last 10 epochs.• Full-batch training.• Maximum number of epochs: 200.• Train set: 20 per class; validation set: 500 nodes; test set: 1000 (as in the Planetoid split). Maximum number of epochs: 3000 for CORA, 1000 for PubMed.• Train set: 20 per class; validation set: 500 nodes; test set: 1000 (as in the Planetoid split).• Alternating optimization of weight matrices and kernel parameters.• Learning rate decay at predefined iterations (only for CORA).</figDesc><table><row><cell>MoNet</cell></row><row><cell>• No early stopping.</cell></row><row><cell>• Full-batch training.</cell></row><row><cell>•</cell></row></table><note>GCN•</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics after standardizing the graphs, adding self-loops and removing classes with too few instances from CORA_full. We ignore 3 classes with less than 50 nodes in CORA-Full dataset (since we cannot perform the 20/30/rest split for them).</figDesc><table><row><cell>Label rate Edge density</cell></row></table><note>rate is the fraction of nodes in the training set. Since we use 20 training instances per class this can be computed as (#classes • 20) / #nodes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Best performing hyperparameter configurations for each model chosen by grid search.</figDesc><table><row><cell></cell><cell>hidden size</cell><cell>Learning rate</cell><cell>Dropout</cell><cell>L 2 reg. strength</cell><cell>Trainable weights</cell></row><row><cell>GCN</cell><cell>64</cell><cell>0.01</cell><cell>0.8</cell><cell>0.001</cell><cell>92K</cell></row><row><cell>GAT</cell><cell>64</cell><cell>0.01</cell><cell>0.6/0.3</cell><cell>0.01</cell><cell>92K</cell></row><row><cell>MoNet</cell><cell>64</cell><cell>0.003</cell><cell>0.7</cell><cell>0.05</cell><cell>92K</cell></row><row><cell>GS-mean</cell><cell>32</cell><cell>0.001</cell><cell>0.4</cell><cell>0.1</cell><cell>92K</cell></row><row><cell>GS-maxpool</cell><cell>32/32</cell><cell>0.001</cell><cell>0.3</cell><cell>0.005</cell><cell>94K</cell></row><row><cell>GS-meanpool</cell><cell>32/8</cell><cell>0.001</cell><cell>0.2</cell><cell>0.01</cell><cell>58K</cell></row><row><cell>MLP</cell><cell>64</cell><cell>0.005</cell><cell>0.8</cell><cell>0.01</cell><cell>92K</cell></row><row><cell>LogReg</cell><cell>-</cell><cell>0.1</cell><cell>-</cell><cell>0.0005</cell><cell>10K</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">Standard deviations are not the best representation of the variance of the accuracy scores, since the scores are not normally distributed. We still include the standard deviations to give the reader a rough idea of the variance of the results for each model. A more accurate picture is given by the box plots in Figure1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">https://kddcup2016.azurewebsites.net/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported by the German Research Foundation, Emmy Noether grant GU 1409/2-1.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure <ref type="figure">1</ref>: Boxplots of the test set accuracy of all models on all datasets over 100 random train/validation/test splits with 20 random weight initializations each. Note that a boxplot displays the median of the data as well as the 50% quantiles. Note further that outliers are excluded in these plots since some models have outliers very far from the median which would shrink the resolution of the plots. For a plot including the outliers refer to Figure <ref type="figure">2</ref>. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep gaussian embedding of attributed graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS</title>
				<imprint>
			<date type="published" when="2001">2001. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Predict then propagate: Graph neural networks meet personalized PageRank</title>
		<author>
			<persName><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Troubling trends in machine learning scholarship</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Steinhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03341</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10337</idno>
		<title level="m">Are GANs created equal? A large-scale study</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model CNNs. CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adversarial attacks on neural networks for graph data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zügner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akbarnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
