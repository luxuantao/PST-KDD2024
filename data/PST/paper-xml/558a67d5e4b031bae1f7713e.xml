<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">51853FD11E9E837C4722EC20EF657738</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Framework for Linear Information Inequalities Raymond W. Yeung, Senior Member, IEEE Abstract-We present a framework for information inequalities, namely, inequalities involving only Shannon's information measures, for discrete random variables. A region in IR 2 01 , denoted by 0 3 , is identified to be the origin of all information in- equalities involving n random variables in the sense that all such inequalities are partial characterizations of 0 3 . A product from this framework is a simple calculus for verifying all unconstrained and constrained linear information identities and inequalities which can be proved by conventional techniques. These include all information identities and inequalities of such types in the literature. As a consequence of this work, most identities and inequalities involving a definite number of random variables can now be verified by a software called ITIP which is available on the World Wide Web. Our work suggests the possibility of the existence of information inequalities which cannot be proved by conventional techniques. We also point out the relation between 0 3 and some important problems in probability theory and information theory.</p><p>Index Terms-Entropy, I-Measure, information identities, information inequalities, mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S HANNON'S information measures refer to entropies, con- ditional entropies, mutual informations, and conditional mutual informations. For information inequalities, we refer to those involving only Shannon's information measures for discrete random variables. These inequalities play a central role in converse coding theorems for problems in information theory with discrete alphabets. This paper is devoted to a systematic study of these inequalities. We begin our discussion by examining the two examples below which exemplify what we call the "conventional" approach to proving such inequalities.</p><p>Example 1: This is a version of the well-known data processing theorem. Let be random variables such that ----form a Markov chain. Then</p><p>In the above, the second equality follows from the Markov condition, while the inequality follows because is always nonnegative.</p><p>Manuscript received <ref type="bibr">August 10, 1995;</ref><ref type="bibr">revised February 10, 1997</ref>. The material in this paper was presented in part at the 1996 IEEE Information Theory Workshop, Haifa, Israel, June 9-13, 1996.</p><p>The author is with the Department of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong.</p><p>Publisher Item Identifier S 0018-9448(97)06816-8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2:</head><p>The inequalities above follow from the nonnegativity of , , and , respectively. In the conventional approach, we invoke certain elementary identities and inequalities in the intermediate steps of a proof. Some frequently invoked identities and inequalities are if ---if ----Proving an identity or an inequality using the conventional approach can be quite tricky, because it may not be easy to see which elementary identity or inequality should be invoked next. For certain problems, like Example 1, we may rely on our insight to see how we should proceed in the proof. But of course, most of our insight in problems is developed from the hindsight. For other problems like or even more complicated than Example 2 (which involves only three random variables), it may not be easy at all to work it out by brute force.</p><p>The proof of information inequalities can be facilitated by the use of information diagrams<ref type="foot" target="#foot_0">1</ref>  <ref type="bibr" target="#b24">[25]</ref>. However, the use of such diagrams becomes very difficult when the number of random variables is more than four.</p><p>In the conventional approach, elementary identities and inequalities are invoked in a sequential manner. In the new framework that we shall develop in this paper, all identities and inequalities are considered simultaneously.</p><p>Before we proceed any further, we would like to make a few remarks. Let and be any expressions depending only on Shannon's information measures. We shall call them information expressions, and specifically linear information expressions if they are linear combinations of Shannon's information measures. Likewise, we shall call inequalities involving only Shannon's information measures information inequalities. Now if and only if . Therefore, if for any expression we can determine whether it is always nonnegative, then we can determine whether any particular inequality always holds. We note that if and only if and . Therefore, it suffices to study inequalities.</p><p>The rest of the paper is organized as follows. In the next section, we first give a brief review of -Measure <ref type="bibr" target="#b24">[25]</ref> on which a few proofs will be based. In Section III, we introduce the canonical form of an information expression and discuss its uniqueness. We also define a region called which is central to the discussion in this paper. In Section IV, we present a simple calculus for verifying information identities and inequalities which can be proved by conventional techniques. In Section V, we further elaborate the significance of by pointing out its relations with some important problems in probability theory and information theory. Concluding remarks are given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. REVIEW OF THE THEORY OF -MEASURE</head><p>In this section, we give a review of the main results regarding -Measure. For a detailed discussion of -Measure, we refer the reader to <ref type="bibr" target="#b24">[25]</ref>. Further results on -Measure can be found in <ref type="bibr" target="#b6">[7]</ref>.</p><p>Let be jointly distributed discrete random variables, and be a set variable corresponding to a random variable . Define the universal set to be and let be the -field generated by . The atoms of have the form , where is either or . Let be the set of all atoms of except for , which is by construction because <ref type="bibr" target="#b0">(1)</ref> Note that . To simplify notations, we shall use to denote and to denote . Let . It was shown in <ref type="bibr" target="#b24">[25]</ref> that there exists a unique signed measure on which is consistent with all Shannon's information measures via the following formal substitution of symbols: Thus ( <ref type="formula">2</ref>) covers all the cases of Shannon's information measures. Let for some <ref type="bibr" target="#b5">(6)</ref> Note that . Let . Define arbitrary one-to-one mappings and let (7) <ref type="bibr" target="#b7">(8)</ref> where and for . Then <ref type="bibr" target="#b8">(9)</ref> where is a unique matrix (independent of ) with if if <ref type="bibr" target="#b9">(10)</ref> An important characteristic of is that it is invertible <ref type="bibr" target="#b24">[25]</ref>, so we can write <ref type="bibr" target="#b10">(11)</ref> In other words, is completely specified by the set of values , , namely, all the joint entropies involving , and it follows from (5) that is the unique measure on which is consistent with all Shannon's information measures. Note that in general is not nonnegative. However, if form a Markov chain, is always nonnegative <ref type="bibr" target="#b6">[7]</ref>.</p><p>As a consequence of the theory of -Measure, the information diagram was introduced as a tool to visualize the relationship among information measures <ref type="bibr" target="#b24">[25]</ref>. Applications of information diagrams can be found in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE CANONICAL FORM</head><p>In the rest of the paper, we shall assume that are the random variables involved in our discussion. We observe that conditional entropies, mutual informations, and conditional mutual informations can be expressed as a linear combination of joint entropies by using the following identity: <ref type="bibr" target="#b11">(12)</ref> where</p><p>. Thus any information expression can be expressed in terms of the joint entropies. We call this the canonical form of an information expression. Now for any their joint entropies correspond to a vector in , where we regard as the coordinates of . On the other hand, a vector in is said to be constructible if there exist whose joint entropies are given by . We are then motivated to define is constructible As we shall see, not only gives a complete characterization of all information inequalities, but it also is closely related to some important problems in probability theory and information theory. Thus a complete characterization of is of fundamental importance. To our knowledge, there has not been such a characterization in the literature (see Section V). Now every information expression can be expressed in canonical form. A basic question to ask is in what sense the canonical form is unique. Toward this end, we shall first establish the following theorem.</p><p>Theorem 1: Let be measurable such that has zero Lebesque measure. Then cannot be identically zero on . We shall need the following lemma which is immediate from the discussion in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr">Sec. 6</ref>]. The proof is omitted here.</p><p>Lemma 1: Let is constructible (cf., <ref type="bibr" target="#b10">(11)</ref>). Then the first quadrant of is a subset of . Proof of Theorem 1: If has positive Lebesque measure, since has zero Lebesque measure and hence has zero Lebesque measure, has positive Lebesque measure. Then cannot be a subset of , which implies that cannot be identically zero on . Thus it suffices to prove that has positive Lebesque measure. Using the above Lemma, we see that the first quadrant of , which has positive Lebesque measure, is a subset of . Therefore has positive Lebesque measure. Since is an invertible linear transformation of , its Lebesque measure must also be positive. This proves the theorem.</p><p>The uniqueness of the canonical form for very general classes of information expressions follows from this theorem. For example, suppose and are two polynomials of the joint entropies such that for all . Let</p><p>. If is not the zero function, then has zero Lebesque measure. By the theorem, cannot be identical to zero on , which is a contradiction. Therefore is the zero function, i.e.,</p><p>. Thus we see that the canonical form is unique for polynomial information expressions. We note that the uniqueness of the canonical form for linear information expressions has been discussed in <ref type="bibr" target="#b3">[4]</ref> and [2, p. 51, Theorem 3.6].</p><p>The importance of the canonical form will become clear in the next section. An application of the canonical form to recognizing the symmetry of an information expression will be discussed in Appendix II-A. We note that any invertible linear transformation of the joint entropies can be used for the purpose of defining the canonical form. Nevertheless, the current definition of the canonical form has the advantage that if and are two sets of random variables such that , then the joint entropies involving the random variables in is a subset of the joint entropies involving the random variables in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. A CALCULUS FOR VERIFYING LINEAR IDENTITIES AND INEQUALITIES</head><p>In this section, we shall develop a simple calculus for verifying all linear information identities and inequalities involving a definite number of random variables which can be proved by conventional techniques. All identities and inequalities in this section are assumed to be linear unless otherwise specified. Although our discussion will primarily be on linear identities and inequalities (possibly with linear constraints), our approach can be extended naturally to nonlinear cases. For nonlinear cases, the amount of computation required is larger. The question of what linear combinations of entropies are always nonnegative was first raised by Han <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unconstrained Identities</head><p>Due to the uniqueness of the canonical form for linear information expressions as discussed in the preceding section, it is easy to check whether two expressions and are identical. All we need to do is to express in canonical form. If all the coefficients are zero, then and are identical, otherwise they are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Unconstrained Inequalities</head><p>Since all information expressions can be expressed in canonical form, we shall only consider inequalities in this form. The following is a simple yet fundamental observation which apparently has not been discussed in the literature.</p><p>For any , always holds if and only if . This observation, which follows immediately from the definition of , gives a complete characterization of all unconstrained inequalities (not necessary linear) in terms of . From this point of view, an unconstrained inequality is simply a partial characterization of .</p><p>The nonnegativity of all Shannon's information measures form a set of inequalities which we shall refer to as the basic inequalities. We observe that in the conventional approach to proving information inequalities, whenever we establish an inequality in an intermediate step, we invoke one of the basic inequalities. Therefore, all information inequalities and conditional information identities which can be proved by conventional techniques are consequences of the basic inequalities. These inequalities, however, are not nonredundant. For example, and , which are both basic inequalities of the random variables and , imply again a basic inequality of and . We shall be dealing with linear combinations whose coefficients are nonnegative. We call such linear combinations nonnegative linear combinations. We observe that any Shannon's information measure can be expressed as a nonnegative linear combination of the following two elemental forms of Shannon's information measures: i) ii)</p><p>, where and . This can be done by successive (if necessary) application(s) of the following identities:</p><formula xml:id="formula_0">(13) (14) (15) (16) (17)<label>(18)</label></formula><p>(Note that all the coefficients in the above identities are nonnegative.) It is easy to check that the total number of Shannon's information measures of the two elemental forms is equal to <ref type="bibr" target="#b18">(19)</ref> The nonnegativity of the two elemental forms of Shannon's information measures form a proper subset of the set of basic inequalities. We call the inequalities in this smaller set the elemental inequalities. They are equivalent to the basic inequalities because each basic inequality which is not an elemental inequality can be obtained by adding a certain set of elemental inequalities in view of ( <ref type="formula">13</ref>)- <ref type="bibr" target="#b17">(18)</ref>. The minimality of the elemental inequalities is proved in Appendix I.</p><p>If the elemental inequalities are expressed in canonical form, then they become linear inequalities in . Denote this set of inequalities by</p><p>, where is an matrix, and define <ref type="bibr" target="#b19">(20)</ref> Since the elemental inequalities are satisfied by any , we have . Therefore, if then i.e., always holds. Let , be the column -vector whose th component is equal to and all the other components are equal to . Since a joint entropy can be expressed as a nonnegative linear combination of the two elemental forms of Shannon's information measures, each can be expressed as a nonnegative linear combination of the rows of . This implies that is a pyramid in the positive quadrant.</p><p>Let be any column -vector. Then , a linear combination of joint entropies, is always nonnegative if . This is equivalent to say that the minimum of the problem (Primal) Minimize subject to is zero. Since gives ( is the only corner of ), all we need to do is to apply the optimality test of the simplex method <ref type="bibr" target="#b18">[19]</ref> to check whether the point is optimal.</p><p>We can obtain further insight in the problem from the Duality Theorem in linear programming <ref type="bibr" target="#b18">[19]</ref>. The dual of the above linear programming problem is , where is a column -vector, i.e., is a nonnegative linear combination of the rows of .</p><p>Proof: We omit the simple proof that is nonempty if and only if for some , where is a column -vector. Let <ref type="bibr" target="#b21">(22)</ref> If for some , then . Let Since can be expressed as a nonnegative linear combination of the rows of <ref type="bibr" target="#b22">(23)</ref> can also be expressed as a nonnegative linear combinations of the rows of . By <ref type="bibr" target="#b21">(22)</ref>, this implies for some . Thus always holds (subject to ) if and only if it is a nonnegative linear combination of the elemental inequalities (in canonical form).</p><p>We now summarize the results in this section. For information expressions and , let be the cost function subject to the elemental inequalities. Then apply the optimality test of the simplex method to the point . If is optimal, then always holds. If not, then may or may not always hold. If it always holds, it is not implied by the elemental inequalities. In other words, it cannot be proved by conventional techniques, namely, invoking the elemental inequalities.</p><p>Han has previously studied unconstrained information inequalities involving three random variables <ref type="bibr" target="#b4">[5]</ref> as well as information inequalities which are symmetrical in all the random variables involved <ref type="bibr" target="#b5">[6]</ref>, and explicit characterizations of such inequalities were obtained. A discussion of these results is found in Appendix II. ------form a Markov chain if and only if and . In order to facilitate our discussion, we now introduce an alternative set of notations for . We do not distinguish elements and singletons of , and we write unions of subsets of as juxtapositions. For any nonempty , we use to denote , i.e., (refer to Section II for the definition of ). We also define for nonempty to simplify notations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Constrained Inequalities</head><p>In general, a constraint is given by a subset of . For instance, for the last example above,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>When</head><p>, there is no constraint. (In fact, there is no constraint if .) Parallel to our discussion in the preceding subsection, we have the following more general observation:</p><p>Under the constraint , for any , always hold if and only if . Again, this gives a complete characterization of all constrained inequalities in terms of . Thus in fact is the origin of all constrained inequalities, with unconstrained inequalities being a special case. In this and the next subsection, however, we shall confine our discussion to the linear case.</p><p>When is a subspace of , we can easily modify the method in the last subsection by taking advantage of the linear structure of the problem. Let the constraints on be given by <ref type="bibr" target="#b23">(24)</ref> where is a matrix (i.e., there are constraints). Following our discussion in the last subsection, a linear combination of joint entropies is always nonnegative under the constraint if the minimum of the problem Minimize subject to and is zero. Let be the rank of . Since is in the null space of , we can write <ref type="bibr" target="#b24">(25)</ref> where is a matrix whose columns form a basis of the orthogonal complement of the row space of , and is a column -vector. Then the elemental inequalities can be expressed as <ref type="bibr" target="#b25">(26)</ref> and in terms of , becomes <ref type="bibr" target="#b26">(27)</ref> which is a pyramid in (but not necessarily in the positive quadrant). Likewise, can be expressed as . With the constraints and all expressions in terms of , is always nonnegative under the constraint if the minimum of the problem Minimize subject to is zero. Again, since gives ( is the only corner of ), all we need to do is to apply the optimality test of the simplex method to check whether the point is optimal.</p><p>By imposing the constraints in <ref type="bibr" target="#b23">(24)</ref>, the number of elemental inequalities remains the same, while the dimension of the problem decreases from to . Again from the Duality Theorem, we see that is always nonnegative if for some</p><p>, where is a columnvector, i.e., is a nonnegative linear combination of the elemental inequalities (in terms of ).</p><p>We now summarize the results in this section. Let the constraints be given in <ref type="bibr" target="#b23">(24)</ref>. For expressions and , let . Then let be the cost function subject to the elemental inequalities (in terms of ) and apply the optimality test to the point . If is optimal, then always holds, otherwise it may or may not always hold. If it always holds, it is not implied by the elemental inequalities. In other words, it cannot be proved by conventional techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Constrained Identities</head><p>We impose the constraints in <ref type="bibr" target="#b23">(24)</ref> as in the last subsection. As we have pointed out at the beginning of the paper, two information expressions and are identical if and only if and always hold. Thus we can apply the method in the last subsection to verify all constrained identities that can be proved by conventional techniques.</p><p>When are unconstrained, the uniqueness of the canonical form for linear information expressions asserts that if and only if . However, when the constraints in <ref type="bibr" target="#b23">(24)</ref> on are imposed, does not imply . We give a simple example to illustrate this point. Suppose and we impose the constraint . Then every information expression can be expressed in terms of and . Now consider <ref type="bibr" target="#b27">(28)</ref> Note that the coefficients in the above expression are nonzero. But from the elemental inequalities, we have</p><p>which imply that . We now discuss a special application of the method described in this subsection. Let us consider the following problem in probability theory. Suppose we are given that ----and ----form a Markov chain, and that and are independent. We ask whether and are always independent. This problem can be formulated in information-theoretic terms with the constraints represented by , , and , and we want to know whether they imply . Problems of such kind can be handled by the method described in this subsection. Our method can prove any independence relation which can be proved by conventional information-theoretic techniques. The advantage of using an information-theoretic formulation of the problem is that we can avoid manipulations of the joint distribution directly, which is awkward <ref type="bibr" target="#b7">[8]</ref>, if not difficult.</p><p>It may be difficult to devise a calculus to handle independence relations of random variables in a general setting,<ref type="foot" target="#foot_1">2</ref> because an independence relation is "discrete" in the sense that it is either true or false. On the other hand, the problem becomes a continuous one if it is formulated in informationtheoretic terms (because mutual informations are continuous functionals), and continuous problems are in general less difficult to handle. From this point of view, the problem of determining independence of random variables is a discrete problem embedded in a continuous problem.</p><p>V. FURTHER DISCUSSION ON We have seen that , but it is not clear whether . If so, and hence all information inequalities are completely characterized by the elemental inequalities. In the following, we shall use the notations and when we refer to and for a specific . For In -Measure notations, the elemental inequalities are , , and . It then follows from Lemma 1 that . Inspired by the current work, the characterization of and has recently been investigated by Zhang and Yeung. They have found that (therefore in general), but , the closure of , is equal to <ref type="bibr" target="#b28">[29]</ref>. This implies that all unconstrained (linear or nonlinear) inequalities involving three random variables are consequences of the elemental inequalities of the same set of random variables. However, it is not clear whether the same is true for all constrained inequalities. They also have discovered the following conditional inequality involving four random variables which is not implied by the elemental inequalities:</p><p>If and , then</p><p>If, in addition, and the above inequality implies that This is a conditional independence relation which is not implied by the elemental inequalities. However, whether remained an open problem. Subsequently, they have determined that by discovering the following unconstrained inequality involving four random variables which is not implied by the elemental inequalities of the same set of random variables <ref type="bibr" target="#b29">[30]</ref>:</p><p>The existence of the above two inequalities indicates that there may be a lot of information inequalities yet to be discovered. Since most converse coding theorems are proved by means of information inequalities, it is plausible that some of these inequalities yet to be discovered are needed to settle certain open problems in information theory.</p><p>In the remainder of the section, we shall further elaborate on the significance of by pointing out its relations with some important problems in probability theory and information theory. and the blocks on the left and right are encoders and decoders, respectively. The random variables , , and are the outputs of the corresponding encoders. Given , , and , where and , we are interested in the admissible region of the triple . Evidently, , , and give the number of bits needed for the encoders. From the encoding and decoding requirements, we immediately have , , , , , and equal to zero. Now there are five random variables involved in this problem. Then the intersection of and the set containing all such that is the set of all possible vectors of the joint entropies involving given that they satisfy the encoding and decoding requirements of the problem as well as the constraints on the joint entropies involving and . Then is given as the projection of this set on the coordinates , , and . In the same spirit as that in the last subsection, an explicit outer bound of , denoted by , is given by replacing by . We refer to an outer bound such as as an LP (linear programming) bound. This is a new tool for proving converse coding theorems for problems in multiuser information theory. The LP bound already has found applications in the recent work of Yeung and Zhang <ref type="bibr" target="#b27">[28]</ref> on a new class of multiterminal source coding problems. We expect that this approach will have impact on other problems in multiuser information theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUDING REMARKS</head><p>We have identified the region as the origin all information inequalities. Our work suggests the possibility of the existence of information inequalities which cannot be proved by conventional techniques, and this has been confirmed by the recent results of Zhang and Yeung <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>.</p><p>A product from the framework we have developed is a simple calculus for verifying all linear information inequalities involving a definite number of random variables possibly with linear constraints which can be proved by conventional techniques; these include all inequalities of such type in the literature. Based on this calculus, a software running on MATLAB called ITIP (Information-Theoretic Inequality Prover) has been developed by Yeung and Yan <ref type="bibr" target="#b26">[27]</ref>, and it is available on World Wide Web. The following session from ITIP contains verifications of Example 1 and 2, respectively, in Section I. &gt;&gt; ITIP('I(Y; Z) &gt;= I(X; Z)', 'I(X; Z|Y) = 0') True &gt;&gt; ITIP('H(X,Y) -1.04 H(Y) + 0.7 I(Y; X,Z) + 0.04 H(Y|Z) &gt;= 0') True</p><p>We see from <ref type="bibr" target="#b18">(19)</ref> that the amount of computation required is moderate when</p><p>. Our work gives a partial answer to Han's question of what linear combinations of entropies are always nonnegative <ref type="bibr" target="#b4">[5]</ref>. A complete answer to this question is impossible without further characterization of .</p><p>The characterization of is a very fundamental problem in information theory. However, in view of the difficulty of some special cases of this problem <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, it is not very hopeful that this problem can be solved completely in the near future. Nevertheless, partial characterizations of may lead to the discovery of some new inequalities which make the solutions of certain open problems in information theory possible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>Theorem, the maximum of the dual problem is also zero. Since the cost function in the dual problem is zero, the maximum of the dual problem is zero if and only if the feasible region and (21) is nonempty. Theorem 2: is nonempty if and only if for some</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A multiterminal source coding problem.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>It was called an I-diagram in [25], but we prefer to call it an information diagram to avoid confusion with an eye diagram in communication theory.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A calculus for independence relations has been devised by Massey<ref type="bibr" target="#b8">[9]</ref> for the special case when the random variables have a causal interpretation.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The author wishes to acknowledge the help of a few individuals during the preparation of this paper. They include I. Csiszár, B. Hajek, F. Matúš, Y.-O. Yan, E.-h. Yang, and Z. Zhang.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Conditional Independence Relations</head><p>For any fixed number of random variables, a basic question is what sets of conditional independence relations are possible. In the recent work of Matúš and Studený <ref type="bibr" target="#b16">[17]</ref>, this problem is formulated as follows. Recall that and let be the family of all couples where and is the union of two, not necessarily different, singletons and of . Having a system of random variables with subsystems , , we introduce the notation where is the abbreviation of the statement " is conditionally independent of given ." For , means is determined by . The subsystem is presumed to be constant. A subfamily is called probabilistically ( -) representable if there exists a system , called a -representation, such that . The problem is to characterize the class of all -representable relations. Note that this problem is more general than the application discussed in Section IV-D. Now is equivalent to . If is a proper subset of , i.e., is not of elemental form, then can be written as a nonnegative combination of the corresponding elemental forms of Shannon's information measure. We observe that if and only if each of the corresponding elemental forms of Shannon's information measures vanishes, and that an elemental form of Shannon's information measure vanishes if and only if the corresponding conditional independence relation holds. Thus it is actually unnecessary to consider , , for separately because it is determined by the other conditional independence relations.</p><p>Let us now look at some examples. For and As pointed out in the last paragraph, the couples are actually redundant. Let be a system of random variables such that is not deterministic, , , and and are not functions of each other. Then it is easy to see that Thus is -representable. On the other hand, is notrepresentable, because and imply . The recent studies on the problem of conditional independence relations was launched by a seminal paper by Dawid <ref type="bibr" target="#b2">[3]</ref>, in which he proposed four axioms as heuristic properties of conditional independence. In information-theoretic terms, these four axioms can be summarized by the following statement: and Subsequent work on this subject has been done by Pearl and his collaborators in the 1980's, and their work is summarized in the book by Pearl <ref type="bibr" target="#b17">[18]</ref>. Their work has mainly been motivated by the study of the logic of integrity constraints from databases. Pearl conjectured that Dawid's four axioms completely characterize the conditional independence structure of any joint distribution. This conjecture, however, was refuted by the work of Studený <ref type="bibr" target="#b19">[20]</ref>. Since then, Matúš and Studený have written a series of papers on this problem <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b23">[24]</ref>. So far, they have solved the problem for three random variables, but the problem for four random variables remains open.</p><p>The relation between this problem and is the following. Suppose we want to determine whether a subfamily of is -representable. Now each corresponds to setting to zero in . Note that is a hyperplane containing the origin in . Thus isrepresentable if and only if there exists a in such that for all . Therefore, the problem of conditional independence relations is a subproblem of the problem of characterizing . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimization of Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiuser Information Theory</head><p>The framework for information inequalities developed in this paper provides new tools for problems in multiuser information theory. Consider the source coding problem in Fig. <ref type="figure">1</ref>, in which and are source random variables,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I MINIMALITY OF THE ELEMENTAL INEQUALITIES</head><p>The elemental inequalities in set-theoretic notations have one of the following two forms:</p><p>1) ; 2)</p><p>, where and . They will be referred to as -inequalities and -inequalities, respectively.</p><p>We are to show that all the elemental inequalities are nonredundant, i.e., none of them is implied by the others. For an -inequality (38) since it is the only elemental inequality which involves the atom , it is clearly not implied by the other elemental inequalities. Therefore, we only need to show that all -inequalities are nonredundant. To show that a -inequality is nonredundant, it suffices to show that there exists a measure on which satisfies all other elemental inequalities except for that one.</p><p>We shall show that the -inequality Finally, we consider the case when . Using the binomial formula in (48), we see that the number of odd atoms and even atoms of in are the same. Therefore, the first term in (51) is equal to if and is equal to otherwise. The former is true if and only if , which implies that is nonempty, or that the second term is at least . Thus in either case (50) is true. This completes the proof that (39) is nonredundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II SOME SPECIAL FORMS OF UNCONSTRAINED INFORMATION INEQUALITIES</head><p>In this appendix, we shall discuss some special forms of unconstrained linear information inequalities previously investigated by Han <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Explicit necessary and sufficient conditions for these inequalities to always hold have been obtained. The relation between these inequalities and the results in the current paper will also be discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Symmetrical Information Inequalities</head><p>An information expression is said to be symmetrical if it is identical under every permutation among . For example, for , the expression is symmetrical. This can be seen by permuting and symbolically in the expression. Now let us consider the expression . If we replace and by each other, the expression becomes , which is symbolically different from the original expression. However, both expression are identical to . Therefore, the two expressions are in fact identical, and the expression is actually symmetrical although it is not readily recognized symbolically.</p><p>The symmetry of an information expression in general cannot be recognized symbolically. However, it is readily recognized symbolically if the expression is in canonical form. This is due to the uniqueness of the canonical form as discussed in Section III.</p><p>Consider a linear symmetrical information expression (in canonical form). As seen in Section IV-B, can be expressed as a linear combination of the two elemental forms of Shannon's information measures. It was shown in <ref type="bibr" target="#b4">[5]</ref> that every symmetrical expression can be written in the form where and, for , Note that is the sum of all Shannon's information measures of the first elemental form, and for , is the sum of all Shannon's information measures of the second elemental form conditioning on random variables.</p><p>It follows trivially from the elemental inequalities that is a sufficient condition for to always hold. The necessity of this condition can be seen by noting the existence of random variables for each such that and for all and . This implies that all unconstrained linear symmetrical information inequalities are consequences of the elemental inequalities. We refer the reader to <ref type="bibr" target="#b4">[5]</ref> for a more detailed discussion of symmetrical information inequalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Information Inequalities Involving Three Random Variables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider</head><p>. Let and and let</p><p>Since is an invertible linear transformation of , all linear information expression can be written as , where</p><p>It was shown in <ref type="bibr" target="#b5">[6]</ref> that always holds if and only if the following conditions are satisfied:</p><p>(53)</p><p>In terms of , the elemental inequalities can be expressed as , where</p><p>From the discussion in Section IV-B, we see that always holds if and only if is a nonnegative linear combination of the rows of . We leave it as an exercise for the reader to show that is a nonnegative linear combination of the rows of if and only if the conditions in (53) are satisfied. Therefore, all unconditional linear inequalities involving three random variables are consequences of the elemental inequalities. This result also implies that is the smallest pyramid containing .</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Elements of Information Theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Information Theory: Coding Theorems for Discrete Memoryless Systems</title>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Academic</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditional independence in statistical theory (with discussion)</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Statist. Soc., Ser. B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Linear dependence structure of the entropy space</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="337" to="368" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nonnegative entropy measures of multivariate symmetric correlations</title>
	</analytic>
	<monogr>
		<title level="j">Inform. Contr</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="133" to="156" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A uniqueness of Shannon&apos;s information distance and related nonnegativity problems</title>
	</analytic>
	<monogr>
		<title level="j">J. Combin.., Inform. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="331" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The structure of the I-Measure of a Markov chain</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kawabata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1146" to="1149" />
			<date type="published" when="1992-05">May 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Determining the independence of random variables</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Massey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1995 IEEE Int. Symp. on Information Theory</title>
		<meeting><address><addrLine>Whistler, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">Sept. 17-22, 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal interpretations of random variables</title>
	</analytic>
	<monogr>
		<title level="m">1995 IEEE Int. Symp. on Information Theory</title>
		<meeting><address><addrLine>Whistler, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995">Sept. 17-22, 1995</date>
		</imprint>
	</monogr>
	<note>Special session in honor of Mark Pinsker on the occasion of his 70th birthday</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Abstract functional dependency structures</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matúš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="117" to="126" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On equivalence of Markov properties over undirected graphs</title>
	</analytic>
	<monogr>
		<title level="j">J. Appl. Probab</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="745" to="749" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ascending and descending conditional independence relations</title>
	</analytic>
	<monogr>
		<title level="m">Trans. 11th Prague Conf. on Information Theory, Statistical Decision Functions and Random Processes</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Academia</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">B</biblScope>
			<biblScope unit="page" from="181" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Probabilistic conditional independence structures and matroid theory: Background</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. General Syst</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Extreme convex set functions with many nonnegative differences</title>
	</analytic>
	<monogr>
		<title level="j">Discr. Math</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="177" to="191" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Conditional independence among four random variables II</title>
	</analytic>
	<monogr>
		<title level="j">Combin., Prob. Comput</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional independence structures examined via minors</title>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Artificial Intell</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional independence among four random variables I</title>
		<author>
			<persName><forename type="first">F</forename><surname>Matúš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Studený</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combin., Prob. Comput</title>
		<imprint/>
	</monogr>
	<note>to be published</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufman</publisher>
			<pubPlace>San Mateo, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Strang</surname></persName>
		</author>
		<title level="m">Linear Algebra and Its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Academic</publisher>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
	<note>nd ed</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attempts at axiomatic description of conditional independence</title>
		<author>
			<persName><forename type="first">M</forename><surname>Studený</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Work. on Uncertainty Processing in Expert Systems, supplement to Kybernetika</title>
		<meeting>Work. on Uncertainty essing in Expert Systems, supplement to Kybernetika</meeting>
		<imprint>
			<date type="published" when="1989">1989</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiinformation and the problem of characterization of conditional independence relations</title>
	</analytic>
	<monogr>
		<title level="j">Probl. Contr. and Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="3" to="16" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Conditional independence relations have no finite complete characterization</title>
	</analytic>
	<monogr>
		<title level="m">Trans. 11th Prague Conf. on Information Theory, Statistical Decision Functions and Random Processes</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>Academia</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">B</biblScope>
			<biblScope unit="page" from="377" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structural semigraphoids</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gen. Syst</title>
		<imprint/>
	</monogr>
	<note>submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Descriptions of structures of stochastic independence by means of faces and imsets</title>
	</analytic>
	<monogr>
		<title level="j">Int. J. Gen. Syst</title>
		<imprint/>
	</monogr>
	<note>in three parts. submitted for publication</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A new outlook on Shannon&apos;s information measures</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="466" to="474" />
			<date type="published" when="1991-05">May 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multilevel diversity coding with distortion</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="412" to="422" />
			<date type="published" when="1995-03">Mar. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-O</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itip</forename></persName>
		</author>
		<ptr target="http://www.ie.cuhk.edu.hk/ITIP" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Miltilevel distributed source coding</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE Int. Symp. on Information Theory</title>
		<meeting><address><addrLine>Ulm, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997-06">June 1997</date>
			<biblScope unit="page">276</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A non-Shannon type conditional information inequality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Yeung</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1982" to="1986" />
		</imprint>
	</monogr>
	<note>this issue</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On the characterization of entropy function via information inequalities</title>
		<imprint>
			<publisher>IEEE Trans. Inform. Theory</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
