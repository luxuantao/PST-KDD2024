<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Distributed and Adaptive Signal Processing Approach to Reducing Energy Consumption in Sensor Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jim</forename><surname>Chou</surname></persName>
							<email>jimchou@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94709</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Dragan</forename><surname>Petrovic</surname></persName>
							<email>dragan@eecs.berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94709</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
							<email>kannanr@eecs.berkeley.edu</email>
							<affiliation key="aff2">
								<orgName type="department">Department of EECS</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94709</postCode>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Distributed and Adaptive Signal Processing Approach to Reducing Energy Consumption in Sensor Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CC1A490DDFC0675576D200BAAA799C42</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel approach to reducing energy consumption in sensor networks using a distributed adaptive signal processing framework and efficient algorithm 1 . While the topic of energy-aware routing to alleviate energy consumption in sensor networks has received attention recently [1,2], in this paper, we propose an orthogonal approach to previous methods. Specifically, we propose a distributed way of continuously exploiting existing correlations in sensor data based on adaptive signal processing and distributed source coding principles. Our approach enables sensor nodes to blindly compress their readings with respect to one another without the need for explicit and energy-expensive inter-sensor communication to effect this compression. Furthermore, the distributed algorithm used by each sensor node is extremely low in complexity and easy to implement (i.e., one modulo operation), while an adaptive filtering framework is used at the data gathering unit to continuously learn the relevant correlation structures in the sensor data. Our simulations show the power of our proposed algorithms, revealing their potential to effect significant energy savings (from 10%-65%) for typical sensor data corresponding to a multitude of sensor modalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>advances in wireless networking and embedded microprocessor designs have enabled the creation of dense low-power sensor networks. These sensor networks consist of nodes endowed with a multitude of sensing modalities such as temperature, pressure, light, magnetometer, infrared, audio, video, etc. The nodes are typically of small physical dimensions and operated by battery power, making energy consumption a major concern. For example, failure of a set of nodes in the sensor network due to energy depletion can lead to a partition of the sensor network and loss of potentially critical information. Motivated by this, there has been considerable recent interest in the area of energy-aware routing for ad hoc and sensor networks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> and efficient information processing <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> to reduce the energy usage of sensor nodes. For example, one method of conserving energy in a sensor node is to aggregate packets along the sensor paths to reduce header overhead. In this paper, we propose a new method of conserving energy in sensor networks that is mutually exclusive to the above approaches, and can be used in combination with them to increase energy reduction.</p><p>Our approach is based on judiciously exploiting existing sensor data correlations in a distributed manner. Correlations in sensor data are brought about by the spatio-temporal characteristics of the physical medium being sensed. Dense sensor networks are particularly rich in correlations, where spatially dense nodes are typically needed to acquire fine spatial resolution in the data being sensed, and for fault tolerance from individual node failures. Examples of correlated sensors include temperature and humidity sensors in a similar geographic region, or magnetometric sensors tracking a moving vehicle. Another interesting example of correlated sensor data involves audio field sensors (microphones) that sense a common event such as a concert or whale cries. Audio data is particularly interesting in that it is rich in spatial correlation structure due to the presence of echoes, causing multiple sensors to pick up attenuated and delayed versions of a common sound origin.</p><p>We propose to remove the redundancy caused by these inherent correlations in the sensor data through a distributed compression algorithm which obviates the need for the sensors to exchange their data among each other in order to strip their common redundancy. Rather surprisingly, we will show that compression can be effected in a fully blind manner without the sensor nodes ever knowing what the other correlated sensor nodes have measured. Our proposed paradigm is particularly effective for sensor network architectures having two types of nodes: sensing nodes and data-gathering nodes. The sensing nodes gather data of a specific type and transmit this data upon being queried. The data gathering node queries specific sensors in order to gather information in which it is interested (see Fig. <ref type="figure">1</ref>). We will assume the above architecture (Fig. <ref type="figure">1</ref>) for the rest of the paper and show that for such an architecture, we can devise compression algorithms that have very lightweight encoders, yet can achieve significant savings. Note, that we target very lightweight encoders in this paper because we assume that the sensors have limited Fig. <ref type="figure">1</ref>. An example sensor network: a computer acts as the data gathering node, and queries various sensors to collect data compute power, but the constructions introduced in this paper can be easily strengthened given greater compute power at the sensors. The savings are achieved by having the data gathering node track the correlation structure among nodes and then use this information to effect distributed sensor data compression. The correlation structure is determined by using an adaptive prediction algorithm. The sensors, however, do not need to know the correlation structure; they need to know only the number of bits that they should use for encoding their measurements. As a result, each sensor node is required to perform very few operations in order to encode its data. The decoder, however, is considerably more complex, but it resides on the data gathering node, which is not assumed to be energy constrained. Preliminary results based on our distributed compression and adaptive prediction algorithms perform well in realistic scenarios, achieving 10-65% energy savings for each sensor in typical cases. In addition, our distributed compression architecture can be combined with other energy saving methods such as packet/data aggregation to achieve further gains. Two of the main challenges in designing a system as described above include (1) devising a computationally inexpensive encoder that can support multiple compression rates and (2) determining an adaptive correlation-tracking algorithm that can continuously track the amount of correlation that exists between the sensor nodes. We will look at the above two issues in the following sections. In the next section, we start by devising a computationally inexpensive compression algorithm for the sensor nodes. In section 3, we will present the correlation tracking algorithm. In section 4, we will integrate the above components into a complete system. Simulation results are given in section 5 and we conclude with some remarks in section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DISTRIBUTED COMPRESSION</head><p>The appeal of using distributed compression lies in the fact that each sensor can compress its data without knowing what the other sensors are measuring. In fact, the sensors do not even need to know the correlation structure between its data and that of the other sensors. As a result, an end-to-end compression system that achieves a significant savings across the network can be built, where the endpoints consist of the sensor node and the data gathering node.</p><p>To build a distributed compression system, we propose to use an asymmetric coding method among the sensors. Specifically, we propose to build upon the architecture of Fig. <ref type="figure">2</ref> which is designed for two nodes. In Fig. <ref type="figure">2</ref>, there are two nodes, each of which measures data using an Analog-to-Digital (A/D) converter. One of the sensor nodes will either transmit its data Y directly to the data gathering node or compress its readings with respect to its own previous readings while the other sensor node compresses its data X with respect to its own previous readings and readings from other sensors and then transmits the compressed data m to the data gathering node. The decoder will then try to decode m to X, given that Y is correlated to X. In the discrete alphabet case, it can be shown that the compression performance of the above architecture can match the case where Y is available to the sensor node that is measuring X.</p><p>To extend the above architecture (Fig. <ref type="figure">2</ref>) to n nodes we will have one node send its data either uncoded (i.e., Y ) or compressed with respect to its past. The data gathering node can decode this reading without receiving anything from the other sensors. The other sensors can compress their data with respect to Y , without even knowing their correlation structure with respect to Y . The data gathering node will keep track of the correlation structure and inform the sensors of the number of bits that they shall use for encoding. In the compression literature, Y is often referred to as side-information and the above architectures are often referred to as compression with side information <ref type="bibr" target="#b5">[6]</ref>.</p><p>To develop code constructions for distributed compression, we will start by giving some background information on source coding with side information and then introduce a code construction that achieves good performance at a low encoding cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Background on compression with side information</head><p>In 1973, Slepian and Wolf presented a suprising result to the source coding (compression) community <ref type="bibr" target="#b5">[6]</ref>. The result states that if two discrete alphabet random variables X and Y are correlated according to some arbitrary probability distribution p(x, y), then X can be compressed without access to Y without losing any compression performance with respect to the case where X is compressed with access to Y . More formally, without having access to Y , X can be compressed using H(X|Y ) bits where</p><formula xml:id="formula_0">H(X|Y ) = - y P Y (y) x P X (x|y)log 2 P X (x|y)<label>(1)</label></formula><p>The quantity, H(X|Y ) is often interpreted as the "uncertainty" remaining in the random variable X given the observation of Y <ref type="bibr" target="#b6">[7]</ref>. This is the same compression performance that would be achieved if X were compressed while having access to Y . To provide the intuition behind this result, we provide the following example. where d H (., .) denotes Hamming distance. When Y is known both at the encoder and decoder, we can compress X to 2 bits, conveying the information about the uncertainty of X given Y (i.e., the modulo-two sum of X and Y given by: (000),(001),( <ref type="formula">010</ref>) and (100)). Now, if Y is known only at the decoder, we can surprisingly still compress X to 2 bits. The method of construction stems from the following argument: if the decoder knows that X=000 or X=111, then it is wasteful to spend any bits to differentiate between the two. In fact, we can group X=000 and X=111 into one coset (it is exactly the so-called principal coset of the length-3 repetition code). In a similar fashion, we can partition the remaining space of 3-bit binary codewords into 3 different cosets with each coset containing the original codewords offset by a unique and correctable error pattern. Since there are 4 cosets, we need to spend only 2 bits to specify the coset in which X belongs. The four cosets are given as coset-1 = (000, 111), coset-2 = (001, 110), coset-3 = (010, 101), coset-4 = (011, 110)</p><p>The decoder can recover X perfectly by decoding Y to the closest (in hamming distance) codeword in the coset specified by the encoder. Thus the encoder does not need to know the realization of Y for optimal encoding. The above results were established only for discrete random variables. In 1976, Wyner and Ziv extended the results of <ref type="bibr" target="#b5">[6]</ref> to lossy distributed compression by proving that under certain conditions <ref type="bibr" target="#b7">[8]</ref>, there are no performance degradations for lossy compression with side information available at the decoder as compared to lossy compression with side information available at both the encoder and decoder.</p><p>The results established by <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b7">[8]</ref> are theoretical results, however, and as a result do not provide intuition as to how one might achieve the predicted theoretical bounds practically. In 1999, Pradhan and Ramchandran <ref type="bibr" target="#b8">[9]</ref> prescribed practical constructions for distributed compression in an attempt to achieve the bounds predicted by <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b7">[8]</ref>. The resulting codes perform well, but cannot be directly used for sensor networks because they are not designed to support different compression rates. To achieve distributed compression in a sensor network, it is desirable to have one underlying codebook that is not changed among the sensors but can also support multiple compression rates. The reason for needing a codebook that supports multiple compression rates is that the compression rate is directly dependent on the amount of correlation in the data, which might be time-varying. Motivated by the above, we have devised a tree-based distributed compression code that can provide variable-rate compression without the need for changing the underlying codebook.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Code construction</head><p>In this section we propose a codebook construction that will allow an encoder to encode a random variable X given that the decoder has access to a correlated random variable Y . This construction can then be applied to a sensor network as shown in Fig. <ref type="figure">2</ref>. The main design goal of our code construction is to support multiple compression rates, in addition to being computationally inexpensive. In support of our goal of minimizing the computations for each sensor node, we will not be looking into code constructions that use complicated error correction codes. Error correction codes, can however, be easily incorporated into our construction but will lead to more complexity for each sensor node. Our uncoded code construction is as follows. We start with a root codebook that contains 2 n representative values on the real axis. We then partition the root codebook into two subsets consisting of the evenindexed representations and the odd-indexed representations. We represent these two sub-codebooks as children nodes of the root codebook. We further partition each of these nodes into sub-codebooks and represent them as children nodes in the second level of the tree structure. This process is repeated n times, resulting in an n-level tree structure that contains 2 n leaf nodes, each of which represents a subcodebook that contains one of the original 2 n values. An example partition is given in Fig. <ref type="figure" target="#fig_1">3</ref>, where we use n = 4 and show only 2 levels of the partition. Note from this tree-based codebook construction that if the spacing between representative values is denoted by ∆, then each of the subcodebooks at level-i in the tree will contain representative values that are spaced apart by 2 i ∆. In a sensor network, a reading will typically be represented as one of the 2 n values in the root codebook assuming that the sensor uses an n-bit A/D converter. Instead of transmitting nbits to represent the sensor reading, as would be traditionally done, we can transmit i &lt; n bits if there is side-information,</p><formula xml:id="formula_1">0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003</formula><p>Y , that is no further than 2 i-1 ∆ away from X available at the decoder. The encoder need only transmit the i bits that specify the subcodebook that X belongs to at level-i, and the decoder will decode Y to the closest value in the subcodebook that the encoder specified. Because Y is no further than 2 i-1 ∆ from the representation of X, the decoder will always decode Y to X. Below, we describe the functionality of the encoder and decoder in detail. 1) Encoder: The encoder will receive a request from the data gathering node requesting that it encode its readings using i bits. The first thing that the encoder does is find the closest representation of the data from the 2 n values in the root codebook (this is typically done by the A/D converter). Next, the encoder determines the subcodebook that X belongs to at level-i. The path through the tree to this subcodebook will specify the bits that are transferred to the data gathering node. The mapping from X to the bits that specify the subcodebook at level i can be done through the following deterministic mapping:</p><formula xml:id="formula_2">f (X) = index(X) mod 2 i (2)</formula><p>where f (X) represents the bits to be transmitted to the decoder and index() is a mapping from values in the root codebook to their respective indices. For a given X and i, f (X) will be an i-bit value which the data gathering node will use to traverse the tree.</p><p>2) Decoder: The decoder (at the data gathering node) will receive the i-bit value, f (X), from the encoder and will traverse the tree starting with the least-significant-bit (LSB) of f (X) to determine the appropriate subcodebook, S to use. The decoder will then decode the side-information, Y , to the closest value in S:</p><formula xml:id="formula_3">X = argmin ri∈S ||Y -r i || (3)</formula><p>where r i represents the i th codeword in S. Assuming that Y is less than 2 i-1 ∆ away from X, where ∆ is the spacing in the root codebook, then the decoder will be able to decode Y to the exact value of X, and recover X perfectly. The following example will elucidate the encoding/decoding operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 2:</head><p>Consider the 4-level tree codebook of Fig. <ref type="figure">4</ref>. Assume that the data is represented by the value r 9 = 0.9 in the root codebook and the data gathering node asks the sensor to encode X using 2 bits. The index of r 9 is 9, so f (X) = 9 mod 4 = 1. Thus, the encoder will send the two bits, 01, to the data gathering node (see Fig. <ref type="figure">4</ref>). The data gathering node will receive 01 and descend the tree using the least-significant bit first (i.e., 1 and then 0) to determine the subcodebook to decode the side-information with. In the example, we assume that the side-information, Y , is 0.8, and we will decode Y in the subcodebook located at 1, 0 in the tree to find the closest codeword. This codeword is r 9 which is exactly the value representing X. Thus, we have used 2 bits to convey the value of X instead of using the 4 bits that would have been needed if we had not done any encoding. Fig. <ref type="figure">4</ref>. An example for the tree based codebook. The encoder is asked to encode X using 2 bits, so it transmits 01 to the decoder. The decoder will use the bits 01 in ascending order from the LSB to determine the path to the subcodebook to use to decode Y with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CORRELATION TRACKING</head><p>In the above encoding/decoding operations we assume that the decoder for sensor j has available to it at time k some side-information Y (j) k that is correlated to the sensor reading, X (j) k . In practice, we choose to use a linear predictive model where Y (j) k is a linear combination of values that are available at the decoder:</p><formula xml:id="formula_4">Y (j) k = M l=1 α l X (j) k-l + j-1 i=1 β i X (i) k (4)</formula><p>where X (j) k-l represents past readings for sensor j and X (i) k represents present sensor readings from sensor i 2 . The variables α l and β i are weighting coefficients. We can then think of Y (j) k as a linear prediction of X (j) k based on past values (i.e., X (j) k-l ; l = 1, ..., M ) and other sensor readings that have already been decoded at the data gathering node (i.e., X (i) k ; i = 1, ..., j -1, where i indexes the sensor and j -1 represents the number of readings from other sensors that have already been decoded). We choose to use a linear predictive model because it is not only analytically tractable but also optimal in the limiting case where the readings can be modeled as i.i.d. Gaussian random variables.</p><p>In order to leverage the inter-node correlations, we require that one of the sensors always sends its data either uncoded or compressed with respect to its own past data. Furthermore, we number the sensors in the order that they are queried. For example, at each time instant, one of the sensors will send its reading, X <ref type="bibr" target="#b0">(1)</ref> k , either uncoded or coded with respect to its own past. The reading for sensor 2 can then be decoded with respect to</p><formula xml:id="formula_5">Y (2) k = M l=1 α l X (2) k-l + β 1 X (1) k (5)</formula><p>2 Note that for simplicity, our above prediction model is based on a finite number of past values and a single present value for each of the other sensor readings that have been decoded. This model can be generalized to the case where past values of other sensors are also included in the prediction.</p><formula xml:id="formula_6">0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003 Each X (i)</formula><p>k that is decoded can then be used to form predictions for other sensor readings according to <ref type="bibr" target="#b3">(4)</ref>. The prediction, Y (j) k , determines the number of bits needed to represent X (j) k . In the extreme case that Y</p><formula xml:id="formula_7">(j) k perfectly predicts X (j) k (i.e., Y (j) k = X (j)</formula><p>k ), then zero bits are needed to represent X (j) k because it is perfectly predictable at the decoder. Thus, the main objective of the decoder is to derive a good estimate of X (j) k for sensor j, j = 1, ..., L, where L represents the number of sensors. In more quantitative terms, we would like for the decoder to be able to find the α l ; l = 1, ..., M and β i ; i = 1, ..., j -1 that minimize the mean squared error between Y (j) k and X (j) k . To find the α l and β i that minimize the mean squared prediction error, let us start by representing the prediction error as a random variable,</p><formula xml:id="formula_8">N j = Y (j) k -X (j)</formula><p>k . We can then rewrite the mean squared error as:</p><formula xml:id="formula_9">E[N 2 j ] = E[(X (j) k - M l=1 α l X (j) k-l + j-1 i=1 β i X (i) k ) 2 ] = E[X (j)2 k ] -2 M l=1 α l E[X (j) k X (j) k-l ]- 2 N i=1 β i E[X (j) k X (i) k ] + 2 M l=1 j-1 i=1 α l β i E[X (j) k-l X (i) k ] + M l,h=1 α l α h E[X (j) k-l X (j) k-h ] + j-1 i,h=1 β i β h E[X (i) k X (h) k ]</formula><p>Now, if we assume that X (j) k and X (i) k are pairwise jointly wide sense stationary <ref type="bibr" target="#b9">[10]</ref> for i = 1, ..., j -1, then we can re-write the mean squared error as:</p><formula xml:id="formula_10">E[N 2 j ] = r x j x j (0) -2 P T j Γ j + Γ T j R j zz Γ j<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">Γ j =             α 1 α 2 ... α M β 1 β 2 ... β j-1             , P j =             r x j x j (1) r x j x j (2) ... r x j x j (M ) r x j x 1 (0) r x j x 2 (0) ... r x j x j-1 (0)            </formula><p>and we use the notation</p><formula xml:id="formula_12">r x j x i (l) = E[X j k X i k+l ].</formula><p>With this notation, we can express R j zz as:</p><formula xml:id="formula_13">R j zz = R x j x j R x j x i R T x j x i R x i x i</formula><p>where R x j x j is given as:</p><formula xml:id="formula_14">    r x j x j (0) r x j x j (1) . r x j x j (M -1) r x j x j (1) r x j x j (0) . r x j x j (M -2) ... ... . ... r x j x j (M -1) r x j x j (M -. r x j x j (0)     ,</formula><p>and R x j x i and R x i x i are given as:</p><formula xml:id="formula_15">R x j x i =     r x j x 1 (1) r x j x 2 (1) ... r x j x j-1 (1) r x j x 1 (2) r x j x 2 (2) ... r x j x j-1 (2) ... ... ... ... r x j x 1 (M ) r x j x 2 (M ) ... r x j x j-1 (M )     and R x i x i =     r x 1 x 1 (0) r x 1 x 2 (0) ... r x 1 x j-1 (0) r x 2 x 1 (0) r x 2 x 2 (0) ... r x 2 x j-1 (0) ... ... ... ... r x j-1 x 1 (0) r x j-1 x 2 (0) ... r x j-1 x j-1 (0)    </formula><p>To find the set of coefficients (represented by Γ j ) that minimize the mean squared error, we differentiate <ref type="bibr" target="#b5">(6)</ref> with respect to Γ j to obtain:</p><formula xml:id="formula_16">∂E[N 2 j ] ∂ Γ j = -2 P j + 2R j zz Γ j (7)</formula><p>Setting the above equal to zero and solving for the optimal Γ j , which we denote by Γ j,opt , we arrive at the standard Wiener estimate <ref type="bibr" target="#b9">[10]</ref>:</p><formula xml:id="formula_17">Γ j,opt = R -1,j zz P j (8)</formula><p>If our assumption of stationarity holds, then the data gathering node can request for uncoded data from all of the sensors for the first K rounds of requests and calculate the Wiener estimate (8) once from these K rounds of samples. The set of coefficients determined from the Wiener estimate can then be used to form the side information for each future round of request. In practice, however, the statistics of the data may be time varying and as a result, the coefficient vector, Γ j , must be continuously adjusted to minimize the mean-squared error. One method of doing this is to move Γ j in the opposite direction of the gradient of the objective function (i.e., the mean squared error) for each new sample received during round k + 1:</p><formula xml:id="formula_18">Γ (k+1) j = Γ (k) j -µ∇ (k) j (<label>9</label></formula><formula xml:id="formula_19">)</formula><p>where ∇ (k) j is given by <ref type="bibr" target="#b6">(7)</ref> and µ represents the amount to descend opposite to the gradient. The goal of this approach is to descend to the global minima of the objective function. We are assured that such a minima exists because the objective function is convex. In fact, it has been shown, that if µ is chosen correctly then (9) will converge to the optimal solution <ref type="bibr" target="#b9">[10]</ref>. In the following subsection we will show how ( <ref type="formula" target="#formula_18">9</ref>) can be calculated in practice and how to incorporate adaptive prediction with the distributed source code discussed in the previous section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Parameter estimation</head><p>From ( <ref type="formula">7</ref>) and ( <ref type="formula" target="#formula_18">9</ref>), we know that the coefficient vector should be updated as:</p><formula xml:id="formula_20">Γ (k+1) j = Γ (k) j - 1 2 µ(-2 P j + 2R j zz Γ (k) j ).<label>(10)</label></formula><p>In practice, however, the data gathering node will not have knowledge of P j and R j zz and will therefore need an efficient method for estimating P j and R j zz . One standard estimate is</p><formula xml:id="formula_21">0-7803-7753-2/03/$17.00 (C) 2003 IEEE IEEE INFOCOM 2003 to use P j = X (j) k Z k,j and R zz = Z k,j Z T k,j where Z k,j =               X (j) k-1 X (j) k-2 ... X (j) k-M X (1) k X (2) k ... X (j-1) k               so that (10) becomes Γ (k+1) j = Γ (k) j -µ Z k,j (-X (j) k + Z T k,j Γ (k) j ) (11) = Γ (k) j + µ Z k,j N k,j</formula><p>where the second equality follows from the fact that Y</p><formula xml:id="formula_22">(j) k = Z T k,j Γ (k) j and N k,j = X (j) k -Y (j)</formula><p>k . The equation described by ( <ref type="formula">12</ref>) is well known in the adaptive filtering literature as the Least-Mean-Squares (LMS) algorithm and the steps in calculating the LMS solution is summarized below:</p><p>1.</p><formula xml:id="formula_23">Y (j) k = Γ (k)T j Z k,j 2. N k,j = X (j) k -Y (j) k 3. Γ (k+1) j = Γ (k) j + µ Z k,j N k,j</formula><p>To use the LMS algorithm, the data gathering node will start by querying all of the sensors for uncoded data for the first K rounds of requests. The value of K should be chosen to be large enough to allow the LMS algorithm to converge. After K rounds of requests have been completed, the data gathering node can then ask for coded values from the sensor nodes and decode the coded value for sensor j with respect to its corresponding side information, Y</p><formula xml:id="formula_24">(j) k = Γ T j Z (j)</formula><p>k,j . The value of Γ j will continue to be updated to adjust to changes in the statistics of the data. More specifically, for each round of request and each value reported by a sensor, the decoder will decode Y (j) k to the closest codeword in the subcodebook, S, specified by the corresponding sensor</p><formula xml:id="formula_25">Xk (j) = argmin ri∈S ||Y (j) k -r i || (12)</formula><p>From section II-B, we know that Xk (j)</p><p>will always equal X (j) k as long as the sensor node encodes X (j) k using i bits so that</p><formula xml:id="formula_26">2 i-1 ∆ &gt; |N k,j |. If |N k,j | &gt; 2 i-1 ∆,</formula><p>however, then a decoding error will occur. We can use Chebyshev's inequality <ref type="bibr" target="#b10">[11]</ref> to bound this probability of error:</p><formula xml:id="formula_27">P [|N k,j | &gt; 2 i-1 ∆] ≤ σ 2 Nj (2 i-1 ∆) 2<label>(13)</label></formula><p>where N k,j is drawn from a distribution with zero mean and variance σ 2 Nj . Thus, to insure that P [|N k,j | &gt; 2 i-1 ∆] is less than some probability of error, P e , we can choose</p><formula xml:id="formula_28">σ 2 N j (2 i-1 ∆) 2 = P e .</formula><p>The value of i that will insure this probability of error is then given as</p><formula xml:id="formula_29">i = 1 2 log 2 ( σ 2 Nj ∆ 2 P e ) + 1<label>(14)</label></formula><p>Thus, for a given P e , the data gathering node should ask for i-bits from each sensor according to <ref type="bibr" target="#b13">(14)</ref>. Note that it is not necessary to be over-conservative when choosing P e because Chebyshev's inequality is a loose bound. From ( <ref type="formula" target="#formula_29">14</ref>), we can see that the data gathering node must maintain an estimate of the variance of the prediction error, σ 2 Nj , for each sensor in order to determine the number of bits to request from each sensor. The data gathering node can initialize σ 2 Nj as:</p><formula xml:id="formula_30">σ 2 Nj = 1 K -1 K i=1 N 2 k,j<label>(15)</label></formula><p>during the first K rounds of requests. To update σ 2 Nj , the data gathering node can form the following filtered estimate:</p><formula xml:id="formula_31">σ 2 Nj ,new = (1 -γ)σ 2 Nj ,old + γN 2 k,j<label>(16)</label></formula><p>where σ 2 Nj ,old is the previous estimate of σ 2 Nj and γ is a "forgetting factor" <ref type="bibr" target="#b9">[10]</ref>. We choose to use a filtered estimate to adapt to changes in statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Decoding error</head><p>As mentioned above, it is always possible for the data gathering node to make a decoding error if the magnitude of the correlation noise, |N k,j |, is larger than 2 i-1 ∆ where i is the number of bits used to encode the sensor reading for sensor j at time k. We propose two approaches for dealing with such errors. One method is to use error detection codes and the other method entails using error correction codes.</p><p>To use error detection, each sensor node can transmit a cyclic redundancy check (CRC) <ref type="bibr" target="#b11">[12]</ref> for every m readings that it transmits. The data gathering node will decode the m readings using the tree-structured codebook as above and compare its own calculation of the CRC (based on the m readings it decodes) to the CRC transmitted by the sensor. If an error is detected (i.e, the CRC does not match), then the data gathering node can either drop the m readings or ask for a retransmission of the m readings. Whether the data gathering node drops the m readings or asks for a retransmission is application dependent, and we do not address this issue in this paper. Furthermore, by using Chebyshev's inequality <ref type="bibr" target="#b12">(13)</ref>, the data gathering node can make the probability of decoding error as small as it desires which translates directly into a lower probability of data drops or retransmissions.</p><p>The other method of guarding against decoding error is to use error-correction codes. We propose using a non-binary error correction code such as an (M,K) Reed-Solomon code <ref type="bibr" target="#b12">[13]</ref> that can operate on K sensor readings and generate M -K parity check symbols. These M -K parity check symbols can be transmitted to the data gathering node along with the K encoded sensor readings. The data gathering node will decode the K sensor readings using the tree-based structure mentioned above and upon receiving the M -K parity check symbols, it can correct for any errors that occurred in the K sensor readings. If more than M -K 2 errors exist in the K sensor readings, then the Reed-Solomon decoder will declare that the errors can not be corrected and in this case, the data must be either dropped or retransmitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. QUERYING AND REPORTING ALGORITHM</head><p>In this section, we combine the concepts of the previous sections to formulate the algorithms to be used by the data gathering node and by the sensor node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data gathering node algorithm</head><p>The data gathering node will, in general make N rounds of queries to the sensor nodes. In the first K rounds of queries, the data gathering node will ask the sensors to send their data uncoded. The reason for this is that the data gathering node needs to determine the correlation structure between sensor readings before asking for compressed readings. Thus, the data gathering node will use the first K rounds of readings for calculating the correlation structure in accordance with Sec. III. After K rounds of readings, the data gathering node will have an estimate of the prediction coefficients to be used for each sensor (see <ref type="bibr" target="#b9">(10)</ref>). Note that K should be chosen large enough to allow for the LMS algorithm to converge. For each round after K, one node will be asked to send its reading "uncompressed" with respect to the other sensors 3 . The data gathering node will alternate the requests for "uncompressed" data among the nodes to insure that no single node is asked to expend more energy than the others. Upon receiving a transmission from a sensor, the data gathering node will decode it (if it is a compressed reading) with respect to a linear estimate of the data for that sensor (see <ref type="bibr" target="#b3">(4)</ref>). After each round of requests, the correlation parameters of each sensor (see <ref type="bibr" target="#b9">(10)</ref> and ( <ref type="formula" target="#formula_31">16</ref>)) are updated. Pseudocode for the data gathering node is given below. Fig. <ref type="figure">5</ref>. Tolerable noise vs. prediction noise for 18,000 samples of humidity. The tolerable noise is the amount of noise that can exist between the prediction of a sensor reading and the actual sensor reading without inducing a decoding error. by sensor j as:</p><formula xml:id="formula_32">Y (j) k = 4 l=1 α l X (j) k-l + X (m) k (17)</formula><p>where m = j. In other words the prediction of the reading for sensor j is derived from its own past values and one other sensor. To test the correlation tracking algorithm, we measured the tolerable noise that the correlation tracking algorithm calculates at each time instant. The tolerable noise is the amount of noise that can exist between the prediction of a sensor reading and the actual sensor reading without inducing a decoding error. Tolerable noise is calculated by using <ref type="bibr" target="#b13">(14)</ref>, and noting that the tolerable noise will be given as 2 i-1 ∆ where i is the number of bits that are requested from the sensor and ∆ is the spacing of values in the A/D converter. We set the bound on probability of decoding error to be less than 1 in 100 and simulated the data gathering algorithm and the sensor node algorithms over 18,000 samples of light, temperature and humidity for each sensor (a total of 90,000 samples). A plot of the tolerable noise vs. actual prediction noise is given in Fig. <ref type="figure">5</ref>. where the top graph represents the tolerable noise and the bottom graph represents the actual prediction noise.</p><p>From the plot it can be seen that the tolerable noise is much larger than the actual prediction noise. The reason for this is that we were conservative in choosing the parameters for estimating the number of bits to request from the sensors. The tolerable noise can be lowered to achieve higher efficiency but this also leads to a higher probability of decoding error. For the simulations that we ran, zero decoding errors were made for 90,000 samples of humidity, temperature and light.</p><p>One other thing to note from the plot is that there are many spikes in the tolerable noise. These spikes occur be-cause we chose an aggressive weighting factor for calculating (16). These spikes can be reduced by weighting the current distortion less in the estimation of the overall distortion (see ( <ref type="formula" target="#formula_31">16</ref>)), but this will lead to slower responses to variations in distortion and will therefore introduce more decoding errors for noisy data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Energy savings</head><p>The next set of simulations were run to measure the amount of energy savings that the sensor nodes achieved. The energy savings were calculated to be the total reduction in energy that resulted from transmission and reception. Note that for reception, energy expenditure is actually not reduced but increased because the sensor nodes need to receive the extra bits that specify the number of bits to encode each sensor reading with. For an n-bit A/D converter, an extra log(n) bits need to be received each time the data gathering node informs a sensor of the number of bits needed for encoding. We assume that the energy used to transmit a bit is equivalent to the energy used to receive a bit. To reduce the extra energy needed for reception, we simulated the data gathering node to only specify the number of encoding bits periodically. In our simulations, we chose for this period to be 100 samples for each sensor node. The 5 sensor nodes were alternately queried to send back readings that were compressed only with respect to its own past readings so that compressed readings from other sensors could be decoded with respect to these readings. The overall average savings in energy is given in Table <ref type="table">1</ref>  assess the performance of our algorithm, we choose to use the work of <ref type="bibr" target="#b13">[14]</ref> as a benchmark for comparison. The work of <ref type="bibr" target="#b13">[14]</ref> is also based on a distributed coding framework but the prediction algorithm uses a filtered estimate for the prediction coefficients instead of using a gradient descent algorithm such as LMS to determine the prediction coefficients. Furthermore, in <ref type="bibr" target="#b13">[14]</ref> the prediction algorithm only uses one measurement from a neighboring sensor to form the prediction estimate. Thus, in order to perform a fair comparison, we changed the model of ( <ref type="formula">17</ref>) to only use one measurement from another sensor to form the prediction estimate and surprisingly was able to achieve roughly the same performance as given in Table <ref type="table">1</ref>. The results for humidity are approximately 24% better than the results cited in <ref type="bibr" target="#b13">[14]</ref> for the same data set. Similarly, the results for temperature and light are approximately 16% and 3% better respectively than the results cited in <ref type="bibr" target="#b13">[14]</ref> for the respective data sets. Thus, it is clear that the LMS algorithm is better suited for tracking correlations than the methods given in <ref type="bibr" target="#b13">[14]</ref>.</p><p>One can achieve even larger energy savings than the savings cited above by using a less conservative estimate of the bits ). This will, however, lead to more decoding errors. In our simulations we chose a bound on the probability of decoding error that resulted in 0 decoding errors over 90,000 samples for each of the data sets. In the following subsection, we will evaluate the robustness of our algorithm to errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Robustness to errors</head><p>There are two types of errors that can occur in our framework. The first type of error is a packet loss. The second type of error is an actual decoding error which results from the code not being able to correct for the prediction noise. We will consider each type of error in the following subsections.</p><p>1) Packet loss: A packet loss may occur if a measurement is lost due to a malfunction of the sensor or if there is a transmission loss. In such a case, it appears that this loss should affect the prediction estimates that depend on this measurement (see ( <ref type="formula">17</ref>)). This is not true, however, because the prediction algorithm may replace this measurement with a previous measurement from the same sensor to form the prediction estimate. In fact, we tested a few scenarios in which the packet drop rate was approximately 10% and we were able to achieve the same compression rate with zero decoding errors. Thus, our algorithm has the additional feature that it is robust to packet loss.</p><p>2) Decoding error: The other type of error is a decoding error. Recall, in Sec. III-B, we mentioned that it is possible to make a decoding error if the actual prediction noise between the prediction estimate and the sensor reading exceeds the tolerable noise specified by the data gathering node. One can bound this probability of decoding error by using Chebyshev's inequality to specify the number of bits needed for encoding (see <ref type="bibr" target="#b13">(14)</ref>). But Chebyshev's inequality is a loose bound, as can be seen from Fig. <ref type="figure">5</ref> and as a result, it is difficult to determine the minimal number of bits that need to be sent by each sensor without inducing a decoding error. We can therefore see that there is a delicate trade-off between energy savings and decoding error.</p><p>To achieve both large energy savings and robustness, the data gathering node can use an aggressive estimate of the number of bits that is needed from each sensor and each sensor can apply an error detection code or error correction code to its readings so that the data gathering node can handle decoding errors appropriately. The other alternative is for the data gathering node to over-estimate the number of bits needed for encoding to decrease the decoding error. This is the approach we took in our simulations (we chose a bound such that the decoding error was 0), but the downside to this approach is that there is a corresponding decrease in energy savings for the sensor nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have proposed a method of reducing energy consumption in sensor networks by using distributed compression and adaptive prediction. Distributed compression leverages the fact that there exist inherent correlations between sensor readings and hence sensor readings can be compressed with respect to past sensor readings and sensor readings measured by other nodes. We proposed a novel method for allowing nodes to compress their readings to different levels without having the nodes know what the other nodes are measuring. Adaptive prediction is used to track the correlation structure of the sensor network, and ultimately determined the number of bits needed to be spent by the sensor nodes. This approach appears to be promising, as preliminary results show that an average energy savings per sensor node of 10 -65% can be achieved using our algorithm.</p><p>The energy savings achieved through our simulations are a conservative estimate of what can be achieved in practice. In practice, one can use richer models at the data gathering node to describe the correlation structure in the sensor network. We chose to use a simple predictive model in our simulations to demonstrate the power of our approach. In addition, our algorithm can be combined with other energy-saving approaches such as data aggregation to achieve additional gains. Future work remains in exploring more robust codes for the sensor nodes and better predictive models for the data gathering node along with incorporating our algorithm with energy-saving routing algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 1 :Fig. 2 .</head><label>12</label><figDesc>Fig.2. Distributed compression: the encoder compresses X given that the decoder has access to Y , which is correlated to X.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. A tree-based contruction for compression with side information. The root of the tree contains 2 4 values, and two partitions of the root quantizer are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. To</figDesc><table><row><cell>Data Set</cell><cell cols="2">Temperature Humidity</cell><cell>Light</cell></row><row><cell>Ave Energy Savings</cell><cell>66.6%</cell><cell>44.9%</cell><cell>11.7%</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Ion Stoica, Rahul Shah and Jan Rabaey for some stimulating conversations.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b0">1</ref> <p>This work was supported in part by DARPA-F30602-00-2-0538, NSF-CCR-0219722 and Intel.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudocode for data gathering node:</head><p>Initialization: for (i = 0; i &lt; K; i + +) for (j = 0; j &lt; num sensors; j + +) Ask sensor j for its uncoded reading for each pair of sensors i,j update correlation parameters using Eqs. ( <ref type="formula">16</ref>) and <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Loop:</head><p>for</p><p>Request a sensor for uncoded reading for each remaining sensor determine number of bits, i, to request for using Eq.( <ref type="formula">14</ref>).</p><p>Request for i bits Decode data for each sensor. Update correlation parameters for each sensor. 3 Note that the sensor may still send its data compressed with respect to its own past</p><p>The decoding is done in accordance with Sec. II-B and the correlation parameters are estimated according to Eq. ( <ref type="formula">10</ref>) and (16).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sensor node algorithm</head><p>The algorithm incorporated into each sensor node is considerably simpler than the algorithm incorporated into the data gathering node. The sensor node will simply listen for requests from the data gathering node. The data gathering node will specify to the sensor the number of bits that it requests the sensor to encode the data with. Each sensor will be equipped with an A/D converter that represents the data using n-bits. Upon receiving a request from the data gathering node, the sensor will encode the n-bit value from the A/D converter using i-bits, where i is specified by the data gathering node. This i-bit value is sent back to the data gathering node. Pseudocode for the sensor node is given below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pseudocode for sensor nodes:</head><p>For each request Extract i from the request Get X[n] from A/D converter Transmit n mod 2 i</p><p>In the above algorithm, we denote X[n] as the value returned from the A/D converter and n as the index to this value. Note that the only extra operation with respect to an uncoded system is for the sensor nodes to perform a modulo operation. This makes it extremely cheap for a sensor node to encode its data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. SIMULATION RESULTS</head><p>In this section we provide simulation results. The simulations were performed for measurements on light, temperature and humidity. We wanted to measure not only the energy savings (due to bit transmissions) but also our correlation tracking algorithm and the robustness of our algorithm to errors. We implemented both the data gathering node algorithm and the sensor node algorithm described in Sec. IV. In our first set of simulations, we ran the data gathering algorithm on one machine and the sensor node algorithms on a set of different machines. The sensor node machines simulated the measurement of data by reading from a file, previously recorded readings from actual sensors. The data measured by the sensors were for light, humidity and temperature. We assumed a 12 bit A/D converter with a dynamic range of <ref type="bibr">[-128, 128]</ref> in our simulations and further assumed a star topology where the data gathering node queried 5 sensor nodes directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Correlation tracking</head><p>The first simulation that we ran was to test our correlation tracking algorithm (see Sec. III). We modeled the data received </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Maximum battery life routing to support ubiquitous mobile computing in wireless ad hoc networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Toh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Communications Magazine</title>
		<imprint>
			<biblScope unit="page" from="138" to="147" />
			<date type="published" when="2001-06">June 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy aware routing for low energy ad hoc sensor networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabaey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE WCNC</title>
		<meeting>of IEEE WCNC</meeting>
		<imprint>
			<date type="published" when="2002-03">Mar 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Directed diffusion: A scalable and robust communication paradigm for sensor networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E C</forename><surname>Intanagonwiwat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Govindan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE MobiCom</title>
		<meeting>of IEEE MobiCom</meeting>
		<imprint>
			<date type="published" when="2000-08">Aug 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wireless sensor networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Pottie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scalable information-driven sensor querying and routing for ad hoc heterogeneous sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Haussecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of High Performance Computing Applications</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>To Appear</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noiseless encoding of correlated information sources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Slepian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="471" to="480" />
			<date type="published" when="1973-07">July 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Elements of Information theory</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Wiley</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The rate-distortion function for source coding with side information at the decoder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1976-01">January 1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distributed source coding using syndromes: Design and construction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Comression Conference (DCC)</title>
		<meeting>the Data Comression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="1999-03">March 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adaptive Filter Theory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Haykin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Prentice Hall</publisher>
			<pubPlace>Upper Saddle River</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woods</surname></persName>
		</author>
		<title level="m">Probability, Random Processes and Estimation Theory for Engineers</title>
		<meeting><address><addrLine>Englewood Cliffs</addrLine></address></meeting>
		<imprint>
			<publisher>Prentice Hall</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A tutorial on crc computations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ramabadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gaitonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Micro</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="1988-08">Aug 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Theory and Practice of Data Transmission Codes</title>
		<author>
			<persName><forename type="first">R</forename><surname>Blahut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tracking and exploiting correlations in dense sensor networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ramchandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asilomar Conference on Signals, Systems and Computers</title>
		<meeting>the Asilomar Conference on Signals, Systems and Computers</meeting>
		<imprint>
			<date type="published" when="2002-11">November 2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
