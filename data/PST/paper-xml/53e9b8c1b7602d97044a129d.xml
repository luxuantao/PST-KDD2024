<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Action Recognition from Video Using Feature Covariance Matrices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013-04-24">April 24, 2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Prakash Ishwar, Senior Member, IEEE</roleName><forename type="first">Kai</forename><surname>Guo</surname></persName>
							<email>kaiguo@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineer-ing</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
							<email>jkonrad@bu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineer-ing</orgName>
								<orgName type="institution">Boston University</orgName>
								<address>
									<postCode>02215</postCode>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Action Recognition from Video Using Feature Covariance Matrices</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2013-04-24">April 24, 2013</date>
						</imprint>
					</monogr>
					<idno type="MD5">DE87DCFF1D16E911377C423C093B49DA</idno>
					<idno type="DOI">10.1109/TIP.2013.2252622</idno>
					<note type="submission">received June 15, 2012; revised March 3, 2013; accepted March 6, 2013. Date of publication March 14, 2013; date of current version</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Action recognition</term>
					<term>feature covariance matrix</term>
					<term>nearest-neighbor (NN) classifier</term>
					<term>optical flow</term>
					<term>Riemannian metric</term>
					<term>silhouette tunnel</term>
					<term>sparse linear approximation (SLA)</term>
					<term>video analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a general framework for fast and accurate recognition of actions in video using empirical covariance matrices of features. A dense set of spatio-temporal feature vectors are computed from video to provide a localized description of the action, and subsequently aggregated in an empirical covariance matrix to compactly represent the action. Two supervised learning methods for action recognition are developed using feature covariance matrices. Common to both methods is the transformation of the classification problem in the closed convex cone of covariance matrices into an equivalent problem in the vector space of symmetric matrices via the matrix logarithm. The first method applies nearest-neighbor classification using a suitable Riemannian metric for covariance matrices. The second method approximates the logarithm of a query covariance matrix by a sparse linear combination of the logarithms of training covariance matrices. The action label is then determined from the sparse coefficients. Both methods achieve state-of-the-art classification performance on several datasets, and are robust to action variability, viewpoint changes, and low object resolution. The proposed framework is conceptually simple and has low storage and computational requirements making it attractive for real-time implementation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE PROLIFERATION of surveillance cameras and smartphones has dramatically changed the video capture landscape. There is more video data generated each day than ever before, it is more diverse and its importance has reached beyond security and entertainment (e.g., healthcare, education, environment). Among many video analysis tasks, the recognition of human actions is today of great interest in visual surveillance, video search and retrieval, and humancomputer interaction.</p><p>Despite a significant research effort, recognizing human actions from video is still a challenging problem due to scene complexity (occlusions, clutter, multiple interacting objects, illumination variability, etc.), acquisition issues (camera distortions and movement, viewpoint), and the complexity of human actions (non-rigid objects, intra-and inter-class action variability). Even when there is only a single uncluttered and unoccluded object, and the acquisition conditions are perfect, the complexity and variability of actions make action recognition a difficult problem. Therefore, in this paper we focus on the subproblem concerned with actions by a single object. A single-object video may be obtained by detecting, tracking and isolating object trajectories but this is not the focus of this work. Furthermore, we assume that the beginning and end of an action are known; methods exist to detect such boundaries <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Finally, interactions between objects are not considered here.</p><p>There are two basic components in every action recognition algorithm that affect its accuracy and efficiency: 1) action representation (model); and 2) action classification method. In this paper, we propose a new approach to action representationone based on the empirical covariance matrix of a bag of local action features. An empirical covariance matrix is a compact representation of a dense collection of local features since it captures their second-order statistics and lies in a space of much lower dimensionality than that of the collection. We apply the covariance matrix representation to two types of local feature collections: one derived from a sequence of silhouettes of an object (the so-called silhouette tunnel) and the other derived from the optical flow. While the silhouette tunnel describes the shape of an action, the optical flow describes the motion dynamics of an action. As we demonstrate, both lead to state-of-the-art action recognition performance on several datasets.</p><p>Action recognition can be considered as a supervised learning problem in which the query action class is determined based on a dictionary of labeled action samples. In this paper, we focus on two distinct types of classifiers: 1) the nearest-neighbor (NN) classifier; and 2) the sparse-linearapproximation (SLA) classifier. The NN classifier has been widely used in many supervised learning problems, since it is simple, effective and free of training. The SLA classifier was proposed by Wright et. al <ref type="bibr" target="#b56">[57]</ref> to recognize human faces. The classification is based on the sparse linear approximation of a query sample using an overcomplete dictionary of training samples (base elements).</p><p>The classical NN classifier (based on Euclidean distance) and the SLA classifier are both designed to work with featurevectors that live in a vector space. The set of covariance matrices do not, however, form a vector space-they form a closed convex cone <ref type="bibr" target="#b27">[28]</ref>. A key idea underlying our work is the transformation of the supervised classification problem in the closed convex cone of covariance matrices into an equivalent problem in the vector space of symmetric matrices via the matrix logarithm. Euclidean distance in the log-transformed space, which is a Riemannian metric for covariance matrices, is then used in the NN classifier. Our log-transformed approach for SLA approximates the logarithm of a query action covariance matrix by a sparse linear combination of the logarithm of training action covariance matrices. The action label is then determined from the sparse coefficients.</p><p>The main contributions of this work are: 1) development of a new framework for low-dimensionality action representation based on the empirical covariance matrix of a bag of local features; 2) specification of new local feature vectors for action recognition based on silhouette tunnels; 3) the use of the matrix logarithm to transform the action recognition problem from the closed convex cone of covariance matrices to the vector space of symmetric matrices; 4) application of the sparse linear classifier in the space of log-transformed covariance matrices to perform robust action classification. The proposed framework is independent of the type of objects performing actions (e.g., humans, animals, man-made objects), however since the datasets commonly used in testing contain human actions our experimental results focus on human action recognition. The analysis and results presented here extend our earlier work <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> in several ways. We develop here a unified perspective for action recognition based on feature covariance matrices that subsumes our previous work. We extensively test our framework on 4 often-used datasets and compare its performance in various scenarios (classification method, metric, etc.) with 15 recent methods from the literature. We report results on robustness to viewing angle change and action variability, as well as feature importance that we have not published before.</p><p>The rest of the paper is organized as follows. Section II reviews the current state of the art in action recognition. Section III develops the proposed action recognition framework and Section IV describes two examples of local action features. Section V discusses various aspects of a practical implementation of the proposed approach. Experimental results are presented in Section VI and concluding remarks and comments about future directions are made in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Human action recognition has been extensively studied in the computer vision community and many approaches have been reported in the literature (see <ref type="bibr" target="#b0">[1]</ref> for an excellent survey). Although various categorizations of the proposed approaches are possible, we focus on grouping them according to the action representation model and the classification algorithm used. In terms of the representation model, human action recognition methods can be coarsely grouped into five categories: those based on shape models, motion models, geometric human body models, interest-point models, and dynamic models. As for action classification, most approaches make use of standard machine learning algorithms, such as the NN classifier, support vector machine (SVM), boosting, and classifiers based on graphical models. Some of the most successful approaches to action recognition today use shape-based models for action representation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Such models rely on an accurate estimate of the silhouette of a moving object within each video frame. A sequence of such silhouettes forms a silhouette tunnel, i.e., a spatio-temporal binary mask of the moving object changing its shape in time. Shape-based action recognition is, ideally, invariant to luminance, color, and texture of the moving object (and background), however robust estimation of silhouette tunnels regardless of luminance, color and texture is still challenging. Although silhouette tunnels do not precisely capture motion within objects, the moving silhouette boundary leaves a very distinctive signature of the occurring activity. An effective method based on silhouette tunnels was developed by Gorelick et al. <ref type="bibr" target="#b20">[21]</ref>. At each pixel, the expected length of a random walk to the silhouette tunnel boundary, which can be computed by solving a Poisson equation, is treated as a shape feature of the silhouette tunnel. An action classification algorithm based on this approach was shown to be remarkably accurate suggesting that the method is capable of extracting highly-discriminative information.</p><p>Methods based on motion models extract various characteristics of object movements and deformations, perhaps the most discriminative attributes of actions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. Recently, Ali et al. <ref type="bibr" target="#b3">[4]</ref> proposed kinematic features derived from optical flow for action representation. Each kinematic feature gives rise to a spatio-temporal pattern. Then, kinematic modes are computed by performing Principle Component Analysis (PCA) on the spatio-temporal volumes of kinematic features. Seo and Milanfar <ref type="bibr" target="#b47">[48]</ref> used 3D local steering kernels as action features, that can reveal global spacetime geometric information. The idea behind this approach is based on analyzing the radiometric (pixel value) differences from the estimated space-time gradients, and using this structure information to determine the shape and size of a canonical kernel. Matikainen and Ke et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b39">[40]</ref> also make use of similar notions of capturing local spatiotemporal orientation structure for action recognition. Approaches that make use of distributions of spatiotemporal orientation measurements for action recognition include those of Chomat and Crowley <ref type="bibr" target="#b8">[9]</ref>, Derpanis et al. <ref type="bibr" target="#b11">[12]</ref>, and Jhuang et al. <ref type="bibr" target="#b26">[27]</ref>.</p><p>Since actions of humans are typically of greatest interest, methods focused on explicitly modeling the geometry of the human body form a powerful category of action recognition algorithms <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b53">[54]</ref>. In these methods, first a parametric model is constructed by estimating static and dynamic body parameters, and then these parameters are used for classification. Such methods are mostly used in controlled environments where human body parts, such as legs and arms, are easy to identify. The early work by Goncalves et al. <ref type="bibr" target="#b19">[20]</ref> promoted three-dimensional (3D) tracking of the human arm against a uniform background using a two-cone arm model and a single camera. However, acquiring 3D coordinates of limbs at large distances (outdoors) is still a very challenging problem.</p><p>Interest points have also been employed to represent actions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b55">[56]</ref>. Such points are sufficiently discriminative to establish correspondence in time but are usually sparse (far fewer interest points than the number of pixels in the video sequence). Niebles <ref type="bibr" target="#b40">[41]</ref> and Dollar <ref type="bibr" target="#b12">[13]</ref> used 2D Gaussian and 1D Gabor filters, respectively, to select interest points in the spatio-temporal volume. Laptev et al. <ref type="bibr" target="#b35">[36]</ref> used the Harris corner detector to locate salient points with significant local variations both spatially and in time. Wong et al. <ref type="bibr" target="#b55">[56]</ref> extracted interest points by considering structural information and detecting cuboids in regions that have a large probability of undergoing movement.</p><p>Dynamic models are among the earliest models used for human action recognition <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b58">[59]</ref>. The general idea is to define each static posture of an action as a state, and describe the dynamics (temporal variations) of the action by using a state-space transition model. An action is modeled as a set of states and connections in the state space are made using a dynamic probabilistic network (DPN). Hidden Markov Model (HMM) <ref type="bibr" target="#b30">[31]</ref>, the most commonly used DPN, has the advantage of directly modeling time variations of data features. The parameters of a dynamic model are learned from a set of training action videos, and action recognition reduces to maximizing the joint probability of model states.</p><p>In terms of action classification, algorithms from the machine learning community have been heavily utilized. Some action recognition methods are based on the NN classifier <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b53">[54]</ref>, a straightforward method that requires no explicit training. Other methods recognize actions by using kernel SVMs <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>. Conceptually, a kernel SVM first uses a kernel function to map training samples to a high-dimensional feature space and then finds a hyperplane in this feature space to separate samples belonging to different classes by maximizing the so-called separationmargin between classes. Another popular classification technique used for action recognition is boosting <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b61">[62]</ref> which improves the performance of any family of the socalled weak classifiers by combining them into a strong one. A detailed discussion of popular classifiers can be found in <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK</head><p>In this section, we develop a general framework for action representation and classification using empirical covariance matrices of local features. We describe our choice of features in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Covariance Matrices</head><p>Video samples are typically high dimensional (even a 20-frame sample of a 176×144 QCIF resolution video has half a million dimensions), whereas the number of training video samples is meager in comparison. It is therefore impractical to learn the global structure of training video samples and build classifiers directly in the high-dimensional space. In this paper, we adopt a "bag of dense local feature vectors" modeling approach wherein a dense set of localized features are extracted from the video to describe the action. The advantage of this approach is that even a single video sample provides a very large number of local feature vectors (one per pixel) from which their statistical properties can be reliably estimated. However, the dimensionality of a bag of dense local feature vectors is even larger than the video sample from which it was extracted since the number of pixels is multiplied by the size of the feature vector. This motivates the need for dimensionality reduction. Ideally, one would like to learn the probability density function (pdf) of these local feature vectors. This however, is not only computationally intensive, but it may not lead to a lower-dimensional representation: a kernel-based density estimation algorithm needs to store all the samples used to form the estimate. The mean featurevector, which is low dimensional, can be learned reliably and rapidly but may not be sufficiently discriminative (cf. Section VI-H). Inspired by Tuzel et al.'s work <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, we have discovered that for suitably chosen action features, the feature-covariance matrix can provide a very discriminative representation for action recognition (as evidenced by the excellent experimental results of Section VI). In addition to their simplicity and effectiveness, covariance matrices of local features have low storage and processing requirements. Our approach to action representation is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Let F = {f n } denote a "bag of feature vectors" extracted from a video sample. Let the size of the feature set |F | be N. The empirical estimate of the covariance estimate of the covariance matrix of F is given by</p><formula xml:id="formula_0">C := 1 N N n=1 (f n -μ)(f n -μ) T<label>(1)</label></formula><p>where μ = 1 N N n=1 f n is the empirical mean feature vector. The covariance matrix provides a natural way to fuse multiple feature vectors. The dimension of the covariance matrix is only related to the dimension of the feature vectors. If f n is d-dimensional, then C is a d × d matrix. Due to its symmetry, C only has (d 2 +d)/2 independent numbers. Since d is usually much less than N, C usually lies in a much lower-dimensional space than the "bag of feature vectors" that need N × d dimensions (without additional quantization or dimensionality reduction).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Log-Covariance Matrices</head><p>Covariance matrices are symmetric and non-negative definite. The set of all covariance matrices of a given size does not form a vector space because it is not closed under multiplication with negative scalars. It does, however, form a closed convex cone <ref type="bibr" target="#b27">[28]</ref>. Most of the common machine learning algorithms work with features that are assumed to live in a Euclidean space, not a convex cone. Thus, it would be unreasonable to expect good classification performance by applying the standard learning algorithms directly to covariance matrices. This is corroborated by the experimental results reported in Section VI-H. In order to re-use the existing knowledge base of machine learning algorithms, a key idea is to map the convex cone of covariance matrices to the vector space of symmetric matrices 1 by using the matrix logarithm proposed by Arsigny et al. <ref type="bibr" target="#b4">[5]</ref>. The matrix logarithm of a covariance matrix C is computed as follows. Suppose that the eigen-decomposition of C is given by C = V DV T , where the columns of V are orthonormal eigenvectors and D is the diagonal matrix of (non-negative) eigenvalues. Then log(C) := V DV T , where D is a diagonal matrix obtained from D by replacing D's diagonal entries by their logarithms. Note that the eigenvalues of C are real and positive while those of log(C) are real but can be positive, negative, or zero due to the log mapping. This makes log(C) symmetric but not necessarily positive semidefinite. The family of all log-covariance matrices of a given order coincides with the family of all symmetric matrices of the same order which, as mentioned above, is closed under linear combinations. We will refer to log(C) as the log-covariance matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Classification Using Log-Covariance Matrices</head><p>We have introduced a new action representation using the low-dimensional covariance matrix of a bag-of-features. We now address the problem of classifying a query sample using the representations of training samples and the query sample. In this context, we have investigated two approaches for action recognition, namely NN and SLA classification.</p><p>1) Nearest-Neighbor (NN) Classification: Nearest-neighbor classification is one of the most widely used algorithms in supervised classification. The idea is simple and straightforward: given a query sample, find the most similar sample in the annotated training set, where similarity is measured with respect to some distance measure, and assign its label to the query sample.</p><p>The success of an NN classifier crucially depends on the distance metric used. Tuzel et al. <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref> have argued that Euclidean distance is not a suitable metric for covariance matrices since they do not form a vector space (as previously discussed). Log-covariance matrices do, however, form a vector space. This suggests measuring distances between covariance matrices in terms of the Euclidean distance between their log-transformed representations, specifically</p><formula xml:id="formula_1">ρ 1 (C 1 , C 2 ) := || log(C 1 ) -log(C 2 )|| 2 (2)</formula><p>where log(•) is the matrix-logarithm and || • || 2 denotes the Frobenius norm on matrices. The distance ρ 1 defined above can be shown to be a Riemannian metric on the manifold of covariance matrices. It is referred to as the log-Euclidean metric and was first proposed by Arsigny et al. in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Another Riemannian metric defined on the manifold of covariance matrices is the so-called affine-invariant Riemannian metric proposed by Förstner and Moonen in <ref type="bibr" target="#b18">[19]</ref>. 1 The linear combination of any number of symmetric matrices of the same order is symmetric.</p><p>If C 1 and C 2 are two covariance matrices, it is defined as follows:</p><formula xml:id="formula_2">ρ 2 (C 1 , C 2 ) := || log(C -1 2 2 C 1 C -1 2 2 )|| 2 = d k=1 log 2 λ k (C 1 , C 2 ) (3)</formula><p>where</p><formula xml:id="formula_3">λ k (C 1 , C 2 ) are the generalized eigenvalues of C 1 and C 2 , i.e., C 1 v k = λ k C 2 v k , with v k = 0 being the k-th</formula><p>generalized eigenvector. This distance measure captures the manifold structure of covariance matrices and can be shown to be invariant to invertible affine transformations of the local features. It has been successfully used in object tracking and face localization applications <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>.</p><p>The Riemannian metrics ρ 1 (C 1 , C 2 ) and ρ 2 (C 1 , C 2 ) look very similar in that they both involve taking logarithms of covariance matrices. They are, however, not identical. They are equal if C 1 and C 2 commute, i.e., C 1 C 2 = C 2 C 1 <ref type="bibr" target="#b4">[5]</ref>. In our NN classification experiments we have found that they have very similar performance.</p><p>2) Sparse Linear Approximation (SLA) Classification: In this section, we leverage the discriminative properties of sparse linear approximations to develop an action classification algorithm based on log-covariance matrices. Recently, Wright et al. <ref type="bibr" target="#b56">[57]</ref> developed a powerful framework (closely related to compressive sampling) for supervised classification in vector spaces based on finding a sparse linear approximation of a query vector using an overcomplete dictionary of training vectors.</p><p>The key idea underlying this approach is that if the training vectors of all the classes are pooled together and a query vector is expressed as a linear combination of the fewest possible training vectors, then the training vectors that belong to the same class as the query vector will contribute most to the linear combination in terms of reducing the energy of the approximation error. The pooling together of training vectors of all the classes is important for classification because the training vectors of each individual class may well span the space of all query vectors. Pooling together the training vectors of all the classes induces a "competition" among the training vectors of different classes to approximate the query vector using the fewest possible number of training vectors. This approach is generic and has been successfully applied to many vision tasks such as face recognition, image super-resolution and image denoising.</p><p>We extend this approach to action recognition by applying it to log-covariance matrices. The use of the SLA framework for log-covariance matrices is new. Specifically, we approximate the log-covariance matrix of a query sample p query by a sparse linear combination of log-covariance matrices of all training samples p 1 , . . . , p N . The overall classification framework based on sparse linear approximation is depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. In the remainder of this section, we first explain how the log-covariance matrix of a query sample can be approximated by a sparse linear combination of logcovariance matrices of all training samples by solving an l 1 -norm minimization problem. We then discuss how the locations of large non-zero coefficients in the sparse linear approximation can be used to determine the label of the query sample.</p><p>For i = 1, . . . , N, let p i denote a column-vectorized representation of log(C i ), i.e., the components of log(C i ) (or just the upper-triangular terms, since log(C i ) is a symmetric matrix) rearranged into a column vector in some order. Let K denote the number of rows in p i . Let P := [p 1 , . . . , p N ] denote the K × N matrix whose column vectors are the column-vectorized representations of the log-covariance matrices of the N training samples. We assume, without loss of generality, that all the columns of P that correspond to the same action class are grouped together. Thus, if there are M classes and n j training samples in class j , for j = 1, . . . , M, then the first n 1 columns of P correspond to all the training samples for class 1, the next n 2 columns correspond to class 2, and so on. In this way, we can partition P into M submatrices P := [P 1 P 2 • • • P M ] where, for j = 1, . . . , M, P j is a K ×n j matrix whose columns correspond to all the n j training samples in class j , and N = M j =1 n j . Given a query sample p query , one may attempt to express it as a linear combination of training samples by solving the matrix-vector equation given by</p><formula xml:id="formula_4">p query = Pα ∈ R K (4)</formula><p>where α ∈ R N is the coefficient vector. In the typical dense bag of words setting that we consider, N K . As a result, the system of linear equations associated with p query = Pα is underdetermined and thus its solution α is not unique. <ref type="foot" target="#foot_0">2</ref> We seek a sparse solution to (4) where, under ideal conditions, the only nonzero coefficients in α are those which correspond to the class of the query sample. Such a sparse solution can be found, in principle, by solving the following NP-hard optimization problem:</p><formula xml:id="formula_5">α * = arg min α 0 , s.t. p query = Pα (5)</formula><p>where • 0 denotes the so-called l 0 -norm which counts the number of non-zero entries in a vector. A key result in the the-ory of compressive sampling is that if the optimal solution α * is sufficiently sparse, then solving the l 0 -minimization problem ( <ref type="formula">5</ref>) is equivalent to solving the following l 1 -minimization problem <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_6">α * = arg min α 1 , s.t. p query = Pα. (6)</formula><p>Unlike ( <ref type="formula">5</ref>), this problem is a convex optimization problem that can be solved in polynomial time.</p><p>We have so far dealt with an l 1 -minimization problem where p query = Pα is assumed to hold exactly. In practice, both the video samples and the estimates of log-covariance matrices may be noisy and (4) may not hold exactly. This difficulty can be overcome by introducing a noise term as follows: p query = Pα + z, where z is an additive noise term whose length is assumed to be bounded by ε, i.e., z 2 ≤ ε. This leads to the following -robust l 1 -minimization problem:</p><formula xml:id="formula_7">α * = arg min α 1 , s.t. Pα -p query 2 ≤ ε. (<label>7</label></formula><formula xml:id="formula_8">)</formula><p>We now discuss how the components of α * can be used to determine the label of the query. Each component of α * weights the contribution of its corresponding training sample to the representation of the query sample. Ideally, the nonzero coefficients should only be associated with the class of the query sample. In practice, however, non-zero coefficients will be spread across more than one action class. To decide the label of the query sample, we follow Wright et al. <ref type="bibr" target="#b56">[57]</ref>, and use a reconstruction residual error (RRE) measure to decide the query class. Let</p><formula xml:id="formula_9">α * i = [α * i,1 , α * i,2 , . . . , α * i,n i</formula><p>] denote the coefficients associated with class i (having label l i ), corresponding to columns of training matrix P i . The RRE measure of class i is defined as</p><formula xml:id="formula_10">R i (p query ) = p query -P i α * i 2 . (<label>8</label></formula><formula xml:id="formula_11">)</formula><p>To annotate the sample p query we assign the class label that leads to the minimum RRE label(p query</p><formula xml:id="formula_12">) := l i * , i * := arg min i R i (p query ). (<label>9</label></formula><formula xml:id="formula_13">)</formula><p>The action recognition algorithm based on the sparse linear approximation framework can be summarized as follows:</p><p>1) compute the log-covariance descriptor for each video sample; 2) given p query , solve the l 1 -minimization problem <ref type="bibr" target="#b6">(7)</ref> to obtain α * ; 3) compute RRE for each class i based on (8); 4) annotate the query sample p query using <ref type="bibr" target="#b8">(9)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ACTION FEATURES</head><p>Thus far, we have introduced a general framework for action recognition comprising an action representation based on empirical feature-covariance matrices and classification based on nearest-neighbor and sparse linear approximation algorithms. This framework is generic and can be applied to different supervised learning problems. The success of this framework in action recognition will depend on the ability of the selected features to capture and discriminate motion dynamics. We now introduce two examples of local feature vectors that capture discriminative characteristics of human actions in videos. The first one is based on the shape of the silhouette tunnel of a moving object while the second one is based on optical flow which explicitly captures the motion dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Silhouette Tunnel Shape Features</head><p>Humans performing similar actions can exhibit very different photometric, chromatic and textural properties in different video samples. Feature vectors used for action recognition should therefore be relatively invariant to these properties. One way to construct feature vectors that possess these invariance properties is to base them on the sequence of 2D silhouettes of a moving and deforming object (see Fig. <ref type="figure" target="#fig_2">3</ref>). Simple background subtraction techniques <ref type="bibr" target="#b16">[17]</ref> and more-advanced spatiotemporal video segmentation methods based on level-sets <ref type="bibr" target="#b42">[43]</ref> can be used to robustly and efficiently estimate an object silhouette sequence from a raw video action sequence. Under ideal conditions, each frame in the silhouette sequence would contain a white mask (white = 1) which exactly coincides with the 2D silhouette of the moving and deforming object against a "static" black background (black = 0). A sequence of such object silhouettes in time forms a spatio-temporal volume in x-y-t space that we refer to as a silhouette tunnel. Silhouette tunnels accurately capture the moving object dynamics (actions) in terms of a 3D shape. Action recognition then reduces to a shape recognition problem which typically requires some measure of similarity between pairs of 3D shapes. There is an extensive body of literature devoted to the representation and comparison of shapes of volumetric objects. A variety of approaches have been explored ranging from deterministic mesh models used in the graphics community to statistical models, both parametric (e.g., ellipsoidal models) and non-parametric (e.g., Fourier descriptors). Our goal is to reliably discriminate between shapes; not to accurately reconstruct them. Hence a coarse, low-dimensional representation of shape would suffice. We capture the shape of the 3D silhouette tunnel by the empirical covariance matrix of a bag of thirteendimensional local shape features <ref type="bibr" target="#b22">[23]</ref>.</p><p>1) Shape Feature Vectors: Let s = (x, y, t) T denote the horizontal, vertical, and temporal coordinates of a pixel. Let A denote the set of coordinates of all pixels belonging to an action segment (a short video clip) which is W pixels wide, H pixels tall, and L frames long, i.e., A := {(x, y, t</p><formula xml:id="formula_14">) T : x ∈ [1, W ], y ∈ [1, H ], t ∈ [1, L]}.</formula><p>Let S denote the subset of pixel-coordinates in A which belong to the silhouette tunnel. With each s within the silhouette tunnel, we associate the Each point s 0 = (x 0 , y 0 , t 0 ) T of a silhouette tunnel within an L-frame action segment has a 13-dimensional feature vector associated with it: 3 position features x 0 , y 0 , t 0 , and 10 shape features given by distance measurements from (x 0 , y 0 , t 0 ) to the tunnel boundary along ten different spatio-temporal directions.</p><p>following 13-dimensional feature vector f(s) that captures certain shape characteristics of the tunnel:</p><formula xml:id="formula_15">f(x, y, t) := [x, y, t, d E , d W , d N , d S , d N E , d SW , d S E , d N W , d T + , d T -] T (<label>10</label></formula><formula xml:id="formula_16">)</formula><p>where (x, y, t) will not always add up to L at every spatial location (x, y), especially near the boundaries, since the silhouette shape is typically not constant in time). Fig. <ref type="figure" target="#fig_3">4</ref> depicts these features graphically. Clearly, these 10 distance measurements capture (coarsely) the silhouette tunnel shape as "seen" from location (x, y, t) T . While there are numerous spatio-temporal local descriptors for shape developed in the literature, our choice of shape features is motivated by considerations of simplicity, computational tractability, and the work of Gorelick et al. <ref type="bibr" target="#b20">[21]</ref> who used, among other features, the expected time it takes a 2D random walk initiated from a point inside the silhouette tunnel to hit the boundary. There is one shape feature vector f associated with each pixel of a silhouette tunnel, and thus there are a large number of feature vectors. The collection of all feature vectors F := {f(s) : s ∈ S} is an overcomplete representation of the shape of the silhouette tunnel because S is completely determined by F and F contains additional data which are redundant. To the best of our knowledge, the use of empirical covariance matrices of local shape descriptors for shape classification is new.</p><p>2) Shape Covariance Matrix: After obtaining 13dimensional silhouette shape feature vectors, we can compute their 13 × 13 covariance matrix, denoted by C, using (1) (with N = |S|). Here we give an alternative interpretation to <ref type="bibr" target="#b0">(1)</ref>. If we let S = (X, Y, T ) T denote a random location vector which is uniformly distributed over S, i.e., the probability mass function of S is equal to zero for all locations s / ∈ S and is equal to 1/|S| at all locations in S, where |S| denotes the volume of the silhouette tunnel, then C = cov(F), where F := f(S). More explicitly</p><formula xml:id="formula_17">C := cov(F) = 1 |S| s∈S (f(s) -μ F )(f(s) -μ F ) T (<label>11</label></formula><formula xml:id="formula_18">)</formula><p>where</p><formula xml:id="formula_19">μ F = E[F] = s∈S 1 |S| f(s)</formula><p>is the mean feature vector. Thus, C is an empirical covariance matrix of the collection of vectors F . It captures the second-order empirical statistical properties of the collection.</p><p>3) Normalization for Spatial Scale-Invariance: The shape covariance matrix C in (11) computed from the 13 features in <ref type="bibr" target="#b9">(10)</ref> is not invariant to spatial scaling of the silhouette tunnel, i.e., two silhouette tunnels S and S that have identical shape but differ in spatial scale will have different covariance matrices. To illustrate the problem, ignoring integer-valued constraints, let a &gt; 0 be a spatial scale factor and let S := {(ax, ay, t) T : (x, y, t) T ∈ S} be a silhouette tunnel obtained from S by stretching the horizontal and vertical dimension (but not time) by the factor a. Then, |S | = a 2 |S|. Consider the covariance between the x-coordinate and the distance to the top boundary d N (both are spatial features) for both S and S . These are respectively given by cov(X, D N ) <ref type="foot" target="#foot_1">3</ref> and cov(X , D N ) where X = a X and</p><formula xml:id="formula_20">D N = a D N . Consequently, cov(X , D N ) = a 2 cov(X, D N ).</formula><p>An identical relationship holds for the covariance between any pair of spatial features. The covariance between any spatial feature and any temporal feature for S will be a times that for S (instead of a 2 ) and the covariance between any pair of temporal features for S and S will be equal. To see how the shape covariance matrix can be made invariant to spatial scaling of the silhouette tunnel, observe that cov</p><formula xml:id="formula_21">(X / √ |S |, D N / √ |S |) = cov(X/ √ |S|, D N / √ |S|).</formula><p>Thus, in order to obtain a spatially scale-invariant shape covariance matrix, we must divide every spatial feature by the square root of the volume of the silhouette tunnel before computing the empirical covariance matrix using <ref type="bibr" target="#b10">(11)</ref>.</p><p>A similar approach can be used for temporal scaling which can arise due to frame-rate differences between the query and training action segments. However, since most cameras run at either 15 or 30 frames per second, in this work we assume that the two frame rates are identical and the segment size L is the same for the query and training action segments. Temporal scaling may also be needed to compensate for variations in execution speeds of actions. We assume that the dictionary is sufficiently rich to capture the typical variations in execution speeds. By construction, the shape covariance matrix is automatically invariant to spatiotemporal translation of the silhouette tunnel. It is, however, not invariant to rotation of the silhouette tunnel about the horizontal, vertical, and temporal axes. Rotations about the temporal axis by multiples of 45°have the effect of permuting the 8 spatial directions of the feature vector. In this work, we assume that the query and training silhouette tunnels have roughly the same spatial orientation (however see Section VI-F for viewpoint robustness experiments). Finally, we do not consider perspective-induced variations that are manifested as anisotropic distortions, keystoning, and the like. These variations can be, in principle, accounted for by enriching the dictionary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optical Flow Features</head><p>We just introduced 13-dimensional local silhouette shape feature vectors. However, silhouette tunnels are sometimes noisy and unreliable due to the complexity of real-life environments (e.g., camera jitter, global illumination change, intermittent object motion) and the intrinsic deficiencies of background subtraction algorithms. Motivated by this, we explore a different family of local feature vectors which are based on optical flow. There have been hundreds of papers written in the past few decades on the computation of optical flow. Here we use a variant of the Horn and Schunck method, which optimizes a functional based on residuals from the intensity constraints and a smoothness regularization term <ref type="bibr" target="#b60">[61]</ref>. Let I (x, y, t) denote the luminance of the raw video sequence at pixel position (x, y, t) and let u(x, y, t) represent the corresponding optical flow vector u = (u, v) T . Based on I (x, y, t) and u(x, y, t), we use the following feature vector f(x, y, t):</p><formula xml:id="formula_22">f(x, y, t) := [x, y, t, I t , u, v, u t , v t , Di v, V or, Gten, Sten] T (<label>12</label></formula><formula xml:id="formula_23">)</formula><p>where (x, y, t) T ∈ A (the set of all pixel coordinates in a video segment), I t is the 1-st order partial derivative of I (x, y, t) with respect to t, i.e., I t = ∂ I (x, y, t)/∂t, u and v are optical flow components, and u t and v t are their 1-st order partial derivatives with respect to t. Di v, V or, Gsten, and Sten, described below, are respectively the divergence, vorticity, and two tensor invariants of the optical flow proposed by Ali et. al <ref type="bibr" target="#b3">[4]</ref> in the context of action recognition. Di v is the spatial divergence of the flow field and is defined at each pixel position as follows:</p><formula xml:id="formula_24">Di v(x, y, t) = ∂u(x, y, t) ∂ x + ∂v(x, y, t) ∂y . (<label>13</label></formula><formula xml:id="formula_25">)</formula><p>Divergence captures the amount of local expansion in the fluid which can indicate action differences. V or is the vorticity of a flow field and is defined as</p><formula xml:id="formula_26">V or(x, y, t) = ∂v(x, y, t) ∂ x - ∂u(x, y, t) ∂y . (<label>14</label></formula><formula xml:id="formula_27">)</formula><p>In fluid dynamics, vorticity is used to measure local spin around the axis perpendicular to the plane of the flow field. In the context of optical flow, this can potentially capture locally circular motions of a moving object. </p><formula xml:id="formula_28">Sten(x, y, t) = 1 2 (tr 2 (S(x, y, t)) -tr(S 2 (x, y, t))) (<label>18</label></formula><formula xml:id="formula_29">)</formula><p>where tr(•) denotes the trace operation. Gten and Sten are scalar properties that combine gradient tensor components thus accounting for local fluid structures. As in the silhouette-based action representation, we can compute the 12 × 12 empirical covariance matrix C of the optical flow feature vectors. Typically, only a small subset of all the pixels in a video segment belong to a moving object and ideally one should use optical flow features only from this subset. Unlike silhouettes, optical flow does not provide an explicit demarcation of the pixels that belong to a moving object. However, the magnitude of I t at a given pixel is a rough indicator of motion, with small values in a largely static background and large values in moving objects. Motivated by this observation, in order to calculate the covariance matrix of optical flow features, we only use feature vectors from locations where |I t | is greater than some threshold. Such a thresholded optical flow provides, in effect, a crude silhouette tunnel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PRACTICAL CONSIDERATIONS</head><p>An important practical issue is the processing of continuous video with limited memory. A simple solution is to break up the video into small action segments and process (classify) each segment in turn. What should be the duration of these segments? An important property shared by many human actions is their repetitive nature. Actions such as walking, running, and waving, consist of many roughly periodic action segments. If the duration of an action segment is too short, then there would not be enough information to perform reliable recognition. On the other hand if it is too long then there is a risk of including more than one repetition of the same action which can exacerbate segment misalignment issues (see below). A somewhat robust choice for the duration of an action segment is its median period. The typical period for many human actions is on the order of 0.4-0.8 s (with the exception of very fast or slow actions). For example, Donelan et al. <ref type="bibr" target="#b13">[14]</ref> measured the average preferred period of walking steps to be about 0.55 s. For a camera operating at 30 fps, a period of 0.4-0.8 s translates to a typical length for an action segment on the order of 12-24 frames. In our experiments, we have used segments as short as L = 8, in order to assure a fair comparison with the results of Gorelick et al. <ref type="bibr" target="#b20">[21]</ref>, and as long as L = 20 that, we feel, match typical scenarios better.</p><p>Another practical issue pertains to temporal misalignments between training and query action segments. A somewhat coarse synchronization can be achieved by making successive action segments overlap. Overlapping action segments has the additional benefit of enriching the training set so that a query action can be classified more reliably.</p><p>After partitioning a query video into overlapping action segments, we can apply our action recognition framework to each segment and obtain a sequence of annotated segments. If each query video contains only a single action, then we can use the majority rule to fuse the individual segment-level decisions (labels) into a global sequence-level decision, i.e., assigning the most popular label among all query segments to the query video.</p><p>With these practical considerations, our overall approach for action recognition can be summarized as follows. We start with a raw query video sequence which has only one moving object. Then, depending on which set of features are to be used, we compute the silhouette tunnel <ref type="foot" target="#foot_2">4</ref> or optical flow of this action sequence, and subsequently extract the local features from either of them and form the feature flow. We break the feature flow into a set of overlapping L-frame-long segments where L is assumed to be large enough so that each segment is representative of the action. In each segment, the feature flows are fused into a covariance matrix. <ref type="foot" target="#foot_3">5</ref> The query covariance matrix is then classified using either the NN or SLA classifier. Finally, the action label of the query sequence is determined by applying the majority rule to all the action segment labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTAL RESULTS</head><p>We evaluated our action recognition framework on four publicly available datasets: Weizmann <ref type="bibr" target="#b20">[21]</ref>, KTH <ref type="bibr" target="#b45">[46]</ref>, UT-Tower <ref type="bibr" target="#b6">[7]</ref> and YouTube <ref type="bibr" target="#b38">[39]</ref>. Fig. <ref type="figure" target="#fig_4">5</ref> shows sample frames from all four datasets. We tested the performance of the NN and SLA classifiers with silhouette features (if available) and optical-flow features. This is a total of four possible combinations of classifiers and feature-vectors. The Weizmann and UT-Tower datasets include silhouette sequences whereas the KTH and YouTube datasets do not. We therefore report results with silhouette features only for the Weizmann and UT-Tower datasets. We estimate the optical flow for all the datasets using a variant of the Horn and Schunck method <ref type="bibr" target="#b60">[61]</ref>. For NN classification, we report results only for the affine-invariant metric (3) since its performance in our experiments was very similar to that for the log-Euclidean metric.</p><p>Our performance evaluation was based on leave-one-out cross validation (LOOCV). In all experiments, we first divided each video sequence into L-frame long overlapping action segments, for L = 8, 20 (see the discussion of segment length selection at the beginning of Section V), with 4-frame overlap. Then, we selected one of the action segments as a query segment and used the remaining segments as the training set (except those segments that came from the same video sequence as the query segment). Finally, we identified action class of the query segment. We repeated the procedure for all query segments in the dataset and calculated the correct classification rate (CCR) as the percentage of query segments that were correctly classified. We call this rate the segmentlevel CCR, or SEG-CCR. In practice, however, one is usually interested in classification of a complete video sequence instead of one of its segments. Since segments provide timelocalized action information, in order to obtain classification for the complete video sequence we employed the majority rule (dominant label wins) to all segments in this sequence. This produces a sequence-level CCR, or SEQ-CCR, defined as the percentage of query sequences that are correctly classified. We also tested our method using "leave-part-out" cross validation (LPOCV), a more challenging test sometimes reported in the literature. In LPOCV, we divided the action segments into non-overlapping training set and test set. After selecting a test segment from the test set, we assigned a class label based on the training set. We repeated this procedure for all test segments. The main difference between LPOCV and LOOCV is that the training set in LPOCV is fixed with less training samples than in LOOCV if both are based on the same dataset. Thus, it is expected that LPOCV will attain poorer performance than LOOCV if other settings remain the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Weizmann Dataset</head><p>We conducted a series of experiments on the Weizmann Human Action Database available on-line 6 <ref type="bibr" target="#b20">[21]</ref>. Although this is not a very challenging dataset, many state-of-the-art approaches report performance on it thus allowing easy comparison. The database contains 90 low-resolution video and silhouette sequences (180 × 144 pixels) that show 9 different people each performing 10 different actions, such as jumping, walking, running, skipping, etc.</p><p>1) Silhouette Features: We tested our method's performance using silhouette-based feature vectors for both NN and SLA classifiers. Table <ref type="table" target="#tab_2">I</ref> shows SEG-CCR and SEQ-CCR for each classifier and CCRs for individual actions taken from the diagonal entries of the corresponding confusion matrices 7 <ref type="bibr" target="#b21">[22]</ref>. 6 Available at http://www.wisdom.weizmann.ac.il/$\ sim$vision/SpaceTime Actions.html. 7 A matrix whose i j-th entry equals the fraction of action-i segments/ sequences that are classified as action-j.</p><p>In order to further compare performance of both classifiers, we computed their CCR's for different segment lengths (L = 8 and L = 20) and different cross validation methods (LOOCV and LPOCV). We show detailed results in Table <ref type="table" target="#tab_3">II</ref>. Note, that in LPOCV we evenly broke up the dataset into training and test sets. We have also compared the performance of our approach with some recent methods from the literature that report LOOCV (Table <ref type="table" target="#tab_4">III</ref>).</p><p>In view of the above results, we conclude that the proposed silhouette-based action recognition framework achieves remarkable recognition rates and outperforms most of the recent methods reported in the literature. We also conclude that longer segments may lead to improved performance, although this improvement is unlikely to be monotonic (it depends on action persistence and period), and that the SLA classifier outperforms the NN classifier in most cases. Finally, taking a majority vote among multiple segments further improves classification performance.</p><p>2) Optical-Flow Features: We also tested our approach using optical-flow feature vectors. Detailed results are shown in Table <ref type="table" target="#tab_5">IV</ref>. Compared with silhouette features, opticalflow features result in performance degradation by 7-10%, indicating that silhouette features better represent an action when reliable silhouettes are available. However, since reliable silhouettes may be difficult to compute in some scenarios, optical-flow features may be a good alternative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. KTH Dataset</head><p>This dataset contains six different human actions: handclapping, hand-waving, walking, jogging, running and boxing, performed repeatedly by 25 people in 4 different scenarios (outdoor, outdoor with zoomed camera, outdoor with different   clothes, and indoor). This is a more challenging dataset because the camera is no longer static (vibration and zoomin/zoom-out) and there are more action variations. This dataset does not include silhouettes. Due to camera movements, we could not obtain reliable silhouettes, and thus we performed evaluation using optical-flow features only. Table <ref type="table" target="#tab_6">V</ref> shows SEG-CCR and SEQ-CCR for each classifier using LOOCV and L = 20. In LPOCV tests, we followed the training/query set break-up as proposed by Schuldt et al. <ref type="bibr" target="#b45">[46]</ref>. We show detailed results in Table VI for both classifiers.</p><p>We also compared the proposed method with some recent action recognition algorithms. Table <ref type="table" target="#tab_8">VII</ref> shows results for LOOCV tests and Table <ref type="table" target="#tab_9">VIII</ref> shows results for LPOCV tests. Since we know that LPOCV is more challenging than LOOCV, it is unfair to compare method A that is tested under LOOCV with method B that is tested under LPOCV. Although the segment-level recognition (SEG-CCR) is a little worse than for Ali et al.'s method <ref type="bibr" target="#b3">[4]</ref>, our sequence-level recognition rates (SEQ-CCR) are in line with the best methods today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. UT-Tower Dataset</head><p>The UT-Tower action dataset was used in the ICPR-2010 contest on Semantic Description of Human Activities (SDHA), and more specifically the "Aerial View Activity Classification Challenge". This dataset contains video sequences of a single person performing various actions taken from the top of the main tower at the University of Texas at Austin. It consists of 108 videos with 360 × 240-pixel resolution and 10 fps frame rate. The contest required classifying video sequences into 9 categories of human actions: {1: pointing, 2: standing, 3: digging, 4: walking, 5: carrying, 6: running, 7: wave1, 8: wave2, 9: jumping}. Each of the 9 actions was performed two times by 6 individuals for a total of 12 video sequences per action category. The pointing, standing, digging, and walking videos have been captured against concrete surface, whereas the carrying, running, wave1, wave2, and jumping videos have grass in the background. The cameras are stationary but have jitter. The average height of human figures in this dataset is about 20 pixels. In addition to the challenges associated with low resolution of objects of interest, further challenges result from shadows and blurry visual cues. Ground-truth action labels were provided for all video sequences for training and testing. Also, moving object silhouettes were included in the dataset.</p><p>1) Silhouette Features: For the NN classifier with silhouette-based features, Table <ref type="table" target="#tab_10">IX</ref> shows the results for LOOCV and L = 8 using each classifier.</p><p>2) Optical-Flow Features: We also tested the performance of our approach using optical-flow features using LOOCV and L = 8. Detailed results are shown in Table <ref type="table" target="#tab_11">X</ref>.</p><p>Clearly, the proposed silhouette-based action recognition outperforms its optical-flow-based counterpart by over 10%. This is not surprising when one closely examines two special actions: pointing and standing. These actions, strictly speaking, are not actions as they involve no movement. Thus, optical flow computed in each case is zero (except for noise and errors) leading to failure of optical-flow-based approaches. On the other hand, silhouette-based approaches are less affected since pointing and standing can still be described by the 3D silhouette shape. Also, note a much higher silhouette-      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. YouTube Dataset</head><p>The Youtube dataset is a very complex dataset based on YouTube videos <ref type="bibr" target="#b38">[39]</ref>. This dataset contains 11 action classes: basketball shooting, biking/cycling, diving, golf swinging, horse-back riding, soccer juggling, swinging, tennis swinging,    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Run-Time Performance</head><p>The proposed approaches are computationally efficient and easy to implement. Feature vectors and empirical feature covariance matrices can be computed quickly using the method of integral images <ref type="bibr" target="#b50">[51]</ref>. The matrix logarithm is no more difficult to compute than performing an eigen-decomposition or an SVD. Our experimental platform was Intel Centrino (CPU: T7500 2.2 GHz + Memory: 2 GB) with Matlab 7.6. The extraction of 13-dimensional feature vectors from a silhouette tunnel and calculation of covariance matrices  <ref type="table" target="#tab_9">VIII</ref>) slightly outperform our approach by about 1%, our methods have a computational advantage. In comparison with the Gorelick et al. method, our approach is conceptually much simpler and thus easier to implement, and also faster since distances to the silhouette boundary are efficiently computed using the concept of integral images (a single sweep through a silhouette returns distances to the boundary along a specific direction for all silhouette points), as opposed to seeking random distances via Poisson equations in the method of Gorelick et al. As for the Ali et al. method, we use only a subset of their optical flow features and we do not search for dominant kinematic modes of each feature that requires the use of PCA thus reducing our computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Robustness Experiments</head><p>Our experiments thus far indicate that the proposed framework performs well when the query action is similar to the dictionary actions. In practice, however, the query action may be "distorted", e.g., a person may be carrying a bag while walking, or may be captured from a different viewpoint. We tested the robustness of our approach to action variability and camera viewpoint on videos originally used by Gorelick <ref type="bibr" target="#b20">[21]</ref> that include 10 walking people in various scenarios (walking with a briefcase, limping, etc.). We tested both silhouette features and optical-flow features using the NN classifier.</p><p>LOOCV experimental results for action variability are shown in Table <ref type="table" target="#tab_12">XII</ref>. Since there is only one instance of each type of test sequence, SEQ-CCR must be either 100% or 0%. Clearly, all test sequences are correctly labeled even if some segments were misclassified. This matches the results reported in <ref type="bibr" target="#b20">[21]</ref>. Also, the optical-flow features perform better overall than the silhouette features (except for "Knees up" at segment level).</p><p>LOOCV experimental results for viewpoint dependence are shown in Table <ref type="table" target="#tab_14">XIII</ref>. The test videos contain the action of walking captured from different angles (varying from 0°to 81°with steps of 9°with 0°being the side view). The action samples in the training dataset (Weizmann dataset) are all captured from the side view. Thus, it is expected that the classification performance will degrade when the camera angle increases. The results indicate that silhouette features are robust for walking up to about 36°in viewpoint change and that confusion starts at about 54°(walking recognized as other actions). Optical-flow features perform slightly better in this case; good performance continues up to about 54°and misclassification starts around 72°. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Analysis of Feature Importance</head><p>We have presented action recognition results on several datasets for two categories of features, namely those based on silhouettes and those based on optical flow. For each category of features, some feature components may be more important than others for classification. To discover their relative importance, we tested the contribution of different subsets of feature components to the classification performance. This set of experiments is based on the Weizmann dataset using LOOCV, segment length L = 8, and NN-classification using the affine-invariant metric (3).</p><p>1) Silhouette Features: The silhouette feature vector f(x, y, t) is defined in <ref type="bibr" target="#b9">(10)</ref>. In order to study the usefulness of individual features, we partitioned f(x, y, t) into three subsets: is the most significant one. In contrast, the subset (d T + , d T -) contributes the least to action classification. However, the combination of (x, y, t) and (d T + )/(d T -) leads to a remarkable classification performance (up to 95.56%). Thus, even if (d T + , d T -) alone are not sufficiently discriminative for action recognition, when combined with other features, they contribute significant additional information for improving discrimination.</p><p>2) Optical Flow Features: The optical-flow feature vector is defined in <ref type="bibr" target="#b11">(12)</ref>. The optical-flow feature components can be partitioned into three groups:</p><p>1) (x, y, t): spatio-temporal coordinates;</p><p>2) (I t , u, v, u t , v t ): optical flow and temporal gradients; 3) (Di v, V or, Gten, Sten): optical-flow descriptors derived from fluid dynamics. Table <ref type="table" target="#tab_16">XV</ref> shows the classification performance (overall SEG-CCR and SEQ-CCR across all actions) using subsets of optical-flow feature components on the Weizmann dataset. From this table we see that (I t , u, v, u t , v t ) is the most significant feature subset for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Representation and Metric Comparison</head><p>The experiments so far indicate that feature covariance matrices are sufficiently discriminative for action recognition, and the log-Euclidean distance between covariance matrices is an appropriate metric. However, how would other representations or metrics fair against them? To answer this question, we performed several LOOCV experiments using silhouette features and the NN classifier on the Weizmann dataset. First, rather than using second-order statistics to characterize localized features we tested first-order statistics, i.e., the mean, under the Euclidean distance metric. As is clear from Table XVI, recognition performance using the mean representation is vastly inferior to that of the covariance representation with the log-Euclidean metric (over 50% drop). Secondly, we used the covariance matrix representation with a Euclidean metric. Again, the performance dropped dramatically compared to the covariance representation with a log-Euclidean metric. Finally, we assumed that feature vectors are drawn from a Gaussian distribution and we estimated this distribution's mean vector and covariance matrix. Then, we used KL-divergence to measure the distance between two Gaussian distributions. This approach fared much better but still trailed the performance of the covariance matrix representation with the log-Euclidean metric by 6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>The action recognition framework that we have developed in this paper is conceptually simple, easy to implement, has good run-time performance, and performs on par with stateof-the-art methods; tested on four datasets, it significantly outperforms most of the 15 methods we compared against. While encouraging, without substantial modifications to the proposed method that are beyond the scope of this work, its action recognition performance is likely to suffer in scenarios where the acquisition conditions are harsh and there are multiple cluttered and occluded objects of interest that cannot be reliably extracted via preprocessing, e.g., in humanhuman and human-vehicle interactions. The TRECVID <ref type="bibr" target="#b62">[63]</ref> and VIRAT <ref type="bibr" target="#b63">[64]</ref> video datasets exemplify these types of realworld challenges and much work remains to be done to address them. Our method's relative simplicity, as compared to some of the top methods in the literature, enables almost tuningfree rapid deployment and real-time operation. This opens new application areas outside the traditional surveillance/security arena, for example in sports video annotation and customizable human-computer interaction (for examples, please visit <ref type="bibr" target="#b64">[65]</ref>). In fact, recently we have implemented a simplified variant of our method that recognizes hand gestures in real time using the Microsoft Kinect <ref type="bibr" target="#b34">[35]</ref>. Our method is robust to user height, body shape, clothing, etc., is easily adaptable to different scenarios, and requires almost no tuning. Furthermore, it has a good recognition accuracy in real-life scenarios. Can a "gesture mouse" replace the computer mouse and touch panel in the near future? Although unsuitable for personal computers, this vision is not without merit in such scenarios as large information displays, "wet labs" where the use of mouse/keyboard could cause contamination, etc. <ref type="bibr" target="#b65">[66]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Action representation based on the low-dimensional empirical covariance matrix of a bag of local feature vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Block diagram of a classification algorithm based on approximating the log-covariance matrix of a query by a sparse linear combination of the log-covariance matrices of training samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Human action sequence. Three frames from (a) jumping-jack action sequence and (b) corresponding silhouettes from the Weizmann human action database.</figDesc><graphic coords="6,82.02,53.24,196.68,107.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4.Each point s 0 = (x 0 , y 0 , t 0 ) T of a silhouette tunnel within an L-frame action segment has a 13-dimensional feature vector associated with it: 3 position features x 0 , y 0 , t 0 , and 10 shape features given by distance measurements from (x 0 , y 0 , t 0 ) to the tunnel boundary along ten different spatio-temporal directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Sample frames for different actions from the datasets. (a) Weizmann. (b) KTH. (c) UT_tower. (d) YouTube.</figDesc><graphic coords="9,104.15,262.97,73.70,55.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 )</head><label>1</label><figDesc>(x, y, t): spatio-temporal coordinates; 2) (d E , d W , d N , d S , d NE , d SW , d SE , d NW ): spatial distances; 3) (d T + , d T -): temporal distances.The spatio-temporal coordinates provide localization information and spatial/temporal distances describe local spatial/temporal shape deformations. Table XIV shows the classification performance (overall SEG-CCR and SEQ-CCR across all actions) using subsets of feature components on the Weizmann dataset. The results indicate that the subset (d NE , d SW , d SE , d NW )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I LOOCV</head><label>I</label><figDesc>RECOGNITION PERFORMANCE FOR SILHOUETTE-BASED FEATURE VECTOR FOR L = 8 ON THE Weizmann DATASET</figDesc><table><row><cell>Classifier</cell><cell>Action</cell><cell>Bend</cell><cell>Jack</cell><cell>Jump</cell><cell>Sjump</cell><cell>Run</cell><cell>Side</cell><cell>Skip</cell><cell>Walk</cell><cell>Wave1</cell><cell>Wave2</cell><cell>Average</cell></row><row><cell>NN</cell><cell cols="12">SEG-CCR 98.6 100 96.1 99.3 SEQ-CCR 100 100 100 100 100 100 100 100 100 100 94 100 86.7 98.7 98.0 95.1 97.05 100</cell></row><row><cell>SLA</cell><cell cols="12">SEG-CCR 91.9 99.4 95.1 96.7 91.6 100 92.7 100 99.4 97.2 96.74 SEQ-CCR 100 100 100 100 100 100 100 100 100 100 100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF SILHOUETTE-BASED NN AND SLA CLASSIFIERS ON THE Weizmann DATASET</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NN Classifier</cell><cell cols="2">SLA Classifier</cell></row><row><cell></cell><cell></cell><cell cols="4">SEG-CCR SEQ-CCR SEG-CCR SEQ-CCR</cell></row><row><cell>L = 8</cell><cell>LOOCV LPOCV</cell><cell>97.05% 90.88%</cell><cell>100% 91.11%</cell><cell>96.74% 91.35%</cell><cell>100% 95.56%</cell></row><row><cell>L = 20</cell><cell>LOOCV LPOCV</cell><cell>98.68% 91.82%</cell><cell>100% 95.56%</cell><cell>99.49% 93.61%</cell><cell>100% 95.56%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF THE SILHOUETTE-BASED ACTION RECOGNITION (L = 8) WITH STATE-OF-THE-ART METHODS USING LOOCV ON THE Weizmann DATASET</figDesc><table><row><cell>Method</cell><cell cols="2">Classifier Classifier</cell><cell>[21]</cell><cell>[41]</cell><cell>[4]</cell><cell>[48]</cell></row><row><cell>SEG-CCR</cell><cell>97.05%</cell><cell>96.74%</cell><cell>97.83%</cell><cell>-</cell><cell>95.75%</cell><cell>-</cell></row><row><cell>SEQ-CCR</cell><cell>100%</cell><cell>100%</cell><cell>-</cell><cell>90%</cell><cell>-</cell><cell>96%</cell></row></table><note><p>NN SLA Gorelick et Niebles et al. Ali et al. Seo et al.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF OPTICAL-FLOW-BASED NN AND SLA CLASSIFIERS ON THE Weizmann DATASET</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NN Classifier</cell><cell cols="2">SLA Classifier</cell></row><row><cell></cell><cell></cell><cell cols="4">SEG-CCR SEQ-CCR SEG-CCR SEQ-CCR</cell></row><row><cell>L = 8</cell><cell>LOOCV LPOCV</cell><cell>89.74% 79.45%</cell><cell>91.11% 80.00%</cell><cell>92.69% 83.20%</cell><cell>94.44% 88.89%</cell></row><row><cell>L = 20</cell><cell>LOOCV LPOCV</cell><cell>91.93% 81.80%</cell><cell>92.22% 82.22%</cell><cell>94.09% 87.35%</cell><cell>94.44% 88.89%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V RECOGNITION</head><label>V</label><figDesc>PERFORMANCE FOR OPTICAL-FLOW FEATURES FOR L = 20 AND LOOCV ON KTH DATASET</figDesc><table><row><cell>Classifier</cell><cell>Action</cell><cell>Clap</cell><cell>Wave</cell><cell>Walk</cell><cell>Jog</cell><cell>Run</cell><cell>Box</cell><cell>Average</cell></row><row><cell>NN</cell><cell cols="8">SEG-CCR 90.6 90.3 94.9 76.6 93.0 89.55 SEQ-CCR 100 100 99 97 93 100 98.17</cell></row><row><cell>SLA</cell><cell cols="8">SEG-CCR 94.1 90.9 97.4 81.4 86.4 92.3 90.84 SEQ-CCR 99 100 100 97 95 100 98.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI COMPARISON</head><label>VI</label><figDesc>OF OPTICAL-FLOW-BASED NN AND SLA CLASSIFIERS ON KTH DATASET (L = 20)</figDesc><table><row><cell></cell><cell cols="2">NN Classifier</cell><cell cols="2">SLA Classifier</cell></row><row><cell></cell><cell cols="4">SEG-CCR SEQ-CCR SEG-CCR SEQ-CCR</cell></row><row><cell>LOOCV</cell><cell>89.55%</cell><cell>98.17%</cell><cell>90.84%</cell><cell>98.50%</cell></row><row><cell>LPOCV</cell><cell>85.42%</cell><cell>96.88%</cell><cell>86.04%</cell><cell>97.40%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII COMPARISON</head><label>VII</label><figDesc>OF THE OPTICAL-FLOW-BASED APPROACH (L = 20) WITH STATE-OF-THE-ART METHODS USING LOOCV ON KTH DATASET Wu et al. Wong et al. Dollar et al. Seo et al.</figDesc><table><row><cell cols="4">NN Kim et al. Method SLA Classifier Classifier [33]</cell><cell>[58]</cell><cell>[56]</cell><cell>[13]</cell><cell>[48]</cell></row><row><cell>SEG-CCR</cell><cell>89.55%</cell><cell>90.84%</cell><cell>-</cell><cell>-</cell><cell>81.0%</cell><cell>81.2%</cell><cell>-</cell></row><row><cell>SEQ-CCR</cell><cell>98.17%</cell><cell>98.50%</cell><cell>95.3%</cell><cell>94.5%</cell><cell>-</cell><cell>-</cell><cell>95.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII COMPARISON</head><label>VIII</label><figDesc>OF THE OPTICAL-FLOW-BASED APPROACH (L = 20) WITH STATE-OF-THE-ART METHODS USING LPOCV ON KTH DATASET NN SLA Ali et al. Laptev et al. Le et al. Wang et al. Kovashka et al.</figDesc><table><row><cell>Method</cell><cell cols="2">Classifier Classifier</cell><cell>[4]</cell><cell>[36]</cell><cell>[37]</cell><cell>[53]</cell><cell>[34]</cell></row><row><cell>SEG-CCR</cell><cell>85.42%</cell><cell>86.04%</cell><cell>87.7%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SEQ-CCR</cell><cell>96.88%</cell><cell>97.40%</cell><cell>-</cell><cell>91.8%</cell><cell>93.9%</cell><cell>94.2%</cell><cell>94.5%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE IX RECOGNITION</head><label>IX</label><figDesc>PERFORMANCE FOR SILHOUETTE-BASED FEATURES FOR LOOCV AND L = 8 ON UT-Tower DATASET</figDesc><table><row><cell>Classifier</cell><cell>Action</cell><cell>Point</cell><cell>Stand</cell><cell>Dig</cell><cell>Walk</cell><cell>Carry</cell><cell>Run</cell><cell>Wave1</cell><cell>Wave2</cell><cell>Jump</cell><cell>Average</cell></row><row><cell>NN</cell><cell cols="11">SEG-CCR 72.3 92.8 94.5 97.3 97.7 100 85.6 100 99.0 93.53 SEQ-CCR 75.0 91.7 100 100 100 100 100 100 100 96.30</cell></row><row><cell>SLA</cell><cell cols="11">SEG-CCR 88.0 94.2 96.0 98.6 99.5 100 94.1 92.5 100 96.15 SEQ-CCR 91.7 83.3 100 100 100 100 100 100 100 97.22</cell></row><row><cell cols="5">based CCR for wave1. Examining the confusion matrix (not</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">shown here) we found that around 50% of wave1 videos are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">misclassified by our optical-flow-based action recognition as</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">wave2. This indicates that the optical-flow features are also</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">not very discriminative for wave1.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE X RECOGNITION</head><label>X</label><figDesc>PERFORMANCE FOR OPTICAL FLOW FEATURES FOR LOOCV AND L = 8 ON UT-Tower DATASET</figDesc><table><row><cell>Classifier</cell><cell>Action</cell><cell cols="7">Point Stand Dig Walk Carry Run Wave1 Wave2 Jump Average</cell></row><row><cell>NN</cell><cell cols="2">SEG-CCR 53.0 SEQ-CCR 83.3</cell><cell>51.5 96.5 93.2 25.0 100 100</cell><cell>87.6 90.9 100 100</cell><cell>66.9 75.0</cell><cell>82.5 91.7</cell><cell>100 100</cell><cell>82.25 86.11</cell></row><row><cell>SLA</cell><cell cols="2">SEG-CCR 53.0 SEQ-CCR 66.7</cell><cell>55.7 96.0 91.8 66.7 100 100</cell><cell>95.9 77.3 100 100</cell><cell>47.4 41.7</cell><cell>81.0 91.7</cell><cell>100 100</cell><cell>81.18 85.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI COMPARISON</head><label>XI</label><figDesc>OF THE OPTICAL-FLOW-BASED NN CLASSIFIER (L = 20) WITH STATE-OF-THE-ART METHODS USING LOOCV ON YouTube DATASET Method Proposed Liu et al. [39] Ikizler et al. [30] Le et al. [37] Wang et al. [53] Noguchi et al.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>[42]</cell></row><row><cell>SEG-CCR</cell><cell>50.4%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SEQ-CCR</cell><cell>78.5%</cell><cell>71.2%</cell><cell>75.2%</cell><cell>75.8%</cell><cell>84.2%</cell><cell>80.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>TABLE XIII LOOCV</head><label>XIII</label><figDesc>RESULTS OF A ROBUSTNESS TEST TO CAMERA VIEWPOINT USING NN CLASSIFIER FOR THE ACTION OF WALKING (L = 8); QUERY ACTION CAPTURED FROM DIFFERENT ANGLES THAN THOSE IN THE DICTIONARY Since silhouette tunnels are not available for this dataset, we only tested the optical flow features. For the NN classifier, we obtained SEG-CCR of 50.4% and SEQ-CCR of 78.5% (L = 20, LOOCV). TableXIshows the SEQ-CCR comparison of our proposed method with state-of-the-art methods. The performance of our method is in line with state-of-the-art methods today.</figDesc><table><row><cell cols="2">Silhouette Features</cell><cell cols="2">Optical-Flow Features</cell></row><row><cell>SEG-CCR</cell><cell>SEQ-CCR</cell><cell>SEG-CCR</cell><cell>SEQ-CCR</cell></row><row><cell>Viewpoint 0°100 %</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 9°100 %</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 18°100 %</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 27°92.3%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 36°90.8%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 45°76.8%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell>Viewpoint 54°38.5%</cell><cell>0%</cell><cell>98.1%</cell><cell>100%</cell></row><row><cell>Viewpoint 63°20.2%</cell><cell>0%</cell><cell>69.1%</cell><cell>100%</cell></row><row><cell>Viewpoint 72°13.8%</cell><cell>0%</cell><cell>40.4%</cell><cell>0%</cell></row><row><cell>Viewpoint 81°5.4 %</cell><cell>0%</cell><cell>30.2%</cell><cell>0%</cell></row><row><cell cols="2">trampoline jumping, volleyball spiking, and walking with a</cell><cell></cell><cell></cell></row><row><cell cols="2">dog. This dataset is very challenging due to large variations in</cell><cell></cell><cell></cell></row><row><cell cols="2">camera motion, acquisition viewpoint, cluttered background,</cell><cell></cell><cell></cell></row><row><cell>etc.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE XIV CLASSIFICATION</head><label>XIV</label><figDesc>PERFORMANCE USING SUBSETS OF SILHOUETTE FEATURES ON THE Weizmann DATASET USING LOOCV (L = 8); SEG-CCR AND SEQ-CCR ARE COMPUTED ACROSS ALL ACTIONS NE , d SW , d SE , d NW ) the computational complexity of the NN classifier is very close to that of the SLA classifier. This method is also memory efficient, since the training sets and test sets essentially store a part of a 13 × 13 or 12 × 12 covariance matrix, instead of video data. As is clear from the Tables I-XI, the proposed methods have excellent performance. In the two instances where the methods of Gorelick et al. (Table III) and of Ali et al. (Table</figDesc><table><row><cell>Selected Features</cell><cell cols="2">SEG-CCR SEQ-CCR</cell></row><row><cell>(x, y, t)</cell><cell>69.84%</cell><cell>84.44%</cell></row><row><cell>(d E , d W , d N , d S )</cell><cell>69.29%</cell><cell>80.00%</cell></row><row><cell>(d NE , d SW , d SE , d NW )</cell><cell>81.83%</cell><cell>89.99%</cell></row><row><cell>(d T + , d T -)</cell><cell>33.02%</cell><cell>41.11%</cell></row><row><cell>(x, y, t, d E )</cell><cell>76.59%</cell><cell>90.00%</cell></row><row><cell>(x, y, t, d W )</cell><cell>76.83%</cell><cell>91.11%</cell></row><row><cell>(x, y, t, d N )</cell><cell>82.06%</cell><cell>92.22%</cell></row><row><cell>(x, y, t, d S )</cell><cell>79.60%</cell><cell>90.00%</cell></row><row><cell>(x, y, t, d E , d W , d N , d S )</cell><cell>90.71%</cell><cell>96.67%</cell></row><row><cell>(x, y, t, d SE )</cell><cell>80.56%</cell><cell>94.44%</cell></row><row><cell>(x, y, t, d NW )</cell><cell>78.65%</cell><cell>88.89%</cell></row><row><cell>(x, y, t, d SW )</cell><cell>79.68%</cell><cell>92.22%</cell></row><row><cell>(x, y, t, d NE )</cell><cell>81.35%</cell><cell>93.33%</cell></row><row><cell cols="2">(x, y, t, d 91.11%</cell><cell>98.89%</cell></row><row><cell>(x, y, t, d T + )</cell><cell>84.92%</cell><cell>95.56%</cell></row><row><cell>(x, y, t, d T -)</cell><cell>85.56%</cell><cell>93.33%</cell></row><row><cell>(x, y, t, d T + , d T -)</cell><cell>88.83%</cell><cell>95.56%</cell></row><row><cell>All features</cell><cell>97.05%</cell><cell>100%</cell></row><row><cell cols="3">take together about 10.1 s for a 180 × 144-pixel, 84-frame</cell></row><row><cell cols="3">silhouette sequence (0.12 s per frame). The computation of</cell></row><row><cell cols="3">12-dimensional optical-flow feature vectors and their covari-</cell></row><row><cell cols="3">ance matrices takes about 6 s (0.07 s per frame) for the same</cell></row><row><cell cols="3">sequence. We note that silhouette and optical flow estimation</cell></row><row><cell cols="3">are not included in these computation times. Given a query</cell></row><row><cell cols="3">sequence with 613 query segments and a training set with</cell></row><row><cell cols="3">605 training segments, the NN classifier requires about 52 s to</cell></row><row><cell cols="3">classify all query segments (0.08 s per query segment), while</cell></row><row><cell cols="3">the SLA classifier needs about 44 s (solving 613 times an</cell></row><row><cell cols="3">l 1 -norm minimization problem, i.e., 0.07 s per query segment).</cell></row><row><cell cols="3">The computation time indicates that optical-flow features</cell></row><row><cell cols="3">have lower computational complexity than silhouette-based</cell></row><row><cell>features, and</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE XV CLASSIFICATION</head><label>XV</label><figDesc>PERFORMANCE USING SUBSETS OF OPTICAL-FLOW FEATURES ON THE Weizmann DATASET USING LOOCV (L = 8); SEG-CCR AND SEQ-CCR ARE COMPUTED ACROSS ALL ACTIONS</figDesc><table><row><cell>Selected Features</cell><cell cols="2">SEG-CCR SEQ-CCR</cell></row><row><cell>(x, y, t)</cell><cell>71.92%</cell><cell>73.33%</cell></row><row><cell>(I t , u, v, u t , v t )</cell><cell>72.33%</cell><cell>83.33%</cell></row><row><cell>(Di v, V or, Gten, Sten)</cell><cell>57.14%</cell><cell>78.89%</cell></row><row><cell>(x, y, t, I t )</cell><cell>73.23%</cell><cell>77.78%</cell></row><row><cell>(x, y, t, u)</cell><cell>83.09%</cell><cell>87.78%</cell></row><row><cell>(x, y, t, v)</cell><cell>82.43%</cell><cell>84.44%</cell></row><row><cell>(x, y, t, u t )</cell><cell>80.13%</cell><cell>83.33%</cell></row><row><cell>(x, y, t, v t )</cell><cell>79.64%</cell><cell>81.11%</cell></row><row><cell>(x, y, t, I t , u, v, u t , v t )</cell><cell>85.63%</cell><cell>88.24%</cell></row><row><cell>(x, y, t, Di v)</cell><cell>79.72%</cell><cell>82.22%</cell></row><row><cell>(x, y, t, V or)</cell><cell>80.54%</cell><cell>81.78%</cell></row><row><cell>(x, y, t, Gten)</cell><cell>76.35%</cell><cell>77.78%</cell></row><row><cell>(x, y, t, Sten)</cell><cell>77.59%</cell><cell>81.11%</cell></row><row><cell>(x, y, t, Di v, V or, Gten, Sten)</cell><cell>81.77%</cell><cell>83.33%</cell></row><row><cell>All features</cell><cell>89.74%</cell><cell>91.11%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XVI LOOCV</head><label>XVI</label><figDesc>RECOGNITION PERFORMANCE FOR VARIOUS FORMULATIONS USING SILHOUETTE FEATURES AND NN CLASSIFIER WITH L = 8 ON THE Weizmann DATASET</figDesc><table><row><cell>Representation</cell><cell>Covariance</cell><cell>Mean</cell><cell>Covariance</cell><cell>Gaussian Fit</cell></row><row><cell>Metric</cell><cell cols="4">Log-Euclidean Euclidean Euclidean KL-Divergence</cell></row><row><cell>SEG-CCR</cell><cell>97.1</cell><cell>45.8</cell><cell>43.6</cell><cell>91.3</cell></row><row><cell>SEQ-CCR</cell><cell>100</cell><cell>48.9</cell><cell>56.7</cell><cell>93.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>More precisely, (4) has a solution except in the highly unlikely circumstance in which there are less than K linearly independent samples across all classes and p query is outside of their span. If a solution to (4) exists, it is necessarily nonunique unless additional prior information, e.g., sparsity, restricts the set of feasible α.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>This denotes the cross-covariance between the x spatial coordinate and the d N distance which are both components of the 13-dimensional feature vector.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that the centroids of silhouettes in each video segment are aligned to eliminate global movement while preserving local movement (deformation) that is critical to action recognition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>In practice, the empirical covariance matrices of some video segments may be singular or nearly so. If many covariance matrices for the same action are available as in our experiments, then one may safely discard the few which are nearly singular from the NN training set or dictionary. If, however, there are only a few covariance matrices available and they are all nearly singular, a practical solution is to add a small positive number to the nearly zero eigenvalues to make them nonsingular.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank Prof. Pierre Moulin of the ECE department at UIUC for introducing them to the log-Euclidean metric.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. This work was supported in part by the U.S. National Science Foundation under Award CCF-0905541 and the U.S. AFOSR under Award FA9550-10-1-0458 (Subaward A1795). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the NSF or AFOSR. The associate editor coordinating the review of this manuscript and approving it for publication was Prof. Carlo S. Regazzoni.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human activity analysis: A review</title>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Silhouette history and energy image information for human movement recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Parvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Multimedia</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation and recognition of continuous human activity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Detect. Recognit. Events Video</title>
		<meeting>IEEE Workshop Detect. Recognit. Events Video</meeting>
		<imprint>
			<date type="published" when="2001-07">Jul. 2001</date>
			<biblScope unit="page" from="28" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human action recognition in videos using kinematic features and multiple instance learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="288" to="303" />
			<date type="published" when="2010-02">Feb. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Log-Euclidean metrics for fast and simple calculus on diffusion tensors</title>
		<author>
			<persName><forename type="first">V</forename><surname>Arsigny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pennec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magn. Resonance Med</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="421" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="257" to="267" />
			<date type="published" when="2001-03">Mar. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">UT-Tower Dataset: Aerial View Activity Classification Challenge [Online]</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
		<ptr target="http://cvrc.ece.utexas.edu/SDHA2010/Aerial_View_Activity.html" />
		<imprint>
			<date type="published" when="2010-08">2010, Aug. 19</date>
		</imprint>
	</monogr>
	<note>Available</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition by Radon transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Data Mining Workshops</title>
		<meeting>IEEE Int. Conf. Data Mining Workshops</meeting>
		<imprint>
			<date type="published" when="2008-12">Dec. 2008</date>
			<biblScope unit="page" from="862" to="868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Probabilistic recognition of activity using local appearance</title>
		<author>
			<persName><forename type="first">O</forename><surname>Chomat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="1999-06">Jun. 1999</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic extraction and description of human gait models for recognition purposes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cunado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Nixon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2003-04">Apr. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition for surveillance applications using optic flow and SVM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Danafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gheissari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis</title>
		<meeting>Asian Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="457" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient action spotting based on a spacetime oriented structure representation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sizintsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cannons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="1990" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd IEEE Int. Workshop Vis</title>
		<meeting>2nd IEEE Int. Workshop Vis</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mechanical work for step-to-step transitions is a major determinant of the metabolic cost of human walking</title>
		<author>
			<persName><forename type="first">J</forename><surname>Donelan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Experim. Biol</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="3717" to="3727" />
			<date type="published" when="2002-12">Dec. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information</title>
		<author>
			<persName><forename type="first">E</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inform. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="489" to="509" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Background and foreground modeling using nonparametric kernel density for visual surveillance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Duraiswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2002-02">Feb. 2002</date>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1151" to="1163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Action recognition by learning mid-level motion features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A metric for covariance matrices</title>
		<author>
			<persName><forename type="first">W</forename><surname>Förstner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Festschrift for Erik W. Grafarend on the Occasion of His 60th Birthday</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Krumm</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Schwarze</surname></persName>
		</editor>
		<meeting><address><addrLine>Stuttgart, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Geodätisches Institut der Universität Stuttgart</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Monocular tracking of the human arm in 3-D</title>
		<author>
			<persName><forename type="first">L</forename><surname>Goncalves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Bernardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ursella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1995-06">Jun. 1995</date>
			<biblScope unit="page" from="764" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2247" to="2253" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Action recognition using log-covariance matrices of silhouette and optical-flow features</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011-09">Sep. 2011</date>
			<pubPlace>Boston, MA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Department of Electrical and Computer Engineering, Boston Univ.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action recognition from video by covariance matching of silhouette tunnels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brazilian Symp</title>
		<meeting>Brazilian Symp</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action change detection in video by covariance matching of silhouette tunnels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Proc. IEEE Int. Conf. Acoust. Speech Signal Process.</title>
		<imprint>
			<biblScope unit="page" from="1110" to="1113" />
			<date type="published" when="2010-03">Mar. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Action recognition in video by sparse representation on covariance manifolds of silhouette tunnels</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit. Semantic Descript. Human Act. Contest</title>
		<meeting>Int. Conf. Pattern Recognit. Semantic Descript. Human Act. Contest</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="294" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition using sparse representation on covariance manifolds of optical flow</title>
		<author>
			<persName><forename type="first">K</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Adv. Video Signal Based Surveill</title>
		<meeting>IEEE Int. Conf. Adv. Video Signal Based Surveill</meeting>
		<imprint>
			<date type="published" when="2010-08">Aug. 2010</date>
			<biblScope unit="page" from="88" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A biologically inspired system for action recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On the cone of positive semidefinite matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="81" to="88" />
			<date type="published" when="1987-10">Oct. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human action recognition using distribution of oriented rectangular patches</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duygulu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Human Motion, Understand. Model. Capture Animation</title>
		<meeting>Human Motion, Understand. Model. Capture Animation</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="271" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Object, scene and actions: Combining multiple features for human action recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ikizler-Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="494" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identification of humans using gait</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sundaresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Rajagopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Cuntoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1163" to="1173" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficient visual event detection using volumetric features</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="166" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis of video volume tensors for action categorization and detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1415" to="1428" />
			<date type="published" when="2009-08">Aug. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a hierarchy of discriminative space-time neighborhood features</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="2046" to="2053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A gesture-driven computer interface using Kinect camera</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Southwest Symp</title>
		<meeting>IEEE Southwest Symp</meeting>
		<imprint>
			<date type="published" when="2012-04">Apr. 2012</date>
			<biblScope unit="page" from="185" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learing realistic human actions from movies</title>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing human actions using multiple features</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos in the wild</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2009-06">Jun. 2009</date>
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast motion consistency through matrix quantization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Matikainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf</title>
		<meeting>Brit. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2008-09">Sep. 2008</date>
			<biblScope unit="page" from="1055" to="1064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of human action categories using spatial-temporal words</title>
		<author>
			<persName><forename type="first">J</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="299" to="318" />
			<date type="published" when="2008-09">Sep. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A SURF-based spatio-temporal feature for feature-fusion-based action recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yanai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. Workshop Human Motion, Understand. Model. Capture Animation</title>
		<meeting>Eur. Conf. Comput. Vis. Workshop Human Motion, Understand. Model. Capture Animation</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Space-time image sequence analysis: Object tunnels and occlusion volumes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ristivojević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="364" to="376" />
			<date type="published" when="2006-02">Feb. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Toward model-based recognition of human movements in image sequences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rohr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVGIP, Image Understand</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="94" to="115" />
			<date type="published" when="1994-01">Jan. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Segmenting visual actions based on spatiotemporal motion patterns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2000-06">Jun. 2000</date>
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Recognizing human actions: A local SVM approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schuldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Pattern Recognit</title>
		<meeting>Int. Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2004-08">Aug. 2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="32" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A 3-D SIFT descriptor and its application to action recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Multimedia</title>
		<meeting>Int. Conf. Multimedia</meeting>
		<imprint>
			<date type="published" when="2007-09">Sep. 2007</date>
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Action recognition from one example</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="867" to="882" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">TemporalBoost for event recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Vitoria Lobo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-10">Oct. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual recognition of American sign language using hidden Markov model</title>
		<author>
			<persName><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Autom. Face Gesture Recognit</title>
		<meeting>IEEE Int. Conf. Autom. Face Gesture Recognit</meeting>
		<imprint>
			<date type="published" when="1995-01">Jan. 1995</date>
			<biblScope unit="page" from="1" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Region covariance: A fast descriptor for detection and classification</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2006-05">May 2006</date>
			<biblScope unit="page" from="589" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pedestrian detection via classification on Riemannian manifolds</title>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1713" to="1727" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fusion of static and dynamic body biometrics for gait recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="2004-02">Feb. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human activity recognition based on R transform</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Extracting spatio-temporal interest points using global information</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2007-10">Oct. 2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Robust face recognition via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="227" />
			<date type="published" when="2009-02">Feb. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Action recognition using context and appearance distribution features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011-06">Jun. 2011</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recognizing human action in time sequential image using hidden markov model</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ohya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="1992-06">Jun. 1992</date>
			<biblScope unit="page" from="379" to="385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Action sketch: A novel action representation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Yilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="984" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L 1 optical flow</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 29th DAGM Conf. Pattern Recognit</title>
		<meeting>29th DAGM Conf. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Boosted exemplar learning for human action recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Comput. Vis</title>
		<meeting>IEEE 12th Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-09">Sep. 2009</date>
			<biblScope unit="page" from="538" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">TREC Video Retrieval Evaluation: TRECVID.</orgName>
		</author>
		<ptr target="http://trecvid.nist.gov" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName><surname>Virat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dataset</forename><surname>Video</surname></persName>
		</author>
		<ptr target="http://www.viratdata.org" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Action Recognition.</orgName>
		</author>
		<ptr target="http://vip.bu.edu/projects/vsns/action-recognition" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">Next-Generation Human-Computer Interfaces.</orgName>
		</author>
		<ptr target="http://vip.bu.edu/projects/hcis" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
