<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Denoising Implicit Feedback for Recommendation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
							<email>wenjiewang96@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
							<email>fulifeng93@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore Xiangnan He</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology of China Liqiang Nie</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shandong University Tat-Seng Chua</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Denoising Implicit Feedback for Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Recommender System</term>
					<term>False-positive Feedback</term>
					<term>Adaptive Denoising Training</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ubiquity of implicit feedback makes them the default choice to build online recommender systems. While the large volume of implicit feedback alleviates the data sparsity issue, the downside is that they are not as clean in reflecting the actual satisfaction of users. For example, in E-commerce, a large portion of clicks do not translate to purchases, and many purchases end up with negative reviews. As such, it is of critical importance to account for the inevitable noises in implicit feedback for recommender training. However, little work on recommendation has taken the noisy nature of implicit feedback into consideration.</p><p>In this work, we explore the central theme of denoising implicit feedback for recommender training. We find serious negative impacts of noisy implicit feedback, i.e., fitting the noisy data hinders the recommender from learning the actual user preference. Our target is to identify and prune the noisy interactions, so as to improve the efficacy of recommender training. By observing the process of normal recommender training, we find that noisy feedback typically has large loss values in the early stages. Inspired by this observation, we propose a new training strategy named Adaptive Denoising Training (ADT), which adaptively prunes noisy interactions during training. Specifically, we devise two paradigms for adaptive loss formulation: Truncated Loss that discards the large-loss samples with a dynamic threshold in each iteration; and Reweighted Loss that adaptively lowers the weights of large-loss samples. We instantiate the two paradigms on the widely used binary cross-entropy loss and test the proposed ADT strategies on three representative recommenders. Extensive experiments on three benchmarks demonstrate that ADT significantly improves the quality of recommendation over normal training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Information systems â†’ Recommender systems; â€¢ Computing methodologies â†’ Learning from implicit feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Recommender systems have been a promising solution for mining user preference over items in various online services such as Ecommerce <ref type="bibr" target="#b28">[29]</ref>, news portals <ref type="bibr" target="#b31">[32]</ref> and social media <ref type="bibr" target="#b2">[3]</ref>. As the clue to user choices, implicit feedback (e.g., click and purchase) are typically the default choice to train a recommender due to their large volume. However, prior work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b38">39]</ref> points out the gap between implicit feedback and the actual user satisfaction due to the prevailing presence of noisy interactions (a.k.a. false-positive interactions) where the users dislike the interacted item. For instance, in Ecommerce, a large portion of purchases end up with negative reviews or being returned. This is because implicit interactions are easily affected by the first impression of users and other factors such as caption bias <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33]</ref> and position bias <ref type="bibr" target="#b18">[19]</ref>. Moreover, existing studies <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref> have demonstrated the detrimental effect of such false-positive interactions on user experience of online services. Nevertheless, little work on recommendation has taken the noisy nature of implicit feedback into consideration.</p><p>In this work, we argue that such false-positive interactions would hinder a recommender from learning the actual user preference, leading to low-quality recommendations. Table <ref type="table" target="#tab_1">1</ref> provides empirical evidence on the negative effects of false-positive interactions when we train a competitive recommender, Neural Matrix Factorization (NeuMF) <ref type="bibr" target="#b15">[16]</ref>, on two real-world datasets. In particular, we construct a "clean" testing set by removing the false-positive interactions for recommender evaluation <ref type="foot" target="#foot_0">1</ref> . As can be seen, training NeuMF with false-positive interactions (i.e., normal training) results in an average performance drop of 16.65% and 10.29% over the two datasets w.r.t. Recall@20 and NDCG@20, as compared to the NeuMF trained without false-positive interactions (i.e., clean training). As such, it is of critical importance to account for the inevitable noises in implicit feedback and eliminate the impact of false-positive interactions for recommender training.</p><p>Indeed, some efforts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> have been dedicated to eliminating the effects of false-positive interactions by: 1) negative experience identification <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref> (illustrated in Figure <ref type="figure" target="#fig_1">1(b)</ref>); and 2) the incorporation of various feedback <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> (shown in Figure <ref type="figure" target="#fig_1">1(c)</ref>). The former processes the implicit feedback in advance by predicting the false-positive ones with additional user behaviors (e.g., dwell time and gaze pattern) and auxiliary item features (e.g., length of the item description) <ref type="bibr" target="#b32">[33]</ref>. The latter incorporates extra feedback (e.g., favorite and skip) into recommender training to prune the effects of false-positive interactions <ref type="bibr" target="#b42">[43]</ref>. A key limitation with these methods is that they require additional data to perform denoising, which may not be easy to collect. Moreover, extra feedback (e.g., rating and favorite) is often of a smaller scale, which may suffer from the sparsity issue. For instance, many users do not give any feedback after watching a movie or purchasing a product <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommender training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data with noisy implicit feedback</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommender training</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Additional features: Dwell time, Item features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Binary Classification</head><p>User satisfaction: 0/1   This work explores denoising implicit feedback for recommender training, which automatically reduces the influence of false-positive interactions without using any additional data (Figure <ref type="figure" target="#fig_1">1(d)</ref>). That is, we only count on the implicit interactions and distill signals of falsepositive interactions across different users and items. Prior study on robust learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> and curriculum learning <ref type="bibr" target="#b1">[2]</ref> demonstrate that noisy samples are relatively harder to fit into models, indicating distinct patterns of noisy samples' loss values in the training procedure. Primary experiments across different recommenders and datasets (e.g., Figure <ref type="figure" target="#fig_4">3</ref>) reveals similar phenomenon: the loss values of false-positive interactions are larger than those of the true-positive ones in the early stages of training, while their loss values decrease to the same range at the end. Consequently, due to the larger loss, false-positive interactions can largely mislead the recommender training in the early stages. Worse still, the recommender ultimately fits the false-positive interactions due to its high representation capacity, which could be overfitting and hurt the generalization. As such, a potential idea of denoising is to reduce the impact of false-positive interactions, e.g., pruning the interactions with large loss values, where the key challenge is to simultaneously decrease the sacrifice of true-positive interactions.</p><p>To this end, we propose Adaptive Denoising Training (ADT) strategies for recommenders, which dynamically prunes the largeloss interactions along the training process. To avoid the lost of generality, we focus only on formulating the training loss, which can be applied to any differentiable models. In detail, we devise two paradigms to formulate the training loss: 1) Truncated Loss, which discards the large-loss interactions dynamically, and 2) Reweighted Loss, which adaptively reweighs the interactions. For each training iteration, the Truncated Loss removes the large-loss samples (i.e., hard samples) with a dynamic threshold which is automatically updated during training. Besides, the Reweighted Loss dynamically assigns "harder" interactions with smaller weights to weaken their effects on the optimization. We instantiate the two loss functions on the basis of the widely used binary cross-entropy loss. On three benchmarks, we test ADT trained with the Truncated Loss or Reweighted Loss over three representative recommenders: Generalized Matrix Factorization (GMF) <ref type="bibr" target="#b15">[16]</ref>, NeuMF <ref type="bibr" target="#b15">[16]</ref>, and Collaborative Denoising Auto-Encoder (CDAE) <ref type="bibr" target="#b39">[40]</ref>. The results show significant performance improvements of ADT over normal training.</p><p>Our main contributions are summarized as:</p><p>â€¢ We formulate the task of denoising implicit feedback for recommender training. We find the negative effect of falsepositive interactions and identify their characteristics (i.e., hard samples) during training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STUDY ON FALSE-POSITIVE FEEDBACK</head><p>The effect of noisy training samples has been studied in conventional machine learning tasks such as image classification <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref>. However, little attention has been paid to such effect on recommendation, which is inherently different from conventional tasks with training samples highly related to each other, e.g., interactions on the same item. We investigate the effects of falsepositive interactions on recommender training by comparing the performance of recommenders trained with and without falsepositive interactions. An interaction is identified as false-positive or true-positive one according to the explicit feedback. For instance, a purchase is false-positive if the following rating score ([1, 5]) &lt; 3.</p><p>Although the size of such explicit feedback is typically insufficient for building robust recommenders in real-world scenarios, the scale is sufficient for a pilot experiment. In detail, we train a competitive recommender model NeuMF under two different settings: 1) "clean training" which trains NeuMF on the true-positive interactions only;</p><p>and 2) "normal training" which trains NeuMF on all observed useritem interactions. We evaluate the recommendation performance on the holdout clean testing set with only true-positive interactions kept, i.e., the evaluation focuses on recommending more satisfying items to users. More details can be seen in Section 5.</p><p>Results. Table <ref type="table" target="#tab_1">1</ref> summarizes the performance of NeuMF under normal training and clean training w.r.t. Recall@20 and NDCG@20 on the two representative datasets, Adressa and Amazon-book. From Table <ref type="table" target="#tab_1">1</ref>, we can observe that, as compared to the ideal setting, i.e., clean training, the performance of normal training drops by 11.77% and 8.8% w.r.t. NDCG@20 on Adressa and Amazon-book, respectively. This result shows the negative effects of false-positive interactions on recommending satisfying items to users. Worse still, recommendations from normal training have higher risk on leading to further false-positive interactions, which would hurt the user experience <ref type="bibr" target="#b32">[33]</ref>. Despite the success of clean training in the pilot study, it is not a reasonable choice in practical applications because of the sparsity issues of reliable feedback such as rating scores. As such, it is worth exploring denoising implicit feedback such as click, view, or buy for recommender training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we detail the proposed Adaptive Denoising Training strategy for recommenders. Prior to that, task formulation and observations that inspire the strategy design are introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Formulation</head><p>Generally, the target of recommender training is to learn user preference from user feedback, i.e., learning a scoring function Å·ğ‘¢ğ‘– = ğ‘“ (ğ‘¢, ğ‘– |Î˜) to assess the preference of user ğ‘¢ over item ğ‘– with parameters Î˜. Ideally, the setting of recommender training is to learn Î˜ from a set of reliable feedback between ğ‘ users (U) and ğ‘€ items (I). That is, given D * = {(ğ‘¢, ğ‘–, ğ‘¦ * ğ‘¢ğ‘– )|ğ‘¢ âˆˆ U, ğ‘– âˆˆ I}, we learn the parameters Î˜ * by minimizing a recommendation loss over D * , e.g., the binary Cross-Entropy (CE) loss:</p><formula xml:id="formula_0">L ğ¶ğ¸ D * = âˆ’ (ğ‘¢,ğ‘–,ğ‘¦ * ğ‘¢ğ‘– ) âˆˆD * ğ‘¦ * ğ‘¢ğ‘– log ( Å·ğ‘¢ğ‘– ) + 1 âˆ’ ğ‘¦ * ğ‘¢ğ‘– log (1 âˆ’ Å·ğ‘¢ğ‘– ) ,</formula><p>where ğ‘¦ * ğ‘¢ğ‘– âˆˆ {0, 1} represents whether the user ğ‘¢ really prefers the item ğ‘–. The recommender with Î˜ * would be reliable to generate high-quality recommendations. In practice, due to the lack of large-scale reliable feedback, recommender training is typically formalized as: Î˜ = min L ğ¶ğ¸ ( D), where D = {(ğ‘¢, ğ‘–, È³ğ‘¢ğ‘– )|ğ‘¢ âˆˆ U, ğ‘– âˆˆ I} is a set of implicit interactions. È³ğ‘¢ğ‘– denotes whether the user ğ‘¢ has interacted with the item ğ‘– implicitly, such as click and purchase.</p><p>However, due to the existence of noisy interactions which would mislead the learning of user preference, the typical recommender training might result in a poor model (i.e., Î˜) that lacks generalization ability on the clean testing set. As such, we formulate a denoising recommender training task as:</p><formula xml:id="formula_1">Î˜ * = min L ğ¶ğ¸ ğ‘‘ğ‘’ğ‘›ğ‘œğ‘–ğ‘ ğ‘’ ( D) ,<label>(1)</label></formula><p>aiming to learn a reliable recommender with parameters Î˜ * by denoising implicit feedback, i.e., pruning the impact of noisy interactions. Formally, by assuming the existence of inconsistency between ğ‘¦ * ğ‘¢ğ‘– and È³ğ‘¢ğ‘– , we define noisy interactions as (ğ‘¢, ğ‘–)|ğ‘¦ * ğ‘¢ğ‘– = 0 âˆ§ È³ğ‘¢ğ‘– = 1 . According to the value of ğ‘¦ * ğ‘¢ğ‘– and È³ğ‘¢ğ‘– , we can separate implicit feedback into four categories similar to a confusion matrix as shown in Figure <ref type="figure" target="#fig_2">2</ref>. In this work, we focus on denoising false-positive interactions and omit the false-negative ones since positive interactions are much fewer in recommendation and thus false-positive interactions would induce worse effects on recommender training. Note that we do not incorporate any additional data such as explicit feedback or reliable implicit feedback into the task of denoising, despite their success in a few applications <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. This is because such feedback is of a smaller scale in most cases, suffering more severely from the sparsity issue.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Observations</head><p>False-positive interactions are harder to fit in the early stages. In robust learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> and curriculum learning <ref type="bibr" target="#b1">[2]</ref>, one theory is that easy samples are more likely to be the clean ones and fitting the hard samples may hurt the generalization. To explore whether it also exists in recommendation, we conduct experiments by training NeuMF with all observed implicit interactions (i.e., normal training) on Adressa and Amazon-book. The loss of true-and false-positive interactions in Adressa is visualized in Figure <ref type="figure" target="#fig_4">3</ref>. Note that similar trends are also found over other recommenders and datasets (see more details in Section 5.2.1). From Figure <ref type="figure" target="#fig_4">3</ref>, we observe that:</p><p>â€¢ Ultimately, the loss of both of true-and false-positive interactions converges to a stable state with close values, which implies that NeuMF fits both of them well. It reflects that deep models with substantial capacity would "memorize" all the training data, including the noisy samples. As such, if the data is noisy, the memorization will lead to poor generalization performance. â€¢ In the early stages of training, the loss values of true-and falsepositive interactions decrease differently. Furthermore, we zoom in to visualize the changes of the loss w.r.t. iterations ranging from 0 to 1,000 in Figure <ref type="figure" target="#fig_4">3</ref> harder to memorize than the true-positive ones in the early stages. The reason might be that false-positive ones represent the items that the user dislikes, and they are more similar to the items that the user didn't interact with (i.e., the negative samples). The findings also support the prior theory in robust learning and curriculum learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13]</ref>. Overall, the results are consistent with the memorization effect <ref type="bibr" target="#b0">[1]</ref>: deep models will first learn the easy and clean patterns in the early stage, and eventually memorize all training samples <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adaptive Denoising Training</head><p>Based on the observations, we propose ADT strategies for recommenders, which estimate ğ‘ƒ (ğ‘¦ * ğ‘¢ğ‘– = 0| È³ğ‘¢ğ‘– = 1, ğ‘¢, ğ‘–) according to the training loss. To reduce the impact of false-positive interactions, ADT dynamically prunes the large-loss interactions during training. In particular, ADT either discards or reweighs the interactions with large loss values to reduce their influences on the training objective. Towards this end, we devise two paradigms to formulate loss functions for denoising training: â€¢ Truncated Loss. This is to truncate the loss values of large-loss interactions to 0 with a dynamic threshold function. â€¢ Reweighted Loss. It adaptively assigns hard samples (i.e., the largeloss ones) with smaller weights during training. Note that the two paradigms formulate various recommendation loss functions, e.g., CE loss, square loss <ref type="bibr" target="#b34">[35]</ref>, and BPR loss <ref type="bibr" target="#b33">[34]</ref>. In the work, we take CE loss as an example to elaborate them. </p><formula xml:id="formula_2">L T-CE (ğ‘¢, ğ‘–) = 0, L ğ¶ğ¸ (ğ‘¢, ğ‘–) &gt; ğœ âˆ§ È³ğ‘¢ğ‘– = 1 L ğ¶ğ¸ (ğ‘¢, ğ‘–), otherwise, (<label>2</label></formula><formula xml:id="formula_3">)</formula><p>where ğœ is a pre-defined threshold. The T-CE loss removes any positive interactions with CE loss larger than ğœ from the training. While this simple T-CE loss is easy to interpret and implement, the fixed threshold may not work properly. This is because the loss value is decreasing with the increase of training iterations. Inspired by the dynamic gradient descent methods <ref type="bibr" target="#b21">[22]</ref>, we replace the fixed threshold with a dynamic threshold function ğœ (ğ‘‡ ) w.r.t. the training iteration ğ‘‡ , which changes the threshold value along the training process (Figure <ref type="figure" target="#fig_5">4</ref>). Besides, since the loss values vary across different Fetch mini-batch data Dğ‘ğ‘œğ‘  from D Update</p><formula xml:id="formula_4">Î˜ T = Î˜ T-1 âˆ’ ğœ‚âˆ‡ 1 | D | ğ‘¢,ğ‘– âˆˆ D L ğ¶ğ¸ (ğ‘¢, ğ‘– |Î˜ T-1 ) 7:</formula><p>Update ğœ– (ğ‘‡ ) = ğ‘šğ‘–ğ‘›(ğ›¼ğ‘‡ , ğœ– ğ‘šğ‘ğ‘¥ ) 8: end for Output: the optimized parameters Î˜ ğ‘‡ ğ‘šğ‘ğ‘¥ of the recommender datasets, it would be more flexible to devise ğœ (ğ‘‡ ) as a function of the drop rate ğœ– (ğ‘‡ ). Note that there is a bijection between the drop rate and the threshold, i.e., for any training iteration, if the drop rate is given, we can calculate the threshold to filter out samples.</p><p>Based on prior observations, a proper drop rate function should have the following properties: 1) ğœ– (â€¢) should have an upper bound to limit the proportion of discarded samples so as to prevent data missing; 2) ğœ– (0) = 0, i.e., it should allow all the samples to be fed into the models in the beginning; and 3) ğœ– (â€¢) should increase smoothly from zero to its upper bound, so that the model can learn and distinguish the true-and false-positive interactions gradually.</p><p>Towards this end, we formulate the drop rate function as:</p><formula xml:id="formula_5">ğœ– (ğ‘‡ ) = ğ‘šğ‘–ğ‘› (ğ›¼ğ‘‡ , ğœ– ğ‘šğ‘ğ‘¥ ),<label>(3)</label></formula><p>where ğœ– ğ‘šğ‘ğ‘¥ is an upper bound and ğ›¼ is a hyper-parameter to adjust the pace to reach the maximum drop rate. Note that we increase the drop rate in a linear fashion rather than a more complex function such as a polynomial function or a logarithm function. Despite the expressiveness of these functions, they will inevitably increase the number of hyper-parameters, resulting in the increasing cost of tuning a recommender. The whole algorithm is explained in Algorithm 1. Note that T-CE loss discards the hard samples which are more likely to be the noisy ones. It is symmetrically contrary to the Hinge loss, and T-CE loss limits the model to be overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.2</head><p>Reweighted Cross-Entropy Loss. Functionally speaking, the Reweighted Cross-Entropy (shorted as R-CE) loss down-weights the positive interactions with large loss values, which is defined as:</p><formula xml:id="formula_6">L R-CE (ğ‘¢, ğ‘–) = ğœ” (ğ‘¢, ğ‘–) L CE (ğ‘¢, ğ‘–),<label>(4)</label></formula><p>where ğœ” (ğ‘¢, ğ‘–) is a weight function that adjusts the contribution of an observed interaction to the training objective. To achieve the target of properly down-weighting the large-loss samples, the weight function ğœ” (ğ‘¢, ğ‘–) is expected to have the following properties: 1) it dynamically adjusts the weights of samples during training; 2) the function will reduce the influence of a hard sample to be weaker than an easy sample; and 3) the degree of weight reduction can be easily adjusted so that it can fit different models and datasets.</p><p>Inspired by the success of Focal Loss <ref type="bibr" target="#b29">[30]</ref>, we estimate ğœ” (ğ‘¢, ğ‘–) with a function of ğ‘“ ( Å·ğ‘¢ğ‘– ) that takes the prediction score as the input. Note that the prediction score and CE loss are equivalent to identify hard samples (i.e., the large-loss ones). We use the prediction score as the input of the weight function since its value is within [0, 1] rather than [0, +âˆ], which is more accountable to further computation. Towards this end, we formulate it as:</p><formula xml:id="formula_7">ğ‘“ ( Å·ğ‘¢ğ‘– ) = Å·ğ›½ ğ‘¢ğ‘– ,<label>(5)</label></formula><p>where ğ›½ âˆˆ [0, +âˆ] is a hyper-parameter to control the range of weights. From Figure <ref type="figure" target="#fig_10">5</ref>(a), we can see that R-CE loss equipped with the proposed weight function can significantly reduce the loss of hard samples (i.e., Å·ğ‘¢ğ‘– â‰ª 0.5) as compared to the original CE loss. Furthermore, the proposed weight function satisfies the aforementioned requirements:</p><p>â€¢ ğ‘“ ( Å·ğ‘¢ğ‘– ) = Å·ğ›½ ğ‘¢ğ‘– is sensitive to Å·ğ‘¢ğ‘– which is closely related to the loss value. As such, it generates dynamic weights during training.</p><p>â€¢ The interactions with extremely large CE loss (e.g., the "outlier" in Figure <ref type="figure" target="#fig_10">5</ref>(b)) will be assigned with very small weights because Å·ğ‘¢ğ‘– is close to 0. Therefore, the influence of such large-loss samples is largely reduced. In addition, as shown in Figure <ref type="figure" target="#fig_10">5</ref> In practice, to ensure the loss values of all samples are within the same range, preventing negative samples with large loss values from dominating the optimization, negative samples are also weighted in this paradigm. Formally, we revise the weight function as:</p><formula xml:id="formula_8">ğœ” (ğ‘¢, ğ‘–) = Å·ğ›½ ğ‘¢ğ‘– , È³ğ‘¢ğ‘– = 1 (1 âˆ’ Å·ğ‘¢ğ‘– ) ğ›½ , otherwise,<label>(6)</label></formula><p>Indeed, it may provide a possible solution to alleviate the impact of false-negative interactions, which is left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.3.3</head><p>In-depth Analysis. Since ADT depends totally on recommenders to identify false-positive interactions, one question might be whether it is reliable. Actually, many existing work <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20]</ref> has pointed out the connection between the large loss and noisy samples, and explained the underlying causality: the "memorization" effect of deep models. That is, deep models will first learn easy and clean patterns in the initial training phase, and then gradually memorize all samples, including noisy ones. As such, the loss of deep models in the early stage can help to filter out noisy interactions. We discuss the memorization effect of recommenders by experiments in Section 3.2 and 5.2.1. And the performance of T-CE loss also shows that it can is explored in Section 5.2.2. Another concern is that some hard samples may be more informative than easy samples and discarding hard samples would limit the model's learning ability. Indeed, as indicated in the prior studies <ref type="bibr" target="#b1">[2]</ref>, hard samples in the noisy data probably confuse the model rather than help it to establish the right decision surface. As such, they may induce poor generalization. It's actually a trade-off  between denoising and learning. In ADT, the ğœ– (â€¢) of T-CE loss and ğ›½ of R-CE loss are to control the balance. And the sensitivity to the hyper-parameters is studied in Section 5.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED WORK</head><p>This work aims to denoise implicit feedback for recommenders, which is highly related to the negative experience identification, incorporating various feedback, and the robustness of recommenders. Negative Experience Identification. To reduce the gap between implicit feedback and the actual user preference, many researchers have paid attention to identify negative experiences in implicit signals <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. Prior work usually collects the various users' feedback (e.g., dwell time <ref type="bibr" target="#b20">[21]</ref>, gaze patterns <ref type="bibr" target="#b45">[46]</ref>, and skip <ref type="bibr" target="#b6">[7]</ref>) and the item characteristics <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> to predict the user's satisfaction. Lu et al. <ref type="bibr" target="#b31">[32]</ref> predicted users' actual preference in news recommendation based on various user behaviors, news quality, and the interaction context. However, these methods need additional feedback and extensive manual label work, e.g., users have to tell if they are satisfied for each interaction. Besides, the quantification of item quality and characteristics is non-trivial <ref type="bibr" target="#b31">[32]</ref>, which largely relies on the manually feature design and the labeling of domain experts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. The unaffordable labor cost hinders the practical usage of these methods, especially in the scenarios with constantly changing items.</p><p>Incorporating Various Feedback. To alleviate the impact of false-positive interactions, previous approaches <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44]</ref> also consider incorporating more feedback (e.g., dwell time <ref type="bibr" target="#b42">[43]</ref>, skip <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b44">45]</ref>, and adding to favorites) into training directly. For instance, Wen et al. <ref type="bibr" target="#b38">[39]</ref> proposed to train the recommender using three kinds of items: "click-complete", "click-skip", and "non-click" ones. The last two kinds of items are both treated as negative samples but with different weights. However, additional feedback might be unavailable in complex scenarios. For example, we cannot acquire dwell time and skip patterns after users buy products or watch movies in a cinema. Most users even don't give any informative feedback after clicks. In an orthogonal direction, this work explores denoising implicit feedback without additional information during training.</p><p>Robustness of Recommender Systems. Gunawardana et al. <ref type="bibr" target="#b11">[12]</ref> defined the robustness of recommender systems as "the stability of the recommendation in the presence of fake information". Prior work <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36]</ref> has tried to evaluate the robustness of recommender systems under various attack methods, such as shilling attacks <ref type="bibr" target="#b24">[25]</ref> and fuzzing attacks <ref type="bibr" target="#b35">[36]</ref>. To build more robust recommender systems, some auto-encoder based models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> introduce the denoising techniques. These approaches (e.g., CDAE <ref type="bibr" target="#b39">[40]</ref>) first corrupt the interactions of user by random noises, and then try to reconstruct the original one with autoencoders. However, these methods focus on heuristic attacks or random noises, and ignore the natural false-positive interactions in data. This work highlights the negative impact of natural noisy interactions, and improve the robustness against them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENT</head><p>Dataset. To evaluate the effectiveness of the proposed ADT on recommender training, we conducted experiments on three publicly accessible datasets: Adressa, Amazon-book, and Yelp.</p><p>â€¢ Adressa: This is a real-world news reading dataset from Adressavisen<ref type="foot" target="#foot_2">2</ref>  <ref type="bibr" target="#b10">[11]</ref>. It includes user clicks over news and the dwell time for each click, where the clicks with dwell time &lt; 10s are thought of as false-positive ones <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b42">43]</ref>. â€¢ Amazon-book: It is from the Amazon-review datasets<ref type="foot" target="#foot_3">3</ref>  <ref type="bibr" target="#b13">[14]</ref>. It covers users' purchases over books with rating scores. A rating score below 3 is regarded as a false-positive interaction. â€¢ Yelp: It's an open recommendation dataset <ref type="foot" target="#foot_4">4</ref> , in which businesses in the catering industry (e.g., restaurants and bars) are reviewed by users. Similar to Amazon-book, the rating scores below 3 are regarded as false-positive feedback. These datasets comprise the common implicit feedback: click, purchase, and consumption, which are suitable to explore the effectiveness of denoising implicit feedback although explicit feedback also exists in each interaction. We followed former work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref> to remove users and items with extremely sparse interactions and split the dataset into training, validation, and testing (see Table <ref type="table" target="#tab_4">2</ref> for statistics). To evaluate the effectiveness of denoising implicit feedback, we kept all interactions, including the false-positive ones, in training and validation, and tested recommenders only on true-positive interactions. That is, the models are expected to recommend more satisfying items to users.</p><p>Evaluation Protocols. For each user in the testing set, we predicted the preference score over all the items except the positive ones used during training. Following existing studies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref>, we reported the recommendation performance w.r.t. two widely used metrics: Recall@K and NDCG@K, where higher scores indicate better performance. For both metrics, we set K as 50 and 100 for Amazonbook and Yelp, while 3 and 20 for Adressa due to its much smaller item space.</p><p>Testing Recommenders. To demonstrate the effectiveness of our proposed ADT strategy on denoising implicit feedback, we compared the performance of recommenders trained with T-CE or R-CE and normal training with standard CE. We selected two representative user-based neural CF models, GMF and NeuMF <ref type="bibr" target="#b15">[16]</ref>, and one item-based model, CDAE <ref type="bibr" target="#b39">[40]</ref>. Note that CDAE is also a representative model of robust recommender which can defend random noises within implicit feedback.</p><p>â€¢ GMF <ref type="bibr" target="#b15">[16]</ref>: This is a generalized version of matrix factorization by replacing the inner product with the element-wise product and a linear neural layer as the interaction function. â€¢ NeuMF <ref type="bibr" target="#b15">[16]</ref>: NeuMF is a representative CF neural model, which models the relationship between users and items by combining GMF and a Multi-Layer Perceptron (MLP).</p><p>â€¢ CDAE <ref type="bibr" target="#b39">[40]</ref>: CDAE corrupts the interactions with random noises, and then employs a MLP model to reconstruct the original input.</p><p>We only tested neural recommenders and omit conventional ones such as MF <ref type="bibr" target="#b23">[24]</ref> and SVD++ <ref type="bibr" target="#b22">[23]</ref> due to their inferior performance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Parameter Settings. For the three testing recommenders, we followed their default settings, and verified the effectiveness of our methods under the same conditions. For GMF and NeuMF, the factor numbers of users and items are both 32. As to CDAE, the hidden size of MLP is set as 200. In addition, the batch size is always 1,024 and Adam <ref type="bibr" target="#b21">[22]</ref> is applied to optimize all the parameters with the learning rate initialized as 0.001. As to the ADT strategies, they have three hyper-parameters in total: ğ›¼ and ğœ– ğ‘šğ‘ğ‘¥ in T-CE loss, and ğ›½ in R-CE loss. In detail, ğœ– ğ‘šğ‘ğ‘¥ is searched in {0.05, 0.1, ..., 0.5} and ğ›½ is tuned in {0.05, 0.1, ..., 0.25, 0.5, 1.0}. As for ğ›¼, we controlled its range by adjusting the iteration number ğœ– ğ‘ to the maximum drop rate ğœ– ğ‘šğ‘ğ‘¥ , and ğœ– ğ‘ is adjusted in {1ğ‘˜, 5ğ‘˜, 10ğ‘˜, 20ğ‘˜, 30ğ‘˜ }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance Comparison</head><p>Table <ref type="table">3</ref> summarizes the recommendation performance comparison of the three testing models trained with standard CE, T-CE, or R-CE over three datasets. From Table <ref type="table">3</ref>, we can observe:</p><p>â€¢ In all cases, both the T-CE loss and R-CE loss effectively improve the performance, e.g., NeuMF+T-CE outperforms vanilla NeuMF by 12.98% on average over three datasets. The significant performance gain indicates the better generalization ability of neural recommenders trained by T-CE loss and R-CE loss. It validates the effectiveness of adaptive denoising training, i.e., discarding or down-weighting hard samples during training. â€¢ By comparing the T-CE Loss and R-CE Loss, we found that the T-CE loss performs better in most cases. We postulate that the recommender still suffers from the false-positive interactions when it is trained with the Reweighted Loss, even though they have smaller weights and contribute little to the overall training loss. In addition, we suspect that the superior performance of the Truncated Loss could be attributed to the additional hyperparameters in the dynamic threshold function which can be tuned more granularly. Further improvement might be achieved by a finer-grained user-specific or item-specific tuning of these parameters, which can be done automatically <ref type="bibr" target="#b4">[5]</ref>. â€¢ Across the recommenders, NeuMF performs worse than GMF and CDAE, especially on Amazon-book and Yelp, which is criticized for the vulnerability to noisy interactions. Because our testing is only on the true-positive interactions, the inferior performance Table <ref type="table">3</ref>: Overall performance of three testing recommenders trained with ADT strategies and normal training over three datasets. Note that Recall@K and NDCG@K are shorted as R@K and N@K to save space, respectively, and "RI" in the last column denotes the relative improvement of ADT over normal training on average. The best results are highlighted in bold. Dataset Adressa Amazon-book Yelp Metric R@3 R@20 N@3 N@20 R@50 R@100 N@50 N@100 R@50 R@100 N@50 N@100 RI GMF 0.0880 0.2141 0. â€¢ Both T-CE and R-CE achieve the biggest performance increase on NeuMF, which validates the effectiveness of ADT to prevent vulnerable models from the disturbance of noisy data. On the contrary, the improvement over CDAE is relatively small, showing that the design of defending random noise can also improve the robustness against false-positive interactions to some extend. Nevertheless, applying T-CE or R-CE still leads to performance gain, which further validates the rationality of denoising implicit feedback.</p><p>In the following, GMF is taken as an example to conduct thorough investigation for the consideration of computation cost.</p><p>Further Comparison against Using Additional Feedback. To avoid the detrimental effect of false-positive interactions, a popular idea is to incorporate the additional user feedback for training although they are usually sparse. Existing work either adopts the additional feedback by multi-task learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, or leverages it to identify the true-positive interactions <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39]</ref>. In this work, we introduce two classical models for comparison: Neural Multi-Task Recommendation (NMTR) <ref type="bibr" target="#b9">[10]</ref> and Negative feedback Reweighting (NR) <ref type="bibr" target="#b38">[39]</ref>. In particular, NMTR with multi-task learning is to capture multiple user behaviors (i.e., click and satisfaction) while NR uses the addition feedback (i.e., dwell time and rating) to identify true-positive interactions with user satisfaction and re-weight the false-positive and non-interacted ones as negative samples. We applied NMTR and NR on the testing recommenders and reported the results of GMF in Table <ref type="table" target="#tab_6">4</ref>. The results of other recommenders with similar trends are omitted to save space.</p><p>From Table <ref type="table" target="#tab_6">4</ref>, we can find that: 1) NMTR and NR achieve better performance than GMF, which validates the effectiveness of additional feedback; and 2) the results of NMTR and NR are inferior to that of ADT, both T-CE and R-CE. This is attributed to the sparsity of additional feedback. Indeed, the clicks with satisfaction is much fewer than the total number of clicks, and thus NR will lose extensive positive training samples. Besides, not all clicks without labeled user satisfaction indicate users' dislikes because many users seldom give explicit feedback even if they are satisfied. Therefore, treating them as negative samples will hurt the performance, which is also found by the experiments in <ref type="bibr" target="#b5">[6]</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L o s s T r a i n i n g i t e r a t i o n F a l s e -p o s i t i v e i n t e r a c t i o n s A l l t r a i n i n g i n t e r a c t i o n s</head><p>(c) Reweighted Loss Performance Comparison w.r.t. Interaction Sparsity. Since ADT prunes many interactions during training, we explored whether ADT hurts the preference learning of inactive users because their interacted items are sparse. Following the former studies <ref type="bibr" target="#b37">[38]</ref>, we split testing users into four groups according to the interaction number of each user where each group has the same number of interactions. Figure <ref type="figure" target="#fig_11">6</ref> shows the group-wise performance comparison where we can observe that the proposed ADT strategies achieve stable performance gain over normal training in all cases. It validates that ADT is also effective for the inactive users.  From Figure <ref type="figure" target="#fig_13">7</ref>(a), we can find that the observations in section 3.2 also exist in the training of GMF on Amazon-book. The loss values of false-positive interactions eventually become similar to other samples, indicating that GMF fits false-positive samples well at last. On the contrary, as shown in Figure <ref type="figure" target="#fig_13">7</ref> Figure <ref type="figure">9</ref> visualizes the changes of the recall and precision along the training process. The green line in Figure <ref type="figure">9</ref> indicates the recall and precision under the settings of random discarding. In particular, the recall of random discarding equals the drop rate during training while its precision is the proportion of noisy interactions in all training samples at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">In-depth Analysis</head><p>From Figure <ref type="figure">9</ref>, we observed that: 1) the Truncated Loss discards nearly half of false-positive interactions after the drop rate keeps stable, greatly reducing the impact of noisy interactions; and 2) the precision of Truncated Loss is about twice as large as that of random discarding. It demonstrates that the Truncated Loss effectively utilizes the distill signals of false-positive interactions and weakens their contributions to the model training. In spite of this, we can find that a key limitation of the Truncated Loss is the low precision, e.g., only 10% precision in Figure <ref type="figure">9</ref>, which implies that it inevitably discards many clean interactions. This also partly proves that it's worth pruning noisy interactions at the cost of losing many clean samples. And the hyper-parameters in ADT control the trade-off between denoising and losing clean samples. Besides, how to further improve the precision so as to decrease the loss of clean samples is a promising research direction in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Hyper-parameter Sensitivity. Our proposed ADT strategies incorporate three hyper-parameters to adjust the dynamic threshold function and the weight function in two paradigms. In particular, ğœ– ğ‘šğ‘ğ‘¥ and ğœ– ğ‘ are used to control the drop rate in the Truncated Loss, and ğ›½ adjusts the weight function in the Reweighted Loss. In this section, we studied how the hyper-parameters affect the performance. Only the results of GMF trained with ADT strategies on Amazon-book and Yelp are reported in Figure <ref type="figure">8</ref> due to space limitation. Other methods over three datasets have similar patterns. From Figure <ref type="figure">8</ref>, we can find that: 1) the recommender trained with the T-CE loss performs better when ğœ– ğ‘šğ‘ğ‘¥ âˆˆ [0.1, 0.3]. If ğœ– ğ‘šğ‘ğ‘¥ exceeds 0.4, the performance drops significantly because a large proportion of samples are discarded. Therefore, the upper bound ğœ– ğ‘šğ‘ğ‘¥ should be restricted. 2) The recommender is relatively sensitive to ğœ– ğ‘ , especially on Amazon-book, and the performance still increases when ğœ– ğ‘ &gt; 30k. Nevertheless, a limitation of T-CE loss is the big search space of hyper-parameters. 3) The adjustment of ğ›½ in the Reweighted Loss is consistent over different datasets, and the best results happen when ğ›½ ranges from 0.15 to 0.3. These observations provide insights on how to tune the hyper-parameters of ADT if it's applied to other recommenders and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND FUTURE WORK</head><p>In this work, we aim to denoise implicit feedback for recommender training. We explore the negative effects of noisy implicit feedback, and propose Adaptive Denoising Training strategies to reduce their impact. In particular, this work contributes two paradigms to formulate the loss functions: Truncated Loss and Reweighted Loss. Both paradigms are general and can be applied to different loss functions, neural recommenders, and optimizers. In this work, we applied the two paradigms on the widely used binary cross-entropy loss and conduct extensive experiments over three recommenders on three datasets, showing that the paradigms effectively reduce the disturbance of noisy implicit feedback.</p><p>This work takes the first step to denoise implicit feedback for recommendation without using additional feedback for training, and points to some new research directions. Specifically, it is interesting to explore how the proposed two paradigms perform on other loss functions, such as Square Loss <ref type="bibr" target="#b34">[35]</ref>, Hinge Loss <ref type="bibr" target="#b34">[35]</ref> and BPR Loss <ref type="bibr" target="#b33">[34]</ref>. Besides, how to further improve the precision of the paradigms is worth studying. Lastly, our Adaptive Denoising Training is not specific to the recommendation task, and it can be widely used to denoise implicit interactions in other domains, such as Web search and question answering.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The comparison between normal training (a); two prior solutions to eliminate false-positive interactions through extra data (b) and (c); and denoising training without extra data (d). Note that the red lines in the useritem graph denote false-positive interactions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Four types of implicit interactions.In this work, we focus on denoising false-positive interactions and omit the false-negative ones since positive interactions are much fewer in recommendation and thus false-positive interactions would induce worse effects on recommender training. Note that we do not incorporate any additional data such as explicit feedback or reliable implicit feedback into the task of denoising, despite their success in a few applications<ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b38">39]</ref>. This is because such feedback is of a smaller scale in most cases, suffering more severely from the sparsity issue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The training loss of true-and false-positive interactions on Adressa in the normal training of NeuMF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of T-CE loss for the observed interactions (i.e., È³ğ‘¢ğ‘– = 1). ğ‘‡ ğ‘– is the iteration number and ğœ (ğ‘‡ ğ‘– ) refers to the threshold. Dash area indicates the effective loss and the loss values larger than ğœ (ğ‘‡ ğ‘– ) are truncated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 3 . 1</head><label>31</label><figDesc>Truncated Cross-Entropy Loss. Functionally speaking, the Truncated Cross-Entropy (shorted as T-CE) loss discards positive interactions with large values of CE loss. Formally, we can define it as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>3 : 4 :</head><label>34</label><figDesc>Sample unobserved interactions Dğ‘›ğ‘’ğ‘” randomly for users in Dğ‘ğ‘œğ‘  with the proportion of 1:1 Define Dğ‘‡ = Dğ‘ğ‘œğ‘  âˆª Dğ‘›ğ‘’ğ‘” 5: Obtain D = arg max D âˆˆ Dğ‘ğ‘œğ‘  , | D |=ğœ– (ğ‘‡ ) | DT | (ğ‘¢,ğ‘–) âˆˆ D L ğ¶ğ¸ (ğ‘¢, ğ‘– |Î˜ T-1 ) 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(b), harder samples always have smaller weights because the function ğ‘“ ( Å·ğ‘¢ğ‘– ) monotonically increases when Å·ğ‘¢ğ‘– âˆˆ [0, 1] and ğ›½ âˆˆ [0, +âˆ]. As such, it can avoid that false-positive interactions with large loss values dominate the optimization during training [42]. â€¢ The hyper-parameter ğ›½ dynamically controls the gap between the weights of hard and easy samples. By observing the examples in Figure 5(b), we can find that: 1) if ğ›½ increases, for the same pair of easy and hard samples, the gap between their weights becomes larger (e.g., ğ‘‘ 0.4 &lt; ğ‘‘ 1.0 in Figure 5(b)); and 2) if we set ğ›½ as 0, the R-CE loss will degrade to the standard CE loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>ğ’Š = âˆ’ ğ¥ğ¨ğ  ğ’š ğ’–ğ’Š ğ“›ğ‘¹âˆ’ğ‘ªğ‘¬ ğ’–, ğ’Š = âˆ’ ğ’š ğ’–ğ’Š ğŸ.ğŸğŸ“ ğ¥ğ¨ğ  ğ’š ğ’–ğ’Š ğ“›ğ‘¹âˆ’ğ‘ªğ‘¬ ğ’–, ğ’Š = âˆ’ ğ’š ğ’–ğ’Š ğŸ.ğŸ ğ¥ğ¨ğ  ğ’š ğ’–ğ’Š ğ“›ğ‘¹âˆ’ğ‘ªğ‘¬ ğ’–, ğ’Š = âˆ’ ğ’š ğ’–ğ’Š ğŸ.ğŸ ğ¥ğ¨ğ  ğ’š ğ’–ğ’Š ğ“›ğ‘¹âˆ’ğ‘ªğ‘¬ ğ’–, ğ’Š = âˆ’ ğ’š ğ’–ğ’Š ğŸ.ğŸ‘ ğ¥ğ¨ğ  ğ’š ğ’–ğ’Š R-CE loss for the observed positive interactions. The contributions of largeloss samples are greatly reduced. The weight function with different parameters ğ›½, where ğ›½ controls the weight difference between hard and easy samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration and analysis of R-CE loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance comparison of GMF over user groups with different sparsity levels on Amazon-book and Yelp. The histograms represent the user number in each group and the lines denote the performance w.r.t. NDCG@100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>s e -p o s i t i v e i n t e r a c t i o n s A l l t r a i n i n g i n t e r a c t i o n s s e -p o s i t i v e i n t e r a c t i o n sA l l t r a i n i n g i n t e r a c t i o n s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Loss of GMF (a), GMF+T-CE (b) and GMF+R-CE (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Performance comparison of GMF trained with ADT on Yelp and Amazon-book w.r.t. different hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>5. 2 . 1</head><label>21</label><figDesc>Memorization of False-positive Interactions. Recall that falsepositive interactions are memorized by recommenders eventually under normal training, leading to poor generalization (cf. Section 3.2). We then investigated whether false-positive interactions are also fitted well by the recommenders trained with ADT strategies. Considering that the original CE loss values indicates the model's fitting ability on the samples, we depicted the CE loss of falsepositive interactions during the training procedure with the real training loss as reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(b), by applying T-CE, the loss values of false-positive interactions keep increasing while the overall training loss stably decreases step by step. The increased loss indicates that the recommender parameters are not optimized over the false-positive interactions, validating the capability of T-CE to identify and discard such interactions. As to R-CE (Figure 7(c)), the loss of false-positive interactions also shows a decreasing trend, showing that the recommender still fits such interactions. However, their loss values are still larger than the real training loss, indicating that the false-positive interactions are assigned with smaller weights by R-CE, which prevents the model from fitting them. Therefore, we can conclude that both paradigms reduce the effect of false-positive interactions on recommender training, which can explain their improvement over normal training. 5.2.2 Study of Truncated Loss. Since the Truncated Loss achieves promising performance in the experiments, we studied how well it performs to identify and discard false-positive interactions. We first defined Recall to represent what percentage of false-positive interactions in the training data are discarded, and precision as the ratio of discarded false-positive interactions to all discarded samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison between the clean training and normal training of NeuMF on Adressa and Amazonbook. #Drop denotes the relative performance drop of normal training as compared to clean training.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Adressa</cell><cell cols="2">Amazon-book</cell></row><row><cell>Metric</cell><cell cols="4">Recall@20 NDCG@20 Recall@20 NDCG@20</cell></row><row><cell>Clean training</cell><cell>0.4040</cell><cell>0.1963</cell><cell>0.0293</cell><cell>0.0159</cell></row><row><cell>Normal training</cell><cell>0.3081</cell><cell>0.1732</cell><cell>0.0265</cell><cell>0.0145</cell></row><row><cell>#Drop</cell><cell>23.74%</cell><cell>11.77%</cell><cell>9.56%</cell><cell>8.81%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 1 Adaptive Denoising Training with T-CE lossInput: the set of all trainable parameters Î˜, the training set of observed implicit interactions D, the maximum number of iterations ğ‘‡ ğ‘šğ‘ğ‘¥ , learning rate ğœ‚, ğœ– ğ‘šğ‘ğ‘¥ , ğ›¼, L ğ¶ğ¸ 1: for ğ‘‡ = 1 â†’ ğ‘‡ ğ‘šğ‘ğ‘¥ do âŠ² shuffle samples every epoch</figDesc><table /><note>2:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the datasets. In particular, #FP interactions refer to the number of false-positive interactions.</figDesc><table><row><cell>Dataset</cell><cell>#User</cell><cell cols="2">#Item #Interaction #FP Interaction</cell></row><row><cell>Adressa</cell><cell cols="2">212,231 6,596 419,491</cell><cell>247,628</cell></row><row><cell cols="3">Amazon-book 80,464 98,663 2,714,021</cell><cell>199,475</cell></row><row><cell>Yelp</cell><cell cols="2">45,548 57,396 1,672,520</cell><cell>260,581</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance w.r.t. GMF on Amazon-book.</figDesc><table><row><cell>Metric</cell><cell>R@50 R@100 N@50 N@100</cell></row><row><cell>GMF</cell><cell>0.0600 0.0945 0.0247 0.0324</cell></row><row><cell cols="2">GMF+T-CE 0.0707 0.1113 0.0292 0.0382</cell></row><row><cell>GMF+R-CE</cell><cell>0.0682 0.1075 0.0275 0.0362</cell></row><row><cell cols="2">GMF+NMTR 0.0616 0.0967 0.0254 0.0332</cell></row><row><cell>GMF+NR</cell><cell>0.0615 0.0958 0.0254 0.0331</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Each false-positive interaction is identified by auxiliary information of postinteraction behaviors, e.g., rating score (<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5]</ref>) &lt; 3, indicating that the interacted item dissatisfies the user. Refer to</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">Section 2 for more details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2">https://www.adressa.no/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3">http://jmcauley.ucsd.edu/data/amazon/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4">https://www.yelp.com/dataset/challenge</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative, and the National Natural Science Foundation of China (61972372, U19A2079). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">StanisÅ‚aw</forename><surname>JastrzÄ™bski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maxinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tegan</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
				<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Curriculum Learning</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ©rÃ´me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
				<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Efficient Adaptive Transfer Neural Network for Social-Aware Recommendation</title>
		<author>
			<persName><forename type="first">Chong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03240</idno>
		<title level="m">Bias and Debias in Recommender System: A Survey and Future Directions</title>
				<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">lambdaOpt: Learn to Regularize Recommender Models in Finer Levels</title>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 25th SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="978" to="986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sampler design for bayesian personalized ranking by leveraging view data</title>
		<author>
			<persName><forename type="first">Jingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating Implicit Measures to Improve Web Search</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuldeep</forename><surname>Karnawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Mydland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Information Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="147" to="168" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fifty Shades of Ratings: How to Benefit from a Negative Feedback in Top-N Recommendations Tasks</title>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Recommender Systems</title>
				<meeting>the 10th Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural multi-task recommendation from multi-behavior data</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Data Engineering</title>
				<meeting>the 35th International Conference on Data Engineering</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1554" to="1557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to Recommend with Multiple Cascading Behaviors</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangning</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Adressa Dataset for News Recommendation</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Atle Gulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã–zlem</forename><surname>Ã–zgÃ¶bek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaomeng</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Web Intelligence</title>
				<meeting>the International Conference on Web Intelligence</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1042" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating Recommender Systems</title>
		<author>
			<persName><forename type="first">Asela</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recommender Systems handbook</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="265" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
				<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ups and Downs: Modeling the Visual Evolution of Fashion Trends with One-Class Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Ruining</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on World Wide Web</title>
				<meeting>the 25th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web</title>
				<meeting>the 26th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On Caption Bias in Interleaving Experiments</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Behr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Information and Knowledge Management</title>
				<meeting>the 21st International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collaborative Filtering for Implicit Feedback Datasets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Data Mining. TEEE</title>
				<meeting>the 8th International Conference on Data Mining. TEEE</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">To Model or to Intervene: A Comparison of Counterfactual and Online Learning to Rank from User Interactions</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Jagerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MentorNet: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
				<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Modeling dwell time to predict click-level satisfaction</title>
		<author>
			<persName><forename type="first">Youngho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Web Search and Data Mining</title>
				<meeting>the 7th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="193" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
				<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 14th SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Shilling Recommender Systems for Fun and Profit</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shyong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on World Wide Web</title>
				<meeting>the 13th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="393" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">When Actions Speak Louder Than Clicks: A Combined Model of Purchase Probability and Long-term Customer Satisfaction</title>
		<author>
			<persName><forename type="first">Gal</forename><surname>Lavee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Koenigstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Barkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Recommender Systems</title>
				<meeting>the 13th Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Click Feedback-Aware Query Recommendation Using Adversarial Examples</title>
		<author>
			<persName><forename type="first">Ruirui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on World Wide Web</title>
				<meeting>the 28th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2978" to="2984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational Autoencoders for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on World Wide Web</title>
				<meeting>the 27th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="689" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">CROSS: Cross-Platform Recommendation for Social E-Commerce</title>
		<author>
			<persName><forename type="first">Tzu-Heng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
				<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr DollÃ¡r</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding web browsing behaviors through Weibull analysis of dwell time</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryen</forename><forename type="middle">W</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Dumais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd international SIGIR Conference on Research and development in information retrieval</title>
				<meeting>the 33rd international SIGIR Conference on Research and development in information retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="379" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Between Clicks and Satisfaction: Study on Multi-Phase User Preferences and Satisfaction for Online News Reading</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 41st International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="435" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effects of User Negative Experience in Mobile News Streaming</title>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ce</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoping</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BPR: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
				<meeting>the 25th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="452" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Are Loss Functions All the Same? Neural Computation</title>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernesto</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Piana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Verri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1063" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluating Recommender System Stability with Influence-Guided Fuzzing</title>
		<author>
			<persName><forename type="first">David</forename><surname>Shriver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">G</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Dwyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Rosenblum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
				<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4934" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremie</forename><surname>Mary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of Neural Information Processing Systems</title>
				<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Neural Graph Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 42nd International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Leveraging Post-click Feedback for Content Recommendations</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Estrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference on Recommender Systems</title>
				<meeting>the 13th Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="278" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collaborative denoising auto-encoders for top-n Recommender Systems</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Dubois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Web Search and Data Mining</title>
				<meeting>the 9th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Exploiting Various Implicit Feedback for Collaborative Filtering</title>
		<author>
			<persName><forename type="first">Byoungju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangkeun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on World Wide Web</title>
				<meeting>the 21st International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="639" to="640" />
		</imprint>
	</monogr>
	<note>Sungchan Park, and Sang goo Lee</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust Asymmetric Recommendation via Min-Max Optimization</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><forename type="middle">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International SIGIR Conference on Research &amp; Development in Information Retrieval</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1077" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond clicks: dwell time for personalization</title>
		<author>
			<persName><forename type="first">Xing</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanthan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suju</forename><surname>Rajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th Conference on Recommender Systems</title>
				<meeting>the 8th Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Silence is Also Evidence: Interpreting Dwell Time for Recommendation from Psychological Perspective</title>
		<author>
			<persName><forename type="first">Peifeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chien</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
				<meeting>the 19th SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">adaqac: Adaptive query auto-completion via implicit negative feedback</title>
		<author>
			<persName><forename type="first">Aston</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anlei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International SIGIR Conference on Research and Development in Information Retrieval</title>
				<meeting>the 38th International SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Gaze Prediction for Recommender Systems</title>
		<author>
			<persName><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Maxwell</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Conference on Recommender Systems</title>
				<meeting>the 10th Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
