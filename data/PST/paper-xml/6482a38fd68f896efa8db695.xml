<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models&apos; Memories</title>
				<funder ref="#_sbqJqa3 #_PHhqn5m">
					<orgName type="full">General Research Fund</orgName>
					<orgName type="abbreviated">GRF</orgName>
				</funder>
				<funder>
					<orgName type="full">Hong Kong Ph.D. Fellowship Scheme</orgName>
					<orgName type="abbreviated">HKPFS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-06-08">8 Jun 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianyang</forename><surname>Xu</surname></persName>
							<email>tianyangxu@whu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiawei</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
							<email>tongzhang@ust.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Equal Contribution</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models&apos; Memories</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-06-08">8 Jun 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2306.05406v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pre-trained language models (PLMs) demonstrate excellent abilities to understand texts in the generic domain while struggling in a specific domain. Although continued pre-training on a large domain-specific corpus is effective, it is costly to tune all the parameters on the domain. In this paper, we investigate whether we can adapt PLMs both effectively and efficiently by only tuning a few parameters. Specifically, we decouple the feed-forward networks (FFNs) of the Transformer architecture into two parts: the original pre-trained FFNs to maintain the old-domain knowledge and our novel domain-specific adapters to inject domainspecific knowledge in parallel. Then we adopt a mixture-of-adapters gate to fuse the knowledge from different domain adapters dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a two-stage adapter-tuning strategy that leverages both unlabeled data and labeled data to help the domain adaptation: i) domain-specific adapter on unlabeled data; followed by ii) the task-specific adapter on labeled data. MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and our experiments demonstrate that MixDA achieves superior performance on in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability, scalability, and efficiency of our method. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pre-trained language models (PLMs) have achieved a multitude of successful applications in natural language understanding <ref type="bibr" target="#b9">(Devlin et al., 2018;</ref><ref type="bibr" target="#b44">Liu et al., 2019;</ref><ref type="bibr">He et al., 2021b)</ref> and generation <ref type="bibr" target="#b38">(Lewis et al., 2019;</ref><ref type="bibr" target="#b74">Zhang et al., 2020;</ref><ref type="bibr" target="#b72">Yang et al., 2020;</ref><ref type="bibr" target="#b4">Brown et al., 2020)</ref>. The predominant methodology for domain adaptation is fine-tuning on labeled domain-specific data or continued pre-training <ref type="bibr" target="#b21">(Gururangan et al., 2020)</ref> on unlabeled domain-specific data. Although effective, both fine-tuning and continued pre-training methods require tuning all the parameters of a PLM, raising high costs beyond many institutions' reach. To mitigate this, multiple parameter-efficient fine-tuning (PEFT) methods are proposed, including prompt-based tuning <ref type="bibr" target="#b17">(Gao et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b56">Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b39">Li and Liang, 2021;</ref><ref type="bibr">Liu et al., 2021a)</ref>, and adapter-based tuning <ref type="bibr" target="#b26">(Houlsby et al., 2019;</ref><ref type="bibr">Pfeiffer et al., 2020b;</ref><ref type="bibr" target="#b27">Hu et al., 2021)</ref>. However, they are more concerned about task adaptation and it is still unclear how to regularly, and inexpensively inject domain knowledge into PLMs for different domain-specific tasks. Moreover, directly tuning PLMs on a domain-specific corpus with PEFT methods will lead to the catastrophic forgetting problem <ref type="bibr" target="#b73">(Yogatama et al., 2019;</ref><ref type="bibr" target="#b21">Gururangan et al., 2020)</ref>. These limitations highlight an important research question: how to adapt PLMs with the new domain knowledge while keeping the old-domain knowledge unmodified?</p><p>Inspired by the recent studies <ref type="bibr" target="#b18">(Geva et al., 2021;</ref><ref type="bibr" target="#b5">Cao et al., 2021;</ref><ref type="bibr" target="#b48">Meng et al., 2022</ref>) that found knowledge is stored in feed-forward networks (FFNs), we decouple the FFNs into two parts: the original pre-trained FFNs to maintain the olddomain knowledge and our novel domain-specific adapters to inject domain-specific knowledge in parallel. Specifically, we propose Mixture-of-Domain-Adapters (MixDA), a mixture of several domain adapters to inject domain-specific knowledge without affecting the old-domain knowledge. Our model has two stages: piq domain-specific tuning multiple knowledge adapters on unlabeled data and then piiq task-specific tuning adapters on labeled data. In the first stage, we train several domain adapters on both domain-specific corpus and pre-training corpus simultaneously while keeping the original feed-forward networks unchanged. In the second stage, we train a mixture-of-adapters gate to dynamically select the desired knowledge adapter and a task-specific adapter for task adaptation.</p><p>We conduct experiments on a broad range of tasks, including 4 out-of-domain datasets, 9 in-domain datasets, and 2 knowledge-intensive datasets. Our experimental results demonstrate the effectiveness of MixDA on 15 datasets, spanning biomedical, computer science publications, news, and reviews. Further analysis displays three key properties of our proposed approach: piq Reliability: it shows superior performance on both in-domain and out-of-domain tasks. piiq Scalability: it scales well to the increasing number of domains. piiiq Efficiency: it adds only a small number of parameters per domain. We claim that these properties are helpful for language models as a service, where a frozen PLM is served, and multiple adapters are inserted to support different customized services.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we will review four research lines related to injecting domain knowledge into pretrained language models: knowledge injection, domain adaptation, parameter-efficient fine-tuning, and mixture-of-adapters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Injection</head><p>Knowledge can be injected into PLMs by pretraining or fine-tuning, each corresponding to a separate research direction. During pre-training, the knowledge carried by knowledge graphs <ref type="bibr" target="#b75">(Zhang et al., 2019;</ref><ref type="bibr" target="#b23">He et al., 2020)</ref>, entities <ref type="bibr" target="#b61">(Sun et al., 2019;</ref><ref type="bibr" target="#b70">Xiong et al., 2020)</ref>, n-grams <ref type="bibr" target="#b11">(Diao et al., 2020)</ref>, knowledge embedding <ref type="bibr">(Wang et al., 2021b)</ref>, synonym and hyponym-hypernym relations in WordNet <ref type="bibr" target="#b33">(Lauscher et al., 2019)</ref>, word-supersense knowledge <ref type="bibr" target="#b36">(Levine et al., 2020)</ref>, and knowledge bases <ref type="bibr" target="#b50">(Peters et al., 2019)</ref> can be injected into PLMs by feeding knowledge inputs and designing new objectives. However, pre-training-based methods are costly, making the application to huge PLMs (e.g., models with 175 Billion parameters) impossible. Fine-tuning-based methods only require an additional fine-tuning process. Some studies inject extra information into the input sentences, like knowledge triples from knowledge graphs <ref type="bibr" target="#b42">(Liu et al., 2020)</ref> and knowledge context <ref type="bibr" target="#b15">(Faldu et al., 2021)</ref>, while other studies explored specific model and training designs, like knowledge adapter networks <ref type="bibr">(Wang et al., 2021a)</ref>, graph convolutional networks and LSTMs <ref type="bibr" target="#b40">(Lin et al., 2019)</ref>, and metalearning <ref type="bibr" target="#b58">(Sinitsin et al., 2020)</ref>. <ref type="bibr" target="#b76">Zhu et al. (2020)</ref> formulated knowledge injection as a constrained optimization problem by adding a constraint on the loss on the unmodified facts. Recent studies <ref type="bibr" target="#b18">(Geva et al., 2021;</ref><ref type="bibr" target="#b5">Cao et al., 2021;</ref><ref type="bibr" target="#b48">Meng et al., 2022)</ref> reveal that knowledge is stored in the feed-forward networks in PLMs. Inspired by these studies, we propose a new efficient tuning method to inject domain knowledge into feed-forward networks with minimal costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Domain Adaptation</head><p>Previous studies have observed that language models suffer from a significant performance drop during the domain shift <ref type="bibr" target="#b2">(Beltagy et al., 2019;</ref><ref type="bibr" target="#b1">Alsentzer et al., 2019;</ref><ref type="bibr" target="#b28">Huang et al., 2019;</ref><ref type="bibr" target="#b34">Lee et al., 2020;</ref><ref type="bibr">Ke et al., 2022b)</ref>. Effective strategies that can bridge the domain gap are introduced. Pre-training language models from scratch is effective but costly, like SciBERT <ref type="bibr" target="#b2">(Beltagy et al., 2019)</ref>, BioBERT <ref type="bibr" target="#b34">(Lee et al., 2020)</ref>, and ClinicalBERT <ref type="bibr" target="#b1">(Alsentzer et al., 2019)</ref>. Recent studies explored continued pretraining <ref type="bibr" target="#b21">(Gururangan et al., 2020)</ref> and adapter networks <ref type="bibr" target="#b12">(Diao et al., 2021)</ref> to save time by training on unlabeled downstream task data. In this paper, we introduce plug-in domain adaptors for domain adaptation, which are effective and mitigate catastrophic forgetting issues because of the explicit learning strategy and efficient model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Parameter-Efficient Fine-tuning</head><p>Another relevant research direction is parameterefficient fine-tuning (PEFT), which only fine-tunes a small number of parameters. Existing works solve this problem from two perspectives: promptbased tuning <ref type="bibr" target="#b17">(Gao et al., 2021;</ref><ref type="bibr">Liu et al., 2021b;</ref><ref type="bibr" target="#b56">Schick and Sch?tze, 2021;</ref><ref type="bibr" target="#b39">Li and Liang, 2021;</ref><ref type="bibr">Liu et al., 2021a)</ref>, and adapter-based tuning <ref type="bibr" target="#b26">(Houlsby et al., 2019;</ref><ref type="bibr">Pfeiffer et al., 2020b;</ref><ref type="bibr" target="#b27">Hu et al., 2021)</ref>. Several works in adapter-based tuning are closely related to ours. AdapterFusion <ref type="bibr" target="#b51">(Pfeiffer et al., 2021)</ref> aims to combine multiple task adapters but does not offer specific architecture or training strategies to learn external knowledge. DEMix <ref type="bibr" target="#b20">(Gururangan et al., 2022)</ref> and MixDA both train adapters that specialize in domains and use mechanisms to route different adapters, but differ in routing methods, base models, and training strategies. K-Adapter <ref type="bibr">(Wang et al., 2021a</ref>) is re-stricted by its training on T-REx triples and lacks the flexibility to train on unstructured knowledge. Similar to MixDA, CPT <ref type="bibr">(Ke et al., 2022a)</ref> integrates domain knowledge into LMs, but it employs a different approach. While MixDA uses domain adapters to substitute FFN layers and task adapters to perform end tasks, CPT adds CL-Plugins that learn domain knowledge. Recent work by <ref type="bibr">He et al. (2021a)</ref> presents a unified framework that establishes connections across different PEFT methods. Our work can leverage any PEFT method and complement them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Mixture-of-Experts</head><p>Mixture-of-Experts (MoE) <ref type="bibr" target="#b57">(Shazeer et al., 2017)</ref> is introduced with several expert networks, gating networks, and load-balancing techniques. The following studies improve MoE on initialization and training schemes <ref type="bibr" target="#b16">(Fedus et al., 2022)</ref>, routing mechanisms <ref type="bibr" target="#b78">(Zuo et al., 2021;</ref><ref type="bibr" target="#b71">Yang et al., 2021)</ref>, and load-balancing issues <ref type="bibr" target="#b37">(Lewis et al., 2021;</ref><ref type="bibr" target="#b55">Roller et al., 2021)</ref>. AdaMix <ref type="bibr" target="#b67">(Wang et al., 2022)</ref> proposed a mixture of adapters to improve the downstream task performance. Instead of mixing different designs of adapters, our domain adapter is a feedforward network specifically designed for domain knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Given a pre-trained language model M, the input is a sentence X " t 1 t 2 ???t i ???t T (t i indicates the i-th token) and the output is the representation of each token. The overall architecture of our model is shown in Figure <ref type="figure" target="#fig_0">1</ref>. The training process is divided into two-stage. In Stage 1 (Figure <ref type="figure" target="#fig_0">1</ref> (a)), we inject new feed-forward networks (FFNs) (namely domain-adapter) paralleled to the original pre-trained FFNs in some Transformer layers, acting as a key-value memory. The newly injected domain-adapter is trained on both domain-specific unlabeled data and original pre-training unlabeled data to store new factual associations while keeping old-domain ones. All modules are frozen except domain-adapter in this stage. In Stage 2 (Figure <ref type="figure" target="#fig_0">1</ref> (b)), we train a mixture-of-adapters (MoA) gate and a task-adapter on downstream tasks with labeled data, and only these two new modules are updated. The MoA gate receives outputs from the old-domain FFNs and domain-adapter, then outputs a weighted sum of them. An additional taskadapter is inserted in each Transformer block to facilitate downstream tasks. Figure <ref type="figure" target="#fig_0">1 (c)</ref> shows the structures of the domain-adapter and the MoA gate.</p><p>In this section, we first introduce domainadapter, which learns and stores domain-specific knowledge, and then describe task-adapters that perform the downstream task. Finally, we discuss how the MoA gate integrates the outputs from the FFN and the domain-adapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Domain-Adapter</head><p>Previous studies <ref type="bibr" target="#b18">(Geva et al., 2021;</ref><ref type="bibr" target="#b5">Cao et al., 2021;</ref><ref type="bibr" target="#b48">Meng et al., 2022)</ref> suggest that factual associations are stored in the FFNs of some Transformer layers. To help models learn domain-specific knowledge, we propose a lightweight domain-adapter that works parallel to the FFNs, and a training method to learn domain-specific knowledge alongside keeping old-domain ones. Domain-adapter has a simple bottleneck architecture consisting of a down projection layer, a nonlinearity (such as ReLU (Agarap, 2018)), and an up projection layer. This helps keep the parameter size low <ref type="bibr" target="#b26">(Houlsby et al., 2019)</ref> with competitive performance.</p><p>In Stage 1, the domain-adapter is trained with the domain-specific and old-domain datasets in one batch. Note that all other parameters are frozen except the domain-adapter at this stage. Let L K denote the knowledge loss related to domain-specific knowledge, and L S denote the sampling loss related to old-domain knowledge. The knowledge loss is a cross-entropy loss on predicting masked tokens, and the sampling loss is designed to align the latent spaces of the old-domain knowledge and new domain-specific knowledge. The total loss L is given by a weighted sum of the two, that is:</p><formula xml:id="formula_0">L " ? ?LK `LS ,<label>(1)</label></formula><p>where ? is a weight for the knowledge loss.</p><p>The knowledge loss is implemented by using cross-entropy loss. Given a sentence with M mask tokens whose answers are m 1 , m 2 , ???, m M , respectively, the knowledge loss L K is given by</p><formula xml:id="formula_1">L K " ?1 M M ? i"1 log ppm i q,<label>(2)</label></formula><p>where ppm i q is the probability for token m i output by M.  2016)), we translate each relation into a sentence, and then mask out its object. For example, the relation "the Eiffel tower-/r/LocatedAt-Paris" is translated into "The Eiffel Tower is located at Paris.", then "Paris" is substituted with the mask token, and the model is trained to fill the mask. ? Unstructured knowledge For unstructured knowledge (e.g., downstream unlabeled texts), we use the masked language model (MLM) similar to RoBERTa pretraining. Some tokens are randomly sampled from the input sentence and replaced with the special token &lt;mask&gt;, and the model is trained to predict the masked token. The cross-entropy loss is calculated to optimize the model. For old-domain knowledge and sampling loss, we train the model on general corpora including Wikipedia and BookCorpus <ref type="bibr" target="#b77">(Zhu et al., 2015)</ref>. Specifically, for each batch, sentences randomly sampled from the dataset are input into the model. Given L layers that have domain-adapters installed, for each such layer l, we collect token representations from the FFN F l , and representations from the domain-adapter K l . The goal is to keep them as similar as possible. Thus, we calculate the sampling loss L S with L2 loss:</p><formula xml:id="formula_2">L S " 1 L L ? l"1 ||F l ?Kl || 2 2 .</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Task-Adapter</head><p>After training domain-adapters, the model is aware of the domain knowledge, which is not directly related to downstream tasks though. Therefore, we add task-adapters on top of the domain-adapter to adapt to downstream tasks. For example, a domainadapter trained in biomedical knowledge can sup- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mixture-of-Adapters Gate</head><p>On downstream tasks, it is possible that the output from the FFN, or a weighted sum of the two, produces better results. Therefore, in Stage 2, we train an additional mixture-of-adapters (MoA) gate simultaneously. The MoA gate receives the outputs from the attention layer q, the domain-adapter K, and the FFN F . q is first sent into a multi-layer perceptron (MLP):</p><formula xml:id="formula_3">h " MLPpqq. (<label>4</label></formula><formula xml:id="formula_4">)</formula><p>The MLP is composed of a down-projection layer W d and an up-projection layer W u , and h " W u ?pW d qq, where ? is the nonlinearity function.</p><p>Then, h is input into a Sigmoid layer to generate the weights of the FFNs and other domain-adapters:</p><p>w " Sigmoidphq.</p><p>(5)</p><p>The final output o is a weighted sum of the outputs of the FFNs and the domain-adapter:</p><formula xml:id="formula_5">o " wrK; F s,<label>(6)</label></formula><p>where r; s denotes matrix concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Settings</head><p>In this section, we first introduce the datasets, then the baseline models, the evaluation metrics, and implementation details in the following four subsections, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.</p><p>? ID: GLUE Benchmark <ref type="bibr" target="#b64">(Wang et al., 2018)</ref> including MNLI <ref type="bibr" target="#b69">(Williams et al., 2017)</ref>, CoLA <ref type="bibr" target="#b68">(Warstadt et al., 2019)</ref>, MRPC <ref type="bibr" target="#b14">(Dolan and Brockett, 2005)</ref>, SST-2 <ref type="bibr" target="#b59">(Socher et al., 2013)</ref>, RTE <ref type="bibr" target="#b7">(Dagan et al., 2005;</ref><ref type="bibr" target="#b22">Haim et al., 2006;</ref><ref type="bibr" target="#b19">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009)</ref>, STS-B <ref type="bibr" target="#b6">(Cer et al., 2017)</ref>, WNLI <ref type="bibr" target="#b35">(Levesque et al., 2012)</ref>, QNLI <ref type="bibr" target="#b54">(Rajpurkar et al., 2016)</ref>, and QQP <ref type="bibr" target="#b29">(Iyer et al., 2017)</ref>. ? OOD: ChemProt <ref type="bibr" target="#b32">(Kringelum et al., 2016)</ref>, RCT <ref type="bibr" target="#b8">(Dernoncourt and Lee, 2017)</ref>, IMDB <ref type="bibr" target="#b47">(Maas et al., 2011)</ref>, and Amazon <ref type="bibr">(He and McAuley, 2016)</ref>. ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings. ? KI: FEVER <ref type="bibr" target="#b63">(Thorne et al., 2018)</ref> and Common-senseQA (CSQA) <ref type="bibr" target="#b62">(Talmor et al., 2019)</ref>. FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Common-senseQA consists of 12,247 questions with 5 choices and requires commonsense knowledge to predict the correct answers.</p><p>For Stage 1, we train the domain-adapter with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table <ref type="table" target="#tab_1">1</ref>. We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following <ref type="bibr" target="#b49">(Perez et al., 2021)</ref> to demonstrate the effectiveness of MixDA. For each dataset class, we randomly sample K " 16 examples from the original training set as the new training set, and another different K " 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>In our experiments, we use the following models as the main baselines. For convenience, we refer to them with the abbreviations in the parentheses later. ? HOULSBY (HO): Houlsby adapter <ref type="bibr" target="#b26">(Houlsby et al., 2019)</ref>  Prefix-Tuning trains a number of prompt embeddings for each task and pre-pends it before tokens. ? FINE-TUNING (FT): Fine-tuning all of the parameters of the RoBERTa-large model on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>We adopt the Pearson correlation for STS-B since it is a regression task. The remaining are text classification tasks. Following <ref type="bibr" target="#b64">Wang et al. (2018)</ref>; Gururangan et al. ( <ref type="formula">2020</ref>); <ref type="bibr" target="#b12">Diao et al. (2021)</ref>, we adopt macro-F1 for MRPC and QQP, and micro-F1 for others as evaluation metrics. Macro-F1 computes the F1 independently for each metric, while micro-F1 computes an average metric of all classes. To account for the instability of small datasets, we report the average performance and the standard deviation of 3 runs with different random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation</head><p>We implement our RoBERTa-large model based on the Transformers library from HuggingFace<ref type="foot" target="#foot_0">2</ref> . The Houlsby adapter, the Pfeiffer adapter, and Prefix-Tuning are implemented based on the adaptertransformers library <ref type="bibr">(Pfeiffer et al., 2020a)</ref>. LoRA is implemented based on OpenDelta <ref type="bibr" target="#b13">(Ding et al., 2022)</ref>. During Stage 1, we train the domain-adapter with learning rate 1e-4, batch size 20, and weight decay 0.05. The knowledge loss factor ? is set to 0.5. We train the 7 and 11 layers of RoBERTa-large with domain-adapter in 10 epochs. In Stage 2, we use the Pfeiffer adapter as the default task-adapter and train 20 epochs. All the experiments are conducted on Nvidia 2080Ti GPUs. We find the best hyper-parameters through grid search and the best results are listed in Appendix A. The computation time can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We compare the performance of MixDA with our baselines on 15 datasets. First, we train the domainadapter for each domain individually and then perform each task with its corresponding domainadapter, which shows significant improvement over our baselines. Next, we plug in several domainadapters trained on different domains parallelly to verify the scalability of our model.  detect the chemical-protein interaction. For example, MixDA shows more familiarity with words associated with that field, such as "gefitinib" and "tyrosine kinase inhibitor". In contrast, MixDA falters on STS-B, falling behind Pfeiffer by 0.8%. This is because the knowledge in Stage 1 is not effectively utilized. STS-B consists of sentence pairs like "The cat sat on the mat" and "The cat did not sit on the mat", with little need for additional knowledge. Across the three task domains, MixDA has an average improvement of 4.8% over RoBERTa + Pfeiffer on out-of-domain tasks, 2.5% on indomain tasks, and 5.0% on knowledge-intensive tasks. It shows that MixDA is not only effective for out-of-domain tasks and knowledge-intensive tasks that require additional knowledge but is helpful for general-domain language tasks as well, demonstrating its ability to excel at both in-domain and out-of-domain tasks (reliability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Single Domain Adapter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Parallel Domain Adapters</head><p>In the previous section, we explored using a single domain-adapter for each downstream task. Next, we show the scalability of MixDA by using parallel domain-adapters and only train the MoA layer and task-adapters in Stage 2. The training process in Stage 2 follows the previous experiments. Table <ref type="table" target="#tab_5">3</ref> shows the comparison across single domainadapter, parallel domain-adapters, and RoBERTa + Pfeiffer on 7 datasets. On average, parallel domainadapters show an improvement of 0.6% over vanilla RoBERTa + Pfeiffer, even though they fall behind the single domain adapter by 1.9%. This could be attributed to the MoA gate choosing the suboptimal domain-adapter for some test data. Still, considering its improvement over Pfeiffer, the MoA gate chooses the correct domain-adapter in most cases. Therefore, MixDA demonstrates its scalability, allowing end users to train Stage 1 on different datasets and combine them later. Overall, in both single and parallel situations, MixDA significantly improves upon the vanilla RoBERTa + Pfeif-  fer model with a small increase in model size. This is due to the ability of MixDA to capture knowledge and the MoA to select useful knowledge for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis</head><p>In this section, we analyze the respective contributions of each part of MixDA through detailed analysis, including the Stage 1 training, task-adapters in Stage 2, and the mixture-of-adapters gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Ablation Study</head><p>In this section, we conduct an ablation study to reveal the contributions of each part of the model. There are three variants: (1) We remove the MoA gate and choose the domain-adapter instead of the RoBERTa feed-forward layer (-MoA). ( <ref type="formula" target="#formula_1">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Structured and Unstructured Knowledge</head><p>In Section 5, the MixDA is only trained on unstructured knowledge. As a comparison, we also train the domain adapter on ConceptNet, a structured knowledge dataset, and then attach both the unstructured and structured to our model and train the MoA layer and the task-adapter during Stage 2.</p><p>Table <ref type="table" target="#tab_7">5</ref> shows the result of combining structured and unstructured knowledge in Stage 1. FEVER and CSQA, two knowledge-intensive tasks, have the greatest improvement: 10.3% for FEVER and 1.2% for CSQA. This is because ConceptNet stores commonsense knowledge that can help both tasks. Meanwhile, MRPC and STS-B also obtain improvement, showing that ConceptNet can benefit general language tasks as well. In conclusion, the experiment demonstrates the ability of MixDA to utilize structured knowledge, the extensibility of our model, and the possible benefits of structured knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effectiveness of Task-Adapters</head><p>In most experiments of this paper, we adopt Pfeiffer as the task-adapter unless otherwise specified. In this section, we test the performance of MixDA combined with other kinds of task-adapters, including Houlsby, Prefix-Tuning, LoRA, and Pfeiffer. parameters compared to Houlsby, making it the optimal choice of task-adapters in our experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we proposed MixDA, a mixture of adapters for domain adaptation. We first decouple the knowledge modules (i.e., FFNs) into the old-domain and domain-specific FFNs. Then we propose a two-stage adapter tuning strategy: first tuning the domain adapter on each domain and then tuning the task adapter on each task. Moreover, our model could be scaled to multiple domains easily with the introduction of the mixture-of-adapters gate. Empirically, MixDA achieved significant improvement over in-domain tasks, out-of-domain tasks, and knowledge-intensive tasks. Further analyses demonstrate the reliability, scalability, and efficiency of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although MixDA achieves promising results on domain adaptation compared with baseline models, there are certain limitations. MixDA is a two-stage approach, which is not fully end-to-end. Our approach requires training a domain adapter and task adapter, respectively. In the future, we will explore the unifying domain and task adapters by merging them into one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental Setup</head><p>Our domain adapter has a reduction factor of 16, consisting of two linear layers 4096 ?256 and 256?1024 (1.31M parameters). With each domain adapter also comes a MoA gate which has an FFN with 4096 ?2 (number of MixDAs) parameters. Since domain adapters are placed in Layers 7 and 11, they have 2.6M parameters in total. Therefore, the domain adapters (excluding task-adapters) only add 0.7% additional parameters to RoBERTa-large.</p><p>We preprocess the unstructured data in Stage 1 similar to the masked language model directive in RoBERTa. From the text, we choose 15% of tokens uniformly to perform possible alterations. In those tokens, 85% are replaced with &lt;mask&gt;, 10% are left unchanged, and 5% are replaced with a random token. The preprocessing is implemented with DataCollatorForLanguageModeling in Huggingface Transformers. In Stage 2, we use few-shot setting with K " 16. For each class of the dataset, we randomly select 16 examples before run.</p><p>In Stages 1 and 2, we use a linear weight scheduler. All the models are optimized by AdamW <ref type="bibr" target="#b46">(Loshchilov and Hutter, 2017)</ref> with weight decay 0.05. The best hyperparameters for Stage 2 are found with grid search, with batch size t2, 4, 8, 16u and learning rate t5e ?5, 1e ?4, 5e 4u. The details can be found in Tables <ref type="table">7</ref> and<ref type="table">8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Computational Budget</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Datasets</head><p>We conduct experiments on three types of datasets: in-domain (ID) tasks that require general-domain knowledge; out-of-domain (OOD) tasks that require domain-specific knowledge; knowledgeintensive (KI) tasks that require commonsense knowledge.</p><p>For in-domain tasks, we evaluate our model on the GLUE Benchmark <ref type="bibr" target="#b64">(Wang et al., 2018)</ref>.</p><p>It includes MNLI <ref type="bibr" target="#b69">(Williams et al., 2017)</ref>, CoLA <ref type="bibr" target="#b68">(Warstadt et al., 2019)</ref>, MRPC <ref type="bibr" target="#b14">(Dolan and Brockett, 2005)</ref>, SST-2 <ref type="bibr" target="#b59">(Socher et al., 2013)</ref>, RTE <ref type="bibr" target="#b7">(Dagan et al., 2005;</ref><ref type="bibr" target="#b22">Haim et al., 2006;</ref><ref type="bibr" target="#b19">Giampiccolo et al., 2007;</ref><ref type="bibr" target="#b3">Bentivogli et al., 2009)</ref>, STS-B <ref type="bibr" target="#b6">(Cer et al., 2017)</ref>, WNLI <ref type="bibr" target="#b35">(Levesque et al., 2012)</ref>, QNLI <ref type="bibr" target="#b54">(Rajpurkar et al., 2016)</ref>, and QQP <ref type="bibr" target="#b29">(Iyer et al., 2017)</ref>. They are all singlesentence or sentence pair classification tasks except STS-B, which is a regression task.</p><p>We also evaluate our model on several out-ofdomain tasks, including ChemProt <ref type="bibr" target="#b32">(Kringelum et al., 2016)</ref>, RCT <ref type="bibr" target="#b8">(Dernoncourt and Lee, 2017)</ref>, IMDB <ref type="bibr" target="#b47">(Maas et al., 2011)</ref>, and Amazon <ref type="bibr">(He and McAuley, 2016)</ref>. ChemProt is a manually annotated chemical-protein interaction dataset extracted from 5,031 abstractions. RCT is a dataset based on PubMed for sentence classification. IMDB provides 25,000 movie reviews for sentiment analysis. Amazon is a dataset containing product reviews from Amazon, annotated with user ratings.</p><p>For knowledge-intensive tasks, we evaluate our model on FEVER <ref type="bibr" target="#b63">(Thorne et al., 2018)</ref> and CommonsenseQA (CSQA) <ref type="bibr" target="#b62">(Talmor et al., 2019)</ref>. FEVER consists of 185,445 claims that correspond to Wikipedia articles and are classified as supported, refuted, and not enough information. Com-monsenseQA consists of 12,247 questions with 5 choices, each of which requires commonsense knowledge to predict the correct answers.</p><p>For Stage 1, we train domain-adapters with unstructured knowledge related to the dataset following Section 3.1. The unstructured knowledge used is listed in Table <ref type="table" target="#tab_1">1</ref>. We also experiment with structured knowledge in Section 6.2. For Stage 2, we adopt the true few-shot setting following <ref type="bibr" target="#b49">(Perez et al., 2021)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overall structure of the model. Our training method includes two stages: (a) In Stage 1, we introduce domain-adapters into the model and freeze other parameters. The model learns from domain-specific knowledge (knowledge loss L K ) and keeps similar outputs with the FFN on old-domain knowledge (sample loss L S ). L K and L S are then combined into the total loss L. (b) In Stage 2, we introduce the mixture-of-adapters gate and task-adapters, then freeze the domain-adapter. The model is trained to perform downstream tasks, which gives us the total loss L. (c) shows the detailed structures of the domain-adapter and the MoA gate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) We exclude old-domain knowledge during Stage 1 (-Old). (3) To examine whether the training procedures, rather than the MixDA structure, contribute the most to our results, we conduct Stage 1 and Stage 2 training only with task-adapters (-DA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Stage 1 training takes relatively longer time, while Stage 2 is fast due to the few-shot setting. The training time of Stage 1 is proportional to the number of tokens. For reference, with 4 Nvidia RTX 2080Ti, Stage 1 training for Biomed (33.6M tokens) takes ~45min per epoch, and training for Review (7.4M tokens) takes ~5min per epoch. Stage 2 training is generally fast: The 20-epoch training process usually takes less than 5min with 4 Nvidia RTX 2080Ti.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to demonstrate the effectiveness of MixDA. For each class of each dataset, we randomly sample K " 16 examples from the original training set as the new training set, and another different K " 16 examples as the validation set. The original validation set will be used as the test set. The Pfeiffer adapter is used in Stage 2 unless stated otherwise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Domain knowledge in Stage 1 training. Section 3.3) are frozen. The training of the adapter follows its corresponding approach, despite the addition of domain-adapters. For example, for a text classification task, we add a classification layer on top of the model, freeze all parameters other than the classification layer, the MoA gate, and the task-adapters, feed input texts into the model, and use cross-entropy as the loss.</figDesc><table><row><cell cols="2">Domain Tasks</cell><cell>Domain Knowledge</cell><cell cols="2"># Tokens Size</cell></row><row><cell>Biomed</cell><cell cols="3">ChemProt, RCT 2.68K papers about biology and chemistry from S2ORC (Lo et al., 2020) 33.6M</cell><cell>144MB</cell></row><row><cell>Review</cell><cell cols="2">Amazon, IMDB 24.75K randomly selected Amazon reviews</cell><cell>7.4M</cell><cell>34MB</cell></row><row><cell>ID</cell><cell>GLUE tasks</cell><cell>Corpus of GLUE tasks</cell><cell>29.0M</cell><cell>146MB</cell></row><row><cell>KI</cell><cell>FEVER, CSQA</cell><cell>Corpus of both CommonsenseQA and FEVER datasets</cell><cell>5.9M</cell><cell>34MB</cell></row><row><cell cols="3">port different tasks in the domain, while training</cell><cell></cell><cell></cell></row><row><cell cols="3">it on a task limits its capability to the specific task.</cell><cell></cell><cell></cell></row><row><cell cols="3">Task-adapters can be any adapter architecture or</cell><cell></cell><cell></cell></row><row><cell cols="3">other parameter-efficient fine-tuning methods, such</cell><cell></cell><cell></cell></row><row><cell cols="3">as the Houlsby adapter (Houlsby et al., 2019), Pfeif-</cell><cell></cell><cell></cell></row><row><cell cols="3">fer adapter (Pfeiffer et al., 2020b), prefix-tuning (Li</cell><cell></cell><cell></cell></row><row><cell cols="3">and Liang, 2021), and so on. At Stage 2, all pa-</cell><cell></cell><cell></cell></row><row><cell cols="3">rameters other than the task-adapters and the MoA</cell><cell></cell><cell></cell></row><row><cell>gate (</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>shows the performance of a single domain</cell></row><row><cell>adapter compared with baselines. It is only trained</cell></row><row><cell>on unstructured knowledge during Stage 1 in the</cell></row><row><cell>following experiments. Results show that Mixture-</cell></row><row><cell>of-Domain-Adapters outperforms our baselines in</cell></row><row><cell>most datasets, with an average of 3.5% improve-</cell></row><row><cell>ment over the best baseline adapter (i.e., Pfeiffer),</cell></row><row><cell>and 3.3% over fine-tuning. Our method even out-</cell></row><row><cell>performs fine-tuning in most datasets, despite far</cell></row><row><cell>less training time and smaller parameter size. Over</cell></row><row><cell>the datasets, MixDA shows the most significant im-</cell></row><row><cell>provement on ChemProt, with 6.9% over Pfeiffer</cell></row><row><cell>and 2.7% over fine-tuning. One possible reason</cell></row><row><cell>is that MixDA learns the necessary knowledge to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The overall performance of single MixDA and baselines on the downstream tasks. We use K " 16 (per class) for few-shot experiments. The best result for each dataset is made bold. We report mean and standard deviation over 3 runs with different random seeds.</figDesc><table><row><cell></cell><cell>Amazon</cell><cell>IMDB</cell><cell>FEVER</cell><cell>WNLI</cell><cell>QQP</cell><cell>RTE</cell><cell>MRPC</cell><cell>Avg.</cell></row><row><cell>Pfeiffer</cell><cell>49.7?1.4</cell><cell>55.4?5.6</cell><cell>27.4?7.5</cell><cell>58.1?2.5</cell><cell>55.2?1.0</cell><cell>54.1?1.3</cell><cell>80.7?0.6</cell><cell>54.4</cell></row><row><cell>Single</cell><cell>54.7?1.6</cell><cell>58.1?5.1</cell><cell>32.6?9.4</cell><cell>60.1?1.7</cell><cell>56.1?0.6</cell><cell>54.9?1.5</cell><cell>81.6?0.5</cell><cell>56.9</cell></row><row><cell>Parallel</cell><cell>51.6?2.4</cell><cell>47.9?2.1</cell><cell>34.5?0.5</cell><cell>58.7?1.8</cell><cell>57.8?3.5</cell><cell>53.8?0.9</cell><cell>81.0?0.2</cell><cell>55.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>The performance of parallel domain-adapters on the chosen downstream tasks. Parallel, Single, and Pfeiffer denote parallel domain-adapters, single domain-adapter, and vanilla RoBERTa + Pfeiffer, respectively. The best result for each dataset is made bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablations of the MoA gate, old-domain knowledge, and the domain-adapter structure and comparisons with other adapter-based tuning methods. For -Old, we omit old-domain knowledge in Stage 1 training. For -DA, we remove the domain-adapter structure and conduct both stages of training only with Pfeiffer adapters. The best results for each dataset are made bold.</figDesc><table><row><cell>Datasets</cell><cell cols="2">ChemProt</cell><cell cols="2">IMDB</cell><cell>MRPC</cell><cell>STS-B</cell><cell>CSQA</cell><cell>Avg.</cell></row><row><cell>MixDA</cell><cell></cell><cell>60.6?4.9</cell><cell cols="2">58.1?5.1</cell><cell>81.6?0.5</cell><cell>89.8?0.4</cell><cell>38.8?4.0</cell><cell>65.8</cell></row><row><cell>-MoA</cell><cell></cell><cell>55.7?1.8</cell><cell cols="2">49.8?0.0</cell><cell>80.9?0.5</cell><cell>88.4?0.4</cell><cell>28.3?1.3</cell><cell>60.6</cell></row><row><cell>-Old</cell><cell></cell><cell>54.3?6.5</cell><cell cols="2">41.4?3.6</cell><cell>78.7?3.7</cell><cell>90.0?0.4</cell><cell>27.1?0.3</cell><cell>58.3</cell></row><row><cell>-DA</cell><cell></cell><cell>21.2?4.7</cell><cell cols="2">56.8?3.9</cell><cell>81.0?0.5</cell><cell>80.8?2.4</cell><cell>27.4?0.5</cell><cell>53.4</cell></row><row><cell cols="2">AdapterFusion</cell><cell>47.7?0.1</cell><cell cols="2">54.4?2.0</cell><cell>78.0?1.5</cell><cell>90.3?0.3</cell><cell>25.0?1.7</cell><cell>59.1</cell></row><row><cell>K-Adapter</cell><cell></cell><cell>58.2?5.0</cell><cell cols="2">55.6?4.5</cell><cell>53.9?5.9</cell><cell>89.7?0.4</cell><cell>26.2?4.7</cell><cell>56.7</cell></row><row><cell>CPT</cell><cell></cell><cell>45.9?0.3</cell><cell cols="2">56.1?5.2</cell><cell>81.0?0.5</cell><cell>90.2?0.1</cell><cell>33.7?2.7</cell><cell>61.4</cell></row><row><cell>Datasets</cell><cell>MRPC</cell><cell>STS-B</cell><cell>FEVER</cell><cell>CSQA</cell><cell>Avg.</cell><cell></cell></row><row><cell>MixDA</cell><cell>81.6?0.5</cell><cell>89.8?0.4</cell><cell>20.2?4.3</cell><cell>38.8?4.0</cell><cell>57.6</cell><cell></cell></row><row><cell>+ ConceptNet</cell><cell>81.7?0.3</cell><cell>90.1?0.1</cell><cell>30.5?3.1</cell><cell>40.0?0.2</cell><cell>60.6</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The results of MixDA trained on structured and unstructured knowledge. + ConceptNet stands for domain-adapters trained on both the unstructured knowledge and ConceptNet.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>shows the results of the ablation study. As ex-</cell></row><row><cell>pected, the average performance drops in all three</cell></row><row><cell>settings. Without MoA gate, old-domain knowl-</cell></row><row><cell>edge FFNs, and structure knowledge, it is observed</cell></row><row><cell>a drop of 5.2%, 7.5%, and 12.4%, respectively,</cell></row><row><cell>showing that the MoA gate, the old-domain knowl-</cell></row><row><cell>edge, and the MixDA structure are all fundamental</cell></row><row><cell>in the model. Relatively, the MoA has the smallest</cell></row><row><cell>impact because the old-domain knowledge in Stage</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Table 6 gives the result of different task-adapters. Pfeiffer surpasses others by at least 6.3%. Even though Houlsby is on par with Pfeiffer, Pfeiffer only requires half the number of newly introduced The results of MixDA combined with different kinds of task-adapters. By default, we use Pfeiffer in previous experiments.</figDesc><table><row><cell>Datasets</cell><cell>ChemProt</cell><cell>IMDB</cell><cell>MRPC</cell><cell>STS-B</cell><cell>CSQA</cell><cell>Avg.</cell></row><row><cell>Houlsby</cell><cell>47.1?12.2</cell><cell>48.1?4.5</cell><cell>80.0?1.5</cell><cell>86.6?3.2</cell><cell>35.8?8.9</cell><cell>59.5</cell></row><row><cell>Prefix-Tuning</cell><cell>17.1?7.3</cell><cell>39.1?7.2</cell><cell>81.6?0.4</cell><cell>88.6?0.5</cell><cell>33.3?0.0</cell><cell>51.9</cell></row><row><cell>LoRA</cell><cell>19.5?11.1</cell><cell>36.1?4.1</cell><cell>81.2?0.0</cell><cell>86.7?1.4</cell><cell>20.3?10.9</cell><cell>48.8</cell></row><row><cell>Pfeiffer</cell><cell>60.6?4.9</cell><cell>58.1?5.1</cell><cell>81.6?0.5</cell><cell>89.8?0.4</cell><cell>38.8?4.0</cell><cell>65.8</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/huggingface/transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We thank the anonymous reviewers for their valuable suggestions. This work was supported by the <rs type="funder">General Research Fund (GRF) of Hong Kong</rs> (No. <rs type="grantNumber">16310222</rs> and No. <rs type="grantNumber">16201320</rs>). <rs type="person">Shizhe Diao</rs> and <rs type="person">Ruijia Xu</rs> were supported by the <rs type="funder">Hong Kong Ph.D. Fellowship Scheme (HKPFS)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sbqJqa3">
					<idno type="grant-number">16310222</idno>
				</org>
				<org type="funding" xml:id="_PHhqn5m">
					<idno type="grant-number">16201320</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep learning using rectified linear units (relu)</title>
		<author>
			<persName><forename type="first">Abien</forename><surname>Fred</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agarap</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Publicly Available Clinical BERT Embeddings</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Hung</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Clinical Natural Language Processing Workshop</title>
		<meeting>the 2nd Clinical Natural Language Processing Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="72" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciBERT: A Pretrained Language Model for Scientific Text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3606" to="3611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note>In TAC</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Editing factual knowledge in language models</title>
		<author>
			<persName><forename type="first">Nicola</forename><surname>De Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilker</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">PubMed 200k RCT: a dataset for sequential sentence classification in medical abstracts</title>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="308" to="313" />
		</imprint>
	</monogr>
	<note>Asian Federation of Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Pre-training</publisher>
			<biblScope unit="volume">BERT</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<idno type="DOI">10.48550/arXiv.1810.04805</idno>
		<title level="m">Deep Bidirectional Transformers for Language Understanding</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zen: Pre-training chinese text encoder enhanced by n-gram representations</title>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4729" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Taming pre-trained language models with n-gram representations for lowresource domain adaptation</title>
		<author>
			<persName><forename type="first">Shizhe</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3336" to="3349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models</title>
		<author>
			<persName><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Min</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weize</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06904</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Paraphrasing</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Ki-bert: Infusing knowledge context for better language and domain understanding</title>
		<author>
			<persName><forename type="first">Keyur</forename><surname>Faldu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sheth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Kikani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hemang</forename><surname>Akbari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08145</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">120</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.295</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3816" to="3830" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformer feed-forward layers are key-value memories</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5484" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third PASCAL recognizing textual entailment challenge</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</title>
		<meeting>the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing</meeting>
		<imprint>
			<publisher>Prague. Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Demix layers: Disentangling domains for modular language modeling</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter</title>
		<meeting>the 2022 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5557" to="5576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<author>
			<persName><forename type="first">Suchin</forename><surname>Gururangan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8342" to="8360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName><forename type="first">Ido</forename><surname>Bar Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PAS-CAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bert-mk: Integrating graph contextualized knowledge into pretrained language models</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Jing Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2281" to="2290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">2021a. Towards a unified view of parameter-efficient transfer learning</title>
		<author>
			<persName><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering</title>
		<author>
			<persName><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872427.2883037</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. Ruining He and Julian McAuley</title>
		<imprint>
			<date type="published" when="2016">2021. 2016</date>
			<biblScope unit="page" from="507" to="517" />
		</imprint>
	</monogr>
	<note>Proceedings of the 25th International Conference on World Wide Web, WWW &apos;16. Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Giurgiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruna</forename><surname>Morrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>De Laroussilhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mona</forename><surname>Attariyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1902.00751</idno>
		<title level="m">Parameter-Efficient Transfer Learning for NLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyuan</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shean</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2106.09685</idno>
		<idno>arXiv</idno>
		<title level="m">LoRA: Low-Rank Adaptation of Large Language Models</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05342</idno>
		<title level="m">ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">First quora dataset release: Question pairs. data. quora. com</title>
		<author>
			<persName><forename type="first">Shankar</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Dandekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korn?l</forename><surname>Csernai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2022a. Continual training of language models for few-shot learning</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="10205" to="10216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adapting a language model while preserving its general knowledge</title>
		<author>
			<persName><forename type="first">Zixuan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haowei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10177" to="10188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ChemProt-3.0: a global chemical biology diseases mapping</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Kringelum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonny</forename><surname>Kim Kjaerulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S?ren</forename><surname>Brunak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ole</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><forename type="middle">I</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Taboureau</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/bav123</idno>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="page">123</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Informing unsupervised pretraining with external linguistic knowledge</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Lauscher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Edoardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName><surname>Glava?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">BioBERT: A Pre-Trained Biomedical Language Representation Model for Biomedical Text Mining</title>
		<author>
			<persName><forename type="first">Jinhyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wonjin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">So</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth international conference on the principles of knowledge representation and reasoning</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sensebert: Driving some sense into bert</title>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barak</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ori</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Padnos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Or</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amnon</forename><surname>Shashua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Shoham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4656" to="4667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Base layers: Simplifying training of large, sparse models</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6265" to="6274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1910.13461</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Translation, and Comprehension. arXiv</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2101.00190</idno>
		<title level="m">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2829" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">2021a. Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>ArXiv preprint, abs/2107.13586</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">K-bert: Enabling language representation with knowledge graph</title>
		<author>
			<persName><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haotang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<idno>abs/2103.10385</idno>
		<title level="m">Zhilin Yang, and Jie Tang. 2021b. GPT Understands, Too</title>
		<imprint/>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1711.05101</idno>
		<title level="m">Decoupled weight decay regularization</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2202.05262</idno>
		<idno>arXiv</idno>
		<title level="m">Locating and Editing Factual Associations in GPT</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">True few-shot learning with language models. Advances in Neural Information Processing Systems</title>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11054" to="11070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Matthew E Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adapterfusion: Non-destructive task composition for transfer learning</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16</title>
		<meeting>the 16</meeting>
		<imprint>
			<publisher>Main Volume</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="487" to="503" />
		</imprint>
	</monogr>
	<note>th Conference of the European Chapter</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adapterhub: A framework for adapting transformers</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>R?ckl?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clifton</forename><surname>Poth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7654" to="7673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<title level="m">Squad: 100,000+ questions for machine comprehension of text</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Hash layers for large sparse models</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="17555" to="17566" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">It&apos;s not just size that matters: Small language models are also fewshot learners</title>
		<author>
			<persName><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.185</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2339" to="2352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Editable neural networks</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Sinitsin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vsevolod</forename><surname>Plokhotnyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Pyrkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1612.03975</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>ConceptNet 5.5: An Open Multilingual Graph of General Knowledge. arXiv</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danxiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09223</idno>
		<title level="m">Ernie: Enhanced representation through knowledge integration</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">CommonsenseQA: A question answering challenge targeting commonsense knowledge</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4149" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">FEVER: a large-scale dataset for fact extraction and VERification</title>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Christos Christodoulopoulos, and Arpit Mittal</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">2021a. K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan-Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<biblScope unit="page" from="1405" to="1418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Kepler: A unified model for knowledge embedding and pre-trained language representation</title>
		<author>
			<persName><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
		</imprint>
	</monogr>
	<note>Transactions of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahaj</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.17451</idno>
		<title level="m">Adamix: Mixtureof-adaptations for parameter-efficient model tuning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<title level="m">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiamang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15082</idno>
		<title level="m">M6-t: Exploring sparse expert models and beyond</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinnian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.140</idno>
		<title level="m">StyleDGPT: Stylized Response Generation with Pretrained Language Models. ACL Anthology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1548" to="1559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<author>
			<persName><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyprien</forename><surname>De Masson D'autume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11373</idno>
		<title level="m">Learning and evaluating general linguistic intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="270" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Ernie: Enhanced language representation with informative entities</title>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1441" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00363</idno>
		<title level="m">Modifying memories in transformer models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</title>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1506.06724</idno>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Taming sparsely activated transformer with stochastic experts</title>
		<author>
			<persName><forename type="first">Simiao</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hany</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
