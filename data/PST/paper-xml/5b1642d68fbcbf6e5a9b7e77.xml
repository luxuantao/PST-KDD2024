<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
							<email>jyzhao@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
							<email>marky@allenai.org</email>
							<affiliation key="aff2">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
							<email>vicente@virginia.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
							<email>kwchang@cs.ucla.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets. Our dataset and code are available at http://winobias.org.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Coreference resolution is a task aimed at identifying phrases (mentions) referring to the same entity. Various approaches, including rule-based <ref type="bibr">(Raghunathan et al., 2010)</ref>, feature-based <ref type="bibr" target="#b6">(Durrett and Klein, 2013;</ref><ref type="bibr" target="#b17">Peng et al., 2015a)</ref>, and neuralnetwork based <ref type="bibr" target="#b4">(Clark and Manning, 2016;</ref><ref type="bibr" target="#b15">Lee et al., 2017)</ref> have been proposed. While significant advances have been made, systems carry the risk of relying on societal stereotypes present in training data that could significantly impact their performance for some demographic groups.</p><p>In this work, we test the hypothesis that coreference systems exhibit gender bias by creating a new challenge corpus, WinoBias.This dataset follows the winograd format <ref type="bibr" target="#b9">(Hirst, 1981;</ref><ref type="bibr" target="#b20">Rahman and Ng, 2012;</ref><ref type="bibr" target="#b18">Peng et al., 2015b)</ref>, and contains references to people using a vocabulary of 40 occupations. It contains two types of challenge sentences that require linking gendered pro-</p><p>The physician called the secretary and told her the cancel the appointment.</p><p>The secretary called the physician and told him about a new patient.</p><p>The secretary called the physician and told her about a new patient.</p><p>The physician called the secretary and told him the cancel the appointment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type 2</head><p>The physician hired the secretary because she was highly recommended.</p><p>The physician hired the secretary because he was highly recommended.</p><p>The physician hired the secretary because she was overwhelmed with clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type 1</head><p>The physician hired the secretary because he was overwhelmed with clients. Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics. nouns to either male or female stereotypical occupations (see the illustrative examples in Figure <ref type="figure" target="#fig_0">1</ref>). None of the examples can be disambiguated by the gender of the pronoun but this cue can potentially distract the model. We consider a system to be gender biased if it links pronouns to occupations dominated by the gender of the pronoun (pro-stereotyped condition) more accurately than occupations not dominated by the gender of the pronoun (anti-stereotyped condition). The corpus can be used to certify a system has gender bias. <ref type="foot" target="#foot_0">1</ref>We use three different  <ref type="bibr" target="#b6">(Durrett and Klein, 2013)</ref> and the current best published system: the UW End-to-end Neural Coreference Resolution System <ref type="bibr" target="#b15">(Lee et al., 2017)</ref>. Despite qualitatively different approaches, all systems exhibit gender bias, showing an average difference in performance between pro-stereotypical and antistereotyped conditions of 21.1 in F1 score. Finally we show that given sufficiently strong alternative cues, systems can ignore their bias.</p><p>In order to study the source of this bias, we analyze the training corpus used by these systems, Ontonotes 5.0 <ref type="bibr">(Weischedel et al., 2012)</ref>. <ref type="foot" target="#foot_1">2</ref>Our analysis shows that female entities are significantly underrepresented in this corpus. To reduce the impact of such dataset bias, we propose to generate an auxiliary dataset where all male entities are replaced by female entities, and vice versa, using a rule-based approach. Methods can then be trained on the union of the original and auxiliary dataset. In combination with methods that remove bias from fixed resources such as word embeddings <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>, our data augmentation approach completely eliminates bias when evaluating on WinoBias , without significantly affecting overall coreference accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">WinoBias</head><p>To better identify gender bias in coreference resolution systems, we build a new dataset centered on people entities referred by their occupations from a vocabulary of 40 occupations gathered from the US Department of Labor, shown in Table <ref type="table">1</ref>. <ref type="foot" target="#foot_2">3</ref> We use the associated occupation statistics to determine what constitutes gender stereotypical roles (e.g. 90% of nurses are women in this survey). Entities referred by different occupations are paired and used to construct test case scenarios. Sentences are duplicated using male and female pronouns, and contain equal numbers of correct coreference decisions for all occupations. In total, the dataset contains 3,160 sentences, split equally for development and test, created by researchers familiar with the project. Sentences were created to follow two prototypical templates but annotators were encouraged to come up with scenar- Table <ref type="table">1</ref>: Occupations statistics used in WinoBias dataset, organized by the percent of people in the occupation who are reported as female. When woman dominate profession, we call linking the noun phrase referring to the job with female and male pronoun as 'pro-stereotypical', and 'anti-stereotypical', respectively. Similarly, if the occupation is male dominated, linking the noun phrase with the male and female pronoun is called, 'pro-stereotypical' and 'antisteretypical', respectively.</p><p>ios where entities could be interacting in plausible ways. Templates were selected to be challenging and designed to cover cases requiring semantics and syntax separately.<ref type="foot" target="#foot_3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type 1: [entity1] [interacts with] [entity2] [conjunction] [pronoun]</head><p>[circumstances]. Prototypical WinoCoRef style sentences, where co-reference decisions must be made using world knowledge about given circumstances (Figure <ref type="figure" target="#fig_0">1</ref>; Type 1). Such examples are challenging because they contain no syntactic cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Type 2: [entity1] [interacts with] [entity2] and then [interacts with] [pronoun] for [circumstances]</head><p>. These tests can be resolved using syntactic information and understanding of the pronoun (Figure <ref type="figure" target="#fig_0">1</ref>; Type 2). We expect systems to do well on such cases because both semantic and syntactic cues help disambiguation.</p><p>Evaluation To evaluate models, we split the data in two sections: one where correct coreference decisions require linking a gendered pronoun to an occupation stereotypically associated with the gender of the pronoun and one that requires linking to the anti-stereotypical occupation. We say that a model passes the WinoBias test if for both Type 1 and Type 2 examples, prostereotyped and anti-stereotyped co-reference decisions are made with the same accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Gender Bias in Co-reference</head><p>In this section, we highlight two sources of gender bias in co-reference systems that can cause them to fail WinoBias: training data and auxiliary resources and propose strategies to mitigate them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Data Bias</head><p>Bias in OntoNotes 5.0 Resources supporting the training of co-reference systems have severe gender imbalance. In general, entities that have a mention headed by gendered pronouns (e.g."he", "she") are over 80% male.<ref type="foot" target="#foot_4">5</ref> Furthermore, the way in which such entities are referred to, varies significantly. Male gendered mentions are more than twice as likely to contain a job title as female mentions. <ref type="foot" target="#foot_5">6</ref> Moreover, these trends hold across genres.</p><p>Gender Swapping To remove such bias, we construct an additional training corpus where all male entities are swapped for female entities and vice-versa. Methods can then be trained on both original and swapped corpora. This approach maintains non-gender-revealing correlations while eliminating correlations between gender and coreference cues.</p><p>We adopt a simple rule based approach for gender swapping. First, we anonymize named entities using an automatic named entity finder <ref type="bibr" target="#b14">(Lample et al., 2016)</ref>. Named entities are replaced consistently within document (i.e. "Barak Obama ... Obama was re-elected." would be annoymized to "E1 E2 ... E2 was re-elected." ). Then we build a dictionary of gendered terms and their realization as the opposite gender by asking workers on Amazon Mechnical Turk to annotate all unique spans in the OntoNotes development set. <ref type="foot" target="#foot_6">7</ref>Rules were then mined by computing the word difference between initial and edited spans. Common rules included "she → he", "Mr." → "Mrs.", "mother" → "father." Sometimes the same initial word was edited to multiple different phrases: these were resolved by taking the most frequent phrase, with the exception of "her → him" and "her → his" which were resolved using part-ofspeech. Rules were applied to all matching tokens in the OntoNotes. We maintain anonymization so that cases like "John went to his house" can be accurately swapped to "E1 went to her house."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Resource Bias</head><p>Word Embeddings Word embeddings are widely used in NLP applications however recent work has shown that they are severely biased: "man" tends to be closer to "programmer" than "woman" <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Current state-of-art co-reference systems build on word embeddings and risk inheriting their bias. To reduce bias from this resource, we replace GloVe embeddings with debiased vectors <ref type="bibr" target="#b1">(Bolukbasi et al., 2016)</ref>.</p><p>Gender Lists While current neural approaches rely heavily on pre-trained word embeddings, previous feature rich and rule-based approaches rely on corpus based gender statistics mined from external resources <ref type="bibr" target="#b0">(Bergsma and Lin, 2006)</ref>. Such lists were generated from large unlabeled corpora using heuristic data mining methods. These resources provide counts for how often a noun phrase is observed in a male, female, neutral, and plural context. To reduce this bias, we balance male and female counts for all noun phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In this section we evaluate of three representative systems: rule based, Rule, (Raghunathan et al., 2010), feature-rich, Feature, <ref type="bibr" target="#b6">(Durrett and Klein, 2013)</ref>, and end-to-end neural (the current state-ofthe-art), E2E, <ref type="bibr" target="#b15">(Lee et al., 2017)</ref>. The following sections show that performance on WinoBias reveals gender bias in all systems, that our methods remove such bias, and that systems are less biased on OntoNotes data. rately in pro-stereotyped and anti-stereotyped conditions ( T1-p vs. T1-a, T2-p vs T2-a). We evaluate the effect of named-entity anonymization (Anon.), debiasing supporting resources 8 (Resour.) and using data-augmentation through gender swapping (Aug.). E2E and Feature were retrained in each condition using default hyperparameters while Rule was not debiased because it is untrainable. We evaluate using the coreference scorer v8.01 <ref type="bibr" target="#b19">(Pradhan et al., 2014)</ref> and compute the average (Avg) and absolute difference (Diff) between pro-stereotyped and antistereotyped conditions in WinoBias. All initial systems demonstrate severe disparity between pro-stereotyped and anti-stereotyped conditions. Overall, the rule based system is most biased, followed by the neural approach and feature rich approach. Across all conditions, anonymization impacts E2E the most, while all other debiasing methods result in insignificant loss 8 Word embeddings for E2E and gender lists for Feature in performance on the OntoNotes dataset. Removing biased resources and data-augmentation reduce bias independently and more so in combination, allowing both E2E and Feature to pass WinoBias without significantly impacting performance on either OntoNotes or WinoBias . Qualitatively, the neural system is easiest to de-bias and our approaches could be applied to future end-toend systems. Systems were evaluated once on test sets, Table <ref type="table" target="#tab_3">3</ref>, supporting our conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WinoBias Reveals Gender Bias</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Systems Demonstrate Less Bias on OntoNotes</head><p>While we have demonstrated co-reference systems have severe bias as measured in WinoBias , this is an out-of-domain test for systems trained on OntoNotes. Evaluating directly within OntoNotes is challenging because sub-sampling documents with more female entities would leave very few evaluation data points. Instead, we apply our gender swapping system (Section 3), to the OntoNotes development set and compare system performance between swapped and unswapped data.<ref type="foot" target="#foot_7">9</ref> If a system shows significant difference between original and gender-reversed conditions, then we would consider it gender biased on OntoNotes data.</p><p>Table <ref type="table" target="#tab_4">4</ref> summarizes our results. The E2E sys-tem does not demonstrate significant degradation in performance, while Feature loses roughly 1.0-F1. 10 This demonstrates that given sufficient alternative signal, systems often do ignore gender biased cues. On the other hand, WinoBias provides an analysis of system bias in an adversarial setup, showing, when examples are challenging, systems are likely to make gender biased predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Machine learning methods are designed to generalize from observation but if algorithms inadvertently learn to make predictions based on stereotyped associations they risk amplifying existing social problems. Several problematic instances have been demonstrated, for example, word embeddings can encode sexist stereotypes <ref type="bibr" target="#b1">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b3">Caliskan et al., 2017)</ref>. Similar observations have been made in vision and language models <ref type="bibr" target="#b25">(Zhao et al., 2017)</ref>, online news <ref type="bibr" target="#b21">(Ross and Carter, 2011)</ref>, web search <ref type="bibr" target="#b12">(Kay et al., 2015)</ref> and advertisements <ref type="bibr" target="#b23">(Sweeney, 2013)</ref>. In our work, we add a unique focus on co-reference, and propose simple general purpose methods for reducing bias. Implicit human bias can come from imbalanced datasets. When making decisions on such datasets, it is usual that under-represented samples in the data are neglected since they do not influence the overall accuracy as much. For binary classification <ref type="bibr" target="#b10">Kamishima et al. (2012</ref><ref type="bibr" target="#b11">Kamishima et al. ( , 2011) )</ref> add a regularization term to their objective that penalizes biased predictions. Various other approaches have been proposed to produce "fair" classifiers <ref type="bibr" target="#b2">(Calders et al., 2009;</ref><ref type="bibr" target="#b7">Feldman et al., 2015;</ref><ref type="bibr" target="#b16">Misra et al., 2016)</ref>. For structured prediction, the work of <ref type="bibr" target="#b25">Zhao et al. (2017)</ref> reduces bias by using corpus level constraints, but is only practical for models with specialized structure. <ref type="bibr" target="#b13">Kusner et al. (2017)</ref> propose the method based on causal inference to achieve the model fairness where they do the data augmentation under specific cases, however, to the best of our knowledge, we are the first to propose data augmentation based on gender swapping in order to reduce gender bias.</p><p>Concurrent work <ref type="bibr" target="#b22">(Rudinger et al., 2018</ref>) also studied gender bias in coreference resolution systems, and created a similar job title based, winograd-style, co-reference dataset to demon- 10 We do not evaluate the Rule system as it cannot be train for anonymized input. strate bias<ref type="foot" target="#foot_8">11</ref> . Their work corroborates our findings of bias and expands the set of systems shown to be biased while we add a focus on debiasing methods. Future work can evaluate on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Bias in NLP systems has the potential to not only mimic but also amplify stereotypes in society. For a prototypical problem, coreference, we provide a method for detecting such bias and show that three systems are significantly gender biased. We also provide evidence that systems, given sufficient cues, can ignore their bias. Finally, we present general purpose methods for making coreference models more robust to spurious, genderbiased cues while not incurring significant penalties on their performance on benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Pairs of gender balanced co-reference tests in the WinoBias dataset. Male and female entities are marked in solid blue and dashed orange, respectively. For each example, the gender of the pronominal reference is irrelevant for the co-reference decision.Systems must be able to make correct linking predictions in pro-stereotypical scenarios (solid purple lines) and anti-stereotypical scenarios (dashed purple lines) equally well to pass the test. Importantly, stereotypical occupations are considered based on US Department of Labor statistics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Table2summarizes development set evaluations using all three systems. Systems were evaluated on both types of sentences in WinoBias (T1 and T2), sepa-F1 on OntoNotes and WinoBias development set. WinoBias results are split between Type-1 and Type-2 and in pro/anti-stereotypical conditions. * indicates the difference between pro/anti stereotypical conditions is significant (p &lt; .05) under an approximate randomized test<ref type="bibr" target="#b8">(Graham et al., 2014)</ref>. Our methods eliminate the difference between pro-stereotypical and anti-stereotypical conditions (Diff), with little loss in performance (OntoNotes and Avg).</figDesc><table><row><cell>Method E2E E2E E2E E2E E2E</cell><cell>Anon. Resour. Aug.</cell><cell>OntoNotes 67.7 66.4 66.5 66.2 66.3</cell><cell cols="3">T1-p T1-a Avg | Diff | 76.0 49.4 62.7 26.6* 73.5 51.2 62.6 21.3* 67.2 59.3 63.2 7.9* 65.1 59.2 62.2 5.9* 63.9 62.8 63.4 1.1</cell><cell cols="4">T2-p T2-a Avg | Diff | 88.7 75.2 82.0 13.5* 86.3 70.3 78.3 16.1* 81.4 82.3 81.9 0.9 86.5 83.7 85.1 2.8* 81.3 83.4 82.4 2.1</cell></row><row><cell>Feature Feature Feature Feature Feature</cell><cell></cell><cell>61.7 61.3 61.2 61.0 61.0</cell><cell>66.7 65.9 61.8 65.0 62.3</cell><cell>56.0 61.4 56.8 61.3 62.0 61.9 57.3 61.2 60.4 61.4</cell><cell>10.6* 9.1* 0.2 7.7* 1.9*</cell><cell>73.0 72.0 67.1 72.8 71.1</cell><cell cols="2">57.4 58.5 63.5 63.2 68.0 65.2 65.3 65.3 68.6 69.9</cell><cell>15.7* 13.5* 3.6 9.6* 2.5</cell></row><row><cell>Rule</cell><cell></cell><cell>57.0</cell><cell>76.7</cell><cell>37.5 57.1</cell><cell>39.2*</cell><cell>50.5</cell><cell>29.2</cell><cell>39.9</cell><cell>21.3*</cell></row><row><cell>Method E2E E2E</cell><cell>Anon. Resour. Aug.</cell><cell>OntoNotes 67.2 66.5</cell><cell cols="3">T1-p T1-a Avg | Diff | 74.9 47.7 61.3 27.2* 62.4 60.3 61.3 2.1</cell><cell cols="4">T2-p T2-a Avg | Diff | 88.6 77.3 82.9 11.3* 78.4 78.0 78.2 0.4</cell></row><row><cell>Feature Feature</cell><cell></cell><cell>64.0 63.6</cell><cell>62.9 62.2</cell><cell>58.3 60.6 60.6 61.4</cell><cell>4.6* 1.7</cell><cell>68.5 70.0</cell><cell cols="2">57.8 63.1 69.5 69.7</cell><cell>10.7* 0.6</cell></row><row><cell>Rule</cell><cell></cell><cell>58.7</cell><cell>72.0</cell><cell>37.5 54.8</cell><cell>34.5*</cell><cell>47.8</cell><cell cols="2">26.6 37.2</cell><cell>21.2*</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>F1 on OntoNotes and Winobias test sets. Methods were run once, supporting development set conclusions.</figDesc><table><row><cell>Model E2E Feature</cell><cell>Original Gender-reversed 66.4 65.9 61.3 60.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance on the original and the genderreversed developments dataset (anonymized).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Note that the counter argument (i.e., systems are gender bias free) may not hold.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">The corpus is used in CoNLL-2011 and CoNLL-2012 shared tasks, http://www.conll.org/previous-tasks</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2">Labor Force Statistics from the Current Population Survey, 2017. https://www.bls.gov/cps/cpsaat11.htm</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3">We do not claim this set of templates is complete, but that they provide representative examples that, pratically, show bias in existing systems.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4">To exclude mentions such as "his mother", we use Collins head finder<ref type="bibr" target="#b5">(Collins, 2003)</ref> to identify the head word of each mention, and only consider the mentions whose head word is gender pronoun.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5">We pick more than 900 job titles from a gazetteer.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6">Five turkers were presented with anonymized spans and asked to mark if it indicated male, female, or neither, and if male or female, rewrite it so it refers to the other gender.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7">This test provides a lower bound on OntoNotes bias because some mistakes can result from errors introduce by the gender swapping system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_8">Their dataset also includes gender neutral pronouns and examples containing one job title instead of two.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported in part by National Science Foundation Grant IIS-1760523, two NVIDIA GPU Grants, and a Google Faculty Research Award. We would like to thank Luke Zettlemoyer, Eunsol Choi, and Mohit Iyyer for helpful discussion and feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bootstrapping path-based pronoun resolution</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Building classifiers with independency constraints</title>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data mining workshops, 2009. ICDMW&apos;09. IEEE international conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantics derived automatically from language corpora contain human-like biases</title>
		<author>
			<persName><forename type="first">Aylin</forename><surname>Caliskan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joanna</forename><forename type="middle">J</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">356</biblScope>
			<biblScope unit="issue">6334</biblScope>
			<biblScope unit="page" from="183" to="186" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods on Natural Language Processing</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Easy victories and uphill battles in coreference resolution</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
				<meeting>the Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Randomized significance tests in machine translation</title>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitika</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WMT@ ACL</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="266" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Anaphora in natural language understanding</title>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Hirst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<publisher>Berlin Springer Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fairness-aware classifier with prejudice remover regularizer. Machine Learning and Knowledge Discovery in Databases</title>
		<author>
			<persName><forename type="first">Toshihiro</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fairness-aware learning through regularization approach</title>
		<author>
			<persName><forename type="first">Toshihiro</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining Workshops (ICDMW), 2011 IEEE 11th International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="643" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unequal representation and gender stereotypes in image search results for occupations</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Matuszek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">A</forename><surname>Munson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Factors in Computing Systems</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3819" to="3828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4069" to="4079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<title level="m">Neural architectures for named entity recognition</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: Visual classifiers from noisy humancentric labels</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2930" to="2939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A joint framework for coreference resolution and mention head detection</title>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015a</date>
		</imprint>
	</monogr>
	<note>In CoNLL. page 10</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Solving hard coreference problems</title>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scoring coreference partitions of predicted mentions: A reference implementation</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Karthik Raghunathan, Heeyoung Lee, Sudarshan Rangarajan</title>
				<editor>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2014. 2010</date>
			<biblScope unit="page" from="492" to="501" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resolving complex cases of definite pronouns: The winograd schema challenge</title>
		<author>
			<persName><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="777" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<author>
			<persName><forename type="first">Karen</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Women and news: A long and winding road. Media</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1148" to="1165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Rudinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Naradowsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrimination in online ad delivery</title>
		<author>
			<persName><forename type="first">Latanya</forename><surname>Sweeney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Queue</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>El-Bachouti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>et al. 2012. Ontonotes release 5.0</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Men also like shopping: Reducing gender bias amplification using corpus-level constraints</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
