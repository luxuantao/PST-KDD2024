<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Ming</forename><surname>Yin</surname></persName>
							<email>yiming@gdut.edu.cn</email>
							<idno type="ORCID">0000-0002-7037-1048</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Guangdong Univer-sity of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sydney Business School</orgName>
								<orgName type="institution" key="instit1">The University</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junbin</forename><surname>Gao</surname></persName>
							<email>junbin.gao@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Guangdong Univer-sity of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">School of Computing, Engineering and Mathemat-ics</orgName>
								<orgName type="institution">Western Sydney University</orgName>
								<address>
									<postCode>2150</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Shengli</forename><surname>Xie</surname></persName>
							<email>shlxie@gdut.edu.cn</email>
							<idno type="ORCID">0000-0003-2041-5214</idno>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Guangdong Univer-sity of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sydney Business School</orgName>
								<orgName type="institution" key="instit1">The University</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Sydney Business School</orgName>
								<orgName type="institution" key="instit1">The University</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<address>
									<postCode>2006</postCode>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Member, IEEE</roleName><forename type="first">Yi</forename><surname>Guo</surname></persName>
							<email>y.guo@westernsydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Guangdong Univer-sity of Technology</orgName>
								<address>
									<postCode>510006</postCode>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C12750B6C7CB6FF180B5911EF1D41980</idno>
					<idno type="DOI">10.1109/TNNLS.2018.2851444</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T13:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Low-rank representation</term>
					<term>multiview clustering</term>
					<term>t-linear</term>
					<term>t-product</term>
					<term>tensor space</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ubiquitous information from multiple-view data, as well as the complementary information among different views, is usually beneficial for various tasks, for example, clustering, classification, denoising, and so on. Multiview subspace clustering is based on the fact that multiview data are generated from a latent subspace. To recover the underlying subspace structure, a successful approach adopted recently has been sparse and/or low-rank subspace clustering. Despite the fact that existing subspace clustering approaches may numerically handle multiview data, by exploring all possible pairwise correlation within views, high-order statistics that can only be captured by simultaneously utilizing all views are often overlooked. As a consequence, the clustering performance of the multiview data is compromised. To address this issue, in this paper, a novel multiview clustering method is proposed by using t-product in the third-order tensor space. First, we propose a novel tensor construction method to organize multiview tensorial data, to which the tensor-tensor product can be applied. Second, based on the circular convolution operation, multiview data can be effectively represented by a t-linear combination with sparse and low-rank penalty using "self-expressiveness." Our extensive experimental results on face, object, digital image, and text data demonstrate that the proposed method outperforms the state-ofthe-art methods for a range of criteria.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>W ITH the advance of information technology, multiple views of objects can be readily acquired in many real-world scenarios <ref type="bibr" target="#b40">[41]</ref> that include different kinds of features <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b51">[52]</ref>. In essence, most data sets are comprised of multiple feature sets or views. For instance, an object can be characterized by a color view and/or a shape view; an image can be depicted by different features, such as a color histogram, Fourier shape information, and so on. These multiview data provide more useful information in comparison with single-view data and boost clustering performance by integrating different views <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In general, multiview clustering <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b56">[57]</ref> is superior to single-view clustering, because it utilizes the complementary information of objects from different feature spaces.</p><p>However, a challenge arises when data from different views show large divergence or heterogeneousness <ref type="bibr" target="#b8">[9]</ref>. This will lead to view disagreement <ref type="bibr" target="#b47">[48]</ref> so as to distort a similarity matrix that is used to depict the samples within the same class. Specifically, the within-class samples across multiple views may show lower affinities than those within the same view but from different classes <ref type="bibr" target="#b8">[9]</ref>. In order to address this problem, a surge of methods in multiview learning have been proposed <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b55">[56]</ref>.</p><p>Tzortzis and Likas <ref type="bibr" target="#b41">[42]</ref> proposed to compute separate kernels on each view and then combined them with a kernelbased method to improve clustering. To better capture the viewwise relationships among data, in <ref type="bibr" target="#b44">[45]</ref>, a novel multiview learning model has been presented via a jointly structured sparsity-inducing norm. For exploiting the correlation consensus, a coregularized multiview spectral clustering <ref type="bibr" target="#b49">[50]</ref> is developed by using two coregularization schemes.</p><p>Liu et al. <ref type="bibr" target="#b27">[28]</ref> proposed a nonnegative matrix factorization (NMF)-based multiview clustering algorithm via seeking for a factorization that gives compatible clustering solutions across multiple views. By taking advantage of graph Laplacian matrices <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> in different views, the algorithm proposed in <ref type="bibr" target="#b4">[5]</ref> learns a common representation under the spectral clustering framework.</p><p>Though the clustering performances of the aforementioned methods are enhanced for multiview data, some useful prior information within data is often ignored, such as sparsity <ref type="bibr" target="#b9">[10]</ref> and low rank <ref type="bibr" target="#b26">[27]</ref>. To tackle this problem, a novel pairwise sparse subspace representation model for multiview clustering was proposed recently <ref type="bibr" target="#b55">[56]</ref>. Ding and Fu <ref type="bibr" target="#b7">[8]</ref> developed a robust multiview subspace learning (RMSL) algorithm by seeking a common low-rank linear projection to mitigate the semantic gap among different views. Xia et al. <ref type="bibr" target="#b50">[51]</ref> presented recovering a shared low-rank transition probability matrix, in favor of low-rank and sparse decomposition, and then input to the standard Markov chain method for clustering. To further mitigate the divergence between different views, Ding and Fu <ref type="bibr" target="#b8">[9]</ref> proposed an RMSL through dual-low-rank decompositions, which is expected to recover a lowdimensional view-invariant subspace for multiview data. In fact, this type of subspace learning approaches aims to achieve a latent subspace shared by multiple views, provided that the input views are drawn from this latent subspace.</p><p>In recent years, subspace clustering has attracted considerable attention in computer vision and machine learning communities due to its capability of clustering data efficiently <ref type="bibr" target="#b42">[43]</ref>. The underlying assumption is that observed data usually lie in/near some low-dimensional subspaces <ref type="bibr" target="#b34">[35]</ref>. By constructing a pairwise similarity graph, data clustering can be readily transformed into a graph partition problem <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. The success of subspace clustering is based on a block-diagonal solution that is achieved, given that the objective functions satisfy some enforced blockdiagonal conditions <ref type="bibr" target="#b31">[32]</ref>. Mathematically, the objective functions are designed as a reconstruction term with a different regularization, such as either 1 -minimization sparse subspace clustering (SSC) <ref type="bibr" target="#b9">[10]</ref>, rank minimization low-rank representation (LRR) <ref type="bibr" target="#b26">[27]</ref>, or 2 -regularization least square regression <ref type="bibr" target="#b31">[32]</ref>.</p><p>Although subspace learning shows a good performance in multiview clustering, they may not fully make use of the properties of multiview data. As discussed previously, most previous methods focus on capturing only the pairwise correlations between different views, rather than the higher order correlation <ref type="bibr" target="#b35">[36]</ref> underlying the multiview data. In fact, the real-world data are ubiquitously in multidimension, often referred to as tensors. Based on this observation, especially for multiview data, omitting correlations in original spatial structure cannot result in an optimal clustering performance generally. To address this issue, Zhang et al. <ref type="bibr" target="#b58">[59]</ref> proposed a low-rank tensor constrained multiview subspace clustering to explore the complementary information from multiple views. However, in fact, the work <ref type="bibr" target="#b58">[59]</ref> only used a tensor to merge the different representations for every view. As such, it fails to capture high-order correlations well since that model did not actually represent multiple view data as a tensor.</p><p>Recently, the t-product <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, one type of tensor-tensor products, was introduced to provide a matrixlike multiplication for the third-order tensors. The t-product shares many similar properties as the matrix product and it has become a better way of exploiting the intrinsic structure of the third-order or higher order tensors <ref type="bibr" target="#b15">[16]</ref>, against the traditional Kronecker product operator <ref type="bibr" target="#b19">[20]</ref>.</p><p>To perform subspace clustering on data with the secondorder tensor structure, i.e., images and multiview data, conventional methods usually unfold the data or map them to vectors. Thus, blindly vectorizing may cause the problem of "curse of dimensionality" and also damage the second-order structure within data, such as spatial information. In contrast, t-product provides a novel algebraic approach for convolution operation rather than scalar multiplication <ref type="bibr" target="#b17">[18]</ref>. Owing to this operator, a third-order tensor can be readily regarded as a "matrix" whose elements are n-tuples or tubes, such that the matrix data can be embedded into a vector-spacelike structure <ref type="bibr" target="#b15">[16]</ref>.</p><p>To exactly recover a low-rank third-order tensor corrupted by sparse errors, the recent work <ref type="bibr" target="#b29">[30]</ref> studied tensor robust principal component. Zhang and Aeron <ref type="bibr" target="#b59">[60]</ref> presented the problem of completion of multidimensional arrays under a novel tensor algebraic framework, where the third-order tensors are treated as linear operators over the set of second-order tensors. Moreover, to perform submodule clustering of multiway data, Piao et al. <ref type="bibr" target="#b36">[37]</ref> proposed a clustering method by sparse and low-rank representation using t-product. However, this method is not developed for multiview data, which favors the linear separability assumption over the complementary information of multiview data. In fact, we can reasonably treat multiview data as a third-order tensor by organizing all different views of an object together. Refer to Section IV-A for more details.</p><p>Motivated by the above observations, we propose a novel low-rank multiview clustering method by using t-product based on the circular convolution in this paper. The proposed method aims to capture within-view relationships among multiview data while respecting the featurewise effect of each data point. By some manipulations, we can naturally transform the multiple-view data of interest into a third-order tensor. In what follows, we apply the recent advance of the third-order tensor algebra tools <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b60">[61]</ref> to perform clustering or classification tasks. Specifically, each sample from different views (i.e., with D × k) is twisted into a thirdorder tensor with D ×1×k, and all samples are organized as a tensor with D × n × k. Then, the tensorial data are represented by the t-linear combination for data "self-expressiveness." The overview of our proposed method is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Our main contributions in this paper are summarized as follows.</p><p>1) How to combine heterogeneous features is an open problem and a big challenge. First, we present an innovative construction method by effectively organizing the multiview data set into the third-order tensorial data. As such, multiple views can be simultaneously exploited, rather than only set of pairwise information, by involving the underlying complementary information from different views and specific statistical properties of the heterogeneous features. 2) More importantly, to the best of our knowledge, it is the first attempt to propose a low-rank multiview clustering in the third-order tensor space via t-linear combination. To efficiently perform the linear combination in a self-expressive way, the t-product is adopted. Specifically, the multiview data are represented in a tlinear combination way, imposed by sparse and lowrank penalty using "self-expressiveness." As such, by the t-product, each representation matrix can also be kept self-expressiveness in the Fourier domain. Thus, highorder structural information among all views can be efficiently explored, and the latent subspace structure within data can also be revealed. 3) We perform the proposed approach on an extensive range of multiview databases, such as facial, object, digits, image, and text data, to verify the effectiveness of the algorithm. The remainder of this paper is organized as follows. Section II briefly reviews the related works. In Section III, we introduce some notations and definitions used throughout this paper. Section IV presents the proposed multiview clustering. In Section V, we provide the experimental results on evaluating clustering performance for several databases. Finally, Section VI concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Before presenting our proposed method, we briefly review the background to our proposed method, which includes sparse and low-rank subspace clustering and t-linear combination for the third-order tensors, followed by multiview clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse and Low-Rank Subspace Clustering</head><p>Sparse and low-rank information of the latent group structure have been utilized for subspace clustering successfully in recent years <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>. The underlying assumption is that data are drawn from a mixture of several lowdimensional subspaces approximately. Given a set of data points, each of them in a union of subspaces can be represented as a linear combination of points belonging to the same subspace via self-expressiveness. Specifically, for data, X = [x 1 , x 2 , . . . , x n ], x i ∈ R d sampled from a union of multiple subspaces M m=1 S m , where S 1 , S 2 ,…, S M are lowdimensional subspaces. The sparse and low-rank subspace clustering <ref type="bibr" target="#b62">[63]</ref> focuses on solving the following optimization problem: min</p><formula xml:id="formula_0">C,E C * + λ C 1 + βE 2,1 s.t. X = XC + E, C ≥ 0 (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where C is the representation matrix and E is the representation error. 2,1 used in ( <ref type="formula" target="#formula_0">1</ref>) is to cope with the gross loss across different data cases. λ and β are the penalty parameters balancing the low-rank constraint, the sparsity term, and the gross error term. In this model, both sparsity and lowest rank criteria, as well as a nonnegative constraint, are all imposed. By imposing low-rank criterion, the global structure of data X is better captured, while the sparsity criterion can further encourage the local structure of each data vector <ref type="bibr" target="#b62">[63]</ref>.</p><p>In general, there are two explanations for C based on this model. First, the (i j)th element of C, that is, c i j , reflects the "similarity" between the pair x i and x j . Hence, C is sometimes called the affinity matrix. Second, the i th column of C, that is, c i , reflects a "better" representation of x i , such that the desired pattern, say subspace structure, is more prominent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. t-Linear Combination for Third-Order Tensor</head><p>To efficiently capture the sample correlations in their original spatial structure, tensor as a suitable representation is widely used for multidimensional data, such as images, video, range data, and medical data, for example, CT and MRI <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Therefore, to perform subspace clustering on tensorial data, Fu et al. <ref type="bibr" target="#b10">[11]</ref> proposed a model of lowrank representation for an input tensor similar to the case of LRR. Specifically, the input tensor is factorized into a Tucker decomposition where the core tensor is the input itself, along with a factor matrix at each mode. Although many tensor decompositions <ref type="bibr" target="#b19">[20]</ref>, such as CANDECOMP/PARAFAC, Tucker, and higher order singular-value decomposition <ref type="bibr" target="#b23">[24]</ref>, facilitate linear algebra tools in a multilinear context, this extension cannot be understood well for third-order tensors. In other words, the traditional tensor methods are not directly applicable to the third-order tensors. Thus, to better capture the higher order correlation among data, especially for original  spatial structures, it is desirable that the third-order tensors can be manipulated, such as matrices using linear algebra tools. In order to address this problem, Kilmer et al. <ref type="bibr" target="#b18">[19]</ref> recently presented t-product to define a matrixlike multiplication for third-order tensors. Given a matrix with a size of m × n, one can twist it into a "page" and then form an m × 1 × n thirdorder tensor ("oriented matrices"), as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Note that an m × 1 × n third-order tensor is really a tensor rather than a matrix. In fact, the tensor with a size of m × 1 × n can be regarded as a vector of length m, where each element is an 1 × 1 × n tube fiber (called tube fiber as usual in the tensor literature). By using the t-product <ref type="bibr" target="#b18">[19]</ref>, one can multiply two tube fibers, and then, we can present "linear" combinations of oriented matrices <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, that is, the operation is defined by the t-linear combination, where the coefficients are not scalars, but tube fibers. The illustration is given in Fig. <ref type="figure" target="#fig_2">3</ref>, where * denotes the circular convolution. Under this definition, a tensor A with a size of m × 1 × n is represented as a combination of a tensor X (with a size of m × m × n) with B (a size of m × 1 × n). In fact, the t-product is a generalization of matrix multiplication for third-order tensors, which can also be extended to tensors of arbitrary order. The multiplication is based on a convolutionlike operation, which can be performed efficiently using the fast Fourier transform (FFT). For more details of t-product, please refer to <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Multiview Clustering</head><p>To sufficiently exploit the complementary information of objects among multiple views, a number of approaches have been proposed recently. In general, the existing methods for multiview clustering can be roughly grouped into three categories. The first class aims at seeking some shared representation via incorporating the information of different views, that is, it maximizes the mutual agreement on two distinct views of the data <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b57">[58]</ref>. For example, Kumar and Daumé III <ref type="bibr" target="#b20">[21]</ref> first proposed the cotraining spectral clustering algorithm for multiview data. Under the assumption that view data are generated by a mixture model, Bickel and Scheffer <ref type="bibr" target="#b1">[2]</ref> applied expectation-maximization in each view and then clustered the data into subsets with high probability. The second one is called ensemble clustering or late fusion <ref type="bibr" target="#b41">[42]</ref>.</p><p>The core idea behind the aforementioned methods is to utilize kernels that naturally correspond to each single view and integrate these kernels either linearly or nonlinearly to get a final grouping output <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Tzortzis and Likas <ref type="bibr" target="#b41">[42]</ref> proposed computing separate kernels on each view and then combining with a kernel-based method to improve clustering. A matrix factorization-based method is presented to group the clusters obtained from each view <ref type="bibr" target="#b12">[13]</ref>, which is termed subspace learning-based methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Based on the assumption that each input view is generated from a latent subspace, it focuses on achieving this latent subspace shared by multiple views. Recent works <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b31">[32]</ref> show that some useful prior knowledge, such as sparse or low-rank information, can help to capture the latent group structure to improve the clustering performance. In addition, Liu et al. <ref type="bibr" target="#b28">[29]</ref> proposed a tensor-based strategy to incorporate heterogeneous multiview data in the context of spectral clustering. However, this method cannot exploit multiple views sufficiently, since the tensor was constructed by simply stacking the similarity matrices associated with the different views.</p><p>Motivated by this observation, in this paper, we aim to take advantage of the higher order correlation underlying the multiview data in a third-order tensor space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. NOTATIONS AND DEFINITIONS</head><p>In this section, we introduce some notations and some relevant definitions. Throughout this paper, we utilize calligraphy letters for tensors, for example, A ∈ R n 1 ×n 2 ×n 3 , bold lowercase letters for vectors, for example, a, uppercase for matrices, for example, A, lowercase letters for entries, and for example, a, a i j denotes the (i, j )th entry of matrix A. a 1 = i |a i | and a 2 = (a T a) 1/2 are the 1 and 2 norms, respectively, where T is the transpose operation. The matrix Frobenius norm is defined as</p><formula xml:id="formula_2">A F = ( i j |a i j | 2 ) 1/2 .</formula><p>A * is the nuclear norm, defined as the sum of all singular values of A, which is the convex envelope of the rank operator. A 2,1 is the 2,1 -norm defined by A 2,1 = j ( i a 2 i j ) 1/2 . For more general tensor operations, please refer to the classic paper <ref type="bibr" target="#b19">[20]</ref>.</p><p>We also use MATLAB notation to denote the elements in tensors. Specifically, A(:, :, i ), A(:, i, :), and A(i, :, :) are represented by the i th frontal, lateral, and horizontal slices, respectively. A(:, i, j ), A(i, :, j ), and A(i, j, :) denote the mode-1, mode-2, and mode-3 fiber, respectively. We denote Â the discrete Fourier transform along mode-3 for a third-order tensor A, that is, Â = fft(A, [ ], 3) in the MATLAB notation.</p><p>Similarly, A can be computed by Â via ifft( Â, [ ], 3), that is, using inverse FFT. A (i) and Â(i) denote the i th frontal slice of A and Â, respectively. We give the following definitions similar to those in <ref type="bibr" target="#b61">[62]</ref>.</p><p>Definition 1 (Block Diagonal Operation (bdiag) <ref type="bibr" target="#b18">[19]</ref>): For A, its block-diagonal matrix is formed by its frontal slice with each block on diagonal bdiag</p><formula xml:id="formula_3">(A) = ⎡ ⎢ ⎢ ⎢ ⎣ A (1)</formula><p>A (2)  . . .</p><formula xml:id="formula_4">A (n 3 ) ⎤ ⎥ ⎥ ⎥ ⎦ . (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>Definition 2 [Block Circulant Operation (bcirc)]: For A, the block circulant matrix is defined as follows:</p><formula xml:id="formula_6">bcirc(A) = ⎡ ⎢ ⎢ ⎢ ⎣ A (1) A (n 3 ) • • • A (2) A (2)</formula><p>A (1)  • • • A (3)  . . . . . . . . . . . .</p><formula xml:id="formula_7">A (n 3 ) A (n 3 -1) • • • A (1) ⎤ ⎥ ⎥ ⎥ ⎦ . (<label>3</label></formula><formula xml:id="formula_8">)</formula><p>Definition 3 (Unfold and Fold Operations): Unfold and fold operations are defined as follows:</p><formula xml:id="formula_9">unfold(A) = ⎡ ⎢ ⎢ ⎢ ⎣ A (1)</formula><p>A (2)  . . .</p><formula xml:id="formula_10">A (n 3 ) ⎤ ⎥ ⎥ ⎥ ⎦ , fold(unfold(A)) = A. (<label>4</label></formula><formula xml:id="formula_11">)</formula><p>Definition 4 (t-Product): If A ∈ R n 1 ×n 2 ×n 3 and B ∈ R n 2 ×n 4 ×n 3 , then the t-product of A and B is defined by C ∈ R n 1 ×n 4 ×n 3 as follows:</p><formula xml:id="formula_12">C = A * B = fold(bcirc(A)unfold(B)).</formula><p>(</p><formula xml:id="formula_13">)<label>5</label></formula><p>In fact, the t-product * is also called the circular convolution operation <ref type="bibr" target="#b17">[18]</ref>. Note that a third-order tensor A can be seen as an n 1 × n 2 matrix with each entry as a tube lying in mode-3. Then, the t-product operation, analogous to matrixmatrix product, is a useful generalization of matrix multiplication for tensors <ref type="bibr" target="#b18">[19]</ref>, except that the circular convolution replaces the product operation. Note that the t-product reduces to the standard matrixmatrix product in the case of n 3 = 1. Moreover, due to its superiority in generalization of matrix multiplication, the t-product has been employed in the third-order or higher order tensors' analysis <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b60">[61]</ref>. To efficiently exploit the linear algebra for tensors with t-product operation, we also need the following.</p><p>Definition 5 (Tensor Multirank): The multirank of A ∈ R n 1 ×n 2 ×n 3 is a vector p ∈ R n 3 with the i th element equal to the rank of the i th frontal slice of Â.</p><p>Definition 6 (Tensor Nuclear Norm): The tensor nuclear norm (TNN), denoted by A TNN , is defined as the average of the nuclear norm of all the frontal slices of Â similar to work <ref type="bibr" target="#b29">[30]</ref>, that is, A TNN = (1/n 3 ) n 3 i=1 Â(:, :, i ) * . Note that our definition differs from that in <ref type="bibr" target="#b61">[62]</ref>. Our definition is more reasonable as it is consistent with the definition of TNN in the original domain.</p><p>Definition 7 (F1 Norm): The F1 norm of a tensor A is defined by A F 1 = i, j A(i, j, :) F . Definition 8 (FF1 Norm): The FF1 norm of a tensor A is defined by A F F1 = i A(i, :, :) F . Definition 9 (Frobenius Norm): The Frobenius norm of a tensor A is defined by</p><formula xml:id="formula_14">A F = ( i, j,k A(i, j, k) 2 ) 1/2 .</formula><p>IV. SUBSPACE CLUSTERING FOR MULTIVIEW DATA IN THE THIRD-ORDER TENSOR SPACE To efficiently incorporate the clustering results from different views, we first organize each data point by a third-order tensor with all views' information in Section IV-A. As a result, one can maximize the agreement on multiple distinct views while recognizing the complementary information contained in every view. Then, in Section IV-B, we propose a sparse and low-rank clustering method for multiview data in the third-order tensor space, followed by an optimization via the alternating direction method of multipliers (ADMM) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b48">[49]</ref> in Section IV-C. Subspace clustering for multiview data is performed through spectral clustering in Section IV-D. Next, convergence and computational complexity analysis of the proposed algorithm are discussed in Section IV-E. Finally, we discuss the relation between the similar methods and ours in Section IV-F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiview Data Represented by the Third-Order Tensor</head><p>Unlike the scenario of homogeneous multiview data, a tensor can be formed by stacking the object-by-feature matrices derived from multiple views <ref type="bibr" target="#b28">[29]</ref>. In the real world, many multiview applications actually deal with heterogeneous data, where the dimensions of various feature spaces are not the same. Thus, it is very challenging to construct tensorial data suitable for subsequent processing. In this paper, we first propose a novel tensorial data construction approach, such that heterogeneous multiview data can be efficiently exploited.</p><p>Given a multiview data set {X v ∈ R d v ×n } (v = 1, 2, . . . , k), totally k views, to integrate all views for the i th object (i = 1, 2, . . . , n), we build a block-diagonal matrix</p><formula xml:id="formula_15">M i ∈ R D×k , D = v d v ,</formula><p>where each of the view data is placed in the near diagonal position. The building process is shown in Fig. <ref type="figure" target="#fig_3">4</ref>, that is, the j th column of M i only consists of the j th normalized view from the i th sample. By organizing the process in this way, the set of {M i } n i=1 will lead to a union of different views while respecting each individual view data. Through using twist manipulation, the multiview data for the i th object is easily transformed into a third-order tensor space, that is, M i ∈ R D×1×k . Collecting all {M i } along the second mode, we can obtain a tensor X ∈ R D×n×k . As a consequence, the proposed clustering method can be effectively applied to this third-order tensor, such that the high-order correlation can be exploited by using all views simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sparse and Low-Rank Clustering in the Third-Order Tensor Space</head><p>Given multiview data X ∈ R D×n×k , it is crucial to find a method to effectively represent the data in a selfexpressive way for the clustering task. A number of studies in the literature use matrix data in order to discover the pairwise correlations between different views <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b55">[56]</ref>. To generalize the clustering methods for the matrix case to the one for third-order or higher order tensorial cases, Kernfeld et al. <ref type="bibr" target="#b15">[16]</ref> recently proposed a sparse submodule clustering method. However, this model cannot be applied to multiview data clustering directly due to the consensus principle in multiview data <ref type="bibr" target="#b51">[52]</ref>. In addition, the low-rank regularizer has been widely used in many works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b52">[53]</ref>- <ref type="bibr" target="#b54">[55]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Thus, in this section, we propose to seek the most sparse and lowest rank representation of multiview data by employing the self-expressive property, as shown in Fig. <ref type="figure">5</ref>. Mathematically, it can be formulated as follows:</p><formula xml:id="formula_16">min C αC F 1 + λC TNN + 1 2 X -X * C 2 F + β 2 1≤i, j ≤k,i = j C(:, :, i ) -C(:, :, j ) 2 F (<label>6</label></formula><formula xml:id="formula_17">)</formula><p>where C ∈ R n×n×k denotes the representation tensor utilized to induce the following "affinity" matrix. • F 1 and • TNN are the tensor sparse and nuclear norm, respectively, as defined in Section III. Based on these two norms, the first and second terms of the objective function aim to induce sparse and lowest rank coefficients.</p><p>Remark: In our model, the third term fits the representation errors in the third-order tensor space by using the t-linear combination, where we are motivated by two aspects. One aspect is that t-product based on circular convolution operation can reconstruct the data by using shifted versions of atoms of dictionary <ref type="bibr" target="#b18">[19]</ref>. For our multiview tensorial data, each column in every lateral slice of tensor has the shifting invariance property <ref type="bibr" target="#b16">[17]</ref>. This will benefit the t-linear combination to represent the third-order tensor. As referred to <ref type="bibr" target="#b29">[30]</ref>, the new tproduct is based on the concept of block circulant matrix, that is, a new matricization of tensor. Different from the previous matricization along certain dimensions, the block circulant Fig. <ref type="figure">5</ref>. Illustration of the proposed sparse and low-rank representation using the t-linear combination. matricization will preserve more spatial correlation within data. Furthermore, the proposed tensor model is distinct from the traditional models, owing to this precise operation.</p><p>Another aspect is that the t-linear combination is a generalization of the standard linear combination widely used in many works, which can elegantly generalize all classical algorithms from numerical linear algebra. As such, the number of required atoms of dictionary can be significantly reduced, which enhances the discriminative performance of the model via sparsity and also reduces the computational complexity.</p><p>Finally, the last term is imposed for multiview data in particular, which encourages consensus representation by forcing all the lowest rank coefficients close in all views. For ease of numeric implementation, we here employ the Frobenius norm rather than 1 norm. In summary, by combining the linear combination via t-product operation, • F 1 and • TNN regularizations, our model will encourage representation C(i, j, :) to have a block sparsity structure if the samples M i and M j are from different subspaces. This point is similar to the matrix case <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optimization via ADMM</head><p>Variable C appears in three terms in the objective function <ref type="bibr" target="#b5">(6)</ref>. To decouple them, we introduce two variables Y = C and Z = C. Then, we have the following problem, such that the standard ADMM <ref type="bibr" target="#b48">[49]</ref> can be efficiently applied to:</p><formula xml:id="formula_18">min C,Y,Z αY F 1 + λZ TNN + 1 2 X -X * C 2 F + β 2 1≤i, j ≤k,i = j C(:, :, i ) -C(:, :, j ) 2 F s.t., Y = C, Z = C. (<label>7</label></formula><formula xml:id="formula_19">)</formula><p>Its augmented Lagrangian formulation is formulated as follows:</p><formula xml:id="formula_20">argmin C,Y,Z αY F 1 + λZ TNN + 1 2 X -X * C 2 F + β 2 1≤i, j ≤k,i = j C(:, :, i ) -C(:, :, j ) 2 F + G 1 , Y -C + G 2 , Z -C + ρ 2 Y -C 2 F + Z -C 2 F (<label>8</label></formula><formula xml:id="formula_21">)</formula><p>where G 1 and G 2 are Lagrange multipliers, and •, • is the inner product defined by A, B = i tr(A (i)T B (i) ). ρ &gt; 0 is a penalty parameter. As convolution-multiplication properties, this problem can be computed efficiently in the Fourier domain. Then, the procedure of solving (8) with the ADMM is defined as follows. 1) Updating Z: According to Parseval's theorem <ref type="bibr" target="#b0">[1]</ref>, it is well known that the sum of the square of a function is equal to the sum of the square of its Fourier transform. Thus, to optimize efficiently, we have the following subproblem of Z via applying FFT:</p><formula xml:id="formula_22">argmin Z λZ TNN + ρ 2n 3 Ẑ -Ĉ + 1 ρ Ĝ2 2 F = λ n 3 n 3 i=1 Ẑ(:, :, i ) * + ρ 2n 3 Ẑ -Ĉ + 1 ρ Ĝ2 2 F . (<label>9</label></formula><formula xml:id="formula_23">)</formula><p>From the frontal side, for example, Ĉi = Ĉ(:, :, i ), Z can be optimized slice-by-slice, that is, the subproblem is equivalent to solving</p><formula xml:id="formula_24">min Ẑi λ Ẑi * + ρ 2 Ẑi -Ĉi + 1 ρ Ĝi 2 2 F (<label>10</label></formula><formula xml:id="formula_25">)</formula><p>which has a closed-form solution by using the singular value thresholding operator <ref type="bibr" target="#b3">[4]</ref>. After obtaining Ẑ, it is easy to compute Z t +1 i by using inverse FFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Updating Y by argmin</head><formula xml:id="formula_26">Y αY F 1 + G 1 , Y -C + ρ 2 Y -C 2 F . (<label>11</label></formula><formula xml:id="formula_27">)</formula><p>Similarly, Y can be efficiently solved from the thirdmode fiber-by-fiber, that is,</p><formula xml:id="formula_28">Y(i, j, :) t +1 = A(i, j, :) F -α ρ A(i, j, :) F A(i, j, :)<label>(12)</label></formula><p>where</p><formula xml:id="formula_29">A = C -1 ρ G 1 . 3) Updating C by argmin C 1 2 X -X * C 2 F + G 1 , Y -C + G 2 , Z -C + β 2 1≤i, j ≤k,i = j C(:, :, i ) -C(:, :, j ) 2 F + ρ 2 Y -C 2 F + Z -C 2 F . (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>By letting</p><formula xml:id="formula_31">P 1 = Z +(1/ρ)G 1 and P 2 = Y +(1/ρ)G 2 and</formula><p>applying FFT, we have the following equivalent problem similar to the process of subproblem Z:</p><formula xml:id="formula_32">argmin Ĉ,Q i 1 2 X -X Ĉ 2 F + β 2 i, j Ĉ(:, :, i )-Ĉ(:, :, j ) 2 F + ρ 2 Ĉ -P1 2 F + Ĉ -P2 2 F (<label>14</label></formula><formula xml:id="formula_33">)</formula><p>where denotes the pointwise multiplication, that is, X Ĉ is an array resulting from pointwise multiplication. Then, we can optimize the problem slice-by-slice from the frontal side, that is, argmin</p><formula xml:id="formula_34">Ĉ(i) 1 2 X(i) -X(i) Ĉ(i) 2 F + β 2 i, j,i = j Ĉ(i) -Ĉ( j ) 2 F + ρ 2 Ĉ(i) - P(i) 1 2 F + Ĉ(i) - P(i) 2 2 F . (<label>15</label></formula><formula xml:id="formula_35">)</formula><p>The subproblem ( <ref type="formula" target="#formula_34">15</ref>) is nonseparable with respect to Ĉ(i) . Therefore, an auxiliary variable, named Q i , is introduced, that is, argmin</p><formula xml:id="formula_36">Ĉ(i) ,Q i 1 2 X(i) -X(i) Ĉ(i) 2 F + β 2 i, j,i = j Q i -Ĉ( j ) 2 F + ρ 2 Ĉ(i) -P(i) 1 2 F + Ĉ(i) -P(i) 2 2 F s.t., Q i = Ĉ(i) . (<label>16</label></formula><formula xml:id="formula_37">)</formula><p>Next, the details for alternatively updating these two blocks are given. For simplicity, we just use one step to update for this subproblem. a) Update Ĉ(i) (i = 1, 2, . . . , k). Each Ĉ(i) can be updated independently by argmin</p><formula xml:id="formula_38">Ĉ(i) X(i) -X(i) Ĉ(i) 2 F + β i, j Q j -Ĉ(i) 2 F + ρ Ĉ(i) - P(i) 1 2 F + Ĉ(i) - P(i) 2 2 F + τ 2 Q j -Ĉ(i) + 1 τ W i 2 F (<label>17</label></formula><formula xml:id="formula_39">)</formula><p>where W i is the Lagrange multiplier and τ denotes a penalty parameter. Equivalently, argmin</p><formula xml:id="formula_40">Ĉ(i)</formula><p>ρ Ĉ(i) -P(i)</p><formula xml:id="formula_41">1 2 F + Ĉ(i) -P(i) 2 2 F + 1 2 β(k -1) Ĉ(i) - 1 k -1 j, j =i Q j 2 F + Xi -Xi Ĉ(i) 2 F + τ 2 Q i -Ĉ(i) + 1 τ W i 2 F . (<label>18</label></formula><formula xml:id="formula_42">)</formula><p>Taking derivation with respect to Ĉ(i) and letting it be zero, we have</p><formula xml:id="formula_43">((β(k -1) + τ + 2ρ)I + ( X(i) ) T X(i) )( Ĉ(i) ) * = β j, j =i Q j + τ Q i + 1 τ W i + ( X(i) ) T X(i) + ρ P(i) 1 + P(i) 2 (<label>19</label></formula><formula xml:id="formula_44">)</formula><p>where I is an identity matrix with suitable dimensions.</p><formula xml:id="formula_45">b) Update Q i (i = 1, 2, . . . , k) argmin Q i β 2 i, j =i Q i -Ĉ( j ) 2 F + τ 2 Q i -Ĉ(i) + 1 τ W i 2 F . (<label>20</label></formula><formula xml:id="formula_46">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS</head><p>Similarly, taking derivation with respect to Q i and letting it be zero, we have</p><formula xml:id="formula_47">(β(k -1) + τ )Q * i = β j, j =i Ĉ( j ) + τ Ĉ(i) -W i . (21) c) Update W i W i = W i + τ Q * i -( Ĉ(i) ) * . (<label>22</label></formula><formula xml:id="formula_48">) 4) Updating G 1 and G 2 G 1 = G 1 + μ(Y -C) G 2 = G 2 + μ(Z -C) ρ = min(ρ max , μρ). (<label>23</label></formula><formula xml:id="formula_49">)</formula><p>The whole procedure of the ADMM for solving ( <ref type="formula" target="#formula_20">8</ref>) is summarized in Algorithm 1. The stopping criterion is given by the following condition in the algorithm: max</p><formula xml:id="formula_50">⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 1 X F Z t +1 -C t +1 F , 1 X F Y t +1 -C t +1 F , 1 Z t F Z t +1 -Z t F , 1 Y t F Y t +1 -Y t F , 1 C t F C t +1 -C t F ⎫ ⎪ ⎪ ⎪ ⎪ ⎪ ⎬ ⎪ ⎪ ⎪ ⎪ ⎪ ⎭ ≤ ε. (<label>24</label></formula><formula xml:id="formula_51">)</formula><p>Algorithm 1 Solving Problem (8) via ADMM Input: X , λ, α and β.</p><formula xml:id="formula_52">Initialization: C 0 = Y 0 = Z 0 = 0, G 0 1 = G 0 2 = 0, ρ = τ = 0.01, μ = 1.9.</formula><p>While not converged (t = 0, 1, . . .) do 1) Update Z t +1 according to <ref type="bibr" target="#b9">(10)</ref>; 2) Update Y t +1 according to <ref type="bibr" target="#b11">(12)</ref>; 3) Update C t +1 according to (13); 4) Update G 1 , G 2 and ρ using (23); 5) Check convergence: If the condition defined by ( <ref type="formula" target="#formula_50">24</ref>) is satisfied, then break. End while Output: C *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Subspace Clustering for Multiview Data</head><p>As discussed earlier, C can be regarded as a new representation learned from multiview data. After solving problem <ref type="bibr" target="#b7">(8)</ref>, the next step is to segment C to find the final subspace clusters. For C, it contains k affinity matrices corresponding to each view from the frontal side. However, how to effectively combine these information is not a trivial issue. Considering the work of <ref type="bibr" target="#b50">[51]</ref>, here we similarly adopt the transition probability matrix to achieve the final cluster. Specifically, we first recover the latent transition probability matrix, utilizing C from all views, by a decomposition method. Then, the latent transition matrix will be used as an input to the standard Markov chain method to separate the data into clusters <ref type="bibr" target="#b50">[51]</ref>. For computational complexity, we are in favor of 2,1 norm rather than nuclear norm on optimizing the transition matrix. We call this algorithm subspace clustering for multiview data in the third-order tensor space, namely SCMV-3DT, and it is outlined in Algorithm 2. Steps: <ref type="formula" target="#formula_20">8</ref>) by ADMM explained in Section IV-C, and obtain the optimal solution C * . 3) Similar to the work of <ref type="bibr" target="#b50">[51]</ref>, compute the latent transition probability matrix by C * , and input to the standard Markov chain method to separate the data. Output: Cluster labels of each sample.</p><formula xml:id="formula_53">1) Organize {X v ∈ R d v ×n } into tensorial data X ∈ R D×n×k (D = v d v ). 2) Solve (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Convergence and Complexity Analysis</head><p>As problem ( <ref type="formula" target="#formula_16">6</ref>) is convex, the algorithm via ADMM is guaranteed to converge at the rate of O(1/t 1 ) <ref type="bibr" target="#b48">[49]</ref>, where t 1 is the number of iterations. Furthermore, the recent work <ref type="bibr" target="#b30">[31]</ref> gives more detail explaining how to guarantee the convergence of the proposed algorithm. Please refer to <ref type="bibr" target="#b30">[31]</ref>.</p><p>The proposed algorithm consists of three steps involved in iteratively updating Z, Y, and C until the convergence condition is met. The time complexity for each update is listed in Table <ref type="table" target="#tab_0">I</ref>. From the table, we can see how our algorithm is related to the size of multiview data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>The basic idea of our model is inspired by several prior works, such as nonnegative low-rank and sparse (NNLRS) model <ref type="bibr" target="#b62">[63]</ref> and low-rank tensor-constrained multiview subspace clustering (LT-MSC) <ref type="bibr" target="#b58">[59]</ref>. However, this paper is by no means a simple extension to those works. As for NNLRS <ref type="bibr" target="#b62">[63]</ref>, it does not consider the multiview clustering task and cannot be applicable to this type of task as well. In addition, the NNLRS mainly focuses on matrix data for semisupervised learning. Furthermore, our proposed method is also distinct from LT-MSC <ref type="bibr" target="#b58">[59]</ref>, since the latter only considers merging of the different representations for every view by tensorial data. Although there are a lot of tensor models, such as the latest TenSR <ref type="bibr" target="#b37">[38]</ref>, this paper exploits the t-product operation, which is based on the block circulant matricization of tensor. This is different from the previous matricization along certain dimensions, and will preserve more spatial correlation within data. As such, the proposed method will benefit from some novel properties of the third-order tensor via t-product <ref type="bibr" target="#b16">[17]</ref>, for example, circular convolution. By virtue of this property, the shifting invariance structure within the multiview data is preserved. This is significantly different from existing multiview clustering methods.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>In order to evaluate the clustering performance, in this section, several experiments are conducted using our proposed approach, which we compare with the stateof-the-art methods. The MATLAB codes of our algorithm implementation can be downloaded at http://www. scholat.com/portaldownloadFile.html?fileId=4623.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data Sets</head><p>Four real-world data sets are used to test multiview data clustering, the statistics for which are summarized in Table <ref type="table" target="#tab_0">II</ref>. The test databases involved are facial, object, digits, image, and text data. Some examples of data sets are shown in Fig. <ref type="figure" target="#fig_5">6</ref>.</p><p>UCI digits is a data set of handwritten digits of 0-9 from the UCI machine learning repository. 1 It is composed of 2000 data points. In our experiments, six published feature sets are utilized to evaluate a clustering performance, including 76 Fourier coefficients of the character shapes (FOU), 216 profile correlations (FAC), 240 pixel averages in 2 × 3 1 https://archive.ics.uci.edu/ml/datasets/Multiple+Features Caltech 101 is an image data set that consists of 101 categories of images for object recognition problem. We choose a subset of Caltech 101, called Caltech7, which contains 1474 images of 7 classes, that is, Face, Motorbikes, Dolla-Bill, Garfield, Snoopy, Stop-Sign, and Windsor-Chair. Six patterns were extracted from all the images, such as Gabor features in dimension 48 <ref type="bibr" target="#b22">[23]</ref>, wavelet moments of dimension 40, CENTRIST features of dimension 254, histogram of oriented gradients (HoG) features of dimension 1984 <ref type="bibr" target="#b6">[7]</ref>, GIST features of dimension 512 <ref type="bibr" target="#b33">[34]</ref>, and local binary pattern (LBP) features of dimension 928 <ref type="bibr" target="#b32">[33]</ref>, as shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>BBCSport consists of news article data. <ref type="foot" target="#foot_1">2</ref> We selected 544 documents from the BBC Sport website corresponding to sports news articles in five topical areas from 2004 to 2005. It contains five class labels, such as athletics, cricket, football, rugby, and tennis.</p><p>The ORL face data set consists of 40 distinct subjects with 10 different images for each. The images are taken at different times with changing lighting conditions, facial expressions, and facial details for some subjects. Three types of features, that is, intensity, LBP features <ref type="bibr" target="#b32">[33]</ref>, and Gabor features <ref type="bibr" target="#b22">[23]</ref>, are extracted and utilized for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Measure Metric</head><p>To evaluate all the approaches in terms of clustering, we here adopt precision, recall, F-score, normalized mutual information (NMI), and adjusted rand index <ref type="bibr" target="#b14">[15]</ref>, as well as clustering accuracy (ACC). For all these criteria, a higher value means a better clustering quality. As each measure penalizes or favors different properties in the clustering, we report results on all the measures for a comprehensive evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Compared Methods</head><p>Next, we will compare the proposed method with the following state-of-the-art algorithms, for which there are public code available. <ref type="foot" target="#foot_2">3</ref>1) Single View: Using the most informative view, that is, one that achieves the best clustering performance using  the graph Laplacian derived from a single view of the data and performing spectral clustering <ref type="bibr" target="#b5">[6]</ref> on it. 2) Feature Concatenation: Combining the features of each view one-by-one and then conducting spectral clustering, as usual, directly on this concatenated feature representation. 3) Kernel Addition: First building a kernel matrix (affinity matrix) from every feature and then averaging these matrices to achieve a single kernel matrix input to spectral clustering. 4) L R R + FC and SSC + FC: Combining the features of each view one-by-one and then applying LRR or SSC to subspace clustering. 5) Centroid-Based Coregularized Spectral Clustering (CCo-reguSC): Adopting a centroid-based coregularization term to spectral clustering via Gaussian kernel <ref type="bibr" target="#b21">[22]</ref>. The parameter for each view is set to be 0.01 as suggested. 6) Pairwise-Based Coregularized Spectral Clustering (PCo-reguSC): Adopting a pairwise-based coregularization term to spectral clustering via a Gaussian kernel <ref type="bibr" target="#b21">[22]</ref>. The parameter for each view is set to be 0.01 as suggested. 7) Multiview NMF (MultiNMF) <ref type="bibr" target="#b27">[28]</ref>: In our experiments, we empirically set parameter (λ v ) to 0.01 for all views and data sets as the authors advised. 8) Robust Multiview Spectral Clustering via Low-Rank and Sparse Decomposition (LRSD-MSC) <ref type="bibr" target="#b50">[51]</ref>: This approach recovers a shared low-rank transition probability matrix for multiview clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9) LT-MSC [59]:</head><p>The method proposes multiview clustering by considering the subspace representation matrices of different views as a tensor. In our experiments, k-means is utilized for all methods at the final step to obtain the clustering results. As k-means relies on initialization, we run k-means 20 trials and present the means and standard deviations of the performance measures. Specifically, we rerun the above-mentioned methods using the suggested parameters by us.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Evaluation</head><p>In this section, we report the clustering results on the choosen test data sets. In order to handle the different scales of heterogeneous views, normalization is first applied to adjusting the values to the same level. In Tables III-VI, the clustering performance of different methods on the test data sets is given. The bold numbers highlight the best results. The parameters are set for all the methods for comparison according to authors' suggestions for their best clustering scores. For the proposed algorithm, we empirically set the parameters and report the results, that is, λ = 10 -3 , α = 0.1, and β = 1.1, throughout all experiments. As can be seen, our proposed method significantly outperforms those of the comparisons on all criteria, for all types of data including facial image, object image, digits image, and text data. Particularly, for BBCSport, our method outperforms the second best algorithm in terms of ACC/NMI by 19.29% and 16.23%, respectively, while for UCI, the leading margins are 10.43% and 4.76%, respectively, in terms of ACC/NMI.   LT-MSC achieves the second best result among most cases, especially for the facial image data ORL. This is claimed in <ref type="bibr" target="#b58">[59]</ref> and verified in our experiments, while for LRSD-MSC and MultiNMF, they achieved a comparable performance. As for LRR or SSC on the concatenated features, it is observed that they outperformed other methods by simply using each view, except for BBCSport. For text data, such as BBCSport, kernel addition can produce a better clustering result than other baselines. It is expected that the different multiview clustering methods may suit varied data. Nevertheless, the proposed method is more suitable and robust for all kinds of multiview data.</p><p>Next, we further analyze the underlying reasons why the proposed method is superior intuitively. Fig. <ref type="figure" target="#fig_7">8</ref> shows the affinity matrices for UCI digits obtained by several methods. From Fig. <ref type="figure" target="#fig_7">8</ref>, we observe that the subspace within data is well recovered by different methods. Among them, the affinity matrix by our method can better reflect the structure of data, that is, block-diagonal structure, so as to benefit the subsequent clustering task. On the other hand, the affinity matrix by other methods is somewhat unsatisfactory for clustering.</p><p>Furthermore, to show the advantage of combining multiview features, we choose a part of views of UCI data to form a subset, termed UCI-2view, which includes 76 Fourier coefficients and 240 pixels. The clustering result is shown in Table <ref type="table" target="#tab_5">VII</ref>. Apparently, the performance degrades when the number of views becomes less compared with Table <ref type="table" target="#tab_1">III</ref>. This verifies that the complementary information is indeed beneficial. In other words, multiview can be employed to comprehensively and accurately describe the data wherever possible <ref type="bibr" target="#b39">[40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Parameter Sensitivity</head><p>In our algorithm, there are three regularization parameters balancing the effect of sparse term, low-rank term, representation term, and consensus terms. In the following, we study the influence of parameters λ, α, and β on BBCSport in terms of ACC and NMI by setting them to different values,  e.g., [10 -5 , 10 -4 , . . . , 1, 10]. We vary a parameter at a time while keeping others fixed. Fig. <ref type="figure" target="#fig_8">9</ref> shows the ACC and NMI on BBCSport, respectively, varying the log value of different parameters. From Fig. <ref type="figure" target="#fig_8">9</ref>, we can see that the proposed method outperforms other algorithms over a large range of parameter values. Especially, when α = 0.1, the best performance can be achieved for a large range of λ. Furthermore, it is noticed that the performance of our algorithm is robust to the choice of parameter β, though the performance decreases rapidly when β becomes too large. Based on the above observations, we conclude that our method is relatively insensitive to its parameters as long as the parameters are in a suitable range. Thus, we choose λ = 10 -3 , α = 0.1, and β = 1.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Convergence</head><p>The algorithm is convergent in theory <ref type="bibr" target="#b25">[26]</ref>, and hence, we numerically demonstrate the convergence speed by showing the objective values versus iteration steps in Fig. <ref type="figure" target="#fig_9">10</ref>. The stopping criterion for the proposed algorithm is defined by <ref type="bibr" target="#b23">(24)</ref>. If it is satisfied, then the algorithm outputs the tensor coefficients. On the other hand, as the convexity of the objective function, the convergence of our algorithm is guaranteed for all variable sequences. Due to the limitation of space, we only report the results of BBCSport and ORL data sets. From Fig. <ref type="figure" target="#fig_9">10</ref>, we can see that the algorithm approaches to converge quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Discussion of the Scalability of the Algorithm</head><p>Recently, some works have been developed to address the scalability issue in multiview clustering <ref type="bibr" target="#b24">[25]</ref>. However, for the proposed method, it cannot be directly applied to very large data sets. First, in this paper, we focus on the effectiveness of the proposed model by examining the performance on small-and medium-sized data comparing the state-of-theart methods under the same settings. Second, the processing of large-scale data depends on the computational platform, such as MapReduce and Hadoop. Nevertheless, there are two directions worth pursuing, that is, parallelization and random sampling <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b54">[55]</ref>. As the extension to "big data" is not trivial and it requires a large amount of work, this is to be considered in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we proposed a novel approach for low-rank multiview subspace clustering over the third-order tensor data. By using t-product based on circular convolution, the multiview tensorial data are reconstructed by itself with sparse and low-rank penalty. The proposed method not only takes advantage of the complementary information from multiview data, but it also exploits the multiorder correlation consensus. Based on this, spectral clustering via a Markov chain is subsequently applied to the final separation. The extensive experiments, on several multiview data, are conducted to validate the effectiveness of our approach and demonstrate its superiority against the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of the proposed framework. (a) Using the third-order tensorial data to represent a multiview data set. Each lateral slice of the tensor is formed by all views from one object, where each of the view data is arranged at the diagonal position one by one. (b) By the t-linear combination, the third-order tensorial data are reconstructed by itself with sparse and low-rank penalty in a self-expressive way. (c) Based on the learned tensor coefficients, a data similarity matrix is built for multiviews, by which spectral clustering is applied to the final separation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Operation between the matrix and the third-order tensor. An m × n matrix (left) can be transformed into a tensor (right) with m × 1 × n by the twist operation and vice versa by the squeeze operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Illustration of a third-order tensor A is a result of a linear combination of basis X with coefficients B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Illustration of the third-order tensor is built from different views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2</head><label>2</label><figDesc>Subspace Clustering for Multiview Data in Third-Order Tensor Space via t-Linear Combination Input: Data matrix of all views {X v ∈ R d v ×n } (v = 1, 2, . . . , k), Number of classes K , parameter λ, α and β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Samples of data sets. (a) Caltech-7. (b) ORL. (c) UCI digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Features extracted from Motorbike of Caltech-7. From left to right, intensity, ColorMoment, LBP, HoG, CENTRIST, and GIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Affinity matrix information of UCI digits obtained by different methods (best seen in larger resolution on monitor). (a) PCo-reguSC. (b) CCo-reguSC. (c) MultiNMF. (d) LRSD-MSC. (e) LT-MSC. (f) SCMV-3DT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Performance of our algorithm on the BBCSport database with different parameter settings (log value). (a) ACC (left) and NMI (right) varying λ and α with β = 1.1. (b) Performance varying with β fixing λ = 0.01 and α = 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Convergence curve of the proposed method. (a) BBCSport. (b) ORL.Note that, for ORL, at early iterations, the variables are not feasible, despite their lower costs. When it converges, the variables are feasible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TIME</head><label>I</label><figDesc>COMPLEXITY ANALYSIS OF THE PROPOSED ALGORITHM, WHERE t 2 IS THE ITERATION TIMES FOR SOLVING SUBPROBLEM (13) AND r IS THE LOWEST RANK FOR Z THAT CAN BE OBTAINED BY OUR ALGORITHM</figDesc><table><row><cell>TABLE II</cell></row><row><cell>DESCRIPTION OF THE TEST DATA SETS</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE III CLUSTERING</head><label>III</label><figDesc>RESULTS ON THE UCI DATABASE (MEAN ± STANDARD DEVIATION)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE IV CLUSTERING</head><label>IV</label><figDesc>RESULTS ON THE CALTECH-7 DATABASE (MEAN ± STANDARD DEVIATION)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE V CLUSTERING</head><label>V</label><figDesc>RESULTS ON THE BBCSPORT DATABASE (MEAN ± STANDARD DEVIATION)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE VI CLUSTERING</head><label>VI</label><figDesc>RESULTS ON THE ORL DATABASE (MEAN ± STANDARD DEVIATION)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VII CLUSTERING</head><label>VII</label><figDesc>RESULTS ON THE UCI-2VIEW DATABASE (MEAN ± STANDARD DEVIATION)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>YIN et al.: MULTIVIEW SUBSPACE CLUSTERING VIA TENSORIAL t-PRODUCT REPRESENTATION</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://mlg.ucd.ie/datasets</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The authors would like to thank these authors for their opening simulation codes.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank E. Kernfel for his helpful discussion and C. Zhang for his opening code <ref type="bibr" target="#b58">[59]</ref>.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This was supported in part by the National Natural Science Foundation of China under Grant 61333013, Grant 61703113, and Grant 61673124, in part by the Science and Technology Planning Project of Guangdong Province under Grant 2017A010101024, in part by the Guangdong Province Natural Science Foundation under Grant 2014A030308009 and Grant 2014B090907010, and in part by the Scientific Research Foundation for the Returned Overseas Chinese Scholars, State Education Ministry, China.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ming <ref type="bibr">Yin</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shengli</head><p>Xie (M'01-SM'02) received the M.S. degree in mathematics from Central China Normal University, Wuhan, China, in 1992, and the Ph.D. degree in automatic control from the South China University of Technology, Guangzhou, China, in 1997.</p><p>He is currently the Director of the Key Laboratory of Intelligent Information Processing, Institute of Intelligent Information Processing, and the Guangdong Key Laboratory of IoT Information Technology, and a Professor with the School of Automation, Guangdong University of Technology, Guangzhou. He has authored or co-authored four monographs and over 100 scientific papers published in journals and conference proceedings. He holds over 30 patents.</p><p>Yi Guo (M'16) received the B.Eng. degree (Hons.) in instrumentation from the North China University of Technology, Beijing, China, in 1998, the M.Eng. degree in automatic control from Central South University, Changsha, China, in 2002, and the Ph.D. degree in computer science from the University of New England, Armidale, NSW, Australia, in 2008, with a focus on dimensionality reduction for structured data with no vectorial representation.</p><p>From 2008 to 2016, he was with CSIRO, Canberra, ACT, Australia, where he was involved in a computational statistician on various projects in spectroscopy, remote sensing, and materials science. He joined the Centre for Research in Mathematics, Western Sydney University, Sydney, NSW, Australia, in 2016. His current research interests include machine learning, computational statistics, and big data.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mathematical Methods for Physicists</title>
		<author>
			<persName><forename type="first">G</forename><surname>Arfken</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>Academic</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="19" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Candés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Optim</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Spectral clustering with convex regularizer on millions of images</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">8691</biblScope>
			<biblScope unit="page" from="282" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral kernel methods for clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Kandola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="649" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Low-rank common subspace for multi-view learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDM</title>
		<meeting>ICDM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust multi-view subspace learning through dual low-rank decompositions</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1181" to="1187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tensor LRR and sparse coding-based subspace clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2120" to="2133" />
			<date type="published" when="2016-09">Sep. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topic driven multimodal similarity learning with multi-view voted convolutional features</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Goulermas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2017.02.035</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="223" to="234" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A matrix factorization approach for integrating multiple data views</title>
		<author>
			<persName><forename type="first">D</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf</title>
		<meeting>Eur. Conf</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="423" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tensor learning for regression</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="816" to="827" />
			<date type="published" when="2012-02">Feb. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Classification</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="193" to="218" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Clustering multi-way data: A novel algebraic approach</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kernfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1412.7056" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tensor-tensor products with invertible linear transforms</title>
		<author>
			<persName><forename type="first">E</forename><surname>Kernfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="page" from="545" to="570" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Factorization strategies for third-order tensors</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra Appl</title>
		<imprint>
			<biblScope unit="volume">435</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="641" to="658" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Third-order tensors as operators on matrices: A theoretical and computational framework with applications in imaging</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Braman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hoover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="148" to="172" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">W</forename><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Rev</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A co-training approach for multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="393" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Co-regularized multi-view spectral clustering</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distortion invariant object recognition in the dynamic link architecture</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="300" to="311" />
			<date type="published" when="1993-03">Mar. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">De</forename><surname>Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Matrix Anal. Appl</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1253" to="1278" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Large-scale multi-view spectral clustering via bipartite graph</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2750" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linearized alternating direction method with adaptive penalty for low-rank representation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="612" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust recovery of subspace structures by low-rank representation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="171" to="184" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-view clustering via joint nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. Data Mining</title>
		<meeting>SIAM Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="252" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multiview partitioning via tensor methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Glänzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Moor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1056" to="1069" />
			<date type="published" when="2013-05">May 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tensor robust principal component analysis: Exact recovery of corrupted lowrank tensors via convex optimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="5249" to="5257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A unified alternating direction method of multipliers by majorization minimization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2017.2689021</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="527" to="541" />
			<date type="published" when="2018-03">Mar. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pietikäinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mäenpää</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07">Jul. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001-05">May 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Subspace clustering for high dimensional data: A review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGKDD Explorations Newslett</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="90" to="105" />
			<date type="published" when="2004-06">Jun. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Clustering multi-way data via adaptive subspace iteration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 17th ACM Conf</title>
		<meeting>17th ACM Conf</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1519" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A submodule clustering method for multi-way data by sparse and low-rank representation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="https://128.84.21.199/abs/1601.00149v2" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">TenSR: Multi-dimensional tensor sparse representation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06">Jun. 2016</date>
			<biblScope unit="page" from="5916" to="5925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of multi-view machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput. Appl</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2031" to="2038" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multiview privileged support vector machines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2728139</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kernel-based weighted multi-view clustering</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tzortzis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Likas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 12th Int. Conf. Data Mining</title>
		<meeting>IEEE 12th Int. Conf. Data Mining</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Subspace clustering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="52" to="68" />
			<date type="published" when="2011-03">Mar. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Low rank subspace clustering (LRSC)</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="61" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-view clustering and feature learning via structured sparsity</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="352" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multimodal graph-based reranking for Web image search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4649" to="4661" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual classification by 1 -hypergraph modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2564" to="2574" />
			<date type="published" when="2015-09">Sep. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust subspace clustering for multi-view data by exploiting correlation consensus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3939" to="3949" />
			<date type="published" when="2015-11">Nov. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Alternating direction augmented Lagrangian methods for semidefinite programming</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program. Comput</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convex multi-view subspace learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust multi-view spectral clustering via low-rank and sparse decomposition</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2149" to="2155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">A survey on multi-view learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1304.5634" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Nonlinear low-rank representation on Stiefel manifolds</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron. Lett</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="749" to="751" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Laplacian regularized low-rank representation and its applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="504" to="517" />
			<date type="published" when="2016-03">Mar. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dual graph regularized latent low-rank representation for subspace clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4918" to="4933" />
			<date type="published" when="2015-12">Dec. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multi-view clustering via pairwise sparse subspace representation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page" from="12" to="21" />
			<date type="published" when="2015-05">May 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multiview clustering via unified and view-specific embeddings learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2017.2786743</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Bayesian cotraining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2649" to="2680" />
			<date type="published" when="2011-09">Sep. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Low-rank tensor constrained multiview subspace clustering</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015-12">Dec. 2015</date>
			<biblScope unit="page" from="1582" to="1590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Exact tensor completion using t-SVD</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Process</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1511" to="1526" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Novel factorization strategies for higher order tensors: Implications for compression and recovery of multi-linear data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1307.0805" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Novel methods for multilinear data completion and de-noising based on tensor-SVD</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Kilmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3842" to="3849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Non-negative low rank and sparse graph for semi-supervised learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2328" to="2335" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
