<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dissecting neural computations in the human auditory pathway using deep neural networks for speech</title>
				<funder ref="#_UR3bEKA">
					<orgName type="full">Shanghai Shenkang Hospital Development Center</orgName>
				</funder>
				<funder>
					<orgName type="full">William and Susan Oberndorf Foundation</orgName>
					<orgName type="abbreviated">.</orgName>
				</funder>
				<funder ref="#_vwgBYYt">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_2RsrrWJ">
					<orgName type="full">Shanghai Rising-Star Program</orgName>
				</funder>
				<funder>
					<orgName type="full">William K. Bowes Foundation</orgName>
					<orgName type="abbreviated">.</orgName>
				</funder>
				<funder>
					<orgName type="full">Joan and Sanford Weill Foundation</orgName>
					<orgName type="abbreviated">.</orgName>
				</funder>
				<funder ref="#_gH8ZEPh">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_5ZthqEY">
					<orgName type="full">National Institute on Deafness and Other Communication Disorders</orgName>
				</funder>
				<funder ref="#_8Zw9K33">
					<orgName type="full">Shanghai Young Talents Program</orgName>
				</funder>
				<funder ref="#_fYmZA6M">
					<orgName type="full">National Institute of Neurological Disorders and Stroke</orgName>
				</funder>
				<funder ref="#_SDu5EEc">
					<orgName type="full">Shurl and Kay Curci Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-30">30 October 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuanning</forename><surname>Li</surname></persName>
							<idno type="ORCID">0000-0003-3277-0600</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurological Surgery</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gopala</forename><forename type="middle">K</forename><surname>Anumanchipal</surname></persName>
							<idno type="ORCID">0000-0002-4729-5702</idno>
							<affiliation key="aff1">
								<orgName type="department">Weill Institute for Neurosciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Meta AI Research</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peili</forename><surname>Chen</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">School of Biomedical Engineering &amp; State Key Laboratory of Advanced Medical Materialsand Devices</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laure</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
							<idno type="ORCID">0000-0002-4729-5702</idno>
							<affiliation key="aff5">
								<orgName type="department">Department of Biomedical Engineering</orgName>
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<settlement>Rochester</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junfeng</forename><surname>Lu</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Neurologic Surgery Department</orgName>
								<orgName type="institution" key="instit1">Huashan Hospital</orgName>
								<orgName type="institution" key="instit2">Shanghai Medical College</orgName>
								<orgName type="institution" key="instit3">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="laboratory">Brain Function Laboratory</orgName>
								<orgName type="institution" key="instit1">Neurosurgical Institute</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jinsong</forename><surname>Wu</surname></persName>
							<affiliation key="aff6">
								<orgName type="department">Neurologic Surgery Department</orgName>
								<orgName type="institution" key="instit1">Huashan Hospital</orgName>
								<orgName type="institution" key="instit2">Shanghai Medical College</orgName>
								<orgName type="institution" key="instit3">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff7">
								<orgName type="laboratory">Brain Function Laboratory</orgName>
								<orgName type="institution" key="instit1">Neurosurgical Institute</orgName>
								<orgName type="institution" key="instit2">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Edward</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
							<email>edward.chang@ucsf.edu</email>
							<idno type="ORCID">0000-0003-2480-4700</idno>
							<affiliation key="aff0">
								<orgName type="department">Department of Neurological Surgery</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Weill Institute for Neurosciences</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco, San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="department">School of Biomedical Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Advanced Medical Materials and Devices</orgName>
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dissecting neural computations in the human auditory pathway using deep neural networks for speech</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-30">30 October 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1038/s41593-023-01468-4</idno>
					<note type="submission">Received: 14 April 2022 Accepted: 13 September 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The human auditory system extracts rich linguistic abstractions from speech signals. Traditional approaches to understanding this complex process have used linear feature-encoding models, with limited success. Artificial neural networks excel in speech recognition tasks and offer promising computational models of speech processing. We used speech representations in state-of-the-art deep neural network (DNN) models to investigate neural coding from the auditory nerve to the speech cortex. Representations in hierarchical layers of the DNN correlated well with the neural activity throughout the ascending auditory system. Unsupervised speech models performed at least as well as other purely supervised or fine-tuned models. Deeper DNN layers were better correlated with the neural activity in the higher-order auditory cortex, with computations aligned with phonemic and syllabic structures in speech. Accordingly, DNN models trained on either English or Mandarin predicted cortical responses in native speakers of each language. These results reveal convergence between DNN model representations and the biological auditory pathway, offering new approaches for modeling neural coding in the auditory cortex.</p><p>Speech perception involves computations that transform acoustic signals into linguistic representations. Listening to speech activates the entire auditory pathway: from the auditory nerve (AN) and subcortical structures to the primary and nonprimary auditory cortical areas. Natural speech perception is a challenging task owing to variable acoustic cues for linguistic perceptual units (phonemes, syllables and words) under contextual factors such as interspeaker variability, emotional condition, prosody, coarticulation and speech rate 1-3 . Despite challenges, the auditory system is sensitive to this variability yet robustly extracts invariant phonetic and lexical information to support speech comprehension 2,4-6 . A central goal of speech and auditory neuroscience, as well as cognitive neuroscience in general, is to understand the computations performed by specific neural circuits and the representations generated by such computations 7 .</p><p>Classical cognitive models such as Cohort 8 , TRACE 9 and their variants account for many psychological aspects of speech perception</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article</head><p>https://doi.org/10.1038/s41593-023-01468-4 different computational architectures (convolution, recurrence and self-attention) and training strategies (supervised and unsupervised objectives). Furthermore, inspection of DNN computations offers insights into the underlying mechanisms driving neural encoding predictions. Unlike previous modeling efforts that focused on a single language, mainly English, we here use a cross-linguistic paradigm to unveil language-invariant and language-specific aspects during speech perception.</p><p>In particular, we demonstrate the following findings: (1) the hierarchy in DNNs trained to learn speech representations correlates with that in the ascending auditory pathway; (2) unsupervised models without explicit linguistic knowledge can learn similar feature representations as the human auditory pathway; (3) deeper layers in speech DNNs correlate with speech-responsive populations in the nonprimary auditory cortex, driven by specific computations aligned with critical linguistically relevant temporal structures, such as phonemic and syllabic contexts; and (4) DNN-based models, unlike traditional linear encoding models, can reveal language-specific properties in cross-language speech perception. Taken together, our findings provide new data-driven approaches to modeling and evaluating neural coding in the auditory cortex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><p>Our overall goal is to understand the computations and representations that occur and emerge throughout the auditory system during speech perception. To model the early pathway, we used a simulation of biophysical models of the auditory periphery and midbrain <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> , which have been highly successful at the cellular level. The biophysical model simulation yielded 50 distinct neurons in the AN and 100 distinct neurons in the inferior colliculus (IC). For the later portion of the pathway, we used intracranial cortical recordings from both the primary and nonprimary auditory cortical areas <ref type="bibr" target="#b33">34</ref> in nine participants (Extended Data Fig. <ref type="figure" target="#fig_0">1</ref>). Local field potentials were recorded using high-density grids while these participants listened to English speech. A total of 553 electrodes were placed over the auditory cortex, 81 over the primary auditory cortex (Heschl gyrus (HG)) and 472 over the nonprimary auditory cortex (superior temporal gyrus (STG)). The amplitude of the local field potential in the high-gamma band (70-150 Hz) was used as a measure of local neuronal activity <ref type="bibr" target="#b34">35</ref> . Neural responses across the early and late auditory systems were assessed using a set of 599 English sentences from the TIMIT corpus <ref type="bibr" target="#b35">36</ref> .</p><p>We used five DNNs for the extraction of speech representations. These models differ in training objectives. In particular, we used two unsupervised models and three supervised models: (1) the HuBERT model, a transformer-based self-supervised model trained to predict masked portions of speech <ref type="bibr" target="#b14">15</ref> ; (2) the Wav2Vec 2 unsupervised model, a transformer-based self-supervised model trained for contrastive learning that distinguishes spans of a speech utterance from distractors <ref type="bibr" target="#b13">14</ref> ;</p><p>(3) the Wav2Vec 2 supervised model, a transformer-based supervised but do not explain neural coding or perform well in natural speech recognition. Conversely, classical neural encoding models <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref> explain neural coding during speech perception but cannot be directly adapted to a unified computational framework of speech perception. Modern artificial intelligence (AI) models using deep neural networks (DNNs) are approaching human-level performance in automatic speech recognition (ASR) <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> . However, their end-to-end 'black box' nature hampers the interpretation of internal computations and representations. Here, we aim to correlate DNN model computations and representations with the neural responses of the human auditory system to enhance the interpretability of AI models and offer new data-driven computational models of sensory perception.</p><p>Task-oriented pretrained DNN models have shown promise as computational models in sensory neuroscience. Using learned features from supervised learning tasks (for example, image recognition or sound classification), encoding models predict, with high accuracy, neural responses in the visual and auditory cortices <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> . In particular, Kell et al. used supervised convolutional neural networks (CNNs) to build encoding models for auditory responses in functional magnetic resonance imaging (fMRI) recordings and showed an aligned hierarchy between the CNNs and the auditory cortex <ref type="bibr" target="#b16">17</ref> . Two of the key ingredients in DNN models are model architecture and training objective. Model architecture determines the computations performed on input signals, whereas the training objective affects representations learned through optimization. Neural coding in the ventral visual cortex is largely driven by spatial statistics in retinotopic space <ref type="bibr" target="#b19">20</ref> , favoring CNNs with hierarchical spatial convolutions as computational models <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21</ref> .</p><p>Unlike core object recognition in vision modeling, which uses static images <ref type="bibr" target="#b21">22</ref> , speech involves dynamic sequences often modeled by sequence-to-sequence (seq2seq) learning in modern AI <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23</ref> . These models extract dynamic representations of speech, shaped by both the current input (a nonlinear transformation of the current input) and the long-term dependencies in the input sequences (for example, the history of an input sequence). Furthermore, supervised model training, which often requires an enormous amount of labeled data, is not plausible as a generic learning strategy for the human auditory system. Human infants can learn phonetic and linguistic categories through speech sound statistics in native languages without explicit word learning <ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25</ref> . Recent works have suggested unsupervised models without labeled data as models of vision and high-level language processing in the brain <ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref> . Therefore, unsupervised speech models capturing transient (local) and longer-context features of speech may yield more suitable speech perception models <ref type="bibr" target="#b28">29</ref> .</p><p>This study directly compares state-of-the-art neural network models of speech to the human auditory pathway, aiming to uncover shared representations and computations between the two systems. Neural responses to natural speech across the ascending auditory pathway and the corresponding DNN speech embeddings are analyzed. Using a neural encoding framework <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30</ref> , we systematically evaluate the similarity between the auditory pathway and DNN models with model based on fine-tuning of the Wav2Vec 2 unsupervised model for ASR <ref type="bibr" target="#b13">14</ref> ; (4) the HuBERT/Wav2Vec 2 supervised model (HuBERT supervised), a fully supervised model trained only for supervised ASR and with no unsupervised pretraining; and (5) the Deep Speech 2 model, a long short-term memory (LSTM)-based supervised ASR model <ref type="bibr" target="#b12">13</ref> . These models share a similar hierarchical framework: a multilayer convolutional feature encoder that extracts temporally constrained lower-level acoustic feature representations using one-and two-dimensional convolutions from a raw speech-audio waveform or spectrogram and a multilayer sequential encoder (with multiple transformer-encoder or recurrent (LSTM) layers) that extracts higher-level, context-dependent phonetic information from the CNN encoder output. We pretrained the speech-learning models on LibriSpeech, a standard corpus of 960 h of continuous naturalistic English speech <ref type="bibr" target="#b36">37</ref> (Table <ref type="table" target="#tab_0">1</ref>). The speech responses from the auditory pathway and DNNs were aligned in time to train linear encoding models. Different representation layers in the DNNs were used to predict neural responses in the auditory pathway (Fig. <ref type="figure" target="#fig_0">1</ref>). The performance of these models (prediction R 2 ) quantifies the similarity between the DNN-learned speech representations and the underlying neural representations. In this way, we tested the hypothesis that speech DNN models converge to a similar representation hierarchy as the ascending auditory pathway. NA, not applicable.</p><p>To address heterogeneous signal-to-noise ratios across the auditory pathway areas, participants and signal modalities, we established benchmark baselines for each electrode and neuron. For each recording site, we trained two baseline models: (1) a linear temporal receptive field (TRF) model using spectrogram features 10 and (2) a linear TRF model using acoustic-phonetic features, including spectrogram, speech envelope/temporal landmark, pitch and phonetic features <ref type="bibr" target="#b33">34</ref> (Extended Data Fig. <ref type="figure">2</ref>). The performance of neural encoding models using different sets of features was normalized against the second baseline model with a heuristic full-feature set in each recording site to make evaluations comparable across sites and areas. This normalized prediction R 2 was termed the brain-prediction score (BPS), a primary metric for prediction accuracy at each site. of each DNN layer were extracted and aligned with the corresponding neural activity from each recording site in the auditory pathway. A ridge regression model was fitted to predict neural activity from time-windowed DNN representations, and the regression coefficient of determination R 2 between the predicted and actual neural activity was used as a metric of prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Article</head><p>https://doi.org/10.1038/s41593-023-01468-4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN hierarchy correlates with the ascending auditory pathway</head><p>We tested whether DNNs trained to learn speech representations converge on the same standard auditory (serial feedforward) hierarchy of AN-IC-HG-STG. To do this, we compared the DNN hierarchy and the ascending auditory pathway from two different perspectives: (1) does the hierarchy of layers in DNNs mirror a similar hierarchy in the ascending auditory pathway? (2) Are the feature representations learned by DNNs more strongly correlated with neural coding than linguistically derived acoustic-phonetic feature sets? First, we considered a representative state-of-the-art selfsupervised DNN, the HuBERT model <ref type="bibr" target="#b14">15</ref> . For every single-layer representation model in HuBERT, we computed the averaged BPS (normalized prediction R 2 ) across all recording sites within each anatomical area (Fig. <ref type="figure">2</ref>; see Extended Data Fig. <ref type="figure" target="#fig_3">3</ref> for raw R 2 and noise-ceiling values). Compared to the linear model with heuristic acoustic-phonetic features, the performance of the DNN encoding model was 39.9% higher in the AN at transformer layer 1 (mean BPS = 1.399, t(50) = 13.97, P = 2.5 ? 10 -44 , two-sided), 76.3% higher in the IC at transformer layer 1 (mean BPS = 1.763, t(100) = 13.75, P = 5 ? 10 -43 , two-sided), 3.4% higher in the HG at transformer layer 1 (mean BPS = 1.033, t(53) = 1.20, P = 0.23, two-sided) and 23.0% higher in the STG at transformer layer 10 (mean BPS = 1.230, t(144) = 16.1, P = 5 ? 10 -58 ) (Fig. <ref type="figure">2a</ref>). Moreover, of all layers in the same unsupervised DNN model, the CNN layers and the first four transformer layers in the hierarchy best predicted the AN and IC responses (Fig. <ref type="figure">2a</ref>). A finer-grain analysis suggested that the early part of the CNN layers predicted AN responses better than IC responses, whereas the late part of the CNN layers predicted IC responses better than AN responses (Extended Data Fig. <ref type="figure" target="#fig_4">4</ref>). The activity of the speech-responsive STG population was best predicted by the later part of the DNN model and peaked at the tenth layer out of all 12 transformer layers (Fig. <ref type="figure">2a</ref>). HG responses were predicted equally well by all transformer layers. However, none of these layers of speech DNNs outperformed the baseline acoustic model in predicting HG responses (Fig. <ref type="figure">2a</ref>). Furthermore, this general hierarchical trend was consistent across several DNN models that shared a similar architecture with the HuBERT model but with different training objectives (Extended Data Fig. <ref type="figure" target="#fig_6">5</ref>).</p><p>Next, we tested the hypothesis that the auditory hierarchy is characterized by increasingly long windows of temporal integration. Using the baseline spectrogram model, we found that the TRFs estimated for each area showed a hierarchy of progressive temporal integration of acoustic inputs: temporal responses in the peripheral areas AN and IC were mostly transient within 100 ms, whereas neural responses in the cortex showed integration time windows longer than 100 ms. More specifically, HG responses on average had a consistent temporal integration window of 200 ms, and some STG electrodes showed a significant sustained temporal integration window of up to 300 ms and longer (Fig. <ref type="figure">2b</ref> and Extended Data Fig. <ref type="figure" target="#fig_3">3</ref>). This trend of increasing temporal integration window was also consistent with the estimated optimal encoding window size that yielded the best prediction in encoding models (Extended Data Fig. <ref type="figure" target="#fig_3">3</ref>).</p><p>Finally, we generalized the evaluations to a set of different DNN models (Table <ref type="table" target="#tab_0">1</ref>). We found that, for all areas, all DNN-based encoding models outperformed the baseline linear models. On average, compared to the linear model using heuristic acoustic-phonetic features, DNN-based encoding models explained 29.3-40.0% more variance in the AN, 61.7-76.3% more variance in the IC, -3.5% to 11.4% more variance in the HG and 3.1-23.0% more variance in the STG (Fig. <ref type="figure">2c</ref>). In particular, the transformer layers in the unsupervised HuBERT model achieved the highest average performance in all areas except the HG. Moreover, we found that neural responses to speech in the auditory periphery (AN and IC) and primary auditory cortex (HG) were also largely characterized by locally resolved filters such as CNN representations, which had a fixed finite receptive field in time (P &gt; 0.05 compared to HuBERT, two-sided t test; Fig. <ref type="figure">2c</ref>). In contrast, speech responses in the nonprimary auditory cortex (STG) were better predicted using the deeper transformer layers in the DNNs (Fig. <ref type="figure">2c</ref> and Extended Data Figs. <ref type="figure" target="#fig_4">4</ref> and<ref type="figure" target="#fig_6">5</ref>).</p><p>To sum up from the above three perspectives, the early to later layers in DNNs trained to learn speech representations correlate with the successive processing in the ascending auditory pathway. HG representation is not modeled well by speech DNNs (P &gt; 0.1 in all layers compared to baseline; Fig. <ref type="figure" target="#fig_0">1a</ref>), although the latencies and temporal integration windows for TRFs would suggest a serial processing pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN layers correlate with distinct STG populations</head><p>Previous studies have identified neural populations in the STG that show distinct speech-responsive profiles, including onset and sustained responses <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38</ref> . Here, we evaluated whether these functionally distinct speech-responsive populations correspond to different layers in the same DNN model.</p><p>To identify functionally distinct populations in the STG, we performed non-negative matrix decomposition on the averaged speechevoked response to cluster speech-responsive electrodes. Among the 144 speech-responsive electrodes in the STG, we found two clusters that showed distinct onset and sustained response profiles based on averaged high-gamma responses across sentences <ref type="bibr" target="#b37">38</ref> (Fig. <ref type="figure" target="#fig_3">3a,</ref><ref type="figure">b</ref> and Extended Data Fig. <ref type="figure">6</ref>). Note that we used a slightly different clustering strategy and clustered trial-averaged responses instead of single-trial responses as in the study by Hamilton et al. <ref type="bibr" target="#b37">38</ref> . We found similar onset and sustained functional populations as in Hamilton et al.'s study <ref type="bibr" target="#b37">38</ref> but not the same anatomical distinctions. However, our results align with those from Hamilton et al.'s recent study <ref type="bibr" target="#b33">34</ref> , which demonstrated that the posterior STG has a concentrated transient onset response and the middle and posterior STG areas have a more distributed sustained phonetic and pitch encoding.</p><p>We then investigated the best prediction model for STG responses, the HuBERT model, and compared the BPSs of different layers with regard to the functional clusters. We found that both clusters were better explained by the contextual layers in the HuBERT model. As shown in Fig. <ref type="figure" target="#fig_3">3c</ref>, for the more sustained cluster (cluster 1), the best prediction model came from the deep layers of the transformer encoder in the DNN (cluster 1: peak BPS = 1.26 at transformer layer 10). The deep layers of the transformer encoder performed significantly better than the early layers in the DNN (P &lt; 0.05, two-sided paired t test; degrees of freedom (d.f.) = 83; no statistical difference across layers 6-12). For the more transient cluster (cluster 2), the best prediction model was from transformer layer 5 in the DNN (peak BPS = 1.20 at layer 5). However, the peak prediction layer did not significantly outperform any other transformer layers in the network except the very first one (P &gt; 0.05 for all two-sided paired t tests, d.f. = 61 for cluster 2). Clusters 1 and 2 showed a similar optimal delay-time window of approximately 200-250 ms (Fig. <ref type="figure" target="#fig_3">3d</ref>). As a result, the sustained speech-responsive neural activity prevalent in the STG can be predicted from the deeper representation layers in the DNN, whereas the more transient speech-responsive neural activity, such as the onset response, can be predicted in both the early and late parts of the transformer hierarchy in the DNN. The DNN maintains the transient onset representation throughout the processing hierarchy, and the later layers represent both transient and sustained representations in parallel. This suggests that some features, especially highly salient ones such as phrasal and sentence onsets, may be represented in multiple layers across the DNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN computations explain neural encoding predictions</head><p>We next examined the computational mechanism underlying representations in the DNN. We asked whether certain types of attentional computation for speech in the DNN explain the ability to predict brain responses. Here, we particularly focused on attention regarding the Article https://doi.org/10.1038/s41593-023-01468-4 phonological context, which corresponds to the neighboring phonemes and syllables of the target speech sound.</p><p>Specifically, we used the HuBERT model as the target model and extracted the attention-weight matrices in each transformer layer of the DNN, which quantified the contributions from different context parts to the feature representation at each time. Critically, these contextual attention-weight matrices were not static filters but rather dynamically changed according to the specific speech sequences. Therefore, they reflect the stimulus-dependent dynamic extraction of contextual information in each speech sequence. Such computations are important for extracting the informative sequential feature representations of acoustic signals.</p><p>As a result, for each sentence in the speech corpus, we defined templates of attention matrices corresponding to different levels of contextual information representation in speech, including contextual information within the same phoneme, contextual information from the previous phoneme(s), contextual information within the same syllable and contextual information from the previous syllable(s) (Fig. <ref type="figure" target="#fig_4">4a,</ref><ref type="figure">b</ref>). We then computed the averaged correlation coefficient between the actual attention-weight matrices in each DNN layer and the templates across all sentences, which we termed the attention score (AS) (Fig. <ref type="figure" target="#fig_4">4c</ref>). We found a general trend that deeper layers had an increased amount of contextual attention to linguistic structures (previous phoneme(s) and syllable(s)) (Fig. <ref type="figure" target="#fig_4">4c</ref>, bar plots). A randomized DNN model with the same architecture but no pretraining on speech data did not show such progressive contextual attention along the hierarchy (Fig. <ref type="figure" target="#fig_4">4c</ref>, black lines). Therefore, the alignment of attention with contextual structures not only was a direct consequence of the hierarchical architecture of  the DNN model that emerges with depth but also reflected computations adapted to extracting speech-specific, linguistically relevant representations through training on natural speech (Fig. <ref type="figure" target="#fig_4">4c</ref>).</p><p>We then tested whether such trends in contextual computations would predict the brain-prediction performance of different layers in the DNN. Specifically, we correlated the AS with the BPS for each brain area in different DNN layers. We found that the phonemic-and syllabic-level attention to the linguistic context in speech was positively correlated with the ability to predict brain activity only in the nonprimary auditory cortex (Fig. <ref type="figure" target="#fig_4">4g</ref>) but not in the auditory periphery or the primary auditory cortex (Fig. <ref type="figure" target="#fig_4">4d-f</ref>). In other words, for a given transformer layer in the model, the better the attention weights aligned with the linguistic contextual structure, the better the layer's learned representation would be able to predict the speech response in the STG. Conversely, the more contextual information attended, the less the learned representation would be correlated with the AN-IC-HG response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN encoding models capture language-specific information</head><p>Next, we tested whether DNN computations and representations are language specific and reflect higher-level language processing beyond the acoustics, such as phonotactic, phonological or lexical representations. To do this, we used a cross-linguistic approach by comparing English and Mandarin (Fig. <ref type="figure" target="#fig_6">5a</ref>). Mandarin shares many consonants and vowels with English but largely differs in how phonetic and prosodic features are combined to give rise to words. In addition to data from English-speaking participants, we also analyzed cortical recordings from three native Mandarin speakers (Extended Data Fig. <ref type="figure" target="#fig_0">1</ref>). Both groups were monolingual and had no comprehension of the foreign language. We adopted the same paradigm and materials as our previous study that focused on cross-linguistic pitch perception <ref type="bibr" target="#b38">39</ref> . The two participant groups were instructed to listen to both naturalistic English speech and Mandarin speech in separate recording blocks. In addition to the previous HuBERT model pretrained on English speech, we also pretrained the same HuBERT model on naturalistic Mandarin speech. We then compared the performance of the two HuBERT models on the two groups when they listened to different languages (Fig. <ref type="figure" target="#fig_6">5a</ref>).</p><p>To explicitly test our hypotheses of linguistically relevant, contextdependent processing in the auditory pathway as shown in the previous section (Fig. <ref type="figure" target="#fig_4">4</ref>), we conducted cross-lingual perception and DNN prediction tests. In particular, we hypothesized that the contextual-dependent computations in the DNN capture language-specific, higher-level processing beyond the acoustics in the STG. Therefore, we expected the English-pretrained model to show higher brain-prediction performance for the STG in native English speakers and that the prediction performance would be better aligned with contextual attention to the phonemic and syllabic structures in English than in Mandarin. On the contrary, we expected the Mandarin-pretrained model to show higher    </p><formula xml:id="formula_0">AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) AS (r) Layer # 12 1</formula><p>Layer #  First, we examined the results with an English-pretrained model and native English speakers. At the acoustic level, the linear spectrogram TRF (STRF) model, which included only spectrogram features, showed similar performance in predicting neural responses in the STG when the participants listened to different languages (mean R 2 = 0.162 and 0.143 for Mandarin and English speech, respectively; paired t(57) = 1.65, P = 0.104, two-sided; Fig. <ref type="figure" target="#fig_6">5b</ref>). This suggests that lower-level acoustic representations are largely shared across languages. However, a performance gap was found in the DNN encoding models between languages, in which the BPS for English speech was significantly higher than that for Mandarin speech (13 of 14 comparisons had P &lt; 0.01, paired t test, two-sided; Fig. <ref type="figure" target="#fig_6">5c</ref>). Moreover, the gap between the two languages monotonically increased in deeper layers of the network: ?BPS = 0.160 at the CNN output layer (paired t(57) = 2.55, two-sided P = 0.013), ?BPS = 0.211 at the first transformer-encoder layer (paired t(57) = 3.20, two-sided P = 0.002) and ?BPS = 0.314 at the tenth transformer-encoder layer (paired t(57) = 4.56, two-sided P = 3 ? 10 -5 ) (Fig. <ref type="figure" target="#fig_6">5c</ref>). This suggests that the representation in the network demonstrates an increasing level of language-specific information. We also evaluated the relationship between the computation of phonemic and syllabic contextual information in DNN layers and the corresponding brain-prediction performance for Mandarin speech in the STG. As opposed to previous results (Fig. <ref type="figure" target="#fig_4">4g</ref>), no significant correlation was found in either the phonemic or syllabic level between the attention patterns in DNN layers and the BPSs when native English speakers listened to Mandarin speech (P &gt; 0.05 for all cases, permutation test; Fig. <ref type="figure" target="#fig_6">5d</ref>).</p><p>In contrast, we found opposite results with a Mandarin-pretrained model and native Mandarin speakers. At the acoustic level, the linear STRF model also showed similar performance for both Mandarin and English speech (mean R 2 = 0.056 and 0.058 for Mandarin and English speech, respectively; paired t(61) = -0.501, P = 0.617, two-sided; Fig. <ref type="figure" target="#fig_6">5e</ref>). The DNN encoding models showed consistently higher performance for neural responses to Mandarin speech than English speech (all 14 of 14 comparisons had P &lt; 0.01, paired t test, two-sided; Fig. <ref type="figure" target="#fig_6">5f</ref>), and the gap also increased in deeper layers: ?BPS = 0.293 at the CNN output layer (P = 6 ? 10 -7 , paired t(61) = 5.57, two-sided) and ?BPS = 0.405 at the ninth transformer-encoder layer (P = 6 ? 10 -9 , paired t(61) = 6.76, two-sided; Fig. <ref type="figure" target="#fig_6">5f</ref>). Moreover, as opposed to the combination of the English-pretrained model and native English speakers, we found consistently significant correlations between phonemic-or syllabic-level ASs and BPSs when listening to Mandarin speech (P &lt; 0.05, permutation test), and no significant correlation when listening to English speech, in these native Mandarin speakers (P &gt; 0.05 for all cases, permutation test; Fig. <ref type="figure" target="#fig_6">5g</ref>).</p><p>Therefore, our results demonstrate a double-dissociation pattern between pretrained models and native languages, suggesting that DNN computations and representations capture higher-level, language-specific linguistic information in the STG that is learned depending on language experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN acoustic-phonetic hierarchy explains brain prediction</head><p>The last question we asked is whether the brain-prediction performance of the DNN layers can be accounted for by an acoustic-to-phonetic processing hierarchy. We tested the feature representations of acoustic, phonetic and prosodic information in the DNN layers. Specifically, we applied similar linear feature-encoding models to predict the activations of hidden units in different DNN layers and computed the unique variance explained by each set of features. These features are statically coded and do not vary according to different contexts. Therefore, our analysis here intentionally reflects the static noncontextual part of acoustic/phonetic/prosodic representations in DNN layers, as addressed in the previous analyses.</p><p>Overall, the results demonstrated an acoustic-to-phonetic transformation along the hierarchy (Fig. <ref type="figure">6a</ref>). In the CNN output layer, acoustic (spectrogram) features uniquely accounted for 20.0% of the total variance, whereas phonetic features accounted for only 1.70% (paired t(768) = 47.6, P &lt; 1 ? 10 -10 , two-sided). However, after the third transformer encoder, phonetic features consistently explained more unique variance than the acoustic features in the network (3.45% versus 2.66% at Tr. 4 for phonetic and acoustic features respectively, paired t(768) = 5.77, P = 5.7 ? 10 -9 ). The unique variance explained by static phonetic features peaked at the 11th transformer-encoder layer with a unique R 2 of 3.98% (paired t(768) = 9.12, P &lt; 1 ? 10 -10 , two-sided t test against acoustic features, which accounted for 2.85%). Meanwhile, temporal landmark (envelope) features (for example, speech envelope and onsets) and prosodic pitch features (absolute and relative pitch) were more uniformly distributed along the hierarchy of the network (Fig. <ref type="figure">6a</ref>).</p><p>Furthermore, when correlated with the BPS of individual layers, spectrogram feature encoding showed a significant positive correlation only in the peripheral areas (AN: Pearson's r = 0.65, P = 0.039, permutation test; IC: Pearson's r = 0.68, P = 0.031, permutation test; Fig. <ref type="figure">6b</ref>). Phonetic feature encoding correlated with the BPS in the STG (Pearson's r = 0.77, P = 0.0025, permutation test; Fig. <ref type="figure">6b</ref>) but not in the other areas (P &gt; 0.05 for all of the other three areas, permutation test; Fig. <ref type="figure">6b</ref>). Taking these together, a similar acoustic-to-phonetic hierarchy was found and correlated with both the self-supervised DNN model and the ascending AN-IC-STG pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We have demonstrated that speech representations learned in state-of-the-art DNNs resemble important aspects of information processing in the human auditory system. DNN feature representations significantly outperform theory-driven acoustic-phonetic feature sets in predicting neural responses to natural speech throughout the auditory pathway. DNN-layer hierarchy correlates with the AN-midbrain-STG ascending auditory pathway. Deeper DNN layers correlate with functionally distinct speech-tuned populations in the nonprimary auditory cortex. We inspected the core contextual computations in DNNs and found that they learn critical linguistically relevant temporal structures, such as phoneme and syllable contexts, from purely unsupervised natural speech training. Such ability to learn language-specific linguistic information predicts DNN-neural coding correlation in the  nonprimary auditory cortex. DNN-based neural encoding models can reveal language-specific coding in the STG during cross-language perception, whereas linear STRF models cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNN models reveal important neural coding properties in the speech-auditory cortex</head><p>Encoding models are prevalent methods to approach the neural coding of sensory perception <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40</ref> . Despite achieving success with lower-level acoustic-phonetic features <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref> , linear encoding models struggle with higher-order speech information, often failing to reveal information beyond acoustic stimulus encoding (Fig. <ref type="figure" target="#fig_6">5b,</ref><ref type="figure">e</ref>). Previous studies using activation contrasts or linear models have not found the language-specific contextual effects of acoustic-phonetic coding in local populations in the STG <ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b44">45</ref> , but DNN-based representations detect such language-specific coding in single STG electrodes (Fig. <ref type="figure" target="#fig_6">5</ref> and Extended Data Fig. <ref type="figure">7</ref>). To account for nonlinear transformations of pure acoustic cues in the auditory system, studies have included higher-order features, such as phonetic, phonemic, features, as predictors <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47</ref> . However, these feature representations rely on strong presumptions of hierarchical neural coding of these exact divisions, potentially missing intermediate representations in the nonprimary auditory cortex <ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49</ref> . Furthermore, these models posit the auditory system as a passive finite response filter, neglecting the prevalent non-onset recurrent activity in higher-order speech areas <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b49">50</ref> .</p><p>Traditional hierarchical models of neurobiology suggest that specific brain areas specialize in distinct representation levels and information is transformed in anatomically defined 'streams' (that is, sound to phoneme to syllable to word and semantics) <ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b50">51</ref> . Our results challenge this traditional view. Although we observed a transformation from spectrogram to phonetic features, instead of phonemes and syllables as discretely encoded representations, we found complex, distributed higher-order representations that also carry forward prosodic information that may originate at earlier auditory levels and that processing is highly context dependent in later layers of computation. These findings explain the existence of both phonetic-feature tuning <ref type="bibr" target="#b41">42</ref> and diverse 'lower-level' (onset, peak rate, frequency tuning) <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b45">46</ref> and 'higher-level' (context dependence, normalization, lexical effects) representations in the STG <ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNNs as computation models of the auditory pathway</head><p>Our results highlight two critical factors behind DNN models' superior performance over heuristic linear models with static speech features:</p><p>(1) DNN model nonlinearity-almost all DNN layers consistently outperformed feature TRF models, even in the auditory periphery. This is consistent with demonstrations of nonlinear processing in the auditory periphery <ref type="bibr" target="#b53">54</ref> . Despite comparable amounts of predictors (on the order of 10 2 ), DNNs learn nonlinear features for better speech representations.</p><p>(2) DNN models' dynamic temporal integration of phonological contextual information-this is especially pivotal for higher-order speech responses in the nonprimary auditory cortex. STG responses were better predicted using deeper DNN layers with extended delay-time windows. Simply using static nonlinear filters in CNN layers with an even longer delay-time window could not achieve similar prediction performance for STG responses (Fig. <ref type="figure">2</ref>). This indicates that specific dynamic temporal integration, aligned with the contextual information in speech and parametrized by computation models such as transformers or recurrent neural networks, is critical for characterizing STG speech responses. Dynamic contextual computations are also correlated with higher-level language processing in the cortical language network <ref type="bibr" target="#b54">55</ref> . Our findings suggest that the STG processes speech at dynamic timescales, possibly underpinning temporal binding of phonological sequences to form dynamic acoustic-phonetic and ultimately perceptual representations of speech <ref type="bibr" target="#b49">50</ref> .</p><p>Our results offer new insights into computations in the auditory pathway. In DNN models, model architecture determines the computation and representation capacity <ref type="bibr" target="#b55">56</ref> . We found that different computational architectures better correlate with different parts of the auditory pathway: the convolution layers in DNNs are apt for the auditory periphery and subcortical areas with locally resolved static nonlinear filters; deeper transformer-encoder and LSTM layers better fit the speech-auditory cortex, with more complex stimulus-dependent temporal dynamics than static spectrotemporal filters. These computational attributes emerge as signatures for respective parts of the auditory pathway: the auditory periphery and subcortical structures are characterized by ascending feedforward synaptic connections for rapid forward-filtering of signals <ref type="bibr" target="#b31">32</ref> , whereas the speech-auditory cortex has a multilayer architecture with reciprocal connections facilitating sustained computations similar to recurrence and attention <ref type="bibr" target="#b56">57</ref> . In contrast to prior cortex-centric studies, our study reveals speech-relevant computations spanning   the entire auditory pathway through the lens of DNNs for speech representation learning. This has major implications for interpreting the functions of the primary and nonprimary auditory cortical areas. Dynamic computations and representations showed limited contribution to predicting speech responses in the primary auditory cortex beyond the static convolutional filters (Figs. <ref type="figure">2</ref> and<ref type="figure" target="#fig_4">4</ref>). In contrast, prediction of sustained STG responses to speech strongly correlated with dynamic computations in DNNs (Figs. <ref type="figure">2</ref><ref type="figure" target="#fig_3">3</ref><ref type="figure" target="#fig_4">4</ref>). This discrepancy aligns with a recent study highlighting distinct phonological and complex sound processing in the STG versus tonotopic, narrow-tuned sound processing in the primary auditory cortex <ref type="bibr" target="#b33">34</ref> . The STG also receives direct thalamic inputs through the nontonotopic, nonlemniscal pathway <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> and does not appear to be solely dependent on the primary auditory cortex <ref type="bibr" target="#b60">61</ref> . Our findings challenge the primary auditory cortex's sole contribution to advanced computational models of speech processing, despite previous assumptions that it causally functions like the primary visual cortex in object recognition processing within the ventral stream <ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62</ref> . Notably, we should also point out that, owing to limited experiment time during awake surgeries, we did not evaluate the cross-language question regarding the HG in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervised seq2seq learning and the speech-auditory cortex</head><p>Our results demonstrate that self-supervised DNNs match or exceed the performance of more prevalent supervised models in predicting brain responses to speech. The training objective critically shapes DNN representations. Previous works have found that supervised discriminant learning, such as word classification <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18</ref> , leads to feature representation correlating with auditory neural responses. Our results are consistent with these findings. However, instead of using a discrete classification task, we show that a specific type of supervised seq2seq learning task, ASR, induces neurally correlated speech features. Furthermore, self-supervised learning, including contrastive and predictive learning, similarly produces matching representations aligning with STG responses to speech. For naturalistic speech perception, previous studies do not support discrete selective coding for word forms in the STG but rather a collection of local populations tuned to complex acoustic-phonetic cues and temporal landmarks in speech <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b62">63</ref> . Therefore, a single supervised task such as word decoding may not capture all computations and representations in the STG. Meanwhile, self-supervised learning yields richer representations beyond the requirement of pure speech recognition, such as prosodic information and speaker identity. Our results show that fine-tuning supervised ASR tasks on top of the unsupervised pretraining does not further improve the overall brain encoding performance in the STG. Conversely, we observed that the brain-prediction performance for the nonprimary auditory cortex decreased in the deep layers after supervised fine-tuning (Extended Data Fig. <ref type="figure" target="#fig_6">5</ref>).</p><p>From a computational modeling perspective, our results extend previous successes in using DNNs as models of sensory systems <ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b63">64</ref> . Recent studies have adopted end-to-end training of DNNs to predict neural responses <ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b65">66</ref> . Although this approach directly optimizes brain-prediction performance, a considerable amount of data is required for training. For instance, the seq2seq DNN models we used here have approximately 100 million parameters and were trained on ~1,000 h of speech for competitive performance <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref> . Collecting an equivalent amount of neural data is unfeasible within our clinical settings. Furthermore, owing to the nature of intracranial recordings, only a sparse sample (~100 electrodes) from the auditory cortex was available for each participant. As a result, the learned representations from a straight end-to-end optimization of brain activity may be biased by the individual difference in electrode sampling. Instead, we used a transfer learning paradigm, pretraining DNNs without any neural data as inputs, and demonstrated that speech representations learned by these DNN models are also transferable to the neural coding process in the auditory pathway. Importantly, the DNNs used in this study were all trained on a completely independent dataset from the one used for neural recordings. Moreover, unsupervised models abstain from explicit speech information or linguistic knowledge. Unlike classical computational models of speech perception, such as TRACE <ref type="bibr" target="#b8">9</ref> , assuming a strict acoustic-phonetic-lexical hierarchy and explicit top-down inference, our pure data-driven self-supervised models yield an emerging acoustic-phonetic hierarchy. The self-supervised models' analogous representation hierarchy to the human auditory system suggests that the two systems may share similar computations that extract critical statistical structures of speech.</p><p>Our results extend the current literature on using task-optimized pretrained DNN models to predict cortical auditory responses. Compared to the previous pioneering study by Kell et al., which mainly used fMRI recordings and CNN models pretrained on tasks such as word recognition <ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">64</ref> , our study offers new insights from models with different architectures and computational objectives. Coupled with use of intracranial electrophysiological recordings with high temporal resolution, our approach allows for analysis of dynamic temporal coding of speech as a rapidly time-varying signal. We also show hierarchical processing, as reported in previous studies; however, our results show that early processing also occurs in subcortical pathways.</p><p>Modern DNN models are complex dynamic systems influenced by factors such as architectures, hyperparameters and optimization procedures. Hierarchical CNNs deterministically enforce receptive field growth across layers; however, transformer encoders have no prior constraint on the hierarchy of temporal context-each attention head in each layer can extend attention to the entire sequence. Therefore, the ascending patterns of contextual attention in DNNs (Fig. <ref type="figure" target="#fig_4">4c</ref> and Extended Data Fig. <ref type="figure">8</ref>) are learned through data-driven optimization, reflecting intrinsic, speech-aligned computations. We have established a correlation between linguistically relevant attention and neural encoding model performance. Future research remains to be done to identify other potential factors and build causal links between specific DNN computations and brain encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our results suggest how different levels of speech representations emerge from hierarchical bottom-up recurrent or self-attentional operations and how these representations correlate with the auditory cortex. Omitted are top-down modules and cortical areas beyond the auditory cortex, such as the frontal areas. Therefore, it remains to be delineated how other areas in the language network interact with the auditory cortex, whether these interactions modulate local and populational representations of speech, and to what extent these interactions can be characterized by our proposed framework. Besides coverage, our analysis focused on the temporal dynamics within individual electrodes. Future work should address how DNN feature representations align with distributed population-level neurodynamics <ref type="bibr" target="#b66">67</ref> in the auditory cortex.</p><p>A potential limitation concerns the biological plausibility of the computational models used in this study. The transformer and LSTM models considered in this study are bidirectional and noncausal. This would complicate the analysis of precise temporal dynamics in speech sequences. We focused on learned feature representations rather than actual parametrizations and implementations of algorithms such as self-attention or the LSTM mechanism. We cannot assert that any of these computations are implemented in the cortex or that gradient-based learning mirrors brain mechanisms. Despite correlational evidence, formal fine-grained causal and ablation analyses remain to be conducted to investigate the detailed relationship between computational components in DNNs and model-predicted neural responses. However, it is promising that in silico models converge on a similar representational basis of speech as the brain, with a Article https://doi.org/10.1038/s41593-023-01468-4 learning algorithm that does not require millions of labeled examples and is a potentially strong candidate for a biologically plausible theory of sensory learning <ref type="bibr" target="#b25">26</ref> or higher-level language processing in general <ref type="bibr" target="#b26">27</ref> .</p><p>Owing to the relatively small number of participants tested, our statistical analyses were performed across electrodes and did not consider between-participant variability, thereby lacking interindividual generalization across the population. This limitation is common in intracranial studies and outweighed by the unique opportunity to record intracranially from human patients. Nonetheless, our results were largely consistent across participants (Extended Data Figs. 9 and 10). Future research could explore and validate these findings in larger and more diverse populations, as well as with a broader spectrum of AI models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Using a comparative approach, we show important representational and computational parallels between speech-learning DNNs and the human auditory pathway. From a neuroscientific perspective, data-driven computational models excel in extracting intermediate speech features from statistical structures, surpassing traditional feature-based encoding models. From the AI perspective, we unveil an avenue to understand the 'black box' representations in DNNs by comparing them to neural responses and selectivity. We show that modern DNNs may have converged on representations that approximate processing in the human auditory system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online content</head><p>Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41593-023-01468-4.</p><p>https://doi.org/10.1038/s41593-023-01468-4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The experimental protocol was approved by the institutional review boards at the University of California, San Francisco (UCSF), and Huashan Hospital, Fudan University. All participants provided written informed consent before undergoing testing. All patient data were stored and analyzed on computing servers within UCSF, and Meta AI Research performed DNN model pretraining using publicly available speech corpora, without access to patient data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Participants</head><p>This study included 12 monolingual participants (6 men and 6 women, aged 31-55 years, all right-handed) who were neurosurgical patients at either the UCSF Medical Center or Huashan Hospital. No statistical methods were used to predetermine sample sizes, but our sample sizes are similar to those reported in previous publications <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b51">52</ref> . Nine native English-speaking participants from UCSF (E1-E9) were either eloquent patients with brain tumors (four patients) undergoing awake language mapping as part of their surgery or patients with intractable epilepsy (five patients) implanted with high-density electrode grids for clinical monitoring of seizure activity (all with left-hemisphere coverage). We included only participants with tumors that had not invaded the auditory cortex. Three native Mandarin-speaking participants from Huashan Hospital (M1-M3) were eloquent patients with brain tumors undergoing awake language mapping as part of their surgery (all with left-hemisphere coverage). The placements of the grids were determined solely by clinical needs. All patients were informed (as detailed in the institutional review board-approved written consent document signed by the participants) that their participation in scientific research was completely voluntary and would not directly affect their clinical care. Additional verbal consent was also acquired at the beginning and during the breaks of each experimental session. Data collection and analysis were not performed blind to the conditions of the experiments. No participants were excluded from the analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental paradigm</head><p>During the experiments, the participants were instructed to passively listen to continuous speech stimuli. No other task was performed during passive listening. The acoustic stimuli used in this study consisted of natural, continuous speech in both American English and Mandarin. The English speech stimuli consisted of materials from the TIMIT dataset <ref type="bibr" target="#b35">36</ref> . The TIMIT set consisted of 499 English sentences selected from the TIMIT corpus, spoken by 402 different speakers (286 male and 116 female speakers). The sentences were separated by 0.4 s of silence. The task was divided into five blocks, with each block lasting ~5 min. The Mandarin speech stimuli were a subset of the Annotated Speech Corpus of Chinese Discourse (ASCCD) from the Chinese Linguistic Data Consortium <ref type="bibr" target="#b67">68</ref> , which included read texts of a variety of discourse structures, such as narrative and prose. The stimulus set consisted of 68 passages of Mandarin speech selected from the ASCCD corpus, spoken by ten different speakers (five male and five female speakers). The length of a single passage varied between 10 and 60 s. The passages were separated by 0.5 s of silence. The task was divided into six blocks, with each block lasting ~5 min.</p><p>Depending on their clinical conditions, all participants finished 3-11 blocks of all tasks. In particular, eight English-speaking participants (E1-E8) completed all five TIMIT blocks; E9 completed three TIMIT blocks; and the three Mandarin-speaking participants (M1-M3) completed two TIMIT blocks. Three English-speaking participants (E1-E3) and all three Mandarin-speaking participants (M1-M3) completed all six ASCCD blocks. E4 completed five ASCCD blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data acquisition and preprocessing</head><p>In all patients, the same types of high-density ECoG grids (manufactured by Integra or PMT) with identical specifications (4-mm center-to-center spacing and 1.17-mm exposed contact diameter) were placed on the lateral surface of the temporal lobe. Depending on the exact clinical need, the grid may have 32 (8 ? 4), 128 (16 ? 8) or 256 (16 ? 16) contact channels in total. In four patients (E6-E9), an additional 32-channel (8 ? 4) grid with 4-mm center-to-center spacing and 1.17-mm exposed contact diameter (Integra) was placed on the temporal plane in each patient. During experimental tasks, neural signals were recorded from the ECoG grids using a multichannel amplifier optically connected to a digital signal processor (Tucker-Davis Technologies). TDT Synapse software was used for data recording. The local field potential at each electrode contact was amplified and sampled at 3,052 Hz. The raw voltage waveform was visually examined, and channels containing signal variations too low to detect from noise or continuous epileptiform activity were removed. Time segments on remaining channels that contained electrical or movement-related artifacts were manually marked and excluded. The signal was then notch-filtered to remove line noise (at 60, 120 and 180 Hz for English-speaking participants and 50, 100 and 150 Hz for Mandarin-speaking participants) and rereferenced to the common average across channels sharing the same connector to the preamplifier.</p><p>The analytic amplitude of eight Gaussian filters (center frequency 70-150 Hz) was computed using the Hilbert transform. The high-gamma signal was taken as the average analytic amplitude across these eight bands. The signal was downsampled to 100 Hz. The tasks were divided into recording blocks of ~5-min length. The high-gamma signal was z-scored across the recording block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrode localization</head><p>For chronic monitoring cases, electrodes were localized by aligning preimplantation MRI scans and postimplantation computed tomography scans. For awake cases, high-density electrode grids were temporarily placed onto the temporal lobe during surgery to record local cortical potentials. The three-dimensional positions of the corners of the grid were recorded using a Medtronic neuronavigation system and then aligned with the preoperative MRI scan. Intraoperative photographs were used as references. The remaining electrodes were localized by interpolation and extrapolation from those points <ref type="bibr" target="#b68">69</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis software</head><p>All analyses were carried out using custom software written in Python and MATLAB. Custom MATLAB code was used for data preprocessing. The open-source scientific Python packages that we used included PyTorch, Fairseq, HuggingFace Transformers, NumPy, SciPy, pandas, librosa and scikit-learn. Cortical surface reconstruction was performed using FreeSurfer, and electrodes were coregistered using the Python package img-pipe. Praat 70 was used to extract pitch features. Figures were created with Matplotlib and Seaborn in Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biophysical models for the auditory periphery and midbrain</head><p>We used neuronal models of the midbrain and auditory periphery <ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> . They consisted of a phenomenological model of AN responses, with nonlinear properties such as rate saturation, adaptation and synchrony capture, and an extended same-frequency inhibition-excitation model of the IC, which included both band-pass and low-pass/band-reject IC cells. The synaptic outputs from 50 AN neurons with characteristic frequencies uniformly distributed on a log scale within 150-8,000 Hz were extracted as the AN signal. These synaptic-output signals were used as inputs to the two different types of midbrain neurons in the IC area, which resulted in 50 band-pass IC neurons and 50 low-pass/band-reject IC cells.</p><p>For each speech sentence, the raw waveform was sent into the model as the input, and the corresponding response sequences from AN and IC cells were extracted and downsampled to 100 Hz to match the high-gamma signals from the cortex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definitions of acoustic, phonetic and prosodic features</head><p>We used a heuristic set of 208 features as the baseline prediction model (161 spectrogram, 13 phonetic, 31 pitch/prosodic and 3 envelope features).</p><p>https://doi.org/10.1038/s41593-023-01468-4</p><p>The spectrogram features of speech were calculated using a short-time Fourier transform, with 161 frequency components ranging from 0 to 8 kHz in log scale.</p><p>The phonetic features were 13-dimensional binary time series similar to those in previous works <ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42</ref> . These features describe single phonemes as a combination of places of articulation (dorsal, coronal, labial), manners of articulation (plosive, fricative, nasal) and voicing of consonants, as well as the place of the vowel (high, mid, low, front, back) and indicator of consonant/vowel. Pitch features, including absolute pitch, speaker-normalized relative pitch and pitch change, were extracted in the same way as in our previous work <ref type="bibr" target="#b38">39</ref> . We also extracted a binary variable indicating when pitch values were present, suggesting voicing in the speech. The fundamental frequency (F 0 ) was calculated using the autocorrelation method in Praat and corrected for halving and doubling errors. Absolute pitch was defined as the natural logarithm of F 0 values in hertz. Relative pitch was computed by z-scoring the absolute pitch values (log(F 0 )) within each sentence/passage (within-speaker). Pitch change was computed by taking the first-order derivative (finite difference) in time for log(F 0 ). We discretized absolute pitch, relative pitch and pitch change into ten bins equally spaced from the 2.5th percentile value to the 97.5th percentile value. The bottom and top 2.5% of the values were placed into the bottom and top bins, respectively. As a result, absolute pitch, relative pitch and pitch change were represented as three 10-dimensional binary feature vectors. For nonpitch periods, these feature vectors would all have a value of zero for all dimensions.</p><p>Envelope features included intensity, sentence onset and peak rate. Intensity is a continuous scalar sequence representing the envelope of speech. Sentence onset is a binary feature with a value of 1 at the onset of the first timestamp of the first phoneme in each sentence and 0 elsewhere. Peak rate was computed as previously described <ref type="bibr" target="#b45">46</ref> (that is, using a sparse time series of local peaks extracted from the first-order derivative of the amplitude envelope of speech).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding models</head><p>We used time-delayed linear encoding models known as TRF models <ref type="bibr" target="#b9">10</ref> . TRF models allow us to predict neural activity based on stimulus features in a window of time preceding neural activity. In particular, we fit the linear model y (t) = ? F f=1 ? T ?=0 ? T f (?)x f (t -?) + ? for each electrode, where y is the high-gamma activity recorded from the electrode, x f (t - ?) is the stimulus representation vector of feature set f at time t - ?, ? f (?) is the regression weight for feature set f at time lag ?, and ? represents the Gaussian noise.</p><p>To prevent model overfitting, we used L2 regularization and cross-validation. Specifically, we divided the data into three mutually exclusive sets representing 80%, 10% and 10% of samples. The first set (80% of samples) was used as the training set. The second set was used to optimize the L2 regularization hyperparameter, and the final set was used as the test set. We evaluated the models using the correlation between the actual and predicted values of neural activity on held-out data. We performed this procedure five times, and the performance of the model was calculated as the mean performance across all testing sets.</p><p>The performance of each encoding model on an individual recording site (electrode/neuron) was quantified as the (normalized) BPS. In particular, BPS = R 2 model /R 2 baseline , where R 2 model is the R 2 value of the prediction model based on cross-validation and R 2 baseline is the R 2 value of the baseline model (full-feature set) for the same electrode/neuron based on cross-validation. A BPS of 1 indicates that the proposed model performs as well as the baseline model, and a BPS of &gt;1 suggests that the proposed model outperforms the baseline model.</p><p>For the STRF model and the baseline full-feature model, we used a fixed delay-time window of 400 ms. For all DNN-based encoding models, we varied the time window length from 0 (using only the current timeframe) to 400 ms and selected the optimal window length based on cross-validation results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noise-ceiling estimation</head><p>In one of the five TIMIT blocks (TIMIT5), ten sentences were repeated ten times. The noise ceiling in each electrode was computed using this repeat block. Let s <ref type="bibr">(k)</ref> i, j ? ? T i be the recorded signal in electrode k for the j th repetition of the ith sentence, where i = 1, ?, 10; j = 1, ?, 10; and T i is the length of the ith sentence. We used a cross-validation strategy to estimate the noise ceiling. Specifically, we computed the averaged response from nine repetitions and correlated the averaged response to the left-out trial. The averaged Pearson correlation coefficient across all repetitions was used as the estimated noise ceiling for this electrode: </p><formula xml:id="formula_1">r (k) =</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electrode selection</head><p>To select speech-responsive electrodes and avoid numerical instability of the BPS caused by dividing the very small R 2 values of the baseline model, we included only speech-responsive electrodes in our analysis. The responsive threshold was set as R 2 baseline &gt; 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNNs: model architectures</head><p>We used five different DNN models: HuBERT 15 , Wav2Vec 2 unsupervised version <ref type="bibr" target="#b13">14</ref> , Wav2Vec 2 ASR supervised version <ref type="bibr" target="#b13">14</ref> , HuBERT supervised version and Deep Speech 2 (ref. 13).</p><p>The HuBERT and Wav2Vec 2 models share the same architecture, consisting of a convolutional waveform encoder and a transformer BERT encoder <ref type="bibr" target="#b70">71</ref> . The network uses 16-kHz raw sound waveforms as the input. The convolution encoder consisted of seven 512-channel, one-dimensional convolution layers with strides of 5, 2, 2, 2, 2, 2, 2 and kernel widths of 10, 3, 3, 3, 3, 2, 2. The convolution encoder downsampled the input to a 512-dimensional feature sequence at a 20-ms framerate (50 Hz). The output of the convolution encoder, noted as 'CNN out', was projected to a 768-dimensional space through a linear layer, noted as 'CNN proj', and fed into the BERT encoder. The architecture of the transformer encoder is similar to that of the BERT base model <ref type="bibr" target="#b70">71</ref> , which consists of 12 identical transformer-encoder blocks, with an embedding dimension of 768, intermediate feedforward layer dimension of 3,072 and 12 attention heads in each layer.</p><p>The Deep Speech 2 model consists of a convolutional spectrogram encoder and a recurrent encoder. This model uses the spectrogram of the raw audio signal as the input. The spectrogram was computed using a short-time Fourier transform with 161 frequency components from 0 to 8 kHz, time window size of 0.02 s and a stride size of 0.01 s. The convolution encoder consisted of two 32-channel, two-dimensional convolution layers, with corresponding two-dimensional strides of 2, 2 and 2, 1 and kernel sizes of 41, 11 and 21, 11. The final output of the convolution encoder was a 1,312-dimensional vector at a 20-ms framerate (50 Hz). The recurrent encoder consisted of five bidirectional LSTM layers, each with a hidden-state size of 1,024. The output of the last LSTM layer was projected to a 29-dimensional feature space by a linear projection layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNNs: unsupervised training</head><p>The HuBERT model was trained using a self-supervised paradigm of masked prediction <ref type="bibr" target="#b14">15</ref> . The unsupervised k-means clustering algorithm was used to generate categorical labels of the acoustic speech signal, mimicking pseudophonetic labels. During training, a random subset of segments in each sentence was selected and masked. After masking, the sequence was passed through the network to generate a feature-embedding sequence. The embedded sequence was then projected to compute cross-entropy loss over discrete code categories.</p><p>The Wav2Vec 2 unsupervised model was trained using a selfsupervised contrastive learning paradigm <ref type="bibr" target="#b13">14</ref> . This model uses a quantization module to discretize the output sequence of the convolution encoder. Similar to the HuBERT model, a random subset of speech https://doi.org/10.1038/s41593-023-01468-4 segments was selected and masked. The final output of the transformer encoder and the quantized representation from the convolution encoder were used to compute the contrastive loss. Specifically, for the target output at a given masked timestep, a random set of distractors was selected from other masked portions in the same sentence. The contrastive loss maximizes the distance between the target and the discretized output in the distractors while minimizing the distance between the target and the discretized output at the target timestep.</p><p>Both English models were trained on the 960-h LibriSpeech corpus <ref type="bibr" target="#b36">37</ref> . For the cross-language comparison, we also trained a HuBERT Mandarin model on the 755-h MAGICDATA corpus of Mandarin speech <ref type="bibr" target="#b71">72</ref> , using the same procedure as in the English HuBERT model and starting from random initializations.</p><p>We trained both the English and Mandarin self-supervised models for two iterations on 32 graphics processing units (GPUs), with a batch size of at most 87.5 s of audio per GPU. The first iteration was trained for 250,000 steps, whereas the second iteration was trained for 400,000 steps using labels generated by clustering the output of the sixth transformer layer in the first iteration. Training for 100,000 steps took ~9.5 h. The Adam optimizer was used with epsilon = 1 ? 10 -6 , beta = (0.9, 0.98) and the learning rate ramped linearly from zero to the peak learning rate of 5 ? 10 -4 for the first 8% of the training steps and then decayed linearly back to zero.</p><p>Data augmentation was applied between the CNN and transformer modules. Temporal masks spanned ~200 ms, with a 0.08 probability of selecting a timestep as the beginning of a mask. We also masked channels by choosing several channels as starting indices and then covered the following 64 channels. Temporal and channel spans may overlap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DNNs: supervised training</head><p>The Wav2Vec 2 supervised model was fine-tuned from the unsupervised pretrained initialization <ref type="bibr" target="#b13">14</ref> . A linear projection layer was used to project the output of the transformer encoder onto 29 classes representing characters, spaces and word boundaries. The model was optimized by minimizing a connectionist temporal classification (CTC) loss <ref type="bibr" target="#b72">73</ref> . During fine-tuning, the weights of the convolution encoder were frozen and only the transformer layers were fine-tuned.</p><p>The HuBERT/Wav2Vec 2 supervised model was trained using a CTC loss. The entire weights of the CNN and transformer layers were trained altogether from random initializations.</p><p>The Deep Speech 2 model was trained, from random initializations, for the best ASR performance by minimizing the CTC loss <ref type="bibr" target="#b12">13</ref> . The 960-h LibriSpeech corpus was used for the supervised training of all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention pattern analysis</head><p>For a given speech sentence, assume that the embedding sequence in a transformer layer was of length T (c 1 , ?, c T ), the phoneme boundaries were indexed as p 1 , ?, p m and the syllable boundaries were indexed as s 1 , ?, s n . The attention templates were defined as follows:</p><p>1. Attention to the current phoneme, phoneme(0): A ph(0) ? ? T?T , A ph(0) (i, j) = 1 if p k ? i &lt; p k+1 and p k ? j &lt; p k+1 for any k; A ph(0) (i, j) = 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Attention to the previous phoneme, phoneme(-1):</head><p>A ph(-1) ? ? T?T , A ph(-1) (i, j) = 1 if p k ? i &lt; p k+1 and p k-1 ? j &lt; p k for any k; A ph(-1) (i, j) = 0 otherwise. 3. Attention to the second to the previous phoneme, phoneme(-2):</p><formula xml:id="formula_2">A ph(-2) ? ? T?T , A ph(-2) (i, j) = 1 if p k ? i &lt; p k+1 and p k-2 ? j &lt; p k-1 for any k; A ph(-<label>2</label></formula><p>) (i, j) = 0 otherwise. 4. Attention to the current syllable, syllable(0): A sy(0) ? ? T?T , A sy(0) (i, j) = 1 if s k ? i &lt; s k+1 and s k ? j &lt; s k+1 for any s; A ph(0) (i, j) = 0 otherwise. To exclude the current phoneme from the current syllable, we used A ? sy(0) = A sy(0) -A ph(0) as the template.</p><p>5. Attention to the previous syllable, syllable(-1): A sy(-1) ? ? T?T , A sy(-1) (i, j) = 1 if s k ? i &lt; s k+1 and s k-1 ? j &lt; s k for any k; A sy(-1) (i, j) = 0 otherwise. 6. Attention to the second to the previous syllable, syllable(-2):</p><p>A sy(-2) ? ? T?T , A sy(-2) (i, j) = 1 if s k ? i &lt; s k+1 and s k-2 ? j &lt; s k-1 for any k; A sy(-2) (i, j) = 0 otherwise.</p><p>For each sentence, we computed the attention matrix W xy at the xth layer and yth attention head. The correlation coefficient corr (W xy , A q ) was computed for all templates. Moreover, the AS for layer x and template q was computed as the average over all attention heads and all speech sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STG clustering analysis</head><p>To identify functional clusters in the STG, we used a similar clustering approach as described previously <ref type="bibr" target="#b37">38</ref> . Note that, instead of using raw single-trial responses, we averaged across sentences and used only averaged time series. Specifically, we applied convex non-negative matrix factorization (convex NMF) <ref type="bibr" target="#b73">74</ref> to decompose the averaged high-gamma time series across all STG electrodes. Specifically, X ? X = FG T and F = XW, where X (T time points ? p electrodes) is the ERP matrix for different STG electrodes averaged across all sentences, G (p electrodes ? k clusters) represents the spatial weight of each electrode for each cluster and W (p electrodes ? k clusters) represents weights applied to the electrode time series. In particular, for X, we considered all 144 speech-responsive STG electrodes across all nine participants and computed the averaged ERP response for each electrode across all 599 TIMIT sentences. We evaluated different k values ranging from 1 to 10 and computed the percentage of variance explained by NMF models with different k values. We chose the number of clusters at the elbow of the variance curve (Extended Data Fig. <ref type="figure">6</ref>), which yielded k = 2, and explained 94% of the total variance.</p><p>After choosing the optimal number of clusters, each electrode was assigned to a cluster with the maximum cluster weight G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistical testing</head><p>We used paired t tests (one-sample) to evaluate and compare the performance of DNN-based encoding models and the baseline models.</p><p>In particular, the performance of different models was evaluated and compared on individual electrodes/units in each area. The d.f. of the t statistic was determined by the total number of individual electrodes/ units in each area. Two-tailed P values were used to determine statistical significance. We also evaluated the effects in single-participant results (Extended Data Figs. 9 and 10). Data distribution was assumed to be normal, but this was not formally tested.</p><p>We used permutation tests to evaluate the statistical significance of the cross-layer correlations between BPSs and ASs in each DNN-layer prediction model for Figs. <ref type="figure" target="#fig_4">4</ref><ref type="figure" target="#fig_6">5</ref><ref type="figure">6</ref>. In particular, we randomly shuffled speech sentences 800 times to disrupt the speech-neural correspondence, and reran the corresponding encoding models to compute R 2 and obtain the surrogated distribution of the correlation coefficients. One-sided P values were estimated using this empirical distribution of correlation coefficients. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 |</head><label>1</label><figDesc>Fig. 1 | Overall framework for comparing representations in DNNs and the auditory pathway. The architecture of a family of DNN models, HuBERT/ Wav2Vec 2, is illustrated on the left. The auditory pathway is illustrated on the right, with highlighted areas indicating the locations of the recorded/simulated electrophysiology signals. The same natural speech stimuli were presented to both the human participants and the DNN models, and the internal activations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 |</head><label>3</label><figDesc>Fig. 3 | Functional subpopulations in the STG correlate with different contextual representation layers in DNNs. a, Anatomical locations of all speech-responsive electrodes, mapped onto a common cortical space in the enlarged image of the boxed region. Different colors indicate different functional clusters. b, Averaged event-related potential (ERP) of each functional cluster. All time points were aligned with sentence onsets and normalized to the restingstate baseline (mean ? s.e.m.). c, Normalized BPSs of the encoding models based on every single layer in HuBERT for each functional cluster (maximum over delay window lengths). Red star indicates the layer with the highest score; black dot indicates other layers that were not statistically different from the best layer (P &gt; 0.05, paired t test, two-sided; n = 83 electrodes for cluster 1, n = 61 electrodes for cluster 2). Box plot shows the first and third quantiles across electrodes (orange line indicates the median; black line indicates the mean value; and whiskers indicate the 5th and 95th percentiles). Horizontal gray line: the performance of the full acoustic-phonetic feature baseline model. d, Histogram of the optimal delay windows corresponding to models in c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 |</head><label>4</label><figDesc>Fig. 4 | Context-dependent computations explain brain correspondence across layers in the DNN. a, Sample speech sentence text, waveform and phonemic annotations. The segmentations of phonemic and syllabic contexts to the current timeframe (black arrow) are marked in different colors: phoneme(0), current phoneme (gray); phoneme(-1), previous phoneme (purple); phoneme(-2), second to the previous phoneme (blue); syllable(0), current syllable (excluding the current phoneme; green); syllable(-1), previous syllable (orange); syllable(-2), second to the previous syllable (red). b, Template attention-weight matrices for different contextual structures as shown in a. 'Query' indicates the target sequence. 'Key' indicates the source sequence. Colored blocks correspond to different contexts. c, Averaged AS (Pearson's</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Articlehttps://doi.org/10.1038/s41593-023-01468-4 brain-prediction performance and better correlation with contextual attention for Mandarin speech in native Mandarin speakers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 |</head><label>5</label><figDesc>Fig. 5 | Cross-language encoding comparisons reveal language-specific representations and computations aligned between the DNN and the STG. a, Schematic of the cross-language paradigm. Both English (darker shade) and Mandarin (lighter shade) speech samples were fed into models pretrained on English or Mandarin. The extracted representations were used to predict neural responses recorded in the STG of native English speakers or native Mandarin speakers when they listened to the corresponding speech. b, Distribution of the prediction R 2 values of the linear STRF model in STG electrode recordings from native English speakers using English or Mandarin speech. Two-sided paired t test. c, Averaged normalized BPS of the encoding model based on every single layer in the English-pretrained HuBERT model in native English speakers when they listened to English versus Mandarin speech. *P &lt; 0.05, **P &lt; 0.01,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Articlehttps://doi.org/10.1038/s41593-023-01468-4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>syllabic and lexical *** *** *** *** *** *** *** *** *** *** *** *** *** *** * ** *** *** *** *** *** *** *** *** *** *** *** *** English speakers with the English-pretrained model English speech Mandarin speech Native Mandarin speakers with the Mandarin-pretrained model Article https://doi.org/10.1038/s41593-023-01468-4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 Fig. 6 |</head><label>36</label><figDesc>Fig. 6 | Representations in neural networks demonstrate an acoustic-tophonetic transformation hierarchy yet preservation of prosodic cues through DNN layers. a, Distribution of the unique variance explained by each set of features across units in each DNN layer. n = 512 units in the last CNN layer and 768 units in each transformer layer. Box plot shows the first and third quantiles across electrodes (orange line indicates the median; black line indicates the mean value; and whiskers indicate the 5th and 95th percentiles). b, Top row, correlation between the BPS and the unique variance explained by spectrogram features in each layer; bottom row, correlation between the BPS and the unique variance explained by phonetic features in each layer. Each panel corresponds to one area, with each area represented by a different color (n = 14 layers, two-sided t test). Red fonts indicate significant positive correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>? , and the R 2 value was the square of the Pearson correlation coefficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>4 Extended Data Fig. 5 |</head><label>45</label><figDesc>https://doi.org/10.1038/s41593-023-01468-4 made available from the corresponding author upon request. Source data are provided with this paper. Extended Data Fig. 2 | Acoustic phonetic feature encoding model. Example of feature extraction for a sample sentence, read by a male speaker:'It is well liked by the children and faculty.' From top to bottom: 1) raw waveform; 2) highgamma (z-scored) activity at an example electrode; 3) Mel-scaled spectrogram; 4) intensity of voicing; 5) sentence onset; 6) time course of peak rate; 7) absolute pitch (binned into 10 bins); 8) relative pitch (binned into 10 bins); 9) pitch change (binned into 10 bins); 10) phonetic features. Extended Data Fig. 3 | Temporal profile, raw encoding R 2 , and noise ceiling. a) The temporal receptive field (absolute beta weights of the spectrotemporal encoding model) of each individual speech-responsive unit/electrode. (Gray shaded areas indicate random permuted distributions of the averaged TRF across all units/electrodes, same as Fig. 2c. b) The histogram of the optimal delay window lengths corresponding to models in Fig. 2a. c) raw prediction R2 of different models. Dashed line: noise ceiling estimated from 10 repeated trials. d) Distribution of the normalized brain prediction score of each model across individual units/electrodes. Dashed line: noise ceiling estimated from 10 repeated trials. Red star (*) indicates the best model for each area, black dot (.) indicates other models that are not statistically different from the best model (p &gt; 0.05, two-sided paired t-test; n = 50 neurons for AN; n = 100 neurons for IC; n = 53 electrodes for HG; n = 144 electrodes for STG). Box plot shows the first and third quantiles across electrodes, orange line indicates the median, black line is the mean value, and whiskers indicate the 5 th and 95 th percentiles. https://doi.org/10.1038/s41593-023-01468-4 Extended Data Fig. 4 | Comparing DNN encoding performance across different convolutional layers in the HuBERT model for AN and IC neurons. a) The brain prediction score of the best-performing neural encoding model based on each single layer (the 4 th -7 th CNN layers and the final convolution output) in the HuBERT model model (maximum over delay window length). b) averaged brain prediction score at CNN4 -CNN7 in the HuBERT model with different delay window lengths. Note that the sampling rates vary at different layers: CNN4-400 Hz, CNN5 -200 Hz, CNN6 -100 Hz, CNN7 &amp; CNN out -50 Hz. AN: light shaded bars; IC: dark shaded bars. Box plot shows the first and third quantiles across electrodes, orange line indicates the median, gray line is the mean value, and whiskers indicate the 5 th and 95 th percentiles. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, two-sample t-test, two-sided, n = 50 unique neurons for AN, n = 100 unique neurons for IC. https://doi.org/10.1038/s41593-023-01468-Comparing DNN encoding performance across different models. The distribution of the normalized brain prediction score of the best-performing neural encoding model based on each single layer in the DNN model (maximum over delay window length) across individual electrodes. a) Wav2Vec 2.0 Unsupervised (SSL) model; b) Wav2Vec 2.0 Supervised (SSL + FT) model; c) HuBERT Unsupervised (SSL) model; d) HuBERT pure supervised model. Each column corresponds to one area in the auditory pathway, from left to right AN/IC/HG/STG. Magenta bars indicate CNN output layers, cyan bars indicate Transformer layers. Red star (*) indicates the best model for each area, black dot (.) indicates other models that are not statistically different from the best model (p &gt; 0.05, two-sided paired t-test). Box plot shows the first and third quantiles across electrodes, orange line indicates the median, black line is the mean value, and whiskers indicate the 5 th and 95 th percentiles. https://doi.org/10.1038/s41593-023-01468-4 Extended Data Fig. 7 | Cross-model encoding comparisons reveal languagespecific representation and computations aligned between DNN and STG. a) Schematic of the cross-model paradigm. Both English (lighter color) and Mandarin (darker color) speech were fed into models pretrained on English or Mandarin. The extracted representations were used to predict neural responses recorded in STG from native English speakers or native Mandarin speakers when they listened to the corresponding speech (English speaker listened to English; Mandarin speaker listened to Mandarin). b) The distribution of normalized brain prediction score of the encoding model based on every single layer in Englishpretrained HuBERT model (light shaded bars) versus Mandarin-pretrained model (dark shaded bars) in native English speakers when listening to English speech. * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, paired two-sided t-test; n = 57 electrodes in STG. c) The AS-BPS correlation across layers in English-pretrained (light shaded bars) and Mandarin-pretrained (dark shaded bars) HuBERT model with STG in native English speakers (Pearson's correlation, * p &lt; 0.05, permutation test, onesided). Each panel corresponds to one type of attention pattern. (See also Fig. 4). d-e) Same as b-c, but using recordings from STG in native Mandarin speakers when listening to Mandarin speech (n = 61 electrodes in STG). The performance of English-pretrained model (light shaded bars) and Mandarin-pretrained HuBERT models (dark shaded bars) are compared. f-j) same as a-e, but for native English speakers or native Mandarin speakers when they listened to speech in the other language (English speaker listened to Mandarin; Mandarin speaker listened to English). Box plot shows the first and third quantiles across electrodes, orange line indicates the median, gray line is the mean value, and whiskers indicate the 5th and 95th percentiles. https://doi.org/10.1038/s41593-023-01468-4 Extended Data Fig. 8 | Analysis on attentions in HuBERT model. a) The averaged attention distance in each Transformer encoder layer of HuBERT model (mean ? s.d., n = 499 independent sentences). The averaged attention distance is computed as token distance weighted by attention weights, averaging across all attention heads and across all tokens. The attention weights in each layer are aggregated over previous layers using attention rollout. b) The AS-BPS correlation across layers in random model versus English-pretrained model for STG in native English speakers (Pearson's correlation, * p &lt; 0.05, permutation test, one-sided). Each panel corresponds to one type of attention pattern. (See also Fig. 4). c) The shifted AS-BPS correlation (with attention matrix shuffled in blocks) across layers versus unshifted original AS-BPS in English-pretrained model for STG in native English speakers (Pearson's correlation, * p &lt; 0.05, permutation test, one-sided). Each panel corresponds to one type of attention pattern. https://doi.org/10.1038/s41593-023-01468-4 Extended Data Fig. 9 | Comparing DNN encoding performance for STG and HG in individual subjects. This is supplement to Fig. 2a. The averaged normalized prediction score on single layer encoding models in the HuBERT model (maximum over delay window length). Three representative layers are used: the CNN output, the first Transformer layer, and the 10 th layer (the optimal layer shown in Fig. a) E1-E9 are the nine native English speakers. Each dot in the swarm plot represents one single electrode in STG (only speech responsive electrodes are plotted). Box plots show the 25/50/75 quantiles, whiskers indicate the 5th and 95th percentiles. The black statistical significance markers are determined using two-tailed paired t-test between different layers (* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, n.s. p &gt; 0.05, n = 17, 25, 9, 6, 22, 42, 10, 10, 3 individual electrodes.). b) Same as a, for the five participants with HG coverage (E5-E9); n = 5, 16, 12, 6, 14 individual electrodes. Extended Data Fig. 10 | Comparing DNN encoding performance across different languages in STG of individual subjects. This is supplement to Fig. 5c. a) The distribution of normalized brain prediction score on single layer encoding models in the English-pretrained HuBERT model (maximum over delay window length) over individual STG electrodes. The 10th (the optimal layer shown in Fig. 2) is used. E1-E4, M1-M3 are the four native English speakers and three native Mandarin speakers that listen to both English and Mandarin speech. Each dot in the swarm plot represents one single electrode in STG (only speech responsive electrodes are plotted). Box plots show the 25/50/75 quantiles, whiskers indicate the 5th and 95th percentiles. The black statistical significance markers are determined using two-tailed paired t-test between different layers (* p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001, n.s. p &gt; 0.05; n = 17, 25, 9, 6 individual electrodes). b) Same as a, for Mandarin speakers M1-M3, n = 26, 15, 20 individual electrodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-24.png" coords="18,40.43,48.96,520.08,254.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-26.png" coords="20,40.37,48.96,520.20,414.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-27.png" coords="21,40.43,48.96,520.08,172.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-30.png" coords="24,40.67,48.96,519.60,390.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-31.png" coords="25,40.67,48.96,519.60,216.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-32.png" coords="26,40.67,48.96,519.60,429.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 | Summary of network training objectives and architectures</head><label>1</label><figDesc></figDesc><table><row><cell>Models</cell><cell>Unsupervised objective</cell><cell cols="2">Supervised objective Architecture</cell><cell>ASR task performance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(word error rate (%))</cell></row><row><cell>HuBERT 15</cell><cell>Masked prediction</cell><cell>NA</cell><cell>7 CNN layers + 12</cell><cell>6 (after fine-tuning)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>transformer-encoder layers</cell><cell></cell></row><row><cell>Wav2Vec 2 (unsupervised) 14</cell><cell>Contrastive learning</cell><cell>NA</cell><cell>7 CNN layers + 12</cell><cell>6.3 (after fine-tuning)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>transformer-encoder layers</cell><cell></cell></row><row><cell>Wav2Vec 2 (supervised) 14</cell><cell>Contrastive learning</cell><cell>ASR</cell><cell>7 CNN layers + 12</cell><cell>6.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell>transformer-encoder layers</cell><cell></cell></row><row><cell>HuBERT/Wav2Vec 2</cell><cell>NA</cell><cell>ASR</cell><cell>7 CNN layers + 12</cell><cell>7.4</cell></row><row><cell>(pure supervised)</cell><cell></cell><cell></cell><cell>transformer-encoder layers</cell><cell></cell></row><row><cell>Deep Speech 2 13</cell><cell>NA</cell><cell>ASR</cell><cell>3 CNN layers + 5 LSTM layers</cell><cell>8.00</cell></row></table><note><p>https://doi.org/10.1038/s41593-023-01468-4</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Fig. 2 | Hierarchy of layers in DNNs correlates with the AN-midbrain-STG ascending auditory pathway. a, Normalized</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">AN (model)</cell><cell></cell><cell>3.5</cell><cell></cell><cell></cell><cell></cell><cell cols="6">IC (model)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">HG (ECoG)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">STG (ECoG)</cell></row><row><cell></cell><cell></cell><cell>1.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BPS (a.u.)</cell><cell>1.6 1.4 1.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.5 2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4 1.2 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.4 1.2 1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="7">CNN Tr. 0 Tr. 1 Tr. 2 Tr. 3 Tr. 4 Tr. 5 Tr. 6 Tr. 7 Tr. 8 Tr. 9 Tr. 10 Tr. 11 Tr. 12</cell><cell></cell><cell cols="13">CNN Tr. 0 Tr. 1 Tr. 2 Tr. 3 Tr. 4 Tr. 5 Tr. 6 Tr. 7 Tr. 8 Tr. 9 Tr. 10 Tr. 11 Tr. 12</cell><cell cols="12">CNN Tr. 0 Tr. 1 Tr. 2 Tr. 3 Tr. 4 Tr. 5 Tr. 6 Tr. 7 Tr. 8 Tr. 9 Tr. 10 Tr. 11 Tr. 12</cell><cell cols="10">CNN Tr. 0 Tr. 1 Tr. 2 Tr. 3 Tr. 4 Tr. 5 Tr. 6 Tr. 7 Tr. 8 Tr. 9 Tr. 10 Tr. 11 Tr. 12</cell></row><row><cell>TRF weights</cell><cell>(a.u.)</cell><cell>0.8 0.2</cell><cell></cell><cell cols="5">Average TRF Permutations</cell><cell></cell><cell>0.10 0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0004 0.0002</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.0004 0.0002</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">300</cell><cell cols="3">200</cell><cell>100</cell><cell>0</cell><cell></cell><cell cols="3">300</cell><cell></cell><cell cols="3">200</cell><cell></cell><cell cols="3">100</cell><cell>0</cell><cell></cell><cell cols="3">300</cell><cell></cell><cell cols="2">200</cell><cell></cell><cell cols="3">100</cell><cell>0</cell><cell></cell><cell cols="3">300</cell><cell></cell><cell cols="2">200</cell><cell></cell><cell cols="2">100</cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Delay time (ms)</cell><cell></cell><cell></cell><cell></cell><cell cols="10">Delay time (ms)</cell><cell></cell><cell></cell><cell></cell><cell cols="9">Delay time (ms)</cell><cell></cell><cell></cell><cell></cell><cell cols="8">Delay time (ms)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">LSTM</cell><cell cols="3">Transformer</cell><cell cols="7">Transformer</cell><cell></cell><cell>Transformer</cell><cell></cell><cell cols="3">CNN</cell><cell></cell><cell></cell><cell cols="4">Transformer</cell></row><row><cell></cell><cell></cell><cell cols="8">Spectrogram Full features</cell><cell cols="8">supervised CNN-SSL</cell><cell cols="5">supervised</cell><cell cols="2">SSL + FT</cell><cell></cell><cell></cell><cell></cell><cell cols="3">SSL</cell><cell></cell><cell></cell><cell></cell><cell>supervised</cell><cell cols="5">random</cell><cell></cell><cell></cell><cell cols="3">random</cell></row><row><cell></cell><cell></cell><cell>2.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BPS (a.u.)</cell><cell>1.5 1.0 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.25 1.00 0.75 0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.25 1.00 0.75 0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Spect.</cell><cell>Full feat. DS2 CNN</cell><cell>W2V CNN</cell><cell>HuB. CNN DS2 LSTM</cell><cell>W2V-A Tr.</cell><cell>W2V Tr. HuB. Tr.</cell><cell>HuB. CNN Sup. HuB. Tr. Sup. HuB. CNN Ran. HuB. Tr. Ran.</cell><cell></cell><cell>Spect.</cell><cell>Full feat.</cell><cell>DS2 CNN</cell><cell>W2V CNN</cell><cell>HuB. CNN</cell><cell>DS2 LSTM</cell><cell>W2V-A Tr.</cell><cell>W2V Tr.</cell><cell>HuB. Tr.</cell><cell>HuB. CNN Sup.</cell><cell>HuB. Tr. Sup.</cell><cell>HuB. CNN Ran.</cell><cell>HuB. Tr. Ran.</cell><cell>Spect.</cell><cell>Full feat.</cell><cell>DS2 CNN</cell><cell>W2V CNN</cell><cell>HuB. CNN</cell><cell>DS2 LSTM</cell><cell>W2V-A Tr.</cell><cell>W2V Tr.</cell><cell>HuB. Tr.</cell><cell>HuB. CNN Sup.</cell><cell>HuB. Tr. Sup.</cell><cell>HuB. Tr. Ran.</cell><cell>Spect.</cell><cell>Full feat.</cell><cell>DS2 CNN</cell><cell>W2V CNN</cell><cell>HuB. CNN</cell><cell>DS2 LSTM</cell><cell>W2V-A Tr.</cell><cell>W2V Tr.</cell><cell>HuB. Tr.</cell><cell>HuB. CNN Sup.</cell><cell>HuB. Tr. Sup.</cell><cell>HuB. Tr. Ran.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="14">BPS of the best-performing neural</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="22">encoding model based on every single layer in the HuBERT model (maximum</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="23">over delay window lengths). Magenta bars indicate CNN output layers; cyan bars</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="21">indicate transformer layers. Red star indicates the best model for each area;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="22">black dot indicates other models that were not statistically different from the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="23">best model (P &gt; 0.05, two-sided paired t test; n = 50 neurons for the AN, n = 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">neurons for the IC, n =</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>53 electrodes for the</head><label></label><figDesc></figDesc><table /><note><p><p>HG, n = 144 electrodes for the STG). From left to right: AN, IC, HG and STG (same for each row in b and c). b, Averaged TRF weights (absolute beta weights of the spectrotemporal encoding model) in speech-responsive units/electrodes of each area (mean ? s.e.m.; light-shaded areas indicate random permuted distributions; black dots indicate time points with TRF weights significantly higher than the chance level; t test, two-sided P &lt; 0.05, Bonferroni-corrected for 20 time points). c, Normalized BPS of the best-performing neural encoding model (maximum over single layers and delay window lengths) for different areas of the pathway. Color key indicates different layer types (CNN supervised, CNN layers from the supervised Deep Speech 2 model or HuBERT supervised model; CNN-SSL, CNN layers from the self-supervised Wav2Vec 2 or HuBERT model; LSTM supervised, LSTM layers from Deep Speech 2; Transformer SSL + FT, transformer layers from the selfsupervised and fine-tuned Wav2Vec 2 model; Transformer SSL, transformer layers from the self-supervised Wav2Vec 2 or HuBERT model; Transformer supervised, transformer layers from the pure supervised HuBERT model; CNN random, CNN layers from the randomized HuBERT model; Transformer random, transformer layers from the randomized HuBERT model). Red star indicates the best model for each area; black dot indicates other models that were not statistically different from the best model (P &gt; 0.05, two-sided paired t test). Dashed horizontal line indicates the baseline model using full acoustic-phonetic features. For a and c, the box plot shows the first and third quantiles across electrodes (orange line indicates the median; black line indicates the mean value; whiskers indicate the 5th and 95th percentiles). a.u., arbitrary units; ECoG, electrocorticography; Spect, spectrogram; feat., features; DS2, Deep Speech 2; W2V, Wav2Vec 2; HuB., HuBERT; W2V-A, Wav2Vec 2 ASR supervised model; Tr., transformer; Sup., supervised; Ran., randomized.</p>Article https://doi.org/10.1038/s41593-023-01468-4</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Extended Data Fig. 6 | Clustering the STG electrodes. a) Percent of total variance explained by the NMF decomposition with different number of factors; b) The time course of the event-related high-gamma activity (HGA) of the two factors from the NMF model; c) the cluster assignment for each STG electrode. Each panel is the sentence averaged neural response for one STG electrode, colored by the cluster assignment.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank <rs type="person">M. Leonard</rs>, <rs type="person">L. Gwilliams</rs>, <rs type="person">I. Bhaya-Grossman</rs>, <rs type="person">Y. Zhang</rs> and <rs type="person">J. Hieronymus</rs> for critically reading the and for their helpful suggestions. We gratefully acknowledge the support granted by the <rs type="funder">National Institute of Neurological Disorders and Stroke</rs> (<rs type="grantNumber">U01NS117765</rs>, E.F.C.), the <rs type="funder">National Institute on Deafness and Other Communication Disorders</rs> (<rs type="grantNumber">R01DC012379</rs>, E.F.C.), the <rs type="funder">William K. Bowes Foundation (E.F.C.)</rs>, the <rs type="funder">William and Susan Oberndorf Foundation (E.F.C.)</rs>, the <rs type="funder">Joan and Sanford Weill Foundation (E.F.C.)</rs>, the <rs type="funder">Shurl and Kay Curci Foundation</rs> (E.F.C.), <rs type="programName">Shanghai Pujiang Program</rs> (<rs type="grantNumber">22PJ1410500</rs>, Y.L.), the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">32371154</rs>, Y.L.), <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (<rs type="grantNumber">2018SHZDZX01</rs>, J.W.), <rs type="funder">Shanghai Shenkang Hospital Development Center</rs> (<rs type="grantNumber">SHDC12018114</rs>, J.W.), <rs type="funder">Shanghai Rising-Star Program</rs> (<rs type="grantNumber">19QA1401700</rs>, J.L.), and <rs type="funder">Shanghai Young Talents Program</rs> (<rs type="grantNumber">2017YQ014</rs>, J.L.).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fYmZA6M">
					<idno type="grant-number">U01NS117765</idno>
				</org>
				<org type="funding" xml:id="_5ZthqEY">
					<idno type="grant-number">R01DC012379</idno>
				</org>
				<org type="funding" xml:id="_SDu5EEc">
					<idno type="grant-number">22PJ1410500</idno>
					<orgName type="program" subtype="full">Shanghai Pujiang Program</orgName>
				</org>
				<org type="funding" xml:id="_gH8ZEPh">
					<idno type="grant-number">32371154</idno>
				</org>
				<org type="funding" xml:id="_vwgBYYt">
					<idno type="grant-number">2018SHZDZX01</idno>
				</org>
				<org type="funding" xml:id="_UR3bEKA">
					<idno type="grant-number">SHDC12018114</idno>
				</org>
				<org type="funding" xml:id="_2RsrrWJ">
					<idno type="grant-number">19QA1401700</idno>
				</org>
				<org type="funding" xml:id="_8Zw9K33">
					<idno type="grant-number">2017YQ014</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>The completely developed code that operates on the full dataset will be made available from the authors upon reasonable request. Source code that implements the core neural encoding algorithm and the DNN analysis can be found at https://github.com/yuanningli/ neural_encoding_demo.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data analysis</head><p>Data analysis used freesurfer 7.2, python 3.6, pytorch 1.10, fairseq 0.10, huggingface transformers 4.18, numpy 1.20, scipy 1.7, pandas 1.3, librosa 0.9, and scikit-learn 0.20. The completely developed code that operates on the full data set will be made available from the authors upon reasonable request. A sample code that implements the core neural encoding algorithm and the DNN analysis can be found at https://github.com/yuanningli/ neural_encoding_demo.</p><p>For manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code &amp; software for further information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Policy information about availability of data</head><p>All manuscripts must include a data availability statement. This statement should provide the following information, where applicable:</p><p>-Accession codes, unique identifiers, or web links for publicly available datasets -A description of any restrictions on data availability -For clinical datasets or third party data, please ensure that the statement adheres to our policy Librispeech dataset is available at https://www.openslr.org/12. MAGICDATA dataset is available at https://www.openslr.org/68/. TIMIT dataset is available at https://doi.org/10.35111/17gk-bn40. ASCCD dataset is available at http://paslab.phonetics.org.cn/?p=1763. The de-identified patient data that support the findings of this study will be made available from the corresponding author upon request.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reporting summary</head><p>Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability</head><p>The LibriSpeech dataset is available at https://www.openslr.org/12. The MAGICDATA dataset is available at https://www.openslr.org/68/. The TIMIT dataset is available at https://doi.org/10.35111/17gk-bn40. The ASCCD dataset is available at http://paslab.phonetics.org.cn/?p=1763. Deidentified patient data that support the findings of this study will be </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare no competing interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reporting Summary</head><p>Nature Portfolio to improve the reproducibility of the that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Statistics</head><p>For all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>n/a Confirmed</head><p>The exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly</p><p>The statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A description of all covariates tested</head><p>A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals) For null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted Give P values as exact values whenever suitable.</p><p>For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above. Field-specific reporting Please select the one below that is the best fit for your research. If you are not sure, read the before making your selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software and code</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Life sciences</head><p>Behavioural &amp; social sciences Ecological, evolutionary &amp; environmental sciences</p><p>For a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Life sciences study design</head><p>All studies must disclose on these points even when the disclosure is negative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample size</head><p>No explicit sample size calculation was performed. The amount of data collected from each participant was purely dependent on their clinical treatment schedule and the amount of time each participant was willing to volunteer for the study. Data exclusions No data were excluded from analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Replication</head><p>No explicit attempt at replication of the results reported has been undertaken. All encoding results were estimated on separated test set using cross-validation.</p><p>Randomization The sentences and paragraphs within the speech corpora were randomly ordered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blinding</head><p>Blinding was not relevant for this study. The participants' task was to passively listen to the speech, and the experimenter did not interact with the participant during the experiment blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reporting for specific materials, systems and methods</head><p>We require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. This study included 12 participants (6 male, 6 female, age from 31 to 55, all right-handed) who were neurosurgical patients at either UCSF or Huashan Hospital. These include patients with intractable epilepsy who had high-density electrode grids implanted for clinical monitoring of seizure activity, and eloquent brain tumor patients undergoing awake language mapping as part of their surgery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recruitment</head><p>Only the patients undergoing awake surgery with direct cortical stimulation were asked to participant in the study. We only included those participants with tumors which did not obviously invade the auditory cortex. All patients have normal hearing and intact speech cognitions, therefore we do not expect selection bias from the population. The placements of the grids were determined solely by clinical needs. All patients were clearly informed (as detailed in the IRB-approved written consent document signed by the participant) that the participation in the scientific research was completely voluntary and would not directly impact their clinical care. Additional verbal consent was also acquired at the beginning and during the breaks of each experiment session. The participants were compensated $50 for each session of experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics oversight</head><p>The experimental protocol was approved by the Institutional Review Board at the University of California, San Francisco (UCSF) and by the Huashan Hospital Institutional Review Board of Fudan University. All participants gave their written, informed consent prior to testing.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the perception of synthetic speech sounds</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Delattre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Borst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Gerstman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="597" to="606" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Perception of the speech code</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Liberman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">S</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Shankweiler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Studdert-Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol. Rev</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="431" to="461" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The search for invariant acoustic correlates of phonetic features</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Blumstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Perspectives on the Study of Speech</title>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Eimas</surname></persName>
		</editor>
		<editor>
			<persName><surname>Miller</surname></persName>
		</editor>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Representations of pitch and timbre variation in human auditory cortex</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Burton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Olman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Oxenham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1284" to="1293" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Course in Phonetics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ladefoged</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cengage Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Static, dynamic, and relational properties in vowel perception</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Nearey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="2088" to="2113" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From understanding computation to understanding neural circuitry</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="https://dspace.mit.edu/bitstream/handle/1721.1/5782/AIM-357.pdf" />
	</analytic>
	<monogr>
		<title level="j">MIT Artificial Intelligence Laboratory</title>
		<imprint>
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Processing interactions and lexical access during word recognition in continuous speech</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Marslen-Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Welsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="29" to="63" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The TRACE model of speech perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimating spatio-temporal receptive fields of auditory and visual neurons from their responses to natural stimuli</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Theunissen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="289" to="316" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selective cortical representation of attended speaker in multi-talker speech perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="page" from="233" to="236" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Emergence of neural encoding of auditory objects while listening to competing speakers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Z</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="11854" to="11859" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Speech 2: end-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 33rd Int. Conf. Mach. Learn</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>33rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">wav2vec 2.0: a framework for self-supervised learning of speech representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HuBERT: self-supervised speech representation learning by masked prediction of hidden units</title>
		<author>
			<persName><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3451" to="3460" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep neural networks rival the representation of primate IT cortex for core visual object recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Cadieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1003963</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J E</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L K</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Shook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Norman-Haignere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Inductive biases, pretraining and fine-tuning jointly account for brain responses to speech</title>
		<author>
			<persName><forename type="first">J</forename><surname>Millet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>King</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2103.01032</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2103.01032" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Performance-optimized hierarchical models predict neural responses in higher visual cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="8619" to="8624" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On the relationship between maps and domains in inferotemporal cortex</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Arcaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Livingstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Using goal-driven deep learning models to understand sensory cortex</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="356" to="365" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How does the brain solve visual object recognition?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zoccolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Rust</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017. 2018</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">At 6-9 months, human infants know the meanings of many common nouns</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bergelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Swingley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="3253" to="3258" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning words&apos; sounds before learning how words sound: 9-month-olds use distinct objects as cues to categorize speech information</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="234" to="243" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised neural network models of the ventral visual stream</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<date type="published" when="2021">2014196118. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The neural architecture of language: integrative modeling converges on predictive processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schrimpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page">2105646118</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interpreting and improving naturallanguage processing (in machines) with natural languageprocessing (in the brain)</title>
		<author>
			<persName><forename type="first">M</forename><surname>Toneva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wehbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">NeurIPS 2019. 2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14887" to="14897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Self-supervised learning: generative or contrastive</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2021.3090866</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2021.3090866" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Natural speech reveals the semantic maps that tile human cerebral cortex</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>De Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Theunissen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">532</biblScope>
			<biblScope unit="page" from="453" to="458" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Updated parameters and expanded simulation options for a model of the auditory periphery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S A</forename><surname>Zilany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-023-01468-4</idno>
		<ptr target="https://doi.org/10.1038/s41593-023-01468-4" />
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="283" to="286" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Speech coding in the brain: representation of vowel formants by midbrain neurons tuned to sound fluctuations</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcdonough</surname></persName>
		</author>
		<idno>ENEURO.0004-15.2015</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Effects of peripheral tuning on the auditory nerve&apos;s representation of speech envelope and temporal fine structure cues</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">C</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Lopez-Poveda</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-4419-5686-6_40</idno>
		<ptr target="https://doi.org/10.1007/978-1-4419-5686-6_40" />
	</analytic>
	<monogr>
		<title level="m">The Neurophysiological Bases of Auditory Perception</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="429" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Parallel and distributed encoding of speech across human auditory cortex</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oganian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="4626" to="4639" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Network rhythms influence the relationship between spike-triggered local field potential and functional connectivity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Maunsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="12674" to="12682" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA STI/Recon Tech</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Pallett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">27403</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2015.7178964</idno>
		<ptr target="https://doi.org/10.1109/ICASSP.2015.7178964" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A spatial map of onset and sustained responses to speech in the human superior temporal gyrus</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Biol</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1860" to="1871" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human cortical encoding of pitch in tonal and non-tonal languages</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Commun</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Predicting human brain activity associated with the meanings of nouns</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="page" from="1191" to="1195" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Ultra-fine frequency tuning revealed in single neurons of human auditory cortex</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bitterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mukamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nelken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">451</biblScope>
			<biblScope unit="page" from="197" to="201" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Phonetic feature encoding in human superior temporal gyrus</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="1006" to="1010" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spectro-temporal modulation transfer function of single voxels in the human auditory cortex measured with high-resolution fMRI</title>
		<author>
			<persName><forename type="first">M</forename><surname>Schonwiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="14611" to="14616" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A cross-linguistic fMRI study of spectral and temporal cues underlying phonological processing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gandour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Cogn. Neurosci</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1076" to="1087" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A cross-linguistic PET study of tone perception in Mandarin Chinese and English speakers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Zatorre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Milner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="646" to="653" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A speech envelope landmark for syllable encoding in human superior temporal gyrus</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Oganian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sci. Adv</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6279</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Intonational speech prosody encoding in the human auditory cortex</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="797" to="801" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Understanding rostral-caudal auditory cortex contributions to auditory perception</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jasmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="425" to="434" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynamic speech representations in the human temporal lobe</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="472" to="479" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The encoding of speech sounds in the superior temporal gyrus</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="1096" to="1110" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The cortical organization of speech processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hickok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poeppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Rev. Neurosci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="393" to="402" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic encoding of speech sequence probability in human temporal cortex</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Neurosci</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7203" to="7214" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Speech computations of the human superior temporal gyrus</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bhaya-Grossman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-psych-022321-035256</idno>
		<ptr target="https://doi.org/10.1146/annurev-psych-022321-035256" />
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nonlinear auditory models yield new insights into representations of vowels</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Carney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mcdonough</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atten. Percept. Psychophys</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="1034" to="1046" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shared computational principles for language processing in humans and deep language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="369" to="380" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">On the computational architecture of the neocortex: II. The role of cortico-cortical loops</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biol. Cybern</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="241" to="251" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The organization and physiology of the auditory thalamus and its role in processing acoustic features important for speech perception</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Lang</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="29" to="48" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multisensory convergence in auditory cortex: II. Thalamocortical connections of the caudal superior temporal plane</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Neurol</title>
		<imprint>
			<biblScope unit="volume">502</biblScope>
			<biblScope unit="page" from="924" to="952" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Thalamic connections of the core auditory cortex and rostral supratemporal plane in the macaque monkey</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comp. Neurol</title>
		<imprint>
			<biblScope unit="volume">525</biblScope>
			<biblScope unit="page" from="3488" to="3513" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Speech perception, rapid temporal processing, and the left hemisphere: a case study of unilateral pure word deafness</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Slevc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Joanisse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropsychologia</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="216" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Subdivisions of auditory cortex and processing streams in primates</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Kaas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Hackett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Natl Acad. Sci. USA</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="11793" to="11799" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Single-cell activity in human STG during perception of phonemes is organized according to manner of articulation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lakretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ossmy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mukamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fried</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">226</biblScope>
			<biblScope unit="page">117499</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Deep neural network models of sensory systems: windows onto the role of task constraints</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="121" to="132" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Brain-optimized extraction of complex sound features that drive continuous auditory perception</title>
		<author>
			<persName><forename type="first">J</forename><surname>Berezutskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">V</forename><surname>Freudenburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>G??l?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A J</forename><surname>Van Gerven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput. Biol</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">1007992</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Estimating and interpreting nonlinear receptive field of sensory neural responses with deep neural network models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Keshishian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">53445</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Latent neural dynamics encode temporal context in speech</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Oganian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Hear. Res</title>
		<imprint>
			<biblScope unit="volume">437</biblScope>
			<biblScope unit="page">108838</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Speech corpus of Chinese discourse and the phonetic research</title>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.21437/ICSLP.2000-740</idno>
		<ptr target="https://doi.org/10.21437/ICSLP.2000-740" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Spoken Language Processing (ICSLP 2000)</title>
		<meeting>the 6th International Conference on Spoken Language Processing (ICSLP 2000)</meeting>
		<imprint>
			<publisher>International Speech Communication Association</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semi-automated anatomical labeling and inter-subject warping of high-density intracranial recording electrodes in electrocorticography</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front. Neuroinform</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">62</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Speak and unSpeak with PRAAT</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Van Heuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glot Int</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="341" to="347" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2019 Conf. North American Chapter of the Ass. for Comp. Ling.: Human Language Technologies</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><surname>Solorio</surname></persName>
		</editor>
		<meeting>2019 Conf. North American Chapter of the Ass. for Comp. Ling.: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<ptr target="https://www.openslr.org/68" />
		<title level="m">MAGICDATA Mandarin Chinese read speech corpus</title>
		<imprint>
			<publisher>Magic Data Technology Co., Ltd</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 23rd Int. Conf. Mach. Learn</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Cohen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Moore</surname></persName>
		</editor>
		<meeting>23rd Int. Conf. Mach. Learn</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Convex and semi-nonnegative matrix factorizations</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41593-023-01468-4</idno>
		<ptr target="https://doi.org/10.1038/s41593-023-01468-4" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Heschl&apos;s gyrus (yellow), planum temporale (blue), planum polare (green). The numbers of significant speech responsive electrodes in each subject, sorted into anatomical areas</title>
	</analytic>
	<monogr>
		<title level="m">For E1-E9, electrodes are marked in colors according to anatomical label: superior temporal gyrus (red)</title>
		<imprint/>
	</monogr>
	<note>Extended Data Fig. 1 | ECoG grid coverage for individual subjects. are summarized. (HG: Heschl&apos;s Gyrus; STG: Superior Temporal Gyrus</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
