<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hypergraph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yifan</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Congnitive Science School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haoxuan</forename><surname>You</surname></persName>
							<email>haoxuanyou@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
							<email>rrji@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Congnitive Science School of Information Science and Engineering</orgName>
								<orgName type="laboratory">Fujian Key Laboratory of Sensing and Computing for Smart City</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
							<email>gaoyue@tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">School of Software</orgName>
								<orgName type="institution" key="instit1">BNRist</orgName>
								<orgName type="institution" key="instit2">KLISS</orgName>
								<orgName type="institution" key="instit3">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hypergraph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a hypergraph neural networks (HGNN) framework for data representation learning, which can encode high-order data correlation in a hypergraph structure. Confronting the challenges of learning representation for complex data in real practice, we propose to incorporate such data structure in a hypergraph, which is more flexible on data modeling, especially when dealing with complex data. In this method, a hyperedge convolution operation is designed to handle the data correlation during representation learning. In this way, traditional hypergraph learning procedure can be conducted using hyperedge convolution operations efficiently. HGNN is able to learn the hidden layer representation considering the high-order data structure, which is a general framework considering the complex data correlations. We have conducted experiments on citation network classification and visual object recognition tasks and compared HGNN with graph convolutional networks and other traditional methods. Experimental results demonstrate that the proposed HGNN method outperforms recent state-of-theart methods. We can also reveal from the results that the proposed HGNN is superior when dealing with multi-modal data compared with existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Graph-based convolutional neural networks (Kipf and Welling 2017), <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> have attracted much attention in recent years. Different from traditional convolutional neural networks, graph convolution is able to encode the graph structure of different input data using a neural network model and it can be used in the semi-supervised learning procedure. Graph convolutional neural networks have shown superiority on representation learning compared with traditional neural networks due to its ability of using data graph structure.</p><p>In traditional graph convolutional neural network methods, the pairwise connections among data are employed. It is noted that the data structure in real practice could be beyond pairwise connections and even far more complicated. Confronting the scenarios with multi-modal data, the situation for data correlation modelling could be more complex.</p><p>Figure <ref type="figure">1</ref>: Examples of complex connections on social media data. Each color point represents a tweet or microblog, and there could be visual connections, text connections and social connections among them.</p><p>Figure <ref type="figure">1</ref> provides examples of complex connections on social media data. On one hand, the data correlation can be more complex than pairwise relationship, which is difficult to be modeled by a graph structure. On the other hand, the data representation tends to be multi-modal, such as the visual connections, text connections and social connections in this example. Under such circumstances, traditional graph structure has the limitation to formulate the data correlation, which limits the application of graph convolutional neural networks. Under such circumstance, it is important and urgent to further investigate better and more general data structure model to learn representation.</p><p>To tackle this challenging issue, in this paper, we propose a hypergraph neural networks (HGNN) framework, which uses the hypergraph structure for data modeling. Compared with simple graph, on which the degree for all edges is mandatory 2, a hypergraph can encode high-order data correlation (beyond pairwise connections) using its degree-free hyperedges, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. In Figure <ref type="figure" target="#fig_0">2</ref>, the graph is represented using the adjacency matrix, in which each edge connects just two vertices. On the contrary, a hypergraph is easy to be expanded for multi-modal and heterogeneous data representation using its flexible hyperedges. For example, a hypergraph can jointly employ multi-modal data for hypergraph generation by combining the adjacency matrix, as illustrated in Figure <ref type="figure" target="#fig_0">2</ref>. Therefore, hypergraph has been employed in many computer vision tasks such as classification and retrieval tasks <ref type="bibr" target="#b5">(Gao et al. 2012</ref>). However, traditional hypergraph learning methods <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref> suffer from their high computation complexity and storage cost, which limits the wide application of hypergraph learning methods.</p><p>In this paper, we propose a hypergraph neural networks framework (HGNN) for data representation learning. In this method, the complex data correlation is formulated in a hypergraph structure, and we design a hyperedge convolution operation to better exploit the high-order data correlation for representation learning. More specifically, HGNN is a general framework which can incorporate with multi-modal data and complicated data correlations. Traditional graph convolutional neural networks can be regarded as a special case of HGNN. To evaluate the performance of the proposed HGNN framework, we have conducted experiments on citation network classification and visual object recognition tasks. The experimental results on four datasets and comparisons with graph convolutional network (GCN) and other traditional methods have shown better performance of HGNN. These results indicate that the proposed HGNN method is more effective on learning data representation using high-order and complex correlations.</p><p>The main contributions of this paper are two-fold:</p><p>1. We propose a hypergraph neural networks framework, i.e., HGNN, for representation learning using hypergraph structure. HGNN is able to formulate complex and highorder data correlation through its hypergraph structure and can be also efficient using hyperedge convolution operations. It is effective on dealing with multi-modal data/features. Moreover, GCN (Kipf and Welling 2017) can be regarded as a special case of HGNN, for which the edges in simple graph can be regarded as 2-order hyperedges which connect just two vertices. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we briefly review existing works of hypergraph learning and neural networks on graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph learning</head><p>In many computer vision tasks, the hypergraph structure has been employed to model high-order correlation among data.</p><p>Hypergraph learning is first introduced in <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref>, as a propagation process on hypergraph structure. The transductive inference on hypergraph aims to minimize the label difference among vertices with stronger connections on hypergraph. In <ref type="bibr" target="#b10">(Huang, Liu, and Metaxas 2009)</ref>, hypergraph learning is further employed in video object segmentation. <ref type="bibr" target="#b9">(Huang et al. 2010</ref>) used the hypergraph structure to model image relationship and conducted transductive inference process for image ranking. To further improve the hypergraph structure, research attention has been attracted for leaning the weights of hyperedges, which have great influence on modeling the correlation of data. In <ref type="bibr" target="#b6">(Gao et al. 2013)</ref>, a l 2 regularize on the weights is introduced to learn optimal hyperedge weights. In <ref type="bibr" target="#b11">(Hwang et al. 2008</ref>), the correlation among hyperedges is further explored by a assumption that highly correlated hyperedges should have similar weights. Regarding the multi-modal data, in <ref type="bibr" target="#b5">(Gao et al. 2012)</ref>, multi-hypergraph structure is introduced to assign weights for different sub-hypergraphs, which corresponds to different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neural networks on graph</head><p>Since many irregular data that do not own a grid-like structure can only be represented in the form of graph, extending neural networks to graph structure has attracted great attention from researchers. In <ref type="bibr" target="#b7">(Gori, Monfardini, and Scarselli 2005)</ref> and <ref type="bibr" target="#b16">(Scarselli et al. 2009</ref>), the neural network on graph is first introduced to apply recurrent neural networks to deal with graphs. For generalizing convolution network to graph, the methods are divided into spectral and non-spectral approaches.</p><p>For spectral approaches, the convolution operation is formulated in spectral domain of graph. <ref type="bibr" target="#b1">(Bruna et al. 2014)</ref> introduces the first graph CNN, which uses the graph Laplacian eigenbasis as an analogy of the Fourier transform. In <ref type="bibr" target="#b8">(Henaff, Bruna, and LeCun 2015)</ref>, the spectral filters can be parameterized with smooth coefficients to make them spatial-localized. In <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref>, a Chebyshev expansion of the graph Laplacian is further used to approximate the spectral filters. Then, in (Kipf and Welling 2017), the chebyshev polynomials are simplified into 1-order polynomials to form an efficient layer-wise propagation model.</p><p>For spatial approaches, the convolution operation is defined in groups of spatial close nodes. In <ref type="bibr" target="#b0">(Atwood and Towsley 2016)</ref>, the powers of a transition matrix is employed to define the neighborhood of nodes. <ref type="bibr" target="#b13">(Monti et al. 2017)</ref> uses the local path operators in the form of Gaussian mixture models to generalize convolution in spatial domain. In <ref type="bibr" target="#b20">(Velickovic et al. 2018)</ref>, the attention mechanisms is introduced into the graph to build attention-based architecture to perform the node classification task on graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph Neural Networks</head><p>In this section, we introduce our proposed hypergraph neural networks (HGNN). We first briefly introduce hypergraph learning, and then the spectral convolution on hypergraph is provided. Following, we analyze the relations between HGNN and existing methods. In the last part of the section, some implementation details will be given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph learning statement</head><p>We first review the hypergraph analysis theory. Different from simple graph, a hyperedge in a hypergraph connects two or more vertices. A hypergraph is defined as G = (V, E, W), which includes a vertex set V, a hyperedge set E. Each hyperedge is assigned with a weight by W, a diagonal matrix of edge weights. The hypergraph G can be denoted by a |V| × |E| incidence matrix H, with entries defined as</p><formula xml:id="formula_0">h(v, e) = 1, if v ∈ e 0, if v ∈ e,<label>(1)</label></formula><p>For a vertex v ∈ V, its degree is defined as <ref type="bibr">v, e)</ref>. For an edge e ∈ E, its degree is defined as δ(e) = v∈V h(v, e). Further, D e and D v denote the diagonal matrices of the edge degrees and the vertex degrees, respectively.</p><formula xml:id="formula_1">d(v) = e∈E ω(e)h(</formula><p>Here let us consider the node(vertex) classification problem on hypergraph, where the node labels should be smooth on the hypergraph structure. The task can be formulated as a regularization framework as introduced by <ref type="bibr" target="#b23">(Zhou, Huang, and Schölkopf 2007)</ref>:</p><formula xml:id="formula_2">arg min f {R emp (f ) + Ω(f )},<label>(2)</label></formula><p>where Ω(f ) is a regularize on hypergraph, R emp (f ) denotes the supervised empirical loss, f (•) is a classification function. The regularize Ω(f ) is defined as:</p><formula xml:id="formula_3">Ω(f ) = 1 2 e∈E {u,v}∈V w(e)h(u, e)h(v, e) δ(e) f (u) d(u) − f (v) d(v) 2 ,<label>(3)</label></formula><p>We let</p><formula xml:id="formula_4">θ = D −1/2 v HWD −1 e H D −1/2 v</formula><p>and ∆ = I − Θ. Then, the normalized Ω(f ) can be written as</p><formula xml:id="formula_5">Ω(f ) = f ∆, (<label>4</label></formula><formula xml:id="formula_6">)</formula><p>where ∆ is positive semi-definite, and usually called the hypergraph Laplacian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral convolution on hypergraph</head><p>Given a hypergraph G = (V, E, ∆) with n vertices, since the hypergraph Laplacian ∆ is a n × n positive semi-definite matrix, the eigen decomposition ∆ = ΦΛΦ can be employed to get the orthonormal eigen vectors Φ = diag(φ 1 , . . . , φ n ) and a diagonal matrix Λ = diag(λ 1 , . . . , λ n ) containing corresponding non-negative eigenvalues. Then, the Fourier transform for a signal x = (x 1 , . . . , x n ) in hypergraph is defined as x = Φ x, where the eigen vectors are regarded as the Fourier bases and the eigenvalues are interpreted as frequencies. The spectral convolution of signal x and filter g can be denoted as</p><formula xml:id="formula_7">g x = Φ((Φ g) (Φ x)) = Φg(Λ)Φ x,<label>(5</label></formula><p>) where denotes the element-wise Hadamard product and g(Λ) = diag(g(λ 1 ), . . . , g(λ n )) is a function of the Fourier coefficients. However, the computation cost in forward and inverse Fourier transform is O(n 2 ). To solve the problem, we can follow <ref type="bibr" target="#b3">(Defferrard, Bresson, and Vandergheynst 2016)</ref> to parametrize g(Λ) with K order polynomials. Furthermore, we use the truncated Chebyshev expansion as one such polynomial. Chebyshv polynomials T k (x) is recursively computed by T k (x) = 2xT k−1 (x) − T k−2 (x), with T 0 (x) = 1 and T 1 (x) = x. Thus, the g(Λ) can be parametried as</p><formula xml:id="formula_8">g x ≈ K k=0 θ k T k ( ∆)x,<label>(6)</label></formula><p>where T k ( ∆) is the Chebyshev polynomial of order k with scaled Laplacian ∆ = 2 λmax ∆ − I. In Equation <ref type="formula" target="#formula_8">6</ref>, the expansive computation of Laplacian Eigen vectors is excluded and only matrix powers, additions and multiplications are included, which brings further improvement in computation complexity. We can further let K = 1 to limit the order of convolution operation due to that the Laplacian in hypergraph can already well represent the high-order correlation between nodes. It is also suggested in (Kipf and Welling 2017) that λ max ≈ 2 because of the scale adaptability of neural networks. Then, the convolution operation can be further simplified to</p><formula xml:id="formula_9">g x ≈ θ 0 x − θ 1 D −1/2 HWD −1 e H D −1/2 v x,<label>(7</label></formula><p>) where θ 0 and θ 1 is parameters of filters over all nodes. We further use a single parameter θ to avoid the overfitting problem, which is defined as</p><formula xml:id="formula_10">θ 1 = − 1 2 θ θ 0 = 1 2 θD −1/2 v HD −1 e H D −1/2 v ,<label>(8)</label></formula><p>Then, the convolution operation can be simplified to the following expression</p><formula xml:id="formula_11">g x ≈ 1 2 θD −1/2 v H(W + I)D −1 e H D −1/2 v x ≈ θD −1/2 v HWD −1 e H D −1/2 v x,<label>(9)</label></formula><p>where (W + I) can be regarded as the weight of the hyperedges. W is initialized as an identity matrix, which means equal weights for all hyperedges.</p><p>When we have a hypergraph signal X ∈ R n×C1 with n nodes and C 1 dimensional features, our hyperedge convolution can be formulated by</p><formula xml:id="formula_12">Y = D −1/2 v HWD −1 e H D −1/2 v XΘ,<label>(10)</label></formula><p>where W = diag(w 1 , . . . , w n ). Θ ∈ R C1×C2 is the parameter to be learned during the training process. The filter Θ is applied over the nodes in hypergraph to extract features.</p><p>After convolution, we can obtain Y ∈ R n×C2 , which can be used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph neural networks analysis</head><p>Figure <ref type="figure" target="#fig_2">3</ref> illustrates the details of the hypergraph neural networks. Multi-modality datasets are divided into training data and testing data, and each data contains several nodes with features. Then multiple hyperedge structure groups are constructed from the complex correlation of the multi-modality datasets. We concatenate the hyperedge groups to generate the hypergraph adjacent matrix H. The hypergraph adjacent matrix H and the node feature are fed into the HGNN to get the node output labels. As introduced in the above section, we can build a hyperedge convolutional layer f (X, W, Θ) in the following formulation</p><formula xml:id="formula_13">X (l+1) = σ(D −1/2 v HWD −1 e H D −1/2 v X (l) Θ (l) ),<label>(11)</label></formula><p>where X (1) ∈ R N ×C is the signal of hypergraph at l layer, X (0) = X and σ denotes the nonlinear activation function.</p><p>The HGNN model is based on the spectral convolution on the hypergraph. Here, we further investigate HGNN in the property of exploiting high-order correlation among data. As is shown in Figure <ref type="figure" target="#fig_3">4</ref>, the HGNN layer can perform nodeedge-node transform, which can better refine the features using the hypergraph structure. More specifically, at first, the initial node feature X (1) is processed by learnable filter matrix Θ (1) to extract C 2 -dimensional feature. Then, the node feature is gathered according to the hyperedge to form the hyperedge feature R E×C2 , which is implemented by the multiplication of H ∈ R E×N . Finally the output node feature is obtained by aggregating their related hyperedge feature, which is achieved by multiplying matrix H. Denote that D v and D e play a role of normalization in Equation <ref type="formula" target="#formula_13">11</ref>. Thus, the HGNN layer can efficiently extract the high-order correlation on hypergraph by the node-edge-node transform.</p><p>Relations to existing methods When the hyperedges only connect two vertices, the hypergraph is simplified into a simple graph and the Laplacian ∆ is also coincident with the Laplacian of simple graph up to a factor of 1 2 . Compared with the existing graph convolution methods, our HGNN can naturally model high-order relationship among data, which is effectively exploited and encoded in forming feature extraction. Compared with the traditional hypergraph method, our model is highly efficient in computation without the inverse operation of Laplacian ∆. It should also be noted that our HGNN has great expansibility toward multi-modal feature with the flexibility of hyperedge generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation</head><p>Hypergraph construction In our visual object classification task, the features of N visual object data can be represented as X = [x 1 , . . . , x n ] . We build the hypergraph according to the distance between two features. More specifically, Euclidean distance is used to calculate d(x i , x j ). In the construction, each vertex represents one visual object, and each hyperedge is formed by connecting one vertex and its K nearest neighbors, which brings N hyperedges that links K + 1 vertices. And thus, we get the incidence matrix H ∈ R N ×N with N × (K + 1) entries equaling to 1 while others equaling to 0. In the citation network classification, where the data are organized in graph structure, each hyperedge is built by linking one vertex and their neighbors according to the adjacency relation on graph. So we also get N hyperedges and H ∈ R N×N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model for node classification</head><p>In the problem of node classification, we build the HGNN model as in Figure <ref type="figure" target="#fig_2">3</ref>. The dataset is divided into training data and test data. Then hypergraph is constructed as the section above, which generates the incidence matrix H and corresponding D e . We build a two-layer HGNN model to employ the powerful capacity of HGNN layer. And the softmax function is used to generate predicted labels. During training, the cross-entropy loss for the training data is back-propagated to update the parameters Θ and in testing, the labels of test data is predicted for evaluating the performance. When there are multi-modal information incorporate them by the construction of hyper-edge groups and then various hyperedges are fused together to model the complex relationship on data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we evaluate our proposed hypergraph neural networks on two task: citation network classification and visual object recognition. We also compare the proposed method with graph convolutional networks and other stateof-the-art methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Citation network classification</head><p>Datasets In this experiment, the task is to classify citation data. Here, two widely used citation network datasets, i.e., Cora and Pubmed <ref type="bibr" target="#b17">(Sen et al. 2008</ref>) are employed. The experimental setup follows the settings in <ref type="bibr" target="#b22">(Yang, Cohen, and Salakhutdinov 2016)</ref>. In both of those two datasets, the feature for each data is the bag-of-words representation of documents. The data connection, i.e., the graph structure, indicates the citations among those data. To generate the hypergraph structure for HGNN, each time one vertex in the graph is selected as the centroid and its connected vertices are used to generate one hyperedge including the centroid itself. Through this we can obtain the same size incidence matrix compared with the original graph. It is noted that as there are no more information for data relationship, the generated hypergraph constructure is quite similar to the graph. The Cora dataset contains 2708 data and 5% are used as labeled data for training. The Pubmed dataset contains 19717 data, and only 0.3% are used for training. The detailed description for the two datasets listed in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Experimental settings In this experiment, a two-layer HGNN is applied. The feature dimension of the hidden layer is set as 16 and the dropout <ref type="bibr" target="#b18">(Srivastava et al. 2014</ref>) is employed to avoid overfitting with drop rate p = 0.5. We choose the ReLU as the nonlinear activation function. During the training process, we use Adam optimizer (Kingma and Ba 2014) to minimize our cross-entropy loss function with a learning rate of 0.001. We have also compared the proposed HGNN with recent methods in these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussion</head><p>The results of the experimental results and comparisons on the citation network dataset are shown in Table <ref type="table" target="#tab_1">2</ref>. For our HGNN model, we report the average classification accuracy of 100 runs on Core and Pumbed, which is 81.6% and 80.1%. As shown in the results, the proposed HGNN model can achieve the best or comparable performance compared with the state-of-the-art methods. Compared with GCN, the proposed HGNN method can achieve a slight improvement on the Cora dataset and 1.1% improvement on the Pubmed dataset. We note that the generated hypergraph structure is quite similar to the graph structure as there is neither extra nor more complex information in these data. Therefore, the gain obtained by HGNN is not very significant.   <ref type="bibr" target="#b19">(Su et al. 2015)</ref> and Group-View Convolutional Neural Network (GVCNN) <ref type="bibr" target="#b4">(Feng et al. 2018)</ref>. These two methods are selected due to that they have shown satisfactory performance on 3D object representation. We follow the experimental settings of MVCNN and GVCNN to generate multiple views of each 3D object. Here, 12 virtual cameras are employed to capture views with a interval angle of 30 degree, and then both the MVCNN and the GVCNN features are extracted accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual object classification</head><p>To compare with GCN method, it is noted that there is no available graph structure in the ModelNet40 dataset and the NTU dataset. Therefore, we construct a probability graph based on the distance of nodes. Given the features of data, the affinity matrix A is generated to represent the relationship among different vertices, and A ij can be calculated by:</p><formula xml:id="formula_14">A ij = exp(− 2Dij 2 ∆ )<label>(12)</label></formula><p>where D ij indicates the Euclidean distance between node i and node j. ∆ is the average pairwise distance between nodes. For the GCN experiment with two features constructed simple graphs, we simply average the two modality adjacency matrices to get the fused graph structure for comparison.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hypergraph structure construction on visual datasets</head><p>In experiments on ModelNet40 and NTU datasets, two hypergraph construction methods are employed. The first one is based on single modality feature and the other one is based on multi-modality feature. In the first case, only one feature is used. Each time one object in the dataset is selected as the centroid, and its 10 nearest neighbors in the selected feature space are used to generate one hyperedge including the centroid itself, as shown in Figure <ref type="figure" target="#fig_5">5</ref>. Then, a hypergraph G with N hyperedges can be constructed. In the second case, multiple features are used to generate a hypergraph G modeling complex multi-modality correlation. Here, for the i th modality data, a hypergraph adjacent matrix H i is constructed accordingly. After all the hypergraphs from different features have been generated, these adjacent matrices H i can be concatenated to build the multi-modality hypergraph adjacent matrix H. In this way, the hypergraphs using single modal feature and multi-modal features can be constructed.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and discussions</head><p>Experiments and comparisons on the visual object recognition task are shown in Table <ref type="table" target="#tab_3">4</ref> and Table <ref type="table" target="#tab_4">5</ref>, respectively. For the ModelNet40 dataset, we have compared the proposed method using two features with recent state-of-the-are methods in Table <ref type="table" target="#tab_5">6</ref>. As shown in the results, we can have the following observations:</p><p>1. The proposed HGNN method outperforms the state-ofthe-art object recognition methods in the ModelNet40 dataset. More specifically, compared with PointCNN and SO-Net, the proposed HGNN method can achieve gains of 4.8% and 3.2%, respectively. These results demonstrate the superior performance of the proposed HGNN method on visual object recognition.</p><p>2. Compared with GCN, the proposed method achieves better performance in all experiments. As shown in Table 4 and Table <ref type="table" target="#tab_4">5</ref>, when only one feature is used for graph/hypergraph structure generation, HGNN can obtain slightly improvement. For example, when GVCNN is used as the object feature and MVCNN is used for graph/hypergraph structure generation, HGNN achieves gains of 0.3% and 2.0% compared with GCN on the ModelNet40 and the NTU datasets, respectively. When more features, i.e., both GVCNN and MVCNN, are used for graph/hypergraph structure generation, HGNN achieves much better performance compared with GCN.</p><p>For example, HGNN achieves gains of 8.3%, 10.4% and 8.1% compared with GCN when GVCNN, MVCNN and GVCNN+MVCNN are used as the object features on the NTU dataset, respectively.</p><p>The better performance can be dedicated to the employed hypergraph structure. The hypergraph structure is able to convey complex and high-order correlations among data, which can better represent the underneath data relationship compared with graph structure or the methods without graph structure. Moreover, when multi-modal data/features are available, HGNN has the advantage of combining such multi-modal information in the same structure by its flexible hyperedges. Compared with traditional hypergraph learning methods, which may suffer from the high computational complexity and storage cost, the proposed HGNN framework is much more efficient through the hyperedge convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a framework of hypergraph neural networks (HGNN). In this method, HGNN generalizes the convolution operation to the hypergraph learning process. The convolution on spectral domain is conducted with hypergraph Laplacian and further approximated by truncated chebyshev polynomials. HGNN is a more general framework which is able to handle the complex and high-order correlations through the hypergraph structure for representation learning compared with traditional graph. We have conducted experiments on citation network classification and visual object recognition tasks to evaluate the performance of the proposed HGNN method. Experimental results and comparisons with the state-of-the-art methods demonstrate better performance of the proposed HGNN model. HGNN is able to take complex data correlation into representation learning and thus lead to potential wide applications in many tasks, such as visual recognition, retrieval and data classification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The comparison between graph and hypergraph.</figDesc><graphic url="image-2.png" coords="2,126.00,54.00,359.99,175.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>2. We have conducted extensive experiments on citation network classification and visual object classification tasks. Comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed HGNN framework. Experiments also indicate the better performance of the proposed method when dealing with multi-modal data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The proposed HGNN framework.</figDesc><graphic url="image-3.png" coords="3,68.40,54.00,475.22,131.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The illustration of the hyperedge convolution layer.</figDesc><graphic url="image-4.png" coords="4,90.00,54.00,431.98,165.61" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Datasets and experimental settings In this experiment, the task is to classify visual objects. Two public benchmarks are employed here, including the Princeton ModelNet40 dataset<ref type="bibr" target="#b21">(Wu et al. 2015)</ref> and the National Taiwan University (NTU) 3D model dataset<ref type="bibr" target="#b2">(Chen et al. 2003)</ref>, as shown in Table 3. The ModelNet40 dataset consists of 12,311 objects from 40 popular categories, and the same training/testing split is applied as introduced in<ref type="bibr" target="#b21">(Wu et al. 2015)</ref>, where 9,843 objects are used for training and 2,468 objects are used for testing. The NTU dataset is composed of 2,012 3D shapes from 67 categories, including car, chair, chess, chip, clock, cup, door, frame, pen, plant leaf and so on. In the NTU dataset, 80% data are used for training and the other 20% data are used for testing. In this experiment, each 3D object is represented by the extracted features. Here, two recent state-of-the-art shape representation methods are employed, includingMulti-view Convolutional Neural Network (MVCNN)  </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: An example of hyperedge generation in the visual object classification task. Left: For each node we aggregate its N neighbor nodes by Euclidean distance to generate a hyperedge. Right: To generate the multi-modality hypergraph adjacent matrix we concatenate adjacent matrix of two modality.</figDesc><graphic url="image-5.png" coords="6,323.55,324.85,230.40,87.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the citation classification datasets.</figDesc><table><row><cell></cell><cell cols="2">Cora Pumbed</cell></row><row><cell>Nodes</cell><cell>2708</cell><cell>19717</cell></row><row><cell>Edges</cell><cell>5429</cell><cell>44338</cell></row><row><cell>Feature</cell><cell>1433</cell><cell>500</cell></row><row><cell>Training node</cell><cell>140</cell><cell>60</cell></row><row><cell cols="2">Validation node 500</cell><cell>500</cell></row><row><cell>Testing node</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Classes</cell><cell>7</cell><cell>3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Classification results on the Cora and Pubmed datasets.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Pubmed</cell></row><row><cell>DeepWalk (Perozzi, Al-Rfou,</cell><cell>67.2%</cell><cell>65.3%</cell></row><row><cell>and Skiena 2014)</cell><cell></cell><cell></cell></row><row><cell>ICA (Lu and Getoor 2003)</cell><cell>75.1%</cell><cell>73.9%</cell></row><row><cell>Planetoid (Yang, Cohen, and</cell><cell>75.7%</cell><cell>77.2%</cell></row><row><cell>Salakhutdinov 2016)</cell><cell></cell><cell></cell></row><row><cell>Chebyshev (Defferrard, Bres-</cell><cell>81.2%</cell><cell>74.4%</cell></row><row><cell>son, and Vandergheynst 2016)</cell><cell></cell><cell></cell></row><row><cell cols="2">GCN (Kipf and Welling 2017) 81.5%</cell><cell>79.0%</cell></row><row><cell>HGNN</cell><cell>81.6%</cell><cell>80.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The detailed information of the ModelNet40 and the NTU datasets.</figDesc><table><row><cell></cell><cell cols="2">ModelNet40 NTU</cell></row><row><cell>Objects</cell><cell>12311</cell><cell>2012</cell></row><row><cell>MVCNN Feature</cell><cell>4096</cell><cell>4096</cell></row><row><cell>GVCNN Feature</cell><cell>2048</cell><cell>2048</cell></row><row><cell>Training node</cell><cell>9843</cell><cell>1639</cell></row><row><cell>Testing node</cell><cell>2468</cell><cell>373</cell></row><row><cell>Classes</cell><cell>40</cell><cell>67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison between GCN and HGNN on the ModelNet40 dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Features for Structure</cell><cell></cell></row><row><cell>Feature</cell><cell cols="2">GVCNN</cell><cell cols="2">MVCNN</cell><cell cols="2">GVCNN+MVCNN</cell></row><row><cell></cell><cell cols="5">GCN HGNN GCN HGNN GCN</cell><cell>HGNN</cell></row><row><cell cols="6">GVCNN (Feng et al. 2018) 91.8% 92.6% 91.5% 91.8% 92.8%</cell><cell>96.6%</cell></row><row><cell>MVCNN (Su et al. 2015)</cell><cell cols="5">92.5% 92.9% 86.7% 91.0% 92.3%</cell><cell>96.6%</cell></row><row><cell>GVCNN+MVCNN</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>94.4%</cell><cell>96.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Features for Structure</cell><cell></cell></row><row><cell>Feature</cell><cell cols="2">GVCNN</cell><cell cols="2">MVCNN</cell><cell cols="2">GVCNN+MVCNN</cell></row><row><cell></cell><cell cols="5">GCN HGNN GCN HGNN GCN</cell><cell>HGNN</cell></row><row><cell cols="6">GVCNN ((Feng et al. 2018)) 78.8% 82.5% 78.8% 79.1% 75.9%</cell><cell>84.2%</cell></row><row><cell>MVCNN ((Su et al. 2015))</cell><cell cols="5">74.0% 77.2% 71.3% 75.6% 73.2%</cell><cell>83.6%</cell></row><row><cell>GVCNN+MVCNN</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>−</cell><cell>76.1%</cell><cell>84.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison between GCN and HGNN on the NTU dataset.</figDesc><table><row><cell>Method</cell><cell>Classification Accuracy</cell></row><row><cell>PointNet (Qi et al. 2017a)</cell><cell>89.2%</cell></row><row><cell>PointNet++ (Qi et al. 2017b)</cell><cell>90.7%</cell></row><row><cell>PointCNN (Li et al. 2018)</cell><cell>91.8%</cell></row><row><cell>SO-Net (Li, Chen, and 2018)</cell><cell>93.4%</cell></row><row><cell>HGNN</cell><cell>96.7%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Experimental comparison among recent classification methods on ModelNet40 dataset.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by National Key R&amp;D Program of China (Grant No. 2017YFC0113000, and  No.2016YFB1001503), and National Natural Science Funds of China (No.U1705262, No.61772443, No.61572410, No.61671267), National Science and Technology Major Project (No. 2016ZX01038101), MIIT IT funds (Research and application of TCN key technologies) of China, and The National Key Technology R and D Program (No. 2015BAG14B01-02), Post Doctoral Innovative Talent Support Program under Grant BX201600094, China Post-Doctoral Science Foundation under Grant 2017M612134, Scientific Research Project of National Language Committee of China (Grant No. YB135-49), and Nature Science Foundation of Fujian Province, China (No. 2017J01125 and No. 2018J01106).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-Convolutional Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="1993">2016. 1993-2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral Networks and Locally Connected Networks on Graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On Visual Similarity Based 3D Model Retrieval</title>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-P</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ouhyoung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
				<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="223" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Graphs with Gast Localized Spectral Filtering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gvcnn: Group-View Convolutional Neural Networks for 3D Shape Recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3-D Object Retrieval and Recognition with Hypergraph Analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4290" to="4303" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visual-Textual Joint Relevance Learning for Tagbased Social Image Search</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A New Model for Learning in Graph Domains</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCNN</title>
				<meeting>IJCNN</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep Convolutional Networks on Graph-Structured Data</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image Retrieval via Probabilistic Hypergraph Ranking</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="3376" to="3383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video Object Segmentation by Hypergraph Cut</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1738" to="1745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning on Weighted Hypergraphs to Integrate Protein Interactions and Gene Expressions for Cancer Outcome Prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kuangy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR. Kipf</title>
				<editor>
			<persName><surname>Nips</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Lee</surname></persName>
		</editor>
		<meeting>ICLR. Kipf<address><addrLine>Li, Y</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008. 2014. 2017. 2018</date>
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Link-based Classification</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGKDD</title>
				<meeting>SIGKDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2017. 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Point-Net: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017a</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2017">2017b</date>
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Collective Classification in Network Data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-View Convolutional Neural Networks for 3D Shape Recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
				<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph Attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A Deep Representation for Volumetric Shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
				<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Revisiting Semi-Supervised Learning with Graph Embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
				<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning with Hypergraphs: Clustering, Classification, and Embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
