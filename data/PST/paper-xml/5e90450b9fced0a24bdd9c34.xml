<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point Cloud Semantic Segmentation Using a Deep Learning Framework for Cultural Heritage</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-20">20 March 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Roberto</forename><surname>Pierdicca</surname></persName>
							<email>r.pierdicca@univpm.it</email>
							<idno type="ORCID">0000-0002-9160-834X</idno>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Ingegneria Civile</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<addrLine>Edile e dell&apos;Architettura</addrLine>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marina</forename><surname>Paolanti</surname></persName>
							<email>m.paolanti@pm.univpm.it</email>
							<idno type="ORCID">0000-0002-5523-7174</idno>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Informazione</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesca</forename><surname>Matrone</surname></persName>
							<email>francesca.matrone@polito.it</email>
							<idno type="ORCID">0000-0002-9160-1674</idno>
							<affiliation key="aff2">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Ambiente</orgName>
								<orgName type="institution" key="instit1">del Territorio e delle Infrastrutture</orgName>
								<orgName type="institution" key="instit2">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Massimo</forename><surname>Martini</surname></persName>
							<email>m.martini@pm.univpm.it</email>
							<idno type="ORCID">0000-0003-1714-4310</idno>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Informazione</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christian</forename><surname>Morbidoni</surname></persName>
							<email>c.morbidoni@univpm.it</email>
							<idno type="ORCID">0000-0003-0244-9322</idno>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Informazione</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eva</forename><forename type="middle">Savina</forename><surname>Malinverni</surname></persName>
							<email>e.s.malinverni@univpm.it</email>
							<idno type="ORCID">0000-0001-6582-2943</idno>
							<affiliation key="aff0">
								<orgName type="department">Dipartimento di Ingegneria Civile</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<addrLine>Edile e dell&apos;Architettura</addrLine>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emanuele</forename><surname>Frontoni</surname></persName>
							<email>e.frontoni@univpm.it</email>
							<idno type="ORCID">0000-0002-8893-9244</idno>
							<affiliation key="aff1">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Informazione</orgName>
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<postCode>60100</postCode>
									<settlement>Ancona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">Maria</forename><surname>Lingua</surname></persName>
							<email>andrea.lingua@polito.it</email>
							<idno type="ORCID">0000-0002-5930-2711</idno>
							<affiliation key="aff2">
								<orgName type="department">Dipartimento di Ingegneria dell&apos;Ambiente</orgName>
								<orgName type="institution" key="instit1">del Territorio e delle Infrastrutture</orgName>
								<orgName type="institution" key="instit2">Politecnico di Torino</orgName>
								<address>
									<postCode>10129</postCode>
									<settlement>Torino</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Point Cloud Semantic Segmentation Using a Deep Learning Framework for Cultural Heritage</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-20">20 March 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">07E1C2B7C30041AAF1E1EC42B5E0FEC6</idno>
					<idno type="DOI">10.3390/rs12061005</idno>
					<note type="submission">Received: 27 February 2020; Accepted: 16 March 2020;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T16:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>classification</term>
					<term>semantic segmentation</term>
					<term>Digital Cultural Heritage</term>
					<term>Point Clouds</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the Digital Cultural Heritage (DCH) domain, the semantic segmentation of 3D Point Clouds with Deep Learning (DL) techniques can help to recognize historical architectural elements, at an adequate level of detail, and thus speed up the process of modeling of historical buildings for developing BIM models from survey data, referred to as HBIM (Historical Building Information Modeling). In this paper, we propose a DL framework for Point Cloud segmentation, which employs an improved DGCNN (Dynamic Graph Convolutional Neural Network) by adding meaningful features such as normal and colour. The approach has been applied to a newly collected DCH Dataset which is publicy available: ArCH (Architectural Cultural Heritage) Dataset. This dataset comprises 11 labeled points clouds, derived from the union of several single scans or from the integration of the latter with photogrammetric surveys. The involved scenes are both indoor and outdoor, with churches, chapels, cloisters, porticoes and loggias covered by a variety of vaults and beared by many different types of columns. They belong to different historical periods and different styles, in order to make the dataset the least possible uniform and homogeneous (in the repetition of the architectural elements) and the results as general as possible. The experiments yield high accuracy, demonstrating the effectiveness and suitability of the proposed approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the Digital Cultural Heritage (DCH) domain, the generation of 3D Point Clouds is, nowadays, the more efficient way to manage CH assets. Being a well-established methodology, the representation of CH artifacts through 3D data is a state of art technology to perform several tasks: morphological analysis, map degradation or data enrichment are just some examples of possible ways to exploit such rich informative virtual representation. Management of DCH information is fundamental for better understanding heritage data and for the development of appropriate conservation strategies. An efficient information management strategy should take into consideration three main concepts: segmentation, organization of the hierarchical relationships and semantic enrichment <ref type="bibr" target="#b0">[1]</ref>. Terrestrial Laser Scanning (TLS) and digital photogrammetry allow to generate large amounts of detailed 3D scenes, with geometric information attributes depending on the method used. As well, the development in the last years of technologies such as Mobile Mapping System (MMS) is contributing the massive 3D metric documentation of the built heritage <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Therefore, Point Clouds management, processing and interpretation is gaining importance in the field of geomatics and digital representation. These geometrical structures are becoming progressively mandatory not only for creating multimedia experiences <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, but also (and mainly) for supporting the 3D modeling process <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, where even neural networks are lately starting to be employed <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Simultaneously, the recent research trends in HBIM (Historical Building Information Modeling) are aimed at managing multiple and various architectural heritage data <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, facing the issue of transforming 3D models from a geometrical representation to an enriched and informative data collector <ref type="bibr" target="#b14">[15]</ref>. Achieving such result is not trivial, since HBIM is generally based on scan-to-BIM processes that allow to generate a parametric 3D model from Point Cloud <ref type="bibr" target="#b15">[16]</ref>; these processes, although very reliable since they are made manually by domain experts, have two drawbacks that are noteworthy: first, it is very time consuming and, secondly, it wastes an uncountable amount of data, given that a 3D scanning (both TLS or Close Range Photogrammetry based), contains much more information than the ones required for describing a parametric object. The literature demonstrates that, up to now, traditional methods applied to DCH field still make extensive use of manual operations to capture the real estate from Point Clouds <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref>. Lately, towards this end, a very promising research field is the development of Deep Learning (DL) frameworks for Point Cloud such as Point-Net/Pointnet++ <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> open up more powerful and efficient ways to handle 3D data <ref type="bibr" target="#b22">[23]</ref>. These methods are designed to recognize Point Clouds. The tasks performed by these frameworks are: Point Cloud classification and segmentation. The Point Cloud classification takes the whole Point Cloud as input and output the category of the input Point Cloud. The segmentation aims at classifying each point to a specific part of the Point Cloud <ref type="bibr" target="#b23">[24]</ref>. Albeit the literature for 3D instance segmentation is limited, if compared to its 2D counterpart (mainly due to the high memory and computational cost required by Convolutional Neural Network (CNN) for scene understanding <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>), these frameworks may facilitate the recognition of historical architectural elements, at an appropriate level of detail, and thus speeding up the process of reconstruction of geometries in the HBIM environment or in object-oriented software <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. To the best of our knowledge, these methods are not applied for the automatic recognition of DCH elements yet. In fact, even if they revealed to be suitable for handling Point Cloud of regular shapes, DCH goods are characterised by complex geometries, highly variable between them and defined only with a high level of detail. In <ref type="bibr" target="#b18">[19]</ref>, the authors study the potential offered by DL approaches for the supervised classification of 3D heritage obtaining promising results. However, the work does not cope with the irregular nature of CH data.</p><p>To address these drawbacks, we propose a DL framework for Point Cloud segmentation, inspired by the work presented in <ref type="bibr" target="#b31">[32]</ref>. Instead of employing individual points like PointNet <ref type="bibr" target="#b20">[21]</ref>, the approach proposed in <ref type="bibr" target="#b31">[32]</ref> exploits local geometric structures by constructing a local neighborhood graph and applying convolution-like operations on the edges connecting neighboring pairs of points. This network has been improved by adding relevant features such as normal and HSV encoded color. The experiments has been performed to a completely new DCH dataset. This dataset comprises 11 labeled points clouds, derived from the union of several single scans or from the integration of the latter with photogrammetric surveys. The involved scenes are both indoor and outdoor, with churches, chapels, cloisters, porticoes and archades covered by a variety of vaults and beared by many different types of columns. They belong to different historical periods and different styles, in order to make the dataset the least possible uniform and homogeneous (in the repetition of the architectural elements) and the results as general as possible. In contrast to many existing datasets, it has been manually labelled by domain experts, thus providing a more precise dataset. We show that the resulting network achieves promising performance in recognizing elements. A comprehensive overall picture of the developed framework is reported in Figure <ref type="figure" target="#fig_0">1</ref>. Moreover, the research community dealing with Point Cloud segmentation can benefit from this work, as it makes available a labelled dataset of DCH elements. As well, the pipeline of work might represent a baseline for further experiments from other researchers dealing with Semantic Segmentation of Point Clouds with DL approaches. The main contributions of this paper with respect to the state-of-the-art approaches are: (i) a DL framework for DCH semantic segmentation of Point Cloud, useful for the 3D documentation of monuments and sites; (ii) an improved DL approach based on DGCNN with additional Point Cloud features; (iii) a new DCH dataset that is publicly available to the scientific community for testing and comparing different approaches and iv) the definition of a consistent set of architectural element classes, based on the analysis of existing standard classifications.</p><p>The paper is organized as follows. Section 2 provides a description of the approaches that were adopted for Point Clouds semantic segmentation. Section 3 describes our approach to present a novel DL network operation for learning from Point Clouds, to better capture local geometric features of Point Clouds and a new challenging dataset for the DCH domain. Section 4 offers an extensive comparative evaluation and a detailed analysis of our approach. Finally, Section 6 draws conclusions and discuss future directions for this field of research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">State of the Art</head><p>In this section we review the relevant literature concerning the classification and semantic segmentation for digital representation of cultural heritage. We then focus on the semantic segmentation of Point Cloud data, discussing different existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Classification and Semantic Segmentation in the Field of Dch</head><p>In the field of CH, the classification and semantic segmentation associated with DL techniques, can help to recognize historical architectural elements, at an adequate level of detail, and thus speed up the process of reconstructing geometries in the BIM environment. To date, there are many studies in which Point Clouds are used for the recognition and reconstruction of geometries related to BIM models <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>, however these methods have not been applied to DCH yet and do not fully exploit DL strategies. Cultural assets are in fact characterized by more complex geometries, very variable even within the same class and describable only with a high level of detail, making therefore much more complicated to apply DL strategies to this domain.</p><p>Despite some works that attempt at classifying DCH images by employing different kinds of techniques <ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> already exist, there are still few researches who seek to directly exploit the Point Clouds of CH for semantic classification or segmentation through ML <ref type="bibr" target="#b36">[37]</ref> or DL techniques. One of them is <ref type="bibr" target="#b37">[38]</ref>, where a segmentation of 3D models of historical buildings is proposed for FEA analysis, starting from Point Clouds and meshes. <ref type="bibr">Barsanti et al.</ref> tested some algorithms such as the region growing, directly on the Point Clouds, proving its effectiveness for the segmentation of flat and well-defined structures, nevertheless more complex geometries such as curves or gaps have not been correctly segmented and the computational times increased considerably. Some software were then tested for the direct segmentation of the meshes, but in this case the results show that the process is still completely manual and, in the only case in which the segmentation is semi-automatic, the software is not able to manage large models, thus it is necessary to split the file and proceed to the analysis of single portions. Finally, it should be emphasized that the case study used for mesh segmentation (The Neptune Temple in Paestum) is a rather regular and symmetrical architecture, hence relatively easy to segment on the basis of some horizontal planes.</p><p>As Point Clouds are geometric structures of irregular nature, characterized by the lack of a grid, with a high variability of density, unordered and invariant to transformation and permutation <ref type="bibr" target="#b38">[39]</ref>, their exploitation with DL approaches is still not straightforward and it is even more challenging when dealing with DCH oriented dataset.</p><p>At the best of our knowledge, the only recent attempt to use DL for the semantic classification of Point Clouds of DCH is <ref type="bibr" target="#b18">[19]</ref>. The method described consists of a workflow composed of feature extraction, feature selection and classification, as proposed in <ref type="bibr" target="#b39">[40]</ref>, for the subdivision into a high level of detailed architectural elements, using and comparing both ML and DL strategies. For the ML, among all the classifiers, the authors use the Random Forest (RF) and One-vs.-One, optimizing the parameters for the RF by choosing those with the value of the higher F1-score in Scikit-learn. In this way, they achieve excellent performances in most of the classes identified, however no feature correlation has been carried out and, most of all, the different geometric features are selected depending on the peculiarities of the case study to be analyzed. On the other side, for the DL approaches, they use a 1D and a 2D CNN, in addition to Bi-LSTM RNN which is usually used for the prediction of sequences or texts. The choice of this type of neural networks is due to the interpretation of the Point Cloud as a sequence of points, nevertheless the results of the ML outperfom those of DL. This could be due to the choice of not using any of the recent networks designed specifically for taking into account the third dimension of the Point Cloud data. Furthermore, the test phase of the DL is carried out on the remaining part of the Point Cloud, very similar to the data presented in the training phase, so this setting does not generalize adequately the methodology proposed.</p><p>Based on the above considerations, in this work some of the latest state-of-art NNs for 3D data have been selected and compared. Moreover, we intend to set up a method that generalizes as much as possible, with case studies one different from the other and with a parameter setting equal for all the scenes, so as to diminish human involvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic Segmentation of Point Clouds</head><p>As well stated in the literature, thanks to their third dimension, Point Clouds can be exploited not only for the 3D reconstruction and modeling of buildings or architectural elements, but also for object detection in many other areas such as, for example, robotics <ref type="bibr" target="#b40">[41]</ref>, indoor navigation or autonomous navigation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>, urban environments <ref type="bibr" target="#b43">[44]</ref><ref type="bibr" target="#b44">[45]</ref><ref type="bibr" target="#b45">[46]</ref>, and it is just in these fields that the exploitation of Point Clouds has been mainly developed. Currently, Point Cloud semantic segmentation approaches in the DL framework can be divided in three categories <ref type="bibr" target="#b46">[47]</ref> A natural choice for the 3D reconstruction in the BIM software is to explore Point Cloud segmentation methods directly on the raw data. Over recent years, several DL approaches are emerging. In contrast to multiview-based <ref type="bibr" target="#b47">[48]</ref><ref type="bibr" target="#b48">[49]</ref><ref type="bibr" target="#b49">[50]</ref><ref type="bibr" target="#b50">[51]</ref><ref type="bibr" target="#b51">[52]</ref> and voxel-based approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>, such approaches do not need specific pre-processing steps, and have been proved to provide state-of-the-art performances in semantic segmentation and classification task on standard benchmarks. Since the main objective of this work is to directly exploit the three-dimensionality of Point Clouds, a comprehensive overview of the multiview and voxel based methods is out of the scope, therefore only the point-based methods will be detailed.</p><p>One of the pioneer and best known DL architectures that works directly on Point Clouds is PointNet <ref type="bibr" target="#b20">[21]</ref>, an end-to-end deep neural network able to learn itself the features for classification, part segmentation and semantic segmentation. It is composed by a sequence of MLPs that first explore the point features and then identify the global features, through the approximation of a symmetric function, the max pooling layer, that also allows to obtain the input permutation invariance. Finally, fully connected layers aggregate the values for labels prediction and classification. However, as stated by the authors, since PointNet does not capture the local geometries, a development has been proposed, PointNet++ <ref type="bibr" target="#b21">[22]</ref>. In this research, a hierarchical grouping is introduced to learn local features thanks to the exploitation of the metric space distances, a good robustness and detail capture has been ensured by the consideration of local neighbourhoods and improved results if compared with their state-of-art have been obtained. Inspiring from PointNet and PointNet++, the works on 3D DL aim at putting attention on feature augmentation, principally to local features and relationships among points. This is done by utilising knowledge from other fields to improve the performance of the basic PointNet and PointNet++ algorithms.</p><p>Several researchers have preferred a substitute to PointNet, applying the convolution as an essential and compelling component, with their deeper understanding on point-based learning. PointCNN adopted a X-transformation instead of symmetric functions to canonicalize the order, which is a generalization of CNNs to feature learning from unorderd and unstructured Point Clouds <ref type="bibr" target="#b54">[55]</ref>. SParse LATtice Networks (SPLATNet) has been proposed as a Point Cloud segmentation framework that could join 2D images with 3D Point Clouds <ref type="bibr" target="#b55">[56]</ref>.</p><p>As regards to convolution kernels as nonlinear functions of the local coordinates of 3D points comprised of weight and density functions, in <ref type="bibr" target="#b56">[57]</ref>, it has been presented PointConv, an extension to the Monte Carlo approximation of the 3D continuous convolution operator.</p><p>Alternative development of PointNet and PointNet++ are <ref type="bibr" target="#b57">[58]</ref><ref type="bibr" target="#b58">[59]</ref><ref type="bibr" target="#b59">[60]</ref> where the features are learnt in a hierarchical manner, in particular <ref type="bibr" target="#b57">[58]</ref>, uses the Kd-trees to subdivide the space and try to take local info into account by also applying a hierarchical grouping. As stated in <ref type="bibr" target="#b60">[61]</ref>, in the Klokov's work the Kd-trees are used as underlying graphs to simulate CNNs and the idea to use graphs for neural networks has been carried out by various researches as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref>. Their use, in recent years, has grown above all thanks to the fact that they have many elements in common with CNNs <ref type="bibr" target="#b42">[43]</ref>, in fact the main elements of the latter such as local connections, the presence of weights and the use of multi-layers, are also characteristic of graphs. Wang et al. <ref type="bibr" target="#b63">[64]</ref> have introduced a Graph Attention Convolution (GAC), in which kernels could be dynamically adapted to the structure of an object. GAC can capture the structural features of Point Clouds while avoiding feature contamination between objects. In <ref type="bibr" target="#b61">[62]</ref>, Landrieu and Simonovsk have proposed SuperPoint Graph (SPG) for achieving richer edge features, offering a representation of the relationships among object parts rather than points. In the partition of the superpoint there is a kind of nonsemantic presegmentation and a downsampling step. After SPG construction, each superpoint is embedded in a basic PointNet network and then refined in Gated Recurrent Units (GRUs) for Point Cloud seantic segmentation. In the DGCNN <ref type="bibr" target="#b31">[32]</ref> the authors reason on the neighborhood, rather than on the single points, building a neighborhood graph that allows to exploit the local geometric structures. With the use of an edge convolutional operator, eventually interleaved with pooling, the process relies on the identification of edge features able to define the relationship between the center point chosen and the edge vector connecting its neighbors to itself. The application of a further K-nn of a point, changing from layer to layer, renders the graph dynamic and allows to calculate the proximity both in the feature space and in the Euclidean one. The advantage of using Dynamic graphs is that they allow to dynamically vary the input signals and, in the case of the DGCNN, they have shown excellent results for classification, part segmentation and indoor scene segmentation respectively tested on the ModelNet40, ShapeNet and S3DIS datasets.</p><p>Another example is <ref type="bibr" target="#b23">[24]</ref>, a recent development of <ref type="bibr" target="#b31">[32]</ref>, in which the authors try to reshape the dimensions of the DGCNN by eliminating the transformation network, used to solve the transformation invariance problem, using MLP to extract the transformation invariant features. It has been noticed that the DGCNN is able to work properly even with very different Point Clouds of considerable size (&gt; 10 M points), see Table <ref type="table" target="#tab_1">1</ref>, while PointNet++ is less generalizable, it seems to have good performances mainly with small datasets and simple classes as the case of ScanNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Materials and Methods</head><p>In this section, we introduce the DL framework as well as the dataset used for evaluation. The framework, as said in the introduction section, is depicted in Figure <ref type="figure" target="#fig_0">1</ref>. We use a novel modified DGCNN for Point Cloud semantic segmentation. Further details are given in the following subsections. The framework is comprehensively evaluated on the ArCH Dataset, a publicly available dataset collected for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ArCH Dataset for Point Cloud Semantic Segmentation</head><p>In the state of the art, the most used datasets to train neural networks are: ModelNet 40 <ref type="bibr" target="#b52">[53]</ref> with more than 100k CAD models of objects, mainly furnitures, from 40 different categories; KITTI <ref type="bibr" target="#b64">[65]</ref> that includes camera images and laser scans for autonomous navigation; Sydney Urban Objects <ref type="bibr" target="#b65">[66]</ref> dataset acquired with Velodyne HDL-64E LiDAR in urban environments with 26 classes and 631 individual scans; Semantic3D <ref type="bibr" target="#b66">[67]</ref> with urban scenes as churches, streets, railroad tracks, squares and so on; S3DIS <ref type="bibr" target="#b67">[68]</ref> that includes mainly office areas and it has been collected with the Matterport scanner with 3D structured light sensors and the Oakland 3-D Point Cloud dataset <ref type="bibr" target="#b68">[69]</ref> consisting of labeled laser scanner 3D Point Clouds, collected from a moving platform in a urban environment. Most of the current datasets collect data from urban environments, with scans composed of around 100 K points, and to date there are still no published datasets focusing on immovable cultural assets with an adequate level of detail.</p><p>Our proposed dataset, named ArCH (Architectural Cultural Heritage) is composed of 11 labeled scenes (Figure <ref type="figure" target="#fig_1">2</ref>), derived from the union of several single scans or from the integration of the latter with photogrammetric surveys. There are also as many Point Clouds which, however, have not been labeled yet, so they have not been used for the tests presented in this work.</p><p>The involved scenes are both indoor and outdoor, with churches, chapels, cloisters, porticoes and archades covered by a variety of vaults and beared by many different types of columns. They belong to different historical periods and different styles, in order to make the dataset the least possible uniform and homogeneous (in the repetition of the architectural elements) and the results as general as possible.</p><p>Different case studies are taken into exam and are described as follows: The Sacri Monti (Sacred Mounts) of Ghiffa (SMG) and Varallo (SMV); The Sanctuary of Trompone (TR); The Church of Santo Stefano (CA); The indoor scene of the Castello del Valentino (VA). The ArCH Dataset is publicly available(http://vrai.dii.univpm.it/content/arch-dataset-point-cloud-semantic-segmentation) for research purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The Sacri Monti (Sacred Mounts) of Ghiffa and Varallo. These two devotional complexes in northern Italy have been included in the UNESCO World Heritage List (WHL) in 2003. In the case of the Sacro Monte di Ghiffa, a 30 m loggia with tuscanic stone columns and half pilasters has been chosen; while for the Sacro Monte of Varallo 6 buildings have been included in the dataset, containing a total of 16 chapels, some of which very complex from an architectural point of view: barrel vaults, sometimes with lunettes, cross vaults, arcades, balustrades and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The Sanctuary of Trompone (TR). This is a wide complex dating back to the 16th century and it consists of a church (about 40 × 10 m) and a cloister (about 25 × 25 m), both included in the dataset. The internal structure of the church is composed of 3 naves covered by cross vaults supported in turn by stone columns. There is also a wide dome at the apse and a series of half-pilasters covering the sidewalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The Church of Santo Stefano (CA) has a completely different compositional structure if compared with the previous one, being a small rural church from the 11th century. There is a stone masonry, not plastered, brick arches above the small windows and a series of Lombard band defining a decorated moulding under the tiled roof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>The indoor scene of the Castello del Valentino (VA) is a courtly room part of an historical building recast from the 17th century. This hall is covered by cross vaults leaning on six sturdy breccia columns. Wide French windows illuminate the room and oval niches surrounded by decorative stuccoes are placed on the sidewalls. This case study is part of a serial site inserted in the WHL. of UNESCO in 1997. In the majority of cases, the final scene was obtained through the integration of different Point Clouds, those acquired with the terrestrial laser scanner (TLS), and those deriving from photogrammetry (mainly aerial for surveying the roofs), after appropriate evaluation of the accuracy. This integration results in a complete Point Cloud, with different density according to the sensors used, however leading to increasing the overall Point Cloud size and requiring a pre-processing phase for the NN.</p><p>The common structure of the Point Clouds is therefore based on the sequence of the coordinates x, y, z and the R, G, B values.</p><p>In the future, other point clouds will be added to the ArCH dataset, to improve the representation of complex CH objects with the potential contribution of all the other researchers involved in this field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Pre-Processing</head><p>To prepare the dataset for the network, pre-processing operations have been carried out in order to make the cloud structures more homogeneous. The pre-processing methods, for this dataset, have followed 3 steps: translation, subsampling and choice of features.</p><p>The spatial translation of the Point Clouds is necessary because of the georeferencing of the scenes, the coordinate values are in fact too large to be processed by the deep network, so the coordinates are truncated and each single scene is spatially moved close to the point cardinal (0,0,0). This operation on the one hand has led to the loss of georeferencing, on the other hand, however, it has made possible to reduce the size of the files and the space to be analyzed, thus also leading to a decrease in the required computational power.</p><p>The subsampling operation, which became necessary due to the high number of points (mostly redundant) present in each scene (&gt;20 M points), was instead more complex. It was in fact necessary to establish which of the three different subsampling options was the most adequate to provide the best typology of input data to the neural network. The option of random subsampling was discarded because it would limit the test repeatability, then both the other two methods have been tested: octree and space. The first is efficient for nearest neighbor extraction, while the second provides, in the output Point Cloud, points not closer than the distance specified. As far as space is concerned, it has been set a minimum space between points of 0.01 m, in this way a high level of detail is ensured, but at the same time it is possible to considerably reduce the number of points and the size of the file, in addition to regularize the geometric structure of the Point Cloud As for the octree, applied only in the first tests on half of the Trompone Church scene, level 20 was set, so that the number of final points was more or less similar to that of the scene subsampled with the space method. The software used for this operation is CloudCompare. An analysis of the number of points for each scene is detailed in Table <ref type="table" target="#tab_1">1</ref>, where it is possible to see the lack of points for some classes and the highest total value for the 'Wall' class.</p><p>The extraction of features directly from the Point Clouds is instead an open and constantly evolving field of research. Most of the features are handcrafted for specific tasks and can be subdivided and classified into intrinsic and extrinsic, or also used for local and global descriptors <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b69">70]</ref>. The local features define statistical properties of the local neighborhood geometric information, while the global features describe the whole geometry of the Point Cloud. Those mostly used are the local ones, such as eigenvalues based descriptors, 3D Shape context and so on, however in our case, since the last networks developed <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b31">32]</ref> tend to let the network itself learn the features and since our main goal is to generalize as much as possible, in addition to reduce the human involvement in the pre-proccessing phases, the only features calculated are the normals and the curvature. The normals are calculated on Cloud Compare and have been computed and orientated with different settings depending on the surface model and 3D data acquisition. Specifically a plane or quadric 'local surface model' as surface approximation for the normals computation has been used and a 'minimum spanning tree' with Knn = 10 has been set for their orientation. The latter has been further checked on MATLAB ® .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Deep Learning for Point Cloud Semantic Segmentation</head><p>State-of-the-art deep neural networks are specifically designed to deal with the irregularity of Point Clouds, directly managing raw Point Cloud data rather than using an intermediate regular representation. In this contribution, the performances obtained with the ArCH dataset of some state-of-art architectures are therefore compared and then evaluated with regards to the DGCNN we have modified. The NNs selected are:</p><p>•</p><p>PointNet <ref type="bibr" target="#b20">[21]</ref>, as it was the pioneer of this approach, obtaining permutation invariance of points by operating on each point independently and applying a symmetric function to accumulate features. • its extensions PointNet++ <ref type="bibr" target="#b21">[22]</ref> that analyzes neighborhoods of points in preference of acting on each separately, allowing the exploitation of local features even if with still some important limitations. • PCNN <ref type="bibr" target="#b70">[71]</ref>, a DL framework for applying CNN to Point Clouds generalizing image CNNs. The extension and restriction operators are involved, permitting the use of volumetric functions associated to the Point Cloud. • DGCNN <ref type="bibr" target="#b31">[32]</ref> that addresses these shortcomings by adding the EdgeConv operation. EdgeConv is a module that creates edge features describing the relationships between a point and its neighbors rather than generating point features directly from their embeddings. This module is permutation invariant and it is able to group points thanks to local graph, learning from the edges that link points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">DGCNN for DCH Point Cloud Dataset</head><p>In our experiments, we build upon the DGCNN implementation provided by <ref type="bibr" target="#b31">[32]</ref>. Such an implementation of DGCNN uses k-nearest neighbors (kNN) to individuate the k points closest to the point to be classified, thus defining the neighboring region of the point. The edge features are then calculated from such a neighboring region and provided as input to the following layer of the network. Such a edge convolution operation is performed on the output of each layer of the network. In the original implementation, at the input layer kNN is fed with normalized points coordinates only, while in our implementation we use all the available features. Specifically, we added color features, expressed as RGB or HSV, and normal vectors.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the overall structure of the network. We give in input a scene block, composed of 12 features for each point: XYZ coords, X'Y'Z' normalized coords, color features (HSV channels), normals features. These blocks pass through 4 EdgeConv layers and a max-pooling layer to extract global features of the block. The original XYZ coordinates are kept to take into account the positioning of the points in the whole scene, while the normalized coordinates represent the positioning within each block. The KNN module is fed with normalized coordinates only and both original and normalized coordinates are used as input features for the neural network. RGB channels have been converted to HSV channels in two steps: first they are normalized to values between 0 and 1, then they are converted to HSV channels using the rgb2hsv() function of the scikit-image library implemented in python. This conversion is useful because the individual channels H, S and V are independent one from the other, each of them has a different typology information, making them independent features. Channels R, G and B are conversely somehow related to each other, they share a part of the same data type, so they should not be used separately.</p><p>The choice of using normals and HSV is based on different reasons. On one side the RGB component, based on the sensors used in data acquisition, is most of the time present as a property of the point cloud and therefore it has been decided to fully exploit this kind of data; on the other the RGB components define the radiometric properties of the point cloud, while the normals define some geometric properties. In this way we are using as input for the NN different kinds of information. Moreover, the decision to convert the RGB into HSV is borrowed from other research works <ref type="bibr" target="#b71">[72]</ref> that, even if developed for different tasks, show the effectiveness of this operation.</p><p>We have therefore tried to support the NN, in order to increase the accuracy, with few common features that could be easily obtained by any user.</p><p>These features are then concatenated with the local features of each EdgeConv. We have modified the first EdgeConv layer so that the kNN could also use color and normal features to be able to select k neighbors for each point. Finally, through 3 convolutional layers and a dropout layer, we will output the same block of points but with segmentation scores (one for each class to be recognized). The output of the segmentation will be given by the class with the largest score. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>In this section, the results of the experiments conducted on "ArCH Dataset" are reported. In addition to the performance of our modified DGCNN, we also present the performance of PointNet <ref type="bibr" target="#b20">[21]</ref>, PointNet++ <ref type="bibr" target="#b21">[22]</ref>, PCNN <ref type="bibr" target="#b70">[71]</ref> and DGCNN <ref type="bibr" target="#b31">[32]</ref> which form the basis of the improvement of our network.</p><p>Our experiments are separated in two phases. In the first one, we attempt at tuning the networks, choosing the best parameters for the task of semantically segmenting our ArCH dataset. To this end we have considered a single scene and used an annotated portion of it for training the network, evaluating the performances on the remaining portion of the scene. We have chosen to perform such first experiments on the TR_church (Trompone) as it presents a relatively high symmetry, allowing us to subdivide it in parts with similar characteristics, and it includes almost all the considered classes (9 out of the 10). Such an experimental setting addresses the problem of automatically annotating a scene that has been only partially annotated manually. While this has in fact practical applications, and could speed up the process of annotating an entire scene, our goal is to evaluate the automatic annotation of an scene that was never seen before. We address this more challenging problem, in the second experimental phase, where we train the networks with 10 different scenes and then attempt at automatically segmenting the remaining one. Segmentation of the entire Point Cloud into sub-parts (blocks) is a needed pre-processing step for all the analysed neural architectures. For each block a fixed number of points have to be sampled. This is due to the fact that neural networks need a constant number of points as input and that it would be computationally unfeasible to provide all the points at once to the networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Segmentation of Partially Annotated Scene</head><p>Two different settings have been evaluated in this phase: a k-fold cross-validation and a single splitting of the labeled dataset into a training set and a test set. In the first case the overall number of test samples is small and the network is trained on more samples. In the second case, an equal number of samples is used to train and to evaluate the network, possibly leading to very different results. This, for completeness, we experimented with both settings.</p><p>In the first setting, the TR_church scene was divided into 6 parts and we have performed a cross-validation with 6 Fold, as shown in Figure <ref type="figure">4</ref>. We have tested different combinations of hyperparameters of the various networks to be able to verify which was the best, as we can see in Table <ref type="table" target="#tab_3">2</ref>, where the mean accuracy is derived from calculating the accuracy of each test (fold), then averaging them.  Regarding the pointcloud preprocessing steps, which consists in segmenting the whole scene into blocks and, for each block, sampling a number of points, we used, for each evaluated model, the settings used in the corresponding original paper. PointNet and PointNet++ use blocks are of size 2 × 2 meters and 4096 points for cloud sampling. In the case of the DGCNN, we have used blocks of size 1 × 1 and 4096 points per block. Finally, the PCNN network was tested using the same sampling as the DGCNN (1 × 1), but using 2048 points, as this is the default setting used in the PCNN paper. We also tested PCNN providing 4096 points per block, but results were slightly worse. We also notice that the performances improve slightly using the color features represented as HSV color-map. The HSV (hue, saturation, value) representation is known for more closely aligning with the human perception of colors and, by representing colors as three independent variables, allows, for example, to take into account variations, e.g., due to shadows and different light conditions.</p><p>In the second setting, we have split the TR-cloister scene in half, choosing the left side for the training and the right side for the test. Furthermore, we split the left side into a training set (80%) and a validation set (20%). We used the validation set to test overall accuracy at the end of each training epoch and we performed evaluation on the test set (right side). In Table <ref type="table" target="#tab_4">3</ref>, the performance of state-of-the-art networks are reported. We report results obtained with the hyperparameters combinations that best performed in the cross-validation experiment. Table <ref type="table" target="#tab_5">4</ref> shows the metrics in the test phase for each class of the Trompone's right side. In this table we report, for each class, precision, recall, F-1 score and Intersection over Union (IoU). This table allows us to understand which are the classes that are best discriminated by the various approaches, to understand which are their weak points and their strengths (as broadly discussed in Section 5).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Segmentation of an Unseen Scene</head><p>In the second experimental phase, we used all the scenes of ArCH Dataset: 9 scenes were used for the Training, 1 scene as Validation (Ghiffa scene), 1 scene for the Final Test (SMV). State-of-the-art networks were evaluated, comparing the results with our DGCNN-based approach. In Table <ref type="table" target="#tab_6">5</ref>, the overall performances are reported for each tested model, while in Table <ref type="table">6</ref> reports detailed results on the individual classes of the test scene. Figures <ref type="figure">6</ref> and<ref type="figure" target="#fig_5">7</ref> depict the confusion matrix and the segmentation results of the last experiment: 9 scenes for Training, 1 scene for validation and 1 scene for test.</p><p>The performance gain provided by our approach is more evident than in previous experiments, leading to an improvement of around 0.8 in overall accuracy as well as in F1-score. The IoU also increases. In Table <ref type="table">6</ref>, one can see that our approach outperforms the others in the segmentation of almost all classes. For some classes values of Precision and Recall are lower than the original DGCNN. However, our modified DGCNN generally improves performance in terms of F1-score. This metric is a combination of Precision and Recall, thus it allows to better understand how the network is learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>This research rises remarkable research directions (and challenges) that is worth to deepen. First of all, looking at the first experimental setting, performances are worse than those obtained in the K-fold experiment (referring to Table <ref type="table" target="#tab_4">3</ref>). This is probably do to the fact that the network has less points to learn on. As in the previous experiment, the results on the test set is obtained with our approach, confirming that HSV and Normals does in fact help the network to learn higher level features of the different classes. Besides, as reported in Table <ref type="table" target="#tab_5">4</ref> and confirmed in Figure <ref type="figure" target="#fig_4">5</ref>, we can notice that using our setting helps in detecting vaults, increasing precision, recall and IoU, as well as columns and stairs, by sensibly increasing recall and IoU.</p><p>Dealing with the second experimental setting (see Section 4.2), it is worth to notice that all evaluated approaches fail in recognizing classes with low support, as doors, windows and arcs. Beside, for these classes we observe a high variability in shapes across the dataset, this probably contributes to the bad accuracy obtained by the networks.</p><p>More insights can be drawn from the confusion matrix, shown in Figure <ref type="figure">6</ref>. It reveals, for example, that arcs are often confused with vaults, as they clearly share geometrical features, while columns are often confused with walls. The latter behaviour can be possibly due to the presence of half-pilasters, which are labeled as columns but have a shape similar to walls. The unbalanced nature of the number of points per class is clearly highlighted in Figure <ref type="figure" target="#fig_6">8</ref>.</p><p>Furthermore, if we consider the classes individually, we can see that the lowest values are in Arc, Dec, Door and Window. More in detail:</p><p>• Arc: the geometry of the elements of this class is very similar to that of the vaults and, although the dimensions of the arcs are not similar to the latter, most of the time they are really close to the vaults, almost a continuation of these elements. For these reasons the result is partly justifiable and could lead to the merging of these two classes. • Dec: in this class, which can also be defined as "Others" or "Unassigned", all the elements that are not part of the other classes (such as benches, paintings, confessionals . . . ) are included. Therefore it is not fully considered among the results. • Door: the null result is almost certainly due to the very low number of points present in this class (Figure <ref type="figure" target="#fig_6">8</ref>). This is due to the fact that, in the proposed case studies of CH, it is more common to find large arches that mark the passage from one space to another and the doors are barely present. In addition, many times, the doors were open or with occlusions, generating a partial view and acquisition of these elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>Window: in this case the result is not due to the low number of windows present in the case study, but to the high heterogeneity between them. In fact, although the number of points in this class is greater, the shapes of the openings are very different from each other (three-foiled, circular, elliptical, square and rectangular) (Figure <ref type="figure" target="#fig_7">9</ref>). Moreover, being mostly composed of glazed surfaces, these surfaces are not detected by the sensors involved such as the TLS, therefore, unlike the use of images, in this case the number of points useful to describe these elements is reduced.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The semantic segmentation of Point Clouds is a relevant task in DCH as it allows to automatically recognise different types of historical architectural elements, thus possibly saving time and speeding up the process of analysing Point Clouds acquired on-site and building parametric 3D representations. In the context of historical buildings, Point Cloud semantic segmentation is made particularly challenging by the complexity and high variability of the objects to be detected. In this paper, we provide a first assessment of state-of-the-art DL based Point Cloud segmentation techniques in the Historical Building context. Beside comparing the performances of existing approaches on ArCH (Architectural Cultural Heritage), created on purpose and released to the research community, we propose an improvement which increase the segmentation accuracy, demonstrating the effectiveness and suitability of our approach. Our DL framweork is based on a modified version of DGCNN and it has been applied to a newly puclic collected dataset: ArCH (Architectural Cultural Heritage). Results prove that the proposed methodology is suitable for Point Cloud semantic segmentation with relevant applications. The proposed research starts from the idea of collecting relevant DCH dataset which is shared together with the framework source codes to ensure comparisons with the proposed method and future improvements and collaborations over this challenging problems. The paper describes one of the more extensive test based on DCH data, and it has huge potential in the field of HBIM, in order to make the scan-to-BIM process affordable.</p><p>However, the developed framework highlighted some shortcomings and open challenges that is fair to mention. First of all, the framework is not able to assess the accuracy performances with respect to the acquisition techniques. In other words, we seek to uncover, in future experiments, if the adoption of Point Clouds acquired with other methods changes the DL framework performances. Moreover, as stated in the results section, the dimensions of points per classes is unbalanced and not homogeneous, with the consequent biases in the confusion matrix. This bottleneck could be solved by labelling a more detailed dataset or creating synthetic Point Clouds. The research group is focusing his efforts even in this direction <ref type="bibr" target="#b72">[73]</ref>. In future works, we plan to improve and better integrate the framework with more effective architectures, in order to improve performances and test also different kind of input features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b73">74]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. DL Framework for Point Cloud Semantic Segmentation.</figDesc><graphic coords="3,87.59,127.12,420.10,193.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. ArCH dataset. On the left column the RGB point clouds and on the right the annotated scenes. 10 classes have been identified: Arc, Column, Door, Floor, Roof, Stairs, Vault, Wall, Window and Decoration. The Decoration class includes all the points unassigned to the previous classes, as benches, balaustrades, paintings, altars and so on.</figDesc><graphic coords="9,87.59,147.99,420.10,491.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of our modified DGCNN architecture.</figDesc><graphic coords="12,87.59,87.88,420.10,224.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 . 6 -</head><label>46</label><figDesc>Figure 4. 6-Fold Cross Validation on the TR_church scene. The white fold in every experiment is the scene part used for the test.</figDesc><graphic coords="13,83.50,87.89,428.27,245.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Ground Truth and Predicted Point Cloud, by using our Approach on Trompone's Test side.</figDesc><graphic coords="15,86.44,87.88,422.40,237.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Ground truth (a) and predicted Point Cloud (b), by using our approach on the last experiment: 9 scenes for Training, 1 scene for Validation and 1 scene for Test.</figDesc><graphic coords="17,86.44,87.88,422.40,172.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Number of points per class.</figDesc><graphic coords="18,90.97,374.69,413.34,231.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Different typologies of windows and doors. For the latter, their opening has sometimes affected the points acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="16,83.14,401.09,429.00,300.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Number of points per class and overall for the whole scene. The point cloud of the Trompone church has been split into right (r) and left (l) part according to the tests conducted in Section 4.</figDesc><table><row><cell>Scene/Class</cell><cell>Arc</cell><cell cols="2">Column Decoration</cell><cell>Floor</cell><cell>Door</cell><cell>Wall</cell><cell>Window</cell><cell>Stairs</cell><cell>Vault</cell><cell>Roof</cell><cell>TOTAL</cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell></cell></row><row><cell>TR_cloister</cell><cell>900,403</cell><cell>955,791</cell><cell>765,864</cell><cell>1,948,029</cell><cell>779,019</cell><cell>10,962,235</cell><cell>863,792</cell><cell>2806</cell><cell>2,759,284</cell><cell>1,223,300</cell><cell>21,160,523</cell></row><row><cell>TR_church_r</cell><cell>466,472</cell><cell>658,100</cell><cell>1,967,398</cell><cell>1,221,331</cell><cell>85,001</cell><cell>3,387,149</cell><cell>145,177</cell><cell>84,118</cell><cell>2,366,115</cell><cell>0</cell><cell>10,380,861</cell></row><row><cell>TR_church_l</cell><cell>439,269</cell><cell>554,673</cell><cell>1,999,991</cell><cell>1,329,265</cell><cell>44,241</cell><cell>3,148,777</cell><cell>128,433</cell><cell>38,141</cell><cell>2,339,801</cell><cell>0</cell><cell>10,022,591</cell></row><row><cell>VAL</cell><cell>300,923</cell><cell>409,123</cell><cell>204,355</cell><cell>1,011,034</cell><cell>69,830</cell><cell>920,418</cell><cell>406,895</cell><cell>0</cell><cell>869,535</cell><cell>0</cell><cell>4,192,113</cell></row><row><cell>CA</cell><cell>17,299</cell><cell>172,044</cell><cell>0</cell><cell>0</cell><cell>30,208</cell><cell>3,068,802</cell><cell>33,780</cell><cell>11,181</cell><cell>0</cell><cell>1,559,138</cell><cell>4,892,452</cell></row><row><cell>SMG</cell><cell>309,496</cell><cell>1,131,090</cell><cell>915,282</cell><cell>1,609,202</cell><cell>18,736</cell><cell>7,187,003</cell><cell>137,954</cell><cell>478,627</cell><cell>2,085,185</cell><cell>7,671,775</cell><cell>21,544,350</cell></row><row><cell>SMV_1</cell><cell>46,632</cell><cell>314,723</cell><cell>409,441</cell><cell>457,462</cell><cell>0</cell><cell>1,598,516</cell><cell>2011</cell><cell>274,163</cell><cell>122,522</cell><cell>620,550</cell><cell>3,846,020</cell></row><row><cell>SMV_naz</cell><cell>472,004</cell><cell>80,471</cell><cell>847,281</cell><cell>1,401,120</cell><cell>42,362</cell><cell>2,846,324</cell><cell>16,559</cell><cell>232,748</cell><cell>4,378,069</cell><cell>527,490</cell><cell>10,844,428</cell></row><row><cell>SMV_24</cell><cell>146,104</cell><cell>406,065</cell><cell>154,634</cell><cell>20,085</cell><cell>469</cell><cell>366,2361</cell><cell>6742</cell><cell>131,137</cell><cell>305,086</cell><cell>159,480</cell><cell>4,992,163</cell></row><row><cell>SMV_28</cell><cell>36,991</cell><cell>495,794</cell><cell>18,826</cell><cell>192,331</cell><cell></cell><cell>1,965,782</cell><cell>4481</cell><cell>13,734</cell><cell>184,261</cell><cell>197,679</cell><cell>3,109,879</cell></row><row><cell>SMV_pil</cell><cell>584,981</cell><cell>595,117</cell><cell>1,025,534</cell><cell>1,146,079</cell><cell>26,081</cell><cell>7,358,536</cell><cell>313,925</cell><cell>811,724</cell><cell>2,081,080</cell><cell>3,059,959</cell><cell>17,003,016</cell></row><row><cell>SMV_10</cell><cell>0</cell><cell>16,621</cell><cell>0</cell><cell>125,731</cell><cell>0</cell><cell>1,360,738</cell><cell>106,186</cell><cell>113,287</cell><cell>0</cell><cell>499,159</cell><cell>2,221,722</cell></row><row><cell>TOTAL</cell><cell>3,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>720,574 5,789,612 8,308,606 10,461,669 1,095,947 47,466,641 2,165,935 2,191,666 17,490,938 15,518,530 114,210,118</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc><ref type="bibr" target="#b5">6</ref>-Fold Cross-Validation on the Trompone scene. We can see different combinations of hyperparameters for the various state-of-the-art networks.</figDesc><table><row><cell>Network</cell><cell>Features</cell><cell>Mean Acc.</cell></row><row><cell>DGCNN</cell><cell>XYZ + RGB</cell><cell>0.897</cell></row><row><cell>PointNet++</cell><cell>XYZ</cell><cell>0.543</cell></row><row><cell>PointNet</cell><cell>XYZ</cell><cell>0.459</cell></row><row><cell>PCNN</cell><cell>XYZ</cell><cell>0.742</cell></row><row><cell>DGCNN-Mod</cell><cell>XYZ + Norm</cell><cell>0.781</cell></row><row><cell>Ours</cell><cell>XYZ + HSV + Norm</cell><cell>0.918</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>The scene was divided into 3 parts: Train, Validation, Test. In this table, we can see the average of the metrics calculated on the different parts: accuracy for Train, Validation and Test; precision, recall, F1-score and support for the Test.</figDesc><table><row><cell>Network</cell><cell cols="4">Train Acc. Valid Acc. Test Acc. Prec.</cell><cell cols="2">Rec. F1-Score</cell><cell>Supp.</cell></row><row><cell>DGCNN</cell><cell>0.993</cell><cell>0.799</cell><cell>0.733</cell><cell cols="2">0.721 0.733</cell><cell>0.707</cell><cell>1,437,696</cell></row><row><cell>PointNet++</cell><cell>0.887</cell><cell>0.387</cell><cell>0.441</cell><cell cols="2">0.480 0.487</cell><cell>0.448</cell><cell>1,384,448</cell></row><row><cell>PointNet</cell><cell>0.890</cell><cell>0.320</cell><cell>0.307</cell><cell cols="2">0.405 0.306</cell><cell>0.287</cell><cell>1,335,622</cell></row><row><cell>PCNN</cell><cell>0.961</cell><cell>0.687</cell><cell>0.623</cell><cell cols="2">0.642 0.608</cell><cell>0.636</cell><cell>1,254,631</cell></row><row><cell>Ours</cell><cell>0.992</cell><cell>0.745</cell><cell>0.743</cell><cell cols="2">0.748 0.742</cell><cell>0.722</cell><cell>1,437,696</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>The scene was divided into 3 parts: Train, Validation, Test. In this table we can see the metrics for every class, calculated on the Test set.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Results of the tests performed on an unknown scene, training the network on the others.</figDesc><table><row><cell>Network</cell><cell cols="3">Valid Acc. Test Acc. Prec.</cell><cell cols="2">Rec. F1-Score</cell><cell>Supp.</cell></row><row><cell>DGCNN</cell><cell>0.756</cell><cell>0.740</cell><cell cols="2">0.768 0.740</cell><cell>0.738</cell><cell>2,613,248</cell></row><row><cell>PointNet++</cell><cell>0.669</cell><cell>0.528</cell><cell cols="2">0.532 0.528</cell><cell>0.479</cell><cell>2,433,024</cell></row><row><cell>PointNet</cell><cell>0.453</cell><cell>0.351</cell><cell cols="2">0.536 0.351</cell><cell>0.269</cell><cell>2,318,440</cell></row><row><cell>PCNN</cell><cell>0.635</cell><cell>0.629</cell><cell cols="2">0.653 0.622</cell><cell>0.635</cell><cell>2,482,581</cell></row><row><cell>Ours</cell><cell>0.831</cell><cell>0.825</cell><cell cols="2">0.809 0.825</cell><cell>0.814</cell><cell>2,613,248</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Funding: This research received no external funding.</p><p>Acknowledgments: This research is partially funded by the CIVITAS (ChaIn for excellence of reflectiVe societies to exploit dIgital culTural heritAge and museumS) project <ref type="bibr" target="#b74">[75]</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Finally, Figure <ref type="figure">5</ref> depicts the manually annotated test scene (ground truth) and the automatic segmentation results obtained with our approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflicts of Interest:</head><p>The authors declare no conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From 2D to 3D supervised segmentation and classification for cultural heritage applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dininno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petrucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-399-2018</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="399" to="406" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Performance Evaluation of Two Indoor Mapping Systems: Low-Cost UWB-Aided Photogrammetry and Backpack Laser Scanning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Masiero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fissore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guarnieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pirotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Visintini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vettore</surname></persName>
		</author>
		<idno type="DOI">10.3390/app8030416</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">416</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laser-Visual-Inertial Odometry based solution for 3D Heritage modeling: The sanctuary of the blessed Virgin of Trompone</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bronzino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Grasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Matrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Osello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Piras</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W15-215-2019</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HBIM and augmented information: Towards a wider user community of image and range-based reconstructions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barazzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Banfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oreni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Previtali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Roncoroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International CIPA Symposium 2015 on the International Archives of the Photogrammetry</title>
		<meeting>the 25th International CIPA Symposium 2015 on the International Archives of the Photogrammetry<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-04">31 August-4 September 2015</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="35" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HBIM and virtual tools: A new chance to preserve architectural heritage</title>
		<author>
			<persName><forename type="first">A</forename><surname>Osello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lucibello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morgagni</surname></persName>
		</author>
		<idno type="DOI">10.3390/buildings8010012</idno>
	</analytic>
	<monogr>
		<title level="j">Buildings</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From point cloud to digital fabrication: A tangible reconstruction of Ca&apos;Venier dei Leoni, the Guggenheim Museum in Venice</title>
		<author>
			<persName><forename type="first">C</forename><surname>Balletti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>D'agnano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vernier</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprsannals-III-5-43-2016</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">43</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">From a Point Cloud Survey to a mass 3D modelling: Renaissande HBIM in Poggio a Caiano</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bolognesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garagnani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="page">42</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Historical buildings models and their handling via 3D survey: From points clouds to user-oriented HBIM</title>
		<author>
			<persName><forename type="first">F</forename><surname>Chiabrando</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sammartano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanò</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprsarchives-xli-b5-633-2016</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Survey and modelling for the BIM of Basilica of San Marco in Venice</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fregonese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Taffurelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chiarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Helder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spezzoni</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W3-303-2017</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vault Modeling with Neural Networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barazzetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Previtali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on 3D Virtual Reconstruction and Visualization of Complex Architectures, 3D-ARCH 2019</title>
		<meeting>the 8th International Workshop on 3D Virtual Reconstruction and Visualization of Complex Architectures, 3D-ARCH 2019<address><addrLine>Bergamo, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Copernicus GmbH</publisher>
			<date type="published" when="2019-02-08">6-8 February 2019</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="81" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<author>
			<persName><forename type="first">P</forename><surname>Borin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cavazzini</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W15-201-2019</idno>
	</analytic>
	<monogr>
		<title level="m">Condition Assessment of RC Bridges. Integrating Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A restoration oriented HBIM system for Cultural Heritage documentation: The case study of Parma cathedral</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Roncella</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-171-2018</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">HBIM and conservation plan of a monumental building damaged by earthquake</title>
		<author>
			<persName><forename type="first">D</forename><surname>Oreni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brumana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Della Torre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Banfi</surname></persName>
		</author>
		<author>
			<persName><surname>Survey</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-5-W1-337-2017</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">D Geomatics Techniques for an integrated approach to Cultural Heritage knowledge: The case of San Michele in Acerboli&apos;s Church in Santarcangelo di Romagna</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bitelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dellapasqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Girelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sanchini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tini</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-5-W1-291-2017</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge-based data enrichment for HBIM: Exploring high-quality models using the semantic-web</title>
		<author>
			<persName><forename type="first">R</forename><surname>Quattrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pierdicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morbidoni</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.culher.2017.05.004</idno>
	</analytic>
	<monogr>
		<title level="j">J. Cult. Herit</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="129" to="139" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scan-to-BIM vs 3D ideal model HBIM: Parametric tools to study domes geometry</title>
		<author>
			<persName><forename type="first">M</forename><surname>Capone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lanzara</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W9-219-2019</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point Cloud Segmentation and Semantic Annotation Aided by GIS Data for Heritage Complexes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murtiyoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grussenmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop 3D-ARCH &quot;3D Virtual Reconstruction and Visualization of Complex Architecture</title>
		<meeting>the 8th International Workshop 3D-ARCH &quot;3D Virtual Reconstruction and Visualization of Complex Architecture<address><addrLine>Bergamo, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-02-08">6-8 February 2019</date>
			<biblScope unit="page" from="523" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic Heritage Building Point Cloud Segmentation and Classification Using Geometrical Rules</title>
		<author>
			<persName><forename type="first">A</forename><surname>Murtiyoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grussenmeyer</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W15-821-2019</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Application of Machine and Deep Learning strategies for the classification of Heritage Point Clouds</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Özdemir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-4-W18-447-2019</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Point cloud segmentation for cultural heritage sites</title>
		<author>
			<persName><forename type="first">S</forename><surname>Spina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Debattista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bugeja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chalmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International conference on Virtual Reality, Archaeology and Cultural Heritage</title>
		<meeting>the 12th International conference on Virtual Reality, Archaeology and Cultural Heritage<address><addrLine>Prato, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-10-21">18-21 October 2011</date>
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName><surname>Pointnet++</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgpn: Similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="2569" to="2578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linked Dynamic Graph CNN</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10014</idno>
	</analytic>
	<monogr>
		<title level="m">Learning on Point Cloud via Linking Hierarchical Features</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV 2014, European Conference on Computer Vision</title>
		<meeting>the ECCV 2014, European Conference on Computer Vision<address><addrLine>Zurich, Switzerland; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014-09-12">6-12 September 2014. 2014</date>
			<biblScope unit="page" from="634" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-07-01">26 June-1 July 2016</date>
			<biblScope unit="page" from="808" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning in remote sensing applications: A meta-analysis and review</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.04.015</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="page" from="166" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic reconstruction of as-built building information models from laser-scanned point clouds: A review of related techniques</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Akinci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lytle</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.autcon.2010.06.007</idno>
	</analytic>
	<monogr>
		<title level="j">Autom. Constr</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="829" to="843" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An Automated Approach to the Generation of Structured Building Information Models from Unstructured 3d Point Cloud Scans</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tamke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Evers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zwierzycki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ochmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IASS Annual Symposia. International Association for Shell and Spatial Structures (IASS)</title>
		<meeting>the IASS Annual Symposia. International Association for Shell and Spatial Structures (IASS)<address><addrLine>Tokio, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-09">September 2016</date>
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From point clouds to building information models: 3D semi-automatic reconstruction of indoors of existing buildings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Macher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grussenmeyer</surname></persName>
		</author>
		<idno type="DOI">10.3390/app7101030</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<date type="published" when="1030">2017, 7, 1030</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Automatic geometry generation from point clouds for BIM</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Boehm</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs70911753</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="11753" to="11775" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="DOI">10.1145/3326362</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph. TOG</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic architectural style recognition</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weissenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Haegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprsarchives-XXXVIII-5-W16-171-2011</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS-Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">3816</biblScope>
			<biblScope unit="page" from="171" to="176" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Image-based delineation and classification of built heritage masonry</title>
		<author>
			<persName><forename type="first">N</forename><surname>Oses</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dornaika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moujahid</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs6031863</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1863" to="1889" />
		</imprint>
	</monogr>
	<note>Remote Sens</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic photogrammetry: Boosting image-based 3D reconstruction with semantic labeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Stathopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-archives-XLII-2-W9-685-2019</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Arch. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Classification of architectural heritage images using deep learning techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Llamas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lerones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Medina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zalama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gómez-García-Bermejo</surname></persName>
		</author>
		<idno type="DOI">10.3390/app7100992</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">992</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Classification of 3D Digital Heritage. Remote Sens</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<idno type="DOI">10.3390/rs11070847</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segmentation of 3D Models for Cultural Heritage Structural Analysis-Some Critical Issues</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Barsanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>De Luca</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-annals-IV-2-W2-115-2017</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<date type="published" when="2017">2017, 4, 115</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06114</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2015.01.016</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="page" from="286" to="304" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)<address><addrLine>Hamburg, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-02">28 September-2 October 2015</date>
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Lonch Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">26-20 June 2019</date>
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fusion of images and point clouds for the semantic segmentation of large-scale 3D scenes based on deep learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2018.04.022</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="page" from="85" to="96" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structural segmentation and classification of mobile laser scanning point clouds with large variations in point density</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ge</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.isprsjprs.2019.05.007</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Object recognition, segmentation, and classification of mobile laser scanning point clouds: A state of the art review</title>
		<author>
			<persName><forename type="first">E</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Olsen</surname></persName>
		</author>
		<idno type="DOI">10.3390/s19040810</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08854</idno>
		<title level="m">A Review of Point Cloud Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unstructured Point Cloud Semantic Labeling Using Deep Segmentation Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>3DOR 2017, 2, 7</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SnapNet: 3D point cloud semantic labeling with 2D deep segmentation networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Guerry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Le Saux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cag.2017.11.010</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep 3d object classification and retrieval using stereographic projection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yavartanoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>Spnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<meeting><address><addrLine>Berlin, German</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="691" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Group-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Gvcnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="264" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Classifying airborne LiDAR point clouds via deep features learned by a multi-scale convolutional neural network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1080/13658816.2018.1431840</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Geogr. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="960" to="979" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">VoxSegNet: Volumetric CNNs for semantic part segmentation of 3D shapes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2019.2896310</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graph</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Pointcnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<title level="m">Convolution on x-transformed points. Advances in Neural Information Processing Systems. arXiv 2018</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Sparse lattice networks for point cloud processing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName><surname>Splatnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pointconv: Deep convolutional networks on 3d point clouds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">16-20 June 2019</date>
			<biblScope unit="page" from="9621" to="9630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hee Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">DContextNet: Kd Tree Guided Hierarchical Learning of Point Clouds Using Local and Global Contextual Cues</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A network architecture for point cloud classification via automatic depth images generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Roveri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rahmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="4176" to="4184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Dynamic edge-conditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="3693" to="3702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Graph Attention Convolution for Point Cloud Semantic Segmentation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-20">16-20 June 2019</date>
			<biblScope unit="page" from="10296" to="10305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Vision meets Robotics: The KITTI Dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913491297</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Robot. Res. IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning for classification of outdoor 3D scans</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Deuge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Quadros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Douillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Conference on Robitics and Automation</title>
		<meeting>the Australasian Conference on Robitics and Automation<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-04">2-4 December 2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">NET: A new large-scale point cloud classification benchmark</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName><surname>Semantic3d</surname></persName>
		</author>
		<idno type="DOI">10.5194/isprs-annals-IV-1-W1-91-2017</idno>
	</analytic>
	<monogr>
		<title level="j">ISPRS Ann. Photogramm. Remote Sens. Spat. Inf. Sci</title>
		<imprint>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Contextual classification with functional max-margin markov networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2009 IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>Hana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02297</idno>
		<title level="m">A comprehensive review of 3d point cloud descriptors</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10091</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Segmentation and histogram generation using the HSV color space for image retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sural</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pramanik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Image Processing</title>
		<meeting>the International Conference on Image Processing<address><addrLine>Rochester, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002-09">September 2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="II" to="II" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Automatic Generation of Point Cloud Synthetic Dataset for Historical Building Representation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Pierdicca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mameli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">S</forename><surname>Malinverni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paolanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frontoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Augmented Reality</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="203" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Evaluation of colour models for computer vision using cluster validation techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fenn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chalup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robot Soccer World Cup</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="261" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Digit(al)isation in Museums: Civitas Project-AR, VR, Multisensorial and Multiuser Experiences at the Urbino&apos;s Ducal Palace</title>
		<author>
			<persName><forename type="first">P</forename><surname>Clini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Quattrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonvini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nespeca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Angeloni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mammoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Dragoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morbidoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sernani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mengoni</surname></persName>
		</author>
		<idno type="DOI">10.4018/978-1-7998-1796-3.ch011</idno>
	</analytic>
	<monogr>
		<title level="m">Virtual and Augmented Reality in Education, Art, and Museums</title>
		<meeting><address><addrLine>Hershey, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IGI Global</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="194" to="228" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
