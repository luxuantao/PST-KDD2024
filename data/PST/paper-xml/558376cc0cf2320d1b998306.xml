<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">22B103FD633B8068B0900763CF136404</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robust Text-Independent Speaker Identification</head><p>Using Gaussian Mixture Speaker Models Douglas A. Reynolds, Member, IEEE, and Richard C . <ref type="bibr">Rose, Member, IEEE</ref> Abstract-This paper introduces and motivates the use of Gaussian mixture models (GMM) for robust text-independent speaker identification. The individual Gaussian components of a GMM are shown to represent some general speaker-dependent spectral shapes that are effective for modeling speaker identity. The focus of this work is on applications which require high identification rates using short utterance from unconstrained conversational speech and robustness to degradations produced by transmission over a telephone channel. A complete experimental evaluation of the Gaussian mixture speaker model is conducted on a 49 speaker, conversational telephone speech database. The experiments examine algorithmic issues (initialization, variance limiting, model order selection), spectral variability robustness techniques, large population performance, and comparisons to other speaker modeling techniques (uni-modal Gaussian, VQ codebook, tied Gaussian mixture, and radial basis functions). The Gaussian mixture speaker model attains 96.8% identification accuracy using 5 second clean speech utterances and 80.8% accuracy using 15 second telephone speech utterances with a 49 speaker population and is shown to outperform the other speaker modeling techniques on an identical 16 speaker telephone speech task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>HE speech signal conveys several levels of information.</p><p>T Primarily, the speech signal conveys the words or message being spoken, but on a secondary level, the signal also conveys information about the identity of the talker. While the area of speech recognition is concerned with extracting the underlying linguistic message in an utterance, the area of speaker recognition is concemed with extracting the identity of the person speaking the utterance. As speech interaction with computers becomes more pervasive in activities such as telephone financial transactions and information retrieval from speech databases, the utility of automatically recognizing a speaker based solely on vocal characteristics increases.</p><p>Depending upon the application, the general area of speaker recognition is divided into two specific tasks: verification and identification. In verification, the goal is to determine from a voice sample if a person is whom he or she claims. In speaker identification, the goal is to determine which one of a group of known voices best matches the input voice sample. Furthermore, in either task the speech can be constrained to be a known phrase <ref type="bibr">(text-dependent)</ref> or totally unconstrained (text-independent). Success in both tasks depends on extracting and modeling the speaker-dependent characteristics of the speech signal which can effectively distinguish one talker from another.</p><p>In this paper a new speaker model based on Gaussian mixture models (GMM) is introduced and evaluated for textindependent speaker identification. The use of Gaussian mixture models for modeling speaker identity is motivated by the interpretation that the Gaussian components represent some general speaker-dependent spectral shapes and the capability of Gaussian mixtures to model arbitrary densities. The Gaussian mixture speaker model is experimentally evaluated on a 49 speaker conversational speech database containing both clean and telephone speech. The experiments examine algorithmic issues such as model initialization, variance limiting, and model order selection. To compensate for spectral variability introduced by the telephone channel and handsets, robustness techniques such as long-term mean removal, difference coefficients, and frequency warping are applied and compared. The experiments also examine the GMM speaker identification performance with respect to an increasing speaker population. Finally, the performance of the Gaussian mixture speaker model, uni-modal Gaussian model The techniques for speaker recognition can be categorized into three major approaches. The first and earliest approach is to use long-term averages of acoustic features, such as spectrum representations or pitch [7], <ref type="bibr">[8]</ref>. The idea is to average out the other factors influencing the acoustic features, such as the phonetic variations, leaving only the speaker dependent component. For spectral features, the long-term average represents a speaker's average vocal tract shape. This approach is equivalent to a Gaussian classifier and has been used successfully for several difficult, text-independent speaker identification tasks [I], [SI. However, the averaging process discards much speaker-dependent information and can require long (&gt;20 s) speech utterances to derive stable longterm speech statistics.</p><p>The second approach is to model the speaker-dependent acoustic features within the individual phonetic sounds that comprise the utterance. By comparing acoustic features from phonetic sounds in a test utterance with speaker-dependent acoustic features from similar phonetic sounds, the comparison measures speaker differences rather than textual difference. This approach can be accomplished using explicit or implicit segmentation of the speech into phonetic sound classes prior to speaker model training or recognition. In [ 101 and [ 111, explicit segmentation was performed using a hidden Markov model (HMM)-based continuous speech recognizer as a front-end segmenter for text-independent speaker recognition systems. It was found in both studies that the front-end speech recognizer provided little or no improvement in speaker recognition performance compared to no front-end segmentation. Moreover, using a continuous speech recognizer front-end imposes a significant increase in computational complexity on both training and recognition.</p><p>Implicit segmentation, on the other hand, relies on some form of unsupervised clustering to provide implicit segmentation of the acoustic features during both training and recognition. The sound classes are not labeled, so separate training of a segmenter is not required. Template based clustering, such as vector quantization [12], [2] and K-nearest neighbor with leader clustering [13], has proven to be very effective for this approach to speaker recognition. In the VQ approach, each speaker is represented by a codebook of spectral templates representing the phonetic sound clusters in hisher speech. While this technique has demonstrated good performance on limited vocabulary (digits) tasks, it is limited in its ability to model the possible variabilities encountered in an unconstrained speech task. As has been shown in speech recognition, probablistic models provide a better model of acoustic speech events and a framework for dealing with noise and channel degradations. HMM's, in a variety of forms, have been used as probabilistic speaker models for both text-independent and text-dependent speaker recognition <ref type="bibr">[14]</ref>, <ref type="bibr">[17]</ref>. The HMM models not only the underlying speech sounds, but also the temporal sequencing among these sounds. Although temporal structure modeling is advantageous for text-dependent tasks, for text-independent tasks the sequencing of sounds found in the training data does not necessarily reflect the sound sequences found in the testing data and contains little speaker-dependent information. This is supported by experimental results in [ 151 and [ 171 which found text-independent performance was unaffected by discarding transition probabilities in HMM speaker models.</p><p>The third and most recent approach to speaker recognition is the use of discriminative neural networks (NN). Rather than train individual models to represent particular speakers, discriminative N"s are trained to model the decision function which best discriminates speakers within a known set. Several different networks, such as multilayer perceptrons [ 181, timedelay N " s [19], and radial basis functions [3], have recently been applied to various speaker recognition tasks. Generally, NN's require a smaller number of parameters than independent speaker models and have produced good speaker recognition performance, comparable to that of VQ systems. The major drawback to many of the NN techniques is that the complete network must be retrained when a new speaker is added to the system.</p><p>The Gaussian mixture speaker model falls into the implicit segmentation approach to speaker recognition. It provides a probabilistic model of the underlying sounds of a person's voice, but unlike HMM's does not impose any Markov-ian constraints between the sound classes. The probabilistic framework also allows the application of newly developed noise and channel robustness techniques from the speech recognition area. In <ref type="bibr">[20]</ref> a statistical background noise model is integrated with the Gaussian mixture speaker model for noise robustness using this framework. Furthermore, the new model is computationally efficient and can easily be implemented on a real-time digital signal processor [21], [22].</p><p>The research in this paper is concemed with realistic speech data encountered in practical applications of speaker identification. Speaker labeling of voice mail, for example, must use unconstrained conversational speech, possibly received over a noisy telephone line. In such an application, the speaker model must have some compensation to be robust to the acoustic distortions produced by telephone handsets and networks. Also, since there is usually no control over how long a person speaks, this research is focused on performance using short (&lt; 10 s) speech utterances for identification. These issues are examined in speaker identification experiments conducted on a telephone quality conversational speech database.</p><p>The rest of the paper is organized as follows. In the next section, we introduce the Gaussian mixture speaker model and motivate its use for text-independent speaker modeling. Section III then presents an experimental study of the Gaussian mixture speaker model on an unconstrained conversational database. The experiments examine parameter estimation, model order selection, spectral variability robustness, effect of population size, and performance comparisons to other speaker classifiers. Finally, Section IV gives a summary and conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">THE GAUSSIAN MIXTURE SPEAKER MODEL</head><p>This section describes the form of the Gaussian mixture model (GMM) and motivates its use as a representation of speaker identity for text-independent speaker identification. The speech analysis for extracting the mel-cepstral feature representation used in this work is presented first. Next, the Gaussian mixture speaker model and its parameterization are described. The use of the Gaussian mixture density for speaker identification is then motivated by two interpretations. First, the individual component Gaussians in a speaker-dependent GMM are interpreted to represent some broad acoustic classes. These acoustic classes reflect some general speaker-dependent vocal tract configurations that are useful for modeling speaker identity. Second, a Gaussian mixture density is shown to provide a smooth approximation to the underlying long-term sample distribution of observations obtained from utterances by a given speaker. Finally, the maximum-likelihood parameter estimation and speaker identification procedures are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speech Analysis</head><p>Although there are no exclusively speaker distinguishing speech features, the speech spectrum has been shown to be very effective for speaker identification <ref type="bibr">[4]</ref>. This is because the spectrum reflects a person's vocal tract structure, the predominant physiological factor which distinguishes one person's voice from others. LPC spectral representations, such as LPC cepstral and reflection coefficients, have been used extensively for speaker recognition; however, these model-based representations can be severely affected by noise [5]. Recent studies have found directly computed filterbank features to be more robust for noisy speech recognition [6]. In this paper we use cepstral coefficients derived from a mel-frequency filterbank to represent the short-time speech spectra.</p><p>Fig. <ref type="figure">1</ref> shows a block diagram of the steps in our frontend feature extraction. The magnitude spectrum from a 20 ms short-time segment of speech is pre-emphasized and processed by a simulated mel-scale filterbank. The filterbank follows that described in [23]. The log-energy filter outputs are then cosine transformed to produce the cepstral coefficients. The zeroth cepstral coefficient is not used in the cepstral feature vector. This processing occurs every 10 ms, producing 100 feature vectors per second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B . Model Description</head><p>A Gaussian mixture density is a weighted sum of M component densities, as depicted in Fig. <ref type="figure">2</ref> and given by the equation</p><formula xml:id="formula_0">M i=l</formula><p>where 5 is a D-dimensional random vector, bi(Z), i = 1,. . . , M, are the component densities and p i , i = 1, . . . , M, are the mixture weights. Each component density is a Dvariate Gaussian function of the form with mean vector @i and covariance matrix E;. The mixture weights satisfy the constraint that Ci=lpi = 1.</p><p>The complete Gaussian mixture density is parameterized by the mean vectors, covariance matrices and mixture weights from all component densities. These parameters are collectively represented by the notation</p><formula xml:id="formula_1">M x = { p . %,&amp;, C;) i = 1,. . . , M.</formula><p>(3)</p><p>For speaker identification, each speaker is represented by a GMM and is referred to by hisher model A. The GMM can have several different forms depending on the choice of covariance matrices. The model can have one covariance matrix per Gaussian component as indicated in (3) (nodal covariance), one covariance matrix for all Gaussian components in a speaker model (grand covariance), or a single covariance matrix shared by all speaker models (global covariance). The covariance matrix can also be full or diagonal. In this paper, nodal, diagonal covariance matrices are primarily used for speaker models, except as noted for some experiments. This choice is based on initial experimental results indicating better identification perfomance using nodal, diagonal variances compared to nodal and grand full covariance matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Model Interpretations</head><p>There are two principal motivations for using Gaussian mixture densities as a representation of speaker identity. The first motivation is the intuitive notion that the individual component densities of a multi-modal density, like the GMM, may model some underlying set of acoustic classes. It is reasonable to assume the acoustic space corresponding to a speaker's voice can be characterized by a set of acoustic classes representing some broad phonetic events, such as vowels, nasals, or fricatives. These acoustic classes reflect some general speaker-dependent vocal tract configurations that are useful for characterizing speaker identity. The spectral shape of the ith acoustic class can in tum be represented by the mean &amp; of the zth component density, and variations of the average spectral shape can be represented by the covariance matrix E;. Because all training or testing speech is unlabeled, the acoustic classes are "hidden" in that the class of an observation is unknown. Assuming independent feature vectors, the observation density of feature vectors drawn from these hidden acoustic classes is a Gaussian mixture.</p><p>The second motivation for using Gaussian mixture densities for speaker identification is the empirical observation that a linear combination of Gaussian basis functions is capable of representing a large class of sample distributions. One of the powerful attributes of the GMM is its ability to form smooth approximations to arbitrarily-shaped densities.</p><p>The classical unimodal Gaussian speaker model represents a speaker's feature distribution by a position (mean vector) and a elliptic shape (covariance matrix) and the VQ model represents a speaker's distribution by a discrete set of characteristic templates. In some sense the GMM acts as a hybrid between these two models by using a discrete set of Gaussian functions, each with their own mean and covariance matrix, to allow a better modeling capability. Fig. <ref type="figure" target="#fig_4">3</ref> compares the densities obtained using a unimodal Gaussian model, a GMM and a VQ model. Plot (a) shows the histogram of a single cepstral coefficient from a 25 second utterance by a male speaker; plot (b) shows the maximum likelihood unimodal Gaussian model; plot (c) shows the GMM and its 10 underlying component densities; and plot (d) shows a histogram of the data assigned to the VQ centroid locations of a IO-element codebook. The GMM not only provides a smooth overall distribution fit, its components also clearly detail the multi-modal nature of the density. Also, because the component Gaussians are acting together to model the overall pdf, full covariance matrices are not necessary even if the features are not statistically independent. The linear combination of diagonal covariance Gaussians is capable of modeling the correlations between feature vector elements. The effect of using a set of A4 full covariance Gaussians can be equally obtained by using a larger set of diagonal covariance Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Maximum Likelihood Parameter Estimation</head><p>Given training speech from a speaker, the goal of speaker model training is to estimate the parameters of the GMM, A, which in some sense best matches the distribution of the training feature vectors. There are several techniques available for estimating the parameters of a GMM [24]. By far the most popular and well-established method is maximum likelihood (ML) estimation.</p><p>The a i m of ML estimation is to find the model parameters which maximize the likelihood of the GMM, given the training data. For a sequence of T training vectors X = {ZI, . . . , Z T } , the GMM likelihood can be written as</p><formula xml:id="formula_2">T (4)</formula><p>Unfortunately, this expression is a nonlinear function of the parameters A and direct maximization is not possible. However, ML parameter estimates can be obtained iteratively using a special case of the expectation-maximization (EM) algorithm</p><p>The basic idea of the EM algorithm is, beginning with an initial model A, to estimate a new model X, such that P ( X I X) 2 p ( X I A). The new model then becomes the initial model for the next iteration and the process is repeated until some convergence threshold is reached. This is the same basic technique used for estimating HMM parameters via the Baum-Welch reestimation algorithm [26].</p><p>On each EM iteration, the following reestimation formulas are used which guarantee a monotonic increase in the model's likelihood value: where CT!, q , and pi refer to arbitrary elements of the vectors The a posteriori probability for acoustic class i is given by C T ~ -2 , Zt, and @i, respectively. Two critical factors in training a Gaussian mixture speaker model are selecting the order M of the mixture and initializing the model parameters prior to the EM algorithm. There are no good theoretical means to guide one in either of these selections, so they are best experimentally determined for a given task. An experimental examination of these factors on speaker ID performance is discussed in Section 111.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Speaker ldentifrcation</head><p>For speaker identification, a group of S speakers S = (1, 2,. . . , S} is represented by GMM's XI , X2,. . . , AS. The objective is to find the speaker model which has the maximum a posteriori probability for a given observation sequence. Formally, where the second equation is due to Bayes' rule. Assuming equally likely speakers (i.e., Pr(Xk) = l/S) and noting that p(X) is the same for all speaker models, the classification rule simplifies to Using logarithms and the independence between observations, the speaker identification system computes</p><formula xml:id="formula_3">T in which p(&amp; 1 x k ) is given in (1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="111.">EXPERIMENTAL EVALUATION</head><p>This section presents the experimental evaluation of the Gaussian mixture speaker model for text-independent speaker identification. The GMM speaker identification system was evaluated in a task domain where utterances are from conversational speech spoken over both wideband, high signal-tonoise ratio (SNR) channels and narrowband telephone channels. The experimental study has four parts. In the first set of experiments, issues related to parameter estimation and model order selection for the Gaussian mixture speaker model are examined. The second set of experiments evaluates several different robustness techniques for improving performance using telephone speech. The third set of experiments examines the effects of speaker population size on identification performance. Finally, the last set of experiments compares the performance of the Gaussian mixture speaker model to several other classifiers, including unimodal Gaussian, vector quantization codebook, tied Gaussian mixture model, and radial basis functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Database Description</head><p>The experiments were primarily conducted using a subset of the KING speech database <ref type="bibr">[27]</ref>. The KING database is a collection of conversational speech from 51 male speakers. For each speaker there are 10 conversations of approximately 45 seconds each recorded during 10 separate sessions. The speech from a session was recorded from a high-quality microphone locally and was transmitted over a long distance telephone link, providing a high-quality (clean) version and a telephone quality version of the speech. The experiments used five sessions per speaker with two-three sessions used for training data and the remaining sessions used for testing data. The model initialization experiments, described in Section 1114-1, were conducted on a different wideband, conversational speech database consisting of 12 speakers (eight male, four female).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B . Pe$ormance Evaluation</head><p>The evaluation of a speaker identification experiment was conducted in the following manner. The test speech was first processed by the front-end analysis to produce a sequence . . .</p><p>A test segment length of 5 seconds corresponds to T = 500 feature vectors at a 10 ms frame rate. Each segment of T vectors was treated as a separate test utterance.</p><p>The identified speaker of each segment was compared to the actual speaker of the test utterance and the number of segments which were correctly identified was tabulated. The above steps were repeated for test utterances from each speaker in the population. The final performance evaluation was then computed as the percent of correctly identified Tlength segments over all test utterances % correct identification # correctly identified segments total # of segments --</p><p>x 100. (12)</p><p>The evaluation was repeated for different values of T to evaluate performance with respect to test utterance length. Each speaker had approximately equal amounts of testing speech so the performance evaluation was not biased to any particular speaker. While there may be variations among the individual speakers' performance, the aim of the evaluation measure was to track the average performance of the system for different speaker identification tasks, allowing a common basis of comparison. trained using different methods of initialization and used for a speaker identification experiment. This experiment used the 12 speaker conversational database. Speakers were modeled by a 50 component GMM with a grand, diagonal covariance matrix trained using approximately 5000 1Zdimensional mel-cepstral vectors (50 seconds). Testing was done using approximately three minutes of speech per speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C . Algorithmic Issues</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I) Initialization</head><p>The first method of initialization used a speaker-independent HMM to automatically segment the training speech. The training data was segmented into 50 labeled phonetic classes which corresponded to the initial mixture components. The class means and global variances then served as the initial model for EM training. The segmentation was performed by a forced Viterbi decoding of the unlabeled training utterance using monophone acoustic models. The acoustic models were obtained from averaging speaker-independent, contextdependent subword HMM's. The subword HMM's were trained using the forward-backward algorithm on orthographically transcribed continuous speech utterances. The second initialization method consisted of randomly choosing 50 vectors from a speaker's training data (after silence removal) for the initial model means and an identity matrix for the starting covariance matrix. Surprisingly, no significant difference in speaker identification performance was found between the two initialization methods. The different initial models may have converged to different local maximizers of the lielihood function, but the difference between the final models is insignificant in terms of speaker identification performance. It was also observed that both methods of initialization required the same number of EM iterations for convergence of the likelihood function, so no training speed advantage was found for either method. These results indicate that elaborate initialization schemes are not necessary for training Gaussian mixture speaker models.</p><p>Subsequent experiments also found no significant difference between the above random mean selection and binary kmeans clustering for initialization. The rest of the experiments in this paper use random mean selection, followed by a single iteration k-means clustering to initialize means, nodal variances, and mixture weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">) Variance Limiting:</head><p>When training a nodal variance GMM, it has been observed that variance elements can become quite small in magnitude. This is particularly true for a mixture model with a large (232) number of component densities. These small variances produce a singularity in the model's likelihood function and can degrade identification performance by distorting speaker model scores used in the maximum likelihood classifier. These singularities can arise when there is not enough data to sufficiently train a component's variance vector or when using noise-corrupted data. The noisy data can contain outliers in the data that give rise to components with very small variances [281.</p><p>To avoid these spurious singularities, a variance limiting constraint is applied. This constraint places a minimum variance value on elements of all variance vectors in a speaker's model. For an arbitrary element of mixture component 2's variance vector, o$, and a minimum variance value, akin, Care must be exercised when setting the minimum variance value. If it is set too high, the component variances are masked to the same value which would overly constrain the model and hence degrade identification performance. Setting the value too low may not perform the desired limiting at all. The variance limit must be empirically determined for any particular data set, feature set, and model size to optimize performance. Preliminary experiments on a 16 speaker set found a variance limit between oLin = 0.01 and uiin = 0.1 to provide the best robustness for mel-cepstral features.</p><p>3 ) Model Order: Determining the number of components M in a mixture needed to model a speaker adequately is an important but difficult problem. There is no theoretical way to estimate the number of mixture components a priori.</p><p>For speaker modeling the objective is to choose the minimum number of components necessary to adequately model a speaker for good speaker identification. Choosing too few mixture components can produce a speaker model which does not accurately model the distinguishing characteristics of a speaker's distribution. Choosing too many components can reduce performance when there are a large number of model parameters relative to the available training data and can also result in excessive computational complexity both in training and classification. The following experiments examine the performance of the GMM speaker ID system for different model orders using a fixed and variable amount of training data.</p><p>To investigate the speaker identification performance of the GMM with respect to the number of component densities per model, the following experiment was conducted on a 16 speaker subset of the KING database. Speaker models with 1, 2,4,8, 16,32, and 64 component Gaussian densities and nodal variances were trained using 60oO 25-dimensional mel-cepstral vectors corresponding to one minute of speech. Sessions one and two were used for model training and sessions three, four, and five were used for testing. Variance limiting was used with uiin = 0.01. Fig. <ref type="figure">4</ref> shows the percent correct identification performance versus the number of Gaussian components for 1, 5 , and 10 second test utterance lengths.</p><p>There are several observations to be made from these results.</p><p>First, the sharp increase in identification performance from 1 to 8 mixture components, and leveling off above 16 components, indicates that there is a lower limit on the number of mixture components necessary to adequately model the speakers. Models must contain at least this minimum number of components to maintain good speaker identification performance. This h t seems to be 16 mixture components for these speakers and this </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TRAIN DATA 90 s /---e--------------</head><p>_--- data. Above this minimum model order, the performance is insensitive to the number of mixture components for the 5 and 10 second length test utterances. For the 1 second test utterance length, the identification performance continues to increase (at a decreasing rate) with the model order. This demonstrates how additional components, which model additional acoustic classes, are effectively used for short utterance identification.</p><p>The increase in performance begins to level out above 32 component Gaussians.</p><p>In the next experiment, speaker models with 8, 16, and 32 component densities were trained using 30,60, and 90 seconds of speech in the same manner as described above. The various amounts of training data were sequentially taken from sessions one, two, and three and sessions four and five were used for testing. Table <ref type="table">I</ref> shows the complete identification results. For each model order, the identification performance for 1, 5, and 10 second test utterance lengths are given for 30, 60, and 90 seconds of training data.</p><p>As expected, with increased training data, identification performance increases. Identification rates for shorter test utterance lengths show the greatest improvement. The largest increase in performance occurs at all test utterance lengths when the amount of training data increases from 30 to 60 seconds. Increasing the training data to 90 seconds also improves performance but with a smaller increment. This suggests that at least one minute of conversational speech is necessary to maintain high speaker identification performance and using more training data improves performance at a decreasing rate. Note, however, that each increase in the training data also adds training data from another session. Thus, the addition of data from different sessions may also be a factor, along with the increase in amount of data.</p><p>It is also evident that model order selection becomes more important with smaller amounts of training data. Fig. <ref type="figure" target="#fig_11">5</ref> plots the identification performance for the 5-second test utterance length versus model order for models trained with different amounts of training data. For all amounts of training data, performance peaks at 16 components. However, performance decreases for the 32 mixture model trained with only 30 seconds of speech. Compared with the constant or slightly increasing performance using 60 and 90 seconds of speech, this is a good example of the effects of having insufficient training samples relative to the number of model parameters being estimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spectral Variabiliry Compensation</head><p>The major spectral degradation found in speech collected from the telephone network is a filtering effect which band limits and imposes some spectral shaping on the speech spectrum [30]. Left uncompensated, this degradation can produce severe reductions in identification performance due to data mismatch between training and recognition data. As a first-order model, the spectral variability introduced by a telephone channel can be modeled by a linear filter effect which modifies the spectral features used by the GMM speaker ID system. Below, some spectral variability compensation techniques to produce robust features for telephone quality speech are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">) Frequency Warping:</head><p>To avoid any differences in channel bandwidth and using any spurious out-of-band spectral components, frequency warping was applied to the magnitude DFT spectrum. The warping maps the frequency axis f to a new frequency axis f' according to the equation where fN is the original Nyquist frequency. The linear warping both eliminates spectral components outside the specified frequency range [fmin, fmax] and expands the spectrum to full bandwidth for subsequent processing.</p><p>2) Spectral Shape Compensation: When the speech signal passes through a linear filter h <ref type="bibr">[n]</ref> representing the telephone channel, its magnitude spectrum is multiplied by the magnitude response of the filter. If it is assumed that the magnitude spectrum of the filter is relatively smooth, it can be shown that the effect of the filter is an additive component on the mel-cepstral feature vector <ref type="bibr">[31]</ref> .+ z'=Z+h</p><formula xml:id="formula_4">(15)</formula><p>where z' is the observed cepstral vector, h' is the channel filter cepstral vector, and Z is the input speech cepstral vector.</p><p>The aim of the-spectral shape compensation is to remove the "bias" term h from the feature vectors. Two methods were applied to the GMM speaker identification system: mean normalization and time difference coefficients.</p><p>The method of mean normalization has been used in many speaker recognition systems [32]- <ref type="bibr">[35]</ref>. Essentially, it consists of removing the bias component by subtracting off the global average vector from each feature vector. For a set of feature vectors {z't}F=l, the global average vector is For each channel from which speech was collected, the global mean is subtracted off each vector before training a speaker model or scoring for recognition. All feature vectors then have the same global mean and speaker discrimination is not affected by different channel biases.</p><p>The above assumes a time-invariant channel filter. If the channel filter is time-varying, an adaptive bias removal method, such as RASTA processing <ref type="bibr">[36]</ref>, can be used to remove the time-varying channel bias.</p><p>Besides removing the channel filter bias, this compensation also removes the global mean of the speech feature vectors. This is equivalent to filtering the speech by the inverse of its average spectrum. Although the average speech spectrum does contain speaker specific information, it exhibits significant inaa-speaker variability over time <ref type="bibr">[37]</ref>, <ref type="bibr">[35]</ref> which can decrease recognition performance when training and testing on speech collected at different times. The average spectra is also susceptible to variations due to speech effort (for example, loud or soft) and health (for example, a speaker has a cold). Using mean normalization for clean speech improves identification performance by minimizing intersession variability. When used on telephone speech, removal of the global average minimizes both intersession variability and removes the spectral shaping imposed by different telephone channels.</p><p>Another way to minimize the channel filter effects is to use "channel invariant" features. One such set of channel invariant features used in speaker recognition systems is cepstrum difference coefficients [381- <ref type="bibr" target="#b36">[40]</ref>.</p><formula xml:id="formula_5">Acomp + +</formula><p>The motivation in using difference coefficients is both to capture dynamic information and to remove time-invariant spectral information generally attributed to the interposed communications channel. This is accomplished by creating a new set of features as the time difference between the cepstral feature vectors. For frame t the difference coefficients, denoted AZt, are formed by taking the difference between cepstral feature vectors that are W frames apart:</p><p>Since the c h T e l filter is time-invariant or slowly varying, the bias term h in (15) is removed, leaving the difference in speech cepstra Because the difference coefficients capture the spectral changes in time, they are also referred to as transitional or dynamic features, with the cepstral vectors called instantaneous or static features.</p><p>Difference coefficients have been shown to contain speaker specific information and to be fairly uncorrelated with the static cepstral feature vectors; however, when used by themselves, they do not perform as well as the static feature vectors <ref type="bibr" target="#b35">[39]</ref>. To combine the two feature sets, the difference coefficients are appended to the cepstral feature vectors. The new feature vector not only contains channel invariant features but also spectral transitional information along with the instantaneous cepstral coefficients.</p><p>Using the above compensation techniques, speaker identification experiments were conducted using speech from different telephone channels. The experiments were conducted using the telephone version speech sessions from a 16 speaker subset of the KING database. Each speaker was modeled by a 50 component GMM trained with speech from sessions one, two, and three (which corresponds to an average of 80 seconds of training speech) using 20-dimensional melcepstrum feature vectors and a variance limit of uiin = 0.1. The experiment with difference coefficients used 20 cepstral coefficients appended with 20 difference coefficients from a 40 ms interval ( f 2 frames) around the current frame. For frequency warping, the telephone bandwidth 300-3300 Hz was linearly warped to full bandwidth. The identification results using the different compensation techniques are shown in Fig. <ref type="figure">6</ref>.</p><p>It is evident that without compensation, speaker identification performance was degraded using telephone speech. For a similar experiment using the clean speech versions of the sessions, a speaker identification accuracy of 94.3% was attained for a 5 second test utterance compared to 64.4% using the uncompensated telephone speech. Of the compensation techniques, the most effective method for minimizing the channel variation effects was mean normalization. This simple method improved the performance by an average 28% over all test utterance lengths. Spectrum frequency warping alone produced a substantial 21% increase compared to no compensation and was the second best effective method of compensation. Using appended difference coefficients provided a Identification performance for different spectral variability compenmodest improvement performance over no compensation on the order of 6%. Performing frequency warping prior to mean normalization and using mean normalized features appended with difference coefficient showed no significant improvement over mean normalization alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Large Population Performance</head><p>One factor which defines the difficulty of the speaker identification task is the size of the speaker population. As the number of speakers that the system must distinguish increases, the probability of an incorrect classification increases. The similarity of the speakers in the population also must be considered, since a set of speakers with dissimilar voice characteristics (e.g., a population of half males and half females) generally produces higher identification performance than a more homogeneous set of speakers (e.g.. all male). The following experiments examined the performance of the GMM speaker identification system as a function of population size for an all-male collection of speakers using both clean and telephone speech.</p><p>For these experiments, each speaker was modeled by a 50 component GMM with nodal variances using 20-dimensional mel-cepstral feature vectors. The models were trained using all the data from sessions one, two, and three (80-100 seconds of speech per speaker) and testing was conducted using sessions four and five. Each session was mean normalized to minimize inter-session variability and channel bias. A variance limit of ~7 : ~~ = 0.1 was used in training. For the telephone speech, the frequency bandwidth 300-3300 Hz was warped to full bandwidth.</p><p>Identification performance versus test utterance lengths for populations of 16, 32, and 49 speakers are shown in Fig. <ref type="figure">7</ref>.</p><p>In the clean speech case, it is clear that the GMM speaker ID system maintains high identification performance as the population size increases. The largest degradation for increasing population size is for the 1-second test utterance length, but almost perfect identification for all population sizes is obtained for 15 second test utterances.</p><p>When compared to the clean speech results, there was a marked decrease in performance using telephone speech. One major contributing factor to this reduced performance is the relatively low S N R of some of the telephone speech. The compensation techniques only address spectral variability and so significant differences in noise levels between channels were not compensated. Examination of the telephone speech found that half of the speakers' telephone speech sessions are very noisy (SNR ranges roughly from 10 to 20 dB). The 16 speaker population used moderate S N R (approximately 30 dB) speech and the results are comparable to those using clean speech. However, as the population size increased, more speakers with noisy speech were added to the population and the performance rapidly declined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Comparison to Other Speaker Models</head><p>The last set of experiments compared the performance of the Gaussian mixture speaker model with other speaker modeling techniques. Specifically, the other techniques are the unimodal Gaussian classifier (GC) [ 11, vector quantization (VQ) codebook [2], tied Gaussian mixture model (TGMM) and radial basis function (RBF) [3]. The aim is to compare the performance of these different identification methods using the same data and front-end processing.</p><p>These different speaker modeling techniques are interesting to compare because they represent different ways of modeling the speaker's acoustic feature distribution. In the simplest case, the GC models each speaker's feature distributions by a unimodal Gaussian distribution. Since the data is mean normalized, the Gaussian mean vector is effectively zero and identification is based only on the covariance modeling of the data. This is similar to "covariance-only" speaker identification method in [9]. The VQ models the distributions by representative templates from hard partitions of the feature space. As discussed earlier, the GMM generalizes on this notion by providing a soft partitioning of a speaker's space using Gaussian basis functions.</p><p>The RBF and TGMM both share the same underlying structure as the GMM, but model the feature space in different ways (see Fig. <ref type="figure" target="#fig_15">8</ref>). The TGMM uses a pool of Gaussians which covers the feature space of all speakers. A maximum likelihood training procedure adjusts each speaker's mixture weights to the underlying Gaussians to best model hisher feature distribution. The underlying Gaussians' parameters are also updated in the training to match the overall feature distribution. The RBF differs from the above models in that it focuses on modeling the boundary regions separating speaker distributions in the feature space. Like the TGMM, it too uses a pool of basis functions to represent all speakers. However, the basis functions are fixed during training and the speaker's connection weights are trained using a discriminative criterion.</p><p>The data used for the experiment was from the 16 speaker KING subset using the telephone speech sessions. All sessions were mean normalized prior to training and testing. Each model was trained using all the speech from sessions one, two, and three, with testing performed on sessions four and five. Twenty-dimensional mel-cepstral feature vectors were used and trained variances were limited to uLin = 0.1.</p><p>Model parameters were set as follows. Two forms of the GMM were used where the first form (GMM-nv) had 50 components with nodal variances and the second form (GMMgv) also had 50 components, but with a single grand variance per model. The VQ-50 speaker model used 50 vectors per codebook while the VQ-lo0 model used 100 vectors per codebook, both trained with the LBG algorithm [41] using the Mahalanobis distance with a global, diagonal covariance matrix. The tied Gaussian mixture model used a pool of 800 Gaussians and a global diagonal covariance matrix. The radial basis function used 5 12 basis functions with empirically determined function widths. Finally, the unimodal Gaussian classifier used a full 20 x 20 covariance matrix.</p><p>The average number of parameters per speaker for each model is shown in Table <ref type="table">11</ref>. For example, the number of Parameters for the GMM-gv is calculated as (Wean-vecs + #variance-vecs) x vec-dim + #mixture-weights = (50+1) x 20+50=1070. The GMM-nv has the most parameters due to the use of nodal variances, while the GC has the least number of parameters due to the limited model structure. The GMM-nv and VQ-lo0 models have comparable number of parameters, as do the GMM-gv and VQ-50 models. The TGMM and RBF have different number of parameters because numerical difficulties prevented the training of an RBF with 800 basis functions .</p><p>Table <ref type="table">I11</ref> shows the percent correct identification for 5 second test utterance lengths for the different models. Also shown is the binomial standard deviation of the tests using only the number of nonoverlapping test intervals as the number of trials (n = 160). The classifiers can be divided into four levels of performance. On the top level, the nodal variance GMM (GMM-nv) has the best absolute performance with the VQ-lo0 about 1.5 percentage points lower. On the second level, the grand variance GMM (GMM-gv), VQ-50, and RBF all have similar classification performances. The drop in performance of the GMM going from nodal to grand variances indicates the importance of variance parameterization in model selection. Also, note that although the RBF has fewer centers per speaker compared to the GMM-gv and VQ models, it maintains similar performance due to the discriminative training. On the third level, the TGMM has significantly lower classification performance. This is likely due to the overlyrestrictive constraint of using a single global variance vector.</p><p>Lastly, the GC, using only covariance matrices, produced the worst identification performance of the classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This paper has introduced and evaluated the use of Gaussian mixture speaker models for robust text-independent speaker identification. The primary focus of this work was on a task domain for real applications, such as voice mail labeling and retrieval. The Gaussian mixture speaker model was specifically evaluated for identification tasks using short duration utterances from unconstrained conversational speech, possibly transmitted over noisy telephone channels.</p><p>Gaussian mixture models were motivated for modeling speaker identity based on two interpretations. The component Gaussians were first shown to represent characteristic spectral shapes (vocal tract configurations) from the phonetic sounds which comprise a person's voice. By modeling the underlying acoustic classes, the speaker model is better able to model the short-term variations of a person's voice, allowing high identification performance for short utterances. The Gaussian mixture speaker model was also interpreted as a nonparametric, multivariate pdf model, capable of modeling arbitrary feature distributions.</p><p>The experimental evaluation examined several aspects of using Gaussian mixture speaker models for text-independent speaker identification. Some observations and conclusions are:</p><p>Identification performance of the Gaussian mixture speaker model is insensitive to the method of model initialization.</p><p>Variance limiting is important in training to avoid model singularities.</p><p>There appears to be a minimum model order needed to adequately model speakers and achieve good identification performance (Sixteen for this 16 speaker database).</p><p>The Gaussian mixture speaker model maintains high identification performance with increasing population size (The system attained a 96.8% identification rate for 5 second clean speech utterances and 80.8% for 15 second telephone speech utterances for an all-male 49 speaker population).</p><p>Cepstral mean normalization is a very effective compensation for telephone spectral variability degradations. With nodal variance parameterization, the Gaussian mixture speaker model outperforms the VQ, RBF, TGMM, and GC speaker modeling techniques on an identical telephone speech task. These results indicate that Gaussian mixture models provide a robust speaker representation for the difficult task of speaker identification using corrupted, unconstrained speech. The models are computationally inexpensive and easily implemented on a real-time platform </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Manuscript received September 8, 1993; revised May 18. 1994. This work was supported by the U.S. Department of the Air Force. The associate editor coordinating the review of this paper and approving it for publication was Dr. Joseph Campbell. D. A. Reynolds is with the Speech Systems Technology Group, MIT Lincoln Laboratory, Lexington, MA 02173 USA. R. C. Rose is with the Speech Research Department, AT&amp;T Bell Laboratones, Murray Hill, NJ 07974-0636 USA. IEEE Log Number 9406779.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>[ 11, vector quantization (VQ) codebook model [2], tied Gaussian mixture model, and radial basis function (RBF) model [3] are compared on a 16 speaker telephone speech identification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 1. Mel-scale cepstral feature analysis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>- 8 Fig. 2 .</head><label>82</label><figDesc>Fig. 2. Depiction of an M component Gaussian mixture density. A Gaussian mixture density is a weighted sum of Gaussian densities, where p ; , i = 1, . . . , M, are the mixture weights and b i ( ) , i = l , . . . , M , are the component Gaussians.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of distribution modeling: (a) Histogram of a single cepstral coefficient from a 25 second utterance by a male speaker; (b) maximum likelihood unimodal Gaussian model; (c) GMM and its 10 underlying component densities; (d) histogram of the data assigned to the VQ centroid locations of a loelement codebook.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>of feature vectors (21 , . . . , Zt}'. To evaluate different test utterance lengths, the sequence of feature vectors was divided into overlapping segments of T feature vectors. The first two segments from a sequence would ,.. . , ~T , Z T + ~, ~T + Z , .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>:</head><label></label><figDesc>As stated in the previous section, the GMM training procedure must be initialized with some starting modelThe EM algorithm is guaranteed to find a local maximum likelihood model regardless of the starting point, but the likelihood equation for a GMM has several local maxima and different starting models can lead to different local maxima [24]. To investigate the effect of model initialization on speaker identification performance, speaker models were 'Periods of silence are removed from the test speech prior to feature extraction using an adaptive energy threshold speecWsilence detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>the variance estimates after each EM iteration to avoid singularities in the final model. This is a constrained version of the EM algorithm which has been been shown to provide more robust parameter estimates than the unconstrained version [24], [29].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>.................................</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig. 4. component densities per speaker model. Speaker identification performance as a function of the number of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Speaker identification performance versus model order for models trained with 30, 60 and 90 seconds of speech. The test utterance length is 5 seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Fig. 6. sation techniques applied to telephone speech.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fig. 7. Speaker identification performance versus test utterance length for population sizes of 16, 32, and 49 speakers: (a) Clean speech performance; (b) telephone speech performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. TGMM and RBF model structure. In each model, speakers are represented by a weighted combination of a common pool of Gaussian or basis functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>[21],[22]. Furthermore, their probabilistic framework allows direct integration with speech recognition systems<ref type="bibr" target="#b38">[42]</ref> and incorporation of newly developed speech robustness techniques [20].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE II FOR SPEAKER MODELS DISCUSSED IN TEXT I Speaker Model 1 Avg number of I NUMBER OF PARAMEERS PER SPEAKER ] parameters per speaker I GMM-nv GMM-gv TGMM TABLE III SPFMER IDENTIFICATION PERFORMANCE FOR SPEAKER MODELS DISCUSSED IN TEXT I Speaker Model I % Correct Identification 1 I (5 second test length) I</head><label>II</label><figDesc></figDesc><table><row><cell>I GMM-nv I</cell><cell>94.5 f1.8</cell><cell>1</cell></row><row><cell>VQ-100</cell><cell>92.9 f2.0</cell><cell></cell></row><row><cell>G M M -p</cell><cell>89.5 f2.4</cell><cell></cell></row><row><cell></cell><cell>90.7 f2.3</cell><cell></cell></row><row><cell></cell><cell>87.2 f2.6</cell><cell></cell></row><row><cell>TGMM</cell><cell>80.1 f3.1</cell><cell></cell></row><row><cell></cell><cell>67.1 f3.7</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors wish to thank Capt. M. S. Ciancetta for his help on the large population and speaker model comparison experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Investigation of text-independent speaker identification over telephone channels</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="379" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Radial basis function networks for speaker recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Oglesby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="393" to="393" />
		</imprint>
	</monogr>
	<note>A vector quantization approach to speaker recognition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic recognition of speakers from their voices</title>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="1976-04">Apr. 1976</date>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page">460475</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A study of LPC analysis of speech in additive noise</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tiemey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="389" to="397" />
			<date type="published" when="1980-08">Aug. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Jankowski</surname></persName>
		</author>
		<imprint/>
		<respStmt>
			<orgName>MIT Lincoln Laboratory</orgName>
		</respStmt>
	</monogr>
	<note>unpublished research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Talker recognition by longtime averaged speech spectrum</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Itakura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron., Commun. in Japan</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="54" to="61" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Methods and experiments for text-independent speaker recognition over telephone channels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Markel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oshika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="865" to="868" />
		</imprint>
	</monogr>
	<note>Long-term feature averaging for speaker recognition</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A text-independent speaker recognition method robust against utterance variations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1985">1991. 1985</date>
			<biblScope unit="page" from="387" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Assp-25</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977-08">Aug. 1977</date>
			<biblScope unit="page" from="330" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Free-text speaker identification over long distance telephone channel using hypothesized phonetic segmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename></persName>
		</author>
		<idno>II. 177-II. 180</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Speaker recognition using linear predictive vector codebooks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Helms</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
		</imprint>
		<respStmt>
			<orgName>Southem Methodist University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Voice identification using nearestneighbor distance measure</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bahler</surname></persName>
		</author>
		<author>
			<persName><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1993-04">Apr. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linear predictive hidden Markov models and the speech signal</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Poritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1982-05">May 1982</date>
			<biblScope unit="page" from="1291" to="1294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the application of mixture AR hidden Markov models to text independent speaker recognition</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Z</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="563" to="570" />
			<date type="published" when="1991-03">Mar. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sub-word talker verification using hidden Markov models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">K</forename><surname>Soong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICASSP</title>
		<imprint>
			<biblScope unit="page" from="269" to="272" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Comparison of text-independent speaker recognition methods using VQ-distortion and discrete/continuous HMMs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
		<idno>II.157-II.164</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Text-independent talker identification with neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rudasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Zahorian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="389" to="392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the use of TDNNextracted features information in talker identification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bennani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="385" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Integrated models of speech and background with application to speaker identification in noise</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Hofstetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Speech Audio Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="245" to="257" />
			<date type="published" when="1994-04">Apr. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Gaussian mixture modeling approach to textindependent speaker identification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Georgia Inst. of Technology</title>
		<imprint>
			<date type="published" when="1992-09">Sept. 1992</date>
		</imprint>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PC-based TMS32OC30 implementation of the Gaussian mixture model text-independent speaker recognition system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J T B</forename><surname>Smith ; Tech-S</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><surname>Mermelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Signal Processing Appl</title>
		<meeting>Int. Conf. Signal essing Appl<address><addrLine>New York Marcel Dekker</addrLine></address></meeting>
		<imprint>
			<publisher>ASSP-G. McLachlan, Mixture Models</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
	<note>Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J . Royal Srar. Soc</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="164" to="171" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
	<note>Ann. Math Srar.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Noise compensation for speech recognition using probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Godfrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graf€</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><surname>Sedgwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ESCA Workshop Automat. Speaker Recognition, Identification, Verification</title>
		<meeting>ESCA Workshop Automat. Speaker Recognition, Identification, Verification</meeting>
		<imprint>
			<date type="published" when="1986">Apr. 1994. 1986</date>
			<biblScope unit="page">3942</biblScope>
		</imprint>
	</monogr>
	<note>Proc. IEEE ICASSP</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A constrained formulation of maximum-likelihood estimation for normal mixture distributions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hathaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Star</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Proakis</surname></persName>
		</author>
		<title level="m">Digital Communications. New York McGraw-Hill Series in Electrical Engineering</title>
		<imprint>
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An integrated speech-background model for robust speaker identification</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date>Mar</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Effectiveness of l i a r prediction characteristics of the speech wave for automatic speaker identification and verification</title>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Amer</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1304" to="1312" />
			<date type="published" when="1974-06">June 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Comparison of speaker recognition methods using statistical features and dynamic features</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acousr., Speech, Signul Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="342" to="350" />
			<date type="published" when="1981-06">June. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Investigation of text-independent speaker identification techniques under conditions of variable data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Krasner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="18B" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On instantaneous and transitional spectral information for text-dependent speaker verification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bemasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="129" to="139" />
			<date type="published" when="1990-04">Apr. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">RASTA-PLP speech analysis technique</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hermansky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cepstral analysis technique for automatic speaker verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Furui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signul Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="254" to="272" />
			<date type="published" when="1981-04">Apr. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On talker verification via orthogonal parameters</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bogner</surname></persName>
		</author>
		<idno>II-375-II-378</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acousr., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1981-02">Feb. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Nov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="7" to="973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ieee Icassp</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992-03">Mar. 1992</date>
			<biblScope unit="page" from="1" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the use of instantaneous and transitional spectral information in speaker recognition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Soong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="871" to="879" />
			<date type="published" when="1988-06">June 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Text-independent speaker identification using automatic acoustic segmentation</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="293" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vector quantization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<biblScope unit="issue">6 2 9</biblScope>
			<date type="published" when="1984-04">Apr. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Integration of speaker and speech recognition systems</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICASSP</title>
		<meeting>IEEE ICASSP</meeting>
		<imprint>
			<date type="published" when="1991-05">May 1991</date>
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
