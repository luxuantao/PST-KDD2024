<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient and Robust Automated Machine Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Feurer</surname></persName>
							<email>feurerm@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aaron</forename><surname>Klein</surname></persName>
							<email>kleinaa@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Katharina</forename><forename type="middle">Eggensperger</forename><surname>Jost</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tobias</forename><surname>Springenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Blum</surname></persName>
							<email>mblum@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient and Robust Automated Machine Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning has recently made great strides in many application areas, fueling a growing demand for machine learning systems that can be used effectively by novices in machine learning. Correspondingly, a growing number of commercial enterprises aim to satisfy this demand (e.g., BigML.com, Wise.io, SkyTree.com, RapidMiner.com, Dato.com, Prediction.io, DataRobot.com, Microsoft's Azure Machine Learning, Google's Prediction API, and Amazon Machine Learning). At its core, every effective machine learning service needs to solve the fundamental problems of deciding which machine learning algorithm to use on a given dataset, whether and how to preprocess its features, and how to set all hyperparameters. This is the problem we address in this work.</p><p>More specifically, we investigate automated machine learning (AutoML), the problem of automatically (without human input) producing test set predictions for a new dataset within a fixed computational budget. Formally, this AutoML problem can be stated as follows:</p><p>Definition 1 (AutoML problem). For i = 1, . . . , n+m, let x i ∈ R d denote a feature vector and y i ∈ Y the corresponding target value. Given a training dataset D train = {(x 1 , y 1 ), . . . , (x n , y n )} and the feature vectors x n+1 , . . . , x n+m of a test dataset D test = {(x n+1 , y n+1 ), . . . , (x n+m , y n+m )} drawn from the same underlying data distribution, as well as a resource budget b and a loss metric L(•, •), the AutoML problem is to (automatically) produce test set predictions ŷn+1 , . . . , ŷn+m . The loss of a solution ŷn+1 , . . . , ŷn+m to the AutoML problem is given by 1 m m j=1 L(ŷ n+j , y n+j ).</p><p>In practice, the budget b would comprise computational resources, such as CPU and/or wallclock time and memory usage. This problem definition reflects the setting of the ongoing ChaLearn AutoML challenge <ref type="bibr" target="#b0">[1]</ref>. The AutoML system we describe here won the first phase of that challenge.</p><p>Here, we follow and extend the AutoML approach first introduced by AUTO-WEKA <ref type="bibr" target="#b1">[2]</ref> (see http://automl.org). At its core, this approach combines a highly parametric machine learning framework F with a Bayesian optimization <ref type="bibr" target="#b2">[3]</ref> method for instantiating F well for a given dataset.</p><p>The contribution of this paper is to extend this AutoML approach in various ways that considerably improve its efficiency and robustness, based on principles that apply to a wide range of machine learning frameworks (such as those used by the machine learning service providers mentioned above). First, following successful previous work for low dimensional optimization problems <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, we reason across datasets to identify instantiations of machine learning frameworks that perform well on a new dataset and warmstart Bayesian optimization with them (Section 3.1). Second, we automatically construct ensembles of the models considered by Bayesian optimization (Section 3.2). Third, we carefully design a highly parameterized machine learning framework from high-performing classifiers and preprocessors implemented in the popular machine learning framework scikit-learn <ref type="bibr" target="#b6">[7]</ref> (Section 4). Finally, we perform an extensive empirical analysis using a diverse collection of datasets to demonstrate that the resulting AUTO-SKLEARN system outperforms previous state-of-the-art AutoML methods (Section 5), to show that each of our contributions leads to substantial performance improvements (Section 6), and to gain insights into the performance of the individual classifiers and preprocessors used in AUTO-SKLEARN (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AutoML as a CASH problem</head><p>We first review the formalization of AutoML as a Combined Algorithm Selection and Hyperparameter optimization (CASH) problem used by AUTO-WEKA's AutoML approach. Two important problems in AutoML are that (1) no single machine learning method performs best on all datasets and (2) some machine learning methods (e.g., non-linear SVMs) crucially rely on hyperparameter optimization. The latter problem has been successfully attacked using Bayesian optimization <ref type="bibr" target="#b2">[3]</ref>, which nowadays forms a core component of an AutoML system. The former problem is intertwined with the latter since the rankings of algorithms depend on whether their hyperparameters are tuned properly. Fortunately, the two problems can efficiently be tackled as a single, structured, joint optimization problem: Definition 2 (CASH). Let A = {A (1) , . . . , A (R) } be a set of algorithms, and let the hyperparameters of each algorithm A (j) have domain Λ (j) . Further, let D train = {(x 1 , y 1 ), . . . , (x n , y n )} be a training set which is split into K cross-validation folds {D train with hyperparameters λ. Then, the Combined Algorithm Selection and Hyperparameter optimization (CASH) problem is to find the joint algorithm and hyperparameter setting that minimizes this loss:</p><formula xml:id="formula_0">A , λ ∈ argmin A (j) ∈A,λ∈Λ (j) 1 K K i=1 L(A (j) λ , D (i) train , D (i) valid ).<label>(1)</label></formula><p>This CASH problem was first tackled by Thornton et al. <ref type="bibr" target="#b1">[2]</ref> in the AUTO-WEKA system using the machine learning framework WEKA <ref type="bibr" target="#b7">[8]</ref> and tree-based Bayesian optimization methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. In a nutshell, Bayesian optimization <ref type="bibr" target="#b2">[3]</ref> fits a probabilistic model to capture the relationship between hyperparameter settings and their measured performance; it then uses this model to select the most promising hyperparameter setting (trading off exploration of new parts of the space vs. exploitation in known good regions), evaluates that hyperparameter setting, updates the model with the result, and iterates. While Bayesian optimization based on Gaussian process models (e.g., Snoek et al. <ref type="bibr" target="#b10">[11]</ref>) performs best in low-dimensional problems with numerical hyperparameters, tree-based models have been shown to be more successful in high-dimensional, structured, and partly discrete problems <ref type="bibr" target="#b11">[12]</ref> such as the CASH problem -and are also used in the AutoML system HYPEROPT-SKLEARN <ref type="bibr" target="#b12">[13]</ref>. Among the tree-based Bayesian optimization methods, Thornton et al. <ref type="bibr" target="#b1">[2]</ref> found the random-forestbased SMAC <ref type="bibr" target="#b8">[9]</ref> to outperform the tree Parzen estimator TPE <ref type="bibr" target="#b9">[10]</ref>, and we therefore use SMAC to solve the CASH problem in this paper. Next to its use of random forests <ref type="bibr" target="#b13">[14]</ref>, SMAC's main distinguishing feature is that it allows fast cross-validation by evaluating one fold at a time and discarding poorly-performing hyperparameter settings early. 3 New methods for increasing efficiency and robustness of AutoML</p><p>We now discuss our two improvements of the AutoML approach. First, we include a meta-learning step to warmstart the Bayesian optimization procedure, which results in a considerable boost in efficiency. Second, we include an automated ensemble construction step, allowing us to use all classifiers that were found by Bayesian optimization.</p><p>Figure <ref type="figure">1</ref> summarizes the overall AutoML workflow, including both of our improvements. We note that we expect their effectiveness to be greater for flexible ML frameworks that offer many degrees of freedom (e.g., many algorithms, hyperparameters, and preprocessing methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Meta-learning for finding good instantiations of machine learning frameworks</head><p>Domain experts derive knowledge from previous tasks: They learn about the performance of machine learning algorithms. The area of meta-learning <ref type="bibr" target="#b14">[15]</ref> mimics this strategy by reasoning about the performance of learning algorithms across datasets. In this work, we apply meta-learning to select instantiations of our given machine learning framework that are likely to perform well on a new dataset. More specifically, for a large number of datasets, we collect both performance data and a set of meta-features, i.e., characteristics of the dataset that can be computed efficiently and that help to determine which algorithm to use on a new dataset.</p><p>This meta-learning approach is complementary to Bayesian optimization for optimizing an ML framework. Meta-learning can quickly suggest some instantiations of the ML framework that are likely to perform quite well, but it is unable to provide fine-grained information on performance. In contrast, Bayesian optimization is slow to start for hyperparameter spaces as large as those of entire ML frameworks, but can fine-tune performance over time. We exploit this complementarity by selecting k configurations based on meta-learning and use their result to seed Bayesian optimization. This approach of warmstarting optimization by meta-learning has already been successfully applied before <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, but never to an optimization problem as complex as that of searching the space of instantiations of a full-fledged ML framework. Likewise, learning across datasets has also been applied in collaborative Bayesian optimization methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>; while these approaches are promising, they are so far limited to very few meta-features and cannot yet cope with the highdimensional partially discrete configuration spaces faced in AutoML.</p><p>More precisely, our meta-learning approach works as follows. In an offline phase, for each machine learning dataset in a dataset repository (in our case 140 datasets from the OpenML <ref type="bibr" target="#b17">[18]</ref> repository), we evaluated a set of meta-features (described below) and used Bayesian optimization to determine and store an instantiation of the given ML framework with strong empirical performance for that dataset. (In detail, we ran SMAC <ref type="bibr" target="#b8">[9]</ref> for 24 hours with 10-fold cross-validation on two thirds of the data and stored the resulting ML framework instantiation which exhibited best performance on the remaining third). Then, given a new dataset D, we compute its meta-features, rank all datasets by their L 1 distance to D in meta-feature space and select the stored ML framework instantiations for the k = 25 nearest datasets for evaluation before starting Bayesian optimization with their results.</p><p>To characterize datasets, we implemented a total of 38 meta-features from the literature, including simple, information-theoretic and statistical meta-features <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, such as statistics about the number of data points, features, and classes, as well as data skewness, and the entropy of the targets. All meta-features are listed in Table <ref type="table" target="#tab_1">1</ref> of the supplementary material. Notably, we had to exclude the prominent and effective category of landmarking meta-features <ref type="bibr" target="#b20">[21]</ref> (which measure the performance of simple base learners), because they were computationally too expensive to be helpful in the online evaluation phase. We note that this meta-learning approach draws its power from the availability of a repository of datasets; due to recent initiatives, such as OpenML <ref type="bibr" target="#b17">[18]</ref>, we expect the number of available datasets to grow ever larger over time, increasing the importance of meta-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Automated ensemble construction of models evaluated during optimization</head><p>While Bayesian hyperparameter optimization is data-efficient in finding the best-performing hyperparameter setting, we note that it is a very wasteful procedure when the goal is simply to make good predictions: all the models it trains during the course of the search are lost, usually including some that perform almost as well as the best. Rather than discarding these models, we propose to store them and to use an efficient post-processing method (which can be run in a second process on-the-fly) to construct an ensemble out of them. This automatic ensemble construction avoids to commit itself to a single hyperparameter setting and is thus more robust (and less prone to overfitting) than using the point estimate that standard hyperparameter optimization yields. To our best knowledge, we are the first to make this simple observation, which can be applied to improve any Bayesian hyperparameter optimization method.</p><p>It is well known that ensembles often outperform individual models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, and that effective ensembles can be created from a library of models <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. Ensembles perform particularly well if the models they are based on (1) are individually strong and (2) make uncorrelated errors <ref type="bibr" target="#b13">[14]</ref>. Since this is much more likely when the individual models are different in nature, ensemble building is particularly well suited for combining strong instantiations of a flexible ML framework.</p><p>However, simply building a uniformly weighted ensemble of the models found by Bayesian optimization does not work well. Rather, we found it crucial to adjust these weights using the predictions of all individual models on a hold-out set. We experimented with different approaches to optimize these weights: stacking [26], gradient-free numerical optimization, and the method ensemble selection <ref type="bibr" target="#b23">[24]</ref>. While we found both numerical optimization and stacking to overfit to the validation set and to be computationally costly, ensemble selection was fast and robust. In a nutshell, ensemble selection (introduced by Caruana et al. <ref type="bibr" target="#b23">[24]</ref>) is a greedy procedure that starts from an empty ensemble and then iteratively adds the model that maximizes ensemble validation performance (with uniform weight, but allowing for repetitions). Procedure 1 in the supplementary material describes it in detail. We used this technique in all our experiments -building an ensemble of size 50. To design a robust AutoML system, as our underlying ML framework we chose scikit-learn <ref type="bibr" target="#b6">[7]</ref>, one of the best known and most widely used machine learning libraries. It offers a wide range of well established and efficiently-implemented ML algorithms and is easy to use for both experts and beginners. Since our AutoML system closely resembles AUTO-WEKA, but -like HYPEROPT-SKLEARN -is based on scikit-learn, we dub it AUTO-SKLEARN.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts AUTO-SKLEARN's overall components. It comprises 15 classification algorithms, 14 preprocessing methods, and 4 data preprocessing methods. We parameterized each of them, which resulted in a space of 110 hyperparameters. Most of these are conditional hyperparameters that are only active if their respective component is selected. We note that SMAC <ref type="bibr" target="#b8">[9]</ref> can handle this conditionality natively.</p><p>All 15 classification algorithms in AUTO-SKLEARN are listed in focused our configuration space on base classifiers and excluded meta-models and ensembles that are themselves parameterized by one or more base classifiers. While such ensembles increased AUTO-WEKA's number of hyperparameters by almost a factor of five (to 786), AUTO-SKLEARN "only" features 110 hyperparameters. We instead construct complex ensembles using our post-hoc method from Section 3.2. Compared to AUTO-WEKA, this is much more data-efficient: in AUTO-WEKA, evaluating the performance of an ensemble with 5 components requires the construction and evaluation of 5 models; in contrast, in AUTO-SKLEARN, ensembles come largely for free, and it is possible to mix and match models evaluated at arbitrary times during the optimization.</p><p>The preprocessing methods for datasets in dense representation in AUTO-SKLEARN are listed in Table <ref type="table" target="#tab_1">1b</ref> (and described in detail in Section A.2 of the supplementary material). They comprise data preprocessors (which change the feature values and are always used when they apply) and feature preprocessors (which change the actual set of features, and only one of which [or none] is used). Data preprocessing includes rescaling of the inputs, imputation of missing values, one-hot encoding and balancing of the target classes. The 14 possible feature preprocessing methods can be categorized into feature selection (2), kernel approximation (2), matrix decomposition (3), embeddings (1), feature clustering (1), polynomial feature expansion <ref type="bibr" target="#b0">(1)</ref> and methods that use a classifier for feature selection (2). For example, L 1 -regularized linear SVMs fitted to the data can be used for feature selection by eliminating features corresponding to zero-valued model coefficients.</p><p>As with every robust real-world system, we had to handle many more important details in AUTO-SKLEARN; we describe these in Section B of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Comparing AUTO-SKLEARN to AUTO-WEKA and HYPEROPT-SKLEARN</head><p>As a baseline experiment, we compared the performance of vanilla AUTO-SKLEARN (without our improvements) to AUTO-WEKA and HYPEROPT-SKLEARN, reproducing the experimental setup with 21 datasets of the paper introducing AUTO-WEKA <ref type="bibr" target="#b1">[2]</ref>. We describe this setup in detail in Section G in the supplementary material.</p><p>Table <ref type="table" target="#tab_2">2</ref> shows that AUTO-SKLEARN performed statistically significantly better than AUTO-WEKA in 6/21 cases, tied it in 12 cases, and lost against it in 3. For the three datasets where AUTO-WEKA performed best, we found that in more than 50% of its runs the best classifier it chose is not implemented in scikit-learn (trees with a pruning component). So far, HYPEROPT-SKLEARN is more of a proof-of-concept -inviting the user to adapt the configuration space to her own needs -than a full AutoML system. The current version crashes when presented with sparse data and missing values. It also crashes on Cifar-10 due to a memory limit which we set for all optimizers to enable a  140 datasets. Note that ranks are a relative measure of performance (here, the rank of all methods has to add up to 10), and hence an improvement in BER of one method can worsen the rank of another. The supplementary material shows the same plot on a log-scale to show the time overhead of meta-feature and ensemble computation. fair comparison. On the 16 datasets on which it ran, it statistically tied the best optimizer in 9 cases and lost against it in 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation of the proposed AutoML improvements</head><p>In order to evaluate the robustness and general applicability of our proposed AutoML system on a broad range of datasets, we gathered 140 binary and multiclass classification datasets from the OpenML repository <ref type="bibr" target="#b17">[18]</ref>, only selecting datasets with at least 1000 data points to allow robust performance evaluations. These datasets cover a diverse range of applications, such as text classification, digit and letter recognition, gene sequence and RNA classification, advertisement, particle classification for telescope data, and cancer detection in tissue samples. We list all datasets in Table <ref type="table">7</ref> and 8 in the supplementary material and provide their unique OpenML identifiers for reproducibility. Since the class distribution in many of these datasets is quite imbalanced we evaluated all AutoML methods using a measure called balanced classification error rate (BER). We define balanced error rate as the average of the proportion of wrong classifications in each class. In comparison to standard classification error (the average overall error), this measure (the average of the class-wise error) assigns equal weight to all classes. We note that balanced error or accuracy measures are often used in machine learning competitions (e.g., the AutoML challenge <ref type="bibr" target="#b0">[1]</ref> uses balanced accuracy).</p><p>We performed 10 runs of AUTO-SKLEARN both with and without meta-learning and with and without ensemble prediction on each of the datasets. To study their performance under rigid time constraints, and also due to computational resource constraints, we limited the CPU time for each run to 1 hour; we also limited the runtime for a single model to a tenth of this (6 minutes). To not evaluate performance on data sets already used for meta-learning, we performed a leave-one-dataset-out validation: when evaluating on dataset D, we only used meta-information from the 139 other datasets.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the average ranks over time of the four AUTO-SKLEARN versions we tested. We observe that both of our new methods yielded substantial improvements over vanilla AUTO-SKLEARN. The most striking result is that meta-learning yielded drastic improvements starting with the first  configuration it selected and lasting until the end of the experiment. We note that the improvement was most pronounced in the beginning and that over time, vanilla AUTO-SKLEARN also found good solutions without meta-learning, letting it catch up on some datasets (thus improving its overall rank).</p><p>Moreover, both of our methods complement each other: our automated ensemble construction improved both vanilla AUTO-SKLEARN and AUTO-SKLEARN with meta-learning. Interestingly, the ensemble's influence on the performance started earlier for the meta-learning version. We believe that this is because meta-learning produces better machine learning models earlier, which can be directly combined into a strong ensemble; but when run longer, vanilla AUTO-SKLEARN without meta-learning also benefits from automated ensemble construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Detailed analysis of AUTO-SKLEARN components</head><p>We now study AUTO-SKLEARN's individual classifiers and preprocessors, compared to jointly optimizing all methods, in order to obtain insights into their peak performance and robustness. Ideally, we would have liked to study all combinations of a single classifier and a single preprocessor in isolation, but with 15 classifiers and 14 preprocessors this was infeasible; rather, when studying the performance of a single classifier, we still optimized over all preprocessors, and vice versa. To obtain a more detailed analysis, we focused on a subset of datasets but extended the configuration budget for optimizing all methods from one hour to one day and to two days for AUTO-SKLEARN. Specifically, we clustered our 140 datasets with g-means <ref type="bibr" target="#b26">[27]</ref> based on the dataset meta-features and used one dataset from each of the resulting 13 clusters (see Table <ref type="table">6</ref> in the supplementary material for the list of datasets). We note that, in total, these extensive experiments required 10.7 CPU years.</p><p>Table <ref type="table" target="#tab_4">3</ref> compares the results of the various classification methods against AUTO-SKLEARN. Overall, as expected, random forests, extremely randomized trees, AdaBoost, and gradient boosting, showed A plot with all classifiers can be found in Figure <ref type="figure" target="#fig_3">4</ref> in the supplementary material. While AUTO-SKLEARN is inferior in the beginning, in the end its performance is close to the best method.</p><p>the most robust performance, and SVMs showed strong peak performance for some datasets. Besides a variety of strong classifiers, there are also several models which could not compete: The decision tree, passive aggressive, kNN, Gaussian NB, LDA and QDA were statistically significantly inferior to the best classifier on most datasets. Finally, the table indicates that no single method was the best choice for all datasets. As shown in the table and also visualized for two example datasets in Figure <ref type="figure" target="#fig_3">4</ref>, optimizing the joint configuration space of AUTO-SKLEARN led to the most robust performance. A plot of ranks over time (Figure <ref type="figure" target="#fig_1">2</ref> and 3 in the supplementary material) quantifies this across all 13 datasets, showing that AUTO-SKLEARN starts with reasonable but not optimal performance and effectively searches its more general configuration space to converge to the best overall performance over time.</p><p>Table <ref type="table" target="#tab_5">4</ref> compares the results of the various preprocessors against AUTO-SKLEARN. As for the comparison of classifiers above, AUTO-SKLEARN showed the most robust performance: It performed best on three of the datasets and was not statistically significantly worse than the best preprocessor on another 8 of 13.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Discussion and Conclusion</head><p>We demonstrated that our new AutoML system AUTO-SKLEARN performs favorably against the previous state of the art in AutoML, and that our meta-learning and ensemble improvements for AutoML yield further efficiency and robustness. This finding is backed by the fact that AUTO-SKLEARN won the auto-track in the first phase of ChaLearn's ongoing AutoML challenge. In this paper, we did not evaluate the use of AUTO-SKLEARN for interactive machine learning with an expert in the loop and weeks of CPU power, but we note that that mode has also led to a third place in the human track of the same challenge. As such, we believe that AUTO-SKLEARN is a promising system for use by both machine learning novices and experts. The source code of AUTO-SKLEARN is available under an open source license at https://github.com/automl/auto-sklearn.</p><p>Our system also has some shortcomings, which we would like to remove in future work. As one example, we have not yet tackled regression or semi-supervised problems. Most importantly, though, the focus on scikit-learn implied a focus on small to medium-sized datasets, and an obvious direction for future work will be to apply our methods to modern deep learning systems that yield state-ofthe-art performance on large datasets; we expect that in that domain especially automated ensemble construction will lead to tangible performance improvements over Bayesian optimization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>train = D train \D (i) valid for i = 1, . . . , K. Finally, let L(A valid ) denote the loss that algorithm A (j) achieves on D (i) valid when trained on D (i)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structured configuration space. Squared boxes denote parent hyperparameters whereas boxes with rounded edges are leaf hyperparameters. Grey colored boxes mark active hyperparameters which form an example configuration and machine learning pipeline. Each pipeline comprises one feature preprocessor, classifier and up to three data preprocessor methods plus respective hyperparameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average rank of all four AUTO-SKLEARN variants (ranked by balanced test error rate (BER)) across</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Performance of a subset of classifiers compared to AUTO-SKLEARN over time. We show median test error rate and the fifth and 95th percentile over time for optimizing three classifiers separately with optimizing the joint space. A plot with all classifiers can be found in Figure4in the supplementary material. While AUTO-SKLEARN is inferior in the beginning, in the end its performance is close to the best method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of hyperparameters for each possible classifier (left) and feature preprocessing method (right) for a binary classification dataset in dense representation. Tables for sparse binary classification and sparse/dense multiclass classification datasets can be found in the Section E of the supplementary material, Tables 2a, 3a, 4a, 2b, 3b and 4b. We distinguish between categorical (cat) hyperparameters with discrete values and continuous (cont) numerical hyperparameters. Numbers in brackets are conditional hyperparameters, which are only relevant when another parameter has a certain value.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>name</cell><cell>#λ</cell><cell>cat (cond)</cell><cell>cont (cond)</cell></row><row><cell>name</cell><cell>#λ</cell><cell>cat (cond)</cell><cell>cont (cond)</cell><cell cols="2">extreml. rand. trees prepr. 5</cell><cell>2 (-)</cell><cell>3 (-)</cell></row><row><cell cols="2">AdaBoost (AB) Bernoulli naïve Bayes decision tree (DT) extreml. rand. trees Gaussian naïve Bayes gradient boosting (GB) 6 4 2 4 5 -kNN 3 LDA 4 linear SVM 4 kernel SVM 7 multinomial naïve Bayes 2 passive aggressive 3</cell><cell>1 (-) 1 (-) 1 (-) 2 (-) --2 (-) 1 (-) 2 (-) 2 (-) 1 (-) 1 (-)</cell><cell>3 (-) 1 (-) 3 (-) 3 (-) -6 (-) 1 (-) 3 (1) 2 (-) 5 (2) 1 (-) 2 (-)</cell><cell>fast ICA feature agglomeration kernel PCA rand. kitchen sinks linear SVM prepr. no preprocessing nystroem sampler PCA polynomial random trees embed. select percentile select rates</cell><cell>4 4 5 2 3 -5 2 3 4 2 3</cell><cell>3 (-) 3 () 1 (-) -1 (-) -1 (-) 1 (-) 2 (-) -1 (-) 2 (-)</cell><cell>1 (1) 1 (-) 4 (3) 2 (-) 2 (-) -4 (3) 1 (-) 1 (-) 4 (-) 1 (-) 1 (-)</cell></row><row><cell>QDA random forest (RF) Linear Class. (SGD)</cell><cell>2 5 10</cell><cell>-2 (-) 4 (-)</cell><cell>2 (-) 3 (-) 6 (3)</cell><cell>one-hot encoding imputation balancing</cell><cell>2 1 1</cell><cell>1 (-) 1 (-) 1 (-)</cell><cell>1 (1) --</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>rescaling</cell><cell>1</cell><cell>1 (-)</cell><cell>-</cell></row><row><cell cols="4">(a) classification algorithms</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) preprocessing methods</cell></row></table><note>Table1a(and described in detail in Section A.1 of the supplementary material). They fall into different categories, such as general linear models (2 algorithms), support vector machines (2), discriminant analysis (2), nearest neighbors (1), naïve Bayes (3), decision trees (1) and ensembles<ref type="bibr" target="#b3">(4)</ref>. In contrast to AUTO-WEKA<ref type="bibr" target="#b1">[2]</ref>, we</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>.00 0.39 51.70 54.81 17.53 5.56 5.51 27.00 1.62 1.74 0.42 12.44 2.84 46.92 7.87 5.24 0.01 14.93 33.76 40.67 AW 73.50 30.00 0.00 56.95 56.20 21.80 8.33 6.38 28.33 2.29 1.74 0.31 18.21 2.84 60.34 8.09 5.24 0.01 14.13 33.36 37.75 Test set classification error of AUTO-WEKA (AW), vanilla AUTO-SKLEARN (AS) and HYPEROPT-SKLEARN(HS), as in the original evaluation of AUTO-WEKA<ref type="bibr" target="#b1">[2]</ref>. We show median percent error across 100 000 bootstrap samples (based on 10 runs), simulating 4 parallel runs. Bold numbers indicate the best result. Underlined results are not statistically significantly different from the best according to a bootstrap test with p = 0.05.</figDesc><table><row><cell>Abalone</cell><cell>Amazon</cell><cell>Car</cell><cell>Cifar-10</cell><cell>Cifar-10</cell><cell>Small</cell><cell>Convex</cell><cell>Dexter</cell><cell>Dorothea</cell><cell>German</cell><cell>Credit</cell><cell>Gisette</cell><cell>KDD09</cell><cell>Appetency</cell><cell>KR-vs-KP</cell><cell>Madelon</cell><cell>MNIST</cell><cell>Basic</cell><cell>MRBI</cell><cell>Secom</cell><cell>Semeion</cell><cell>Shuttle</cell><cell>Waveform</cell><cell>Wine</cell><cell>Quality</cell><cell>Yeast</cell></row><row><cell cols="4">AS 73.50 16HS 76.21 16.22 0.39 -</cell><cell cols="4">57.95 19.18 -</cell><cell>-</cell><cell cols="4">27.67 2.29 -</cell><cell></cell><cell cols="6">0.42 14.74 2.82 55.79 -</cell><cell cols="6">5.87 0.05 14.07 34.72 38.45</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Median balanced test error rate (BER) of optimizing AUTO-SKLEARN subspaces for each classification method (and all preprocessors), as well as the whole configuration space of AUTO-SKLEARN, on 13 datasets. All optimization runs were allowed to run for 24 hours except for AUTO-SKLEARN which ran for 48 hours. Bold numbers indicate the best result; underlined results are not statistically significantly different from the best according to a bootstrap test using the same setup as for Table2.</figDesc><table><row><cell>OpenML</cell><cell>dataset ID</cell><cell>AUTO-</cell><cell>SKLEARN</cell><cell>densifier</cell><cell>extreml. rand.</cell><cell>trees prepr.</cell><cell>fast ICA</cell><cell>feature</cell><cell>agglomeration</cell><cell>kernel PCA</cell><cell>rand.</cell><cell>kitchen sinks</cell><cell>linear</cell><cell>SVM prepr.</cell><cell>no</cell><cell>preproc.</cell><cell>nystroem</cell><cell>sampler</cell><cell>PCA</cell><cell>polynomial</cell><cell>random</cell><cell>trees embed.</cell><cell>select percentile</cell><cell>classification</cell><cell>select rates</cell><cell>truncatedSVD</cell></row><row><cell cols="2">38</cell><cell cols="2">2.15</cell><cell>-</cell><cell cols="2">4.03</cell><cell>7.27</cell><cell cols="2">2.24</cell><cell>5.84</cell><cell cols="2">8.57</cell><cell cols="2">2.28</cell><cell cols="2">2.28</cell><cell cols="2">7.70</cell><cell>7.23</cell><cell cols="3">2.90 18.50</cell><cell cols="2">2.20</cell><cell>2.28</cell><cell>-</cell></row><row><cell cols="2">46</cell><cell cols="2">3.76</cell><cell>-</cell><cell cols="2">4.98</cell><cell>7.95</cell><cell cols="2">4.40</cell><cell>8.74</cell><cell cols="2">8.41</cell><cell cols="2">4.25</cell><cell cols="2">4.52</cell><cell cols="2">8.48</cell><cell>8.40</cell><cell>4.21</cell><cell cols="2">7.51</cell><cell cols="2">4.17</cell><cell>4.68</cell><cell>-</cell></row><row><cell cols="4">179 16.99</cell><cell cols="7">-17.83 17.24 16.92 100.00</cell><cell cols="9">17.34 16.84 16.97 17.30 17.64</cell><cell cols="6">16.94 17.05 17.09 16.86</cell><cell>-</cell></row><row><cell cols="4">184 10.32</cell><cell cols="6">-55.78 19.96 11.31</cell><cell>36.52</cell><cell cols="2">28.05</cell><cell cols="7">9.92 11.43 25.53 21.15</cell><cell cols="6">10.54 12.68 45.03 10.47</cell><cell>-</cell></row><row><cell cols="2">554</cell><cell cols="2">1.55</cell><cell>-</cell><cell cols="2">1.56</cell><cell>2.52</cell><cell cols="5">1.65 100.00 100.00</cell><cell cols="2">2.21</cell><cell cols="2">1.60</cell><cell cols="2">2.21</cell><cell cols="2">1.65 100.00</cell><cell cols="2">3.48</cell><cell cols="2">1.46</cell><cell>1.70</cell><cell>-</cell></row><row><cell cols="4">772 46.85</cell><cell cols="6">-47.90 48.65 48.62</cell><cell>47.59</cell><cell cols="9">47.68 47.72 48.34 48.06 47.30</cell><cell cols="6">48.00 47.84 47.56 48.43</cell><cell>-</cell></row><row><cell cols="4">917 10.22</cell><cell>-</cell><cell cols="5">8.33 16.06 10.33</cell><cell>20.94</cell><cell cols="2">35.44</cell><cell cols="2">8.67</cell><cell cols="5">9.44 37.83 22.33</cell><cell cols="6">9.11 17.67 10.00 10.44</cell><cell>-</cell></row><row><cell cols="4">1049 12.93</cell><cell cols="6">-20.36 19.92 13.14</cell><cell>19.57</cell><cell cols="9">20.06 13.28 15.84 18.96 17.22</cell><cell cols="6">12.95 18.52 11.94 14.38</cell><cell>-</cell></row><row><cell cols="4">1111 23.70</cell><cell cols="7">-23.36 24.69 23.73 100.00</cell><cell cols="9">25.25 23.43 22.27 23.95 23.25</cell><cell cols="6">26.94 26.68 23.53 23.33</cell><cell>-</cell></row><row><cell cols="4">1120 13.81</cell><cell cols="6">-16.29 14.22 13.73</cell><cell>14.57</cell><cell cols="9">14.82 14.02 13.85 14.66 14.23</cell><cell cols="6">13.22 15.03 13.65 13.67</cell><cell>-</cell></row><row><cell cols="2">1128</cell><cell cols="2">4.21</cell><cell>-</cell><cell cols="2">4.90</cell><cell>4.96</cell><cell cols="2">4.76</cell><cell>4.21</cell><cell cols="2">5.08</cell><cell cols="2">4.52</cell><cell cols="2">4.59</cell><cell cols="2">4.08</cell><cell>4.59</cell><cell>50.00</cell><cell cols="2">9.23</cell><cell cols="2">4.33</cell><cell>4.08</cell><cell>-</cell></row><row><cell cols="2">293</cell><cell cols="3">2.86 24.40</cell><cell cols="2">3.41</cell><cell>-</cell><cell></cell><cell cols="2">-100.00</cell><cell cols="2">19.30</cell><cell cols="2">3.01</cell><cell cols="4">2.66 20.94</cell><cell>-</cell><cell>-</cell><cell cols="2">8.05</cell><cell cols="2">2.86</cell><cell>2.74</cell><cell>4.05</cell></row><row><cell cols="7">389 19.65 20.63 21.40</cell><cell>-</cell><cell></cell><cell>-</cell><cell>17.50</cell><cell cols="8">19.66 19.89 20.87 18.46</cell><cell>-</cell><cell cols="7">-44.83 20.17 19.18 21.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Like Table3, but instead optimizing subspaces for each preprocessing method (and all classifiers).</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by the German Research Foundation (DFG), under Priority Programme Autonomous Learning (SPP 1527, grant HU 1900/3-1), under Emmy Noether grant HU 1900/2-1, and under the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Design of the 2015 ChaLearn AutoML Challenge</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cawley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Macià</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Statnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Viegas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IJCNN&apos;15</title>
				<meeting>of IJCNN&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of KDD&apos;13</title>
				<meeting>of KDD&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="847" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Brochu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Cora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<idno>CoRR, abs/1012.2599</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Initializing Bayesian hyperparameter optimization via metalearning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI&apos;15</title>
				<meeting>of AAAI&apos;15</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1128" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-learning for evolutionary parameter optimization of classifiers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Reif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="357" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combining meta-learning and search techniques to select parameters for support vector machines</title>
		<author>
			<persName><forename type="first">T</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Prudêncio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="13" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The WEKA data mining software: An update</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reutemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
				<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential model-based optimization for general algorithm configuration</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of LION&apos;11</title>
				<meeting>of LION&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="507" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS&apos;11</title>
				<meeting>of NIPS&apos;11</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2546" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Practical Bayesian optimization of machine learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS&apos;12</title>
				<meeting>of NIPS&apos;12</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2960" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards an empirical foundation for assessing Bayesian optimization of hyperparameters</title>
		<author>
			<persName><forename type="first">K</forename><surname>Eggensperger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Optimization in Theory and Practice</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn</title>
		<author>
			<persName><forename type="first">B</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on AutoML</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLJ</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Metalearning: Applications to Data Mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vilalta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collaborative hyperparameter tuning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;13</title>
				<meeting>of ICML&apos;13</meeting>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient transfer learning method for automatic hyperparameter tuning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AISTATS&apos;14</title>
				<meeting>of AISTATS&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OpenML: Networked science in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rijn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Torgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explorations</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Michie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<title level="m">Machine Learning, Neural and Statistical Classification</title>
				<imprint>
			<publisher>Ellis Horwood</publisher>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Algorithm Selection via Meta-Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalousis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Geneve</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning by landmarking various learning algorithms</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bensusan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Giraud-Carrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of (ICML&apos;00)</title>
				<meeting>of (ICML&apos;00)</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="743" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model selection: Beyond the Bayesian/Frequentist divide</title>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cawley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="61" to="87" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Agnostic Bayesian learning of ensembles</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;14</title>
				<meeting>of ICML&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ensemble selection from libraries of models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Crew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ksikes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML&apos;04</title>
				<meeting>of ICML&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Getting the most out of ensemble selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Munson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICDM&apos;06</title>
				<meeting>of ICDM&apos;06</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="828" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning the k in k-means</title>
		<author>
			<persName><forename type="first">G</forename><surname>Hamerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NIPS&apos;04</title>
				<meeting>of NIPS&apos;04</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m">Proc. of ICML&apos;13</title>
				<meeting>of ICML&apos;13</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
