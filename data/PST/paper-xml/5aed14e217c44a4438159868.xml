<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training Tips for the Transformer Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018">APRIL 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
							<email>popel@ufal.mff.cuni.cz</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<region>Czechia</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University Malostranské náměstí 25</orgName>
								<address>
									<postCode>118 00</postCode>
									<settlement>Praha</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Czech Republic</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Faculty of Mathematics and Physics</orgName>
								<orgName type="department" key="dep2">Institute of Formal and Applied Linguistics</orgName>
								<orgName type="institution">Charles University</orgName>
								<address>
									<settlement>Prague</settlement>
									<region>Czechia</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Formal and Applied Linguistics Faculty of Mathematics and Physics</orgName>
								<orgName type="institution">Charles University Malostranské náměstí 25</orgName>
								<address>
									<postCode>118 00</postCode>
									<settlement>Praha</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training Tips for the Transformer Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018">APRIL 2018</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.2478/pralin-2018-0002</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>BLEU+case</term>
					<term>lc+lang</term>
					<term>en-cs+numrefs</term>
					<term>1+smooth</term>
					<term>exp+test</term>
					<term>wmt13+tok</term>
					<term>intl+version</term>
					<term>1</term>
					<term>2</term>
					<term>3</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article describes our experiments in neural machine translation using the recent Ten-sor2Tensor framework and the Transformer sequence-to-sequence model <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref>. We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>It has been already clearly established that neural machine translation (NMT) is the new state of the art in machine translation, see e.g. the most recent evaluation campaigns <ref type="bibr" target="#b3">(Bojar et al., 2017a;</ref><ref type="bibr" target="#b7">Cettolo et al., 2017)</ref>. Many fundamental changes of the underlying neural network architecture are nevertheless still frequent and it is very difficult to predict which of the architectures has the best combination of properties to win in the long term, considering all relevant criteria like translation quality, model size, stability and speed of training, interpretability but also practical availability of good implementations. A considerable part of a model's success in translation quality consists in the training data, the model's sensitivity to noise in the data but also on a wide range of hyper-parameters that affect the training. Having the right setting of them turns out to be often a critical component for the success.</p><p>tional-tokenization) and automatically downloads the reference translation for a given WMT testset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Considerations on Stopping Criterion</head><p>The situation in NMT is further complicated by the fact that the training of NMT systems is usually non-deterministic,<ref type="foot" target="#foot_0">4</ref> and (esp. with the most recent models) hardly ever converges or starts overfitting<ref type="foot" target="#foot_1">5</ref> on reasonably big datasets. This leads to learning curves that never fully flatten let alone start decreasing (see Section 4.2). The common practice of machine learning to evaluate the model on a final test set when it started overfitting (or a bit sooner) is thus not applicable in practice.</p><p>Many papers in neural machine translation do not specify any stopping criteria whatsoever. Sometimes, they mention only an approximate number of days the model was trained for, e.g. <ref type="bibr" target="#b0">Bahdanau et al. (2015)</ref>, sometimes the exact number of training steps is given but no indication on "how much converged" the model was at that point, e.g. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref>. Most probably, the training was run until no further improvements were clearly apparent on the development test set, and the model was evaluated at that point. Such an approximate stopping criterion is rather risky: it is conceivable that different setups were stopped at different stages of training and their comparison is not fair.</p><p>A somewhat more reliable method is to keep training for a specified number of iterations or a certain number of epochs. This is however not a perfect solution either, if the models are not quite converged at that time and the difference in their performance is not sufficiently large. It is quite possible that e.g. a more complex model would need a few more epochs and eventually arrived at a higher score than its competitor. Also, the duration of one training step (or one epoch) differs between models (see Section 4.1) and from the practical point of view, we are mostly interested in the wall-clock time.</p><p>When we tried the standard technique of early stopping, when N subsequent evaluations on the development test set do not give improvements larger than a given delta, we saw a big variance in the training time and final BLEU, even for experiments with the same hyper-parameters and just a different random seed. Moreover to get the best results, we would have had to use a very large N and a very small delta.</p><p>PBML 110 APRIL 2018</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Our Final Choice: Full Learning Curves</head><p>Based on the discussion above, we decided to report always the full learning curves and not just single scores. This solution does not fully prevent the risk of premature judgments, but the readers can at least judge for themselves if they would expect any sudden twist in the results or not.</p><p>In all cases, we plot the case-insensitive BLEU score against the wall-clock time in hours. This solution obviously depends on the hardware chosen, so we always used the same equipment: one up to eight GeForce GTX 1080 Ti GPUs with NVIDIA driver 375.66. Some variation in the measurements is unfortunately unavoidable because we could not fully isolate the computation from different processes on the same machine and from general network traffic, but based on our experiments with replicated experiments such variation is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Terminology</head><p>For clarity, we define the following terms and adhere to them for the rest of the paper: Translation quality is an automatic estimate of how well the translation carried out by a particular fixed model expresses the meaning of the source. We estimate translation quality solely by BLEU score against one reference translation. Training Steps denote the number of iterations, i.e. the number of times the optimizer update was run. This number also equals the number of (mini)batches that were processed. Batch Size is the number of training examples used by one GPU in one training step.</p><p>In sequence-to-sequence models, batch size is usually specified as the number of sentence pairs. However, the parameter batch_size in T2T translation specifies the approximate number of tokens (subwords) in one batch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Tools for Evaluation within Tensor2Tensor</head><p>T2T, being implemented in TensorFlow, provides nice TensorBoard visualizations of the training progress. The original implementation was optimized towards speed of evaluation rather than towards following the standards of the field. T2T thus reports "approx-bleu" by default, which is computed on the internal subwords (never exposed to the user, actually) instead of words (according to BLEU tokenization). As a result, "approx-bleu" is usually about 1.2-1.8 times higher than the real BLEU. Due to its dependence on the training data (for the subword vocabulary), it is not easily reproducible in varying experiments and thus not suitable for reporting in publications. We implemented a helper script t2t-bleu which computes the "real" BLEU (giving the same result as sacréBLEU with --tokenization intl). Our script can be used in two ways:</p><p>• To evaluate one translated file: t2t-bleu --translation=my-wmt13.de --reference=wmt13_deen.de</p><p>• To evaluate all translations in a given directory (created e.g. by t2t-translateall) and store the results in a TensorBoard events file. All the figures in this article were created this way.</p><p>We also implemented t2t-translate-all and t2t-avg-all scripts, which translate all checkpoints in a given directory and average a window of N subsequent checkpoints, respectively. 9 For details on averaging see Section 4.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Data Selection and Preprocessing</head><p>We focused on the English-to-Czech translation direction. Most of our training data comes from the CzEng parallel treebank, version 1.7 (57M sentence pairs), 10 and the rest (1M sentence pairs) comes from three smaller sources (Europarl, News Commentary, Common Crawl) as detailed in Table <ref type="table" target="#tab_1">1</ref>.</p><p>We use this dataset of 58M sentence pairs for most our experiments. In some experiments (in Sections 4.2 and 4.6), we substitute CzEng 1.7 with an older and considerably smaller CzEng 1.0 <ref type="bibr" target="#b1">(Bojar et al., 2012)</ref> containing 15M sentence pairs (233M/206M of en/cs words).</p><p>To plot the performance throughout the training, we use WMT newstest2013 as a development set (not overlapping with the training data). In Section 5, we apply our best model (judged from the performance on the development set) to the WMT newstest2017, for comparison with the state-of-the-art systems. 9 All three scripts are now merged in the T2T master. All three scripts can be used while the training is still in progress, i.e. they wait a given number of minutes for new checkpoints to appear. 10 http://ufal.mff.cuni.cz/czeng/czeng17, which is a subset of CzEng 1.6 <ref type="bibr" target="#b2">(Bojar et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>48</head><p>Unauthenticated Download Date | 11/13/18 8:36 AM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training Data Preprocessing</head><p>Data preprocessing such as tokenization and truecasing has always been a very important part of the setup of statistical machine translation systems. A huge leap in scaling NMT to realistic data size has been achieved by the introduction of subword units <ref type="bibr" target="#b18">(Sennrich et al., 2016)</ref>, but the long-term vision of the deep-learning community is to leave all these "technicalities" up to the trained neural network and feed it with as original input as possible (see e.g. <ref type="bibr" target="#b14">Lee et al., 2016)</ref>.</p><p>T2T adopts this vision and while it supports the use of external subword units, it comes with its own built-in method similar to the word-piece algorithm by <ref type="bibr" target="#b23">Wu et al. (2016)</ref> and does not expect the input to be even tokenized. Based on a small sample of the training data, T2T will train a subword vocabulary and apply it to all the training and later evaluation data.</p><p>We follow the T2T default and provide raw plain text training sentences. We use the default parameters: shared source and target (English and Czech) subword vocabulary of size 32k.<ref type="foot" target="#foot_6">11</ref> After this preprocessing, the total number of subwords in our main training data is 992 millions (taking the maximum of English and Czech lengths for each sentence pair, as needed for computing the number of epochs, see Section 2.3). The smaller dataset CzEng 1.0 has 327 million subwords. In both cases the average number of subwords per (space-delimited) word is about 1.5.</p><p>Even when following the defaults, there are some important details that should be considered. We thus provide our first set of technical tips here:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on Training Data Preprocessing</head><p>• Make sure that the subword vocabulary is trained on a sufficiently large sample of the training data.<ref type="foot" target="#foot_7">12</ref> • As discussed in Section 4.5, a higher batch size may be beneficial for the training and the batch size can be higher when excluding training sentences longer than a given threshold. This can be controlled with parameter max_length (see Section 4.4), but it may be a good idea to exclude too long sentences even before preparing the training data using t2t-datagen. This way the TFRecords training files will be smaller and their processing a bit faster.<ref type="foot" target="#foot_8">13</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present several experiments, always summarizing the observations and giving some generally applicable tips that we learned. All experiments were done with T2T v1.2.9 unless stated otherwise.</p><p>We experiment with two sets of hyper-parameters pre-defined in T2T: transfor-mer_big_single_gpu (BIG) and transformer_base_single_gpu (BASE), which differ mainly in the size of the model. Note that transformer_big_single_gpu and trans-former_base_single_gpu are just names of a set of hyper-parameters, which can be applied even when training on multiple GPUs, as we do in our experiments, see Section 4.7. <ref type="foot" target="#foot_9">14</ref>Our baseline setting uses the BIG model with its default hyper-parameters except for:</p><p>• batch_size=1500 (see the discussion of different sizes in Section 4.5),</p><p>• --train_steps=6000000, i.e. high enough, so we can stop each experiment manually as needed, • --save_checkpoints_secs=3600 which forces checkpoint saving each hour (see Section 4.10), • --schedule=train which disables the internal evaluation with approx_bleu and thus makes training a bit faster (see Section 2).<ref type="foot" target="#foot_10">15</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Computation Speed and Training Throughput</head><p>We are primarily interested in the translation quality (BLEU learning curves and Time Till Score) and we discuss it in the following sections 4.2-4.10. In this section, we focus however only on the computation speed and training throughput. Both are affected by three important factors: batch size, number of used GPUs and model size. The speed is usually almost constant for a given experiment. <ref type="foot" target="#foot_11">16</ref>Table <ref type="table" target="#tab_3">2</ref> shows the computation speed and training throughput for a single GPU and various batch sizes and model sizes <ref type="bibr">(BASE and BIG)</ref>. The BASE model allows for using a higher batch size than the BIG model. The cells where the BIG model resulted in out-of-memory errors are marked with "OOM". <ref type="foot" target="#foot_12">17</ref>   tion speed decreases with increasing batch size because not all operations in GPU are fully batch-parallelizable. The training throughput grows sub-linearly with increasing batch size, so based on these experiments only, there is just a small advantage when setting the batch size to the maximum value. We will return to this question in Section 4.5, while taking into account the translation quality.</p><p>We can also see the BASE model has approximately two times bigger throughput as well as computation speed relative to the BIG model. Table <ref type="table" target="#tab_4">3</ref> uses the BIG model and batch_size=1500, while varying the number of GPUs. The overhead in GPU synchronization is apparent from the decreasing computation speed. Nevertheless, the training throughput still grows with more GPUs, so e.g. with 6 GPUs we process 3.2 times more training data per hour relative to a single GPU (while without any overhead we would hypothetically expect 6 times more data). The overhead when scaling to multiple GPUs is smaller than the overhead when scaling to a higher batch size. Scaling from a single GPU to 6 GPUs increases the throughput 3.2 times, but scaling from batch size 1000 to 6000 on a single GPU increases the throughput 1.3 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Data Size</head><p>For this experiment, we substituted CzEng 1.7 with CzEng 1.0 in the training data, so the total training size is 16 million sentence pairs (255M / 226M of English/Czech words). Figure <ref type="figure" target="#fig_0">1</ref>  With batch_size=1500 and 8 GPUs, training one epoch of the smaller dataset (with CzEng 1.0) takes 27k steps (5 hours of training), compared to 83k steps (15 hours) for the bigger dataset (with CzEng 1.7). This means about 10 epochs in the smaller dataset were needed for reaching the convergence and this is also the moment when the bigger dataset starts being clearly better. However, even 18 epochs in the bigger dataset were not enough to reach the convergence. enough to reach the convergence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Model Size</head><p>Choosing the right model size is important for practical reasons: larger models may not fit any more on your GPU or they may require to use a very small batch size.</p><p>We experiment with two models,<ref type="foot" target="#foot_15">20</ref> as pre-defined in Tensor2Tensortransfor-mer_big_single_gpu (BIG) and transformer_base_single_gpu (BASE), which differ in four hyper-parameters summarized in Table <ref type="table" target="#tab_6">4</ref> Figure <ref type="figure">2</ref> shows that on a single GPU, the BIG model becomes clearly better than the BASE model after 4 hours of training if we keep the batch size the same -2000 (and we have confirmed it with 1500 in other experiments). However, the BASE model takes less memory, so we can afford a higher batch size, in our case 4500 (with no max_length restriction, see the next section), which improves the BLEU (see Section 4.5). But even Figure <ref type="figure">3</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Maximum Training Sentence Length</head><p>The parameter max_length specifies the maximum length of a sentence in subwords. Longer sentences (either in source or target language) are excluded from the training completely. If no max_length is specified (which is the default), batch_size is used instead. Lowering the max_length allows to use a higher batch size or a bigger model. Since the Transformer implementation in T2T can suddenly run out of memory even after several hours of training, it is good to know how large batch size fits in your GPU. Table <ref type="table" target="#tab_7">5</ref> presents what we empirically measured for the BASE and BIG models with Adam and Adafactor<ref type="foot" target="#foot_16">21</ref> optimizers and various max_length values.</p><p>Setting max_length too low would result in excluding too many training sentences and biasing the translation towards shorter sentences, which would hurt the translation quality. The last two columns in Table <ref type="table" target="#tab_7">5</ref> show that setting max_length to 70 (resp. 100) results in excluding only 2.1% (resp. 0.7%) of sentences in the training data, and only 2.2% (resp. 0.3%) sentences in the development test data are longer, so the detrimental effect of smaller training data and length bias should be minimal in this setting. However, our experiments with batch_size=1500 in Figure <ref type="figure" target="#fig_3">4</ref>  restriction. The training loss of max_length=25 (and 50 and 70) has high variance and stops improving after the first hour of training but shows no sudden increase (as in the case of diverged training discussed in Section 4.6 when the learning rate is too high). We have no explanation for this phenomenon. <ref type="foot" target="#foot_17">22</ref>We did another set of experiments with varying max_length, but this time with batch_size=2000 instead of 1500. In this case, max_length 25 and 50 still results in slower growing BLEU curves, but 70 and higher has the same curve as no max_length restriction. So in our case, if the batch size is high enough, the max_length has almost no effect on BLEU, but this should be checked for each new dataset.</p><p>We trained several models with various max_length for three days and observed that they are not able to produce longer translations than what was the maximum length used in training, even if we change the decoding parameter alpha.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on max_length</head><p>• Set (a reasonably low) max_length. This allows to use a higher batch size and prevents out-of-memory errors after several hours of training. Also, with a higher percentage of training sentences that are almost max_length long, there is a higher chance that the training will fail either immediately (if the batch size is too high) or never (otherwise)., </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Batch Size</head><p>The default batch_size value in recent T2T versions is 4096 subwords for all models except for transformer_base_single_gpu, where the default is 2048. However, we recommend to always set the batch size explicitly 23 or at least make a note what was the default in a given T2T version when reporting experimental results.</p><p>Figure <ref type="figure" target="#fig_4">5</ref> shows learning curves for five different batch sizes (1000, 1500, 3000, 4500 and 6000) for experiments with a single GPU and the BASE model. 24 A higher batch size up to 4500 is clearly better in terms of BLEU as measured by Time Till Score and Examples Till Score metrics defined in Section 4.1. For example, to get over BLEU of 18 with batch_size=3000, we need 7 hours (260M examples), and with batch_size=1500, we need about 3 days (2260M examples) i.e. 10 times longer (9 time more examples). From Table <ref type="table" target="#tab_3">2a</ref> we know that bigger batches have slower computation speed, so when re-plotting Figure <ref type="figure" target="#fig_4">5</ref> with steps instead of time on the x-axis, the difference between the curves would be even bigger. From Table <ref type="table" target="#tab_3">2b</ref> we know that bigger batches have slightly higher training throughput, so when re-plotting with number of examples processed on the x-axis, the difference will be smaller, but still visible. The only exception is the difference between batch size 4500 and 6000, which is very small and can be fully 23 e.g. --hparams="batch_size=1500,learning_rate=0.20,learning_rate_warmup_steps=16000"</p><p>As the batch size is specified in subwords, we see no advantage in using power-of-two values. 24 All the experiments in Figure <ref type="figure" target="#fig_4">5</ref> use max_length=70, but we have got the same curves when re-running without any max_length restrictions, except for batch_size=6000 which failed with OOM. explained by the fact that batch size 6000 has 7% higher throughput than batch size 4500.</p><p>So for the BASE model, a higher batch size gives better results, although with diminishing returns. This observation goes against the common knowledge in other NMT frameworks and deep learning in general <ref type="bibr" target="#b12">(Keskar et al., 2017)</ref> that smaller batches proceed slower (training examples per hour) but result in better generalization (higher test-set BLEU) in the end. In our experiments with the BASE model in T2T, bigger batches are not only faster in training throughput (as could be expected), but also faster in convergence speed, Time Till Score and Examples Till Score.</p><p>Interestingly, when replicating these experiments with the BIG model, we see quite different results, as shown in Figure <ref type="figure" target="#fig_5">6</ref>. The BIG model needs a certain minimal batch size to start converging at all, but for higher batch sizes there is almost no difference in the BLEU curves (but still, bigger batch never makes the BLEU worse in our experiments). In our case, the sharp difference is between batch size 1450, which trains well, and 1400, which drops off after two hours of training, recovering only slowly.</p><p>According to Smith and Le (2017) and <ref type="bibr">Smith et al. (2017)</ref>, the gradient noise scale, i.e. scale of random fluctuations in the SGD (or Adam etc.) dynamics, is proportional to learning rate divided by the batch size (cf. Section 4.8). Thus when lowering the batch size, we increase the noise scale and the training may diverge. This may be either permanent, as in the case of batch size 1000 in Figure <ref type="figure" target="#fig_5">6</ref>, or temporary, as in the case of batch size 1300 and 1400, where the BLEU continues to grow after the temporary drop, but much more slowly than the non-diverged curves.</p><p>We are not sure what causes the difference between the BASE and BIG models with regards to the sensitivity to batch size. One hypothesis is that the BIG model is more difficult to initialize and thus more sensitive to divergence in the early training phase. Also while for BASE, increasing the batch size was highly helpful until 4500, for BIG this limit may be below 1450, i.e. below the minimal batch size needed for preventing diverged training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tip on Batch Size</head><p>• Batch size should be set as high as possible while keeping a reserve for not hitting the out-of-memory errors. It is advisable to establish the largest possible batch size before starting the main and long training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Learning Rate and Warmup Steps on a Single GPU</head><p>The default learning rate in T2T translation models is 0.20. Figure <ref type="figure" target="#fig_6">7</ref> shows that varying the value within range 0.05-0.25 makes almost no difference. Setting the learning rate too low (0.01) results in notably slower convergence. Setting the learning rate too high (0.30, not shown in the figure) results in diverged training, which means in this case that the learning curve starts growing as usual, but at one moment drops down almost to zero and stays there forever.</p><p>A common solution to prevent diverged training is to decrease the learning_rate parameter or increase learning_rate_warmup_steps or introduce gradient clipping. The learning_rate_warmup_steps parameter configures a linear_warmup_rsqrt_decay schedule 25 and it is set to 16 000 by default (for the BIG model), meaning that within 25 The schedule was called noam in T2T versions older than 1.4.4. the first 16k steps the learning rate grows linearly and then follows an inverse square root decay (t −0.5 , cf. Section 4.8.3). At 16k steps, the actual learning rate is thus the highest.</p><p>If a divergence is to happen, it usually happens within the first few hours of training, when the actual learning rate becomes the highest. Once we increased the warmup steps from 16k to 32k, we were able to train with the learning rate of 0.30 and even 0.50 without any divergence. The learning curves looked similarly to the baseline one (with default values of 16k warmup steps and learning rate 0.20). When trying learning rate 1.0, we had to increase warmup steps to 60k (with 40k the training diverged after one hour) -this resulted in a slower convergence at first (about 3 BLEU lower than the baseline after 8 hours of training), but after 3-4 days of training having the same curve as the baseline.</p><p>Figure <ref type="figure" target="#fig_7">8</ref> shows the effect of different warmup steps with a fixed learning rate (the default 0.20). Setting warmup steps too low (12k) results in diverged training. Setting them too high (48k, green curve) results in a slightly slower convergence at first, but matching the baseline after a few hours of training.</p><p>We can conclude that for a single GPU and the BIG model, there is a relatively large range of learning rate and warmup steps values that achieve the optimal results. The default values learning_rate=0.20 and learning_rate_warmup_steps=16000 are within this range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on Learning Rate and Warmup Steps</head><p>• In case of diverged training, try gradient clipping and/or more warmup steps.</p><p>• If that does not help (or if the warmup steps are too high relative to the expected total training steps), try decreasing the learning rate. • Note that when you decrease warmup steps (and keep learning rate), you also increase the maximum actual learning rate because of the way how the lin-ear_warmup_rsqrt_decay (aka noam) schedule is implemented.<ref type="foot" target="#foot_18">26</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Number of GPUs</head><p>T2T allows to train with multiple GPUs on the same machine simply using the parameter --worker_gpus.<ref type="foot" target="#foot_19">27</ref> As explained in Section 2.3, the parameter batch_size is interpreted per GPU, so with 8 GPUs, the effective batch size is 8 times bigger.</p><p>A single-GPU experiment with batch size 4000, should give exactly the same results as two GPUs and batch size 2000 and as four GPUs and batch size 1000 because the effective batch size is 4000 in all three cases. We have confirmed this empirically. By the "same results" we mean BLEU (or train loss) versus training steps on the x-axis. When considering time, the four-GPU experiment will be the fastest one, as explained in Section 4.1.</p><p>Figure <ref type="figure" target="#fig_8">9</ref> shows BLEU curves for different numbers of GPUs and the BIG model with batch size, learning rate and warmup steps fixed on their default values (1500, 0.20 and 16k, respectively). As could be expected, training with more GPUs converges faster. What is interesting is the Time Till Score.  We can see that two GPUs are more than three times faster than a single GPU when measuring the Time Till Score and need much less training examples (i.e. they have lower Examples Till Score). Similarly, eight GPUs are more than five times faster than two GPUs and 1.7 times less training data is needed.</p><p>Recall that in Figure <ref type="figure" target="#fig_5">6</ref> we have shown that increasing the batch size from 1450 to 2000 has almost no effect on the BLEU curve. However, when increasing the effective batch size by using more GPUs, the improvement is higher than could be expected from the higher throughput. <ref type="foot" target="#foot_20">28</ref> We find this quite surprising, especially considering the fact that we have not tuned the learning rate and warmup steps (see the next section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on the Number of GPUs</head><p>• For the fastest BLEU convergence use as many GPUs as available (in our experiments up to 8). • This holds even when there are more experiments to be done. For example, it is better to run one 8-GPUs experiment after another, rather than running two 4-GPUs experiments in parallel or eight single-GPU experiments in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Learning Rate and Warmup Steps on Multiple GPUs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.1.">Related Work</head><p>There is a growing number of papers on scaling deep learning to multiple machines with synchronous SGD (or its variants) by increasing the effective batch size. We will focus mostly on the question how to adapt the learning rate schedule, when scaling from one GPU (or any device, in general) to k GPUs. <ref type="bibr" target="#b13">Krizhevsky (2014)</ref> says "Theory suggests that when multiplying the batch size by k, one should multiply the learning rate by √ k to keep the variance in the gradient expectation constant", without actually explaining which theory suggests so. However, in the experimental part he reports that what worked the best, was a linear scaling heuristics, i.e. multiplying the learning rate by k, again without any explanation nor details on the difference between √ k scaling and k scaling. The linear scaling heuristics become popular, leading to good scaling results in practice <ref type="bibr" target="#b8">(Goyal et al., 2017;</ref><ref type="bibr">Smith et al., 2017)</ref> and also theoretical explanations <ref type="bibr" target="#b6">(Bottou et al., 2016;</ref><ref type="bibr">Smith and Le, 2017;</ref><ref type="bibr" target="#b11">Jastrzebski et al., 2017)</ref>. <ref type="bibr">Smith and Le (2017)</ref> interpret SGD (and its variants) as a stochastic differential equation and show that the gradient noise scale g = ϵ</p><formula xml:id="formula_0">( N B − 1 )</formula><p>, where ϵ is the learning rate, N is the training set size, and B is the effective batch size. This noise "drives SGD away from sharp minima, and therefore there is an optimal batch size which maximizes the test set accuracy". In other words for keeping the optimal level of gradient noise (which leads to "flat minima" that generalize well), we need to scale the learning rate linearly when increasing the effective batch size.</p><p>However, <ref type="bibr" target="#b9">Hoffer et al. (2017)</ref> suggest to use √ k scaling instead of the linear scaling and provide both theoretical and empirical support for this claim. They show that cov(∆w, ∆w) ∝ ϵ 2 NB , thus if we want to keep the the covariance matrix of the parameters update step ∆w in the same range for any effective batch size B, we need to scale the learning rate proportionally to the square root of B. They found that √ k scaling works better than linear scaling on CIFAR10. <ref type="foot" target="#foot_21">29</ref>  <ref type="bibr" target="#b24">You et al. (2017)</ref> confirm linear scaling does not perform well on ImageNet and suggest to use Layer-wise Adaptive Rate Scaling.</p><p>We can see that large-batch training is still an open research question. Most of the papers cited above have experimental support only from the image recognition tasks (usually ImageNet) and convolutional networks (e.g. ResNet), so it is not clear whether their suggestions can be applied also on sequence-to-sequence tasks (NMT) with self-attentional networks (Transformer). There are several other differences as well: Modern convolutional networks are usually trained with batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy, 2015)</ref>, which seems to be important for the scaling, while Transformer uses layer normalization <ref type="bibr" target="#b15">(Lei Ba et al., 2016)</ref>. <ref type="foot" target="#foot_22">30</ref> Also, Transformer uses Adam together with an inverse-square-root learning-rate decay, while most ImageNet papers use SGD with momentum and piecewise-constant learning-rate decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.2.">Our Experiments</head><p>We decided to find out empirically the optimal learning rate for training on 8 GPUs. Increasing the learning rate from 0.20 to 0.30 resulted in diverged training (BLEU dropped to almost 0 after two hours of training). Similarly to our single-GPU experiments (Section 4.6), we were able prevent the divergence by increasing the warmup steps or by introducing gradient clipping (e.g. with clip_grad_norm=1.0, we were able to use learning rate 0.40, but increasing it further to 0.60 led to divergence anyway). However, none of these experiments led to any improvements over the default learning rate -all had about the same BLEU curve after few hours of training. <ref type="bibr" target="#b11">Jastrzebski et al. (2017)</ref> shows that "the invariance under simultaneous rescaling of learning rate and batch size breaks down if the learning rate gets too large or the batch size gets too small". A similar observation was reported e.g. by <ref type="bibr" target="#b6">Bottou et al. (2016)</ref>. Thus our initial hypothesis was that 0.20 (or 0.25) is the maximal learning rate suitable for stable training in our experiments even when we scale from a single GPU to 8 GPUs. Considering this initial hypothesis, we were surprised that we were able to achieve so good Time Till Score with 8 GPUs (more than 8 times smaller relative to a single GPU, as reported in Table <ref type="table" target="#tab_8">6</ref>). To answer this riddle we need to understand how learning rate schedules are implemented in T2T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.3.">Parametrization of Learning Rate Schedules in T2T</head><p>In most works on learning rate schedules<ref type="foot" target="#foot_23">31</ref> the "time" parameter is actually interpreted as the number of epochs or training examples. For example a popular setup for piecewise-constant decay in ImageNet training (e.g. <ref type="bibr" target="#b8">Goyal et al., 2017)</ref> is to divide the learning rate by a factor of 10 at the 30-th, 60-th, and 80-th epoch.</p><p>However, in T2T, it is the global_step variable that is used as the "time" parameter. So when increasing the effective batch size 8 times, e.g. by using 8 GPUs instead of a single GPU, the actual learning rate 32 achieves a given value after the same number of steps, but this means after 8 times less training examples. For the inverse-square-root decay, we have actual_lr(steps) = c • steps −0.5 = 1 √ 8 • actual_lr(steps • 8), where c is a constant containing also the learning_rate parameter. So with 8 GPUs, if we divide the learning_rate parameter by √ 8, we achieve the same actual learning rate after a given number of training examples as in the original single-GPU setting.</p><p>This explains the riddle from the previous section. By keeping the learning_rate parameter the same when scaling to k times bigger effective batch, we actually increase the actual learning rate √ k times, in accordance with the suggestion of <ref type="bibr" target="#b9">Hoffer et al. (2017)</ref>.<ref type="foot" target="#foot_25">33</ref> This holds only for the linear_warmup_rsqrt_decay (aka noam) schedule and ignoring the warmup steps.</p><p>If we want to keep the same learning rate also in the warmup phase, we would need to divide the warmup steps by k. However, this means that the maximum actual learning rate will be √ k times higher, relative to the single-GPU maximal actual learning rate and this leads to divergence in our experiments. In deed, many researchers (e.g. <ref type="bibr" target="#b8">Goyal et al., 2017)</ref> suggest to use a warmup when scaling to more GPUs in order to prevent divergence. Transformer uses learning rate warmup by default even for single-GPU training (cf. Section 4.6), but it makes sense to use more warmup training examples in multi-GPU setting.</p><p>In our experiments with 8 GPUs and the default learning rate 0.20, using 8k warmup steps instead of the default 16k had no effect on the BLEU curve (it was a bit higher in the first few hours, but the same afterwards). Further decreasing the warmup steps resulted in a retarded BLEU curve (for 6k) or a complete divergence (for 2k).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on Learning Rate and Warmup Steps on Multiple GPUs</head><p>• Keep the learning_rate parameter at its optimal value found in single-GPU experiments. • You can try decreasing the warmup steps, but less than linearly and you should not expect to improve the final BLEU this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Resumed Training</head><p>T2T allows to resume training from a checkpoint, simply by pointing the output_dir parameter to a directory with an existing checkpoint (specified in the checkpoint file). This may be useful when the training fails (e.g. because of hardware error), when we need to continue training on a different machine or during hyper-parameter search, when we want to continue with the most promising setups. T2T saves also Adam momentum into the checkpoint, so the training continues almost as if it had not been stopped. However, it does not store the position in the training data -it starts from a random position. Also the relative time (and wall-clock time) in TensorBoard graphs will be influenced by the stopping.</p><p>Resumed training can also be exploited for changing some hyper-parameters, which cannot be meta-parametrized by the number of steps. For example, <ref type="bibr">Smith et al. (2017)</ref> suggest to increase the effective batch size (and number of GPUs) during training, instead of decaying the learning rate.</p><p>Yet another usage is to do domain adaptation by switching from (large) generaldomain training data to (small) target-domain training data for the few last epochs. In this case, consider editing also the learning rate or learning rate schedule (or faking the global_step stored in the checkpoint) to make sure the learning rate is not too small. <ref type="bibr" target="#b22">Vaswani et al. (2017)</ref> suggest to average the last 20 checkpoints saved in 10-minute intervals (using utils/avg_checkpoints.py). According to our experiments slightly better results are achieved with averaging checkpoints saved in 1-hour intervals. This has also the advantage that less time is spent with checkpoint saving, so the training is faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Checkpoint Averaging</head><p>Figure <ref type="figure" target="#fig_9">10</ref> shows the effect of averaging is twofold: the averaged curve has lower variance (flickering) from checkpoint to checkpoint and it is almost always better than the baseline without averaging (usually by about 0.2 BLEU). In some setups, we have seen improvements due to averaging over 1 BLEU. In the early phases of training, while the (baseline) learning curve grows fast, it is better to use fewer checkpoints for averaging. In later phases (as shown in Figure <ref type="figure" target="#fig_9">10</ref>, after 4.5-7.5 days of training), it seems that 16 checkpoints (covering last 16 hours) give slightly better results on average than 8 checkpoints, but we have not done any proper evaluation for significance (using paired bootstrap testing for each hour and then summarizing the results). The fact that resumed training starts from a random position in the training data (cf. Section 4.9) can be actually exploited for "forking" a training to get two (or more) copies of the model, which are trained for the same number of steps, but independently in the later stages and thus ending with different weights saved in the final checkpoint. These semi-independent models can be averaged in the same way as checkpoints from the same run, as described above. Our preliminary results show this helps a bit (on top of checkpoint averaging).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tips on Checkpoint Averaging</head><p>• Use it. Averaging 8 checkpoints takes about 5 minutes, so it is a "BLEU boost for free" (compared with the time needed for the whole training). • See the tools for automatic checkpoint averaging and evaluation described in Section 2.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Comparison with WMT17 Systems</head><p>Table <ref type="table" target="#tab_9">7</ref> provides the results of WMT17 English-to-Czech news translation task, with our best Transformer model (BIG trained on 8 GPUs for 8 days, averaging 8 checkpoints) evaluated using the exact same implementation of automatic metrics. While the automatic evaluation is not fully reliable (see e.g. the high BLEU score for CU-Chimera despite its lower manual rank), we see that the Transformer model out- performs the best system in BLEU, TER, CharacTER and BEER, despite it does not use any back-translated data, reranking with other models (e.g. right-to-left reranking) nor ensembling (as is the case of uedin-nmt and other systems). Note that our Transformer uses a subset of the constrained training data for WMT17, so the results are comparable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented a broad range of basic experiments with the Transformer model <ref type="bibr" target="#b22">(Vaswani et al., 2017)</ref> for English-to-Czech neural machine translation. While we limit our exploration to the more or less basic parameter settings, we believe this report can be useful for other researchers. In sum, experiments done for this article took about 4 years of GPU time.</p><p>Among other practical observations, we've seen that for the Transformer model, larger batch sizes lead not only to faster training but more importantly better translation quality. Given at least a day and a 11GB GPU for training, the larger setup (BIG) should be always preferred. The Transformer model and its implementation in Tensor2Tensor is also best fit for "intense training": using as many GPUs as possible and running experiments one after another should be preferred over running several single-GPU experiments concurrently.</p><p>The best performing model we obtained on 8 GPUs trained for 8 days has outperformed the WMT17 winner in a number of automatic metrics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Training data size effect. BLEU learning curves for our main training dataset with 58 million sentence pairs and an alternative training dataset with 16 million sentence pairs. Both trained with 8 GPUs, BIG model and batch_size=1500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>compares the BLEU learning curves of two experiments which differ only in the training data: the baseline CzEng 1.7 versus the smaller CzEng 1.0. Both are trained on the same hardware with the same hyper-parameters (8 GPUs, BIG, batch_size=1500). Training on the smaller dataset (2.5 times smaller in the number of words) converges to BLEU of about 25.5 after two days of training and does not improve over the next week of training. Training on the bigger dataset gives slightly worse results in the first eight hours of training (not shown in the graph) but clearly better results after two days of training, reaching over 26.5 BLEU after eight days. 18</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: Effect of model size and batch size on a single GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effect of restricting the training data to various max_length values. All trained on a single GPU with the BIG model and batch_size=1500. An experiment without any max_length is not shown, but it has the same curve as max_length=400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>•Figure 5 :</head><label>5</label><figDesc>Figure 5: Effect of the batch size with the BASE model. All trained on a single GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Effect of the batch size with the BIG model. All trained on a single GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Effect of the learning rate on a single GPU. All trained on CzEng 1.0 with the default batch size (1500) and warmup steps (16k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Effect of the warmup steps on a single GPU. All trained on CzEng 1.0 with the default batch size (1500) and learning rate (0.20).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure9: Effect of the number of GPUs. BLEU=25.6 is marked with a black line.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Effect of checkpoint averaging. All trained on 6 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Effective Batch Size is the number of training examples consumed in one training step. When training on multiple GPUs, the parameter batch_size is interpreted per GPU. That is, with batch_size=1500 and 8 GPUs, the system actually digests 12k subwords of each language in one step. Training Epoch corresponds to one complete pass over the training data. Unfortunately, it is not easy to measure the number of training epochs in T2T. 7 T2T reports only the number of training steps. In order to convert training steps to epochs, we need to multiply the steps by the effective batch size and divide by the number of subwords in the training data (see Section 3.1). The segmentation of the training data into subwords is usually hidden to the user and the number of subwords must be thus computed by a special script. Computation Speed is simply the observed number of training steps per hour. Computation speed obviously depends on the hardware (GPU speed, GPU-CPU communication) and software (driver version, CUDA library version, implementation). The main parameters affecting computation speed are the model size, optimizer and other settings that directly modify the formula of the neural network. Training Throughput is the amount of training data digested by the training. We report training throughput in subwords per hour. Training Throughput equals to the Computation Speed multiplied by the effective batch size. Convergence Speed or BLEU Convergence is the increase in BLEU divided by time.</figDesc><table /><note>6 This allows to use a higher number of short sentences in one batch or a smaller number of long sentences. Convergence speed changes heavily during training, starting very high and decreasing as the training progresses. A converged model should have convergence speed of zero. Time Till Score is the training time needed to achieve a certain level of translation quality, in our case BLEU. We use this as an informal measure because it is not clear how to define the moment of "achieving" a given BLEU score. We define it as time after which the BLEU never falls below the given level. 8 Examples Till Score is the number of training examples (in subwords) needed to achieve a certain level of BLEU. It equals to the Time Till Score multiplied by Training Throughput.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Training data resources</figDesc><table><row><cell></cell><cell cols="3">sentences EN words CS words</cell></row><row><cell>CzEng 1.7</cell><cell>57 M</cell><cell>618 M</cell><cell>543 M</cell></row><row><cell>europarl-v7</cell><cell>647 k</cell><cell>15 M</cell><cell>13 M</cell></row><row><cell>news-commentary-v11</cell><cell>190 k</cell><cell>4.1 M</cell><cell>3.7 M</cell></row><row><cell>commoncrawl</cell><cell>161 k</cell><cell>3.3 M</cell><cell>2.9 M</cell></row><row><cell>Total</cell><cell>58 M</cell><cell>640 M</cell><cell>563 M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>We can see that the computa-</figDesc><table><row><cell></cell><cell cols="2">model</cell><cell>model</cell></row><row><cell cols="2">batch_size BASE</cell><cell>BIG</cell><cell>batch_size BASE</cell><cell>BIG</cell></row><row><cell cols="3">500 43.4k 23.6k</cell><cell cols="2">500 21.7M 11.9M</cell></row><row><cell cols="3">1000 30.2k 13.5k</cell><cell cols="2">1000 30.2M 13.5M</cell></row><row><cell cols="2">1500 22.3k</cell><cell>9.8k</cell><cell cols="2">1500 33.4M 14.7M</cell></row><row><cell cols="2">2000 16.8k</cell><cell>7.5k</cell><cell cols="2">2000 33.7M 15.0M</cell></row><row><cell cols="2">2500 14.4k</cell><cell>6.5k</cell><cell cols="2">2500 36.0M 16.2M</cell></row><row><cell cols="3">3000 12.3k OOM</cell><cell cols="2">3000 37.0M OOM</cell></row><row><cell>4500</cell><cell cols="2">8.2k OOM</cell><cell cols="2">4500 36.7M OOM</cell></row><row><cell>6000</cell><cell cols="2">6.6k OOM</cell><cell cols="2">6000 39.4M OOM</cell></row><row><cell cols="3">(a) Computation speed (steps/hour)</cell><cell cols="2">(b) Training throughput (subwords/hour)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Computation speed and training throughput for a single GPU.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Computation speed and training throughput for various numbers of GPUs, with the BIG model and batch_size=1500.</figDesc><table><row><cell cols="3">GPUs steps/hour subwords/hour</cell></row><row><cell>1</cell><cell>9.8k</cell><cell>14.7M</cell></row><row><cell>2</cell><cell>7.4k</cell><cell>22.2M</cell></row><row><cell>6</cell><cell>5.4k</cell><cell>48.6M</cell></row><row><cell>8</cell><cell>5.6k</cell><cell>67.2M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>.</figDesc><table><row><cell cols="5">model hidden_size filter_size num_heads adam_beta2</cell></row><row><cell>BASE</cell><cell>512</cell><cell>2048</cell><cell>8</cell><cell>0.980</cell></row><row><cell>BIG</cell><cell>1024</cell><cell>4096</cell><cell>16</cell><cell>0.998</cell></row></table><note>transformer_big_single_gpu (BIG) and transformer_base_single_gpu (BASE) hyper-parameter differences.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>confirms this with 8 GPUs -here BIG with batch size 1500 becomes clearly better than BASE with batch size 4500 after 18 hours of training. Prefer the BIG over the BASE model if you plan to train longer than one day and have 11 GB (or more) memory available on GPU. • With less memory you should benchmark BIG and BASE with the maximum possible batch size. Maximum batch size which fits into 11GB memory for various combinations of max_length (maximum sentence length in subwords), model size (base or big) and optimizer (Adam or Adafactor). The last two columns show the percentage of sentences in the train (CzEng 1.7) and test (wmt13) data that are longer than a given threshold.</figDesc><table><row><cell>Tips on Model Size</cell></row><row><cell>• 54</cell></row><row><cell>Unauthenticated</cell></row><row><cell>Download Date | 11/13/18 8:36 AM</cell></row></table><note>• For fast debugging (of model-size-unrelated aspects) use a model called trans-former_tiny.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Table 6 lists the approximate training time and number of training examples (in millions of subwords) needed to "surpass" (i.e. achieve and never again fall below) BLEU of 25.6. Time and training data consumed to reach BLEU of 25.6, i.e. Time Till Score and Examples Till Score. Note that the experiment on 1 GPU was ended after 25 days of training without clearly surpassing the threshold (already outside of Figure9).</figDesc><table><row><cell cols="3"># GPUs hours subwords (M)</cell></row><row><cell cols="2">1 &gt; 600</cell><cell>&gt; 9000</cell></row><row><cell>2</cell><cell cols="2">203 2322•2 = 4644</cell></row><row><cell>6</cell><cell>56</cell><cell>451•6 = 2706</cell></row><row><cell>8</cell><cell>40</cell><cell>341•8 = 2728</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>WMT17 systems for English-to-Czech and our best T2T training run. Manual scores are from the official WMT17 ranking. Automatic metrics were provided by http://matrix.statmt.org/. For *TER metrics, lower is better. Best results in bold, second-best in italics.</figDesc><table><row><cell></cell><cell>Manual</cell><cell></cell><cell></cell><cell cols="2">Automatic Scores</cell><cell></cell><cell></cell></row><row><cell cols="2"># Ave %</cell><cell>Ave z</cell><cell>BLEU</cell><cell>TER</cell><cell cols="2">CharacTER BEER</cell><cell>System</cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>23.8</cell><cell>0.662</cell><cell>0.582</cell><cell>0.543</cell><cell>T2T 8 GPUs 8 days</cell></row><row><cell>1</cell><cell>62.0</cell><cell>0.308</cell><cell>22.8</cell><cell>0.667</cell><cell>0.588</cell><cell>0.540</cell><cell>uedin-nmt</cell></row><row><cell>2</cell><cell>59.7</cell><cell>0.240</cell><cell>20.1</cell><cell>0.703</cell><cell>0.612</cell><cell>0.519</cell><cell>online-B</cell></row><row><cell>3</cell><cell>55.9</cell><cell>0.111</cell><cell>20.2</cell><cell>0.696</cell><cell>0.607</cell><cell>0.524</cell><cell>limsi-factored</cell></row><row><cell></cell><cell>55.2</cell><cell>0.102</cell><cell>20.0</cell><cell>0.699</cell><cell>-</cell><cell>-</cell><cell>LIUM-FNMT</cell></row><row><cell></cell><cell>55.2</cell><cell>0.090</cell><cell>20.2</cell><cell>0.701</cell><cell>0.605</cell><cell>0.522</cell><cell>LIUM-NMT</cell></row><row><cell></cell><cell>54.1</cell><cell>0.050</cell><cell>20.5</cell><cell>0.696</cell><cell>0.624</cell><cell>0.523</cell><cell>CU-Chimera</cell></row><row><cell></cell><cell>53.3</cell><cell>0.029</cell><cell>16.6</cell><cell>0.743</cell><cell>0.637</cell><cell>0.503</cell><cell>online-A</cell></row><row><cell>8</cell><cell>41.9</cell><cell>-0.327</cell><cell>16.2</cell><cell>0.757</cell><cell>0.697</cell><cell>0.485</cell><cell>PJATK</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0">Even if we fix the random seed (which was not done properly in T2T v1.2.9), a change of some hyperparameters may affect the results not because of the change itself, but because it influenced the random initialization.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1">5 By overfitting we mean here that the translation quality (test-set BLEU) begins to worsen, while the training loss keeps improving.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2">Unauthenticated Download Date | 11/13/18 8:36 AM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3">For this purpose, the number of tokens in a sentence is defined as the maximum of source and target subwords. T2T also does reordering and bucketing of the sentences by their length to minimize the use of padding symbols. However, some padding is still needed, thus batch_size only approximates the actual number of (non-padding) subwords in a batch.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4">https://github.com/tensorflow/tensor2tensor/issues/415</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5">Such definition of Time Till Score leads to a high variance of its values because of the relatively high BLEU variance between subsequent checkpoints (visible as a "flickering" of the learning curves in the figures). To decrease the variation one can use a bigger development test set.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_6">More details on T2T with BPE subword units by<ref type="bibr" target="#b18">Sennrich et al. (2016)</ref> vs. the internal implementation can be found in the technical report "Morphological and Language-Agnostic Word Segmentation for NMT" attached to the Deliverable 2.3 of the project QT21: http://www.qt21.eu/resources/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_7">This is controlled by a file_byte_budget constant, which must be changed directly in the source code in T2T v1.2.9. A sign of too small training data for the subword vocabulary is that the min_count as reported in the logs is too low, so the vocabulary is estimated from words seen only once or twice.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_8">We did no such pre-filtering in our experiments.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_9">According to our experiments (not reported here), transformer_big_single_gpu is better than trans-former_big even when training on 8 GPUs, although the naming suggests that the T2T authors had an opposite experience.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="15" xml:id="foot_10">15 Also there are some problems with the alternative schedules train_and_evaluate (it needs more memory) and continuous_train_and_eval (see https://github.com/tensorflow/tensor2tensor/issues/556</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_11">).16 TensorBoard shows global_step/sec statistics, i.e. the computation speed curve. These curves in our experiments are almost constant for the whole training with variation within 2%, except for moments when a checkpoint is being saved (and the computation speed is thus much</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_12">slower).17  For these experiments, we used max_length=50 in order to be able to test bigger batch sizes. However, in additional experiments we checked that max_length does not affect the training throughput itself.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_13">We compared the two datasets also in another experiment with two GPUs, where CzEng 1.7 gave slightly worse results than CzEng 1.0 during the first two days of training but clearly better results after eight days. We hypothesize CzEng 1.0 is somewhat cleaner than CzEng 1.7.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_14">Although such an expectation may seem naïve, we can find it in literature. For example,<ref type="bibr" target="#b5">Bottou (2012)</ref> in Section 4.2 writes: "Expect the validation performance to plateau after a number of epochs roughly comparable to the number of epochs needed to reach this point on the small training set."</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_15">We tried also a model three times as large as BASE (1.5 times as large as BIG), but it did not reach better results than BIG, so we don't report it here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_16">The Adafactor optimizer<ref type="bibr" target="#b19">(Shazeer and Stern, 2018)</ref> is available only in T2T 1.4.2 or newer and has three times smaller models than Adam because it does not store first and second moments for all weights. We leave further experiments with Adafactor for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_17">https://github.com/tensorflow/tensor2tensor/issues/582</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="26" xml:id="foot_18">This holds at least in T2T versions 1.2.9-1.5.2, but as it is somewhat unexpected/unintuitive for some users, it may be fixed in future, see https://github.com/tensorflow/tensor2tensor/issues/517.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="27" xml:id="foot_19">and making sure environment variable CUDA_VISIBLE_DEVICES is set so enough cards are visible. T2T allows also distributed training (on multiple machines), but we have not experimented with it. Both singlemachine multi-gpu and distributed training use synchronous Adam updates by default.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="28" xml:id="foot_20">It would be interesting to try simulating multi-GPU training on a single GPU, simply by doing the update once after N batches (and summing the gradients). This is similar to the ghost batches of<ref type="bibr" target="#b9">Hoffer et al. (2017)</ref>, but using ghost batch size higher than the actual batch size. We leave this for future work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="29" xml:id="foot_21">To close the gap between small-batch training and large-batch training, Hoffer et al. (2017) introduce (in addition to √ k scaling) so-called ghost batch normalization and adapted training regime, which means decaying the learning rate after a given number of steps instead of epochs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="30" xml:id="foot_22">Applying batch normalization on RNN is difficult. Transformer does not use RNN, but still we were not successful in switching to batch normalization (and possibly ghost batch normalization) due to NaN loss errors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="31" xml:id="foot_23">Examples of learning rate schedules are inverse-square-root decay, inverse-time decay, exponential decay, piecewise-constant decay, see https://www.tensorflow.org/api_guides/python/train#Decaying_ the_learning_rate for TF implementations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32" xml:id="foot_24">By actual learning rate we mean the learning rate after applying the decay schedule. The learning_rate parameter stays the same in this case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="33" xml:id="foot_25">In addition to suggesting the √ k learning-rate scaling,<ref type="bibr" target="#b9">Hoffer et al. (2017)</ref> show that to fully close the "generalization gap", we need to train longer because the absolute number of steps (updates) matters. So from this point of view, using steps instead of epochs as the time parameter for learning rate schedules may not be a completely wrong idea.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research was supported by the grants 18-24210S of the Czech Science Foundation, H2020-ICT-2014-1-645452 (QT21) of the EU, SVV 260 453, and using language resources distributed by the LINDAT/CLARIN project of the Ministry of Education, Youth and Sports of the Czech Republic (LM2015071).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ELRA, European Language Resources Association</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdeněk</forename><surname>Žabokrtský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Galuščáková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Majliš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mareček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiří</forename><surname>Maršík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleš</forename><surname>Tamchyna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Language Resources and Evaluation Conference (LREC&apos;12)</title>
				<meeting>the Eighth International Language Resources and Evaluation Conference (LREC&apos;12)<address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-05">May 2012</date>
			<biblScope unit="page" from="3921" to="3928" />
		</imprint>
	</monogr>
	<note>The Joy of Parallelism with CzEng 1.0</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CzEng 1.6: Enlarged Czech-English Parallel Corpus with Processing Tools Dockered</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kocmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jindřich</forename><surname>Libovický</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Novák</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Popel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roman</forename><surname>Sudarikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dušan</forename><surname>Variš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text, Speech, and Dialogue: 19th International Conference, TSD 2016, number 9924 in Lecture Notes in Artificial Intelligence</title>
				<editor>
			<persName><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aleš</forename><surname>Horák</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ivan</forename><surname>Kopeček</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Karel</forename><surname>Pala</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
		<respStmt>
			<orgName>Masaryk University</orgName>
		</respStmt>
	</monogr>
	<note>Unauthenticated Download Date | 11/13/18 8:36 AM editors</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Findings of the 2017 Conference on Machine Translation (WMT17)</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Monz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Negri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Rubino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Turchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">September 2017a. ACL</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Results of the WMT17 Metrics Shared Task</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Bojar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yvette</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Kamran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Machine Translation</title>
				<meeting>the Second Conference on Machine Translation<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">September 2017b. ACL</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Stochastic Gradient Descent Tricks</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-35289-8_25</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-35289-8_25" />
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="421" to="436" />
			<pubPlace>Berlin Heidelberg; Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Optimization Methods for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1606.04838" />
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the IWSLT 2017 Evaluation Campaign</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Stüker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katsuhito</forename><surname>Sudoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koichiro</forename><surname>Yoshino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Federmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Spoken Language Translation (IWSLT)</title>
				<meeting>the 14th International Workshop on Spoken Language Translation (IWSLT)<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour. CoRR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry ; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1731" to="1741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR, abs/1502.03167</idno>
		<ptr target="http://arxiv.org/abs/1502.03167" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Three Factors Influencing Minima in SGD</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Kenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amos</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
		<idno>CoRR, abs/1711.04623</idno>
		<ptr target="http://arxiv.org/abs/1711.04623" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dheevatsa</forename><surname>Shirish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Tak</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1609.04836" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
				<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">One weird trick for parallelizing convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno>CoRR, abs/1404.5997</idno>
		<ptr target="http://arxiv.org/abs/1404.5997" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fully Character-Level Neural Machine Translation without Explicit Segmentation</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1610.03017.69" />
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer Normalization. ArXiv e-prints</title>
				<imprint>
			<date type="published" when="2016-07">July 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BLEU: a Method for Automatic Evaluation of Machine Translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2002</title>
				<meeting>ACL 2002<address><addrLine>Philadelphia, Pennsylvania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">chrF: character n-gram F-score for automatic MT evaluation</title>
		<author>
			<persName><forename type="first">Maja</forename><surname>Popović</surname></persName>
		</author>
		<ptr target="http://aclweb.org/anthology/W15-3049" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth Workshop on Statistical Machine Translation</title>
				<meeting>the Tenth Workshop on Statistical Machine Translation<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09">September 2015</date>
			<biblScope unit="page" from="392" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2016</title>
				<meeting>ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</title>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stern</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1804.04235" />
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Bayesian Perspective on Generalization and Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.06451" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Second workshop on Bayesian Deep Learning</title>
				<meeting>Second workshop on Bayesian Deep Learning<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Don&apos;t Decay the Learning Rate, Increase the Batch Size</title>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Pieter-Jan</surname></persName>
		</author>
		<author>
			<persName><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1711.00489" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin ; Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/1609.08144</idno>
		<ptr target="http://arxiv.org/abs/1609.08144" />
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Greg Corrado, Macduff Hughes, and Jeffrey Dean</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Scaling SGD Batch Size to 32K for ImageNet Training</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno>CoRR, abs/1708.03888</idno>
		<ptr target="http://arxiv.org/abs/1708.03888" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
