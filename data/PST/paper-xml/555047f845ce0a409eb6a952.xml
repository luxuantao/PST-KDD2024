<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NUMA-Aware Graph-Structured Analytics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaiyuan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Key Laboratory of Scalable Computing and Systems Institute of Parallel and Distributed Systems</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<address>
									<addrLine>PPoPP&apos;15, February 7-11</addrLine>
									<postCode>2015</postCode>
									<settlement>San Francisco</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NUMA-Aware Graph-Structured Analytics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">800129906DCB8CDC973B87C8FD70D60A</idno>
					<idno type="DOI">978-1-4503-3205-7/15/02...$15.00.10.1145/nnnnnnn.nnnnnnn</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T08:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>D</term>
					<term>1</term>
					<term>3 [Programming Techniques]: Concurrent Programming-Distributed programming Graph-structured Analytics; Non-uniform Memory Access (NUMA)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-structured analytics has been widely adopted in a number of big data applications such as social computation, web-search and recommendation systems. Though much prior research focuses on scaling graph-analytics on distributed environments, the strong desire on performance per core, dollar and joule has generated considerable interests of processing large-scale graphs on a single server-class machine, which may have several terabytes of RAM and 80 or more cores. However, prior graph-analytics systems are largely neutral to NUMA characteristics and thus have suboptimal performance.</p><p>This paper presents a detailed study of NUMA characteristics and their impact on the efficiency of graph-analytics. Our study uncovers two insights: 1) either random or interleaved allocation of graph data will significantly hamper data locality and parallelism; 2) sequential inter-node (i.e., remote) memory accesses have much higher bandwidth than both intra-and inter-node random ones. Based on them, this paper describes Polymer, a NUMAaware graph-analytics system on multicore with two key design decisions. First, Polymer differentially allocates and places topology data, application-defined data and mutable runtime states of a graph system according to their access patterns to minimize remote accesses. Second, for some remaining random accesses, Polymer carefully converts random remote accesses into sequential remote accesses, by using lightweight replication of vertices across NUMA nodes. To improve load balance and vertex convergence, Polymer is further built with a hierarchical barrier to boost parallelism and locality, an edge-oriented balanced partitioning for skewed graphs, and adaptive data structures according to the proportion of active vertices. A detailed evaluation on an 80-core machine shows that Polymer often outperforms the state-of-the-art single-machine graph-analytics systems, including Ligra, X-Stream and Galois, for a set of popular real-world and synthetic graphs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many machine learning, data mining and scientific computation can be modeled as graph-structured computation, resulting in a new application domain called graph analytics. Nowadays, it has been widely adopted in areas including social computation, web search, natural language processing, and recommendation systems <ref type="bibr" target="#b5">[7,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b45">47]</ref>. The strong desire for efficiency has driven the design of a number of distributed graph analytics frameworks such as Pregel <ref type="bibr" target="#b33">[35]</ref>, GraphLab <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b30">32]</ref> and Cyclops <ref type="bibr" target="#b11">[13]</ref>.</p><p>The multicore evolution has led to drastic increases in CPU core counts and memory sizes. Actually it is now not uncommon to see server-class machines with 80 or more cores and several terabyte of memory. This has led to the design and implementation of several recent single-machine graph-analytics systems to achieve more performance per core, dollar and joule, due to the significantly cheaper cost and lower latency in communication. Examples include GraphChi <ref type="bibr" target="#b26">[28]</ref>, Ligra <ref type="bibr" target="#b41">[43]</ref>, X-Stream <ref type="bibr" target="#b40">[42]</ref>, and Galois <ref type="bibr" target="#b37">[39]</ref>, which can usually process graphs on a single machine with hundreds of billions of edges.</p><p>On the other hand, commodity multicore machines have shifted into cache-coherent NUMA (cc-NUMA) architectures, where the latency of memory accesses on remote chips is much higher than that on local ones <ref type="bibr" target="#b16">[18]</ref>. Though the NUMA effects may have significant impact on the efficiency of graph analytics, existing systems are largely NUMA-oblivious but focus on other aspects such as improving out-of-core accesses <ref type="bibr" target="#b26">[28]</ref>, selecting appropriate execution modes <ref type="bibr" target="#b41">[43]</ref>, supporting sophisticated task scheduler <ref type="bibr" target="#b37">[39]</ref>, and reducing random operations on edges <ref type="bibr" target="#b40">[42]</ref>.</p><p>In this paper, we make a comprehensive study on the characteristics of commodity NUMA machines and how they affect the efficiency of existing single-machine graph analytics systems. Though conventional wisdom is that remote memory accesses have higher latency and lower throughput than local ones, we quantitatively show that sequential remote accesses have much higher bandwidth than both random local and random remote ones (2.92X and 6.85X on our tested machines). Further, either interleaved or centralized allocation of graph data on existing graph analytics systems causes poor data locality and limited parallelism.</p><p>Based on the above observations, this paper describes Polymer 1 , a NUMA-aware graph-analytics system that inherits the scattergather programming interface from Ligra <ref type="bibr" target="#b41">[43]</ref> but is built with several NUMA-aware designs. The key of Polymer is to minimize both random and remote memory accesses by optimizing graph data layout and access strategies.</p><p>Polymer adopts a general design principle for NUMA machines by co-locating graph data and computation within NUMA-nodes as much as possible, with the goal of reducing remote memory accesses and balancing cross-node interconnect bandwidth. Specifi- 1 The source code and a brief instruction of how to use Polymer are at http://ipads.se.sjtu.edu.cn/projects/polymer.html Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. cally, Polymer differentially allocates and places graph data according to their access patterns. For graph topology data such as vertices and edges that are always accessed only their own threads, Polymer uses corporative allocation by letting an accessing thread to allocate the memory in its local memory node, and thus eliminating remote accesses. Second, for application-defined data (like the ranks of a web page in PageRank) whose memory locations are static but data will be updated dynamically, though corporative allocation may eliminate a lot of remote accesses, there are still inevitable remote accesses due to frequent exchanges of application-defined data during computation. Hence, Polymer allocates such data with contiguous virtual addresses, but distributes actual physical memory frames to the NUMA-node of the owning thread. This makes it seamless to access cross-node data. For mutable graph runtime states such as the current active vertices, as they are dynamically allocated in each iteration, Polymer allocates and updates such data in a distributed way but accesses it through a global lookup table to avoid contention.</p><p>Graph analytics has been long recognized to have many random access and poor data locality <ref type="bibr" target="#b31">[33]</ref>. Based on the observation that sequential inter-node (i.e., remote) memory accesses have much higher bandwidth than both intra-and inter-node random ones, Polymer borrows the idea from distributed graph systems <ref type="bibr" target="#b20">[22]</ref> by replicating vertex data across NUMA-nodes in a lightweight way. (This essentially follows the philosophy of current multicore OS designs by treating a large-scale machine as a distributed system <ref type="bibr" target="#b3">[5]</ref>). Specifically, a vertex only conducts computation on edges within the local NUMA-node and uses its replicas in other NUMA-node to initiate the computation on other edges. Unlike distributed graph systems, Polymer does not distribute applicationdefined data but still applies all updates to a vertex within a single copy of application-defined data.</p><p>Polymer has three optimizations to improve scheduling on NUMA machines and handle different properties of graphs. First, being aware of the hierarchical parallelism and locality in NUMA machines, Polymer is extended with a hierarchical scheduler to reduce the cost for synchronizing threads on multiple NUMA-nodes. Second, inspired by vertex-cuts <ref type="bibr" target="#b20">[22]</ref> from distributed graph systems, Polymer improves the balance of workload among NUMAnodes by evenly partitioning edges rather than vertices for skewed graphs <ref type="bibr" target="#b18">[20]</ref>. Finally, as most graph algorithms converge asymmetrically, using a single data structure for runtime states may drastically degrade the performance, especially for traversal algorithms on high-diameter graphs, Polymer adopts adaptive data structures boost performance.</p><p>We have implemented Polymer and several typical graph applications, which comprise about 5,300 lines of C++ code. Our evaluation on both an 80-core Intel machine and a 64-core AMD machine with a number of graph algorithms using both real-world and synthetic graph datasets shows that Polymer often outperforms existing state-of-the-art graph-parallel systems and scales well in terms of the number of sockets, due to its graph-aware data layout, NUMAaware computation, reduced synchronization and traversal cost, and improved load balance.</p><p>This paper makes the following contributions:</p><p>• A comprehensive analysis that uncovers several NUMA characteristics and issues with existing NUMA-oblivious graph analytics systems (Section 3).</p><p>• The Polymer system that exploits both NUMA-and graphaware data layout and memory access strategies (Section 4)</p><p>• Three optimizations that improve global synchronization efficiency, load balance and data structure flexibility (Section 5).</p><p>• A detailed evaluation that demonstrates the performance and scalability benefit of Polymer (Section 6). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background 2.1 Graph Analytics</head><p>In-memory data structure: It mainly consists of three parts: graph topology data, application-defined data and graph runtime states. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, existing systems commonly split graph topology into separately ordered arrays for vertices and edges. The array for in-edges of all vertices are partitioned by their target vertex and storing the source vertices. Similarly, the array for out-edges of all vertices are partitioned by their target vertex and storing the source vertices. Both of them are optional. The metadata of all vertices are kept in one array, in which each one stores the start of its in-edge and out-edge partitions, and also maintains the in-degree and out-degree. Two arrays store two versions of the application-defined vertex data separately. One (i.e., current) maintains the value computed in the previous iteration, and the other (i.e., next) keeps the update value computed in current execution. The application-defined edge data can be handled similarly if necessary. The optional runtime states, i.e., active or not, of vertex for current and next execution are also stored in two arrays respectively to support dynamic computation <ref type="bibr" target="#b30">[32]</ref>.</p><p>Graph computation: Existing graph analytics systems follow a scatter-gather iterative computation model <ref type="bibr" target="#b20">[22]</ref>, which abstracts the computation in each iteration as consecutive scatter and gather phases. The scatter phase propagates the current value of a vertex to its neighbors along edges, while the gather phase accumulates values from neighbors to compute the next value of a vertex. There are two main approaches to implementing the scatter-gather model, namely vertex-centric <ref type="bibr" target="#b33">[35]</ref> and edge-centric <ref type="bibr" target="#b40">[42]</ref>.</p><p>The vertex-centric system such as Ligra <ref type="bibr" target="#b41">[43]</ref> iterates over all active vertices in both scatter and gather phases. The propagation of vertex data in scatter phase can be implemented in either push or pull mode. The left part of Ligra system in Figure <ref type="figure" target="#fig_1">2</ref> illustrates the execution flow and in-memory data access patterns under the two modes for vertex 3.</p><p>In the push mode, the worker thread first scans the current state array (SEQ|R) to identify an active vertex in vertices array (SEQ|R), and then obtains its neighbors through the out-edges array (SEQ|R). Further, the worker thread pushes the value of the active vertex in the current data array (SEQ|R) to its neighbors in the next data array (RAND|W), and sets the next state array (RAND|W). For example, the value of vertex 3 will be pushed to its neighboring vertex 2, 5 and 6 along out-edges, which are labeled bold arrow on the sample graph.</p><p>In the pull mode, for each vertex in the vertices array (SEQ|R), the worker thread first obtains its active neighbors through the in-edges (SEQ|R) and current state arrays (RAND|R). Further, the worker thread pulls the value from active neighbors in the  current data array (RAND|R) to update the vertex in the next data array (SEQ|W) and set the next state array (SEQ|W). For example, the vertex 3 will pull the value of its neighboring vertex 1, 2, 4 and 5 along in-edges to update its value. The inactive vertex 2 will be skipped.</p><formula xml:id="formula_0">1 0 1 1 1 0 X X 1 X X X<label>1</label></formula><p>The edge-centric system such X-Stream <ref type="bibr" target="#b40">[42]</ref> iterates over all edges rather than vertices to avoid random accesses to edges, instead of sequentially accessing them. Further, it introduces an additional shuffle phase and "tiling strategy" to improve random access to vertices. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, all edges are split by their source vertex into two partitions and the engine processes one partition associated with their source vertices at a time. In the scatter phase, the worker thread iterates over edges array (SEQ|R) and appends the update from the current data array (RAND|R) to a list Uout (SEQ|W) for active edges, which are identified by the state of its source vertex from the current state array (RAND|R). In the shuffle phase, all updates in the list Uout (SEQ|R) are re-arranged to the update list Uin (SEQ|W) by their target vertex. Finally, all updates will be applied to the next data array (RAND|W) in the gather phase, and the next state array (RAND|W) will be set simultaneously. For example, in the second partition, the update along the edge from vertex 4 to vertex 3 will be appended to local Uout first and then shuffled to the Uin of partition 1, and finally written to vertex 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NUMA Characteristics</head><p>A commodity NUMA machine consists of several processor nodes (i.e., socket), each of which contains multiple cores and a local DRAM. The nodes are connected by the high-speed interconnect into a cache-coherent system, forming a globally shared memory abstraction to applications.</p><p>The distribution of memory to processor nodes leads to nonuniform memory access: the latency of accessing locally-attached cache and memory is significantly lower than those attached to other processor. Further, the latency highly depends on the distance (i.e., hops) between nodes.</p><p>To make a quantitative study on the non-uniform feature of commodity NUMA machines, we measure the latency of memory accesses along with distance on both an 80-core (8 sockets X 10 cores) Intel machine and a 64-core (4 sockets X 2 dies X 8 cores) AMD machine. The dies within a socket have a 1-hop distance. The topologies of the two machines are depicted in Figure <ref type="figure" target="#fig_2">3</ref>(a) <ref type="foot" target="#foot_0">2</ref> .</p><p>As shown in Figure <ref type="figure" target="#fig_2">3</ref>(b), for the 80-core machine, the latency of load and store on the same node through memory are 117 and 108 cycles respectively, whereas an access over one or two hop(s) is approximately 2 or 3 times more expensive than an access on the same node accordingly<ref type="foot" target="#foot_1">3</ref> . The 64-core machine has a close trend.</p><p>Figure <ref type="figure">4</ref> shows the bandwidth of memory access along with distances on the 80-core machine <ref type="foot" target="#foot_2">4</ref> . The bandwidth of remote accesses is unsurprisingly quite lower than that of local access, up to 34% and 57% performance degradation for sequential and random access over two hops respectively. Further, the bandwidth of interleaved access, where the maximum distance between the two cores is 2 hops, is even lower than that of remote access with 1-hop distance. The results on 64-core AMD machine are similar, and the two values for 1-hop distance indicate within a socket or not.</p><p>Further, it is a little bit surprising that sequential remote accesses, have much higher throughput than random ones (both local and remote), by factors from 2.92X to 6.85X with increasing distance on 80-core Intel machine (see Figure <ref type="figure">4</ref>). For example, random local accesses only have 720 MB/s throughput, which is far smaller than sequential accesses with 2-hop distance (2101 MB/s). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Challenges and Issues</head><p>This section discusses why prior single-machine graph-analytics systems fall shorts on NUMA platforms, by attributing them to two issues: data layout and access strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Issue 1: Data Layout</head><p>By default, Linux employs the "first-touch" policy to bind virtual pages to physical frames locating on a memory node where a thread first touches the pages. Graph analytics usually consists of two stages: graph construction and computation. In existing frameworks, the long-term in-memory data structures, including both graph topology and application-defined data, are allocated and initialized by multiple constructing threads on different NUMA-nodes during the construction stage. Such data is then accessed by graph processing threads bound to such NUMA-nodes at the computation stage. Under the default allocation policy, namely "first-touch", there is mismatch between allocation threads and processing threads, which forms interleaved page allocation. This notably hurts locality and degrade performance due to a large number of remote and random memory accesses. As shown in Section 2.2, both remote random memory accesses have much higher latency and throughput than sequential and random ones.</p><p>Further, the short-term in-memory data structures, such as runtime states, are allocated and initiated by the main thread at the beginning of each iteration, and accessed by all processing threads from NUMA-nodes in each iteration. Such centralized allocation will result in not only excessive remote memory accesses, but also congestion on interconnects and memory controllers <ref type="bibr" target="#b15">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Issue 2: Access Strategy</head><p>For graph analytics, it is inevitable to access remote memory even under ideal data layout, due to the lack of access locality when traversing edges. However, as we observed in Section 2.2, sequential remote accesses have much higher bandwidth than both random local and random remote accesses. Yet, the access pattern (i.e., sequential or random) for the remote memory access is either completely overlooked or restricted in an inefficient way, leading to suboptimal performance and scalability by prior work.</p><p>In vertex-centric system such as Ligra <ref type="bibr" target="#b41">[43]</ref>, even if the local accesses to in-memory data structures have been carefully arranged in a sequential order, the remote accesses to next or curr data and state arrays for push or pull mode are coupled with random orders accordingly. Worse even, concurrent remote accesses by threads bound on different processors will further cause congestion on interconnect.</p><p>The right part of Ligra system in Figure <ref type="figure" target="#fig_1">2</ref> shows the execution on a 2-node NUMA machine using push and pull modes. Note that each operation is labeled with access pattern, and local(L) or global(G) symbol associated for the NUMA case. In the push mode, the worker thread randomly pushes the value of the vertex 3 to its neighbors allocated on remote memory nodes (e.g., vertex 5 and 6), and also randomly writes the state array for these neighbors. They are labeled as random, write and global (i.e., RAND|W|G). In the pull mode, the worker thread randomly pulls the value from active neighbors allocated on remote memory nodes (e.g. vertex 4 and 5) to update vertex 3. They are labeled as random, read and global (i.e., RAND|R|G).</p><p>In edge-centric system such as X-Stream <ref type="bibr" target="#b40">[42]</ref>, based on "tiling strategy" <ref type="bibr" target="#b8">[10]</ref>, each core can independently process one partition in both the gather and the scatter phases, and only exchanges data among all of memory nodes in the shuffle phase. Even if the remote access is sequential, the shuffle phase causes additional memory allocation and remote copy overhead. Worse even, it is quite costly to identify the state of edges instead of vertices, since the number of edges is commonly a few tens or hundreds of times than the number of vertices. It will significantly degrade performance and scalability for traversal algorithms (e.g., SSSP), especially for high-diameter graphs like road networks (see Table <ref type="table" target="#tab_4">3</ref>).</p><p>The X-Stream system in Figure <ref type="figure" target="#fig_1">2</ref> also illustrates the execution on a 2-node NUMA machine. The updates to target vertices in a set of Uouts allocated on different memory node are shuffled to different Uins by their target vertices. It is labeled as sequential, write and global (i.e., SEQ|W|G). For example, all updates on vertex 3 will be grouped and eventually updated to the next data array on the same node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitive Performance and Scalability on NUMA</head><p>We study the scalability of Ligra, Galois and X-Stream using PageRank algorithm for Twitter follower graph from two dimensions. We increase the number of cores within one socket first and then increase the number of sockets with fixed 10 cores per socket on our 80-core machine <ref type="foot" target="#foot_3">5</ref> .</p><p>Figure <ref type="figure" target="#fig_3">5</ref>(a) shows that existing systems can scale well in terms of number of cores, up to 6.92X using 8 cores on X-Stream. However, none of them has a very good scalability in terms of number of sockets, because of inefficient data layout and access strategy on NUMA machines. As shown in Figure <ref type="figure" target="#fig_3">5</ref>(b) and (c), X-Stream has a slightly better scalability (4.58X) but the worst performance due to additional time-consuming shuffling. On the contrary, Galois has currently best performance due to fully optimized infrastructure, while its scalability is very poor (2.90X on 8 sockets). We also reran the experiment on our 64-core AMD machine, and gained a worse scalability (see Figure <ref type="figure" target="#fig_3">5(d)</ref>). The performance of X-Stream and Galois even degrades when the number of sockets exceeds 4, since the HyperTransport interconnect in AMD systems can only ensure the distance between two nodes to one hop for at most 4 sockets. In short, existing systems are still largely NUMA-oblivious, resulting in less optimal performance and scalability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">NUMA-Aware Graph-Analytics</head><p>This section describes Polymer, a NUMA-aware graph analytics system, which provides a vertex-centric programming interface.</p><p>The key to Polymer is aligning graph-specific data structure and computation with the NUMA characteristics to reduce remote and random memory accesses as much as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Computation Model and Interface</head><p>Polymer follows the typical scatter-gather model by providing two interfaces inherited from Ligra <ref type="bibr" target="#b41">[43]</ref>: EdgeMap and VertexMap. A graph program P in Polymer synchronously runs on a directed graph G = (V, E), where V is the vertex set and E is the edge set. For undirected graphs, each of their edges is presented by a pair of directed edges, one per direction. The topology data of vertex v includes the set of in-or out-neighbors (N in (v) or N out (v)) and its in-or out-degree (|N in (v)| or |N out (v)|). A SubVertex type is used to define a subset of vertices U ⊆ V . Like most prior systems, Polymer assumes that the graph topology is immutable during graph computation; how to extend Polymer to support mutable topology is our future work.</p><p>The following describes the main interface of Polymer.</p><p>1. EdgeMap(G, A, F) : SubVertex For the input directed graph G = (V, E), EdgeMap applies the application-define function F to all edges whose source vertices belong to an active vertex set A. More precisely, for an active edge set</p><formula xml:id="formula_1">E active = � (v, u) ∈ E � � v ∈ A �</formula><p>, the function F is applied to each edge in E active , and returns an updated vertex set SubVertex: R =</p><formula xml:id="formula_2">� u � � (v, u) ∈ E active ∧ F(v, u) = true � .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VertexMap(G, A, F) : SubVertex</head><p>VertexMap applies the application-define function F to all vertices in an active set A, and also returns an updated vertex set</p><formula xml:id="formula_3">SubVertex: R = � v ∈ A � � F(v) = true � .</formula><p>Algorithm 4.1 illustrates the pseudocode of PageRank <ref type="bibr" target="#b5">[7]</ref> in Polymer. It assumes that the graph topology data is stored in G. D curr is used to store the current rank of vertices and initiated to 1/|V | (line 1), while the new rank of vertices will be stored to D next , which is initiated to 0 (line 2). The PREdgeF function (line 3-6) passed to EdgeMap atomically pushes the current rank of a source vertex v to the target vertex t by AtomicAdd (line 4). The PRVertF function (line 7-12) passed to VertexMap normalizes the sum of updates from in-neighbors by multiplying a factor of 0.85, and adds a rank of "random surfer" (line 8). If the absolute difference of the new rank is larger than a user-defined threshold ε, the vertex will be alive in next iteration (line 9). The main function, namely PageRank, iteratively calls EdgeMap and VertexMap until the iteration number exceeds the threshold or all vertices have converged (line 17-23). The runtime states A maintains the active vertices.</p><p>To enjoy the benefit of appropriate execution modes, like Ligra <ref type="bibr" target="#b41">[43]</ref>, Polymer also provides two execution modes (i.e., push and pull). Figure <ref type="figure" target="#fig_5">6</ref> demonstrates the execution flow and in-memory data access pattern under two modes on a 2-node NUMA machine. In the following sections, we will introduce our design on data layout and graph computation in detail based on this figure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph-Aware Data Structure and Layout</head><p>The in-memory data structure of Polymer is similar with other vertex-centric systems, which consist of graph topology, application-defined data and runtime states. However, Polymer is designed specifically for NUMA by co-locating graph data and computation within the same NUMA node as much as possible, with the goal of reducing remote memory accesses and balancing cross-node interconnect bandwidth. The key is leveraging </p><formula xml:id="formula_4">PRVertF(v) begin D next [v] ← 0.15/|V | + (0.85 × D next [v]) alive ← |D next [v] -D curr [v]| &gt; ε D curr [v] ← 0.0 return alive end PageRank(G, ε, max_iter) begin iter ← 0 A ← V while iter ≤ max_iter ∧ A � = / 0 do A ← EdgeMap (G, A, PREdgeF) A ← VertexMap(G, A, PRVertF) Swap(D curr , D next ) iter ++ end end</formula><p>the unique access patterns of graph computation to place the inmemory graph data.</p><p>NUMA-aware graph partitioning: Polymer treats a NUMA machine as a distributed system and partitions graph data structures across nodes. For a NUMA-machine with N memory nodes, Polymer splits its in-memory data structures P into N disjoint partitions P 1 , P 2 ...P N , where � N i=1 P i = P. All vertices are indexed from 0 to |V | -1 and evenly assigned to N disjoint vertex sets V 1 ,V 2 ...V N , where the V i belongs to P i . All edges are also assigned to N disjoint out-edge or in-edge sets by their target vertex or source vertex respectively, where the E i also belongs to P i .</p><p>One approach is co-locating all edges with its source vertices to a single node <ref type="bibr" target="#b41">[43]</ref>. However, as we discussed in Section 3.2 (Figure <ref type="figure" target="#fig_1">2</ref>), this will lead to excessive random remote accesses of application-defined data (such as rank) and runtime states. Hence, Polymer only co-locates edges with their target vertices in a NUMA node in the push mode. All other application-defined data and runtime states are stored together with its owning vertices.</p><p>Further, to eliminate remote accesses due to accesses to graph topology data, Polymer borrows the idea of vertex replication from distributed graph systems <ref type="bibr" target="#b11">[13,</ref><ref type="bibr" target="#b20">22]</ref>, and introduces lightweight vertex replicas, namely agents, for the vertices owned by other partitions. An agent is immutable and only maintains partial topology data, such as the start of neighboring edges and the degree of the vertex. The sole purpose of the agent is used to initiate computation over its master vertex within the node. This essentially eliminates random remote accesses when accessing application-defined data and runtime states. Since the number of vertices is much smaller than that of edges for most graphs, the agents will not cause much memory pressure (see Section 6.5). Figure <ref type="figure" target="#fig_5">6</ref> shows an example of partitioning and placing graph data structures for the sample graph in Figure <ref type="figure" target="#fig_1">2</ref> for both push and pull modes. Vertices 1 to 3 are assigned to node 1 and vertices 4 to 6 are assigned to node 2. In push mode, Polymer co-locates out-edges with their target vertices (vertices 1 to 3). For such edges, Polymer also creates agents for edges whose source vertices are not located in this node. As a result, Polymer creates agents for vertices 4 to 6 accordingly. NUMA-aware allocation of graph data: To overcome the random or interleaved allocation of graph data due to Linux's "first touch" policy, Polymer tackles the mismatch between the allocating threads and the computing threads by letting threads on the ith NUMA-node to allocate and initiate all of data in partition P i . However, simply using co-location may cause the physical addresses of in-memory data discrete, which may result in additional cost for indirect accesses.</p><p>To this end, Polymer differentiates the placement policy of virtual address for these data according to their memory access behavior. First, since the graph topology data is immutable and only accessed in local, Polymer simply uses multiple worker threads bound on different NUMA-nodes to allocate and initiate them, which results in discrete virtual addresses. Second, the applicationdefined data is mutable and globally accessed in sequential order (see Section 4.3). Fortunately, we observe that such data is only allocated once at the construction stage and then repeatedly accessed at the computation stage. Hence, Polymer maps all partitions of application-defined data on discrete physical pages to a contiguous virtual address space, to avoid indirect accesses.</p><p>Finally, the runtime states are mutable. They are also globally read in sequential order, but are re-allocated and locally updated in each iteration. Hence, it may not be worthwhile to repeatedly construct a contiguous virtual address space for them due to high overhead of doing this. Instead, Polymer uses a lock-less tree-structure lookup table to collect all partitions of the runtime states allocated on different NUMA nodes (see lower-left corner of Figure <ref type="figure" target="#fig_5">6</ref>). Each partition is concurrently allocated and linked to an indirect router array without contention.</p><p>Table <ref type="table" target="#tab_2">1</ref> summarizes the data layout of various in-memory data in Polymer. Compared to existing systems, Polymer exploits colocated accesses for all in-memory data and selectively constructs the contiguous virtual address space for them. This may significantly reduce remote accesses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">NUMA-Aware Graph Computation</head><p>Even under co-located data layout, Polymer can still not thoroughly eliminate remote accesses as well as random accesses when accessing neighboring vertices owned by other partitions. One approach would use an additional phase like global shuffling in X-Stream after each iteration, which, however, would incur non-trivial overhead. To this end, Polymer borrows the idea from distributed graph systems <ref type="bibr" target="#b20">[22]</ref> to distribute the computations on a singe vertex over multiple machines. However unlike distributed graph systems, Polymer only replicates the immutable topology data by us- ing agent. For application-defined data, all updates are still applied into a single copy of them by shard memory rather than using replication and explicit messages for synchronization. By factoring computations, the worker threads of each NUMA node only perform part of computations for all of vertices, instead of all of computations for part of vertices. For example, in the push mode of Figure <ref type="figure" target="#fig_5">6</ref>, the computation on three out-edges of vertex 3 are distributed to different threads on two partitions. The first thread will only push the value of vertex 3 to its local neighboring vertex 2, and the second thread will also push the value of vertex 3 to its two local neighboring vertices 5 and 6. In contrasts to Ligra, where all computations are performed by the first thread (see Figure <ref type="figure" target="#fig_1">2</ref>). This neat change of strategy drastically converts the combination of access patterns. For example, in push mode, compared to the combination of local sequential read (SEQ|R|L) and global random write (RAND|W|G) in Ligra, Polymer adopts global sequential read for the data of source vertices (SEQ|R|G) and local random write for the data of target vertices (RAND|W|L). This access pattern during computation essentially conforms to our observations on NUMA characteristics Unfortunately, in the pull mode, the access pattern is first local random read from source vertices (RAND|R|L, and then global sequential write for the target vertices (SEQ|W|G). For example, in Figure <ref type="figure" target="#fig_5">6</ref>, both worker threads will pull the data from local inneighbors to the vertex 3 in parallel. Due to the same sequential order of updating, the same vertex may be updated simultaneously or closely by multiple worker threads on different NUMA-nodes, which may cause heavy contention and frequent cache invalidation. Similarly, in the push mode, the simultaneous read operations on nearby vertices owned by the same NUMA-node may cause congestion on interconnects and memory controllers. Inspired by solutions to the similar problem for handling messages in distributed graph systems <ref type="bibr" target="#b10">[12]</ref>, Polymer makes worker threads on different NUMAnodes process vertices in a rolling order, starting with the own vertices. For example, the worker thread in the second partition will start with vertex 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Optimization on Polymer</head><p>This section describes three optimizations that aim at improving scheduling on NUMA machines and leveraging different properties of graphs.</p><p>Hierarchical and Efficient Barrier: As a result of the hierarchical structure of cache and memory in NUMA machines, the cost of synchronization among threads on different cores rapidly increases with the growing of involved sockets. This leads to the necessity of a topology-aware synchronization mechanism. The default pthread_barrier uses a uniform "flat" model to make all participants on different cores wait on a globally shared variable and trap into the kernel, which can lead to one order of magnitude performance degradation from intra-node to inter-node synchronization (30µs vs. 570µs).</p><p>Inspired by hierarchical scheduling in Tiled-MapReduce <ref type="bibr" target="#b9">[11]</ref>, Polymer designs a hierarchical barrier. The worker threads on the same NUMA-node are grouped to share a partition of data and computation tasks. All worker threads are synchronized within a group first, and then the last thread of each group will further synchronize across the groups. With the increase of involved sockets, the hierarchical barrier can obviously decrease cache coherence broadcasts.</p><p>However, the hierarchical implementation based on pthread_barrier still suffers from the penalty of trapping into kernel. Polymer implements a sense_reversing centralized barrier <ref type="bibr" target="#b34">[36]</ref> based on the atomic fetch-and-add instruction, which is used as a building block for our hierarchical NUMA-aware barrier.</p><p>Balanced Partitioning: For synchronous execution, it is crucial to balance the load among worker threads, especially with hierarchical scheduling. First, it is relatively simple to balance the load among threads within a group on the same NUMA node, since all of them share the same partition and the memory access is uniformed. Polymer adopts dynamic task scheduling among worker threads within a NUMA node, in which each worker thread dynamically fetch a portion of tasks after finished its previous tasks.</p><p>Second, Polymer still has to balance the load across NUMA nodes. However, due to the co-location of data and computation, Polymer requires a balanced partitioning to evenly split the entire workload into multiple groups, and separately processes them on different NUMA nodes. A natural approach is evenly assigning vertices to multiple nodes, whereas the computation complexity and memory access frequency of the scatter and gather phases is linear to the number of edges.</p><p>However, for skewed graphs, such partitioning can lead to substantial work imbalance. Inspired by vertex-cut <ref type="bibr" target="#b20">[22]</ref> in distributed graph systems, Polymer treats edges as first-class citizen in partitioning, and evenly assigns edges to groups on different NUMAnodes. For a graph G = (V, E), the edge-oriented balanced partitioning splits vertices into disjoint sets V 1 ,V 2 ...V N to minimize the deviation of</p><formula xml:id="formula_5">{∑ v∈V i |N in/out (v)| � � 1 ≤ i ≤ N}.</formula><p>Polymer can efficiently implement such balanced partitioning due to little communication cost in single machine. In addition, it should be noted that it is hard to evenly partition both in-edges and out-edges at same time. Fortunately, in most cases, Polymer only uses one type of edges in either push or pull mode, so that it is enough to evenly partition edges in one direction.</p><p>Adaptive Data Structures: As shown in Figure <ref type="figure" target="#fig_5">6</ref>, Polymer uses a lock-less tree-structure lookup table to represent the runtime states. Each leaf partition of the table is a bitmap, which is efficient for a large proportion of active vertices. However, as most graph algorithms converge asymmetrically, the proportion of active vertices will sustainedly decline and reach zero. The overhead of traversing through sparse bitmaps is non-trivial in each iteration, leading to non-trivial performance slowdown, especially for traversal algorithms with high-diameter graphs. Inspired by the solution in Ligra <ref type="bibr" target="#b41">[43]</ref>, Polymer uses adaptive data structures for runtime states within the tree-structure table and automatically switches data structure for leaf partitions according to the proportion of active vertices. Unlike the bitmap shared by all worker threads within a NUMA-node, each thread on different cores will allocate a private queue and append active vertex ID to it without contention. The queues within a NUMA-node can be merged when de-duplication of vertex ID if necessary or linked to a local indirect router array to form a two-level tree-structure. Finally, Polymer uses the total degrees of active vertices and the applicationdefined threshold to decide whether to switch data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Polymer is implemented in C++ using the Pthreads library. It currently supports both push and pull mode using a synchronous scheduler. Polymer and its applications are approximately 5,300 LOC, and are compiled using gcc version 4.8.1.</p><p>We evaluate Polymer against other three state-of-the-art graph analytics systems on a single machine: Ligra, X-Stream v0.9 and Galois v2.2. We omit comparison with GraphChi, which were shown to have inferior performance than others when the input graphs fit in memory <ref type="bibr" target="#b37">[39,</ref><ref type="bibr" target="#b41">43]</ref>. Unless otherwise mentioned, all experiments were performed on the 80-core Intel machine (without hyper-threading), which consists of eight 2.0GHz Intel Xeon E7-8850 processors connected with QPI. These form a twisted hypercube, maximizing the distance between two nodes to two hops. Each NUMA socket has 10 cores and a 128GB local DRAM. We also ran all experiments on our 64-core AMD machine, which consists of four multi-chip modules connected with HT. Each module has two 8-core die with independent memory controllers, thereby the system comprises eight memory nodes (i.e., sockets). Due to the space restriction, we only report representative results for scalability and skip the rest similar results on the AMD machine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Algorithms and Graphs</head><p>We use six popular graph algorithms to evaluate Polymer.</p><p>PageRank (PR) computes the rank of each vertex based on the ranks of its neighbors <ref type="bibr" target="#b5">[7]</ref>. We use the synchronous, push-based PageRank for Polymer, Ligra and X-Stream in all cases because it is relatively faster. Galois chooses a synchronously pull-based implementation to reduce synchronization overhead.</p><p>Sparse matrix-vector multiplication (SpMV) multiplies the sparse adjacency matrix of a directed graph with a dense vector of values, one per vertex.</p><p>Bayesian belief propagation (BP) estimates the probabilities of vertices by iterative message passing between vertices along weighted edges <ref type="bibr" target="#b23">[25]</ref>. Since only X-Stream provides SpMV and BP applications, we implement the algorithms for other three systems.</p><p>Breadth-first search (BFS) traverses an unweighted graph by visiting the sibling vertices before visiting the child vertices. Ligra follows a data-driven hybrid implementation <ref type="bibr" target="#b4">[6]</ref> by switching between sparse and dense representation of runtime states. Galois mixes synchronous and asynchronous scheduling to implement the BFS application.</p><p>Connected components (CC) calculates a maximal set of vertices that are reachable from each other for a directed graph. Polymer, Ligra and X-Stream all adopt label propagation <ref type="bibr" target="#b47">[49]</ref> to im- plement this algorithm, while Galois provides a topology-driven algorithm based on a concurrent union-find data structure <ref type="bibr" target="#b37">[39]</ref>.</p><p>Single-source shortest-paths (SSSP) computes the distance of the shortest path from a given source vertex to each vertex. The SSSP implementation on Polymer, Ligra and X-Stream is based on the Bellman-Ford algorithm <ref type="bibr" target="#b14">[16]</ref> with synchronously data-driven scheduling, while Galois uses a data-driven and asynchronously scheduled delta-stepping algorithm <ref type="bibr" target="#b35">[37]</ref>.</p><p>The input graphs used in our experiments are shown in Table <ref type="table" target="#tab_3">2</ref>. twitter is a real-world social follower graph <ref type="bibr" target="#b25">[27]</ref>. The synthetic scale-free graphs, rMat24 and rMat27, are generated by the R-MAT generator <ref type="bibr" target="#b7">[9]</ref> in Graph500 <ref type="bibr" target="#b1">[2]</ref>. The synthetic power-law graph (powerlaw) with fixed power-law constant 2.0 was generated by tools in PowerGraph <ref type="bibr" target="#b20">[22]</ref>, which randomly sample the degree of each vertex from a Zipf distribution <ref type="bibr" target="#b2">[4]</ref> and then add edges. Finally, the road network of the United States (roadUS) is from the 9th DIMACS shortest paths challenge <ref type="bibr" target="#b0">[1]</ref> with much high diameter. All graphs are unweighted except roadUSA. To provide a weighted input for the SpMV and SSSP algorithms, we add a random edge weight in the range (0, 100] to each edge. For the other applications, we simply ignore the weights of roadUSA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Overall Performance</head><p>Table <ref type="table" target="#tab_4">3</ref> gives a complete runtime comparison between Polymer and other three state-of-the-art graph-analytics systems on a single machine: Ligra, X-Stream and Galois. We report the execution time of first five iterations for PageRank, SpMV and BP, as other systems.</p><p>For three sparse matrix multiplication algorithms (PR, SpMV and BP), Polymer achieves optimal performance against other systems in all cases using 80 threads on multiple NUMA memory nodes, except BP on the roadUS graph. The largest improvement is from BP on the twitter graph with respect of X-Stream by 53.09X. For PR and SpMV, Polymer still outperforms X-Stream by up to 5.48X and 7.89X accordingly. Compared with Ligra, the largest improvements for three algorithms are all on the powerlaw graph by 18.9X, 17.3X and 3.80X respectively, benefiting from load balancing. Polymer also outperforms Galois by up to 4.11X, 3.46X and 1.58X respectively. The graph traversal algorithms, including BFS, CC and SSSP, are not sensitive to the memory accesses of NUMA systems, since they have much fewer active vertices in each iteration, and then resulting in fewer memory accesses. Polymer still can provide optimal or close performance due to several optimizations, such as balance partitioning for power-law graphs and adaptive data structure for high-diameter graphs.</p><p>The performance of Ligra is with a similar pace as Polymer, due to the same execution modes (i.e., push and pull) and similar optimizations like automatic mode switching. However, Polymer outperforms Ligra in most cases due to its NUMA-aware designs. The edge-centric system such X-Stream has extremely poor performance for traversal algorithms, especially for high-diameter graphs like road networks, due to excessive accesses to edges and inefficient data structure for the runtime states. All edges (e.g., 58M for roadUS) must be identified whether to participant computation by accessing their state of source vertex in each iteration, even there are just several active vertices. Further, the extremely slow convergence (e.g. 6237 iterations for BFS with roadUS) of highdiameter graphs amplify the problem <ref type="foot" target="#foot_4">6</ref> . For high-diameter graphs like roadUS, the asynchronous scheduling and special implementations in Galois are able to exploit more parallelism for the graph traversal algorithms, such as CC and SSSP. Unfortunately, they do not work well with other graphs, due to their relative higher average degrees and lower diameter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Scalability</head><p>Since all of existing systems have scaled well in terms of number of cores and Polymer mainly improves the performance and scalability on NUMA machines, we concentrate on the study of the scalability in terms of number of sockets using PageRank and BFS algorithms for the twitter graph on two NUMA machines.</p><p>On our 80-core Intel NUMA machine, as shown in Figure <ref type="figure">7</ref>, the scalability of Polymer for PageRank is much better than all of existing systems. The scalability ratio even exceeds the number of sockets (i.e., 12.1 vs. 8). As the number of sockets increases, the size of total caches increases and the size of partitions for each socket decreases, which lead to less cache misses and thus a superlinear speedup.</p><p>Even if the performance of Polymer is just close or even worse on single NUMA-node compared with other systems, with the increasing number of sockets, Polymer can enjoys much larger speedup and outperforms Ligra, X-Stream and Galois by 2.84X, 5.45X and 2.19X. This conforms the effectiveness of NUMAaware implementation and associated optimizations.</p><p>Figure <ref type="figure">8</ref> illustrates the performance and scalability results for PageRank on our 64-core AMD machine. Polymer exhibits a similar trend with the increase of sockets from 1 to 8, however, the scalability ratio on the AMD machine is 6.01X, which is lower than that on the Intel machine with the same 8 sockets, probably due to relative small last level cache (i.e. 16 MB vs. 24 MB) and different interconnect. Two sockets within multi-chip model share more bandwidth, restricting the scalability.</p><p>As shown in Figure <ref type="figure" target="#fig_7">9</ref>, the scalability for BFS is relatively poor in all of systems, due to fewer active vertices in each iteration. However, Polymer still exhibits much better scalability and outperforms other existing systems using 8 sockets. Note that missing the execution time for X-Stream is due to out of range (from 69.4s to 28.7s). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Reduced Remote and Random Accesses</head><p>To understand the source of the improvement in Polymer, we evaluate various systems using PageRank and BFS algorithms with the twitter graph, to compare the remote access rate, the total number of remote accesses and the LLC miss rate due to remote access. For  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Hierarchical and Efficient Barrier</head><p>To demonstrate the benefit of hierarchical and user-level synchronization, we compare our NUMA-aware barrier (N-Barrier) with default pthread_barrier (P-Barrier). To breakdown the improvement, we further implement a hierarchical_barrier (H-Barrier) using pthread_barrier with hierarchical mechanism. Figure <ref type="figure" target="#fig_8">10</ref>(a) presents absolute performance of three barriers with the increasing number of sockets. We bind 10 threads on each sockets, one thread per core. The hierarchical mechanism in H-Barrier can obviously improve the scalability by decreasing cache coherence broadcasts between NUMA-nodes. H-Barrier outperforms P-Barrier by almost one order of magnitude on eight sockets (6182µs vs. 612µs). However, the performance of H-Barrier still suffers from frequently trapping into the kernel. Based on H-Barrier, the N-Barrier further replace the pthread_barrier with atomic memory access, and achieves additional two order of magnitude improvement (612µs vs. 8µs). We further compare the performance of various algorithms on Polymer with and without the optimization for the roadUS graph. As shown in Figure <ref type="figure" target="#fig_8">10</ref>(b), NUMA-aware barrier can only provide a limited improvement on PageRank, SpMV and BP by up to 8% (7%, 2% and 8%). While for three traversal algorithms, NUMAaware barrier can improve the performance by 58.6X, 5.51X and 1.28X, due to high proportional synchronization overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Adaptive Data Structure</head><p>To study the benefit of adaptive data structure, Table <ref type="table" target="#tab_7">6</ref>(a) compares the performance of various algorithms on Polymer with and without the optimization for roadUS graph. As the major improvement is from reducing the performance cost to check runtime states when there are few active vertices. For sparse matrix multiplication algorithms (PR, SpMV and BP), the active number of vertices is quite stable. The switching of data structure occurs much later or even no switch at all, thereby the improvement is limited, up to 9%. In contrast, the traversal algorithms, including BFS, CC and SSSP, can significantly benefit from more efficient data structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Balanced Partitioning</head><p>We first compare the performance of Polymer with and without balance partitioning using the twitter graph. The out-degree power-law constant (α) of the twitter graph is close to 2.0. Table <ref type="table" target="#tab_7">6</ref>(a) shows that the speedup ranges from 1.29X to 3.67X. To reveal the benefit of balanced partitioning, we first estimate the number of edges in each partition for the twitter graph. As shown in Figure <ref type="figure" target="#fig_10">11(a)</ref>, the default partitioning will evenly assign vertices among NUMAnodes and incur the imbalance of edges, while the balance partitioning can prominently narrow the fluctuation into a range from -0.5% to 0.8%. We further collect the pure execution time of the threads on each socket using the PageRank algorithm. Due to synchronous scheduling, the overall execution time is decided by the slowest worker thread. With default partitioning, the imbalance of edges causes the difference of execution time, which will be amplified by the congestion on some interconnects and memory controllers. In Figure <ref type="figure" target="#fig_10">11</ref>(b), the black and red dotted lines indicate the whole execution time with and without balanced partitioning. The execution time on each socket without optimization ranges from 4.16 to 9.32 seconds. In contrast, the range for balanced execution is from 4.72 to 4.86 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Other Related Work</head><p>Polymer directly departs from prior graph analytics systems such as Ligra <ref type="bibr" target="#b41">[43]</ref>, X-Stream <ref type="bibr" target="#b40">[42]</ref> and Galois <ref type="bibr" target="#b37">[39]</ref>, but differs with them by adopting NUMA-and graph-aware data layout and access strategy.</p><p>Other single-machine graph-analytics systems: There are several efforts aiming at leveraging multicore platforms for graph processing <ref type="bibr" target="#b26">[28,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b46">48]</ref>. For example, GraphChi <ref type="bibr" target="#b26">[28]</ref> targets at disk-based graph computation by using parallel sliding windows to preserve access locality for graph data. Medusa <ref type="bibr" target="#b46">[48]</ref> provides users with a simple interface to write graph-parallel code on GPUs. Such techniques should be useful when extending Polymer for diskbased processing, CPU-GPU co-processing and streaming processing. However, none of them focus on leveraging NUMA characteristics. Polymer borrows some designs from prior systems, and our techniques may be helpful to boost performance of such systems on NUMA machines.</p><p>Distributed graph-analytics systems: The popularity of graph analytics is also embodied in distributed graph analytics systems, such as Pregel <ref type="bibr" target="#b33">[35]</ref>, GraphLab <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b30">32]</ref>, Cyclops <ref type="bibr" target="#b11">[13]</ref> and GraphX <ref type="bibr" target="#b21">[23]</ref>. Mizan <ref type="bibr" target="#b24">[26]</ref> leverages vertex migration for dynamic load balancing. PowerSwitch <ref type="bibr" target="#b44">[46]</ref> proposes a hybrid execution mode that adaptively switches a graph-parallel program between synchronous and asynchronous execution modes for optimal performance. Imitator <ref type="bibr" target="#b43">[45]</ref> reuses computational replication for fault tolerance in large-scale graph processing to provide low-overhead normal execution and fast crash recovery. There are also a few systems considering streaming processing <ref type="bibr" target="#b13">[15,</ref><ref type="bibr" target="#b36">38]</ref> or graph properties <ref type="bibr" target="#b10">[12,</ref><ref type="bibr" target="#b12">14,</ref><ref type="bibr" target="#b22">24]</ref>. Many techniques aimed at NUMA systems in Polymer are borrowed from these distributed systems.</p><p>Other NUMA-aware computation systems: Liu et. al. <ref type="bibr" target="#b28">[30]</ref> recently developed a tool to analyze and quantify the bottlenecks of multithreaded program on NUMA platforms. David et. al. <ref type="bibr" target="#b16">[18]</ref> presented an exhaustive study of synchronization on various multicore and NUMA systems. MemProf <ref type="bibr" target="#b27">[29]</ref> is a memory profiler aimed at NUMA systems for optimizing multithread programs Carrefour <ref type="bibr" target="#b15">[17]</ref> is a memory placement algorithm for data-intensive applications, which aims at eliminating the congestion on memory controllers and interconnects. N-MASS <ref type="bibr" target="#b32">[34]</ref> is a scheduling algorithm that simultaneously considers data locality and cache contention for NUMA systems. Gaud et. al. <ref type="bibr" target="#b19">[21]</ref> discovered that large pages may hurt performance on NUMA systems and proposed a memory placement to recover the performance. Lock cohorting <ref type="bibr" target="#b17">[19]</ref> can transform general spin-lock algorithms into scalable NUMA-aware versions. Calciu et. al. <ref type="bibr" target="#b6">[8]</ref> further extended the lock cohorting technique to tailor reader-writer locks in a NUMAfriendly fashion. Ostrich <ref type="bibr" target="#b9">[11]</ref> improved the performance of MapReduce on NUMA machines with tiling. Polymer is inspired by prior work on NUMA platforms, such as reducing remote memory accesses, but is specially designed to leverage graph-specific characteristics to boost performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper described Polymer, a NUMA-aware graph analytics system, which is motivated by a detailed study of NUMA characteristics. The key of Polymer's performance is using graph-aware graph data allocation, data layout and access strategy that reduces remote memory accesses as much as possible and turns inevitable remote accesses from random to sequential ones. This significantly boosted the performance of Polymer on NUMA platforms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The in-memory data structure of scatter-gather model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The in-memory data layout and execution flow of existing graph-analytics systems for a sample graph. The pattern to access in-memory data is labeled by SEQ/RAND, R/W and L/G. Yellow indicates source vertex, and blue indicates target vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The characteristics of NUMA machines for experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) The speedup with the increasing number of cores within one socket. (b) and (c) The speedup and execution time with the increasing number of sockets with fixed 10 cores per socket. (d) The speedup with the increasing number of sockets with fixed 8 cores per socket on the 64-core AMD machine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 4 . 1 :</head><label>41</label><figDesc>PageRank D curr ← {1/|V |, 1/|V |, ... 1/|V |} D next ← {0.0, 0.0, ... 0.0} PREdgeF(s,t) begin AtomicAdd (&amp;D next [t], D curr [s]/|N out (s)|) return true end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The in-memory data layout and execution flow of Polymer for two modes on the sample graph in Figure 2. The pattern to access in-memory data is also labeled by SEQ/RAND, R/W and L/G. Yellow indicates source vertex, and blue indicates target vertex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. The execution time and normalized speedup for PageRank with the increasing number of sockets (using full cores) on the Intel machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The execution time and normalized speedup for BFS with the increasing number of sockets (using full cores) on the Intel machines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. (a) The synchronization time with the increasing number of sockets using various barriers. (b) A comparison of execution time w/ and w/o NUMA-aware barrier using various algorithms on the roadUS graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. (a) The normalized difference of edges in each partition for the twitter graph. (b) The execution time w/ and w/o balance partitioning on each socket for PageRank with the twitter graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>A summary of the data layout of for various systems. P and V represent physical and virtual address, while D and C represent discrete and contiguous memory address. CE, IL and CO represent centralized, interleaved and co-located memory access respectively.</figDesc><table><row><cell>Data</cell><cell cols="2">Existing Systems</cell><cell cols="2">Polymer</cell></row><row><cell>Layout</cell><cell>Alloc(P,V)</cell><cell>Access</cell><cell>Alloc(P,V)</cell><cell>Access</cell></row><row><cell>Topo</cell><cell>D, C</cell><cell>IL</cell><cell>D, D</cell><cell>CO</cell></row><row><cell>Data</cell><cell>D, C</cell><cell>IL</cell><cell>D, C</cell><cell>CO</cell></row><row><cell>Stat</cell><cell>C, C</cell><cell>CE</cell><cell>D, D</cell><cell>CO</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>A collection of real-world and synthetic graphs.</figDesc><table><row><cell>Graph</cell><cell>Num. Vertices</cell><cell>Num. Edges</cell></row><row><cell>twitter</cell><cell>41.7 M</cell><cell>1.47 B</cell></row><row><cell>rMat24</cell><cell>16.8 M</cell><cell>268 M</cell></row><row><cell>rMat27</cell><cell>134.2 M</cell><cell>2.14 B</cell></row><row><cell>powerlaw</cell><cell>10.0 M</cell><cell>105 M</cell></row><row><cell>roadUS</cell><cell>23.9 M</cell><cell>58 M</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Runtimes (in seconds) of algorithms over various datasets with 80 threads on the 80-core Intel machine. Red times are the best for each input and graph problem pair. ( †) Galois adopts different algorithms.</figDesc><table><row><cell>Algo.</cell><cell>Graph</cell><cell>Polymer</cell><cell>Ligra</cell><cell>X-Stream</cell><cell>Galois</cell></row><row><cell></cell><cell>twitter</cell><cell>5.28</cell><cell>15.03</cell><cell>28.91</cell><cell>11.55</cell></row><row><cell></cell><cell>rMat24</cell><cell>1.84</cell><cell>4.10</cell><cell>4.80</cell><cell>2.89</cell></row><row><cell>PR</cell><cell>rMat27</cell><cell>9.63</cell><cell>28.00</cell><cell>18.20</cell><cell>19.61</cell></row><row><cell></cell><cell>powerlaw</cell><cell>1.61</cell><cell>30.50</cell><cell>6.06</cell><cell>6.62</cell></row><row><cell></cell><cell>roadUS</cell><cell>1.21</cell><cell>2.32</cell><cell>2.79</cell><cell>1.38</cell></row><row><cell></cell><cell>twitter</cell><cell>7.55</cell><cell>29.00</cell><cell>59.57</cell><cell>11.68</cell></row><row><cell></cell><cell>rMat24</cell><cell>1.86</cell><cell>4.30</cell><cell>5.24</cell><cell>6.02</cell></row><row><cell>SpMV</cell><cell>rMat27</cell><cell>19.15</cell><cell>54.25</cell><cell>52.54</cell><cell>41.86</cell></row><row><cell></cell><cell>powerlaw</cell><cell>1.80</cell><cell>31.00</cell><cell>5.53</cell><cell>6.21</cell></row><row><cell></cell><cell>roadUS</cell><cell>1.29</cell><cell>2.83</cell><cell>2.98</cell><cell>3.55</cell></row><row><cell></cell><cell>twitter</cell><cell>38.00</cell><cell>63.10</cell><cell>2017.29</cell><cell>57.06</cell></row><row><cell></cell><cell>rMat24</cell><cell>7.73</cell><cell>9.88</cell><cell>44.27</cell><cell>12.20</cell></row><row><cell>BP</cell><cell>rMat27</cell><cell>58.30</cell><cell>92.80</cell><cell>736.62</cell><cell>74.98</cell></row><row><cell></cell><cell>powerlaw</cell><cell>8.08</cell><cell>30.70</cell><cell>38.26</cell><cell>8.58</cell></row><row><cell></cell><cell>roadUS</cell><cell>5.18</cell><cell>2.59</cell><cell>19.99</cell><cell>7.05</cell></row><row><cell></cell><cell>twitter</cell><cell>0.90</cell><cell>1.13</cell><cell>28.70</cell><cell>2.67</cell></row><row><cell></cell><cell>rMat24</cell><cell>0.53</cell><cell>0.50</cell><cell>4.30</cell><cell>0.40</cell></row><row><cell>BFS</cell><cell>rMat27</cell><cell>1.56</cell><cell>1.86</cell><cell>30.18</cell><cell>2.54</cell></row><row><cell></cell><cell>powerlaw</cell><cell>0.36</cell><cell>0.39</cell><cell>2.58</cell><cell>0.36</cell></row><row><cell></cell><cell>roadUS</cell><cell>1.16</cell><cell>6.93</cell><cell>557.68</cell><cell>5.01</cell></row><row><cell></cell><cell>twitter</cell><cell>4.60</cell><cell>5.51</cell><cell>54.80</cell><cell>31.91</cell></row><row><cell></cell><cell>rMat24</cell><cell>1.11</cell><cell>0.98</cell><cell>11.01</cell><cell>11.55</cell></row><row><cell>CC</cell><cell>rMat27</cell><cell>8.72</cell><cell>7.74</cell><cell>39.95</cell><cell>33.86</cell></row><row><cell></cell><cell>powerlaw</cell><cell>1.23</cell><cell>2.56</cell><cell>5.13</cell><cell>3.51</cell></row><row><cell></cell><cell>roadUS</cell><cell>57.50</cell><cell>63.20</cell><cell>985.15</cell><cell>†1.18</cell></row><row><cell></cell><cell>twitter</cell><cell>2.26</cell><cell>3.17</cell><cell>165.15</cell><cell>26.29</cell></row><row><cell></cell><cell>rMat24</cell><cell>1.04</cell><cell>1.25</cell><cell>17.86</cell><cell>1.95</cell></row><row><cell>SSSP</cell><cell>rMat27</cell><cell>5.78</cell><cell>5.26</cell><cell>126.38</cell><cell>28.50</cell></row><row><cell></cell><cell>powerlaw</cell><cell>0.85</cell><cell>1.12</cell><cell>12.36</cell><cell>26.58</cell></row><row><cell></cell><cell>roadUS</cell><cell>341</cell><cell>338</cell><cell>1225</cell><cell>†0.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>A comparison of the remote access rate, the number of remote accesses and the LLC miss rate due to remote accesses.</figDesc><table><row><cell></cell><cell>Polymer</cell><cell cols="3">Ligra X-Stream Galois</cell></row><row><cell>Access Rate/R</cell><cell>37.5%</cell><cell>83.3%</cell><cell>47.4%</cell><cell>83.6%</cell></row><row><cell>Num. Accesses/R</cell><cell cols="2">3,090M 6,116M</cell><cell cols="2">5,016M 7,887M</cell></row><row><cell>LLC Miss Rate/R</cell><cell>3.94%</cell><cell>9.47%</cell><cell cols="2">8.67% 13.17%</cell></row><row><cell></cell><cell cols="2">(a) PageRank</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Polymer Ligra X-Stream Galois</cell></row><row><cell>Access Rate/R</cell><cell cols="2">28.6% 37.5%</cell><cell>33.3%</cell><cell>47.4%</cell></row><row><cell>Num. Accesses/R</cell><cell>340M</cell><cell>591M</cell><cell>374M</cell><cell>709M</cell></row><row><cell>LLC Miss Rate/R</cell><cell cols="2">1.70% 2.27%</cell><cell>1.72%</cell><cell>7.10%</cell></row><row><cell></cell><cell cols="2">(b) BFS</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>The peak memory usage (in GB) on Polymer and existing systems over various graphs for PageRank using 80 threads. The memory usage for agents is shown in brackets.Polymer introduces lightweight replication of vertices across NUMA-nodes to factor computation and reduce remote memory accesses, but increasing the memory consumption. In Table5, we investigate the memory consumption of Polymer and existing systems over various graphs. Since the number of replicas increases with the increasing number of sockets, we evaluate the peak memory consumption for PageRank algorithm using all of the eight NUMA-nodes. Since only Galois uses its own optimized memory allocator and carefully reuses memory between iterations, it is no surprise that it obtains the best results. X-Stream consumes the most memory due to additional buffers in the shuffle phase. The memory consumption of Polymer and Ligra is close, both of which use the default memory allocator. The increase of memory consumption is lower than 30%, except the roadUS graph (38.3%) due to much lower ration of edges to vertices (2.43X). The major extra usage (around 80%) of Polymer is from the lightweight replication.</figDesc><table><row><cell>Graph</cell><cell cols="4">Polymer(agent) Ligra X-Stream Galois</cell></row><row><cell>twitter</cell><cell>39.2(2.95)</cell><cell>37.0</cell><cell>39.9</cell><cell>25.1</cell></row><row><cell>rMat24</cell><cell>13.1(1.68)</cell><cell>10.2</cell><cell>17.4</cell><cell>10.2</cell></row><row><cell>rMat27</cell><cell>71.1(4.37)</cell><cell>66.6</cell><cell>75.4</cell><cell>64.0</cell></row><row><cell>powerlaw</cell><cell>7.0(1.13)</cell><cell>5.8</cell><cell>10.1</cell><cell>5.3</cell></row><row><cell>roadUS</cell><cell>11.2(1.52)</cell><cell>8.1</cell><cell>17.0</cell><cell>7.3</cell></row><row><cell cols="5">PageRank, as shown in Table 4(a), Polymer has much fewer remote</cell></row><row><cell cols="5">accesses in both the rate and the number due to co-locating graph</cell></row><row><cell cols="5">data and computation. Further, the remote accesses in Polymer is</cell></row><row><cell cols="5">sequential, reducing up to 70% (from 55%) LLC miss rate due to</cell></row><row><cell cols="5">remote accesses. For BFS, which leaves limited room for improve-</cell></row><row><cell cols="5">ment due to fewer memory accesses, the results in Table 4(b) still</cell></row><row><cell cols="3">confirms the better performance of Polymer.</cell><cell></cell><cell></cell></row><row><cell cols="2">6.5 Memory Consumption</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>A comparison of execution time w/ and w/o (a) adaptive data structure and (b) balanced partitioning using various algorithms.</figDesc><table><row><cell>Algo.</cell><cell>w/o</cell><cell>w/</cell><cell>Algo.</cell><cell>w/o</cell><cell>w/</cell></row><row><cell>PR</cell><cell>1.32</cell><cell>1.21</cell><cell>PR</cell><cell>10.30</cell><cell>5.28</cell></row><row><cell>SpMV</cell><cell>1.30</cell><cell>1.29</cell><cell>SpMV</cell><cell>13.05</cell><cell>7.55</cell></row><row><cell>BP</cell><cell>5.69</cell><cell>5.18</cell><cell>BP</cell><cell>49.20</cell><cell>38.00</cell></row><row><cell>BFS</cell><cell>827</cell><cell>1.16</cell><cell>BFS</cell><cell>3.30</cell><cell>0.90</cell></row><row><cell>CC</cell><cell>868</cell><cell>57.5</cell><cell>CC</cell><cell>8.85</cell><cell>4.60</cell></row><row><cell>SSSP</cell><cell>1720</cell><cell>341</cell><cell>SSSP</cell><cell>5.32</cell><cell>2.26</cell></row><row><cell cols="3">(a) Adaptive data structure</cell><cell cols="3">(b) Balanced Partitioning</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Detailed machine configurations can be found in Section 6</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>The latency is evaluated by ccbench<ref type="bibr" target="#b16">[18]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The bandwidth is evaluated by numademo [3] with some extensions.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>We select sockets with minimized total distances. Since the number of threads in X-Stream must be a power of 2, we evaluate it using up to 8 cores in each socket.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Vertex-centric systems (e.g., Ligra, Polymer and Galois) can avoid such overhead by adaptively using queue-based data structure to maintain active vertices. For example, the average overhead in each iteration for BFS with roadUS are 0.032ms, 0.043ms and 92ms for Polymer, Ligra and X-Stream respectively (Galois uses asynchronous scheduling).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the anonymous reviewers for their insightful comments. This work is supported in part by the Doctoral Fund of Ministry of Education of China (No. 20130073120040), the Program for New Century Excellent Talents in University, Ministry of Education of China (No. ZXZY037003), a foundation for the Author of National Excellent Doctoral Dissertation of PR China (No. TS0220103006), the National Natural Science Foundation of China (No. 61303011 and 61402284), the Open Project Program of the State Key Laboratory of Mathematical Engineering and Advanced Computing (No. 2014A05), the Shanghai Science and Technology Development Fund for high-tech achievement translation (No. 14511100902), a research grant from Intel and the Singapore NRF (CREATE E2S2).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.dis.uniroma1.it/challenge9/" />
		<title level="m">The 9th dimacs implementation challenge -shortest paths</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="http://www.graph500.org" />
		<title level="m">Graph 500</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Zipf&apos;s law and the internet</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glottometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The multikernel: a new os architecture for scalable multicore systems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-E</forename><surname>Dagand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Roscoe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schüpbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Singhania</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Direction-optimizing breadth-first search</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Asanović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Programming</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="137" to="148" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Numa-aware reader-writer locks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Calciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Luchangco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-mat: A recursive model for graph mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="442" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Tiled-mapreduce: Efficient and flexible mapreduce processing on multicore with tiling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>ACM TACO</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tiled-mapreduce: optimizing resource usages of data-parallel applications on multicore with tiling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PACT</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Powerlyra: Differentiated graph computation and partitioning on skewed graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno>2013-11-001</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
		<respStmt>
			<orgName>IPADS, SJTU</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Computation and communication efficient graph processing with distributed immutable view</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HPDC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bipartite-oriented distributed graph partitioning for big learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APSys</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kineograph: taking the pulse of a fast-changing and connected world</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Introduction to algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Traffic management: a holistic approach to memory placement on numa systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Everything you always wanted to know about synchronization but were afraid to ask</title>
		<author>
			<persName><forename type="first">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Trigonakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Lock cohorting: a general technique for designing numa locks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Marathe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shavit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="247" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On power-law relationships of the internet topology</title>
		<author>
			<persName><forename type="first">M</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGCOMM</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="251" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large pages may be harmful on numa systems</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Decouchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Grenoble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PowerGraph: Distributed graph-parallel computation on natural graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graphx: Graph processing in a distributed dataflow framework</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chronos: a graph engine for temporal graph analysis</title>
		<author>
			<persName><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inference of beliefs on billion-scale graphs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD-LDMTA</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mizan: a system for dynamic load balancing in large-scale graph processing</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Khayyat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Awara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alonazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jamjoom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kalnis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EuroSys</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="169" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What is twitter, a social network or a news media?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="591" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GraphChi: Large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memprof: A memory profiler for numa multicore systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quéma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX ATC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tool to analyze the performance of multithreaded programs on numa architectures</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bpgm: A big graph mining tool</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed GraphLab: a framework for machine learning and data mining in the cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB Endowment</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="716" to="727" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Challenges in parallel graph processing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lumsdaine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hendrickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Berry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Parallel Processing Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Memory management in numa multicore systems: trapped between cache contention and interconnect overhead</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Majo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pregel: a system for large-scale graph processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Algorithms for scalable synchronization on shared-memory multiprocessors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mellor-Crummey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOCS</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="65" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">δ -stepping: a parallelizable shortest path algorithm</title>
		<author>
			<persName><forename type="first">U</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Algorithms</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="152" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Naiad: a timely dataflow system</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PLANET: massively parallel learning of tree ensembles with MapReduce</title>
		<author>
			<persName><forename type="first">B</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Herbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bayardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Managing large graphs on multi-cores with graph awareness</title>
		<author>
			<persName><forename type="first">V</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Haridasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Usenix ATC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SOSP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An architecture for parallel topic models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanamurthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Replication-based fault-tolerance for large-scale graph processing</title>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSN</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sync or async: Time to fuse for distributed graph-parallel computation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the embeddability of random walk distances</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medusa: Simplified Graph Processing on GPUs. TPDS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning from labeled and unlabeled data with label propagation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<idno>CMU- CALD-02-107</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
