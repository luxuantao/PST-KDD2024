<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Under review as a conference paper at ICLR 2020 CONDITIONAL NEGATIVE SAMPLING FOR CON-TRASTIVE LEARNING OF VISUAL REPRESENTATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">Under review as a conference paper at ICLR 2020 CONDITIONAL NEGATIVE SAMPLING FOR CON-TRASTIVE LEARNING OF VISUAL REPRESENTATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -in a "ring" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.</p><p>Under review as a conference paper at ICLR 2020 mator of mutual information based on the popular noise-contrastive estimator (NCE) that supports sampling negatives from conditional distributions. We summarize our contributions below:</p><p>1. We prove our Conditional-NCE (CNCE) objective to lower bound mutual information.</p><p>Further, we show that although CNCE is a looser bound than NCE, it has lower variance. This motivates its value for representation learning. 2. We use CNCE to generalize contrastive algorithms that utilize a memory structure like IR, CMC, and MoCo to sample semi-hard negatives in just a few lines of code and minimal compute overhead. 3. We find that the naive strategy of sampling hard negatives throughout training can be detrimental. We then show that slowly introducing harder negatives yields good performance. 4. On four image classification benchmarks, we find improvements of 2-5% absolute points.</p><p>We also find consistent improvements (1) when transferring features to new image datasets and (2) in object detection, instance segmentation, and key-point detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>We focus on exemplar-based contrastive objectives, where examples are compared to one another to learn a representation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Supervised learning has given rise to human-level performance in several visual tasks <ref type="bibr" target="#b33">(Russakovsky et al., 2015;</ref><ref type="bibr" target="#b17">He et al., 2017)</ref>, relying heavily on large image datasets paired with semantic annotations. These annotations vary in difficulty and cost, spanning from simple class labels to more granular descriptions like bounding boxes and key-points. As it is impractical to scale high quality annotations, this reliance on supervision poses a barrier to widespread adoption. While supervised pretraining is still the dominant approach in computer vision, recent studies using unsupervised "contrastive" objectives, have achieved remarkable results in the last two years, closing the gap to supervised baselines <ref type="bibr" target="#b44">(Wu et al., 2018;</ref><ref type="bibr" target="#b29">Oord et al., 2018;</ref><ref type="bibr" target="#b20">Hjelm et al., 2018;</ref><ref type="bibr" target="#b49">Zhuang et al., 2019;</ref><ref type="bibr" target="#b19">Hénaff et al., 2019;</ref><ref type="bibr" target="#b27">Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b15">Grill et al., 2020)</ref>.</p><p>Many contrastive algorithms are estimators of mutual information <ref type="bibr" target="#b29">(Oord et al., 2018;</ref><ref type="bibr" target="#b20">Hjelm et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019)</ref>, capturing the intuition that a good low-dimensional "representation" is one that linearizes the useful information embedded within a high-dimensional data point. In vision, these estimators maximize the similarity of encodings for two augmentations of the same image. This is trivial (e.g. assign all image pairs maximum similarity) unless the similarity function is normalized. This is typically done by comparing an image to "negative examples", which a model must assign low similarity to. We hypothesize that how we choose these negatives greatly impacts the representation quality. With harder negatives, the encoder is encouraged to capture more granular information that may improve performance on downstream tasks. While research in contrastive learning has explored architectures, augmentations, and pretext tasks, there has been little attention given to the negative sampling procedure. Meanwhile, there is a rich body of work in deep metric learning showing semi-hard negative mining to improve the efficacy of triplet losses. Inspired by this, we hope to bring harder negative sampling to modern contrastive learning.</p><p>Naively choosing difficult negatives may yield an objective that no longer bounds mutual information, removing a theoretical connection that is core to contrastive learning and has been shown to be important for downstream performance <ref type="bibr" target="#b37">(Tian et al., 2020)</ref>. In this paper, we present a new esti-</p><formula xml:id="formula_0">I(X; Y ) I NCE (X; Y ) = E xi,yi⇠p(x,y) E y 1:k ⇠p(y)</formula><p>" log e f ✓ (xi,yi)</p><formula xml:id="formula_1">1 k+1 P j2{i,1:k} e f ✓ (xi,yj ) #<label>(1)</label></formula><p>where x, y are realizations of two random variables, X and Y , and f ✓ : X ⇥ Y ! R is a similarity function. We call y 1:k = {y 1 , . . . y k } negative examples, being other realizations of Y .</p><p>Suppose the two random variables in Eq. 1 are both transformations of a common random variable X. Let T be a family of transformations where each member t is a composition of cropping, color jittering, gaussian blurring, among others <ref type="bibr" target="#b44">(Wu et al., 2018;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020a)</ref>. We call a transformed input t(x) a "view" of x. Let p(t) denote a distribution over T , a common choice being uniform. Next, introduce an encoder g ✓ : X ! S n 1 that maps an example to a L 2 -normalized representation. Suppose we have a dataset D = {x i } n i=1 of n values for X sampled from a distribution p(x). Then, the contrastive objective for the i-th example is:</p><formula xml:id="formula_2">L(x i ) = E t,t 0 ,t 1:k ⇠p(t) E x 1:k ⇠p(x) " log e g ✓ (t(xi)) T g ✓ (t 0 (xi))/⌧ 1 k+1 P j2{i,1:k} e g ✓ (t(xi)) T g ✓ (tj (xj ))/⌧ # (2)</formula><p>where ⌧ is a temperature. The equivalence of Eq. 2 to NCE is immediate given f ✓ (x, y) = g ✓ (x) T g ✓ (y)/⌧ . Maximizing Eq. 2 chooses an embedding that pulls two views of the same example together while pushing two views of distinct examples apart. A drawback to this framework is that the number of negatives k must be large to faithfully approximate the true partition. In practice, k is limited by memory. Recent innovations have focused on tackling this challenge:</p><p>Instance Discrimination <ref type="bibr" target="#b44">(Wu et al., 2018)</ref>, or IR, introduces a memory bank of n entries to cache embeddings of each example throughout training. Since every epoch we observe each example once, the memory bank will save the embedding of the view of the i-th example observed last epoch in its ith entry. Representations stored in the memory bank are removed from the automatic differentiation tape, but in return, we can choose a large k by querying M . A follow up work, Contrastive Multiview Coding <ref type="bibr" target="#b36">(Tian et al., 2019)</ref>, or CMC, decomposes an image into two color modalities. Then, CMC sums two IR losses where the memory banks for each modality are swapped.</p><p>Momentum Contrast <ref type="bibr" target="#b18">(He et al., 2019)</ref>, or MoCo, observed that the representations stored in the memory bank grow stale, since possibly thousands of optimization steps pass before updating an entry again. So, MoCo makes two important changes. First, it replaces the memory bank with a first-in first-out (FIFO) queue of size k. During each minibatch, representations are cached into the queue while the most stale ones are removed. Second, MoCo introduces a second (momentum) encoder g 0 ✓ 0 as a copy of g ✓ . The primary encoder g ✓ is used to embed one view of x i whereas the momentum encoder is used to embed the other. Again, gradients are not propagated to g 0 ✓ 0 . In this work, we focus on contrastive algorithms that utilize a memory structure that we repurpose in Sec. 4 to efficiently sample hard negatives from. In Sec. 7, we briefly discuss generalizations to contrastive algorithms that do not use a memory structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONDITIONAL NOISE CONTRASTIVE ESTIMATION</head><p>In NCE, the negative examples are sampled i.i.d. from the marginal distribution, p(y). Indeed, the existing proof that NCE lower bounds mutual information <ref type="bibr" target="#b32">(Poole et al., 2019)</ref> assumes this to be true. However, choosing negatives in this manner may not be the best choice for learning a good representation. For instance, prior work in metric learning has shown the effectiveness of semihard negative mining in optimizing triplet losses <ref type="bibr" target="#b42">(Wu et al., 2017;</ref><ref type="bibr" target="#b47">Yuan et al., 2017;</ref><ref type="bibr" target="#b34">Schroff et al., 2015)</ref>. We similarly wish to exploit choosing semi-hard negatives in NCE conditional on the current example but to do so in a manner that preserves the lower bound on mutual information. In presenting the theory, we assume two random variables X and Y , deriving a general bound; we will return to the contrastive learning setting in Sec. 4. To begin, in Eq. 1, suppose we sample negatives from a distribution q(y|x) conditional on a value x ⇠ p(x) rather than the marginal p(y), which is independent of X. Ideally, we would like to freely choose q(y|x) to be any distribution but not all choices preserve a bound on mutual information<ref type="foot" target="#foot_0">1</ref> . This does not, however, imply that we can only sample negatives from p(y) <ref type="bibr" target="#b32">(Poole et al., 2019;</ref><ref type="bibr" target="#b29">Oord et al., 2018)</ref>. One of our contributions is to formally define a family of conditional distributions Q such that for any q(y|x) 2 Q, drawing negative examples from q defines an estimator that lower bounds I(X; Y ). We call this new bound the Conditional Noise Contrastive Estimator, or CNCE. We first prove CNCE to be a bound:  x,y) ], the expected exponentiated similarity. Pick a set B ⇢ R strictly lower-bounded by c. Assume the pulled back set S B = {y|e f (x,y) 2 B} has non-zero probability (i.e. p(S B ) &gt; 0). For A 1 , . . . , A k in the Borel -algebra over R d , define  log e f (x,y)</p><formula xml:id="formula_3">A = A 1 ⇥ . . . ⇥ A k and let q((Y 1 , . . . , Y k ) 2 A|X = x) = k Y j=1 p(A j |S B ). Let I CNCE (X; Y ) = E x,</formula><formula xml:id="formula_4">1 k P k j=1 e f (x,y j ) . Then I CNCE  I NCE .</formula><p>Proof. To show I CNCE  I NCE , we show E p [log ,yj ) ]. To see this, apply Jensen's to the left-hand side of log E p [ P k j=1 e f (x,yj ) ] &lt; log ,yj ) , which holds if y j 2 S B for j = 1, . . . , k, and then take the expectation E q of both sides. The last inequality holds by monoticity of log, linearity of expectation, and the fact that x,yj ) ]  e f (x,yj ) . Theorem Intuition. For intuition, although using arbitrary negative distributions in NCE does not bound mutual information, we have found a restricted class of distributions Q where every member q(y|x) "subsets the support" of the distribution p(y). That is, given some fixed value x, we have defined q(y|x) to constrain the support of p(y) to a set S B whose members are "close" to x as measured by the similarity function f . For every element y 2 S B , the distribution q(y|x) wants to assign to it the same probability as p(y). However, as q(y|x) is not defined outside of S B , we must renormalize it to sum to one (hence p(A j |S B ) = p(Aj \S B ) p(S B ) ). Intuitively, q(y|x) cannot change p(y) too much: it must redistribute mass proportionally. The primary distinction then, is the smaller support of q(y|x), which forces samples from it to be harder for f to distinguish from x. Thm. 3.1 shows that substituting q(y|x) for p(y) in NCE still bounds mutual information. </p><formula xml:id="formula_5">P k j=1 e f (x,yj ) ] &lt; E q [log P k j=1 e f (x</formula><formula xml:id="formula_6">P k j=1 e f (x</formula><formula xml:id="formula_7">E p [e f (</formula><formula xml:id="formula_8">(x ) i t(x ) i t'(x ) i t(x ) j (a) IR, CMC, MoCo q(x |t(x )) t'(x ) i t(x ) i i j t(x ) j (b) Ring t(x ) j t'(x ) i t(x ) i t(x ) j t'(x ) i t(x ) i t(x ) j t'(x ) i t(x ) i (c) Annealed Ring</formula><p>Figure <ref type="figure">1</ref>: Visual illustration of Ring Discrimination. Black: view of example x i ; gray: second view of x i ; red: negative samples; gray area: distribution q(x|t(x i )). In subfigure (c), the negative samples are annealed to be closer to t(x i ) through training. In other words, the support of q shrinks. Theorem Example 3.1. We give a concrete example for the choice B that will be used in Sec. 4. For any realization x, suppose we define two similarity thresholds ! `, ! u 2 R where c &lt; ! `&lt; ! u . Then, choose B = [w `, w u ]. In this case, the set S B , which defines the support of the distribution q(y|x), contains values of y that are not "too-close" to x but not "too-far". In contrastive learning, we might pick these similarity thresholds to vary the difficulty of negative samples. Interestingly, Thm. 3.1 states that CNCE is looser than NCE, which raises the question: when is a looser bound useful? In reply, we show that while CNCE is a more biased estimator than NCE, in return it has lower variance. Intuitively, because q(y|x) is the result of restricting p(y) to a smaller support, samples from q(y|x) have less opportunity to deviate, hence lower variance. Formally: Theorem 3.2. (Bias and Variance Tradeoff) Pick any x, y ⇠ p(x, y). Fix the distribution q(y 1:k |x)</p><formula xml:id="formula_9">as stated in Theorem 3.1. Define a new random variable Z(y 1:k ) = log ✓ e f (x,y) 1 k P k j=1 e f (x,y j ) ◆ repre-</formula><p>senting the normalized similarity. By Theorem 3.1, the expressions</p><formula xml:id="formula_10">E p(y 1:k ) [Z] and E q(y 1:k |x) [Z] are estimators for I(X; Y ). Suppose that the set S B is chosen to ensure Var q(y 1:k |x) [Z]  Var q(y 1:k |x) [Z], where q(A) = p(A| complement of S B ).</formula><p>That is, we assume the variance of the normalized similarity when using y 1:k 2 S B is smaller than when using y 1:k / 2 S B . Then Bias p(y 1:k ) (Z)  Bias q(y 1:k |x) (Z) and Var p(y 1:k ) (Z) Var q(y 1:k |x) (Z).</p><p>The proof can be found in Sec. A.2. Thm. 3.2 provides one answer to our question of looseness. In stochastic optimization, a lower variance objective may lead to better local optima. For representation learning, using CNCE to sample more difficult negatives may (1) encourage the representation to distinguish fine-grained features useful in transfer tasks, and (2) provide less noisy gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RING DISCRIMINATION</head><p>We have shown CNCE to be a new bound on the mutual information that uses hard negative samples. Now we wish to apply CNCE to contrastive learning where the two random variables are again transformations of a single variable X. In this setting, for a fixed x i ⇠ p(x), the CNCE distribution is written as q(x|t(x i )) for some transform t 2 T . Samples from x ⇠ q(x|t(x i )) will be such that the exponentiated distance, exp{g</p><formula xml:id="formula_11">✓ (t(x i )) T g ✓ (t 0 (x))}, is at least a minimum value c. As in Example 3.1, we will choose B = [! `, ! u ], a closed interval in R defined by two thresholds.</formula><p>Picking thresholds. We pick the thresholds conditioned on the i-th example in the dataset, hence each example has a different set B. We first describe how to pick the upper threshold ! u . Given the i-example x i , we pick a number u 2 [0, 100] representing an upper "percentile". We consider each example x in the dataset to be in the support S B if and only if the (exponentiated) distance between the embedding of x i and x, or exp{g ✓ (t(x i )) T g ✓ (t 0 (x))}, is below the u-th percentile for all x 2 D. Call this maximum distance ! u . In other words, we construct q(x|t(x i )) such that we ignore examples from the dataset whose embedding dot producted with the embedding of x i is above ! u . (Note that u = 100 recovers NCE.) For a small enough choice of u, the upper similarity threshold ! u will be greater than c (defined in Thm. 3.1 as the expected distance with respect to p(x)), and the samples from q(x|t(x i )) will be harder negatives to discriminate from x i .</p><p>In picking the lower threshold ! `, one could choose it to be 0, so B = [0, ! u ). However, picking the closest examples to t(x i ) as its negative examples may be inappropriate, as these examples might be better suited as positive views rather than negatives <ref type="bibr" target="#b49">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b45">Xie et al., 2020)</ref>. As an extreme case, if the same image is included in the dataset twice, we would not like to select it as a negative example for itself. Furthermore, choosing negatives "too close" to the current instance may result in representations that pick up on fine-grain details only, ignoring larger semantic concepts. This suggests removing examples from q(x|t(x i )) we consider "too close" to x i . To do this, we pick a lower percentile 0  `&lt; u. For each example x 2 D, we say it is in S B if exp{g ✓ (t(x i )) T g ✓ (t 0 (x))} is below ! u but also if it is above the `-th percentile of all distances with respect to D. Call this minimum distance ! `. Fig. <ref type="figure">2</ref> visualizes this whole procedure.</p><formula xml:id="formula_12">dist(x ,x )= i 1 =dist(x ,x ) i 1 dist(x ,x )= i 1 q(x|t(x )) i T</formula><p>Step 3: Sort distances.</p><p>Step 4: Compute threhsolds.</p><p>Step 5: Define distribution q.</p><p>l-th u-th</p><formula xml:id="formula_13">S B l w w u R 5 4 1 3 4 x x x x x</formula><p>Step 1: Pick two percentiles.</p><p>Step 2: Compute distances.</p><p>dist(x,y)= e g(t(x)) g(t'(y)) T 0 l u 1 0 0</p><formula xml:id="formula_14">1 2 3 4 5 x x x x x x p(x) i S B x i i 5 d i s t ( x , x ) i 1 d i s t ( x , x ) Figure 2: Defining the CNCE distribution q(x|t(x i ))</formula><p>. By choosing a lower and upper percentile ànd u, we implicitly define similarity thresholds ! `and ! u to construct a support of valid negative examples, S B , which in turn, defines the distribution q(x|t(x i )).</p><p>Algorithm 1: MoCoRing # g q , g k : e n c o d e r n e t w o r k s # m: momentum ; t : t e m p e r a t u r e # u : r i n g up pe r p e r c e n t i l e # l : r i n g lo we r p e r c e n t i l e t x 1 =aug ( x ) # random a u g m e n t a t i o n t x 2 =aug ( x ) emb1=norm ( g q ( tx1 ) ) emb2=norm ( g k ( tx2 ) ) . d e t a c h ( ) dps =sum ( tx1⇤ t x 2 ) / t # d o t p r o d u c t # s o r t from c l o s e s t t o f a r t h e s t neg a l l d p s = s o r t ( emb1@queue . T / t ) # f i n d i n d i c e s o f t h r e s h o l d s i x l = l ⇤ l e n ( queue ) i x u =u⇤ l e n ( queue ) r i n g d p s = a l l d p s [ : , i x l : ix u ] # n o n p a r a m e t r i c s o f t m a x l o s s= dps + logsumexp ( r i n g d p s ) l o s s . backward ( ) s t e p ( g q . params ) # moco u p d a t e s g k . params = m⇤g k . params+\</p><p>(1 m)⇤ g q . params enqueue ( queue , emb2 ) ; dequeue ( queue ) # t h r e s h o l d u p d a t e s a n n e a l ( w l ) ; anneal ( w u ) Ring Discrimination. Having defined ! `and ! u , we have a practical method of choosing B, and thus S B to define q(x|t(x i )) for i-th example. Intuitively, we construct a conditional distribution for negative examples that are (1) not too easy since their representations are fairly similar to that of x i and ( <ref type="formula">2</ref>) not too hard since we remove the "closest" instances to x i from S B . We call this algorithm Ring Discrimination, or Ring, inspired by the shape of negative set (see Fig. <ref type="figure">1</ref>).</p><p>Ring can be easily added to popular contrastive algorithms. For IR and CMC, this amounts to simply sampling entries in the memory bank that fall within the `-th to u-th percentile of all distances to the current example view (in representation space). Similarly, for MoCo, we sample from a subset of the queue (chosen to be in the `-th to u-th percentile), preserving the FIFO ordering. In our experiments, we refer to these as IRing, CM-CRing, MoCoRing, respectively. Alg. 1 shows PyTorch-like pseudocode for MoCoRing. One of the strengths of this approach is the simplicity: the algorithm requires only a few lines of code on top of existing implementations.</p><p>Annealing Policy. Naively using hard negatives can collapse to a poor representation, especially if we choose the upper threshold, ! u , to be very small early in training. At the start of training, the encoder g ✓ is randomly initialized and cannot guarantee that elements in the `-th to u-th percentile are properly calibrated: if the representations are near random, choosing negatives that are close in embedding distance may detrimentally exclude those examples that are "actually" close. This could lock in poor local minima. To avoid this, we propose to use an annealing policy that reduces ! u (and thus the size of the support S B ) throughout training. Early in training we choose ! u to be large. Over many epochs, we slowly decrease ! u closer to ! l , thereby selecting more difficult negatives. We explored several annealing policies and found a linear schedule to be well-performing and simple (see Appendix E). In our experiments, annealing is shown to be crucial: being too aggressive with negatives early in training produced representations that performed poorly on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We explore our method applied to IR, CMC, and MoCo in four commonly used visual datasets. As in prior work <ref type="bibr" target="#b44">(Wu et al., 2018;</ref><ref type="bibr" target="#b49">Zhuang et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b27">Misra &amp; Maaten, 2020;</ref><ref type="bibr" target="#b19">Hénaff et al., 2019;</ref><ref type="bibr" target="#b23">Kolesnikov et al., 2019;</ref><ref type="bibr" target="#b10">Donahue &amp; Simonyan, 2019;</ref><ref type="bibr" target="#b1">Bachman et al., 2019;</ref><ref type="bibr" target="#b36">Tian et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020a)</ref>, we evaluate each method by linear classification on frozen embeddings. That is, we optimize a contrastive objective on a pretraining dataset to learn a representation; then, using a transfer dataset, we fit logistic regression on representations only. A better representation would contain more "object-centric" information, thereby achieving a higher classification score.</p><p>Training Details. We pick the upper percentile u = 10 and the lower percentile `= 1 although we anneal u starting from 100. We resize input images to be 256 by 256 pixels, and normalize them using dataset mean and standard deviation. The temperature ⌧ is set to 0.07. We use a composition of a 224 by 224-pixel random crop, random color jittering, random horizontal flip, and random grayscale conversion as our augmentation family T . We use a ResNet-18 encoder with a output dimension of 128. For CMC, we use two ResNet-18 encoders, doubling the number of parameters. For linear classification, we treat the pre-pool output (size 512 ⇥ 7 ⇥ 7) after the last convolutional layer as the input to the logistic regression. Note that this setup is equivalent to using a linear projection head <ref type="bibr" target="#b6">(Chen et al., 2020a;</ref><ref type="bibr">b)</ref>. In pretraining, we use SGD with learning rate 0.03, momentum 0.9 and weight decay 1e-4 for 300 epochs and batch size 256 (128 for CMC). We drop the learning rate twice by a factor of 10 on epochs 200 and 250. In transfer, we use SGD with learning rate 0.01, momentum 0.9, and no weight decay for 100 epochs without dropping learning rate. Future work can explore orthogonal factors such as choice of architecture or pretext task.  Ablations: Annealing and Upper Boundary. Having found good performance with Ring Discrimination, we want to assess the importance of the individual components that comprise Ring. We focus on the annealing policy and the exclusion of very close negatives from S B . Concretely, we measure the transfer accuracy of (1) IRing without annealing and (2) IRing with an lower percentile `= 0, thereby excluding no close negatives. That is, S B contains all examples in the dataset with representation similarity less than the ! u (a "ball" instead of a "ring"). Table <ref type="table" target="#tab_2">2</ref> compares these ablations to IR and full IRing on CIFAR10 and ImageNet classification transfer. We observe that both ablations result in worse transfer accuracy, with proper annealing being especially important to prevent convergence to bad minima. We also find even with `= 0, IRing outperforms IR, suggesting both removing negatives that are "too close" and "too far" contribute to the improved representation quality.</p><p>Transferring Features. Thus far we have only evaluated the learned representations on unseen examples from the training distribution. As the goal of unsupervised learning is to capture general representations, we are also interested in their performance on new, unseen distributions. To gauge this, we use the same linear classification paradigm on a suite of image datasets from the "Meta Dataset" collection <ref type="bibr" target="#b38">(Triantafillou et al., 2019)</ref> that have been used before in contrastive literature <ref type="bibr" target="#b6">(Chen et al., 2020a)</ref>. All representations were trained on CIFAR10.</p><p>For each transfer dataset, we compute mean and variance from a training split to normalize input images, which we found important for generalization to new visual domains. We find in Table <ref type="table" target="#tab_3">3</ref> that the Ring models are competitive with the non-Ring analogues, with increases in transfer accuracies of 0.5 to 2% absolute. Most notable are the TrafficSign and VGGFlower datasets in which Ring models surpass others by a larger margin. We also observe that IRing largely outperforms LA. This suggests the features learned with more difficult negatives are not only useful for the training distribution but may also be transferrable to many visual datasets.</p><p>More Downstream Tasks. Object classification is a popular transfer task, but we want our learned representations to capture holistic knowledge about the contents of an image. We must thus evaluate performance on transfer tasks such as detection and segmentation that require different kinds of visual information. We study four additional downstream tasks: object detection on COCO <ref type="bibr" target="#b25">(Lin et al., 2014)</ref> and Pascal VOC'07 <ref type="bibr" target="#b12">(Everingham et al., 2010)</ref>, instance segmentation on COCO, and keypoint detection on COCO. In all cases, we employ embeddings trained on ImageNet with a ResNet-18 encoder. We base these experiments after those found in <ref type="bibr" target="#b18">He et al. (2019)</ref> with the same hyperparameters. However, we use a smaller backbone (ResNet-18 versus ResNet-50) and we freeze its parameters instead of finetuning them. We adapt code from Detectron2 <ref type="bibr" target="#b43">(Wu et al., 2019)</ref>. We find IRing outperforms IR by around 2.3 points in COCO object detection, 2.5 points in COCO Instance Segmentation, 2.6 points in COCO keypoint detection, and 2.1 points in VOC object detection. Similarly, MoCoRing finds consistent improvements of 1-3 points over MoCo on the four tasks. Future work can investigate orthogonal directions of using larger encoders (e.g. ResNet-50) and finetuning ResNet parameters for these individual tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>Several of the ideas in Ring Discrimination relate to existing work. Below, we explore these connections, and at the same time, place our work in a fast-paced and growing field.</p><p>Hard negative mining. While it has not been deeply explored in modern contrastive learning, negative mining has a rich line of research in the metric learning community. Deep metric learning utilizes triplet objectives of the form</p><formula xml:id="formula_15">L triplet = d(g ✓ (x i ), g ✓ (x + )) d(g ✓ (x i ), g ✓ (x )+↵)</formula><p>where d is a distance function (e.g. L 2 distance), x + and x are a positive and negative example, respectively, relative to x i , the current instance, and ↵ 2 R + is a margin. In this context, several approaches pick semi-hard negatives: <ref type="bibr" target="#b34">Schroff et al. (2015)</ref> treats the furthest (in L 2 distance) example in the same minibatch as x i as its negative, whereas Oh <ref type="bibr" target="#b28">Song et al. (2016)</ref> weight each example in the minibatch by its distance to g ✓ (x i ), thereby being a continuous version of <ref type="bibr" target="#b34">Schroff et al. (2015)</ref>. More sophisticated negative sampling strategies developed over time. In <ref type="bibr" target="#b42">Wu et al. (2017)</ref>, the authors pick negatives from a fixed normal distribution that is shown to approximate L 2 normalized embeddings in high dimensions. The authors show that weighting by this distribution samples more diverse negatives. Similarly, HDC <ref type="bibr" target="#b47">(Yuan et al., 2017)</ref> simulataneously optimizes a triplet loss using many levels of "hardness" in negatives, again improving the diversity. Although triplet objectives paved the way for modern NCE-based objectives, the focus on negative mining has largely been overlooked. Ring Discrimination, being inspired by the deep metric learning literature, reminds that negative sampling is still an effective way of learning stronger representations in the new NCE framework. As such, an important contribution was to do so while retaining the theoretical properties of NCE, namely in relation to mutual information. This, to the best of our knowledge, is novel as negative mining in metric learning literature was not characterized in terms of information theory.</p><p>That being said, there are some cases of negative mining in contrastive literature. In CPC <ref type="bibr" target="#b29">(Oord et al., 2018)</ref>, the authors explore using negatives from the same speaker versus from mixed speakers in audio applications, the former of which can be interpreted as being more difficult. A recent paper, InterCLR <ref type="bibr" target="#b45">(Xie et al., 2020)</ref>, also finds that using "semi-hard negatives" is beneficial to contrastive learning whereas negatives that are too difficult or too easy produce worse representations. Where InterCLR uses a margin-based approach to sample negatives, we explore a wider family of negative distributions and show analysis that annealing offers a simple and easy solution to choosing between easy and hard negatives. Further, as InterCLR's negative sampling procedure is a special case of CNCE, we provide theory grounding these approaches in information theory. Finally, a separate line of work in contrastive learning explores using neighboring examples (in embedding space) as "positive" views of the instance <ref type="bibr" target="#b49">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b45">Xie et al., 2020;</ref><ref type="bibr" target="#b0">Asano et al., 2019;</ref><ref type="bibr" target="#b4">Caron et al., 2020;</ref><ref type="bibr" target="#b24">Li et al., 2020)</ref>. That is, finding a set {x j } such that we consider x j = t(x i ) for the current instance x i . While this does not deal with negatives explicitly, it shares similarities to our approach by employing other examples in the contrastive objective to learn better representations.</p><p>In the Appendix, we discuss how one of these algorithms, LA <ref type="bibr" target="#b49">(Zhuang et al., 2019)</ref>, implicitly uses hard negatives and expand the Ring family with ideas inspired by it.</p><p>Contrastive learning. We focused primarily on comparing Ring Discrimination to three recent and highly performing contrastive algorithms, but the field contains much more. The basic idea of learning representations to be invariant under a family of transformations is an old one, having been explored with self-organizing maps <ref type="bibr" target="#b3">(Becker &amp; Hinton, 1992)</ref> and dimensionality reduction <ref type="bibr" target="#b16">(Hadsell et al., 2006)</ref>. Before IR, the idea of instance discrimination was studied <ref type="bibr" target="#b11">(Dosovitskiy et al., 2014;</ref><ref type="bibr" target="#b40">Wang &amp; Gupta, 2015)</ref> among many pretext objectives such as position prediction <ref type="bibr" target="#b9">(Doersch et al., 2015)</ref>, color prediction <ref type="bibr" target="#b48">(Zhang et al., 2016)</ref>, multi-task objectives <ref type="bibr" target="#b8">(Doersch &amp; Zisserman, 2017)</ref>, rotation prediction <ref type="bibr" target="#b14">(Gidaris et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref>, and many other "pretext" objectives <ref type="bibr" target="#b31">(Pathak et al., 2017)</ref>. As we have mentioned, one of the primary challenges to instance discrimination is making such a large softmax objective tractable. Moving from a parametric <ref type="bibr" target="#b11">(Dosovitskiy et al., 2014)</ref> to a nonparametric softmax reduced issues with vanishing gradients, shifting the challenge to efficient negative sampling. The memory bank approach <ref type="bibr" target="#b44">(Wu et al., 2018)</ref> is a simple and memory-efficient solution, quickly being adopted by the research community <ref type="bibr" target="#b49">(Zhuang et al., 2019;</ref><ref type="bibr" target="#b36">Tian et al., 2019;</ref><ref type="bibr" target="#b18">He et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2020b;</ref><ref type="bibr" target="#b27">Misra &amp; Maaten, 2020)</ref>. With enough computational resources, it is now also possible to reuse examples in a large minibatch and negatives of one another <ref type="bibr" target="#b46">(Ye et al., 2019;</ref><ref type="bibr" target="#b22">Ji et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2020a)</ref>. In our work, we focus on hard negative mining in the context of a memory bank or queue due to its computational efficiency. However, the same principles should be applicable to batch-based methods (e.g. SimCLR): assuming a large enough batch size, for each example, we only use a subset of the minibatch as negatives as in Ring.</p><p>Finally, more recent work <ref type="bibr" target="#b15">(Grill et al., 2020)</ref> removes negatives altogether, which is speculated to implicitly use negative samples via batch normalization <ref type="bibr" target="#b21">(Ioffe &amp; Szegedy, 2015)</ref>; we leave a more thorough understanding of negatives in this setting to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSION</head><p>Computational cost of Ring. To measure the cost of CNCE, we compare the cost an epoch of training MoCo/IR versus MoCoRing/IRing on four image datasets.   <ref type="bibr" target="#b2">(Becker et al., 2018)</ref> 87.4 91.3 (+3.9) Google Commands <ref type="bibr" target="#b41">(Warden, 2018)</ref> 38.5 41.4 (+2.9) Fluent Actions <ref type="bibr" target="#b26">(Lugosch et al., 2019)</ref> 36.5 36.8 (+0.3) Fluent Objects <ref type="bibr" target="#b26">(Lugosch et al., 2019)</ref> 41.9 44.1 (+2.2) Fluent Locations <ref type="bibr" target="#b26">(Lugosch et al., 2019)</ref> 60.9 63.9 (+3.0) (a) Speech Extension  reuse embeddings and (2) gradients are not propagated through the memory structure, the additional compute of Ring amounts to one matrix multiplication, which is cheap on modern hardware. We used a single Titan X GPU with 8 CPU workers, and PyTorch Lightning <ref type="bibr" target="#b13">(Falcon et al., 2019)</ref>.</p><p>Generalization to other modalities. We have focused on visual representation learning, although the same ideas apply to other domains. To exemplify the generality of CNCE, we apply MoCoRing to learning speech representations. Table <ref type="table" target="#tab_8">6a</ref> reports linear evaluation on six transfer datasets, ranging from predicting speaker identity to speech recognition to intent prediction. We find significant gains of 1 to 4 percent over 4 datasets and 6 transfer tasks with an average of 2.2% absolute points.</p><p>Experiment Details. We adapt the experimental setup from <ref type="bibr" target="#b35">Tamkin et al. (2020)</ref>: a contrastive representation is trained with the LibriSpeech 100 hour corpus <ref type="bibr" target="#b30">(Panayotov et al., 2015)</ref> in which waveforms are truncated to 150,526 timesteps and processed to log Mel spectrograms with hop length 2,360 to output a matrix of size 64 by 64. Spectrograms are z-scored using training statistics. We use masking of time and frequency features as augmentations on training examples. Spectrograms are encoded with a ResNet18 to a 128-dim. embedding. In transfer, we also preprocess waveforms to a 64 by 64 matrix. Spectral augmentations are used in training the Logistic regression model but no augmentations are used when evaluating the test split. The input to the logistic regression model is the ResNet18 pre-pool features prior to the final linear layer.</p><p>Batch-based negative sampling. In Ring, we assumed to have a memory structure that stores embeddings, which led to an efficient procedure of mining semi-hard negatives. However, another flavor of contrastive algorithms removes the memory structure entirely, using the examples in the minibatch as negatives of one another. Here, we motivate a possible extension of Ring to SimCLR, and leave more careful study to future work. In SimCLR, we are given a minibatch M of examples. To sample hard negatives, as before, pick `and u as lower and upper percentiles. For every example x i in the minibatch, only consider the subset of the minibatch {x : x ✓ M, exp{g ✓ (t(x i )) T g ✓ (t 0 (x))} in the `-th and u-th percentiles in M } as negative examples for x i . This can be efficiently implemented as a matrix operation using an element-wise mask. Thus, we ignore gradient signal for examples too far or too close to x i in representation. As before, we anneal u from 100 to 10 and set `= 1. Table <ref type="table" target="#tab_8">6b</ref> report consistent but moderate gains over SimCLR, showing promise but room for improvement in future research.</p><p>To conclude, we presented a family of mutual information estimators that approximate the partition function using samples from a class of conditional distributions. We proved several theoretical statements about this family, showing a bound on mutual information and a tradeoff between bias and variance. Then, we applied these estimators as objectives in contrastive representation learning.</p><p>In doing so, we found that our representations outperform existing approaches consistently across a spectrum of contrastive objectives, data distributions, and transfer tasks. Overall, we hope our work to encourage more exploration of negative sampling in the recent growth of contrastive learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Theorem 3.1. (The Conditional NCE bound) Define d-dimensional random variables X and Y by a joint distribution p(x, y) and let Y 1 , ..., Y k be i.i.d. copies of Y with the marginal distribution p(y). Fix any function f : (X, Y ) ! R, any realization x of X, and let c = E y⇠p(y) [e f (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>y⇠p(x,y) E y1,...,y k ⇠q(y1,...,y k |x)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of contrastive algorithms on four image domains. Superscript (</figDesc><table><row><cell>Model</cell><cell cols="2">Linear Evaluation</cell><cell>Model</cell><cell>Linear Evaluation</cell><cell>Model</cell><cell>Linear Evaluation</cell><cell>Model</cell><cell>Linear Evaluation</cell></row><row><cell>IR</cell><cell>81.2</cell><cell></cell><cell>IR</cell><cell>60.4</cell><cell>IR</cell><cell>61.4</cell><cell>IR</cell><cell>43.2</cell></row><row><cell>IRing CMC ⇤ CMCRing ⇤</cell><cell cols="2">83.9 (+2.7) 85.6 87.6 (+2.0)</cell><cell>IRing CMC ⇤ CMCRing ⇤</cell><cell>62.3 (+1.9) 56.0 56.0 (+0.0)</cell><cell>IRing CMC ⇤ CMCRing ⇤</cell><cell>64.3 (+2.9) 63.8 66.4 (+2.6)</cell><cell>IRing CMC ⇤ CMCRing ⇤</cell><cell>48.4 (+5.2) 48.2 50.4 (+2.2)</cell></row><row><cell>MoCo</cell><cell>83.1</cell><cell></cell><cell>MoCo</cell><cell>59.1</cell><cell>MoCo</cell><cell>63.8</cell><cell>MoCo</cell><cell>52.8</cell></row><row><cell>MoCoRing</cell><cell cols="2">86.1 (+3.0)</cell><cell>MoCoRing</cell><cell>61.5 (+2.4)</cell><cell>MoCoRing</cell><cell>65.2 (+1.4)</cell><cell>MoCoRing</cell><cell>54.6 (+1.8)</cell></row><row><cell>LA</cell><cell>83.9</cell><cell></cell><cell>LA</cell><cell>61.4</cell><cell>LA</cell><cell>63.0</cell><cell>LA</cell><cell>48.0</cell></row><row><cell cols="2">(a) CIFAR10</cell><cell></cell><cell cols="2">(b) CIFAR100</cell><cell cols="2">(c) STL10</cell><cell cols="2">(d) ImageNet</cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Linear Eval.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IR</cell><cell></cell><cell>81.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRing</cell><cell></cell><cell>83.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IRing (No Anneal)</cell><cell>81.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IRing (`= 0)</cell><cell>82.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">(a) CIFAR10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Linear Eval.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IR</cell><cell></cell><cell>43.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRing</cell><cell></cell><cell>48.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IRing (No Anneal)</cell><cell>41.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IRing (`= 0)</cell><cell>47.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">(b) ImageNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>⇤ ) indicates models that use twice as many parameters as others e.g. CMC has "L" and "ab" encoders.The results for CIFAR10, CIFAR100, STL10, and ImageNet are in Table1. Overall, IR, CMC, and MoCo all benefit from using more difficult negatives as shown by 2-5% absolute points of improvement across the four datasets. While we find different contrastive objectives to perform best in each dataset, the improvements from Ring are consistent: the Ring variant outperforms the base for every model and every dataset. We also include as a baseline Local Aggregation, or LA<ref type="bibr" target="#b49">(Zhuang et al., 2019)</ref>, a popular contrastive algorithm (see Sec. F) that implicitly uses hard negatives without annealing. We find our methods to outperform LA by up to 4% absolute.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">: Lesioning the effects of</cell></row><row><cell>annealing and choice of</cell><cell>`.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Transferring CIFAR10 embeddings to various image distributions.</figDesc><table><row><cell>Model</cell><cell>Aircraft</cell><cell>CUBirds</cell><cell>DTD</cell><cell>Fungi</cell><cell>MNIST</cell><cell>FashionMNIST</cell><cell>TrafficSign</cell><cell>VGGFlower</cell><cell>MSCOCO</cell></row><row><cell>IR</cell><cell>40.9</cell><cell>17.9</cell><cell>39.2</cell><cell>2.7</cell><cell>96.9</cell><cell>91.7</cell><cell>97.1</cell><cell>68.1</cell><cell>52.4</cell></row><row><cell>IRing</cell><cell>40.6 (-0.3)</cell><cell>17.9 (+0.0)</cell><cell>39.5 (+0.3)</cell><cell>3.4 (+0.7)</cell><cell>97.8 (+0.9)</cell><cell>91.6 (+0.1)</cell><cell>98.8 (+1.7)</cell><cell>68.5 (+0.4)</cell><cell>52.5 (+0.1)</cell></row><row><cell>MoCo</cell><cell>41.5</cell><cell>18.0</cell><cell>39.7</cell><cell>3.1</cell><cell>96.9</cell><cell>90.9</cell><cell>97.3</cell><cell>64.5</cell><cell>52.0</cell></row><row><cell>MoCoRing</cell><cell>41.6(+0.1)</cell><cell>18.6 (+0.6)</cell><cell>39.5 (-0.2)</cell><cell>3.6 (+0.5)</cell><cell>97.9 (+1.0)</cell><cell>91.3 (+0.4)</cell><cell>99.3 (+2.0)</cell><cell>69.1 (+4.6)</cell><cell>52.6 (+0.6)</cell></row><row><cell>CMC</cell><cell>40.1</cell><cell>15.8</cell><cell>38.3</cell><cell>4.3</cell><cell>97.5</cell><cell>91.5</cell><cell>94.6</cell><cell>67.1</cell><cell>51.4</cell></row><row><cell>CMCRing</cell><cell>40.8 (+0.7)</cell><cell>16.8 (+1.0)</cell><cell>40.6 (+2.3)</cell><cell>4.2 (-0.1)</cell><cell>97.9 (+0.4)</cell><cell>92.1 (+0.6)</cell><cell>97.1 (+2.5)</cell><cell>69.1 (+2.0)</cell><cell>52.1 (+0.7)</cell></row><row><cell>LA</cell><cell>41.3</cell><cell>17.8</cell><cell>39.0</cell><cell>2.3</cell><cell>97.2</cell><cell>92.3</cell><cell>98.2</cell><cell>66.9</cell><cell>52.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Evaluation of ImageNet representations using four visual transfer tasks.</figDesc><table><row><cell></cell><cell cols="3">COCO: Object Detection</cell><cell cols="3">COCO: Inst. Segmentation</cell><cell cols="5">COCO: Keypoint Detection</cell><cell cols="3">VOC: Object Detection</cell></row><row><cell>Arch.</cell><cell></cell><cell cols="4">Mask R-CNN, R 18 -FPN, 1x schedule</cell><cell></cell><cell cols="4">R-CNN, R 18 -FPN</cell><cell></cell><cell cols="3">Faster R-CNN, R 18 -C4</cell></row><row><cell>Model</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell><cell>AP mk</cell><cell>AP mk 50</cell><cell>AP mk 75</cell><cell>AP kp</cell><cell>AP</cell><cell>kp 50</cell><cell>AP</cell><cell>kp 75</cell><cell>AP bb</cell><cell>AP bb 50</cell><cell>AP bb 75</cell></row><row><cell>IR</cell><cell>8.6</cell><cell>19.0</cell><cell>6.6</cell><cell>8.5</cell><cell>17.4</cell><cell>7.4</cell><cell>34.6</cell><cell cols="2">63.0</cell><cell cols="2">32.9</cell><cell>5.5</cell><cell>14.5</cell><cell>3.3</cell></row><row><cell>IRing</cell><cell>10.9</cell><cell>22.9</cell><cell>8.7</cell><cell>11.0</cell><cell>20.9</cell><cell>9.6</cell><cell>37.2</cell><cell cols="2">66.1</cell><cell cols="2">35.7</cell><cell>7.6</cell><cell>20.3</cell><cell>4.4</cell></row><row><cell>MoCo</cell><cell>6.0</cell><cell>14.3</cell><cell>4.0</cell><cell>10.8</cell><cell>21.4</cell><cell>9.7</cell><cell>37.6</cell><cell cols="2">66.5</cell><cell cols="2">36.9</cell><cell>7.3</cell><cell>17.9</cell><cell>4.1</cell></row><row><cell>MoCoRing</cell><cell>9.4</cell><cell>20.3</cell><cell>7.6</cell><cell>12.0</cell><cell>22.9</cell><cell>10.8</cell><cell>38.7</cell><cell cols="2">67.7</cell><cell cols="2">37.9</cell><cell>8.0</cell><cell>22.1</cell><cell>4.8</cell></row><row><cell>LA</cell><cell>10.2</cell><cell>22.0</cell><cell>8.1</cell><cell>10.0</cell><cell>20.3</cell><cell>9.0</cell><cell>36.3</cell><cell cols="2">65.3</cell><cell cols="2">35.1</cell><cell>7.6</cell><cell>20.0</cell><cell>4.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Table5reports the average cost over 200 epochs. We observe that Ring models cost no more than 1.5 times the cost of standard contrastive algorithms, amounting to a difference of 3 to 7 minutes in ImageNet and 10 to 60 seconds in three other datasets per epoch. In the context of deep learning, we do not find the cost increases to be substantial. In particular, since (1) the memory structure in IR and MoCo allow us to store and</figDesc><table><row><cell>Model</cell><cell>Cost (sec.)</cell><cell>Model</cell><cell>Cost (sec.)</cell><cell>Model</cell><cell>Cost (sec.)</cell><cell>Model</cell><cell>Cost (min.)</cell></row><row><cell>IR IRing</cell><cell>136.0 ± 4 141.1 ± 5 (1.1x)</cell><cell>IR IRing</cell><cell>105.2 ± 3 154.0 ± 3 (1.4x)</cell><cell>IR IRing</cell><cell>13.5 ± 1 14.2 ± 1 (1.0x)</cell><cell>IR IRing</cell><cell>43.9 ± 1 51.0 ± 1 (1.2x)</cell></row><row><cell>MoCo MoCoRing</cell><cell>318.4 ± 16 383.4 ± 12 (1.2x)</cell><cell>MoCo MoCoRing</cell><cell>346.6 ± 3 371.8 ± 3 (1.1x)</cell><cell>MoCo MoCoRing</cell><cell>25.9 ± 3 34.1 ± 4 (1.3x)</cell><cell>MoCo MoCoRing</cell><cell>61.1 ± 1 64.9 ± 1 (1.1x)</cell></row><row><cell cols="2">(a) CIFAR10</cell><cell cols="2">(b) CIFAR100</cell><cell cols="2">(c) STL10</cell><cell cols="2">(d) ImageNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Cost of one training epoch in seconds, averaged over 200 epochs.</figDesc><table><row><cell>Transfer Task</cell><cell>MoCo</cell><cell>MoCoRing</cell></row><row><cell>LibriSpeech Spk. ID (Panayotov et al., 2015)</cell><cell>95.5</cell><cell>96.6 (+1.1)</cell></row><row><cell>AudioMNIST</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Generalizations of Ring to a new modality (a) and a batch-based algorithm (b).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">We provide a counterexample in Sec. A.1.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName><forename type="first">Yuki</forename><surname>Markus Asano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interpreting and explaining deep neural networks for classification of audio signals</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03418</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Self-organizing neural network that discovers surfaces in random-dot stereograms</title>
		<author>
			<persName><forename type="first">Suzanna</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="issue">6356</biblScope>
			<biblScope unit="page" from="161" to="163" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-supervised gans via auxiliary rotation loss</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12154" to="12163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A simple framework for contrastive learning of visual representations</title>
				<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020b</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large scale adversarial representation learning</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10542" to="10552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pytorch lightning. GitHub</title>
		<author>
			<persName><surname>Wea Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/williamFalcon/pytorch-lightningCitedby" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<pubPlace>Note</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07728</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Pierre H Richemond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaohan</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Daniel Guo</surname></persName>
		</author>
		<author>
			<persName><surname>Gheshlaghi Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aravind</forename><surname>Olivier J Hénaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Ch</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName><forename type="first">Loren</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikrant</forename><surname>Singh Tomar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName><forename type="first">Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2701" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><surname>Tucker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06922</idno>
		<title level="m">On variational bounds of mutual information</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
				<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Viewmaker networks: Learning views for unsupervised representation learning</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07432</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">On mutual information maximization for representation learning</title>
		<author>
			<persName><forename type="first">Josip</forename><surname>Michael Tschannen</surname></persName>
		</author>
		<author>
			<persName><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Paul K Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13625</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Yew Soon Ong, and Chen Change Loy. Delving into interimage invariance for unsupervised visual representations</title>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11702</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised embedding learning via invariant and spreading instance feature</title>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pong</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer vision and pattern recognition</title>
				<meeting>the IEEE Conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6210" to="6219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hard-aware deeply cascaded embedding</title>
		<author>
			<persName><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
				<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
				<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
