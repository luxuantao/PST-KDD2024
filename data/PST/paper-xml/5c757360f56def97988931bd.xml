<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
							<email>asaxe@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
							<email>ybansal@g.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Dapello</surname></persName>
							<email>dapello@g.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Madhu</forename><surname>Advani</surname></persName>
							<email>madvani@fas.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Artemy</forename><surname>Kolchinsky</surname></persName>
							<email>artemyk@gmail.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Santa Fe Institute</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brendan</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
							<email>tracey.brendan@gmail.com</email>
							<affiliation key="aff1">
								<address>
									<settlement>Santa Fe Institute</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
							<email>davidcox@fas.harvard.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Harvard University MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep neural networks <ref type="bibr" target="#b25">(Schmidhuber, 2015;</ref><ref type="bibr" target="#b18">LeCun et al., 2015)</ref> are the tool of choice for real-world tasks ranging from visual object recognition <ref type="bibr" target="#b16">(Krizhevsky et al., 2012)</ref>, to unsupervised learning <ref type="bibr" target="#b11">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b19">Lotter et al., 2016)</ref> and reinforcement learning <ref type="bibr" target="#b28">(Silver et al., 2016)</ref>. These practical successes have spawned many attempts to explain the performance of deep learning systems <ref type="bibr" target="#b12">(Kadmon &amp; Sompolinsky, 2016)</ref>, mostly in terms of the properties and dynamics of the optimization problem in the space of weights <ref type="bibr" target="#b24">(Saxe et al., 2014;</ref><ref type="bibr" target="#b8">Choromanska et al., 2015;</ref><ref type="bibr" target="#b1">Advani &amp; Saxe, 2017)</ref>, or the classes of functions that can be efficiently represented by deep networks <ref type="bibr" target="#b20">(Montufar et al., 2014;</ref><ref type="bibr" target="#b23">Poggio et al., 2017)</ref>. This paper analyzes a recent inventive proposal to study the dynamics of learning through the lens of information theory <ref type="bibr" target="#b29">(Tishby &amp; Zaslavsky, 2015;</ref><ref type="bibr" target="#b27">Shwartz-Ziv &amp; Tishby, 2017)</ref>. In this view, deep learning is a question of representation learning: each layer of a deep neural network can be seen as a set of summary statistics which contain some but not all of the information present in the input, while retaining as much information about the target output as possible. The amount of information in a hidden layer regarding the input and output can then be measured over the course of learning, yielding a picture of the optimization process in the information plane. Crucially, this method holds the promise to serve as a general analysis that can be used to compare different architectures, using the common currency of mutual information. Moreover, the elegant information bottleneck (IB) theory provides a fundamental bound on the amount of input compression and target output information that any representation can achieve <ref type="bibr" target="#b30">(Tishby et al., 1999)</ref>. The IB bound thus serves as a method-agnostic ideal to which different architectures and algorithms may be compared.</p><p>A preliminary empirical exploration of these ideas in deep neural networks has yielded striking findings <ref type="bibr" target="#b27">(Shwartz-Ziv &amp; Tishby, 2017)</ref>. Most saliently, trajectories in the information plane appear to consist of two distinct phases: an initial "fitting" phase where mutual information between the hidden layers and both the input and output increases, and a subsequent "compression" phase where mutual information between the hidden layers and the input decreases. It has been hypothesized that this compression phase is responsible for the excellent generalization performance of deep networks, and further, that this compression phase occurs due to the random diffusion-like behavior of stochastic gradient descent.</p><p>Here we study these phenomena using a combination of analytical methods and simulation. In Section 2, we show that the compression observed by <ref type="bibr" target="#b27">Shwartz-Ziv &amp; Tishby (2017)</ref> arises primarily due to the double-saturating tanh activation function used. Using simple models, we elucidate the effect of neural nonlinearity on the compression phase. Importantly, we demonstrate that the ReLU activation function, often the nonlinearity of choice in practice, does not exhibit a compression phase. We discuss how this compression via nonlinearity is related to the assumption of binning or noise in the hidden layer representation. To better understand the dynamics of learning in the information plane, in Section 3 we study deep linear networks in a tractable setting where the mutual information can be calculated exactly. We find that deep linear networks do not compress over the course of training for the setting we examine. Further, we show a dissociation between generalization and compression. In Section 4, we investigate whether stochasticity in the training process causes compression in the information plane. We train networks with full batch gradient descent, and compare the results to those obtained with stochastic gradient descent. We find comparable compression in both cases, indicating that the stochasticity of SGD is not a primary factor in the observed compression phase. Moreover, we show that the two phases of SGD occur even in networks that do not compress, demonstrating that the phases are not causally related to compression. These results may seem difficult to reconcile with the intuition that compression can be necessary to attain good performance: if some input channels primarily convey noise, good generalization requires excluding them. Therefore, in Section 5 we study a situation with explicitly task-relevant and task-irrelevant input dimensions. We show that the hidden-layer mutual information with the task-irrelevant subspace does indeed drop during training, though the overall information with the input increases. However, instead of a secondary compression phase, this task-irrelevant information is compressed at the same time that the taskrelevant information is boosted. Our results highlight the importance of noise assumptions in applying information theoretic analyses to deep learning systems, and put in doubt the generality of the IB theory of deep learning as an explanation of generalization performance in deep architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">COMPRESSION AND NEURAL NONLINEARITIES</head><p>The starting point for our analysis is the observation that changing the activation function can markedly change the trajectory of a network in the information plane. In Figure <ref type="figure">1A</ref>, we show our replication of the result reported by <ref type="bibr" target="#b27">Shwartz-Ziv &amp; Tishby (2017)</ref> for networks with the tanh nonlinearity. <ref type="foot" target="#foot_0">1</ref> This replication was performed with the code supplied by the authors of Shwartz-Ziv &amp; Tishby (2017), and closely follows the experimental setup described therein. Briefly, a neural network with 7 fully connected hidden layers of width 12-10-7-5-4-3-2 is trained with stochastic gradient descent to produce a binary classification from a 12-dimensional input. In our replication we used 256 randomly selected samples per batch. The mutual information of the network layers with respect to the input and output variables is calculated by binning the neuron's tanh output activations into 30 equal intervals between -1 and 1. Discretized values for each neuron in each layer are then used to directly calculate the joint distributions, over the 4096 equally likely input patterns and true output labels. In line with prior work <ref type="bibr" target="#b27">(Shwartz-Ziv &amp; Tishby, 2017)</ref>, the dynamics in Fig. <ref type="figure">1</ref>   <ref type="bibr">Kolchinsky &amp; Tracey (2017)</ref>; <ref type="bibr">Kolchinsky et al. (2017)</ref>, no compression is observed except in the final classification layer with sigmoidal neurons. See Appendix B for the KDE MI method applied to the original Tishby dataset; additional results using a second popular nonparametric k-NN-based method <ref type="bibr" target="#b15">(Kraskov et al., 2004)</ref>; and results for other neural nonlinearities.</p><p>transition between an initial fitting phase, during which information about the input increases, and a subsequent compression phase, during which information about the input decreases.</p><p>We then modified the code to train deep networks using rectified linear activation functions (f (x) = max(0, x)). While the activities of tanh networks are bounded in the range [−1, 1], ReLU networks have potentially unbounded positive activities. To calculate mutual information, we first trained the ReLU networks, next identified their largest activity value over the course of training, and finally chose 100 evenly spaced bins between the minimum and maximum activity values to discretize the hidden layer activity. The resulting information plane dynamics are shown in Fig. <ref type="figure">1B</ref>. The mutual information with the input monotonically increases in all ReLU layers, with no apparent compression phase. To see whether our results were an artifact of the small network size, toy dataset, or simple binning-based mutual information estimator we employed, we also trained larger networks on the MNIST dataset and computed mutual information using a state-of-the-art nonparametric kernel density estimator which assumes hidden activity is distributed as a mixture of Gaussians (see Appendix B for details). Fig. <ref type="figure">C-D</ref> show that, again, tanh networks compressed but ReLU networks did not. Appendix B shows that similar results also obtain with the popular nonparametric k-nearest-neighbor estimator of <ref type="bibr" target="#b15">Kraskov et al. (2004)</ref>, and for other neural nonlinearities. Thus, the choice of nonlinearity substantively affects the dynamics in the information plane.</p><p>To understand the impact of neural nonlinearity on the mutual information dynamics, we develop a minimal model that exhibits this phenomenon. In particular, consider the simple three neuron network shown in Fig. <ref type="figure" target="#fig_1">2A</ref>. We assume a scalar Gaussian input distribution X ∼ N (0, 1), which is fed through the scalar first layer weight w 1 , and passed through a neural nonlinearity f (•), yielding the hidden unit activity h = f (w 1 X). To calculate the mutual information with the input, this hidden unit activity is then binned yielding the new discrete variable T = bin(h) (for instance, into 30 evenly spaced bins from -1 to 1 for the tanh nonlinearity). This binning process is depicted in Fig. <ref type="figure" target="#fig_1">2B</ref>. In this simple setting, the mutual information I(T ; X) between the binned hidden layer activity T and the input X can be calculated exactly. In particular,</p><formula xml:id="formula_0">I(T ; X) = H(T ) − H(T |X) (1) = H(T ) (2) = − N i=1 p i log p i<label>(3)</label></formula><p>where H(•) denotes entropy, and we have used the fact that H(T |X) = 0 since T is a deterministic function of X. Here the probabilities p i = P (h ≥ b i and h &lt; b i+1 ) are simply the probability that an input X produces a hidden unit activity that lands in bin i, defined by lower and upper bin limits b i and b i+1 respectively. This probability can be calculated exactly for monotonic nonlinearities f (•) using the cumulative density of X,</p><formula xml:id="formula_1">p i = P (X ≥ f −1 (b i )/w 1 and X &lt; f −1 (b i+1 )/w 1 ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_2">f −1 (•) is the inverse function of f (•).</formula><p>As shown in Fig. <ref type="figure" target="#fig_1">2C-D</ref>, as a function of the weight w 1 , mutual information with the input first increases and then decreases for the tanh nonlinearity, but always increases for the ReLU nonlinearity.</p><p>Intuitively, for small weights w 1 ≈ 0, neural activities lie near zero on the approximately linear part of the tanh function. Therefore f (w 1 X) ≈ w 1 X, yielding a rescaled Gaussian with information that grows with the size of the weights. However for very large weights w 1 → ∞, the tanh hidden unit nearly always saturates, yielding a discrete variable that concentrates in just two bins. This is more or less a coin flip, containing mutual information with the input of approximately 1 bit. Hence the distribution of T collapses to a much lower entropy distribution, yielding compression for large weight values. With the ReLU nonlinearity, half of the inputs are negative and land in the bin containing a hidden activity of zero. The other half are Gaussian distributed, and thus have entropy that increases with the size of the weight.</p><p>Hence double-saturating nonlinearities can lead to compression of information about the input, as hidden units enter their saturation regime, due to the binning procedure used to calculate mutual information. The crux of the issue is that the actual I(h; X) is infinite, unless the network itself adds noise to the hidden layers. In particular, without added noise, the transformation from X to the continuous hidden activity h is deterministic and the mutual information I(h; X) would generally be to produce hidden unit activity h. (B) The continuous activity h is binned into a discrete variable T for the purpose of calculating mutual information. Blue: continuous tanh nonlinear activation function. Grey: Bin borders for 30 bins evenly spaced between -1 and 1. Because of the saturation in the sigmoid, a wide range of large magnitude net input values map to the same bin. (C) Mutual information with the input as a function of weight size w 1 for a tanh nonlinearity. Information increases for small w 1 and then decreases for large w 1 as all inputs land in one of the two bins corresponding to the saturation regions. (D) Mutual information with the input for the ReLU nonlinearity increases without bound. Half of all inputs land in the bin corresponding to zero activity, while the other half have information that scales with the size of the weights.</p><p>infinite (see Appendix C for extended discussion). Networks that include noise in their processing (e.g., <ref type="bibr">Kolchinsky et al. (2017)</ref>) can have finite I(T ; X). Otherwise, to obtain a finite MI, one must compute mutual information as though there were binning or added noise in the activations. But this binning/noise is not actually a part of the operation of the network, and is therefore somewhat arbitrary (different binning schemes can result in different mutual information with the input, as shown in Fig. <ref type="figure" target="#fig_13">14</ref> of Appendix C).</p><p>We note that the binning procedure can be viewed as implicitly adding noise to the hidden layer activity: a range of X values map to a single bin, such that the mapping between X and T is no longer perfectly invertible <ref type="bibr" target="#b17">(Laughlin, 1981)</ref>. The binning procedure is therefore crucial to obtaining a finite MI value, and corresponds approximately to a model where noise enters the system after the calculation of h, that is, T = h + , where is noise of fixed variance independent from h and X. This approach is common in information theoretic analyses of deterministic systems, and can serve as a measure of the complexity of a system's representation (see Sec 2.4 of Shwartz-Ziv &amp; Tishby (2017)). However, neither binning nor noise is present in the networks that Shwartz-Ziv &amp; Tishby (2017) considered, nor the ones in Fig. <ref type="figure" target="#fig_1">2</ref>, either during training or testing. It therefore remains unclear whether robustness of a representation to this sort of noise in fact influences generalization performance in deep learning systems.</p><p>Furthermore, the addition of noise means that different architectures may no longer be compared in a common currency of mutual information: the binning/noise structure is arbitrary, and architectures that implement an identical input-output map can nevertheless have different robustness to noise added in their internal representation. For instance, Appendix C describes a family of linear networks that compute exactly the same input-output map and therefore generalize identically, but yield different mutual information with respect to the input. Finally, we note that approaches which view the weights obtained from the training process as the random variables of interest may sidestep this issue <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2017)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INFORMATION PLANE DYNAMICS IN DEEP LINEAR NETWORKS</head><p>The preceding section investigates the role of nonlinearity in the observed compression behavior, tracing the source to double-saturating nonlinearities and the binning methodology used to calculate mutual information. However, other mechanisms could lead to compression as well. Even without nonlinearity, neurons could converge to highly correlated activations, or project out irrelevant directions of the input. These phenomena are not possible to observe in our simple three neuron minimal model, as they require multiple inputs and hidden layer activities. To search for these mechanisms, we turn to a tractable model system: deep linear neural networks <ref type="bibr" target="#b3">(Baldi &amp; Hornik (1989)</ref>; Fukumizu (1998); <ref type="bibr" target="#b24">Saxe et al. (2014)</ref>). In particular, we exploit recent results on the generalization dynamics in simple linear networks trained in a student-teacher setup <ref type="bibr" target="#b26">(Seung et al., 1992;</ref><ref type="bibr" target="#b1">Advani &amp; Saxe, 2017)</ref>. In a student-teacher setting, one "student" neural network learns to approximate the output of another "teacher" neural network. This setting is a way of generating a dataset with interesting structure that nevertheless allows exact calculation of the generalization performance of the network, exact calculation of the mutual information of the representation (without any binning procedure), and, though we do not do so here, direct comparison to the IB bound which is already known for linear Gaussian problems <ref type="bibr" target="#b6">(Chechik et al., 2005)</ref>.</p><p>We consider a scenario where a linear teacher neural network generates input and output examples which are then fed to a deep linear student network to learn (Fig. <ref type="figure" target="#fig_2">3A</ref>). Following the formulation of <ref type="bibr" target="#b1">(Advani &amp; Saxe, 2017)</ref>, we assume multivariate Gaussian inputs X ∼ N (0, 1 Ni I Ni ) and a scalar output Y . The output is generated by the teacher network according to Y = W 0 X + o , where o ∼ N (0, σ 2 o ) represents aspects of the target function which cannot be represented by a neural network (that is, the approximation error or bias in statistical learning theory), and the teacher weights W o are drawn independently from N (0, σ 2 w ). Here, the weights of the teacher define the rule to be learned. The signal to noise ratio SNR = σ 2 w /σ 2 o determines the strength of the rule linking inputs to outputs relative to the inevitable approximation error. We emphasize that the "noise" added to the teacher's output is fundamentally different from the noise added for the purpose of calculating mutual information: o models the approximation error for the task-even the best possible neural network may still make errors because the target function is not representable exactly as a neural network-and is part of the construction of the dataset, not part of the analysis of the student network.</p><p>To train the student network, a dataset of P examples is generated using the teacher. The student network is then trained to minimize the mean squared error between its output and the target output using standard (batch or stochastic) gradient descent on this dataset. Here the student is a deep linear neural network consisting of potentially many layers, but where the the activation function of each neuron is simply f (u) = u. That is, a depth D deep linear network computes the output</p><formula xml:id="formula_3">Ŷ = W D+1 W D • • • W 2 W 1 X.</formula><p>While linear activation functions stop the network from computing complex nonlinear functions of the input, deep linear networks nevertheless show complicated nonlinear learning trajectories <ref type="bibr" target="#b24">(Saxe et al., 2014)</ref>, the optimization problem remains nonconvex <ref type="bibr" target="#b3">(Baldi &amp; Hornik, 1989)</ref>, and the generalization dynamics can exhibit substantial overtraining <ref type="bibr" target="#b10">(Fukumizu, 1998;</ref><ref type="bibr" target="#b1">Advani &amp; Saxe, 2017)</ref>.</p><p>Importantly, because of the simplified setting considered here, the true generalization error is easily shown to be</p><formula xml:id="formula_4">E g (t) = ||W o − W tot (t)|| 2 F +σ 2 o (5)</formula><p>where W tot (t) is the overall linear map implemented by the network at training epoch t (that is,</p><formula xml:id="formula_5">W tot = W D+1 W D • • • W 2 W 1 ).</formula><p>Furthermore, the mutual information with the input and output may be calculated exactly, because the distribution of the activity of any hidden layer is Gaussian. Let T be the activity of a specific hidden layer, and let W be the linear map from the input to this activity (that is, for layer l,</p><formula xml:id="formula_6">W = W l • • • W 2 W 1</formula><p>). Since T = W X, the mutual information of X and T calculated using differential entropy is infinite. For the purpose of calculating the mutual information, therefore, we assume that Gaussian noise is added to the hidden layer activity, T = W X + M I , with mean 0 and variance σ 2 M I = 1.0. This allows the analysis to apply to networks of any size, including overcomplete layers, but as before we emphasize that we do not add this noise either during training or testing. With these assumptions, T and X are jointly Gaussian and we have</p><formula xml:id="formula_7">I(T ; X) = log| W W T + σ 2 M I I N h |− log|σ 2 M I I N h | (6)</formula><p>where |•| denotes the determinant of a matrix. Finally the mutual information with the output Y , also jointly Gaussian, can be calculated similarly (see Eqns. ( <ref type="formula" target="#formula_26">22</ref>)-( <ref type="formula">25</ref>) of Appendix G). Fig. <ref type="figure" target="#fig_2">3</ref> shows example training and test dynamics over the course of learning in panel C, and the information plane dynamics in panel D. Here the network has an input layer of 100 units, 1 hidden layer of 100 units each and one output unit. The network was trained with batch gradient descent on a dataset of 100 examples drawn from the teacher with signal to noise ratio of 1.0. The linear network behaves qualitatively like the ReLU network, and does not exhibit compression. Nevertheless, it learns a map that generalizes well on this task and shows minimal overtraining. Hence, in the setting we study here, generalization performance can be acceptable without any compression phase.</p><p>The results in <ref type="bibr" target="#b1">(Advani &amp; Saxe (2017)</ref>) show that, for the case of linear networks, overtraining is worst when the number of inputs matches the number of training samples, and is reduced by making the number of samples smaller or larger. Fig. <ref type="figure" target="#fig_3">4</ref> shows learning dynamics with the number of samples matched to the size of the network. Here overfitting is substantial, and again no compression is seen in the information plane. Comparing to the result in Fig. <ref type="figure" target="#fig_2">3D</ref>, both networks exhibit similar information dynamics with respect to the input (no compression), but yield different generalization performance.</p><p>Hence, in this linear analysis of a generic setting, there do not appear to be additional mechanisms that cause compression over the course of learning; and generalization behavior can be widely different for networks with the same dynamics of information compression regarding the input. We note that, in the setting considered here, all input dimensions have the same variance, and the weights of the teacher are drawn independently. Because of this, there are no special directions in the input, and each subspace of the input contains as much information as any other. It is possible that, in real world tasks, higher variance inputs are also the most likely to be relevant to the task (here, have large weights in the teacher). We have not investigated this possibility here.  To see whether similar behavior arises in nonlinear networks, we trained tanh networks in the same setting as Section 2, but with 30% of the data, which we found to lead to modest overtraining. Fig. <ref type="figure" target="#fig_3">4C</ref>-D shows the resulting train, test, and information plane dynamics. Here the tanh networks show substantial compression, despite exhibiting overtraining. This establishes a dissociation between behavior in the information plane and generalization dynamics: networks that compress may (Fig. <ref type="figure">1A</ref>) or may not (Fig. <ref type="figure" target="#fig_3">4C-D</ref>) generalize well, and networks that do not compress may (Figs.1B, 3A-B) or may not (Fig. <ref type="figure" target="#fig_3">4A-B</ref>) generalize well.</p><p>from a high to a low gradient signal-to-noise ratio (SNR), i.e., the onset of the diffusion phase. The proposed mechanism behind this diffusion-driven compression is as follows. The authors state that during the diffusion phase, the stochastic evolution of the weights can be described as a Fokker-Planck equation under the constraint of small training error. Then, the stationary distribution over weights for this process will have maximum entropy, again subject to the training error constraint. Finally, the authors claim that weights drawn from this stationary distribution will maximize the entropy of inputs given hidden layer activity, H(X|T ), subject to a training error constraint, and that this training error constraint is equivalent to a constraint on the mutual information I(T ; Y ) for small training error. Since the entropy of the input, H(X), is fixed, the result of the diffusion dynamics will be to minimize I(X; T ) := H(X) − H(X|T ) for a given value of I(T ; Y ) reached at the end of the drift phase.</p><p>However, this explanation does not hold up to either theoretical or empirical investigation. Let us assume that the diffusion phase does drive the distribution of weights to a maximum entropy distribution subject to a training error constraint. Note that this distribution reflects stochasticity of weights across different training runs. There is no general reason that a given set of weights sampled from this distribution (i.e., the weight parameters found in one particular training run) will maximize H(X|T ), the entropy of inputs given hidden layer activity. In particular, H(X|T ) reflects (conditional) uncertainty about inputs drawn from the data-generating distribution, rather than uncertainty about any kind of distribution across different training runs.</p><p>We also show empirically that the stochasticity of the SGD is not necessary for compression. To do so, we consider two distinct training procedures: offline stochastic gradient descent (SGD), which learns from a fixed-size dataset, and updates weights by repeatedly sampling a single example from the dataset and calculating the gradient of the error with respect to that single sample (the typical procedure used in practice); and batch gradient descent (BGD), which learns from a fixed-size dataset, and updates weights using the gradient of the total error across all examples. Batch gradient descent uses the full training dataset and, crucially, therefore has no randomness or diffusion-like behavior in its updates.</p><p>We trained tanh and ReLU networks with SGD and BGD and compare their information plane dynamics in Fig. <ref type="figure" target="#fig_4">5</ref> (see Appendix H for a linear network). We find largely consistent information dynamics in both instances, with robust compression in tanh networks for both methods. Thus randomness in the training process does not appear to contribute substantially to compression of information about the input. This finding is consistent with the view presented in Section 2 that compression arises predominantly from the double saturating nonlinearity.</p><p>Finally, we look at the gradient signal-to-noise ratio (SNR) to analyze the relationship between compression and the transition from high to low gradient SNR. Fig. <ref type="figure" target="#fig_1">20</ref> of Appendix I shows the gradient SNR over training, which in all cases shows a phase transition during learning. Hence the gradient SNR transition is a general phenomenon, but is not causally related to compression. Appendix I offers an extended discussion and shows gradient SNR transitions without compression on the MNIST dataset and for linear networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SIMULTANEOUS FITTING AND COMPRESSION</head><p>Our finding that generalization can occur without compression may seem difficult to reconcile with the intuition that certain tasks involve suppressing irrelevant directions of the input. In the extreme, if certain inputs contribute nothing but noise, then good generalization requires ignoring them. To study this, we consider a variant on the linear student-teacher setup of Section 3: we partition the input X into a set of task-relevant inputs X rel and a set of task-irrelevant inputs X irrel , and alter the teacher network so that the teacher's weights to the task-irrelevant inputs are all zero. Hence the inputs X irrel contribute only noise, while the X rel contain signal. We then calculate the information plane dynamics for the whole layer, and for the task-relevant and task-irrelevant inputs separately. Fig. <ref type="figure" target="#fig_5">6</ref> shows information plane dynamics for a deep linear neural network trained using SGD (5 samples/batch) on a task with 30 task-relevant inputs and 70 task-irrelevant inputs. While the overall dynamics show no compression phase, the information specifically about the task-irrelevant subspace does compress over the course of training. This compression process occurs at the same time as the fitting to the task-relevant information. Thus, when a task requires ignoring some inputs, the information with these inputs specifically will indeed be reduced; but overall mutual information with the input in general may still increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION</head><p>Our results suggest that compression dynamics in the information plane are not a general feature of deep networks, but are critically influenced by the nonlinearities employed by the network. Doublesaturating nonlinearities lead to compression, if mutual information is estimated by binning activations or by adding homoscedastic noise, while single-sided saturating nonlinearities like ReLUs do not compress in general. Consistent with this view, we find that stochasticity in the training process does not contribute to compression in the cases we investigate. Furthermore, we have found instances where generalization performance does not clearly track information plane behavior, questioning the causal link between compression and generalization. Hence information compression may parallel the situation with sharp minima: although empirical evidence has shown a correlation with generalization error in certain settings and architectures, further theoretical analysis has shown that sharp minima can in fact generalize well <ref type="bibr" target="#b9">(Dinh et al., 2017)</ref>. We emphasize that compression still may occur within a subset of the input dimensions if the task demands it. This compression, however, is interleaved rather than in a secondary phase and may not be visible by information metrics that track the overall information between a hidden layer and the input. Finally, we note that our results address the specific claims of one scheme to link the information bottleneck principle with current practice in deep networks. The information bottleneck principle itself is more general and may yet offer important insights into deep networks <ref type="bibr" target="#b0">(Achille &amp; Soatto, 2017)</ref>. Moreover, the information bottleneck principle could yield fundamentally new training algorithms for networks that are inherently stochastic and where compression is explicitly encouraged with appropriate regularization terms <ref type="bibr" target="#b5">(Chalk et al., 2016;</ref><ref type="bibr" target="#b2">Alemi et al., 2017;</ref><ref type="bibr">Kolchinsky et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 KERNEL DENSITY ESTIMATION OF MI</head><p>The KDE approach of <ref type="bibr">Kolchinsky &amp; Tracey (2017)</ref>; <ref type="bibr">Kolchinsky et al. (2017)</ref> estimates the mutual information between the input and the hidden layer activity by assuming that the hidden activity is distributed as a mixture of Gaussians. This assumption is well-suited to the present setting under the following interpretation: we take the input activity to be distributed as delta functions at each example in the dataset, corresponding to a uniform distribution over these specific samples. In other words, we assume that the empirical distribution of input samples is the true distribution. Next, the hidden layer activity h is a deterministic function of the input. As mentioned in the main text and discussed in more detail in Appendix C, without the assumption of noise, this would have infinite mutual information with the input. We therefore assume for the purposes of analysis that Gaussian noise of variance σ 2 is added, that is, T = h + where ∼ N (0, σ 2 I). Under these assumptions, the distribution of T is genuinely a mixture of Gaussians, with a Gaussian centered on the hidden activity corresponding to each input sample. We emphasize again that the noise is added solely for the purposes of analysis, and is not present during training or testing the network. In this setting, an upper bound for the mutual information with the input is <ref type="bibr">(Kolchinsky &amp; Tracey, 2017;</ref><ref type="bibr">Kolchinsky et al., 2017</ref>)</p><formula xml:id="formula_8">I(T ; X) ≤ − 1 P i log 1 P j exp − 1 2 ||h i − h j || 2 2 σ 2<label>(7)</label></formula><p>where P is the number of training samples and h i denotes the hidden activity vector in response to input sample i. Similarly, the mutual information with respect to the output can be calculated as</p><formula xml:id="formula_9">I(T ; Y ) = H(T ) − H(T |Y ) (8) ≤ − 1 P i log 1 P j exp − 1 2 h i − h j 2 2 σ 2 (9) − L l p l   − 1 P l i,Yi=l log 1 P l j,Yj =l exp − 1 2 h i − h j 2 2 σ 2   (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where L is the number of output labels, P l denotes the number of data samples with output label l, p l = P l /P denotes the probability of output label l, and the sums over i, Y i = l indicate a sum over all examples with output label l. Figure <ref type="figure" target="#fig_6">8A</ref>-B shows the result of applying this MI estimation method on the dataset and network architecture of Shwartz-Ziv &amp; Tishby (2017), with MI estimated on the full dataset and averaged over 50 repetitions. Mutual information was estimated using data samples from the test set, and we took the noise variance σ 2 = 0.1. These results look similar to the estimate derived from binning, with compression in tanh networks but no compression in ReLU networks. Relative to the binning estimate, it appears that compression is less pronounced in the KDE method. The network was trained using SGD with minibatches of size 128. As before, mutual information was estimated using data samples from the test set, and we took the noise variance σ 2 = 0.1. The smaller layer sizes in the top three hidden layers were selected to ensure the quality of the kernel density estimator given the amount of data in the test set, since the estimates are more accurate for smaller-dimensional data. Because of computational expense, the MNIST results are from a single training run.</p><p>More detailed results for the MNIST dataset are provided in Figure <ref type="figure" target="#fig_8">9</ref> for the tanh activation function, and in Figure <ref type="figure" target="#fig_9">10</ref> for the ReLU activation function. In these figures, the first row shows the evolution of the cross entropy loss (on both training and testing data sets) during training. The second row shows the mutual information between input and the activity of different hidden layers, using the nonparametric KDE estimator described above. The blue region in the second row shows the range of possible MI values, ranging from the upper bound described above (Eq. 10) to the following lower bound <ref type="bibr">(Kolchinsky &amp; Tracey, 2017)</ref>,</p><formula xml:id="formula_11">I(T ; Y ) ≥ − 1 P i log 1 P j exp − 1 2 h i − h j 2 2 4σ 2 (11) − L l p l   − 1 P l i,Yi=l log 1 P l j,Yj =l exp − 1 2 h i − h j 2 2 4σ 2   . (<label>12</label></formula><formula xml:id="formula_12">)</formula><p>The third row shows the mutual information between input and activity of different hidden layers, estimated using the binning method (here, the activity of each neuron was discretized into bins of size 0.5). For both the second and third rows, we also plot the entropy of the inputs, H(X), as a dashed line. H(X) is an upper bound on the mutual information I(X; T ), and is computed using the assumption of a uniform distribution over the 10,000 testing points in the MNIST dataset, giving H(X) = log 2 10000.</p><p>Finally, the fourth row visualizes the dynamics of the SGD updates during training. For each layer and epoch, the green line shows the 2 norm of the weights. We also compute the vector of mean updates across SGD minibatches (this vector has one dimension for each weight parameter), as well as the vector of the standard deviation of the updates across SGD minibatches. The 2 norm of the mean update vector is shown in blue, and the 2 norm of the standard deviation vector is shown in orange.</p><p>The gradient SNR, computed as the ratio of the norm of the mean vector to the norm of the standard deviation vector, is shown in red. For both the tanh and ReLU networks, the gradient SNR shows a phase transition during training, and the norm of the weights in each layer increases. Importantly, this phase transition occurs despite a lack of compression in the ReLU network, indicating that noise in SGD updates does not yield compression in this setting. Upper and lower bounds for the mutual information I(X; T ) between the input (X) and each layer's activity (T ), using the nonparametric KDE estimator <ref type="bibr">(Kolchinsky &amp; Tracey, 2017;</ref><ref type="bibr">Kolchinsky et al., 2017)</ref>. Dotted line indicates H(X) = log 2 10000, the entropy of a uniform distribution over 10,000 testing samples. Row 3: Binning-based estimate of the mutual information I(X; T ), with each neuron's activity discretized using a bin size of 0.5. Row 4: Gradient SNR and weight norm dynamics. The gradient SNR shows a phase transition during training, and the norm of the weights in each layer increases. Upper and lower bounds for the mutual information I(X; T ) between the input (X) and each layer's activity (T ), using the nonparametric KDE estimator <ref type="bibr">(Kolchinsky &amp; Tracey, 2017;</ref><ref type="bibr">Kolchinsky et al., 2017)</ref>. Dotted line indicates H(X) = log 2 10000, the entropy of a uniform distribution over 10,000 testing samples. Row 3: Binning-based estimate of the mutual information I(X; T ), with each neuron's activity discretized using a bin size of 0.5. Row 4: Gradient SNR and weight norm dynamics. The gradient SNR shows a phase transition during training, and the norm of the weights in each layer increases. Importantly, this phase transition occurs despite a lack of compression in the ReLU network, indicating that noise in SGD updates does not yield compression in this setting.</p><p>Published as a conference paper at ICLR 2018 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 OTHER ACTIVATION FUNCTIONS</head><p>Next, in Fig. <ref type="figure" target="#fig_6">8C-D</ref>, we show results from the kernel MI estimator from two additional nonlinear activation functions, the softsign function</p><formula xml:id="formula_13">f (x) = x 1 + |x| ,</formula><p>and the softplus function</p><formula xml:id="formula_14">f (x) = ln(1 + e x ).</formula><p>These functions are plotted next to tanh and ReLU in Fig. <ref type="figure" target="#fig_10">11</ref>. The softsign function is similar to tanh but saturates more slowly, and yields less compression than tanh. The softplus function is a smoothed version of the ReLU, and yields similar dynamics with no compression. Because softplus never saturates fully to zero, it retains more information with respect to the input than ReLUs in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 KRASKOV ESTIMATOR</head><p>We additionally investigated the widely-used nonparametric MI estimator of <ref type="bibr" target="#b15">Kraskov et al. (2004)</ref>. This estimator uses nearest neighbor distances between samples to compute an estimate of the entropy of a continuous random variable. Here we focused for simplicity only on the compression phenomenon in the mutual information between the input and hidden layer activity, leaving aside the information with respect to the output (as this is not relevant to the compression phenomenon). Again, without additional noise assumptions, the MI between the hidden representation and the input would be infinite because the mapping is deterministic. Rather than make specific noise assumptions, we instead use the Kraskov method to estimate the entropy of the hidden representations T . Note that the entropy of T is the mutual information up to an unknown constant so long as the noise assumption is homoscedastic, that is, T = h + Z where the random variable Z is independent of X. To see this, note that</p><formula xml:id="formula_15">I(T ; X) = H(T ) − H(T |X) (13) = H(T ) − H(Z) (14) = H(T ) − c<label>(15)</label></formula><p>where the constant c = H(Z). Hence observing compression in the layer entropy H(T ) is enough to establish that compression occurs in the mutual information.</p><p>The Kraskov estimate is given by</p><formula xml:id="formula_16">d P P i=1 log(r i + ) + d 2 log(π) − log Γ(d/2 + 1) + ψ(P ) − ψ(k) (<label>16</label></formula><formula xml:id="formula_17">)</formula><p>where d is the dimension of the hidden representation, P is the number of samples, r i is the distance to the k-th nearest neighbor of sample i, is a small constant for numerical stability, Γ(•) is the Published as a conference paper at ICLR 2018 Gamma function, and ψ(•) is the digamma function. Here the parameter prevents infinite terms when the nearest neighbor distance r i = 0 for some sample. We took = 10 −16 .</p><p>Figure <ref type="figure" target="#fig_11">12</ref> shows the entropy over training for tanh and ReLU networks trained on the dataset of and with the network architecture in Shwartz-Ziv &amp; Tishby (2017), averaged over 50 repeats. In these experiments, we used k = 2. Compression would correspond to decreasing entropy over the course of training, while a lack of compression would correspond to increasing entropy. Several tanh layers exhibit compression, while the ReLU layers do not. Hence qualitatively, the Kraskov estimator returns similar results to the binning and KDE strategies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A B</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C NOISE ASSUMPTIONS AND DISCRETE VS CONTINUOUS ENTROPY</head><p>A recurring theme in the results reported in this paper is the necessity of noise assumptions to yield a nontrivial information theoretic analysis. Here we give an extended discussion of this phenomenon, and of issues relating to discrete entropy as opposed to continuous (differential) entropy.</p><p>The activity of a neural network is often a continuous deterministic function of its input. That is, in response to an input X, a specific hidden layer might produce activity h = f (X) for some function f . The mutual information between h and X is given by</p><formula xml:id="formula_18">I(h; X) = H(h) − H(h|X).<label>(17)</label></formula><p>If h were a discrete variable, then the entropy would be given by</p><formula xml:id="formula_19">H(h) = − N i=1 p i log p i (<label>18</label></formula><formula xml:id="formula_20">)</formula><p>where p i is the probability of the discrete symbol i, as mentioned in the main text. Then H(h|X) = 0 because the mapping is deterministic and we have I(h; X) = H(h).</p><p>However h is typically continuous. The continuous entropy, defined for a continuous random variable Z with density p Z by analogy to Eqn. (18) as</p><formula xml:id="formula_21">H(Z) = − p Z (z) log p Z (z)dz,<label>(19)</label></formula><p>can be negative and possibly infinite. In particular, note that if p Z is a delta function, then H(Z) = −∞. The mutual information between hidden layer activity h and the input X for continuous h, X is</p><formula xml:id="formula_22">I(h; X) = H(h) − H(h|X). (<label>20</label></formula><formula xml:id="formula_23">)</formula><p>Now H(h|X) = −∞ since given the input X, the hidden activity h is distributed as a delta function at f (X). The mutual information is thus generally infinite, so long as the hidden layer activity has finite entropy (H(h) is finite). In contrast to linear binning, the mutual information continues to increase as weights grow.</p><p>To yield a finite mutual information, some noise in the mapping is required such that H(h|X) remains finite. A common choice (and one adopted here for the linear network, the nonparametric kernel density estimator, and the k-nearest neighbor estimator) is to analyze a new variable with additive noise, T = h + Z, where Z is a random variable independent of X. Then H(T |X) = H(Z) which allows the overall information I(T ; X) = H(T ) − H(Z) to remain finite. This noise assumption is not present in the actual neural networks either during training or testing, and is made solely for the purpose of calculating the mutual information.</p><p>Another strategy is to partition the continuous variable h into a discrete variable T, for instance by binning the values (the approach taken in Shwartz-Ziv &amp; Tishby ( <ref type="formula">2017</ref>)). This allows use of the discrete entropy, which remains finite. Again, however, in practice the network does not operate on the binned variables T but on the continuous variables h, and the binning is solely for the purpose of calculating the mutual information. Moreover, there are many possible binning strategies, which yield different discrete random variables, and different mutual information with respect to the input. The choice of binning strategy is an assumption analogous to choosing a type of noise to add to the representation in the continuous case: because there is in fact no binning in the operation of the network, there is no clear choice for binning methodology. The strategy we use in binning-based experiments reported here is the following: for bounded activations like the tanh activation, we use evenly spaced bins between the minimum and maximum limits of the function. For unbounded activations like ReLU, we first train the network completely; next identify the minimum and maximum hidden activation over all units and all training epochs; and finally bin into equally spaced bins between these minimum and maximum values. We note that this procedure places no restriction on the magnitude that the unbounded activation function can take during training, and yields the same MI estimate as using infinite equally spaced bins (because bins for activities larger than the maximum are never seen during training).</p><p>As an example of another binning strategy that can yield markedly different results, we consider evenly spaced bins in a neuron's net input, rather than its activity. That is, instead of evenly spaced bins in the neural activity, we determine the bin edges by mapping a set of evenly spaced values through the neural nonlinearity. For tanh, for instance, this spaces bins more tightly in the saturation region as compared to the linear region. Figure <ref type="figure" target="#fig_12">13</ref> shows the results of applying this binning strategy to the minimal three neuron model with tanh activations. This binning scheme captures more information as the weights of the network grow larger. Figure <ref type="figure" target="#fig_13">14</ref> shows information plane dynamics for this binning structure. The tanh network no longer exhibits compression. (We note that the broken DPI in this example is an artifact of performing binning only for analysis, as discussed below).</p><p>Any implementation of a neural network on digital hardware is ultimately of finite precision, and hence is a binned, discrete representation. However, it is a very high resolution binning compared to that used here or by Shwartz-Ziv &amp; Tishby (2017): single precision would correspond to using roughly 2 32 bins to discretize each hidden unit's activity, as compared to the 30-100 used here. If the binning is fine-grained enough that each input X yields a different binned activity pattern h, then H(h) = log(P ) where P is the number of examples in the dataset, and there will be little to no change in information during training. As an example, we show in Fig. <ref type="figure" target="#fig_14">15</ref> the result of binning at full machine precision.</p><p>Finally, we note two consequences of the assumption of noise/binning for the purposes of analysis. First, this means that the data processing inequality (DPI) does not apply to the noisy/binned mutual information estimates. The DPI states that information can only be destroyed through successive transformations, that is, if X → h 1 → h 2 form a Markov chain, then I(X; h 1 ) ≥ I(X; h 2 ) (see, eg, <ref type="bibr" target="#b29">Tishby &amp; Zaslavsky (2015)</ref>). Because noise is added only for the purpose of analysis, however, this does not apply here. In particular, for the DPI to apply, the noise added at lower layers would have to propagate through the network to higher layers. That is, if the transformation from hidden layer 1 to hidden layer 2 is h 2 = f (h 1 ) and T 1 = h 1 + Z 1 is the hidden layer activity after adding noise, then the DPI would hold for the variable T2 = f (T 1 ) + Z 2 = f (h 1 + Z 1 ) + Z 2 , not the quantity T 2 = h 2 + Z 2 = f (h 1 ) + Z 2 used in the analysis. Said another way, the Markov chain for</p><formula xml:id="formula_24">T 2 is X → h 1 → h 2 → T 2 , not X → h 1 → T 1 → T 2</formula><p>, so the DPI states only that I(X; h 1 ) ≥ I(X; T 2 ).</p><p>A second consequence of the noise assumption is the fact that the mutual information is no longer invariant to invertible transformations of the hidden activity h. A potentially attractive feature of a theory based on mutual information is that it can allow for comparisons between different architectures: mutual information is invariant to any invertible transformation of the variables, so two hidden representations could be very different in detail but yield identical mutual information with respect to the input. However, once noise is added to a hidden representation, this is no longer the case: the variable T = h + Z is not invariant to reparametrizations of h. As a simple example, consider a minimal linear network with scalar weights w 1 and w 2 that computes the output ŷ = w 2 w 1 X.</p><p>The hidden activity is h = w 1 X. Now consider the family of networks in which we scale down w 1 and scale up w 2 by a factor c = 0, that is, these networks have weights w1 = w 1 /c and w2 = cw 2 , yielding the exact same input-output map ŷ = w2 w1 X = cw 2 (w 1 /c)X = w 2 w 1 X. Because they compute the same function, they necessarily generalize identically. However after introducing the noise assumption the mutual information is</p><formula xml:id="formula_25">I(T ; X) = log w 2 1 /c 2 + σ 2 M I − log σ 2 M I<label>(21)</label></formula><p>where we have taken the setting in Section 3 in which X is normal Gaussian, and independent Gaussian noise of variance σ 2 M I is added for the purpose of MI computation. Clearly, the mutual information is now dependent on the scaling c of the internal layer, even though this is an invertible linear transformation of the representation. Moreover, this shows that networks which generalize identically can nevertheless have very different mutual information with respect to the input when it is measured in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D WEIGHT NORMS OVER TRAINING</head><p>Our argument relating neural saturation to compression in mutual information relies on the notion that in typical training regimes, weights begin small and increase in size over the course of training. We note that this is a virtual necessity for a nonlinearity like tanh, which is linear around the origin: when initialized with small weights, the activity of a tanh network will be in this linear regime and the network can only compute a linear function of its input. Hence a real world nonlinear task can only be learned by increasing the norm of the weights so as to engage the tanh nonlinearity on some examples. This point can also be appreciated from norm-based capacity bounds on neural networks, which show that, for instance, the Rademacher complexity of a neural network with small weights must be low <ref type="bibr" target="#b4">(Bartlett &amp; Mendelson, 2002;</ref><ref type="bibr" target="#b22">Neyshabur et al., 2015)</ref>. Finally, as an empirical matter, the networks trained in this paper do in fact increase the norm of their weights over the course of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G LINEAR MUTUAL INFORMATION CALCULATION</head><p>For the linear setting considered here, the mutual information between a hidden representation T and the output Y may be calculated using the relations  still occurs under batch gradient descent learning, suggesting that in fact noise in the gradient updates is not the cause of compression. Here we investigate a related claim, namely that during training, networks switch between two phases. These phases are defined by the ratio of the mean of the gradient to the standard deviation of the gradient across training examples, called the gradient signal-to-noise ratio. In the first "drift" phase, the SNR is high, while in the second "diffusion" phase the SNR is low. Shwartz-Ziv &amp; Tishby (2017) hypothesize that the drift phase corresponds to movement toward the minimum with no compression, while the diffusion phase corresponds to a constrained diffusion in weight configurations that attain the optimal loss, during which representations compress. However, two phases of gradient descent have been described more generally, sometimes known as the transient and stochastic phases or search and convergence phases <ref type="bibr" target="#b21">(Murata, 1998;</ref><ref type="bibr" target="#b7">Chee &amp; Toulis, 2017)</ref>, suggesting that these phases might not be related specifically to compression behavior.</p><formula xml:id="formula_26">H(Y ) = N o 2 log(2πe) + 1 2 log|W o W T o + σ 2 o I No |,<label>(22)</label></formula><formula xml:id="formula_27">H(T ) = N h 2 log(2πe) + 1 2 log| W W T + σ 2 M I I N h |,<label>(23)</label></formula><formula xml:id="formula_28">H(Y ; T ) = N o + N h 2 log(2πe) + 1 2 log W W T + σ 2 M I I N h W W T o , W o W T W o W T o + σ 2 o I N h ,<label>(24)</label></formula><p>In Fig. <ref type="figure" target="#fig_1">20</ref> we plot the gradient SNR over the course of training for the tanh and ReLU networks in the standard setting of Shwartz-Ziv &amp; Tishby (2017). In particular, for each layer l we calculate the mean and standard deviation as</p><formula xml:id="formula_29">m l = ∂E ∂W l F (<label>26</label></formula><formula xml:id="formula_30">)</formula><formula xml:id="formula_31">s l = STD ∂E ∂W l F<label>(27)</label></formula><p>where • denotes the mean and ST D(•) denotes the element-wise standard deviation across all training samples, and • F denotes the Frobenius norm. The gradient SNR is then the ratio m l /s l . We additionally plot the norm of the weights W l F over the course of training.</p><p>Both tanh and ReLU networks yield a similar qualitative pattern, with SNR undergoing a step-like transition to a lower value during training. Figures 9 and 10, fourth row, show similar plots for MNIST-trained networks. Again, SNR undergoes a transition from high to low over training. Hence the two phase nature of gradient descent appears to hold across the settings that we examine here. Crucially, this finding shows that the SNR transition is not related to the compression phenomenon because ReLU networks, which show the gradient SNR phase transition, do not compress.</p><p>Finally, to show the generality of the two-phase gradient SNR behavior and its independence from compression, we develop a minimal model of this phenomenon in a three neuron linear network. We consider the student-teacher setting of Fig. <ref type="figure" target="#fig_2">3</ref> but with N i = N h = 1, such that the input and hidden layers have just a single neuron (as in the setting of Fig. <ref type="figure" target="#fig_1">2</ref>). Here, with just a single hidden neuron, clearly there can be no compression so long as the first layer weight increases over the course of training. Figure <ref type="figure" target="#fig_1">21AC</ref> shows that even in this simple setting, the SNR shows the phase transition but the weight norm increases over training. Hence again, the two phases of the gradient are present even though there is no compression. To intuitively understand the source of this behavior, note that the weights are initialized to be small and hence early in learning all must be increased in magnitude, yielding a consistent mean gradient. Once the network reaches the vicinity of the minimum, the mean weight change across all samples by definition goes to zero. The standard deviation remains finite, however, because on some specific examples error could be improved by increasing or decreasing the weights-even though across the whole dataset the mean error has been minimized.</p><p>Hence overall, our results show that a two-phase structure in the gradient SNR occurs in all settings we consider, even though compression occurs only in a subset. The gradient SNR behavior is therefore not causally related to compression dynamics, consistent with the view that saturating nonlinearities are the primary source of compression. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: Information plane dynamics and neural nonlinearities. (A) Replication of Shwartz-Ziv &amp; Tishby (2017) for a network with tanh nonlinearities (except for the final classification layer which contains two sigmoidal neurons). The x-axis plots information between each layer and the input, while the y-axis plots information between each layer and the output. The color scale indicates training time in epochs. Each of the six layers produces a curve in the information plane with the input layer at far right, output layer at the far left. Different layers at the same epoch are connected by fine lines. (B) Information plane dynamics with ReLU nonlinearities (except for the final layer of 2 sigmoidal neurons). Here no compression phase is visible in the ReLU layers. For learning curves of both networks, see Appendix A. (C) Information plane dynamics for a tanh network of size 784 − 1024 − 20 − 20 − 20 − 10 trained on MNIST, estimated using the non-parametric kernel density mutual information estimator of Kolchinsky &amp; Tracey (2017); Kolchinsky et al. (2017), no compression is observed except in the final classification layer with sigmoidal neurons. See Appendix B for the KDE MI method applied to the original Tishby dataset; additional results using a second popular nonparametric k-NN-based method<ref type="bibr" target="#b15">(Kraskov et al., 2004)</ref>; and results for other neural nonlinearities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Nonlinear compression in a minimal model. (A) A three neuron nonlinear network which receives Gaussian inputs x, multiplies by weight w 1 , and maps through neural nonlinearity f (•) to produce hidden unit activity h. (B) The continuous activity h is binned into a discrete variable T for the purpose of calculating mutual information. Blue: continuous tanh nonlinear activation function. Grey: Bin borders for 30 bins evenly spaced between -1 and 1. Because of the saturation in the sigmoid, a wide range of large magnitude net input values map to the same bin. (C) Mutual information with the input as a function of weight size w 1 for a tanh nonlinearity. Information increases for small w 1 and then decreases for large w 1 as all inputs land in one of the two bins corresponding to the saturation regions. (D) Mutual information with the input for the ReLU nonlinearity increases without bound. Half of all inputs land in the bin corresponding to zero activity, while the other half have information that scales with the size of the weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Generalization and information plane dynamics in deep linear networks. (A) A linear teacher network generates a dataset by passing Gaussian inputs X through its weights and adding noise. (B) A deep linear student network is trained on the dataset (here the network has 1 hidden layer to allow comparison with Fig. 4A, see Supplementary Figure 18 for a deeper network). (C) Training and testing error over time. (D) Information plane dynamics. No compression is observed. Hence when a tanh network is initialized with small weights and over the course of training comes to saturate its nonlinear units (as it must to compute most functions of practical interest, see discussion in Appendix D), it will enter a compression period where mutual information decreases. Figures 16-17 of Appendix E show histograms of neural activity over the course of training, demonstrating that activities in the tanh network enter the saturation regime during training. This nonlinearity-based compression furnishes another explanation for the observation that training slows down as tanh networks enter their compression phase (Shwartz-Ziv &amp; Tishby, 2017): some fraction of inputs have saturated the nonlinearities, reducing backpropagated error gradients.</figDesc><graphic url="image-3.png" coords="6,297.34,170.57,145.00,96.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Overtraining and information plane dynamics. (A) Average training and test mean square error for a deep linear network trained with SGD. Overtraining is substantial. Other parameters: N i = 100, P = 100, Number of hidden units = 100, Batch size = 5 (B) Information plane dynamics. No compression is observed, and information about the labels is lost during overtraining. (C) Average train and test accuracy (% correct) for nonlinear tanh networks exhibiting modest overfitting (N = 8). (D) Information plane dynamics. Overfitting occurs despite continued compression.</figDesc><graphic url="image-6.png" coords="8,306.10,175.40,138.63,97.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Stochastic training and the information plane. (A) tanh network trained with SGD. (B) tanh network trained with BGD. (C) ReLU network trained with SGD. (D) ReLU network trained with BGD. Both random and non-random training procedures show similar information plane dynamics.</figDesc><graphic url="image-8.png" coords="8,108.00,363.95,395.97,87.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Simultaneous fitting and compression. (A) For a task with a large task-irrelevant subspace in the input, a linear network shows no overall compression of information about the input. (B)The information with the task-relevant subspace increases robustly over training. (C) However, the information specifically about the task-irrelevant subspace does compress after initially growing as the network is trained.</figDesc><graphic url="image-9.png" coords="10,368.12,84.02,132.10,88.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Information plane dynamics for the network architecture and training dataset of Shwartz-Ziv &amp; Tishby (2017), estimated with the nonparametric KDE method of Kolchinsky &amp; Tracey (2017); Kolchinsky et al. (2017) and averaged over 50 repetitions. (A) tanh neural network layers show compression. (B) ReLU neural network layers show no compression. (C) The soft-sign activation function, a double-saturating nonlinearity that saturates more gently than tanh, shows modest compression. (D) The soft-plus activation function, a smoothed version of the ReLU, exhibits no compression. Hence double-saturating nonlinearities exhibit the compression effect while singlesaturating nonlinearities do not.</figDesc><graphic url="image-13.png" coords="14,167.40,81.85,277.20,255.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure</head><label></label><figDesc>Figure 1C-D of the main text shows the results of this estimation technique applied to a neural network of size 784 − 1024 − 20 − 20 − 20 − 10 on the MNIST handwritten digit classification dataset.The network was trained using SGD with minibatches of size 128. As before, mutual information was estimated using data samples from the test set, and we took the noise variance σ 2 = 0.1. The smaller layer sizes in the top three hidden layers were selected to ensure the quality of the kernel density estimator given the amount of data in the test set, since the estimates are more accurate for smaller-dimensional data. Because of computational expense, the MNIST results are from a single training run.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Detailed tanh activation function results on MNIST. Row 1: Loss over training. Row 2:Upper and lower bounds for the mutual information I(X; T ) between the input (X) and each layer's activity (T ), using the nonparametric KDE estimator(Kolchinsky &amp; Tracey, 2017; Kolchinsky  et al., 2017). Dotted line indicates H(X) = log 2 10000, the entropy of a uniform distribution over 10,000 testing samples. Row 3: Binning-based estimate of the mutual information I(X; T ), with each neuron's activity discretized using a bin size of 0.5. Row 4: Gradient SNR and weight norm dynamics. The gradient SNR shows a phase transition during training, and the norm of the weights in each layer increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Detailed ReLU activation function results on MNIST. Row 1: Loss over training. Row 2:Upper and lower bounds for the mutual information I(X; T ) between the input (X) and each layer's activity (T ), using the nonparametric KDE estimator(Kolchinsky &amp; Tracey, 2017; Kolchinsky  et al., 2017). Dotted line indicates H(X) = log 2 10000, the entropy of a uniform distribution over 10,000 testing samples. Row 3: Binning-based estimate of the mutual information I(X; T ), with each neuron's activity discretized using a bin size of 0.5. Row 4: Gradient SNR and weight norm dynamics. The gradient SNR shows a phase transition during training, and the norm of the weights in each layer increases. Importantly, this phase transition occurs despite a lack of compression in the ReLU network, indicating that noise in SGD updates does not yield compression in this setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Alternative activation functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Entropy dynamics over training for the network architecture and training dataset of Shwartz-Ziv &amp; Tishby (2017), estimated with the nonparametric k-nearest-neighbor-based method of<ref type="bibr" target="#b15">Kraskov et al. (2004)</ref>. Here the x-axis is epochs of training time, and the y-axis plots the entropy of the hidden representation, as calculated using nearest-neighbor distances. Note that in this setting, if T is considered to be the hidden activity plus independent noise, the entropy is equal to the mutual information up to a constant (see derivation in text). Layers 0-4 correspond to the hidden layers of size 10-7-5-4-3. (A) tanh neural network layers can show compression over the course of training. (B) ReLU neural network layers show no compression.</figDesc><graphic url="image-14.png" coords="19,305.72,183.65,192.56,144.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Effect of binning strategy on minimal three neuron model. Mutual information for the simple three neuron model shown in Fig. 2 with bin edges b i ∈ tanh(linspace(−50, 50, N )).In contrast to linear binning, the mutual information continues to increase as weights grow.</figDesc><graphic url="image-16.png" coords="20,167.40,131.55,277.20,207.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Effect of binning strategy on information plane dynamics. Results for the same tanh network and training regime as 1A, but with bin edges b i ∈ tanh(linspace(−50, 50, N )). Measured with this binning structure, there is no compression in most layers.</figDesc><graphic url="image-17.png" coords="21,147.60,81.86,316.79,326.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Effect of binning at full machine precision. (A) ReLU network. (B) tanh network. Information in most layers stays pinned to log 2 (P ) = 12. Compression is only observed in the highest and smallest layers near the very end of training, when the saturation of tanh is strong enough to saturate machine precision.</figDesc><graphic url="image-18.png" coords="22,150.24,82.06,307.07,151.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Figure 17: Histogram of neural activities in a ReLU network during training. ReLU layers 1-5 have a roughly constant fraction of activities at zero, corresponding to instances where the ReLU is off; the nonzero activities disperse over the course of training without bound, yielding higher entropy distributions. The sigmoid output layer 6 converges to its saturation limits, and is the only layer that compresses during training (c.f. Fig. 1B). x-axis: training time in epochs. y-axis: Hidden activity value. Colormap: density of hidden layer activities across all input examples.</figDesc><graphic url="image-20.png" coords="24,167.40,81.88,277.16,468.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>I</head><label></label><figDesc>Figure19shows information plane dynamics for stochastic and batch gradient descent learning in a linear network. Randomness in the training process does not dramatically alter the information plane dynamics.</figDesc><graphic url="image-23.png" coords="25,108.00,490.79,395.97,132.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 19 :</head><label>19</label><figDesc>Figure 19: Effect of stochastic training in linear networks. (A) Information plane dynamics for stochastic gradient descent in a linear network (same setting as Fig. 4). (B) Information plane dynamics for batch gradient descent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 20 :Figure 21 :</head><label>2021</label><figDesc>Figure 20: Gradient SNR phase transition. (A) tanh networks trained in the standard setting of Shwartz-Ziv &amp; Tishby (2017) show a phase transition in every layer. (B) ReLU networks also show a phase transition in every layer, despite exhibiting no compression.</figDesc><graphic url="image-27.png" coords="27,-88.23,160.46,594.02,151.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="3,108.00,156.51,396.00,319.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-19.png" coords="23,167.40,196.23,277.15,468.88" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">Code for our results is available at https://github.com/artemyk/ibsgd/tree/iclr2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1">COMPRESSION IN BATCH GRADIENT DESCENT AND SGDNext, we test a core theoretical claim of the information bottleneck theory of deep learning, namely that randomness in stochastic gradient descent is responsible for the compression phase. In particular, because the choice of input samples in SGD is random, the weights evolve in a stochastic way during training.Shwartz-Ziv &amp; Tishby (2017) distinguish two phases of SGD optimization: in the first "drift" phase, the mean of the gradients over training samples is large relative to the standard deviation of the gradients; in the second "diffusion" phase, the mean becomes smaller than the standard deviation of the gradients. The authors propose that compression should commence following the transition</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Ariel Herbert-Voss for useful discussions. This work was supported by grant numbers IIS 1409097 and CHE 1648973 from the US National Science Foundation, and by IARPA contract #D16PC00002. Andrew Saxe and Madhu Advani thank the Swartz Program in Theoretical Theoretical Neuroscience at Harvard University. Artemy Kolchinsky and Brendan Tracey would like to thank the Santa Fe Institute for helping to support this research. Artemy Kolchinsky was supported by Grant No. FQXi-RFP-1622 from the FQXi foundation and Grant No. CHE-1648973 from the US National Science Foundation. Brendan Tracey was supported by AFOSR MURI on Multi-Information Sources of Multi-Physics Systems under Award Number FA9550-15-1-0038.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A LEARNING CURVES FOR tanh AND RELU NETWORKS Supplementary Figure <ref type="figure">7</ref> shows the learning curves for tanh and ReLU networks depicted in Fig. <ref type="figure">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ROBUSTNESS OF FINDINGS TO MI ESTIMATION METHOD AND NEURAL ACTIVATION FUNCTIONS</head><p>This Appendix investigates the generality of the finding that compression is not observed in neural network layers with certain activation functions. Figure <ref type="figure">1</ref> of the main text shows example results using a binning-based MI estimator and a nonparametric KDE estimator, for both the tanh and ReLU activation functions. Here we describe the KDE MI estimator in detail, and present extended results on other datasets. We also show results for other activation functions. Finally, we provide entropy estimates based on another nonparametric estimator, the popular k-nearest neighbor approach of <ref type="bibr" target="#b15">Kraskov et al. (2004)</ref>. Our findings consistently show that double-saturating nonlinearities can yield compression, while single-sided nonlinearities do not.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F INFORMATION PLANE DYNAMICS IN DEEPER LINEAR NETWORKS</head><p>Supplementary Figure <ref type="figure">18</ref> shows information plane dynamics for a deep neural network with five hidden layers each containing 50 hidden units.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the Emergence of Invariance and Disentangling in Deep Representations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01350</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High-dimensional dynamics of generalization error in neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Advani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03667</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural networks and principal component analysis: Learning from examples without local minima</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="53" to="58" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relevant sparse codes with variational information bottleneck</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chalk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Marre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tkacik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1957" to="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Information bottleneck for gaussian variables</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="page" from="165" to="188" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Toulis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06382</idno>
		<title level="m">Convergence diagnostics for stochastic gradient descent with constant step size</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Loss Surfaces of Multilayer Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Arous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Artificial Intelligence</title>
				<meeting>the 18th International Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">38</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sharp Minima Can Generalize For Deep Nets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Effect of Batch Learning In Multilayer Neural Networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Neural Information Processing</title>
				<meeting>the 5th International Conference on Neural Information Processing</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="67" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative Adversarial Nets. Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal Architectures in a Solvable Model of Deep Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kadmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating mixture entropy with pairwise distances</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Kolchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Tracey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02436</idno>
		<title level="m">Nonlinear information bottleneck</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A simple coding procedure enhances a neuron&apos;s information capacity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Laughlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Zeitschrift für Naturforschung c</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="910" to="912" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On the Number of Linear Regions of Deep Neural Networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A statistical study of on-line learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Murata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On-line Learning in Neural Networks</title>
				<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="63" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Norm-Based Capacity Control in Neural Networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Neyshabur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 28th Conference on Learning Theory</title>
				<meeting>The 28th Conference on Learning Theory</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review</title>
		<author>
			<persName><forename type="first">H</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Automation and Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exact solutions to the nonlinear dynamics of learning in deep linear neural networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Conference on Learning Representations</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Learning in Neural Networks: An Overview</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Statistical mechanics of learning from examples</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sompolinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review A</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="6056" to="6091" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Opening the black box of deep neural networks via information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00810</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The information bottleneck method</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37-th Annual Allerton Conference on Communication, Control and Computing</title>
				<meeting>the 37-th Annual Allerton Conference on Communication, Control and Computing</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
