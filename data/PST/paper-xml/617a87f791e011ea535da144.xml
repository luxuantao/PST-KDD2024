<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-08-23">23 Aug 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vijay</forename><surname>Korthikanti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dmitri</forename><surname>Vainbrand</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Prethvi</forename><surname>Kashinkunti</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Julie</forename><surname>Bernauer</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Nvidia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stanford</forename><surname>University</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Microsoft</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-08-23">23 Aug 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2104.04473v5[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large language models have led to state-of-the-art accuracies across several tasks. However, training these models efficiently is challenging because: a) GPU memory capacity is limited, making it impossible to fit large models on even a multi-GPU server, and b) the number of compute operations required can result in unrealistically long training times. Consequently, new methods of model parallelism such as tensor and pipeline parallelism have been proposed. Unfortunately, naive usage of these methods leads to scaling issues at thousands of GPUs. In this paper, we show how tensor, pipeline, and data parallelism can be composed to scale to thousands of GPUs. We propose a novel interleaved pipelining schedule that can improve throughput by 10+% with memory footprint comparable to existing approaches. Our approach allows us to perform training iterations on a model with 1 trillion parameters at 502 petaFLOP/s on 3072 GPUs (per-GPU throughput of 52% of theoretical peak).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Transformer-based language models <ref type="bibr">[13, 27, 33-35, 42, 46]</ref> in Natural Language Processing (NLP) have driven rapid progress in recent years as computation at scale has become more available and datasets have become larger. Recent work <ref type="bibr" target="#b8">[11,</ref><ref type="bibr" target="#b37">40]</ref> has shown large language models to be effective zero-or few-shot learners, with high accuracy on many NLP tasks and datasets. These large language models have a number of exciting downstream applications such as client feedback summarization, automatic dialogue generation, semantic search, and code autocompletion <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">4,</ref><ref type="bibr">5]</ref>. As a result, the number of parameters in state-of-the-art NLP models have grown at an exponential rate (Figure <ref type="figure" target="#fig_0">1</ref>). Training such models, however, is challenging for two reasons: (a) it is no longer possible to fit the parameters of these models in the main memory of even the largest GPU (NVIDIA recently released 80GB-A100 cards), and (b) even if we are able to fit the model in a single GPU (e.g., by swapping parameters between host and device memory <ref type="bibr" target="#b35">[38]</ref>), the high number of compute operations required can result in unrealistically long training times (e.g., training GPT-3 with 175 billion parameters <ref type="bibr" target="#b8">[11]</ref> would require approximately 288 years with a single V100 NVIDIA GPU). This calls for parallelism. Data-parallel scale-out usually works well, but suffers from two limitations: a) beyond a point, the per-GPU batch size becomes too small, reducing GPU utilization and increasing communication cost, and b) the maximum number of devices that can be used is the batch size, limiting the number of accelerators that can be used for training. Various model parallelism techniques have been proposed to address these two challenges. For example, recent work <ref type="bibr" target="#b36">[39,</ref><ref type="bibr" target="#b37">40]</ref> has shown how tensor (intra-layer) model parallelism, where matrix multiplications within each transformer layer are split over multiple GPUs, can be used to overcome these limitations. Although this approach works well for models of sizes up to 20 billion parameters on NVIDIA DGX A100 servers (with 8 80GB-A100 GPUs), it breaks down for larger models. Larger models need to be split across multiple multi-GPU servers, which leads to two problems: (a) the all-reduce communication required for tensor parallelism needs to go through inter-server links, which are slower than the highbandwidth NVLink <ref type="bibr" target="#b6">[9]</ref> available within a multi-GPU server, and (b) a high degree of model parallelism can create small matrix multiplications (GEMMs), potentially decreasing GPU utilization.</p><p>Pipeline model parallelism <ref type="bibr" target="#b11">[14,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b20">23,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b42">45]</ref> is another technique to support the training of large models, where layers of a model are striped over multiple GPUs. A batch is split into smaller microbatches, and execution is pipelined across these microbatches. Layers can be assigned to workers in various ways, and various schedules for the forward and backward passes of inputs can be used. The layer assignment and scheduling strategy results in different performance tradeoffs. Regardless of schedule, to preserve strict optimizer semantics, optimizer steps need to be synchronized across devices, leading to a pipeline flush at the end of every batch, where microbatches are allowed to complete execution (and no new microbatches are injected). As much as 50% of time can be spent flushing the pipeline depending on the number of microbatches injected into the pipeline. The larger the ratio of number of microbatches to the pipeline size, the smaller the time spent in the pipeline flush. Therefore, to achieve high efficiency, a larger batch size is often necessary. In this work, we also introduce a new pipeline schedule that improves efficiency at small batch sizes.</p><p>Users can thus train their large models using various techniques, each with different tradeoffs. Moreover, these techniques can be combined. However, combining these techniques leads to non-trivial interactions, which need to be reasoned through carefully for good performance. In this paper, we address the following question:</p><p>How should parallelism techniques be combined to maximize the training throughput of large models given a batch size while retaining strict optimizer semantics?</p><p>In particular, we show how to combine pipeline, tensor, and data parallelism, a technique we call PTD-P, to train large language models with good computational performance (52% of peak device throughput) on 1000s of GPUs. Our method leverages the combination of pipeline parallelism across multi-GPU servers, tensor parallelism within a multi-GPU server, and data parallelism, to practically train models with a trillion parameters with graceful scaling in an optimized cluster environment with high-bandwidth links between GPUs on the same server and across servers. We can use similar ideas to train larger models as well, given more training resources. In our experiments, we demonstrate close to linear scaling to 3072 A100 GPUs, with an achieved end-to-end training throughput of 163 teraFLOP/s per GPU (including communication, data processing, and optimization), and an aggregate throughput of 502 petaFLOP/s, on a GPT model <ref type="bibr" target="#b8">[11]</ref> with a trillion parameters using mixed precision. This throughput facilitates practical training times: we estimate end-to-end training of this model to take ∼ 3 months. We believe this is the fastest training throughput achieved for this size of model: past systems <ref type="bibr" target="#b26">[29,</ref><ref type="bibr" target="#b37">40]</ref> cannot train such large models since they do not combine pipeline and tensor parallelism. We also compared to ZeRO <ref type="bibr" target="#b33">[36]</ref>, and found that our approach outperforms ZeRO-3 by 70% for models with 175 and 530 billion parameters due to less cross-node communication. These models are too large to fit on a multi-GPU server.</p><p>Achieving this throughput at scale required innovation and careful engineering along multiple axes: efficient kernel implementations that allowed most of the computation to be compute-bound as opposed to memory-bound, smart partitioning of computation graphs over the devices to reduce the number of bytes sent over network links while also limiting device idle periods, domain-specific communication optimization, and fast hardware (state-of-the-art GPUs and high-bandwidth links between GPUs on the same and different servers). We are hopeful that our open-sourced software (available at https://github.com/nvidia/megatron-lm) will enable other groups to train large NLP models efficiently at scale.</p><p>In addition, we studied the interaction between the various components affecting throughput, both empirically and analytically when possible. Based on these studies, we offer the following guiding principles on how to configure distributed training:</p><p>• Different forms of parallelism interact in non-trivial ways:</p><p>the parallelization strategy has an impact on the amount of communication, the compute efficiency with which kernels are executed, as well as the idle time workers spend waiting for computation due to pipeline flushes (pipeline bubbles). For example, in our experiments, we found that sub-optimal combinations of tensor and pipeline model parallelism can lead to up to 2× lower throughput, even with high-bandwidth network links between servers; tensor model parallelism is effective within a multi-GPU server, but pipeline model parallelism must be used for larger models.</p><p>• The schedule used for pipeline parallelism has an impact on the amount of communication, the pipeline bubble size, and memory used to store activations. We propose a novel interleaved schedule that can improve throughput by as much as 10% compared to previously-proposed schedules <ref type="bibr" target="#b17">[20,</ref><ref type="bibr" target="#b27">30]</ref> with comparable memory footprint. • Values of hyperparameters such as microbatch size have an impact on the memory footprint, the arithmetic efficiency of kernels executed on the worker, and the pipeline bubble size.</p><p>In our experiments, the optimal value of the microbatch size is problem-dependent and can increase throughput by 15%.</p><formula xml:id="formula_0">• At scale, distributed training is communication-intensive.</formula><p>When training a trillion-parameter model on 3072 GPUs, our implementation used an effective bisection bandwidth of 892 GB/s for pipeline-parallel communication, and 13 TB/s for data-parallel communication. Using slower inter-node interconnects or more communication-intensive partitionings would hinder scaling performance.</p><p>We should note that we do not automatically explore the search space of parallelism strategies (such as FlexFlow <ref type="bibr" target="#b19">[22]</ref>, PipeDream <ref type="bibr" target="#b26">[29]</ref>, Tarnawski et al. <ref type="bibr" target="#b38">[41]</ref>, and DAPPLE <ref type="bibr" target="#b11">[14]</ref>), but instead suggest heuristics (in §3) that we found work well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">MODES OF PARALLELISM</head><p>In this section, we discuss the parallelism techniques that facilitate the efficient training of large models that do not fit in the memory of a single GPU. In this work, we combine pipeline model parallelism and tensor model parallelism (combination shown in Figure <ref type="figure">2</ref>) with data parallelism. We call this PTD-P for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data Parallelism</head><p>With data parallelism <ref type="bibr" target="#b22">[25,</ref><ref type="bibr" target="#b40">43]</ref>, each worker has a copy of the full model, the input dataset is sharded, and workers aggregate their gradients periodically to ensure that all workers see a consistent version of the weights. For large models which do not fit on a single worker, data parallelism can be used on smaller model shards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Pipeline Model Parallelism</head><p>With pipeline parallelism, the layers of a model are sharded across multiple devices. When used on models with the same transformer block repeated, each device can be assigned an equal number of transformer layers. We do not consider more asymmetric model architectures, where assignment of layers to pipeline stages is harder; we defer to related work <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b38">41]</ref> to solve this problem.</p><p>A batch is split into smaller microbatches; execution is then pipelined across microbatches. Pipelining schemes need to ensure that inputs see consistent weight versions across forward and backward passes for well-defined synchronous weight update semantics. Specifically, naive pipelining can lead to an input seeing weight updates in the backward pass not seen in the forward pass.</p><p>To retain strict optimizer semantics exactly, we introduce periodic pipeline flushes so that optimizer steps are synchronized across devices. At the start and end of every batch, devices are idle. We call this idle time the pipeline bubble, and want to make it as small as possible. Asynchronous and bounded-staleness approaches such as PipeMare, PipeDream, and PipeDream-2BW <ref type="bibr" target="#b20">[23,</ref><ref type="bibr" target="#b26">29,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b42">45]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline flush</head><p>Figure <ref type="figure">3</ref>: GPipe pipeline schedule with forward passes (blue) for all microbatches (represented by numbers) followed by backward passes (green). The gray area represents the pipeline bubble. For simplicity, we assume that the backward pass takes twice as long as the forward pass. The efficiency of the pipeline schedule does not depend on this factor. Each batch in this example consists of 8 microbatches, and the numbers in each blue or green box are unique identifiers given to the corresponding microbatch (in particular, the first batch consists of microbatches 1 − 8, the second batch consists of microbatches 9 − 16, and so on). The optimizer is stepped and weight parameters updated at the pipeline flush to ensure strict optimizer semantics, leading to idle devices and a pipeline bubble.  away with flushes completely, but relax weight update semantics. We defer consideration of such schemes to future work.</p><p>There are several possible ways of scheduling forward and backward microbatches across devices; each approach offers different tradeoffs between pipeline bubble size, communication, and memory footprint. We discuss two such approaches in this section. <ref type="bibr" target="#b17">[20]</ref> proposes a schedule where the forward passes for all microbatches in a batch are first executed, followed by backward passes for all microbatches (shown in Figure <ref type="figure">3</ref>). We can quantify the size of GPipe's pipeline bubble (𝑡 𝑝𝑏 ). We denote the number of microbatches in a batch as 𝑚, the number of pipeline stages (number of devices used for pipeline parallelism) as 𝑝, the ideal time per iteration as 𝑡 𝑖𝑑 (assuming perfect or ideal scaling), and the time to execute a single microbatch's forward and backward pass as 𝑡 𝑓 and 𝑡 𝑏 . In this schedule, the pipeline bubble consists of 𝑝 − 1 forward passes at the start of a batch, and 𝑝 − 1 backward passes at the end. The total amount of time spent in the pipeline bubble is then 𝑡 𝑝𝑏 = (𝑝 − 1) • (𝑡 𝑓 + 𝑡 𝑏 ). The ideal processing time for the batch is 𝑡 𝑖𝑑 = 𝑚 • (𝑡 𝑓 + 𝑡 𝑏 ). Therefore, the fraction of ideal computation time spent in the pipeline bubble is:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Default Schedule. GPipe</head><formula xml:id="formula_1">Bubble time fraction (pipeline bubble size) = 𝑡 𝑝𝑏 𝑡 𝑖𝑑 = 𝑝 − 1 𝑚 .</formula><p>For the bubble time fraction to be small, we thus need 𝑚 ≫ 𝑝. However, for such large 𝑚, this approach has a high memory footprint as it requires stashed intermediate activations (or just input activations for each pipeline stage when using activation recomputation) to be kept in memory for all 𝑚 microbatches through the lifetime of a training iteration.</p><p>Instead, we use the PipeDream-Flush schedule <ref type="bibr" target="#b27">[30]</ref>. In this schedule, we first enter a warm-up phase where workers perform differing numbers of forward passes as shown in Figure <ref type="figure" target="#fig_1">4 (top)</ref>. This schedule limits the number of in-flight microbatches (the number of microbatches for which the backward pass is outstanding and activations need to be maintained) to the depth of the pipeline, instead of the number of microbatches in a batch. After the warm-up phase, each worker then enters a steady state, where workers perform one forward pass followed by one backward pass (1F1B for short). Finally, at the end of a batch, we complete backward passes for all remaining in-flight microbatches. The time spent in the bubble is the same for this new schedule, but the number of outstanding forward passes is at most the number of pipeline stages for the PipeDream-Flush schedule. As a result, this schedule requires activations to be stashed for 𝑝 or fewer microbatches (compared to 𝑚 microbatches for the GPipe schedule). Consequently, when 𝑚 ≫ 𝑝, PipeDream-Flush is much more memory-efficient than GPipe.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Schedule with Interleaved Stages.</head><p>To reduce the size of the pipeline bubble, each device can perform computation for multiple subsets of layers (called a model chunk), instead of a single contiguous set of layers. For example, if each device had 4 layers before (i.e., device 1 had layers 1 − 4, device 2 had layers 5 − 8, and so on), we could have each device perform computation for two model chunks (each with 2 layers), i.e., device 1 has layers 1, 2, 9, 10; device 2 has layers 3, 4, 11, 12; and so on. With this scheme, each device in the pipeline is assigned multiple pipeline stages (each pipeline stage has less computation compared to before).</p><p>As before, we can use an "all-forward, all-backward" version of this schedule, but this has a high memory footprint (proportional to 𝑚). Instead, we developed an interleaved schedule that adapts the memory-efficient 1F1B schedule from before. This new schedule is shown in Figure <ref type="figure" target="#fig_1">4</ref>, and requires the number of microbatches in a batch to be an integer multiple of the degree of pipeline parallelism (number of devices in the pipeline). For example, with 4 devices, the number of microbatches in a batch must be a multiple of 4.</p><p>As shown in Figure <ref type="figure" target="#fig_1">4</ref>, the pipeline flush for the same batch size happens sooner in the new schedule. If each device has 𝑣 stages (or model chunks), then the forward and backward time for a microbatch for each stage or chunk will now be 𝑡 𝑓 /𝑣 and 𝑡 𝑏 /𝑣.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The pipeline bubble time thus reduces to 𝑡</head><formula xml:id="formula_2">int. 𝑝𝑏 = (𝑝−1) •(𝑡 𝑓 +𝑡 𝑏 ) 𝑣</formula><p>, and the bubble time fraction is then:</p><formula xml:id="formula_3">Bubble time fraction (pipeline bubble size) = 𝑡 int. 𝑝𝑏 𝑡 𝑖𝑑 = 1 𝑣 • 𝑝 − 1 𝑚 .</formula><p>This means that the new schedule reduces the bubble time by 𝑣. This reduced pipeline bubble size, however, does not come for free: this schedule requires extra communication. Quantitatively, the amount of communication also increases by 𝑣. In the next section, we discuss how we can utilize the 8 InfiniBand networking cards in a multi-GPU server (e.g., a DGX A100 node) to reduce the impact of this extra communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Tensor Model Parallelism</head><p>With tensor model parallelism, individual layers of the model are partitioned over multiple devices. In this paper, we use the particular partitioning strategy used by Megatron <ref type="bibr" target="#b37">[40]</ref> for transformer layers, the bedrock of language models. We can apply similar ideas to other types of models, like CNNs, as well. We briefly outline this strategy, illustrated in Figure <ref type="figure">5</ref>, below.</p><p>A transformer layer consists of a self-attention block followed by a two-layer multi-layer perceptron (MLP). Further details of the transformer layer can be found in Vaswani et al <ref type="bibr" target="#b39">[42]</ref>.</p><p>The MLP block consists of two GEMMs and a GeLU non-linearity:</p><formula xml:id="formula_4">𝑌 = GeLU(𝑋𝐴). 𝑍 = Dropout(𝑌 𝐵).</formula><p>We can split 𝐴 along its columns 𝐴 = [𝐴 1 , 𝐴 2 ]. This partitioning allows the GeLU non-linearity to be independently applied to the output of each partitioned GEMM:</p><formula xml:id="formula_5">[𝑌 1 , 𝑌 2 ] = [GeLU(𝑋𝐴 1 ), GeLU(𝑋𝐴 2 )].</formula><p>This is advantageous as it removes the need for synchronization (needed if 𝐴 is split along its rows since GeLU is non-linear).</p><p>The rows of the second weight matrix 𝐵 can then be split along its rows to remove the need for any communication between the GEMMs (shown in Figure <ref type="figure">5a</ref>), as shown below:</p><formula xml:id="formula_6">𝐵 = 𝐵 1 𝐵 2 , 𝑌 = [𝑌 1 , 𝑌 2 ].</formula><p>The output of the second GEMM is then reduced across the GPUs before the dropout layer. We exploit the inherent parallelism in the multi-head attention operation to partition the self-attention block (shown in Figure <ref type="figure">5b</ref>). The key (𝐾), query (𝑄), and value (𝑉 ) matrices can be partitioned in a column-parallel fashion. The output linear layer can then directly operate on the partitioned output of the attention operation (weight matrix partitioned across rows).</p><p>This approach splits GEMMs in the MLP and self-attention blocks across GPUs while requiring only two all-reduce operations in the forward pass (𝑔 operator) and two all-reduces in the backward pass (𝑓 operator). We implemented 𝑓 and 𝑔 in a few lines of code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PERFORMANCE ANALYSIS OF PARALLELIZATION CONFIGURATIONS</head><p>In this section, we consider the performance implications of combining pipeline and tensor model parallelism with data parallelism. Given a fixed budget of GPUs and batch size, one can use different degrees of the parallelism types in PTD-P to train models; each dimension exposes tradeoffs between memory footprint, device utilization, and amount of communication.</p><p>We discuss these tradeoffs in the rest of this section, and then show empirical results in §5.4. We present analytical models where</p><formula xml:id="formula_7">GeLU GeLU Dropout 𝑌 = GeLU(𝑋𝐴) 𝑍 = Dropout(𝑌𝐵) 𝐴 = [𝐴 ! , 𝐴 " ] 𝐵 = 𝐵 ! 𝐵 " 𝑌 ! 𝑌 " 𝑋𝐴 ! 𝑋𝐴 " 𝑋 𝑋 𝑓 𝑋 𝑌 ! 𝐵 ! 𝑌 " 𝐵 " 𝑔 𝑍 𝑍 ! 𝑍 " (a) MLP. Dropout Softmax Dropout Softmax Dropout 𝐵 = 𝐵 ! 𝐵 " 𝑍 = Dropout(𝑌𝐵) 𝑌 ! 𝐵 ! 𝑌 " 𝐵 " 𝑍 ! 𝑍 " 𝑍 𝑌 = Self-Attention(𝑋) Split attention heads → &amp; 𝑄 = [𝑄 ! , 𝑄 " ] 𝐾 = [𝐾 ! , 𝐾 " ] 𝑉 = [𝑉 ! ,𝑉 " ] 𝑔 𝑓 𝑋 𝑋 𝑋 𝑌 " 𝑌 ! 𝑉 ! 𝑄 ! 𝐾 ! 𝐾 " 𝑄 " 𝑉 " (b) Self-Attention.</formula><p>Figure <ref type="figure">5</ref>: Blocks of transformer model partitioned with tensor model parallelism (figures borrowed from Megatron <ref type="bibr" target="#b37">[40]</ref>). 𝑓 and 𝑔 are conjugate. 𝑓 is the identity operator in the forward pass and allreduce in the backward pass, while 𝑔 is the reverse.</p><p>relevant for the pipeline bubble size. We qualitatively describe how communication time behaves and present cost models for amount of communication; however, we do not present direct cost models for communication time, which is harder to model for a hierarchical network topology where interconnects between GPUs on the same server have higher bandwidth than interconnects between servers. To the best of our knowledge, this is the first work to analyze the performance interactions of these parallelization dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notation</head><p>We use the following notation in this section:</p><p>• (𝑝, 𝑡, 𝑑): Parallelization dimensions. 𝑝 for the pipeline-modelparallel size, 𝑡 for the tensor-model-parallel size, and 𝑑 for the data-parallel size. • 𝑛: Number of GPUs. We require 𝑝 • 𝑡 • 𝑑 = 𝑛.</p><p>• 𝐵: Global batch size (provided as input).</p><p>• 𝑏: Microbatch size.</p><p>• 𝑚 = 1 𝑏 • 𝐵 𝑑 : Number of microbatches in a batch per pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Tensor and Pipeline Model Parallelism</head><p>Tensor and pipeline model parallelism can both be used to partition a model's parameters over multiple GPUs. As stated earlier, using pipeline parallelism with periodic flushes results in a pipeline bubble of size (𝑝 − 1)/𝑚. Let us assume that 𝑑 = 1 (data-parallel size); consequently, 𝑡 • 𝑝 = 𝑛. The pipeline bubble size in terms of 𝑡 is:</p><formula xml:id="formula_8">𝑝 − 1 𝑚 = 𝑛/𝑡 − 1 𝑚 .</formula><p>As 𝑡 increases, the pipeline bubble thus decreases for fixed 𝐵, 𝑏, and 𝑑 (𝑚 = 𝐵/(𝑏 • 𝑑) is fixed as well). The amount of communication performed between different GPUs is also affected by the values of 𝑝 and 𝑡. Pipeline model parallelism features cheaper point-to-point communication. Tensor model parallelism, on the other hand, uses all-reduce communication (two all-reduce operations each in the forward and backward pass, see §2.3). With pipeline parallelism, the total amount of communication that needs to be performed between every pair of consecutive devices (for either the forward or backward pass) for each microbatch is 𝑏𝑠ℎ, where 𝑠 is the sequence length and ℎ is the hidden size. With tensor model parallelism, tensors of total size 𝑏𝑠ℎ need to be all-reduced among 𝑡 model replicas twice each in the forward and backward pass for each layer, leading to a total communication of 8𝑏𝑠ℎ 𝑡 −1 𝑡 per layer per device for each microbatch. Each device typically has multiple layers; the total amount of tensor-parallel-communication per device for each microbatch</p><formula xml:id="formula_9">(EXETEVEPPIPWM^IH 4MTIPMRIFYFFPIWM^I R!F! R!F! R!F! R!F!</formula><formula xml:id="formula_10">is then 𝑙 stage • 8𝑏𝑠ℎ 𝑡 −1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑡</head><p>, where 𝑙 stage is the number of layers in a pipeline stage.</p><p>Consequently, we see that tensor model parallelism increases the amount of communication between devices. Thus, when 𝑡 is larger than the number of GPUs in a single node, the overhead of performing tensor model parallelism across slower inter-node links can be impractical. We see these results empirically in §5.4.</p><p>Takeaway #1: When considering different forms of model parallelism, tensor model parallelism should generally be used up to degree 𝑔 when using 𝑔-GPU servers, and then pipeline model parallelism can be used to scale up to larger models across servers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Data and Model Parallelism</head><p>We also want to consider the interaction between data parallelism and the two types of model parallelism. In this section, we consider these interactions independently for simplicity. As 𝑑 becomes larger, 𝑛 − 𝑑 becomes smaller, and thus the pipeline bubble becomes smaller. Figure <ref type="figure" target="#fig_2">6</ref> shows the behavior of the pipeline bubble size for various values of 𝑑, 𝑛, and 𝑏 ′ . It might not be possible to increase 𝑑 all the way to 𝑛 for all models, since a model's full training memory footprint might be larger than the memory capacity of a single accelerator. Overall throughput will thus increase if the all-reduce communication needed for data parallelism does not drastically increase with higher 𝑑, which should hold since the communication time for a ring-based implementation scales with 𝑑−1 𝑑 = 1 − 1 𝑑 . We can also analyze the impact of increasing the batch size 𝐵. For a given parallel configuration, as the batch size 𝐵 increases, 𝑏 ′ = 𝐵/𝑏 increases, (𝑛 − 𝑑)/𝑏 ′ decreases, consequently increasing throughput. All-reduce communication required by data parallelism also becomes more infrequent, further increasing throughput.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Data and Tensor Model</head><p>Parallelism. With tensor model parallelism, all-reduce communication needs to be performed for every microbatch. This can be expensive across multi-GPU servers. On the other hand, data parallelism only needs to perform expensive all-reduce communication once per batch. Moreover, with tensor model parallelism, each model-parallel rank performs a subset of the computation in each model layer, and thus for insufficientlylarge layers, modern GPUs might not perform these sub-matrix computations with peak efficiency.</p><p>Takeaway #2: When using data and model parallelism, a total model-parallel size of 𝑀 = 𝑡 • 𝑝 should be used so that the model's parameters and intermediate metadata fit in GPU memory; data parallelism can be used to scale up training to more GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Microbatch Size</head><p>The choice of the microbatch size 𝑏 also affects model-training throughput. For example, we see in Figure <ref type="figure">7</ref> that per-GPU throughput increases by up to 1.3× with a larger microbatch size on a single GPU. We now want to determine the optimal microbatch size 𝑏 given a parallel configuration (𝑝, 𝑡, 𝑑) and batch size 𝐵. The amount of data-parallel communication will be the same regardless of the microbatch size. Given functions 𝑡 𝑓 (𝑏) and 𝑡 𝑏 (𝑏) that map the microbatch size to the forward and backward computation times for a single microbatch, the total time spent computing a batch, ignoring communication cost, is (as before, define 𝑏 ′ as 𝐵/𝑑): The microbatch size thus affects both the arithmetic intensity of operations as well as the pipeline bubble size (by affecting 𝑚). Figure <ref type="figure" target="#fig_4">8</ref> shows estimated throughput (equation (1) used to estimate processing time) for a GPT model with a billion parameters and (𝑝, 𝑡) = (8, 8). The optimal 𝑏 for both batch sizes is 4.</p><formula xml:id="formula_11">𝑏 ′ /𝑏 + 𝑝 − 1 • 𝑡 𝑓 (𝑏) + 𝑡 𝑏 (𝑏) .<label>(1)</label></formula><p>Takeaway #3: The optimal microbatch size 𝑏 depends on the throughput and memory footprint characteristics of the model, as well as the pipeline depth 𝑝, data-parallel size 𝑑, and batch size 𝐵.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Activation Recomputation</head><p>Activation recomputation <ref type="bibr" target="#b9">[12,</ref><ref type="bibr" target="#b15">18,</ref><ref type="bibr" target="#b17">20,</ref><ref type="bibr" target="#b18">21]</ref> is an optional technique that trades off an increase in the number of compute operations performed for additional memory footprint, by running the forward pass a second time just before the backward pass (and stashing only the input activations for a given pipeline stage, as opposed to the entire set of intermediate activations, which is much larger). Activation recomputation is required to train reasonably large models with pipeline parallelism to keep memory footprint acceptably low. Previous work like PipeDream-2BW <ref type="bibr" target="#b27">[30]</ref> has looked at the performance ramifications of activation recomputation.</p><p>The number of activation checkpoints does not impact throughput, but impacts memory footprint. Let 𝐴 input be the size of the input activations of a layer, and 𝐴 intermediate be the size of intermediate activations per layer. If a model stage has 𝑙 layers, and if 𝑐 is the number of checkpoints, the total memory footprint is going to be 𝑐</p><formula xml:id="formula_12">• 𝐴 input +𝑙/𝑐 • 𝐴 intermediate . The minimum value of this function is obtained when 𝑐 = √︃ 𝑙 • 𝐴 intermediate /𝐴 input .</formula><p>In practice, we measure 𝐴 intermediate empirically. For most cases, checkpointing every 1 or 2 transformer layers is optimal.</p><p>Other techniques such as activation partitioning <ref type="bibr" target="#b33">[36]</ref> can also be used in conjunction with tensor model parallelsim to reduce the memory footprint due to activations further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPLEMENTATION</head><p>We implemented PTD-P as an extension to the Megatron-LM codebase. Our implementation is built using PyTorch <ref type="bibr" target="#b29">[32]</ref>. We use NCCL <ref type="bibr" target="#b4">[7]</ref> for communication between devices. To obtain good performance, we implemented optimizations targeting both communication and computation, which we outline below. Figure <ref type="figure">9</ref>: Scatter/gather communication optimization. Light blue blocks are layers in the first pipeline stage, and dark blue blocks are layers in the second pipeline stage. Without the scatter/gather optimization, the same tensor is sent redundantly over inter-node InfiniBand links. Instead, at the sender, we can scatter the tensor into smaller chunks, reducing the sizes of tensors sent over Infini-Band links. The final tensor can then be rematerialized at the receiver using a gather operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Communication Optimizations</head><p>When using pipeline parallelism, we want to send and receive tensors in the forward and backward direction in parallel. Each DGX A100 is equipped with 8 InfiniBand (IB) networking cards. Unfortunately, sends and receives are point-to-point, and only happen between a pair of GPUs on two servers, making it hard to leverage all 8 cards for a single communication call within the pipeline. However, we can leverage the fact that we use both tensor model parallelism and pipeline model parallelism to reduce the overhead of cross-node communication. In particular, we note that the output of each transformer layer is replicated (after 𝑔 in MLP block, see Figure <ref type="figure">5a</ref>) across the tensor-parallel ranks. As a result, ranks in two consecutive pipeline stages that are performing tensor model parallelism send and receive the exact same set of tensors (Figure <ref type="figure">9a</ref>).</p><p>For large enough models, we use a tensor-model-parallel size of 8. This means we are sending the same set of tensors 8 times between corresponding GPUs on adjacent multi-GPU servers. To reduce this redundancy, we can instead split the tensor on the send side into equal-sized chunks, and then only send one chunk to the corresponding rank on the next node using the rank's own InfiniBand card (e.g., rank 1 sends to rank 3 and rank 2 sends to rank 4 in Figure <ref type="figure">9</ref>). With 8 tensor-model-parallel ranks, each chunk would be one-eighth smaller. Then, on the receive side, we can perform an all-gather over NVLink, which is much faster than the InfiniBand interconnect, to re-materialize the full tensor. This is shown in Figure <ref type="figure">9b</ref>. We call this the scatter/gather communication optimization. This optimization helps better leverage the multiple IB cards on the DGX A100 servers, and makes more communicationintensive schedules such as the interleaved one feasible.</p><p>Quantitatively, with the scatter-gather communication optimization, the total amount of communication that needs to be performed between every pair of consecutive stages is reduced to 𝑏𝑠ℎ 𝑡 , where 𝑡 is the tensor-model-parallel size, 𝑠 is the sequence length, and ℎ is the hidden size (𝑡 = 8 in our experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Computation Optimizations</head><p>We implemented three model-specific optimizations to the computation graph to attain high performance. First, we changed the data layout in the transformer layer to avoid memory-intensive transpose operations, and to enable the use of strided batched GEMM kernels. Specifically, we changed the data layout from [𝑏, 𝑠, 𝑎, ℎ] to [𝑠, 𝑏, 𝑎, ℎ], where 𝑏, 𝑠, 𝑎, and ℎ are batch, sequence, attention-head, and hidden-size dimensions, respectively. Second, we generated fused kernels for a sequence of element-wise operations (bias + GeLU and bias + dropout + add) using PyTorch JIT <ref type="bibr" target="#b7">[10]</ref>. Third, we created two custom kernels to enable the fusion of scale, mask, and softmax (reduction) operations: one to support general masking (used in models such as BERT) and another to support implicit causal masking (used in auto-regressive models such as GPT). We quantify the effect of these optimizations in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION</head><p>In this section, we seek to answer the following questions:</p><p>• How well does PTD-P perform? Does it result in realistic end-to-end training times? • How well does pipeline parallelism scale for a given model and batch size? How much impact does the interleaved schedule have on performance? • How do different parallelization dimensions interact with each other? What is the impact of hyperparameters such as microbatch size? • What is the impact of the scatter-gather communication optimization? What types of limits do we put on hardware when running training iterations at scale? All of our results are run with mixed precision on the Selene supercomputer <ref type="bibr" target="#b5">[8]</ref>. Each cluster node has 8 NVIDIA 80-GB A100 GPUs <ref type="bibr" target="#b3">[6]</ref>, connected to each other by NVLink and NVSwitch <ref type="bibr" target="#b6">[9]</ref>. Each node has eight NVIDIA Mellanox 200Gbps HDR Infiniband HCAs for application communication, with an additional two HCAs per node for dedicated storage. The nodes are connected in a threelevel (leaf, spine, core) fat-tree topology with 850 switches. This topology allows efficient all-reduce communication (dominant communication pattern in deep learning training). The cluster uses an all-NVME shared parallel filesystem for high-performance data access and storage. The peak device throughput of an A100 GPU with 16-bit precision is 312 teraFLOP/s. For most of our results, we report throughput per GPU. Aggregate throughput can be computed by multiplying with the number of GPUs used.</p><p>For our experiments, we use GPT models of appropriate sizes. In particular, for any given microbenchmark, the model needs to fit on the number of model-parallel GPUs used in the experiment. We use standard model architectures such as GPT-3 <ref type="bibr" target="#b8">[11]</ref> when appropriate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">End-to-End Performance</head><p>We consider the end-to-end performance of our system on GPT models ranging from a billion to a trillion parameters, using tensor, pipeline, and data parallelism (degrees picked using heuristics described in §3). In particular, we use the interleaved pipeline schedule with the scatter/gather optimization enabled. All models use a vocabulary size (denoted by 𝑉 ) of 51,200 (multiple of 1024) and a sequence length (𝑠) of 2048. We vary hidden size (ℎ), number of attention heads, and number of layers (𝑙). The number of parameters in a model, 𝑃, can be computed as: As the model size increases, we also increase the batch size (𝐵) and the number of GPUs (𝑛). The majority of floating-point operations in the model are performed in the matrix multiplications (GEMMs) in the transformer and logit layers. Considering just these GEMMs, the number of FLOPs per iteration is (more details in the Appendix):</p><formula xml:id="formula_13">𝑃 = 12𝑙ℎ 2 1 + 13 12ℎ + 𝑉 + 𝑠 12𝑙ℎ . (<label>2</label></formula><formula xml:id="formula_14">)</formula><formula xml:id="formula_15">𝐹 = 96𝐵𝑠𝑙ℎ 2 1 + 𝑠 6ℎ + 𝑉 16𝑙ℎ . (<label>3</label></formula><formula xml:id="formula_16">)</formula><p>This is a lower bound for the true FLOP count but should be close to the actual value. We count a FLOP as a floating-point operation regardless of precision. We also note that equation ( <ref type="formula" target="#formula_15">3</ref>) assumes activation recomputation and takes into account the floating-point operations associated with the extra forward pass.</p><p>Table <ref type="table" target="#tab_2">1</ref> shows the model configurations along with the achieved FLOP/s (both per GPU and aggregate over all GPUs). We see superlinear scaling to 3072 A100 GPUs (384 DGX A100 nodes), since GPU utilization improves as the models get larger (larger matrix multiplications) without significant increase in the communication time relative to computation time. Note that throughput is measured for end-to-end training, i.e., includes all operations including data loading, optimizer steps, communication, and logging. We achieve 52% of peak device throughput for the largest model, and 44% of peak device throughput for the smallest model.</p><p>Training Time Estimates. Given these throughputs, we can also estimate the total amount of time needed for end-to-end training on 𝑇 tokens. Training requires 𝐼 = 𝑇 /(𝐵 • 𝑠) iterations. Using the value of 𝐹 from equation ( <ref type="formula" target="#formula_15">3</ref>) and empirical end-to-end throughputs from  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison to ZeRO-3</head><p>We compare PTD-P to ZeRO-3 <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b34">37]</ref> in Table <ref type="table" target="#tab_5">2</ref> and Figure <ref type="figure" target="#fig_6">10</ref> for the standard GPT-3 model architecture, as well as the 530-billionparameter model from Table <ref type="table" target="#tab_2">1</ref>. The results provide a point of comparison to a method that does not use model parallelism. We integrated ZeRO into our codebase using the DeepSpeed Python library <ref type="bibr" target="#b1">[3]</ref>. We keep the global batch size the same as we increase the number of GPUs. With fewer GPUs and a microbatch size of 4, PTD-P results in 6% and 24% higher throughput for the 175-and 530-billion-parameter models respectively. As we increase the number of GPUs, PTD-P scales more gracefully than ZeRO-3 in isolation (see Figure <ref type="figure" target="#fig_6">10</ref>). For example, by doubling the number of GPUs (keeping the batch size the same), PTD-P outperforms ZeRO-3 by 70% for both models due to less cross-node communication. We note that we have only considered ZeRO-3 without tensor parallelism. ZeRO-3 can be combined with model parallelism to potentially improve its scaling behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pipeline Parallelism</head><p>We now evaluate the weak-scaling performance of pipeline parallelism in isolation, and also compare the performance of the noninterleaved schedule to the interleaved schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Weak</head><p>Scaling. We evaluate the scaling of the default noninterleaved pipeline-parallel schedule using a weak scaling setup, a GPT model with 128 attention heads and a hidden size of 20480,   and a microbatch size of 1. As we increase the number of pipeline stages, we also increase the size of the model by proportionally increasing the number of layers in the model, e.g., with a pipelineparallel size of 1, we use a model with 3 transformer layers and 15 billion parameters, and with a pipeline-parallel size of 8, we use a model with 24 transformer layers and 121 billion parameters. We use a tensor-parallel size of 8 for all configurations, and vary the total number of A100 GPUs used from 8 to 64. Figure <ref type="figure" target="#fig_0">11</ref> shows throughput per GPU for two different batch sizes to illustrate the impact of the pipeline bubble, which behaves as 𝑝−1 𝑚 ( §2.2.1). As expected, the higher batch size scales better since the pipeline bubble is amortized over more microbatches. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Interleaved versus</head><p>Non-Interleaved Schedule. Figure <ref type="figure" target="#fig_0">12</ref> shows the per-GPU-throughput for interleaved and non-interleaved schedules on the GPT-3 <ref type="bibr" target="#b8">[11]</ref> model with 175 billion parameters (96 layers, 96 attention heads, hidden size of 12288). The interleaved schedule with the scatter/gather communication optimization has higher computational performance than the non-interleaved (default) schedule. This gap closes as the batch size increases due to two reasons: (a) as the batch size increases, the bubble size in the default schedule decreases, and (b) the amount of point-to-point communication within the pipeline is proportional to the batch size, and consequently the non-interleaved schedule catches up as the amount of communication increases (the interleaved schedule features more communication per sample). Without the scatter/gather optimization, the default schedule performs better than the interleaved schedule at larger batch sizes (not shown).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison of Parallel Configurations</head><p>In this sub-section, we show the various tradeoffs associated with combining different parallelization dimensions. In particular, we show the performance for parallel configurations using the same number of GPUs for a given model and multiple batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.1</head><p>Tensor versus Pipeline Parallelism. We evaluate the impact of pipeline and tensor model parallelism on performance for a given model and batch size. The empirical results in Figure <ref type="figure" target="#fig_8">13</ref> show the importance of using both tensor and pipeline model parallelism in conjunction to train a 161-billion-parameter GPT model (32 transformer layers to support pipeline-parallel size of 32, 128 attention heads, hidden size of 20480) with low communication overhead and high compute resource utilization. We observe that tensor model parallelism is best within a node (DGX A100 server) due to its expensive all-reduce communication. Pipeline model parallelism, on the other hand, uses much cheaper point-to-point communication that can be performed across nodes without bottlenecking the entire computation. However, with pipeline parallelism, significant time can be spent in the pipeline bubble: the total number of pipeline stages should thus be limited so that the number of microbatches in the pipeline is a reasonable multiple of the number of pipeline stages. Consequently, we see peak performance when the tensorparallel size is equal to the number of GPUs in a single node (8 with DGX A100 nodes). This result indicates that neither tensor model parallelism (used by Megatron <ref type="bibr" target="#b37">[40]</ref>) nor pipeline model parallelism (used by PipeDream <ref type="bibr" target="#b27">[30]</ref> and others) in isolation can match the performance of using both techniques in conjunction. 5.4.2 Pipeline versus Data Parallelism. We evaluate the impact of data and pipeline model parallelism on performance for a GPT model with 5.9 billion parameters (32 transformer layers, 32 attention heads, hidden size of 3840) in Figure <ref type="figure" target="#fig_1">14</ref>. We use a smaller model than before since we want to show performance for models that fit when the model-parallel size is only 2. For simplicity, we keep the microbatch size equal to 1 in these experiments. We see that for each batch size, the throughput decreases as the pipelineparallel size increases, matching our analytical model from §3.3. Pipeline model parallelism should be used primarily to support the training of large models that do not fit on a single worker, and data parallelism should be used to scale up training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.3</head><p>Tensor versus Data Parallelism. We also evaluate the impact of data and tensor model parallelism on performance for the same GPT model with 5.9 billion parameters in Figure <ref type="figure" target="#fig_0">15</ref> (smaller model used for same reason as above). As before, we keep the microbatch size equal to 1 initially. With larger batch sizes and a microbatch size of 1, data-parallel communication is infrequent; the all-to-all communication required in tensor model parallelism needs to be performed for every microbatch in a batch. This all-to-all communication with tensor model parallelism dominates end-to-end training time, especially when communication needs to be performed across multi-GPU nodes. Additionally, as the tensor-model-parallel size increases, we perform smaller matrix multiplications on every GPU, decreasing utilization on each GPU.</p><p>We should note that although data parallelism can lead to efficient scaling, we cannot use data parallelism in isolation for very large models with a limited training batch size because of a) insufficient memory capacity, and b) scaling limitations of data parallelism (e.g., GPT-3 was trained to convergence with a batch size of 1536. Data parallelism thus supports parallelization to only 1536 GPUs; however, roughly 10, 000 GPUs were used to train this model in a reasonable amount of time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Microbatch Size</head><p>We evaluate the impact of the microbatch size on the performance of parallel configurations that combine pipeline and tensor model parallelism in Figure <ref type="figure" target="#fig_2">16</ref> for a model with 91 billion parameters ((𝑡, 𝑝) = (8, 8)). We see that the best microbatch size is 2 for this model; the optimal microbatch size is different for other models (not shown in Figure <ref type="figure">)</ref> and model-dependent. For a given batch size, increasing the microbatch size decreases the number of microbatches in the pipeline (𝑚), leading to a larger pipeline bubble; however, increasing the microbatch size can also improve GPU utilization by increasing the arithmetic intensity of executed kernels. These two factors are at odds with each other, which makes the choice of optimal microbatch size challenging. Our analytical model from §3.3 reasonably approximates true performance, and can be used as a proxy to determine how to pick this hyperparameter value for various training configurations and models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp;EXGLWM^I 8LVSYKLTYX WIUYIRGIWWIGSRH %GXVIGSQTYXEXMSR ;SEGXVIGSQTYXEXMSR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Activation Recomputation</head><p>Figure <ref type="figure" target="#fig_0">17</ref> shows throughput with and without activation recomputation for a GPT model with 145 billion parameters (80 transformer layers, 96 attention heads, hidden size of 12288) using 128 A100 GPUs, (𝑡, 𝑝) = (8, 16), and a range of batch sizes. For small batch sizes, activation recomputation leads to up to 33% lower throughput (in sequences per second) due to the extra forward pass that needs to be executed during the backward pass. However, activation recomputation is needed to support larger batch sizes. Throughput at large batch sizes with activation recomputation is up to 2× higher than the best throughput achieved without activation recomputation (for a smaller batch size) due to a smaller pipeline bubble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Scatter-Gather Optimization</head><p>Figure <ref type="figure" target="#fig_4">18</ref> shows per-GPU-throughput with and without (unoptimized) the scatter/gather communication optimization for the GPT-3 model with 175 billion parameters. We see an improvement of up to 11% in throughput for communication-intensive schedules (large batch size with interleaving) by reducing the amount of communication over cross-node links.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Fused Operators</head><p>We also evaluate the performance impact of operator fusion described in §4.2. For the GPT-3 model (175 billion parameters), throughput increased by 19% with fusion (113 teraFLOP/s per GPU to 135 teraFLOP/s per GPU). For the larger GPT model with 530 billion parameters (model configuration in Figure <ref type="figure" target="#fig_0">1</ref>), throughput increased by 11% (133 teraFLOP/s per GPU to 148 teraFLOP/s per GPU).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Inter-Node Communication Bandwidth</head><p>Our strong results are a byproduct of using an optimized software and hardware stack together. In particular, we take advantage of the high-bandwidth communication links between GPUs on the same server and across servers. On the trillion-parameter model with 3072 GPUs, we observed that the effective bisection bandwidth of point-to-point communication among pipeline stages is 892 GB/s, while the effective bisection bandwidth of all-reduce operations among data-parallel replicas is 12.9 TB/s. A less-optimized partitioning of operators across devices would lead to more inter-node communication, hampering scaling performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.10">Checkpoint Loading and Saving</head><p>An important practical consideration for the training of large models is loading and saving model checkpoints, which are especially large for the models considered in this paper. For example, the trillion-parameter model has a checkpoint of size 13.8 terabytes.</p><p>The initial load of checkpoints for the trillion-parameter model by all 384 nodes (3072 GPUs) reaches a peak read bandwidth of 1TB/s, the maximum read throughput possible from the parallel filesystem. Checkpoint saves reach 40% of peak write bandwidth (273 GB/s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RELATED WORK</head><p>In this section, we discuss other techniques to train models at scale.</p><p>Parallelism for Large Models. Pipeline model parallelism is a common technique used to train large models. Pipeline parallelism comes in a few flavors: the mode discussed in this paper uses flushes to ensure strict optimizer semantics. TeraPipe <ref type="bibr" target="#b23">[26]</ref> exposes finegrained pipeline parallelism across tokens in a single training sequence for auto-regressive models like GPT. PipeTransformer <ref type="bibr" target="#b16">[19]</ref> elastically adjusts the degree of pipelining and data parallelism by freezing layers with "stable" weights, and instead dedicates resources to train the remaining "active" layers. HetPipe <ref type="bibr" target="#b28">[31]</ref> uses a combination of pipeline and data parallelism on a set of heterogeneous accelerators. Pipeline parallelism can also be implemented with relaxed semantics: PipeDream-2BW <ref type="bibr" target="#b27">[30]</ref> maintains two weight versions and guarantees 1-stale weight updates without expensive flushes, while PipeMare <ref type="bibr" target="#b42">[45]</ref> and Kosson et al. <ref type="bibr" target="#b20">[23]</ref> use asynchoronous pipeline parallelism. These techniques have improved throughput compared to the techniques with pipeline flushes considered in this paper, but potentially at the cost of convergence rate or final accuracy. Moreover, pipeline parallelism in isolation can still only scale to a number of devices equal to the number of layers in the model, which is limiting for certain model architectures.</p><p>PipeDream <ref type="bibr" target="#b26">[29]</ref> combined pipeline parallelism and data parallelism in a principled way to reduce cross-device communication. DeepSpeed [2] combined pipeline parallelism with tensor and data parallelism to train models with up to a trillion parameters, but with lower throughput than what was shown in this paper (52% vs. 36% of peak) for a few reasons: operator fusion to keep most of the operator graph compute-bound, a more-efficient pipeline parallelism schedule to minimize the pipeline bubble size, fast hardware (A100 vs. V100 GPUs and high-bandwidth links between GPUs on the same and different servers), and scaling to more GPUs. We want to emphasize that this higher throughput makes estimated training times much more practical (about 3 months); an aggregate throughput of 37.6 petaFLOP/s would take about 40 months to train an equivalently-sized model. We can scale to larger models as well, but would need more GPUs to keep training time practical. Mesh-TensorFlow <ref type="bibr" target="#b36">[39]</ref> proposes a language for easily specifying parallelization strategies that combine data and model parallelism. Switch Transformers <ref type="bibr" target="#b12">[15]</ref> used Mesh-Tensorflow to train a sparsely activated expert-based model with 1.6 trillion parameters, with improved pre-training speed over the T5-11B model <ref type="bibr" target="#b32">[35]</ref>.</p><p>Sharded Data Parallelism. As part of performance optimizations for MLPerf 0.6 <ref type="bibr" target="#b25">[28]</ref>, sharded data parallelism <ref type="bibr" target="#b21">[24,</ref><ref type="bibr" target="#b41">44]</ref>, where optimizer state is sharded over data-parallel workers, was introduced. This method has two advantages: (a) it does not introduce extra communication over vanilla data parallelism, and (b) it divides the optimizer's computation and memory cost across the data-parallel partitions. ZeRO <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b34">37]</ref> extends this idea: weight parameters and gradients are sharded across data-parallel workers as well, and workers fetch relevant state from their "owning" workers before performing computations. This adds additional communication, which can be partially hidden by carefully overlapping computation and communication. However, this can become harder if tensor parallelism is not used or the batch size is not large enough to hide the extra communication overhead (Figure <ref type="figure" target="#fig_6">10</ref>). ZeRO-Infinity <ref type="bibr" target="#b34">[37]</ref> uses NVMe to efficiently swap parameters, enabling the training of very large models on a small number of GPUs. We note that using a small number of GPUs for training a very large model results in unrealistic training times (e.g., thousands of years to converge). Automatic Partitioning. FlexFlow <ref type="bibr" target="#b19">[22]</ref>, PipeDream <ref type="bibr" target="#b26">[29]</ref>, DAP-PLE <ref type="bibr" target="#b11">[14]</ref>, and Tarnawski et al. <ref type="bibr" target="#b38">[41]</ref> all auto-partition model training graphs over multiple devices with the help of cost models. However, each of these do not consider all the parallelism dimensions considered in this paper: pipeline and tensor model parallelism, data parallelism, microbatch size, and the effect of memory-savings optimizations like activation recomputation on the training of models larger than the memory capacity of an accelerator. These added dimensions increase the search space that needs to be explored. Gholami et al. <ref type="bibr" target="#b13">[16]</ref> show how communication costs for combinations of data and model parallelism can be modeled.</p><p>HPC for Model Training. Goyal et al. <ref type="bibr" target="#b14">[17]</ref> and You et al. <ref type="bibr" target="#b44">[47]</ref> both demonstrate the use of High Performance Computing techniques to train highly-accurate ImageNet models in minutes. However, the image classification models considered fit comfortably on a single accelerator, rendering model parallelism unnecessary, support very large batch sizes (&gt; 32k) that allow scaling data parallelism to large worker counts with infrequent communication, and are composed of compact convolutional layers that are inherently amenable to data-parallel communication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION AND CONCLUSION</head><p>In this paper, we have shown how PTD-P (inter-node pipeline parallelism, intra-node tensor parallelism, and data parallelism) can be composed to achieve high aggregate throughput (502 petaFLOP/s) while training large models with a trillion parameters. This facilitates end-to-end training in reasonable times (estimated time of around 3 months for a trillion-parameter model). We discussed the various tradeoffs associated with each of these types of parallelism, and how the interactions between them need to be considered carefully when combined.</p><p>Even though the implementation and evaluation in this paper is GPU-centric, many of these ideas translate to other types of accelerators as well. Concretely, the following are ideas that are accelerator-agnostic: a) the idea of smartly partitioning the model training graph to minimize the amount of communication while still keeping devices active, b) minimizing the number of memorybound kernels with operator fusion and careful data layout, c) other domain-specific optimizations (e.g., scatter-gather optimization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: FLOATING-POINT OPERATIONS</head><p>In this section, we describe how we calculate the number of floatingpoint operations (FLOPs) in a model. We consider a language model with 𝑙 transformer layers, hidden size ℎ, sequence length 𝑠, vocabulary size 𝑉 , and training batch size 𝐵.</p><p>A 𝐴 𝑚×𝑘 × 𝑋 𝑘×𝑛 matrix multiplication requires 2𝑚 × 𝑘 × 𝑛 FLOPs (factor of 2 needed to account for multiplies and adds).</p><p>A transformer layer consists of an attention block followed by a 2-layer feed-forward network. For the attention block, the main FLOP contributors are the key, query, and value transformation (6𝐵𝑠ℎ 2 operations), attention matrix computation (2𝐵𝑠 2 ℎ operations), attention over values (2𝐵𝑠 2 ℎ operations), and post-attention linear projection (2𝐵𝑠ℎ 2 operations). The feed-forward network increases the hidden size to 4ℎ and then reduces it back to ℎ; this requires 16𝐵𝑠ℎ 2 FLOPs. Summing these together, each transformer layer results in 24𝐵𝑠ℎ 2 + 4𝐵𝑠 2 ℎ FLOPs for the forward pass. The backward pass requires double the number of FLOPs since we need to calculate the gradients with respect to both input and weight tensors. In addition, we are using activation recomputation, which requires an additional forward pass before the backward pass. As a result, the total number of FLOPs per transformer layer is 4 × 24𝐵𝑠ℎ 2 + 4𝐵𝑠 2 ℎ = 96𝐵𝑠ℎ 2 1 + 𝑠 6ℎ . The other main contributor to the FLOP count is the logit layer in the language model head, which transforms features of dimension ℎ to the vocabulary dimension 𝑉 . The required FLOPs for this operation is 2𝐵𝑠ℎ𝑉 in the forward pass and 4𝐵𝑠ℎ𝑉 in the backward pass, resulting in 6𝐵𝑠ℎ𝑉 FLOPs in total.</p><p>Thus, for a transformer model with 𝑙 transformer layers, the total number of floating-point operations is:</p><formula xml:id="formula_17">96𝐵𝑠𝑙ℎ 2 1 + 𝑠 6ℎ + 𝑉 16𝑙ℎ</formula><p>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>★Figure 1 :</head><label>1</label><figDesc>Figure 1: Trend of sizes of state-of-the-art Natural Language Processing (NLP) models with time. The number of floating-point operations to train these models is increasing at an exponential rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Default and interleaved 1F1B pipeline schedules. The top figure shows the default non-interleaved 1F1B schedule. The bottom figure shows the interleaved 1F1B schedule, where each device is assigned multiple chunks (in this case, 2). Dark colors show the first chunk and light colors show the second chunk. The size of the pipeline bubble is smaller (the pipeline flush happens sooner in the interleaved timeline).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Fraction of time spent idling due to pipeline flush (pipeline bubble size) versus data-parallel size (𝑑), for different numbers of GPUs (𝑛) and ratio of batch size to microbatch size (𝑏 ′ = 𝐵/𝑏).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 3 . 1 Figure 7 :</head><label>317</label><figDesc>Figure 7: Per-GPU throughput versus microbatch size for a GPT model with a billion parameters (128 attention heads, hidden size of 4096, 4 transformer layers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Behavior of normalized estimated throughput (time computed as 𝑡 = (𝑏 ′ /𝑏 + 𝑝 − 1) • 𝑡 𝑓 (𝑏) + 𝑡 𝑏 (𝑏) ) with respect to the microbatch size 𝑏 for the same GPT model from Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>With scatter/gather optimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Throughput per GPU of PTD-P and ZeRO-3 for two different GPT models (the 175B GPT-3 model is shown with dotted lines, and the 530B model is shown with solid lines). Global batch sizes are fixed and ZeRO-3 is used without any model parallelism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11 :Figure 12 :</head><label>1112</label><figDesc>Figure 11: Throughput per GPU of pipeline parallelism using two different batch sizes in a weak-scaling experiment setup (model size increases with the pipeline-parallel size).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Throughput per GPU of various parallel configurations that combine pipeline and tensor model parallelism using a GPT model with 162.2 billion parameters and 64 A100 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 :Figure 15 :Figure 16 :</head><label>141516</label><figDesc>Figure14: Throughput per GPU of various parallel configurations that combine data and pipeline model parallelism using a GPT model with 5.9 billion parameters, three different batch sizes, microbatch size of 1, and 64 A100 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Figure 17: Throughput (in sequences per second) with and without activation recomputation for a GPT model with 145 billion parameters using 128 A100 GPUs ((𝑡, 𝑝) = (8, 16)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>do</figDesc><table><row><cell cols="2">Transformer layer #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Transformer layer #2</cell></row><row><cell></cell><cell cols="2">Tensor MP partition #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tensor MP partition #1</cell></row><row><cell></cell><cell cols="2">Tensor MP partition #2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Tensor MP partition #2</cell></row><row><cell cols="2">Pipeline MP partition #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pipeline MP partition #2</cell></row><row><cell cols="13">Figure 2: Combination of tensor and pipeline model parallelism (MP) used in this work for transformer-based models.</cell></row><row><cell>Device 1</cell><cell>1 2 3 4 5 6 7 8</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8 9 10111213141516</cell></row><row><cell>Device 2</cell><cell>1 2 3 4 5 6 7 8</cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9 10111213141516</cell></row><row><cell>Device 3</cell><cell>1 2 3 4 5 6 7 8</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell></cell><cell>9 10111213141516</cell><cell>9</cell></row><row><cell>Device 4</cell><cell>1 2 3 4 5 6 7 8 1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell></cell><cell></cell><cell cols="2">9 10111213141516 9 10</cell></row><row><cell>Time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Devices idle</cell></row><row><cell></cell><cell></cell><cell cols="4">Forward Pass</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Backward Pass</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Weak-scaling throughput for GPT models ranging from 1 billion to 1 trillion parameters.</figDesc><table><row><cell>Number of parameters (billion)</cell><cell>Attention heads</cell><cell>Hidden size</cell><cell>Number of layers</cell><cell>Tensor model-parallel size</cell><cell>Pipeline model-parallel size</cell><cell>Number of GPUs</cell><cell>Batch size</cell><cell>Achieved teraFlOP/s per GPU</cell><cell>Percentage of theoretical peak FLOP/s</cell><cell>Achieved aggregate petaFLOP/s</cell></row><row><cell>1.7</cell><cell>24</cell><cell>2304</cell><cell>24</cell><cell>1</cell><cell>1</cell><cell>32</cell><cell>512</cell><cell>137</cell><cell>44%</cell><cell>4.4</cell></row><row><cell>3.6</cell><cell>32</cell><cell>3072</cell><cell>30</cell><cell>2</cell><cell>1</cell><cell>64</cell><cell>512</cell><cell>138</cell><cell>44%</cell><cell>8.8</cell></row><row><cell>7.5</cell><cell>32</cell><cell>4096</cell><cell>36</cell><cell>4</cell><cell>1</cell><cell>128</cell><cell>512</cell><cell>142</cell><cell>46%</cell><cell>18.2</cell></row><row><cell>18.4</cell><cell>48</cell><cell>6144</cell><cell>40</cell><cell>8</cell><cell>1</cell><cell>256</cell><cell>1024</cell><cell>135</cell><cell>43%</cell><cell>34.6</cell></row><row><cell>39.1</cell><cell>64</cell><cell>8192</cell><cell>48</cell><cell>8</cell><cell>2</cell><cell>512</cell><cell>1536</cell><cell>138</cell><cell>44%</cell><cell>70.8</cell></row><row><cell>76.1</cell><cell>80</cell><cell>10240</cell><cell>60</cell><cell>8</cell><cell>4</cell><cell>1024</cell><cell>1792</cell><cell>140</cell><cell>45%</cell><cell>143.8</cell></row><row><cell>145.6</cell><cell>96</cell><cell>12288</cell><cell>80</cell><cell>8</cell><cell>8</cell><cell>1536</cell><cell>2304</cell><cell>148</cell><cell>47%</cell><cell>227.1</cell></row><row><cell>310.1</cell><cell>128</cell><cell>16384</cell><cell>96</cell><cell>8</cell><cell>16</cell><cell>1920</cell><cell>2160</cell><cell>155</cell><cell>50%</cell><cell>297.4</cell></row><row><cell>529.6</cell><cell>128</cell><cell>20480</cell><cell>105</cell><cell>8</cell><cell>35</cell><cell>2520</cell><cell>2520</cell><cell>163</cell><cell>52%</cell><cell>410.2</cell></row><row><cell>1008.0</cell><cell>160</cell><cell>25600</cell><cell>128</cell><cell>8</cell><cell>64</cell><cell>3072</cell><cell>3072</cell><cell>163</cell><cell>52%</cell><cell>502.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 (</head><label>1</label><figDesc>denoted by 𝑋 ), we can estimate total training time. We note that for the configurations in Table1, we have 6ℎ ≫ 𝑠, 16𝑙ℎ ≫ (𝑉 + 𝑠), and 12𝑙ℎ ≫ 𝑉 . Combining these observations with equations (2) and (3), we arrive at</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>TIV+49 %GLMIZIHXIVE*034W</cell><cell>&gt;I63&amp; &gt;I63&amp;</cell><cell>48(4&amp; 48(4&amp;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>2YQFIVSJ+49W</cell></row><row><cell>End-to-end training time ≈</cell><cell>8𝑇 𝑃</cell><cell>.</cell><cell>(4)</cell></row><row><cell></cell><cell>𝑛𝑋</cell><cell></cell><cell></cell></row><row><cell cols="4">Let us consider the GPT-3 model with 𝑃 =175 billion parameters as</cell></row><row><cell cols="4">an example. This model was trained on 𝑇 = 300 billion tokens. On</cell></row><row><cell cols="4">𝑛 = 1024 A100 GPUs using batch size 1536, we achieve 𝑋 = 140 ter-</cell></row><row><cell cols="4">aFLOP/s per GPU. As a result, the time required to train this model</cell></row><row><cell cols="4">is 34 days. For the 1 trillion parameter model, we assume that 450</cell></row></table><note>billion tokens are needed for end-to-end training. With 3072 A100 GPUs, we can achieve a per-GPU throughput of 163 teraFLOP/s, and end-to-end training time of 84 days. We believe these training times (using a reasonable number of GPUs) are practical.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Comparison of PTD Parallelism to ZeRO-3 (without model paralllelism). The 530-billion-parameter GPT model did not fit on 560 GPUs when using a microbatch size of 4 with ZeRO-3, so we increased the number of GPUs used to 640 and global batch size to 2560 to provide a throughput estimate (relevant row marked in table with a *).</figDesc><table><row><cell>%GLMIZIHXIVE*034W TIV+49</cell><cell>&amp;EXGLWM^I! &amp;EXGLWM^I!</cell></row><row><cell></cell><cell>4MTIPMRITEVEPPIPWM^I</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We thank the anonymous reviewers, Seonmyeong Bak, Keshav Santhanam, Trevor Gale, Dimitrios Vytiniotis, and Siddharth Karamcheti for their help and feedback that improved this work. This research was supported in part by NSF Graduate Research Fellowship grant DGE-1656518 and NSF CAREER grant CNS-1651570. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors alone.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://openai.com/blog/gpt-3-apps/" />
		<title level="m">Applications of GPT-3</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Deepspeed</forename><surname>Repository</surname></persName>
		</author>
		<ptr target="https://www.deepspeed.ai/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Github</forename><surname>Copilot</surname></persName>
		</author>
		<ptr target="https://copilot.github.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://www.nvidia.com/en-us/data-center/a100/" />
		<title level="m">NVIDIA A100 Tensor Core GPU</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<ptr target="https://developer.nvidia.com/nccl" />
		<title level="m">NVIDIA Collective Communication Library (NCCL)</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Selene</forename><surname>Nvidia</surname></persName>
		</author>
		<author>
			<persName><surname>Supercomputer</surname></persName>
		</author>
		<ptr target="https://www.top500.org/system/179842/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Nvswitch</forename><surname>Nvlink</surname></persName>
		</author>
		<ptr target="https://www.nvidia.com/en-us/data-center/nvlink/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Jit</forename><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/docs/stable/jit.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<title level="m">Language Models are Few-Shot Learners</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06174</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Training Deep Nets with Sublinear Memory Cost. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DAPPLE: A Pipelined Data Parallel Approach for Training Large Models</title>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongyan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixue</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
				<meeting>the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<title level="m">Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Integrated Model, Batch, and Domain Parallelism in Training Neural Networks</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariful</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aydin</forename><surname>Buluc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th on Symposium on Parallelism in Algorithms and Architectures</title>
				<meeting>the 30th on Symposium on Parallelism in Algorithms and Architectures</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Revolve: An Implementation of Checkpointing for the Reverse or Adjoint Mode of Computational Differentiation</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Griewank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Walther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software (TOMS)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="45" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">PipeTransformer: Automated Elastic Pipelining for Distributed Training of Transformers</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Avestimehr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03161</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism</title>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Breaking the Memory Wall with Optimal Tensor Rematerialization</title>
		<author>
			<persName><forename type="first">Paras</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gholami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="497" to="511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond Data and Model Parallelism for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Conference on Machine Learning and Systems (MLSys)</title>
				<meeting>the 2nd Conference on Machine Learning and Systems (MLSys)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pipelined Backpropagation at Scale: Training Large Models without Batches</title>
		<author>
			<persName><forename type="first">Atli</forename><surname>Kosson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Chiley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Venigalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Hestness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urs</forename><surname>Köster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">Scale MLPerf-0.6 Models on Google TPU-v3 Pods</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Shen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omkar</forename><surname>Salpekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vaughan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15704</idno>
		<title level="m">Pritam Damania, et al. PyTorch Distributed: Experiences on Accelerating Data Parallel Training</title>
				<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danyang</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07988</idno>
		<title level="m">Dawn Song, and Ion Stoica. TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cody</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Gu-Yeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><surname>Bittorf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01500</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">MLPerf Training Benchmark. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PipeDream: Generalized Pipeline Parallelism for DNN Training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Harlap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Seshadri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">R</forename><surname>Nikhil R Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ganger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM Symposium on Operating Systems Principles</title>
				<meeting>the 27th ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory-Efficient Pipeline-Parallel DNN Training</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
				<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7937" to="7947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism</title>
		<author>
			<persName><forename type="first">Gyeongchan</forename><surname>Jay H Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungmin</forename><surname>Nguyen T Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Ri</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 USENIX Annual Technical Conference (USENIX ATC 20)</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="307" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Improving Language Understanding by Generative Pre-Training</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Language Models are Unsupervised Multitask Learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><surname>Zero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02054</idno>
		<title level="m">Memory Optimization Towards Training A Trillion Parameter Models</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Rasley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07857</idno>
		<title level="m">Shaden Smith, and Yuxiong He. ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samyam</forename><surname>Rajbhandari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Yazdani Aminabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olatunji</forename><surname>Ruwase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuangyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06840</idno>
		<title level="m">ZeRO-Offload: Democratizing Billion-Scale Model Training</title>
				<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mesh-TensorFlow: Deep Learning for Supercomputers</title>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penporn</forename><surname>Koanantakool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Shoeybi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostofa</forename><surname>Patwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Legresley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08053</idno>
		<title level="m">Megatron-LM: Training Multi-Billion Parameter Language Models using GPU Model Parallelism</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient Algorithms for Device Placement of DNN Graph Operators</title>
		<author>
			<persName><forename type="first">Amar</forename><surname>Jakub M Tarnawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divya</forename><surname>Devanur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fanny</forename><forename type="middle">Nina</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><surname>Paravecino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="15451" to="15463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Attention is All You Need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Petuum: A New Platform for Distributed Machine Learning on Big Data</title>
		<author>
			<persName><forename type="first">Qirong</forename><surname>Eric P Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">Kyu</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinliang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhimanu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="67" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Automatic Cross-Replica Sharding of Weight Updates in Data-Parallel Training</title>
		<author>
			<persName><forename type="first">Yuanzhong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.13336</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PipeMare: Asynchronous Pipeline Parallel DNN Training</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Aberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">De</forename><surname>Sa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
				<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/1906.08237</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ima-geNet Training in Minutes</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th International Conference on Parallel Processing</title>
				<meeting>the 47th International Conference on Parallel Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
