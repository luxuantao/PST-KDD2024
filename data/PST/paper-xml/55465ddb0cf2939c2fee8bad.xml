<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">W</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Information Engineering in Surveying, Mapping, and Remote Sensing (LIESMARS)</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430039</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">College of Architectural Engineering, Civil Engineering and Environment</orgName>
								<orgName type="institution">Ningbo University</orgName>
								<address>
									<postCode>315211</postCode>
									<settlement>Ningbo</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">State Key Laboratory for Information Engineering in Surveying, Mapping, and Remote Sensing (LIESMARS)</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430039</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<postCode>430039</postCode>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Urban Studies</orgName>
								<orgName type="institution">Shanghai Normal University</orgName>
								<address>
									<postCode>200234</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Institute for Computational Engineering and Sciences (ICES)</orgName>
								<orgName type="institution">University of Texas at Austin</orgName>
								<address>
									<postCode>78712</postCode>
									<settlement>Austin</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C262109DC16BD1AF056B99B0944A43D3</idno>
					<idno type="DOI">10.1109/JSTARS.2015.2417156</idno>
					<note type="submission">received October 17, 2014; revised January 25, 2015; accepted March 13, 2015.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T15:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Band selection</term>
					<term>classification</term>
					<term>hyperspectral imagery (HSI)</term>
					<term>improved sparse subspace clustering (ISSC)</term>
				</keywords>
			</textClass>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Band Selection Using Improved Sparse Subspace Clustering for Hyperspectral Imagery Classification</head><p>Weiwei Sun, Liangpei Zhang, Senior Member, IEEE, Bo Du, Senior Member, IEEE, Weiyue Li, and Yenming Mark Lai</p><p>Abstract-An improved sparse subspace clustering (ISSC) method is proposed to select an appropriate band subset for hyperspectral imagery (HSI) classification. The ISSC assumes that band vectors are sampled from a union of low-dimensional orthogonal subspaces and each band can be sparsely represented as a linear or affine combination of other bands within its subspace. First, the ISSC represents band vectors with sparse coefficient vectors by solving the L2-norm optimization problem using the least square regression (LSR) algorithm. The sparse and block diagonal structure of the coefficient matrix from LSR leads to correct segmentation of band vectors. Second, the angular similarity measurement is presented and utilized to construct the similarity matrix. Third, the distribution compactness (DC) plot algorithm is used to estimate an appropriate size of the band subset. Finally, spectral clustering is implemented to segment the similarity matrix and the desired ISSC band subset is found. Four groups of experiments on three widely used HSI datasets are performed to test the performance of ISSC for selecting bands in classification. In addition, the following six state-of-the-art band selection methods are used to make comparisons: linear constrained minimum variance-based band correlation constraint (LCMV-BCC), affinity propagation (AP), spectral information divergence (SID), maximum-variance principal component analysis (MVPCA), sparse representationbased band selection (SpaBS), and sparse nonnegative matrix factorization (SNMF). Experimental results show that the ISSC has the second shortest computational time and also outperforms the other six methods in classification accuracy when using an appropriate band number obtained by the DC plot algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>O WNING to advantages in collecting tens to hundreds of continuous bands of spectrum responses from visible to near-infrared wavelength, hyperspectral imagery (HSI) has powerful performance in recognizing different ground objects with subtle spectrum divergences through the classification implementation <ref type="bibr" target="#b0">[1]</ref>. The classification results of HSI dataset are now widely used in many realistic applications, such as ocean monitoring <ref type="bibr" target="#b1">[2]</ref>, land cover mapping <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, precision farming <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, and mine exploitation <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Unfortunately, numerous bands as well as strong intra-band correlations also bring about big problems to the classification implementation <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Especially, the "curse of dimensionality" renders that the HSI dataset requires extremely more training samples if accurate classification result is wanted, whereas collecting so many training samples is expensive and time-consuming <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Therefore, making dimensionality reduction is an alternative way to conquer these problems.</p><p>Dimensionality reduction of HSI datasets can typically be divided into two categories: band selection (i.e., feature selection) and feature extraction <ref type="bibr" target="#b12">[13]</ref>. Band selection selects an appropriate band subset from the original band set of the HSI dataset while feature extraction preserves important spectral features through mathematical transformations. In this paper, we focus on the band selection category of dimensionality reduction, since the selected band combination could perfectly solve the problem of "curse of dimensionality" and inherit the original spectral meanings from the original HSI dataset.</p><p>Previous work in the dimensionality reduction of band selection can also be roughly divided into two classes: 1) the maximum information or minimum correlation (MIMC) scheme and 2) the maximum inter-class separability (MIS) scheme. MIMC selects an appropriate band subset in which each single-band image has the maximum information or minimum correlation with other bands. The MIMC scheme typically uses the three main criteria of entropy criterion, the intra-band correlation criterion, and the cluster criterion. The entropy criterion algorithm collects an appropriate band subset by maximizing the overall amount of information using entropy-like measurements <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The intra-band correlation criterion algorithms select the appropriate band subset having minimum intra-band correlations. Examples include the mutual information-based algorithm <ref type="bibr" target="#b15">[16]</ref> and the constrained band selection algorithm based on constrained energy minimization (CBS-CEM) <ref type="bibr" target="#b16">[17]</ref>. The cluster criterion algorithm also considers intra-band corrections and selects a representative band from each band cluster using certain clustering algorithms. Examples include the hierarchical clustering algorithm using the mutual information measurement <ref type="bibr" target="#b17">[18]</ref> and the affinity propagation (AP) algorithm with noise-removal bands using wavelet shrinkage <ref type="bibr" target="#b18">[19]</ref>.</p><p>In contrast, the MIS scheme selects an appropriate band subset that maximizes the separability of different ground objects in the image scene. The MIS scheme is typically implemented using one of the following algorithms: the distance measurement criterion algorithm, the feature transformation criterion algorithm, and the realistic application criterion algorithm. The distance measurement criterion algorithm maximizes the inter-class differences using a distance-like measurement such as the spectral information divergence (SID), the transformed divergence (TD), or Mahalanobis distance <ref type="bibr" target="#b19">[20]</ref>. The feature transformation criterion algorithm selects an appropriate band subset by analyzing the inter-class separability of ground objects in a low-dimensional feature space found through feature transformations. Examples include linear prediction algorithm <ref type="bibr" target="#b20">[21]</ref> and the complex network algorithm <ref type="bibr" target="#b21">[22]</ref>. The realistic application criterion algorithm selects an appropriate band subset through maximizing or minimizing the defined objective function suitable for realistic applications of HSI dataset, and the typical examples are the band selection algorithm using high-order movements <ref type="bibr" target="#b22">[23]</ref> and the supervised band selection algorithm using the known class spectral signatures <ref type="bibr" target="#b23">[24]</ref>.</p><p>In recent years, the study of sparsity in HSI datasets has attracted much interest in the remote sensing community. The sparsity theory states that each band vector or spectrum vector can be sparsely represented using only a few nonzero coefficients in a suitable basis or dictionary <ref type="bibr" target="#b24">[25]</ref>. Sparse representations of a band vector can then reveal certain underlying structures such as the clustering structure within the HSI dataset and also drastically reduce the computational burden in processing HSI datasets <ref type="bibr" target="#b25">[26]</ref>. Accordingly, researchers began to study the band selection problem using sparsitybased (SB) schemes and have proposed algorithms such as the sparse representation-based band selection (SpaBS) algorithm <ref type="bibr" target="#b26">[27]</ref>, the sparse nonnegative matrix factorization (SNMF) algorithm <ref type="bibr" target="#b27">[28]</ref>, the collaborative sparse model (CSM) algorithm <ref type="bibr" target="#b28">[29]</ref>, and the sparse support vector machine (SSVM) algorithm <ref type="bibr" target="#b29">[30]</ref>.</p><p>In this paper, we address the band selection problem using an idea inspired by sparse space clustering (SSC). We present a band selection method using an improved version of SSC which we call improved sparse space clustering (ISSC). In particular, our motivation is to ameliorate the SSC techniques by ISSC and implement the ISSC into HSI dataset in order to solve the band selection problem. Our contributions are as follows: First, we are the first to explore band selection from the SSC perspective. The ISSC method assumes that all bands (i.e., band vectors) of the HSI dataset are drawn from a union of low-dimensional orthogonal subspaces rather than a single uniform subspace, and that each band can be sparsely represented as a linear or affine combination of other bands <ref type="bibr" target="#b30">[31]</ref>. The two above assumptions differ from the assumptions of current SB algorithms and clustering algorithms. Second, our proposed ISSC method improves the SSC method with three following modifications: The ISSC uses the l 2 -norm to avoid the "too sparse" coefficient vector solution when using the l 1 -norm and to ensure the coefficient matrix is sparse and block diagonal. Our proposed angular similarity measurement replaces the l 1 -directed graph construction (DGC) measurement in SSC and better represents the total similarity between two sparse coefficient band vectors than the isolated coefficient values. The distribution compactness (DC) plot algorithm intelligently estimates the size of band subset and eliminates errors from artificial estimation in SSC. The above improvements in SSC technique by the proposed method ensure good performance in selecting an appropriate band subset.</p><p>This paper is organized as follows. Section II reviews the classical sparse subspace clustering. Section III presents the band selection method using the proposed ISSC. Section IV analyzes the performance of ISSC in band selection for classification on three widely used HSI datasets. Section V states conclusion and outlines our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. A REVIEW OF SPARSE SUBSPACE CLUSTERING</head><p>In this section, we review the classical SSC method. We choose to use a noise-free dataset rather than noisy dataset to illustrate principles more clearly. SSC proposed by Ehsan Elhamifar regards that each data point lies in a union of subspaces corresponding to several classes or clusters to which the dataset belongs; therefore, each point can be sparsely represented by other points of a union of subspaces. The SSC then uses sparse representations of data points to cluster the points into separate subspaces <ref type="bibr" target="#b30">[31]</ref>. SSC has been used in a wide variety of applications such as motion segmentation and face clustering <ref type="bibr" target="#b31">[32]</ref>. It typically consists of three stages: 1) finding sparse representations of each data point through convex optimization; 2) learning a similarity matrix (i.e., a weight matrix); and 3) clustering the similarity matrix using spectral clustering <ref type="bibr" target="#b32">[33]</ref>.</p><p>Assume a high-dimensional dataset without noises</p><formula xml:id="formula_0">Y = {y i } N i=1 ∈ R D×N are actually lying in a union of linear sub- spaces {C l } k l=1 with dimensions {d l } k l=1</formula><p>, where D is the dimension of high-dimensional space and k is the number of subspaces or clusters. Specifically, for a band dataset in hyperspectral field, all band vectors constitute the high-dimensional dataset, the number of bands N corresponds to the size of data points, and the number of pixels D determines the dimensionality of high-dimensional space. We assume each point y i ∈ R D×1 lies in exactly one of the k linear spaces C l . Hence, each linear subspace C l contains a cluster of unique N l data points and the number of points within subspace C l , N l is greater than the subspace dimension size d l , i.e., for each subspace C l , we have the cluster of data points</p><formula xml:id="formula_1">Y l = y l j N l j=1 ∈ R D×N l with N l &gt; d l . This placement of N points into the k subspaces also implies k l=1 N l = N . Accordingly, each data point y i ∈ R D×1</formula><p>belonging to C l can be represented as</p><formula xml:id="formula_2">y i = y l 1 α l 1 + y l 2 α l 2 + • • • y l j α l j • • • + y l N l α l N l = y l 1 y l 2 • • • y l j • • • y l N l ⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣ α l 1 α l 2 . . . α l j . . . α l N l ⎤ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎥ ⎦ = Y l α l , α l j = 0 if y i = y l j (<label>1</label></formula><formula xml:id="formula_3">)</formula><p>where α l ∈ R N l ×1 and Y l are the coefficient vector and dictionary of y i , respectively. The constraint α l j = 0 if y i = y l j in (1) is to avoid the trivial solution of reconstructing point y i as a linear combination of itself. If we combine all the cluster dictionaries Y l k l=1 , the point y i can be represented using</p><formula xml:id="formula_4">y i = Y 1 α 1 + Y 2 α 2 + • • • + Y k α k = Y 1 Y 2 • • • Y k ⎡ ⎢ ⎢ ⎢ ⎣ α 1 α 2 . . . α k ⎤ ⎥ ⎥ ⎥ ⎦ = ΦA i (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where D×N is the arrangement of k cluster dictionaries and A i ∈ R N ×1 is the coefficient vector from all data points. Assume that Y = ΦΓ, ΓΓ T = Γ T Γ = I, where Γ is the permutation matrix of all cluster dictionaries, when combining all N data points into a matrix format by placing points in individual columns, (2) can be transformed into</p><formula xml:id="formula_6">Φ = Y 1 Y 2 • • • Y k ∈ R</formula><formula xml:id="formula_7">Y = YZ, diag (Z) = 0<label>(3)</label></formula><p>where</p><formula xml:id="formula_8">Z = Γ T [A 1 A 2 • • • A N ] ∈ R N ×N</formula><p>is the coefficient matrix of all data points and diag(Z) is the vector of diagonal entries in Z. The constraint diag(Z) = 0 is to eliminate the trivial solution that each data points is simply a linear combination of itself. The ideal solution of (3) finds a set data points from a single subspace where the number of nonzero entries of y i coincides with the dimensionality of this subspace. The ideal solution guarantees that the coefficient matrix Z is sparse or block diagonal and could benefit the correct segmentation of all data points into separate subspaces.</p><p>The solution Ẑ of (3) can be regarded as the optimization problem of minimizing the following objection function:</p><formula xml:id="formula_9">Ẑ = arg min Z q , subject to Y = YZ and diag(Z) = 0 (4)</formula><p>where Z q represents the l q -norm of Z defined as</p><formula xml:id="formula_10">Z q = N i=1 N j=1 |Z ij | q 1</formula><p>q . The l 0 -norm minimization counts the number of nonzero entries in Z and can be solved using the smoothed L 0 algorithm <ref type="bibr" target="#b33">[34]</ref>. The l 1 -norm can be efficiently minimized using convex programming algorithms such as the interior point method <ref type="bibr" target="#b34">[35]</ref>, the basis pursuit algorithm <ref type="bibr" target="#b35">[36]</ref>, and the alternating directions algorithm <ref type="bibr" target="#b36">[37]</ref>.</p><p>Sparse matrix Ẑ is then utilized to construct the similarity matrix W for inferring the segmentation of all data points into different subspaces. The matrix W can be regarded as an undirected weighted graph, where each entry W ij represents the weight of the edges between pairwise points y i and y j . The SSC method utilizes the l 1 -directed graph construction (DGC) measurement to guarantee the symmetrization of weights between pairwise points y i and y j . The DGC measurement is performed as follows:</p><formula xml:id="formula_11">W ij = |Z ij | + |Z ji | 2<label>(5)</label></formula><p>where |Z ij | is the absolute value of the entry Z ij in Ẑ. After constructing the similarity matrix W, the SSC uses spectral clustering <ref type="bibr" target="#b32">[33]</ref> to cluster all data points into their underlying subspaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BAND SELECTION OF HSI DATASET USING ISSC</head><p>In this section, the band selection method using ISSC is described. Section III-A presents sparse representations of each band vector through solving the l 2 -norm. Section III-B describes the construction of the similarity matrix using our proposed angular similarity measurement. Section III-C explains the appropriate estimation of band clusters using the DC plot algorithm and shows how to select an appropriate band subset using spectral clustering. Finally, Section III-D summarizes our band selection method using ISSC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Representations of Band Vectors Using L2-Norm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider a collection of HSI band vectors</head><formula xml:id="formula_12">Y = {y i } N i=1 ∈ R D×N that are drawn from a union of orthogonal subspaces {C l } k l=1</formula><p>, where D is the dimension of high-dimensional space and is equal to the number of pixels in the image scene, N is the number of bands with N &lt;&lt; D, and k is the number of band clusters (i.e., the number of underlying subspaces). Each band vector y i is contaminated with Gaussian noise and is sparsely represented by other band vectors as follows:</p><formula xml:id="formula_13">y i = YZ i + e, Z ii = 0<label>(6)</label></formula><p>where</p><formula xml:id="formula_14">Z i = [Z i,1 Z i,2 • • • Z i,N</formula><p>] T is the coefficient vector of band vector y i , and e is the error term with a bounded norm. The error in ( <ref type="formula" target="#formula_13">6</ref>) results from noises in band vectors and the approximation errors in sparse representation by other band vectors.</p><p>We combine all the band vectors by placing them column by column to achieve the following matrix format:</p><formula xml:id="formula_15">Y = YZ + E, diag(Z) = 0<label>(7)</label></formula><p>where respectively. Like ( <ref type="formula">4</ref>), ( <ref type="formula" target="#formula_15">7</ref>) can be solved by optimizing the following problem:</p><formula xml:id="formula_16">Z = [Z 1 Z 2 • • • Z N ] and E = [e 1 e 2</formula><formula xml:id="formula_17">Ẑ = arg min Z q , subject to Y -YZ ≤ ε and diag(Z) = 0 (8)</formula><p>where ε is the norm bound of the error. When choosing q = 1, the optimization problem ( <ref type="formula" target="#formula_18">9</ref>) can be transformed into the wellknown lasso problem <ref type="bibr" target="#b37">[38]</ref> Ẑ = arg min Y -YZ</p><formula xml:id="formula_18">2 2 + β Z 1 , subject to diag(Z) = 0<label>(9)</label></formula><p>where</p><formula xml:id="formula_19">Z 1 denotes the l 1 -norm of the matrix Z, Z 1 = N i=1 N j=1 |Z ij |</formula><p>and β &gt; 0 is a scalar regularization parameter that balances the weight of the error terms and the sparsity of the coefficient matrix. The assumption of underlying subspaces in all band vectors guarantees that the l 1 -norm achieves a sparse and block diagonal coefficient matrix for further segmentation. However, the extremely high intra-band correlations may cause the sparse representations of band vectors to select only one band at random <ref type="bibr" target="#b38">[39]</ref>, which would bring about the "too sparse" solution of problem ( <ref type="formula" target="#formula_18">9</ref>) by segmenting the withincluster bands into different subspaces. Therefore, we relax <ref type="bibr" target="#b6">(7)</ref> by optimizing the following l 2 -norm problem:</p><formula xml:id="formula_20">Ẑ = arg min Y-YZ 2 2 +β Z 2 2 , subject to diag(Z) = 0<label>(10)</label></formula><p>where</p><formula xml:id="formula_21">Z 2 represents the l 2 -norm (the Frobenius norm) of matrix Z, Z 2 = N i=1 N j=1 Z 2 ij 1 2</formula><p>. It has been proved that both the orthogonal subspace assumption of band vectors and the l 2 -norm of matrix Z can guarantee the optimal solution in problem ( <ref type="formula" target="#formula_20">10</ref>) is both sparse and block diagonal even if the band numbers N is smaller than the dimension of high-dimensional space D (i.e., insufficient data sampling with N &lt;&lt; D) <ref type="bibr" target="#b39">[40]</ref>. Moreover, the optimal solution in problem ( <ref type="formula" target="#formula_20">10</ref>) is proven to have a grouping effect with intra-band correlation dependent band vectors, and this grouping effect can segment highly correlated band vectors <ref type="bibr" target="#b40">[41]</ref>. Therefore, the optimal solution of problem <ref type="bibr" target="#b9">(10)</ref> can minimize the intra-cluster affinities of band vectors and capture the correlation structure of band vectors from the same orthogonal subspace for correct clustering.</p><p>In this paper, we utilize the least squares regression (LSR) <ref type="bibr" target="#b40">[41]</ref> algorithm to solve the optimization problem <ref type="bibr" target="#b9">(10)</ref>. Let</p><formula xml:id="formula_22">Y * i = Y\y i = [y 1 y 2 • • • y i-1 y i+1 • • • y N ]</formula><p>be the remaining column set of Y after removing the column y i , and let</p><formula xml:id="formula_23">E i = (Y * T i Y i * + βI) -1 and YΓ = [Y * i y i ],</formula><p>where Γ is the permutation matrix with ΓΓ T = Γ T Γ = I. The LSR factorizes matrix Γ T (Y T Y + βI)Γ -1 to achieve the optimal solution Ẑ, using the Woodbury formula [42]</p><formula xml:id="formula_24">Γ T (Y T Y + βI)Γ -1 = Y * T i Y * i + βI Y * T i y i y T i Y * i y T i y i + β -1 = (Y * T i Y * i + βI) -1 0 0 0 + γ i [ Ẑ] i [ Ẑ] T i -[ Ẑ] i -[ Ẑ] T i 1<label>(11)</label></formula><p>where</p><formula xml:id="formula_25">[ Ẑ] i = (Y * T i Y * i + βI) -1 Y * T i y i is the ith column of desired optimal solution Ẑ and γ i = y T i y i + β - y T i Y * i (Y * T i Y * i + βI) -1 Y * T i y i . We then substitute the equation Γ T (Y T Y + βI) -1 Γ = Γ T (Y T Y + βI)Γ -1 into</formula><p>(11) resulting from the property of matrix Γ, and the optimal solution is achieved as</p><formula xml:id="formula_26">Ẑ = -(Y T Y + βI) -1 (diag((Y T Y + βI) -1 )) -1</formula><p>, where diag( Ẑ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Angular Similarity Measurement Between Sparse Coefficient Band Vectors</head><p>After solving problem <ref type="bibr" target="#b9">(10)</ref>, the ISSC constructs the similarity matrix W using the sparse coefficient matrix Ẑ of all band vectors. The similarity matrix can be regarded as an undirected weighted graph G = (V, W), where V ij ∈ V represents the edge between pairwise bands y i and y j , and the weight W ij ∈ W measures the similarity between sparse representations of pairwise band vectors y i and y j . The DGC similarity measurement in SSC assumes that the nonzero entries of sparse coefficient vector Ẑi reflect the closeness or similarity with other bands from the same subspace, since the ideal similarity matrix has only edges between pairwise band vectors from the same subspace <ref type="bibr" target="#b32">[33]</ref>. Some authors have proposed relevant measurements to improve the performance of the DGC similarity measurement, such as the sparsity induced similarity <ref type="bibr" target="#b42">[43]</ref> and the nonnegative sparsity induced similarity <ref type="bibr" target="#b43">[44]</ref>. The above measurements utilize individual sparse coefficients to compute the weights of two bands. However, the local similarity represented by sparse coefficients cannot describe the true similarity between two bands because sparse coefficients from real-world HSI dataset are sensitive to noise and outliers <ref type="bibr" target="#b44">[45]</ref>.</p><p>We present an angular similarity (AS) measurement using the angular information between two sparse coefficient band vectors. The AS measurement assumes that sparse representations of two similar bands from the same subspace should have a small angle between the two of them since both of them are sparsely represented in a similar combination using other bands. For each pairwise sparse coefficient vectors Ẑi and Ẑj that represent coefficient vectors of y i and y j , respectively, the AS similarity measurement is defined as follows:</p><formula xml:id="formula_27">W ASij = ⎛ ⎜ ⎝ Ẑi • Ẑj Ẑi 2 × Ẑj 2 ⎞ ⎟ ⎠ 2 (<label>12</label></formula><formula xml:id="formula_28">)</formula><p>where • denotes the inner product of two band vectors and • 2 is the norm of the vector. The use of the squaring operation in ( <ref type="formula" target="#formula_27">12</ref>) is to guarantee a positive value for each W ASij and to increase the separability of sparse band vectors from different subspaces. By computing all combinations of columns indexes and row indexes using <ref type="bibr" target="#b11">(12)</ref>, the similarity matrix W is found for further clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Band Clustering With an Appropriate Cluster Number</head><p>When constructing the similarity matrix, the spectral clustering algorithm is utilized to segment the weighted graph into k clusters. However, a difficult problem is how to estimate an appropriate number k of band clusters, because the k significantly affects the clustering result of band vectors. In general, the cluster number k is arbitrarily determined with estimation. However, a too small k would divide highly correlated bands into other different subspaces and render an error-prone band subset. In contrast, a too large k would bring about too much computational burden for further classification of HSI dataset. To address these problems, a variety of methods have been proposed to help estimate an appropriate k and can be classified into two main classes, the posterior method and the prior method. The posterior method tests all possible number of clusters and then selects an appropriate cluster number using defined criteria such as the over information-theoretic criterion <ref type="bibr" target="#b45">[46]</ref>, the gap statistic criterion <ref type="bibr" target="#b46">[47]</ref>, and the minimum description length criterion <ref type="bibr" target="#b47">[48]</ref>. The posterior method is computationally expensive, and all candidate numbers of clusters have to be tested explicitly. In contrast, the prior method estimates the cluster number before implementing spectral clustering. Prior methods include the Eigen-gap heuristic algorithm <ref type="bibr" target="#b48">[49]</ref> and the edge based-algorithm <ref type="bibr" target="#b49">[50]</ref>.</p><p>Since the similarity matrix W is ideally sparse and block diagonal, we introduce a DC plot algorithm <ref type="bibr" target="#b50">[51]</ref> to estimate the appropriate number k of band clusters. The algorithm uses a nonparametric estimate of the probability density function of the band dataset and determines the appropriate number k of band clusters through analyzing the DC of sparse representations of band vectors in the kernel space constructed by the AS measurement. The DC measurement of all clusters uses an eigenvalue decomposition of the similarity matrix as follows:</p><formula xml:id="formula_29">DC = Ẑ p( Ẑ) d Ẑ ≈ 1 T N W1 N = N i=1 λ i 1 T N u i 2 (<label>13</label></formula><formula xml:id="formula_30">)</formula><p>where 1 N is a N × 1 dimensional vector that contains all ones, W is the similarity matrix, and λ i and u i are the ith eigenvalues and eigenvectors of W, respectively. The appropriate cluster number is obtained by analyzing the plot of log λ i 1 T N u i 2 against i. The logarithm value rather than the direct value of λ i 1 T N u i 2 is used to better explain large shifts of the plot at i = 1 and also to smooth the DC data. The logarithm plot is additionally smoothed using the average filter of size 3 since the log-likelihood function is sensitive to variance in the data. The number corresponding to the "elbow" of the logarithm plot is then selected as the appropriate number k. Given the appropriate cluster size k of band vectors, spectral clustering then implements band selection in the following four stages: 1) The symmetric normalized Laplacian matrix L sym is built from the similarity matrix W using L sym = D -1/2 WD -1/2 , where D is a diagonal matrix constructed with the diagonal entries of W. 2) The first k eigenvectors</p><formula xml:id="formula_31">U k = [u 1 u 2 • • • u k ] ∈ R N ×k</formula><p>are found through a singular value decomposition of the Laplacian matrix L sym and each row vector H i of U k is normalized to norm 1 using</p><formula xml:id="formula_32">H ij = u ij / k u 2 ik 1/2 . 3) Row vectors in normalized U k are clus-</formula><p>tered into k clusters using the K-means algorithm, and the corresponding band vectors are segmented into their underlying subspaces. 4) The band whose corresponding row vectors in U k is closest to the mean vector (i.e., the vector of the centroid) of its cluster {C l } k l=1 in terms of Euclidean distance is chosen as an element of the band subset for HSI dataset and hence the appropriate band subset from ISSC is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Summary of Band Selection Using ISSC</head><p>In the above three sections, we provided three improvements of the classical SSC when implementing band selection on a HSI dataset. 1) We set the l q -norm problem in (5) to the l 2 -norm problem to achieve sparse representations of each band vector using the LSR algorithm. The l 2 -norm guarantees the coefficient matrix is both sparse and block diagonal and can also minimize the between-cluster similarity for correct segmentation of band vectors. 2) The AS measurement improves the performance of the similarity matrix constructed with the DGC measurement. The AS measurement considers the overall similarity between two sparse coefficient band vectors rather than only the similarity between isolated sparse coefficients. 3) We use the DC plot algorithm to estimate an appropriate number k of band clusters to avoid errors caused from variance in data. The process of band selection using the ISSC is shown in Fig. <ref type="figure" target="#fig_0">1</ref> and is implemented in the following five steps.</p><p>1) The HSI imagery is transformed from a data cube into two-dimensional (2-D) band dataset Y ∈ R D×N , where D and N are the dimension of band vectors (i.e., the number of pixels) and the number of bands, respectively. 2) Sparse representations of band vectors are constructed using the LSR algorithm by solving the optimization problem <ref type="bibr" target="#b9">(10)</ref> and sparse coefficient matrix Ẑ of band vectors is found. 3) The similarity matrix W is found with the AS measurement in <ref type="bibr" target="#b11">(12)</ref>, where each entry of W represents the similarity between pairwise sparse coefficient band vectors. 4) The appropriate number k of band clusters is estimated using the DC plot algorithm in <ref type="bibr" target="#b12">(13)</ref>. Spectral clustering is then used to segment the similarity matrix into separate clusters. 5) The bands nearest to the centroid of their cluster are then the desired band subset using ISSC. In ISSC, the computational complexity of sparse representations of all band vectors using LSR is O(N 2 D), the computational complexity in constructing the angular similarity matrix is O(N 2 ), and the computational complexity of clustering bands with the appropriate number is O(kN t), where D and N are the dimension of the band vectors and the number of bands, respectively, and k and t represent the number of band clusters and iterations, respectively. Therefore, the total complexity of ISSC roughly equals O(N 2 D + N 2 + kN t). Considering the fact that O(N 2 D) dominates O(N 2 ) and O(kN t), the complexity of ISSC is approximately O(N 2 D). We compare the complexity of ISSC with six other state-of-the-art band selection methods. We test two MIMC approaches, linear constrained minimum variance-based band correlation constraint (LCMV-BCC) <ref type="bibr" target="#b16">[17]</ref> and AP <ref type="bibr" target="#b51">[52]</ref>, two MIS approaches, SID <ref type="bibr" target="#b19">[20]</ref> and maximum-variance principal component analysis (MVPCA) <ref type="bibr" target="#b52">[53]</ref>, and two SB approaches, SpaBS <ref type="bibr" target="#b26">[27]</ref> and SNMF <ref type="bibr" target="#b29">[30]</ref>. The comparison in computational complexity of all seven methods is listed in Table <ref type="table" target="#tab_1">I</ref>, where K denotes the sparsity level of SpaBS. Since N 2 &lt; D, N 2 D &lt; D 2 &lt;&lt; D 2 N , and K &lt; k &lt;&lt; N &lt;&lt; D, we deduce that the computational complexity of ISSC is lower than that of AP, MVPCA, SpaBS, and LCMV-BCC but higher than that of SID. Moreover, the SpaBS has the highest computational complexity among all the methods, whereas LCMV-BCC has the second highest computational complexity. In addition, the computational complexity of AP is higher than that of the other four methods SID, MVPCA, SNMF, and ISSC. In summary, ISSC has a lower computational complexity among all seven band selection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND ANALYSIS</head><p>In this section, four groups of experiments on three famous HSI datasets are implemented in order to testify our proposed ISSC method when selecting an appropriate band subset. Section IV-A describes the relevant information of the three </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Descriptions of Three HSI Datasets</head><p>The first dataset is Indian Pines from the Multispectral Image Data Analysis System Group at Purdue University (https://engineering.purdue.edu/~biehl/MultiSpec/aviris_docum entation.html). The dataset was acquired by NASA on June 12, 1992 using the AVIRIS sensor from JPL. The dataset has 20 m spatial resolutions and 10 nm spectral resolutions covering a spectrum range of 200-2400 nm. A subset of the image scene of size 145 × 145 pixels depicted in Fig. <ref type="figure" target="#fig_1">2</ref> is used in our experiment and covers an area of 6 miles west of West Lafayette, Indiana. The dataset was preprocessed with radiometric corrections and bad band removal, and afterward 200 bands remained with calibrated data values proportional to radiances. Sixteen classes of ground objects exist in the image scene, and the ground truth for both training and testing samples for each class is listed in Table <ref type="table" target="#tab_2">II</ref>.</p><p>The second is Pavia University (PaviaU) dataset taken from the Computational Intelligence Group in the Basque University (http://www.ehu.es/ccwintco/index.php/Hyperspectral_Remote _Sensing_Scenes). The dataset was achieved from ROSIS sensor with 1.3 m spatial resolutions and 115 bands. After removing low signal to noise ratio (SNR) bands, 103 bands were left for further analysis. The small subset of the larger dataset shown in Fig. <ref type="figure" target="#fig_2">3</ref> is used in our experiments. The test dataset contains 350 × 340 pixels and covers the area of Pavia University. The image scene has nine classes of ground objects including shadows, and the ground truth information of training and testing samples in each class is listed in Table <ref type="table" target="#tab_3">III</ref>.</p><p>The third dataset is the Urban dataset acquired from the U.S. Army Geospatial Center (www.tec.army.mil/hypercube). The dataset was collected by a HYDICE sensor with 10 nm spectral resolution and 2 m spatial resolutions. The low SNR band sets <ref type="bibr">[1-4, 76, 87, 101-111, 136-153, 198-210]</ref> were eliminated from the initial 210 bands, leaving the final 162 bands. Fig. <ref type="figure" target="#fig_3">4</ref> shows a small image subset of size 307 × 307 pixels selected from the larger image. The small dataset covers an area at Copperas Cove near Fort Hood, TX, USA, and has 22 classes of ground objects in the image scene. Table <ref type="table" target="#tab_4">IV</ref> shows the ground truth information for training and testing samples in each class.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>We conduct four groups of experiments using three HSI datasets above to test the performance of our ISSC method in selecting an appropriate band subset for classification. Six state-of-the-art methods in band selection are used to make holistic comparisons with our methods, including two MIMC approaches LCMV-BCC and AP, two MIS approaches SID and MVPCA, and two SB approaches SpaBS and SNMF. First, we quantify the band selection performance of ISSC and compare the results with those of the other six methods. The experiment assesses the performance of the ISSC in band selection before classification. Second, we compare the computational time of ISSC against six other band selection methods when varying the size of band subset k. The experiment investigates the computational performance of ISSC. Third, we compare the classification accuracies of ISSC against that of the six other methods. Two widely used classifiers are used in the experiment, K-nearest neighbor (KNN) <ref type="bibr" target="#b53">[54]</ref> and support vector machine (SVM) <ref type="bibr" target="#b54">[55]</ref> classifiers. The overall classification accuracy (OCA) and average classification accuracy (ACA) is used to measure the classification performance of all seven methods in the experiment. The KNN classifier uses the Euclidean distance, and the SVM classifier uses the radial basis function (RBF) kernel function with the variance parameter and the penalization factor obtained via cross-validation. For each dataset, we repeatedly subsample the training samples and testing samples ten times. Finally, we investigate the relationships between the scalar parameter β in ISSC and the classification accuracies. The experiment helps to determine a proper β when using ISSC in real-world applications of HSI classification. The following experimental results without specific notations are the average results of ten different and independent experiments.</p><p>1) Quantitative Evaluation of the ISSC Band Subset: The experiment evaluates the band selection results obtained before classification from ISSC and other six methods using three quantitative measures. We use the average information entropy (AIE) to measure the information amount and to evaluate the richness of spectrum information in the band subset. We compute the average correlation coefficient (ACC) to estimate the intra-band correlations in the band subset. The average relative entropy (ARE) (also called average Kullback-Leibler divergence, AKLD) is used to measure the inter-separabilities of selected bands and assess the distinguishability within the band subset for classification. We use the three above quantitative measures because they measure the three desired performance characteristic of selecting an appropriate band subset having high information amount, low intra-band correlations, and high inter-separabilities in the band subset. In the experiment, the appropriate size of band subset k (i.e., the number of bands in the subset) is estimated using the DC plot algorithm and is then set as the dimension of band subset for all the methods. The k in Indian Pines dataset is 12, the k in PaviaU dataset is 10, and the k in Urban dataset is 20. In the SNMF method, the parameter α that controls the entry size of dictionary matrix and the parameter γ that determines the sparseness of coefficient matrix are determined using cross-validation and the optimal α and γ having the best result are selected. For the Indian Pines dataset, the α and γ of SNMF are chosen as 3.0 and 0.05, respectively, and the scalar parameter β in ISSC chosen to be 0.1 after cross-validation. In the PaviaU dataset, the α and γ are 4.0 and 0.001, respectively, and the β is 0.001. The α, γ, and β in Urban dataset are 3.5, 0.01, and 0.05, respectively. The iteration time t for the learning dictionary in SpaBS is manually set as 5 for all three datasets. Table V lists detailed information about the parameters of the above seven methods on the three HSI datasets.</p><p>Table <ref type="table" target="#tab_6">VI</ref> illustrates the quantitative evaluation results of all the methods on the three HSI datasets. For the Indian Pines dataset, the ISSC has the highest AIE and ARE, whereas SNMF has the second lowest ACC since the ACC of SNMF. The  SID and MVPCA behave worse due to their higher ACC and lower AIE and ARE. For the PaviaU dataset, the ISSC performs best among all the methods for all three quantitative measures. When comparing four other methods with ISSC excluded, the SID and MVPCA show higher ACC and lower AIE and ARE; therefore, the band subset of the two methods have poorer performance than that of the two other methods. The ISSC performance in the Urban dataset has the highest AIE and lowest ACC, and ARE of the ISSC is the second only to that of SNMF. The SID performance on the Urban dataset was similar to its performance on the Pavia U dataset. It again performed worse than the other methods in the three quantitative measures.</p><p>2) Computational Performance of ISSC: The experiment tests the computational speed performance of ISSC and the other six methods when varying the sizes of band subset k. For the Indian pines dataset, k is set between 6 and 24 with a step interval of 6; k in PaviaU dataset is set between 5 and 25 with a step interval of 5; and k in Urban dataset is set between 10 and 40 with a step interval of 10. Other parameters in all the seven methods are the same as their counterparts in the previous experiment. Table V details parameter configurations of all the methods.</p><p>We run the experiment on a Windows 7 computer with Inter i7-4700 Quad Core Processor and 16 GB of RAM. Both the ISSC and the other six methods' algorithms are implemented in MATLAB 2014a. Table <ref type="table" target="#tab_7">VII</ref> shows the comparisons in the computations times of the seven methods on the three HSI datasets. For each HSI dataset, the computational times of all the methods gradually increase with increasing k. The SpaBS takes the longest computational times among all the methods. The LCMV-BCC has a shorter computational time than SpaBS but is still slower than the other five methods. The computational time of AP is longer than those of MVPCA, SID, SNMF, and ISSC, and the computational time of SNMF is longer than those of MVPCA, ISSC, and SID. The MVPCA has longer computational time than SID and ISSC. The computational time of SID is shorter than that of ISSC and has the shortest computational time among all the methods. We found that these computational times coincide with the analysis of computational complexity of all the methods in Section III-D. The computational times in increasing order of all the seven methods are as follows: SID, ISSC, MVPCA, SNMF, AP, LCMV-BCC, and SpaBS.</p><p>3) Classification Performance of ISSC: This experiment measures the classification performance of the ISSC method. Our aim is to make holistic evaluations in classification performance by varying the size of band subset k rather than using a certain predefined band number. As we did in the above experiments, we compare classification accuracies using the OCA and ACA. For each dataset, we repeatedly subsample the training samples and testing samples ten times to achieve accurate classification accuracies. In the experiment, the size of band subset k in Indian Pines dataset varies from 2 to 45 with a step interval of 2, and the size of band subset k in PaviaU dataset and Urban datasets vary between 2 and 50 with a step interval of 2. The neighbor size k 1 in the KNN classifier is set as 3, and the threshold of total distortion in the SVM classifier is set as 0.01. Using cross-validation, the α and γ in SNMF of PaviaU dataset are chosen as 3.0 and 0.1, respectively, the α and γ in Urban dataset are chosen as 4.0 and 1.5, respectively; and the α and γ in Urban dataset are chosen as 3.5 and 0.05, respectively. Other parameters are the same as their counterparts in the above experiments. Table V details parameter configurations of all the methods. Fig. <ref type="figure" target="#fig_4">5</ref> illustrates the OCA results of all seven methods using the SVM and KNN classifiers on the three datasets. We did not list the ACA results because of the similarity between the ACA curve and the OCA curve. For each dataset and each classifier, the OCA is small for band number k less than 5 and the OCA rises with increasing k. The OCA changes slowly after a certain threshold of the band number k and most curves become almost flat with slight fluctuations. The SID with each classifier and each HSI dataset always has the lowest value among all the methods. For each dataset using KNN and SVM classifiers, ISSC outperforms the other six methods after a certain threshold k. The OCA results of ISSC are the best among all the seven methods.</p><p>Moreover, we found that the threshold k for the ISSC curves of each dataset is located around the appropriate band number estimated by the DC plot algorithm. Therefore, using the appropriate band number k from the DC plot algorithm, we    Table <ref type="table" target="#tab_8">VIII</ref> shows that the ISSC has the best classification accuracies for all three datasets using different classifiers, that the AP behaves better than SID and MVPCA in classification, and that the SID has the worst performance of all the method. The above observations further support the results in Fig. <ref type="figure" target="#fig_4">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Effect From the Scalar Parameter on the Sensitivity of Classification Accuracy:</head><p>The experiment explores the effect from the scalar parameter β in ISSC on the OCA and ACA results of three HSI datasets when varying β from smaller to larger values. Since the spectral values have similar magnitudes  in all three datasets, we vary the scalar parameter β in all three datasets from 0.001 and 100, and choose the test candidates from the set [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 5, 10, 50, 100]. We did not investigate the effect of β in classification with a certain step interval value because the scale of the parameter is too large to make detailed analysis. As in experiment 3), the sizes of the ISSC band subset on the three datasets are estimated using the DC plot algorithm. The band number k in Indian Pines dataset is 12, the k in PaviaU dataset is 10, and the k in Urban dataset is 20. Other parameter configurations in the experiment are the same as their counterparts in experiment 3) and the detailed parameter settings in the experiment are listed in Table <ref type="table" target="#tab_5">V</ref>.</p><p>Table <ref type="table" target="#tab_9">IX</ref> shows the OCA and ACA results of ISSC using different classifiers and different HSI datasets. For all three datasets, we observe that when increasing the scalar parameter β, the classification accuracies of ISSC gradually decrease with small fluctuations. Moreover, the OCA and ACA decrease quickly for β between 0.0001 and 1, whereas the decline in OCA and ACA is much slower when β is from 1 to 100. This implies the scalar parameter β has a great effect on classification accuracy of the ISSC band subset for HSI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis and Discussion</head><p>The above four groups of experiments on the three HSI datasets test the performance of our ISSC method in classification. ISSC is compared against the six popular band selection methods LCMV-BCC, AP, SID, MVPCA, SpaBS, and SNMF. The three quantitative measures of AIE, ACC, and ARE show that the ISSC has the best performance among all seven methods. The ISSC assumes that all bands lie in a union of subspaces and that each band can be sparsely represented with other bands from its subspace. The sparse and block diagonal coefficient matrix found by solving the l 2 -norm optimization problem with LSR shows a grouping effect. Furthermore, the optimization solution can guarantee subspace segmentation of band vectors. The ISSC band subset has high intra-band separabilities and low intra-band correlations and each band image has high information amount. The ISSC satisfies the demands of band subset selection and hence is an appropriate method for selecting an appropriate band subset for classification. In contrast, the SID and MVPCA perform worse in the quantitative evaluations and are poor choices for band selection.</p><p>The experiment in computational performance shows that the ISSC has shorter computational times than those of LCMV-BCC, AP, MVPCA, SpaBS, and SNMF. The SID has the shortest computational times of all. The speed advantage of SID is because SID only computes the diagonal entries of the similarity matrix. The AP has longer computational times than SID because it computes the entire similarity matrix of band vectors rather than only computing the diagonal elements of the similarity matrix. The longer computational times of MVPCA are due to the computation in principal component analysis (PCA) transformation of HSI dataset. The lowest computational speeds in SpaBS results from the huge computational complexity of dictionary learning using the K-SVD algorithm <ref type="bibr" target="#b55">[56]</ref>.</p><p>The experiment in classification performance compares classification accuracies of ISSC against those of six other methods (LCMV-BCC, AP, SID, MVPCA, SpaBS, and SNMF). The results show that SID has the worst performance in classification although its computational times are shortest. That coincides with the conclusions in quantitative evaluations of SID. Fortunately, when comparing the ACA and OCA of ISSC against those of other methods, ISSC performs better than the other six methods when using a cluster size obtained from the DC plot algorithm. This implies the DC plot algorithm finds an appropriate band number for selecting an appropriate band subset and guarantee good performance of ISSC in HSI dataset classification. Finally, the experiment on the effect from the scalar parameter β on the classification sensitivity of ISSC shows that the ACA and OCA of ISSC decrease with increasing β. Therefore, a smaller scalar parameter β is appropriate for the ISSC when selecting an appropriate band subset, since smaller parameter β produce higher classification accuracies and shorter computational times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION AND FUTURE WORK</head><p>This paper proposes the ISSC method to select an appropriate band subset from the HSI dataset. The ISSC assumes that each band is drawn from a union of low-dimensional subspaces and each band can be sparsely represented by other bands in its subspace. Our algorithm constructs a similarity matrix with sparse coefficient band vectors. The appropriate band subset is then selected from band clusters using the similarity matrix with an appropriate band number found by the DC plot algorithm. Four groups of experiments are designed and implemented to completely investigate the performance of ISSC. First, the band subset chosen by ISSC is quantitatively evaluated using the AIE, ACC, and ARE measures. Its performance is compared against the performance of the six popular methods LCMV-BCC, AP, MVPCA, SID, SpaBS, and SNMF. The results show that the ISSC band subset contains higher information amount, lower intra-band correlations, and higher intra-band separabilities. Second, the experiment in computational performance illustrates that the ISSC has a shorter computational time than do the other five methods (LCMV-BCC, AP, MVPCA, SpaBS, and SNMF). Third, the experiment in classification performance shows that classification accuracy of ISSC measured by ACA and OCA is better than the six other methods when using an appropriate number found by the DC plot algorithm. In short, the ISSC achieves the best classification performance with the second shortest computational time. In contrast, the SID proves to be worse both in quantitative evaluation results and in classification accuracy. Finally, the experiment on the effect from the scalar parameter β on the sensitivity of classification accuracy of HSI dataset shows that a choice of a small scalar parameter β leads to accurate classification. In the future, we will test our ISSC method against more HSI datasets to further understand its performance in real-world applications. Moreover, we will try the l 0 -norm for the ISSC hoping to further ameliorate and improve its classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Band selection using ISSC.</figDesc><graphic coords="5,357.99,66.65,140.16,256.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Image of Indian Pines dataset.</figDesc><graphic coords="6,371.52,67.37,114.96,114.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Image of PaviaU dataset.</figDesc><graphic coords="7,107.48,233.23,114.96,152.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Image of Urban dataset.</figDesc><graphic coords="8,106.00,195.38,120.00,120.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. OCA results of all the seven methods on the three HSI datasets. (a), (c), and (e): SVM; (b), (d), (f): KNN.</figDesc><graphic coords="10,105.52,264.37,384.00,371.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,96.49,91.57,400.08,158.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,53.99,301.44,485.28,290.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="9,66.49,644.27,460.08,111.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,66.49,91.21,460.08,141.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,45.49,269.13,502.08,143.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I CONTRAST</head><label>I</label><figDesc>IN COMPUTATIONAL COMPLEXITY BETWEEN ISSC AND SIX OTHER BAND SELECTION METHODS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II GROUND</head><label>II</label><figDesc>TRUTH OF TRAINING AND TESTING SAMPLES IN EACH CLASS FOR INDIAN PINES DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III GROUND</head><label>III</label><figDesc>TRUTH OF TRAINING AND TESTING SAMPLES IN EACH CLASS FOR PAVIAU DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV GROUND</head><label>IV</label><figDesc>TRUTH OF TRAINING AND TESTING SAMPLES IN EACH CLASS FOR URBAN DATASET</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V LISTS</head><label>V</label><figDesc>OF PARAMETERS IN ALL THE EXPERIMENTS ON THE THREE HSI DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI CONTRAST</head><label>VI</label><figDesc>IN QUANTITATIVE EVALUATION OF BAND SUBSETS FROM ALL SEVEN METHODS ON THE THREE DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPUTATIONAL</head><label>VII</label><figDesc>TIMES OF SEVEN BAND SELECTION METHODS USING DIFFERENT CHOICES OF K ON THREE HSI DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII CLASSIFICATION</head><label>VIII</label><figDesc>ACCURACIES OF ALL THE METHODS USING AN APPROPRIATE SIZE OF BAND SUBSET ON THREE DATASETS</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX EFFECT</head><label>IX</label><figDesc>FROM SCALAR PARAMETER IN ISSC ON CLASSIFICATION ACCURACIES OF THE THREE DATA HSI DATASETS</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the editor and referees for their suggestions which improved this paper.</p></div>
			</div>


			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Natural Science Foundation under Grant 41401389, Grant 41431175, and Grant 61471274, in part by the Research Project of Zhejiang Educational Committee under Grant Y201430436, in part by Ningbo Natural Science Foundation under Grant 2014A610173, in part by the Discipline Construction Project of Ningbo University under Grant ZX2014000400, Normal project of Shanghai Normal University under Grant SK201525, in part by the Key Laboratory of Mining Spatial Information Technology of NASMG under Grant KLM201309, and in part by the K. C. Wong Magna Fund in Ningbo University.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A discriminative metric learning based anomaly detection method</title>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6844" to="6857" />
			<date type="published" when="2014-11">Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Detecting trend and seasonal changes in bathymetry derived from HICO imagery: A case study of Shark Bay, Western Australia</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Fearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">I</forename><surname>Mckinna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page" from="186" to="205" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Assessing the performance of two unsupervised dimensionality reduction techniques on hyperspectral APEX data for high resolution urban land-cover mapping</title>
		<author>
			<persName><forename type="first">L</forename><surname>Demarchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="166" to="179" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Urban tree species mapping using hyperspectral and lidar data fusion</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alonzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bookhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sens. Environ</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="70" to="83" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Use of airborne hyperspectral imagery to map soil properties in tilled agricultural fields</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Hively</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Environ. Soil Sci</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ground-level hyperspectral imagery for detecting weeds in wheat fields</title>
		<author>
			<persName><forename type="first">I</forename><surname>Herrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Precis. Agric</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="637" to="659" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mapping the distribution of ferric iron minerals on a vertical mine face using derivative analysis of hyperspectral imagery (430-970 nm)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Monteiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="29" to="39" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using airborne hyperspectral data to characterize the surface pH and mineralogy of pyrite mine tailings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zabcic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Appl. Earth Observ. Geoinf</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="152" to="162" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The curse of dimensionality and dimension reduction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multivariate Density Estimation: Theory, Practice, and Visualization</title>
		<meeting><address><addrLine>Hoboken, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="195" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction and classification of hyperspectral image data using sequences of extended morphological transformations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="466" to="479" />
			<date type="published" when="2005-03">Mar. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature extraction of hyperspectral images using wavelet and matching pursuit</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS J. Photogramm. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="92" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Feature selection for classification of hyperspectral data by SVM</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Foody</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2297" to="2307" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction via the ENH-LTSA method for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="375" to="388" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised feature extraction and band subset selection techniques based on relative entropy criteria for hyperspectral data analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Arzuaga-Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Jimenez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Velez-Reyes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<publisher>AeroSense</publisher>
			<biblScope unit="page" from="462" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Methodology for hyperspectral band selection</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bajcsy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="793" to="802" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Band selection for hyperspectral image classification using mutual information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="522" to="526" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Constrained band selection for hyperspectral imagery</title>
		<author>
			<persName><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1575" to="1585" />
			<date type="published" when="2006-06">Jun. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustering-based hyperspectral band selection using information measures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martínez-Usó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4158" to="4171" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised band selection for hyperspectral imagery classification without manual band removal</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="531" to="543" />
			<date type="published" when="2012-04">Apr. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimum band selection for supervised classification of multispectral data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mausel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kramber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Photogramm. Eng. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="55" to="60" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Similarity-based unsupervised band selection for hyperspectral image analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="564" to="568" />
			<date type="published" when="2008-10">Oct. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Band selection for hyperspectral imagery: A new approach based on complex networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1229" to="1233" />
			<date type="published" when="2013-09">Sep. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Band selection and its impact on target detection and classification in hyperspectral image analysis</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Workshop Adv</title>
		<meeting>IEEE Workshop Adv<address><addrLine>Greenbelt, MD, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">Oct. 27-28, 2003</date>
			<biblScope unit="page" from="374" to="377" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Anal. Remotely Sens. Data</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An efficient method for supervised hyperspectral band selection</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geosci. Remote Sens. Lett</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="138" to="142" />
			<date type="published" when="2011-01">Jan. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A sparse representation-based binary hypothesis model for target detection in hyperspectral images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1346" to="1354" />
			<date type="published" when="2015-03">Mar. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using dictionary-based sparse representation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3973" to="3985" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sparse representation for target detection in hyperspectral imagery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="629" to="640" />
			<date type="published" when="2011-06">Jun. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse representation based band selection for hyperspectral images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th IEEE Int. Conf. Image Process. (ICIP)</title>
		<meeting>18th IEEE Int. Conf. Image ess. (ICIP)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11-14">Sep. 11-14, 2011</date>
			<biblScope unit="page" from="2693" to="2696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Clustering-based hyperspectral band selection using sparse nonnegative matrix factorization</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">T</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Zhejiang Univ. Sci. C</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="542" to="549" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hyperspectral band selection using a collaborative sparse model</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)</title>
		<meeting>IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012">Jul. 22-27, 2012</date>
			<biblScope unit="page" from="3054" to="3057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Band selection in hyperspectral imagery using sparse support vector machines</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chepushtanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gittins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE DSS Conf</title>
		<meeting>SPIE DSS Conf</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">90881</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recog. (CVPR&apos;09)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recog. (CVPR&apos;09)<address><addrLine>Miami, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">Jun. 20-26, 2009</date>
			<biblScope unit="page" from="2790" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparse subspace clustering: Algorithm, theory, and applications</title>
		<author>
			<persName><forename type="first">E</forename><surname>Elhamifar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2765" to="2781" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Comput</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast sparse representation based on smoothed L0 norm</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Mohimani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Babaie-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jutten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Independent Component Analysis and Signal Separation</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="389" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">An interior-point method for largescale L1-regularized logistic regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1519" to="1555" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Atomic decomposition by basis pursuit</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Saunders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="61" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Alternating direction algorithms for L1-problems in compressive sensing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Sci. Comput</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="250" to="278" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficient subspace segmentation via quadratic programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th AAAI Conf</title>
		<meeting>25th AAAI Conf<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-07-11">Aug. 7-11, 2011</date>
			<biblScope unit="page" from="519" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Robust and efficient subspace segmentation via least squares regression</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 12th Eur. Conf. Comput. Vis. Part VII</title>
		<meeting>12th Eur. Conf. Comput. Vis. Part VII<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-13">Oct. 7-13, 2012</date>
			<biblScope unit="page" from="347" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Van Loan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Matrix Computations</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Johns Hopkins University Press</publisher>
			<pubPlace>Baltimore, Maryland</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by sparse representation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIAM Int. Conf. Data Min. (SDM&apos;09)</title>
		<meeting>SIAM Int. Conf. Data Min. (SDM&apos;09)<address><addrLine>Sparks, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-05-02">Apr. 30/May 2, 2009</date>
			<biblScope unit="page" from="792" to="801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A nonnegative sparsity induced similarity measure with application to cluster analysis of spam images</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP)</title>
		<meeting>IEEE Int. Conf. Acoust. Speech Signal ess. (ICASSP)<address><addrLine>Dallas, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">Mar. 14-19, 2010</date>
			<biblScope unit="page" from="5594" to="5597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Spectral clustering of high-dimensional data exploiting sparse representation vectors</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="229" to="239" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">How many clusters? An information-theoretic perspective</title>
		<author>
			<persName><forename type="first">S</forename><surname>Still</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2483" to="2506" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimating the number of clusters in a data set via the gap statistic</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Walther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Roy. Stat. Soc. B</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="411" to="423" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Classifying existing and generating new training image patterns in kernel space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Honarkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 21st Stanford Center Reservoir Forecast. Aliate Meeting (SCRF)</title>
		<meeting>21st Stanford Center Reservoir Forecast. Aliate Meeting (SCRF)<address><addrLine>Stanford University, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Speaker diarization exploiting the eigengap criterion and cluster ensembles</title>
		<author>
			<persName><forename type="first">N</forename><surname>Bassiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Moschou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio Speech Lang. Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2134" to="2144" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Edge based technique to estimate number of clusters in k-means color image segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jondhale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE Int. Conf. Comput. Sci</title>
		<meeting>3rd IEEE Int. Conf. Comput. Sci<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-09-11">Jul. 9-11, 2010</date>
			<biblScope unit="page" from="117" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Stochastic simulation of patterns using distance-based pattern modeling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Honarkhah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Geosci</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="487" to="517" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Band selection for hyperspectral imagery using affinity propagation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="213" to="222" />
			<date type="published" when="2009-12">Dec. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A joint band prioritization and band-decorrelation approach to band selection for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geosci. Remote Sens</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2631" to="2641" />
			<date type="published" when="1999-11">Nov. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nearest neighbor pattern classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="27" />
			<date type="published" when="1967-01">Jan. 1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<title level="m">Support Vector Machines</title>
		<meeting><address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zibulevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS Technion</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
