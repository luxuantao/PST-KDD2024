<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental Few-Shot Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-12">12 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Juan-Manuel</forename><surname>Pérez-Rúa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre</orgName>
								<address>
									<settlement>Cambridge</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Surrey</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental Few-Shot Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-12">12 Mar 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2003.04668v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T13:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Most existing object detection methods rely on the availability of abundant labelled training samples per class and offline model training in a batch mode. These requirements substantially limit their scalability to open-ended accommodation of novel classes with limited labelled training data. We present a study aiming to go beyond these limitations by considering the Incremental Few-Shot Detection (iFSD) problem setting, where new classes must be registered incrementally (without revisiting base classes) and with few examples. To this end we propose OpeNended Centre nEt (ONCE), a detector designed for incrementally learning to detect novel class objects with few examples. This is achieved by an elegant adaptation of the CentreNet detector to the few-shot learning scenario, and meta-learning a class-specific code generator model for registering novel classes. ONCE fully respects the incremental learning paradigm, with novel class registration</head><p>requiring only a single forward pass of few-shot training samples, and no access to base classes -thus making it suitable for deployment on embedded devices. Extensive experiments conducted on both the standard object detection and fashion landmark detection tasks show the feasibility of iFSD for the first time, opening an interesting and very important line of research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Despite the success of deep convolutional neural networks (CNNs) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b48">49]</ref> in object detection <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b27">28]</ref>, most existing models can be only trained offline via a lengthy process of many iterations in a batch setting. Under this setting, all the target classes are known, each class has a large number of annotated training samples, and all training images are used for training. This annotation cost and training complexity severely restricts the potential for these methods to grow and accommodate new classes online. Such a missing capability is required in robotics applications <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>, when the detector is running on embedded devices, or simply to scale up to addressing the long tail of object categories to recognise <ref type="bibr" target="#b30">[31]</ref>. In contrast, humans learn new concepts such as object classes incrementally without forgetting previously learned knowledge <ref type="bibr" target="#b13">[14]</ref>, and often requiring only a few visual examples per class <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b2">3]</ref>. Motivated by the vision of closing this gap between state-of-the-art object detection and human-level intelligence, a couple of very recent studies <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22]</ref> proposed methods for few-shot object detector learning.</p><p>Nonetheless, both methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22]</ref> are fundamentally unscalable to real-world deployments in open-ended or robotic learning settings, due to lacking the capability of incremental learning of novel concepts from a data stream over time. Specifically, they have to perform a costly training/updating of the detection model using the data of both old (base) and new (novel) classes together, whenever a novel class should be added. Consequently, while they successfully reduce annotation requirements, these models essentially reduce to the conventional batch learning paradigm. This leads to a prohibitively expensive quadratic computation cost in number of categories in an incremental scenario, and also raises issues in data privacy over time <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>. Meanwhile, storage and compute requirements prohibit on-device deployment in robotic scenarios where a robot might want to incrementally register objects encountered in the world for future detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>To overcome the aforementioned limitation, we study a very practical learning setting -Incremental Few-Shot Detection (iFSD). The iFSD setting is defined by: (1) The detection model can be pre-trained in advance on a set of base classes each with abundant training samples available -it makes sense to use existing annotated datasets to bootstrap a model <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b1">(2)</ref> Once trained, an iFSD model should be capable of deployment to real-world applications where novel classes can be registered at any time using only a few annotated examples. The model should provide good performance for all classes observed so far (i.e., learning without forgetting). (3) The learning of novel classes from an unbounded stream of examples should be feasible in terms of memory footprint, storage, and compute cost. Ideally the model should support deployment on resource-limited devices such as robots and smart phones.</p><p>Conventional object detection methods are unsuited to the proposed setting due to the intrinsic need for batch learning on large datasets as discussed earlier. An obvious idea is to fine-tune the trained model with novel class training data. However without revisiting old data (batch setting) this causes a dramatic degradation in performance for existing categories due to the catastrophic forgetting challenge <ref type="bibr" target="#b13">[14]</ref>. The state-of-the-art few-shot object detection methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22]</ref> suffer from the same problem too, if denied access to the base (old) class training data and adapted sequentially to novel classes (see the evaluations in Table <ref type="table" target="#tab_1">2</ref>).</p><p>In this work, as the first step towards the proposed incremental few-shot object detection problem in the context of deep neural networks, we introduce OpeN-ended Centre nEt (ONCE). The model is built upon the recently proposed CentreNet <ref type="bibr" target="#b55">[56]</ref>, which was originally designed for conventional batch learning of object detection. We take a feature-based knowledge transfer strategy, decomposing CentreNet into class-generic and class-specific components for enabling incremental few-shot learning. More specifically, ONCE uses the abundant base class training data to first train a class-generic feature extractor. This is followed by meta-learning a class-specific code generator with simulated few-shot learning tasks. Once trained, given a handful of images of a novel object class, the meta-trained class code generator elegantly enables the ONCE detector to incrementally learn the novel class in an efficient feed-forward manner during the meta-testing stage (novel class registration). This is achieved without requiring access to base class data or iterative updating. Compared with <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>, ONCE better fits the iFSD setting in that its performance is insensitive to the arrival order and choice of novel classes. This is due to not using softmax-based classification but per-class thresholding in decision making. Importantly, since each class-specific code is generated independent of other classes, ONCE is intrinsically able to maintain the detection performance of the base classes and any novel classes registered so far.</p><p>We make three contributions in this work: (1) We investigate the heavily understudied Incremental Few-Shot Detection problem, which is crucial to many real-world applications. To the best of our knowledge, this is the first attempt to reduce the reliance of a deeply-learned object detector on batch training with large base class datasets during few-shot enrolment of novel classes, unlike recent few-shot detection alternatives <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>. <ref type="bibr" target="#b1">(2)</ref> We formulate a novel OpeN-ended Centre nEt (ONCE) by adapting the recent CentreNet detector to the incremental few-shot scenario. (3) We perform extensive experiments on both standard object detection (COCO <ref type="bibr" target="#b28">[29]</ref>, PASCAL VOC <ref type="bibr" target="#b11">[12]</ref>) and fashion landmark detection (DeepFashion2 <ref type="bibr" target="#b14">[15]</ref>) tasks. The results show ONCE's significant performance advantage over existing alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object detection Existing deep object detection models fall generally into two categories: (1) Two-stage detectors <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b7">8]</ref>, (2) One-stage detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b24">25]</ref>. While usually being superior in detection performance, the two-stage methods are less efficient than the one-stage ones due to the need for object region inference (and subsequent classification from the set of object proposals). Typically, both approaches assume a large set of training images per class and need to train the detector in an offline batch mode. This restricts their usability and scalability when novel classes must be added on the fly during model deployment. Despite being non-incremental, they can serve as the detection backbone of a few-shot detector. Our ONCE method is based on the one-stage CentreNet <ref type="bibr" target="#b55">[56]</ref> which is chosen because of its efficiency and competitive detection accuracy, as well as the fact that it can be easily decomposed into class-generic and specific parts for adaptation to the Incremental Few-Shot Detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Few-shot learning</head><p>For image recognition, efficiently accommodating novel classes on the fly is widely studied under the name of few-shot learning (FSL) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5]</ref>. Assuming abundant labelled examples of a set of base classes, FSL methods aim to metalearn a data-efficient learning strategy that subsequently allows novel classes to be learned from very limited perclass examples. A large body of FSL work has investigated how to learn from such scarce data without overfitting <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref>. Nonetheless, these FSL works usually focus on classification of whole images, or well cropped object images. This is much simpler than object detection as object instances do not need to be separated from diverse background clutter, or localised in space and scale. In this work we extend few-shot classification to the more challenging object detection task.</p><p>Few-shot object detection and beyond A few recent works have attempted to exploit few-shot learning techniques for object detection <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. However, these differ significantly from ours in that they consider a nonincremental learning setting, which restricts dramatically their scalability and applicability in scenarios where access to either large-scale base class data is prohibitive. This is the case for example, due to limited computational resources and/or data privacy issues. We therefore consider the more practical incremental few-shot learning setting, eliminating the infeasible requirement of repeatedly training the model on the large-scale base class training data 1 . While this more challenging scenario inevitably lead to inferior performance compared to non-incremental learning, as shown in our experiments, it is more representative of natural human learning capabilities and thus provides a good research target with great application potential once solved.</p><p>There are other techniques that aim to minimise the amount of data labelling for object detection such as weakly supervised learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> and zero-shot learning <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b1">2]</ref>. They assume different forms of training data and prior knowledge, and conceptually are complementary to our iFSD problem setting. They can thus be combined when there are multiple input sources available (e.g., unlabelled data or semantic class descriptor).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Problem Definition We consider the problem of Incremental Few-Shot Detection (iFSD): obtaining a learner able to incrementally recognise novel classes using only a few labelled examples per class. We consider two disjoint object class sets: the base classes used to bootstrap the system, which are assumed to come with abundant labelled data; and the novel classes which are sparsely annotated, and to be enrolled incrementally. That is, in a computationally efficient way, and without revisiting the base class data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object Detection Architecture</head><p>To successfully learn object detection from sparsely annotated novel classes, we wish to build upon an effective architecture, and exploit knowledge transfer from base classes already learned by this architecture.</p><p>However, the selection of the base object detection architecture cannot be arbitrary given that we need to adapt the detection model to novel classes on-the-fly. For example, while Faster R-CNN <ref type="bibr" target="#b42">[43]</ref> is a common selection and provides strong performance given large scale annotation, it is less flexible to accommodate novel classes due to a two-stage design and the use of softmax-based classification.</p><p>In this work, we build upon the recently developed object detection model CentreNet <ref type="bibr" target="#b55">[56]</ref> based on several considerations: (1) It is a highly-efficient one-stage object detection pipeline with better speed-accuracy trade-off than alternatives such as SSD <ref type="bibr" target="#b29">[30]</ref>, RetinaNet <ref type="bibr" target="#b27">[28]</ref>, and YOLO <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>. (2) Importantly, it follows a class-specific modelling paradigm, which allows easy and efficient introduction of novel classes in a plug-in manner. Indeed, the core feature of CentreNet, the per-class heatmap-based centroid 1 Some works have studied incremental learning of object detectors <ref type="bibr" target="#b34">[35]</ref>. However, they do not tackle the few shot regime. prediction is naturally appropriate for incremental learning as required in our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">A Review of the CentreNet Model</head><p>The key idea of CentreNet is to reformulate object detection as a point+attribute regression problem. It is inspired by keypoint detection methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b51">52]</ref>, taking a similar spirit to <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b56">57]</ref> without the need for point grouping and postprocessing. The architecture of CentreNet is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Specifically, as the name suggests, CentreNet takes the centre point and the spatial size (i.e. the width &amp; height) of an object bounding box as the regression target. Both are represented with 2D heatmaps, generated according to the ground-truth annotation. In training, the model is optimised to predict such heatmaps, supervised by an L 1 regression loss. For model details, we refer the readers to the original paper due to space limit.</p><p>Remarks It is worth mentioning that this keypoint estimation based formulation for object detection not only eliminates the need for region proposal generation, but also enables object location and size prediction in a common format by predicting corresponding pixel-wise aligned heatmaps. For few-shot object detection in particular, a key merit of CentreNet is that each individual class maintains its own prediction heatmap and makes independent detection by activation thresholding. We show next how to exploit this property of CentreNet in order to support incremental enrolment of novel classes in an order-free and combination-insensitive manner, without interference between old &amp; new classes. This is in contrast to the softmax classification used in existing models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22]</ref> where interactions between classes make this vision hard to achieve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Incremental Few-shot Object Detection</head><p>As CentreNet is a batch learning model, it is unsuited for iFSD. We address this problem by incorporating a metalearning strategy <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b43">44]</ref> to CentreNet architecture, which results in the proposed OpeN-ended Centre nEt (ONCE).</p><p>Model formulation ONCE starts by decomposing Cen-treNet into two components: (i) feature extractor, which is shared by all the base and novel classes, and (ii) object locator, which contains class-specific parameters for each individual class to be detected. Specifically, feature extractor takes as input an image, and outputs a 3D feature map. Then, the object locator analyses the feature map with a class-specific code used as convolutional kernel and yields the object detection result for that class in form of a heatmap.</p><p>In a standard extractor/locator decomposition of Cen-treNet, the object locator still needs to be trained in batch mode, and with large scale training data. In ONCE, we further paramaterise the object locator by a meta-learned generator network, where the generator network synthesises the parameters of the locator network (i.e., the class-specific convolutional kernel weights) given a few-shot support set. In this way, we transform the conventional batch-mode detector learning problem (second CNN in Fig. <ref type="figure" target="#fig_0">1</ref>, indicated with the tag "object locator") into a feed-forward pass of the parameter generator meta-network (class code genera-tor in Fig. <ref type="figure" target="#fig_1">2</ref>). To achieve this, we perform meta-learning to train the class code generator to solve few-shot detector learning tasks via weight synthesis given a support set (resulting in the green and orange class-specific object locators in Fig. <ref type="figure" target="#fig_1">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Training: Learning a Few-Shot Detector</head><p>To fully exploit the base classes with rich training data, we train ONCE in two sequential stages. In the first stage, we train the class-agnostic feature extractor on the base-class training data. This feature extractor is then fixed in subsequent steps as per other few-shot strategies <ref type="bibr" target="#b47">[48]</ref>. In the second stage, we learn few-shot object-detection by jointly training the object locator, which is conditioned on a classspecific code; along with a meta-network that generates it given a support set. This is performed by episodic training that simulates few-shot episodes that will be encountered during deployment. In the following sections we describe the training process in more detail.</p><p>Meta-Testing: Enrolling New Classes At test time, given a support set of novel classes each with a few labelled bounding boxes, we directly deploy the trained feature extractor, object locator, and code generator learned during meta-training above. The meta-network generates object-specific weights from the support-set (few-shot) and the object locator uses these to detect objects in the test images. This means that novel class objects are detected in test images in a feed forward manner, without model training and/or adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Stage I: Feature Extractor Learning</head><p>We aim to learn a class-agnostic feature extractor f in ONCE. This can be simply realised by standard supervised learning with bound-box level supervision on the base classes, similarly to the original CentreNet <ref type="bibr" target="#b55">[56]</ref>. Concretely, we perform keypoint detection and train the detection model (including both feature extractor f (•) and object locator h(•)) by heatmap regression loss. In this stage we train a complete feature extractor and object locator pipeline, although the objective of this stage is solely to learn a robust feature extractor f (•). The locator learned in this stage is a regular CentreNet locator, which will be discarded in stage II, but will be used at the test time for the base classes. </p><formula xml:id="formula_0">Y k = h(m, c k ) = m ⊙ c k , k ∈ {1, 2, • • • , K b },<label>(1)</label></formula><p>where ⊙ denotes the convolutional operation, and K b denotes the number of base classes.</p><p>For locating the object instances of class k, we start by identifying local peaks P k = {(x i , y i )} n i=1 , which are the points whose activation value is higher or equal to its 8connected spatial neighbours in Y k . The bounding box prediction is inferred as</p><formula xml:id="formula_1">(x i + δx i − w i /2, y i + δy i − h i /2,<label>(2)</label></formula><formula xml:id="formula_2">x i + δx i + w i /2, y i + δy i + h i /2) where (δx i , δy i ) = O xi,yi is the offset prediction, O ∈ R h r × w r ×2</formula><p>, and (h i , w i ) = S xi,yi is the size prediction, S ∈ R h r × w r ×2 , generated by the offset and size codes in the same fashion as the class codes. Given the ground-truth bounding boxes and this prediction, we use the L 1 regression loss for model optimisation on the parameters of feature extractor f and parameters c of locator h. In practice, the feature extractor model is implemented with the ResNet-based backbone of <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Stage II: Class Code Generator Learning</head><p>The class code parameters c learned for detection above are fixed parameters for base-classes only. To deal with the iFSD setting, besides these base-class codes we need an inductive class code generator g(•) that can efficiently synthesise class codes for novel classes on the fly during deployment, given only a few labelled samples. To train the class code generator g(•), we exploit an episodic metalearning strategy <ref type="bibr" target="#b50">[51]</ref>. This uses the base class data to sample a large number of few-shot tasks, thus simulating the test-time requirement of few-shot learning of new tasks. While episodic meta-learning is widely used in few-shot recognition, we customise a strategy for detection here.</p><p>Specifically, we define an iFSD task T as uniform distribution over possible class label sets L, each with one or a few unique classes. To form an episode to compute gradients and train the class code generator g(•), we start by sampling a class label set L from T (e.g., L = {person, bottle, • • • }). With L, we sample a support (metatraining) set S and a query (meta-validation) set Q. Both S and Q are labelled samples of the classes in L.</p><p>In the forward pass, the support set S is used for generating a class code for each sampled class k as:</p><formula xml:id="formula_3">ck = g(S k ),<label>(3)</label></formula><p>where S k are the support samples of class k. With these codes {c k }, our method then performs object detection for query images I by using the feature extractor (Eq. ( <ref type="formula" target="#formula_4">4</ref>)) and object locator (Eq.( <ref type="formula" target="#formula_5">5</ref>)):</p><formula xml:id="formula_4">m = f (I), with I ∈ Q,<label>(4)</label></formula><formula xml:id="formula_5">Ỹ = h(m, ck ).<label>(5)</label></formula><p>ONCE is then trained to minimise the mean prediction error on Q by updating solely the parameters of the code generator (cf. Eq. ( <ref type="formula" target="#formula_3">3</ref>)). Same as CentreNet, L 1 loss is used as the objective function in this stage, defined as | Ỹ − Z| where Z is the ground-truth heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Meta Testing: Enrolling New Classes</head><p>Given the feature extractor (f , trained in Stage I), the code generator (g, trained in stage II), and the object locator (h, Eq. ( <ref type="formula" target="#formula_0">1</ref>)), At test time, ONCE can efficiently enrol any new class with a few labelled samples in a feed forward manner without model adaptation and update. The meta-testing for a novel class is summarised as:</p><p>1. Obtaining its class code with a few-shot labelled set using Eq. (3);</p><p>2. Computing the test image features by using Eq. ( <ref type="formula" target="#formula_4">4</ref>); This process applies for the base classes except that Step 1 is no longer needed since their class codes are already obtained from the training stage I (cf. Eq. ( <ref type="formula" target="#formula_0">1</ref>)). In doing so, we can easily introduce novel classes independently which facilitates the model iFSD deployment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Architecture</head><p>For the feature extractor function f , we start from a strong and simple baseline architecture <ref type="bibr" target="#b51">[52]</ref> that uses ResNet <ref type="bibr" target="#b20">[21]</ref> as backbone. This architecture consists of an encoderdecoder pair that first extracts a low resolution 3D map and then expands it by means of learnable upsampling convolutions, outputting high resolution feature maps f (I) for an input image I. We leverage the same backbone for the class code generator (without the upsampling operations). Before meta-training (stage II), the class code generator weights are initialised by cloning the weights of the encoder part of the feature extractor. The final convolution outputs are globally pooled to form the class codes c k , giving a code size of 256<ref type="foot" target="#foot_0">2</ref> . To handle support sets with variable size, we adopt the invariant set representation of <ref type="bibr" target="#b53">[54]</ref> by average pooling of the class code generator outputs for every image I k,s i in S k . The code and trained model will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Non-Incremental Few-Shot Detection</head><p>We start the experimental section with an important contextual experiment. We evaluated the performance of ONCE in the non-incremental setting as studied in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>. In particular, we use COCO <ref type="bibr" target="#b28">[29]</ref>, a popular object detection benchmark, covering 80 object classes from which 20 are left-out to be used as novel classes. These meta-testing classes happen to be the 20 categories covered by the PAS-CAL VOC dataset <ref type="bibr" target="#b11">[12]</ref>. The remaining 60 classes in COCO serve as base classes. For model training, we used 10 shots per novel class along with all the base class training data. The results on COCO in Table <ref type="table" target="#tab_0">1</ref> show that, while not directly comparable due to using different detection backbones and/or data split, ONCE approaches the performance of the two state-of-the-art models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>. We continue our experimental analysis hereafter with the incremental setting, which happens to be much more challenging and not trivially tackled with previous methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Incremental Few-Shot Object Detection</head><p>Experimental setup For evaluating iFSD, we followed the evaluation setup of <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref> but with the key differences that the base class training data is not accessible during meta-testing, and incremental update for novel classes is required. In particular, the widely used object detection benchmarks COCO <ref type="bibr" target="#b28">[29]</ref> and PASCAL VOC <ref type="bibr" target="#b11">[12]</ref> are used. As mentioned before, COCO covers 80 object classes including all 20 classes in PASCAL VOC. We treated the 20 VOC/COCO shared object classes as novel classes, and the remaining 60 classes in COCO serve as base classes. This leads to two dataset splits: same-dataset on COCO and cross-dataset with 60 COCO classes as base.</p><p>For the same-dataset evaluation on COCO, we used the train images of base classes for model meta-training. In each episode, we randomly sampled 32 tasks, each of which consisted of a 3-class detection problem, and for which each class had 5 annotated bounding boxes per-class. Larger learning tasks may be beneficial for performance but requiring more GPU memory in training, thus not possible with the resources at our disposal. For meta-testing, we randomly sampled a support set from the training split of all 20 novel classes. To enrol these 20 novel classes to the model, we consider two settings: incremental batch, or continuous incremental learning. In the incremental batch setting, all 20 novel classes are added at once with a single model update. In the continual incremental learning setting, the 20 novel classes are added one by one with 20 models updates. We evaluated few-shot detection learning with k ∈ {1, 5, 10}) bounding boxes annotated for each novel class. In practice, we used the same support sets of novel classes as <ref type="bibr" target="#b21">[22]</ref> for enabling a direct comparison between incremental learning and non-incremental learning (the data split used in <ref type="bibr" target="#b52">[53]</ref> is not publicly available). We then evaluated the model performance on the validation set of the novel classes.</p><p>For the cross-dataset evaluation from COCO to VOC, we used the same setup and train/test data partitions as above, except that the model was evaluated on the PASCAL VOC 2007 test set. That is, the meta-training support/query sets were drawn from COCO, while meta-testing was performed using VOC training data for few-shot detector learning, and VOC testing data for evaluation. Competitors We compared our ONCE method with several alternatives: (1) The standard Fine-Tuning method, (2) a popular meta-learning method MAML (the first-order variant) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>, and (3) a state-of-the-art (non-incremental) few-shot object detection method Feature-Reweight <ref type="bibr" target="#b21">[22]</ref>. In particular, since <ref type="bibr" target="#b21">[22]</ref> was originally designed for the nonincremental setting, we adapted it to the iFSD setting based on its publicly released code <ref type="foot" target="#foot_1">3</ref> . We note that Meta R-CNN   <ref type="bibr" target="#b21">[22]</ref> is adapted to use the same detection backbone (CentreNet) and setting for fair comparison.</p><p>[53] shares the same formulation as <ref type="bibr" target="#b21">[22]</ref>, with the difference of reweighting the regional proposals rather than the whole images. However, without released code we cannot reproduce the Meta R-CNN. All methods are implemented on CentreNet/ResNet50 for both the backbone of the detector network and the code generator meta-networks.</p><p>Object detection on COCO We first evaluated the incremental batch setting. The results of all the methods are compared in Table <ref type="table" target="#tab_1">2</ref>. We have several observations: (1)</p><p>The standard Fine-Tuning method is not only ineffective for learning from few-shot samples per novel class, but also suffers from catastrophic forgetting (massive base class performance drop), making it unsuited for iFSD.</p><p>(2) As a representative meta-learning method 4 , MAML improves slightly the few-shot detection capability over Fine-Tuning. However, without access to the support sets of base (old) classes in iFSD, it is incapable of performing object detection for base classes. After all, MAML is not designed for incremental learning.</p><p>(3) Feature-Reweight, similarly to Fine-Tuning, suffers from catastrophic forgetting when used in the incremental setting. It is inferior to our method for most test time. 4 Note that there are many more recent FSL methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16]</ref> that produce better performance on image classification tasks than MAML. However, they are designed for classification only and cannot be easily adapted for the object detection task here. metrics, with a slight edge on novel class detection for the 10-shot experiment. This occurs, at the cost of intensive optimisation in testing time, which is not ideal in many practical scenarios. (4) ONCE achieves the best performance on most experiments for both novel and base classes simultaneously. The improvements over baselines are more significant with fewer shots. In particular, by class-specific detector learning ONCE keeps the performance on base classes unchanged, naturally solving the learning without forgetting challenge. <ref type="foot" target="#foot_2">5</ref> (5) While the absolute performance on novel classes is still low, this is a new and extremely challenging problem, for which ONCE provides a promising first solution without requiring test-time optimisation. Some qualitative results are shown in Fig. <ref type="figure" target="#fig_4">3</ref>.</p><p>We also evaluated the continuous incremental learning setting: novel classes are added one at a time. We reported the all-classes accuracy. In this test, we excluded MAML since it is not designed for iFSD with no capability of detecting base class objects. The results in Fig. <ref type="figure">4</ref> show that the performance of ONCE changes little, while the performance of the competitors drops quickly due to increased forgetting as more classes are added. These results validate our ONCE's ability to add new classes on-the-fly.</p><p>Object detection transfer from COCO to VOC. We evaluated iFSD in a cross-dataset setting from COCO to VOC. We considered the incremental batch setting, and reported the novel class performance since there are no base class images in VOC. The results in Table <ref type="table" target="#tab_2">3</ref> show that: (1) The same relative results are obtained as in Table <ref type="table" target="#tab_1">2</ref>, confirming that the performance advantages of our model transfers to a test domain different from the training one. (2) Higher performance is obtained on VOC by all methods compared to COCO. This makes sense as COCO images are more challenging and unconstrained than those in VOC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Few-Shot Fashion Landmark Detection</head><p>Experimental setup.</p><p>Besides object detection, we further evaluated our method for fashion landmark detection on the DeepFashion2 benchmark <ref type="bibr" target="#b14">[15]</ref>. This dataset has 801K clothing items from 13 categories. A single clothing category contains 8∼19 landmark classes, giving a total of 294 classes. This forms a two-level hierarchical semantic structure, which is not presented in the COCO/VOC dataset.</p><p>Each image, captured either in commercial shopping stores or in-the-wild consumer scenarios, presents one or multiple clothing items.</p><p>On top of the original train/test data split, we developed an iFSD setting. Specifically, we split the 294 landmark classes into three sets: 153 for training (8 clothing categories), 95 for validation (3 clothing categories), and 46 for testing (2 clothing categories). This split is categorydisjoint across train/val/test sets for testing model generalisation over clothing categories. The most sparse clothing categories are assigned to the test set.</p><p>In each episode of iFSD training, we randomly sampled 1 task each with k-shot annotated landmarks per class and a total of 5 landmark classes, with k ∈ {1, 5, 10}. We used the val set for model selection, i.e. selecting the final model based on the validation accuracy. To avoid overfitting to the training clothing categories, we randomly sampled 5 out of all the available (8∼19) landmark classes in each episode. This is made possible by the class-specific modelling nature of ONCE, while <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref> cannot do this. In meta-testing, we randomly sampled a support set from the original training set of novel landmark classes (part of the iFSD test set), including k ∈ {1, 5, 10} bounding boxes annotated for each novel landmark class, and used it for model learning. We tested the model performance on the original testing set of novel landmark classes (part of the iFSD test set). We repeated the test process 100 times and report the average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competitors.</head><p>In this controlled test, we compared ONCE with the Fine-Tuning baseline. Other methods for few-shot object detection (i.e., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b52">53]</ref>) are not trivially adaptable for this task due to its hierarchical semantic structure. Indeed, the inter-class independence obtained by adopting CentreNet as detection backbone, and our proposed few-shot detection framework allows for such general applicability of ONCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation results.</head><p>We evaluated the incremental batch setting and reported the novel class performance. The results in Table <ref type="table" target="#tab_3">4</ref> show that ONCE consistently and significantly outperforms Fine-Tuning. This suggests that our model is better at transferring the landmark appearance information from base classes to novel classes, even when only as little as one shot training example is available for learning. due to not having to perform iterative optimisation during meta-test. Note that the absolute accuracy achieved on this task is much higher than the object detection tasks (Table <ref type="table" target="#tab_3">4</ref> vs. Tables <ref type="table" target="#tab_2">2&amp;3</ref>). This is due to the fact that all classes are clothing landmarks so there is much more transferable knowledge from base to novel classes. An example of one-shot landmark detection by ONCE is shown in Fig. <ref type="figure" target="#fig_5">5</ref>. It can be seen that the model can detect landmarks accurately after seeing them only once.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have investigated the challenging yet practical incremental few-shot object detection problem. Our proposed ONCE provides a promising initial solution to this problem. Critically, ONCE is capable of incrementally registering novel classes with few examples in a feed-forward manner, without revisiting the base class training data. It yields superior performance over a number of alternatives on both object and landmark detection tasks in the incremental fewshot setting. Our work is evidence of the need for further efforts towards solving more effectively the iFSD problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Overview of Centre-Net. A backbone network, which can be implemented with resolution-reducing blocks followed by upsampling operations, generates feature maps (top). These feature maps are further processed by a small CNN (the object locator) and transformed into a set of per-class heatmaps encoding the object box centre points and its size (bottom).</figDesc><graphic url="image-2.png" coords="3,354.80,167.32,95.74,66.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of our OpeN-ended Centre nEt (ONCE) model. Specifically, the feature extractor (an encoder-decoder model in our implementation, coloured blue in the top-left) generates the class-generic feature maps f (I) of a test image. These maps are further convolved with the class-specific codes (coloured orange for the dog class, and green for the cat class) predicted by the class code generator (bottom-left, coloured yellow) from a few labelled support samples per class, to generate the object detection result in heatmap format (not shown for simplicity). The model training of ONCE involves two stages: (1) Stage I: a regular CentreNet-like supervised learning is performed on the abundant training data of base classes. (2) Stage II: episodic metatraining is performed with the weights of the feature extractor being frozen, allowing the class code generator to learn how to generate a class-specific code from a small per-class support set such that the model can generalise well to unseen novel classes (right). The base classes are used as fake novel classes in meta-training. It is noted that ONCE can also be applied for other detection problems, e.g., fashion landmark localisation.</figDesc><graphic url="image-4.png" coords="4,383.81,105.45,136.48,90.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Given a training image I ∈ R h×w×3 of height h and width w, we extract a class-agnostic feature map m = f (I), m ∈ R h r × w r ×c . The object locator then detects objects of each class k by processing the feature map with a learned class convolutional kernel c k ∈ R 1×1×c , where r is the output stride and c the number of feature channels. We then obtain the heatmap prediction Y k ∈ R h r × w r for class k as:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3 . 4 . 5 .</head><label>345</label><figDesc>Locating object instances of the novel class by Eq. (1); Obtaining all the object candidates using Eq. (2); Finding the heatmap local maxima to output the final detection result of that class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Novel class object detection on the COCO val2017 set. Top: our method. Bottom: Fine-Tuning. The 10-shot iFSD setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Fashion landmark detection by ONCE. Column 1: a support sample with five randomly selected landmark ground-truth. Column 2: a test image with the predicted landmarks. Columns 3-7: predicted heatmaps. Shot Method AP AP 50 AR AR 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Non-incremental few-shot object detection performance on COCO val2017 set. Training setting: 10-shot per novel class and all the base class training data. '</figDesc><table><row><cell>Method</cell><cell cols="6">Novel Classes Base Classes All Classes AP AR AP AR AP AR</cell></row><row><cell>Fine-Tuning</cell><cell>1.4</cell><cell>8.2</cell><cell cols="4">20.7 23.4 15.8 24.4</cell></row><row><cell cols="2">Feature-Reweight [22] 5.6</cell><cell>10.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Meta R-CNN  *  [53]</cell><cell>8.7</cell><cell>12.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ONCE</cell><cell>5.1</cell><cell>9.5</cell><cell cols="4">22.9 29.9 18.4 24.8</cell></row></table><note>* ': Using different (unknown) support sets of novel classes. '-': No reported results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Incremental few-shot object detection performance on COCO val2017 set. Setting: Incremental learning in batch of all 20 novel classes. ' † ': the code of</figDesc><table><row><cell>Shot</cell><cell>Method</cell><cell cols="2">Novel Classes AP AR</cell><cell cols="2">Base Classes AP AR</cell><cell cols="2">All Classes AP AR</cell></row><row><cell></cell><cell>Fine-Tuning</cell><cell>0.0</cell><cell>0.0</cell><cell>1.1</cell><cell>1.8</cell><cell>0.8</cell><cell>1.4</cell></row><row><cell>1</cell><cell>MAML [13] Feature-Reweight  † [22]</cell><cell>0.1 0.1</cell><cell>0.5 0.3</cell><cell>N/A 2.5</cell><cell>N/A 4.3</cell><cell>N/A 1.9</cell><cell>N/A 3.3</cell></row><row><cell></cell><cell>ONCE</cell><cell>0.7</cell><cell>6.3</cell><cell>17.9</cell><cell>19.5</cell><cell>13.6</cell><cell>16.2</cell></row><row><cell></cell><cell>Fine-Tuning</cell><cell>0.2</cell><cell>3.5</cell><cell>2.6</cell><cell>7.4</cell><cell>2.0</cell><cell>6.4</cell></row><row><cell>5</cell><cell>MAML [13] Feature-Reweight  † [22]</cell><cell>0.4 0.8</cell><cell>3.9 5.1</cell><cell>N/A 3.3</cell><cell>N/A 8.2</cell><cell>N/A 2.6</cell><cell>N/A 7.4</cell></row><row><cell></cell><cell>ONCE</cell><cell>1.0</cell><cell>7.4</cell><cell>17.9</cell><cell>19.5</cell><cell>13.7</cell><cell>16.4</cell></row><row><cell></cell><cell>Fine-Tuning</cell><cell>0.6</cell><cell>4.2</cell><cell>2.8</cell><cell>8.0</cell><cell>2.3</cell><cell>7.0</cell></row><row><cell>10</cell><cell>MAML [13] Feature-Reweight  † [22]</cell><cell>0.8 1.5</cell><cell>4.9 8.3</cell><cell>N/A 3.7</cell><cell>N/A 8.9</cell><cell>N/A 3.1</cell><cell>N/A 8.7</cell></row><row><cell></cell><cell>ONCE</cell><cell>1.2</cell><cell>7.6</cell><cell>17.9</cell><cell>19.5</cell><cell>13.7</cell><cell>16.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Incremental few-shot object detection performance on COCO val2017 set. We plot accuracy for all-classes vs. the number of incrementally-added novel classes. Training setting: 10-shot per novel class. ' † ': the code of<ref type="bibr" target="#b21">[22]</ref> is adapted to use the same detection backbone (CentreNet) and setting for fair comparison. AP M AP L AR AR S AR M AR L Incremental few-shot object detection transfer performance on VOC2007 test set. Training data: COCO. Setting: Incremental batch. We only reported the novel class performance as there are no base class images in VOC.</figDesc><table><row><cell></cell><cell></cell><cell>35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ONCE (AP)</cell></row><row><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ONCE (AR) Feature Reweighting (AP)</cell></row><row><cell></cell><cell></cell><cell>25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Feature Reweighting (AR) Fine tuning (AP)</cell></row><row><cell></cell><cell>Score (%)</cell><cell>15 20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Fine tuning (AR)</cell></row><row><cell></cell><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>+p er so n +b icy cle</cell><cell>+c ar +m oto rcy cle +a irp lan e +b us</cell><cell>+tr ain</cell><cell>+b oa t +b ird</cell><cell>+c at</cell><cell>+d og</cell><cell>+h or se +s he ep</cell><cell>+c ow</cell><cell>+b ott le +c ha ir +c ou ch +p ott ed pla nt +d ini ng tab le +tv mo nit or</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7"># incrementally added classes</cell></row><row><cell cols="10">Figure 4: Shot Method Fine-Tuning 0.1 0.1 0.8 0.3 1.9 0.7 2.9 7.6 AP AP S 5 MAML [34] 0.6 0.2 1.1 1.3 2.2 1.2 4.0 10.6</cell></row><row><cell></cell><cell cols="2">ONCE</cell><cell cols="7">2.4 1.2 2.4 3.4 12.2 5.9 16.4 33.6</cell></row><row><cell></cell><cell cols="9">Fine-Tuning 0.3 0.1 0.8 1.0 2.8 0.9 3.3 10.2</cell></row><row><cell>10</cell><cell cols="9">MAML [34] 1.0 0.4 1.7 2.1 3.2 7.9 5.1 12.2</cell></row><row><cell></cell><cell cols="2">ONCE</cell><cell cols="7">2.6 5.7 2.2 4.9 11.6 8.3 19.4 32.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Incremental few-shot landmark detection performance on the novel classes of DeepFashion2. Setting: Incremental batch.</figDesc><table><row><cell>1</cell><cell>Fine-Tuning ONCE</cell><cell>2.8 4.6</cell><cell>9.0 26.5</cell><cell>6.7 11.8</cell><cell>16.5 49.9</cell></row><row><cell>5</cell><cell cols="2">Fine-Tuning 17.0 ONCE 29.5</cell><cell>35.9 77.5</cell><cell>25.1 42.1</cell><cell>42.1 87.4</cell></row><row><cell>10</cell><cell cols="2">Fine-Tuning 17.1 ONCE 32.2</cell><cell>37.1 79.5</cell><cell>24.6 44.3</cell><cell>43.0 88.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0">In practice, a novel class code is</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1">× 256 when accounting for classspecific width and height heatmaps. This is omitted during description of our method for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2">The base class AP we get is lower than in the normal supervised setting. This is due to early stopping during base class training. Without it, we do reach the results reported in<ref type="bibr" target="#b55">[56]</ref>. However, early stopping for training base class detector is important for iFSD as otherwise the features will be overfit to known classes and generalise less to novel classes.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Customizing object detectors for indoor robots</title>
		<author>
			<persName><forename type="first">Saif</forename><surname>Alabachi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gita</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Zero-shot object detection</title>
		<author>
			<persName><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Sikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Divakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition-by-components: A theory of human image understanding</title>
		<author>
			<persName><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="115" to="147" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image deformation meta-networks for one-shot learning</title>
		<author>
			<persName><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spot and learn: A maximum-entropy patch sampler for few-shot image classification</title>
		<author>
			<persName><forename type="first">Wen-Hsuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Jhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Chiang</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Matthias</forename><surname>De Lange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Masana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.08383</idno>
		<title level="m">Continual learning: A comparative study on how to defy forgetting in classification tasks</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Diversity with cooperation: Ensemble methods for few-shot classification</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Dvornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting in connectionist networks</title>
		<author>
			<persName><forename type="first">French</forename><surname>Robert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="128" to="135" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 4, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Amit Aides, Rogerio Feris, Raja Giryes, and Alex M. Bronstein. Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale few-shot learning: Knowledge transfer with class hierarchy</title>
		<author>
			<persName><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Core50: a new dataset and benchmark for continuous object recognition</title>
		<author>
			<persName><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CORL</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On firstorder meta-learning algorithms</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2008">2018. 2, 6, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Incremental learning of object detectors using a visual shape alphabet</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Opelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">How the mind works. Penguin UK</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Pinker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zero-shot object detection: Learning to simultaneously recognize and localize novel concepts</title>
		<author>
			<persName><forename type="first">Shafin</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
				<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Few-shot learning with embedded class models and shot-free meta training</title>
		<author>
			<persName><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Bhotika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName><surname>Sylvestre-Alvise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolov3: An incremental improvement. arXiv</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">One-shot learning with memory-augmented neural networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Bernt Schiele. Meta-transfer learning for few-shot learning</title>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoyao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Philip Hs Torr</surname></persName>
		</author>
		<author>
			<persName><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning compositional representations for few-shot recognition</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
				<imprint>
			<date type="published" when="2005">2016. 2, 4, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
				<imprint>
			<date type="published" when="2006">2018. 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
				<imprint>
			<date type="published" when="2008">2019. 1, 2, 4, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Deep sets</title>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Fewshot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2007">2019. 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Zero shot detection</title>
		<author>
			<persName><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
