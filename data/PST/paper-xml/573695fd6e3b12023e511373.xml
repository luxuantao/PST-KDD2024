<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2015-06-16">16 Jun 2015</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Courant Institute of Mathematical Sciences New York University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
							<email>joan.bruna@berkeley.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Courant Institute of Mathematical Sciences New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Convolutional Networks on Graph-Structured Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2015-06-16">16 Jun 2015</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1506.05163v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T14:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning's recent successes have mostly relied on Convolutional Networks, which exploit fundamental statistical properties of images, sounds and video data: the local stationarity and multi-scale compositional structure, that allows expressing long range interactions in terms of shorter, localized interactions. However, there exist other important examples, such as text documents or bioinformatic data, that may lack some or all of these strong statistical regularities. In this paper we consider the general question of how to construct deep architectures with small learning complexity on general non-Euclidean domains, which are typically unknown and need to be estimated from the data. In particular, we develop an extension of Spectral Networks which incorporates a Graph Estimation procedure, that we test on large-scale classification problems, matching or improving over Dropout Networks with far less parameters to estimate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent times, Deep Learning models have proven extremely successful on a wide variety of tasks, from computer vision and acoustic modeling to natural language processing <ref type="bibr" target="#b8">[9]</ref>. At the core of their success lies an important assumption on the statistical properties of the data, namely the stationarity and the compositionality through local statistics, which are present in natural images, video, and speech. These properties are exploited efficiently by ConvNets <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, which are designed to extract local features that are shared across the signal domain. Thanks to this, they are able to greatly reduce the number of parameters in the network with respect to generic deep architectures, without sacrificing the capacity to extract informative statistics from the data. Similarly, Recurrent Neural Nets (RNNs) trained on temporal data implicitly assume a stationary distribution.</p><p>One can think of such data examples as being signals defined on a low-dimensional grid. In this case stationarity is well defined via the natural translation operator on the grid, locality is defined via the metric of the grid, and compositionality is obtained from downsampling, or equivalently thanks to the multi-resolution property of the grid. However, there exist many examples of data that lack the underlying low-dimensional grid structure. For example, text documents represented as bags of words can be thought of as signals defined on a graph whose nodes are vocabulary terms and whose weights represent some similarity measure between terms, such as co-occurence statistics. In medicine, a patient's gene expression data can be viewed as a signal defined on the graph imposed by the regulatory network. In fact, computer vision and audio, which are the main focus of research efforts in deep learning, only represent a special case of data defined on an extremely simple lowdimensional graph. Complex graphs arising in other domains might be of higher dimension, and the statistical properties of data defined on such graphs might not satisfy the stationarity, locality and compositionality assumptions previously described. For such type of data of dimension N , deep learning strategies are reduced to learning with fully-connected layers, which have O(N 2 ) parameters, and regularization is carried out via weight decay and dropout <ref type="bibr" target="#b16">[17]</ref>.</p><p>When the graph structure of the input is known, <ref type="bibr" target="#b1">[2]</ref> introduced a model to generalize ConvNets using low learning complexity similar to that of a ConvNet, and which was demonstrated on simple lowdimensional graphs. In this work, we are interested in generalizing ConvNets to high-dimensional, general datasets, and, most importantly, to the setting where the graph structure is not known a priori. In this context, learning the graph structure amounts to estimating the similarity matrix, which has complexity O(N 2 ). One may therefore wonder whether the graph estimation followed by graph convolutions offers advantages with respect to learning directly from the data with fully connected layers. We attempt to answer this question experimentally and to establish baselines for future work.</p><p>We explore these approaches in two areas of application for which it has not been possible to apply convolutional networks before: text categorization and bioinformatics. Our results show that our method is capable of matching or outperforming large, fully-connected networks trained with dropout using fewer parameters. Our main contributions can be summarized as follows:</p><p>• We extend the ideas from <ref type="bibr" target="#b1">[2]</ref> to large-scale classification problems, specifically Imagenet Object Recognition, text categorization and bioinformatics. • We consider the most general setting where no prior information on the graph structure is available, and propose unsupervised and new supervised graph estimation strategies in combination with the supervised graph convolutions.</p><p>The rest of the paper is structured as follows. Section 2 reviews similar works in the literature. Section 3 discusses generalizations of convolutions on graphs, and Section 4 addresses the question of graph estimation. Finally, Section 5 shows numerical experiments on large scale object recogniton, text categorization and bioinformatics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There have been several works which have explored architectures using the so-called local receptive fields <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14]</ref>, mostly with applications to image recognition. In particular, <ref type="bibr" target="#b3">[4]</ref> proposes a scheme to learn how to group together features based upon a measure of similarity that is obtained in an unsupervised fashion. However, it does not attempt to exploit any weight-sharing strategy.</p><p>Recently, <ref type="bibr" target="#b1">[2]</ref> proposed a generalization of convolutions to graphs via the Graph Laplacian. By identifying a linear, translation-invariant operator in the grid (the Laplacian operator), with its counterpart in a general graph (the Graph Laplacian), one can view convolutions as the family of linear transforms commuting with the Laplacian. By combining this commutation property with a rule to find localized filters, the model requires only O(1) parameters per "feature map". However, this construction requires prior knowledge of the graph structure, and was shown only on simple, low-dimensional graphs. More recently, <ref type="bibr" target="#b11">[12]</ref> introduced Shapenet, another generalization of convolutions on non-Euclidean domains based on geodesic polar coordinates, which was successfully applied to shape analysis, and allows comparison across different manifolds. However, it also requires prior knowledge of the manifolds.</p><p>The graph or similarity estimation aspects have also been extensively studied in the past. For instance, <ref type="bibr" target="#b14">[15]</ref> studies the estimation of the graph from a statistical point of view, through the identification of a certain graphical model using 1 -penalized logistic regression. Also, <ref type="bibr" target="#b2">[3]</ref> considers the problem of learning a deep architecture through a series of Haar contractions, which are learnt using an unsupervised pairing criteria over the features.</p><p>3 Generalizing Convolutions to Graphs</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spectral Networks</head><p>Our work builds upon <ref type="bibr" target="#b1">[2]</ref> which introduced spectral networks. We recall the definition here and its main properties. A spectral network generalizes a convolutional network through the Graph Fourier Transform, which is in turn defined via a generalization of the Laplacian operator on the grid to the graph Laplacian. An input vector x ∈ R N is seen as a a signal defined on a graph G with N nodes. Definition 1. Let W be a N × N similarity matrix representing an undirected graph G, and let</p><formula xml:id="formula_0">L = I − D −1/2 W D −1/2 be its graph Laplacian with D = W • 1 eigenvectors U = (u 1 , . . . , u N ).</formula><p>Then a graph convolution of input signals x with filters g on G is defined by x * G g = U T (U x U g), where represents a point-wise product.</p><p>Here, the unitary matrix U plays the role of the Fourier Transform in R d . There are several ways of computing the graph Laplacian L <ref type="bibr" target="#b0">[1]</ref>. In this paper, we choose the normalized version L =</p><formula xml:id="formula_1">I −D −1/2 W D −1/2</formula><p>, where D is a diagonal matrix with entries D ii = j W ij . Note that in the case where W represents the lattice, from the definition of L we recover the discrete Laplacian operator ∆. Also note that the Laplacian commutes with the translation operator, which is diagonalized in the Fourier basis. It follows that the eigenvectors of ∆ are given by the Discrete Fourier Transform (DFT) matrix. We then recover a classical convolution operator by noting that convolutions are by definition linear operators that diagonalize in the Fourier domain (also known as the Convolution Theorem <ref type="bibr" target="#b10">[11]</ref>).</p><p>Learning filters on a graph thus amounts to learning spectral multipliers w g = (w 1 , . . . , w N )</p><p>x</p><formula xml:id="formula_2">* G g := U T (diag(w g )U x) .</formula><p>Extending the convolution to inputs x with multiple input channels is straightforward. If x is a signal with M input channels and N locations, we apply the transformation U on each channel, and then use multipliers</p><formula xml:id="formula_3">w g = (w i,j ; i ≤ N , j ≤ M ).</formula><p>However, for each feature map g we need convolutional kernels are typically restricted to have small spatial support, independent of the number of input pixels N , which enables the model to learn a number of parameters independent of N . In order to recover a similar learning complexity in the spectral domain, it is thus necessary to restrict the class of spectral multipliers to those corresponding to localized filters.</p><p>For that purpose, we seek to express spatial localization of filters in terms of their spectral multipliers. In the grid, smoothness in the frequency domain corresponds to the spatial decay, since</p><formula xml:id="formula_4">∂ k x(ξ) ∂ξ k ≤ C |u| k |x(u)|du ,</formula><p>where x(ξ) is the Fourier transform of x. In <ref type="bibr" target="#b1">[2]</ref> it was suggested to use the same principle in a general graph, by considering a smoothing kernel K ∈ R N ×N0 , such as splines, and searching for spectral multipliers of the form w g = K wg .</p><p>The algorithm which implements the graph convolution is described in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Train Graph Convolution Layer</head><p>1: Given GFT matrix U , interpolation kernel K, weights w.</p><p>2: Forward Pass: 3: Fetch input batch x and gradients w.r.t outputs ∇y. 4: Compute interpolated weights:</p><formula xml:id="formula_5">w f f = K w f f . 5: Compute output: y sf = U T f U x sf w f f . 6: Backward Pass: 7: Compute gradient w.r.t input: ∇x sf = U T f ∇y sf w f f 8: Compute gradient w.r.t interpolated weights: ∇w f f = U T ( s ∇y sf x sf ) 9: Compute gradient w.r.t weights ∇ w f f = K T ∇w f f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling with Hierarchical Graph Clustering</head><p>In image and speech applications, and in order to reduce the complexity of the model, it is often useful to trade off spatial resolution for feature resolution as the representation becomes deeper.</p><p>For that purpose, pooling layers compute statistics in local neighborhoods, such as the average amplitude, energy or maximum activation.</p><p>The same layers can be defined in a graph by providing the equivalent notion of neighborhood.</p><p>In this work, we construct such neighborhoods at different scales using multi-resolution spectral clustering <ref type="bibr" target="#b19">[20]</ref>, and consider both average and max-pooling as in standard convolutional network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Graph Construction</head><p>Whereas some recognition tasks in non-Euclidean domains, such as those considered in <ref type="bibr" target="#b1">[2]</ref> or <ref type="bibr" target="#b11">[12]</ref>, might have a prior knowledge of the graph structure of the input data, many other real-world applications do not have such knowledge. It is thus necessary to estimate a similarity matrix W from the data before constructing the spectral network. In this paper we consider two possible graph constructions, one unsupervised by measuring joint feature statistics, and another one supervised using an initial network as a proxy for the estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Unsupervised Graph Estimation</head><p>Given data X ∈ R L×N , where L is the number of samples and N the number of features, the simplest approach to estimating a graph structure from the data is to consider a distance between features i and j given by</p><formula xml:id="formula_6">d(i, j) = X i − X j 2 ,</formula><p>where X i is the i-th column of X. While correlations are typically sufficient to reveal the intrinsic geometrical structure of images <ref type="bibr" target="#b15">[16]</ref>, the effects of higher-order statistics might be non-negligible in other contexts, especially in presence of sparsity. Indeed, in many situations the pairwise Euclidean distances might suffer from unnormalized measurements. Several strategies and variants exist to gain some robustness, for instance replacing the Euclidean distance by the Z-score (thus renormalizing each feature by its standard deviation), the "square-correlation" (computing the correlation of squares of previously whitened features), or the mutual information.</p><p>This distance is then used to build a Gaussian diffusion Kernel <ref type="bibr" target="#b0">[1]</ref> </p><formula xml:id="formula_7">ω(i, j) = exp − d(i,j) σ 2 .<label>(1)</label></formula><p>In our experiments, we also consider the variant of self-tuning diffusion kernel <ref type="bibr" target="#b20">[21]</ref> ω(i, j) = exp</p><formula xml:id="formula_8">− d(i,j) σ i σ j</formula><p>, where σ i is computed as the distance d(i, i k ) corresponding to the k-th nearest neighbor i k of feature i. This defines a kernel whose variance is locally adapted around each feature point, as opposed to (1) where the variance is shared.</p><p>The main advantage of ( <ref type="formula" target="#formula_7">1</ref>) is that it does not require labeled data. Therefore, it is possible to estimate the similarity using several datasets that share the same features, for example in text classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Graph Estimation</head><p>As discussed in the previous section, the notion of feature similarity is not well defined, as it depends on our choice of kernel and criteria. Therefore, in the context of supervised learning, the relevant statistics from the input signals might not correspond to our imposed similarity criteria. It may thus be interesting to ask for the feature similarity that best suits a particular classification task.</p><p>A particularly simple approach is to use a fully-connected network to determine the feature similarity. Given a training set with normalized<ref type="foot" target="#foot_0">1</ref> features X ∈ R L×N and labels y ∈ {1, . . . , C} L , we initially train a fully connected network φ with K layers of weights W 1 , . . . , W K , using standard ReLU activations and dropout. We then extract the first layer features</p><formula xml:id="formula_9">W 1 ∈ R N ×M1</formula><p>, where M 1 is the number of first-layer hidden features, and consider the distance</p><formula xml:id="formula_10">d sup (i, j) = W 1,i − W 1,j 2 ,<label>(2)</label></formula><p>that is then fed into the Gaussian kernel as in <ref type="bibr" target="#b0">(1)</ref>. The interpretation is that the supervised criterion will extract through W 1 a collection of linear measurements that best serve the classification task. Thus two features are similar if the network decides to use them similarly within these linear measurements.</p><p>This constructions can be seen as "distilling" the information learnt by a first network into a kernel. In the general case where no assumptions are made on the dimension of the graph, it amounts to extracting N 2 /2 parameters from the first learning stage (which typically involves a much larger number of parameters). If, moreover, we assume a low-dimensional graph structure of dimension m, then mN parameters are extracted by projecting the resulting kernel into its leading m directions.</p><p>Finally, observe that one could simply replace the eigen-basis U obtained by diagonalizing the graph Laplacian by an arbitrary unitary matrix, which is then optimized by back-propagation together with the rest of the parameters of the model. We do not report results on this strategy, although we point out that it has the same learning complexity as the Fully Connected network (requiring O(KN 2 ) parameters, where K is the number of layers and N is the input dimension).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In order to measure the performance of spectral networks on real-world data and to explore the effect of the graph estimation procedure, we conducted experiments on three datasets from text categorization, computational biology and computer vision. All experiments were done using the Torch machine learning environment with a custom CUDA backend.</p><p>We based the spectral network architecture on that of a classical convolutional network, namely by interleaving graph convolution, ReLU and graph pooling layers, and ending with one or more fully connected layers. As noted above, training a spectral network requires an O(N 2 ) matrix multiplication for each input and output feature map to perform the Graph Fourier Transform, compared to the efficient O(N logN ) Fast Fourier Transform used in classical ConvNets. We found that training the spectral networks with large numbers of feature maps to be very time-consuming and therefore chose to experiment mostly with architectures with fewer feature maps and smaller pool sizes. We found that performing pooling at the beginning of the network was especially important to reduce the dimensionality in the graph domain and mitigate the cost of the expensive Graph Fourier Transform operation.</p><p>In this section we adopt the following notation to descibe network architectures: GCk denotes a graph convolution layer with k feature maps, Pk denotes a graph pooling layer with stride k and pool size 2k, and FCk denotes a fully connected layer with k hidden units. In our results we also denote the number of free parameters in the network by P net and the number of free parameters when estimating the graph by P graph .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reuters</head><p>We used the Reuters dataset described in <ref type="bibr" target="#b17">[18]</ref>, which consists of training and test sets each containing 201,369 documents from 50 mutually exclusive classes. Each document is represented as a log-normalized bag of words for 2000 common non-stop words. As a baseline we used the fullyconnected network of <ref type="bibr" target="#b17">[18]</ref> with two hidden layers consisting of 2000 and 1000 hidden units regularized with dropout.</p><p>We chose hyperparameters by performing initial experiments on a validation set consisting of onetenth of the training data. Specifically, we set the number of subsampled weights to k = 60, learning rate to 0.01 and used max pooling rather than average pooling. We also found that using AdaGrad <ref type="bibr" target="#b4">[5]</ref> made training faster. All architectures were then trained using the same hyperparameters. Since the experiments were computationally expensive, we did not train all models until full convergence. This enabled us to explore more model architectures and obtain a clearer understanding of the effects of graph construction.  67.66 -Note that our architectures are designed so that they factor the first hidden layer of the fully connected network across feature maps and a subsampled graph, trading off resolution in the graph domain for resolution across feature maps. The number of inputs into the last fully connected layer is always the same as for the fully-connected network. The idea is to reduce the number of parameters in the first layer of the network while avoiding too much compression in the second layer. We note that as we increase the tradeoff between resolution in the graph domain and across features, there reaches a point where performance begins to suffer. This is especially pronounced for the unsupervised graph estimation strategies. When using the supervised method, the network is much more robust to the factorization of the first layer. Table <ref type="table" target="#tab_0">1</ref> compares the test accuracy of the fully connected network and the GC4-P4-FC1000 network. Figure <ref type="figure">5</ref>.2-left shows that the factorization of the lower layer has a beneficial regularizing effect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Merck Molecular Activity Challenge</head><p>The Merck Molecular Activity Challenge is a computational biology benchmark where the task is to predict activity levels for various molecules based on the distances in bonds between different atoms.</p><p>For our experiments we used the DPP4 dataset which has 8193 samples and 2796 features. We chose this dataset because it was one of the more challenging and was of relatively low dimensionality which made the spectral networks tractable. As a baseline architecture, we used the network of <ref type="bibr" target="#b9">[10]</ref> which has 4 hidden layers and is regularized using dropout and weight decay. We used the same hyperparameter settings and data normalization recommended in the paper.</p><p>As before, we used one-tenth of the training set to tune hyperparameters of the network. For this task we found that k = 40 subsampled weights worked best, and that average pooling performed better than max pooling. Since the task is to predict a continuous variable, all networks were trained by minimizing the Root Mean-Squared Error loss. Following <ref type="bibr" target="#b9">[10]</ref>, we measured performance by computing the squared correlation between predictions and targets.   GC16-P4-GC16-P4-FC1000-FC1000 3.8 • 10 6 3.9 • 10 6 0.2773 Supervised GC64-P8-GC64-P8-FC1000-FC1000 3.8 • 10 6 3.9 • 10 6 0.2580 RBF Kernel GC64-P8-GC64-P8-FC1000-FC1000 3.8 • 10 6 3.9 • 10 6 0.2037 RBF Kernel (local) GC64-P8-GC64-P8-FC1000-FC1000 3.8 • 10 6 3.9 • 10 6 0.1479</p><p>We again designed our architectures to factor the first two hidden layers of the fully-connected network across feature maps and a subsampled graph, and left the second two layers unchanged. As before, we see that the unsupervised graph estimation strategies yield a significant drop in performance whereas the supervised strategy enables our network to perform similarly to the fully-connected network with much fewer parameters. This indicates that it is able to factor the lower-level representations in such a way as to retain useful information for the classification task. Figure <ref type="figure">5</ref>.2-right shows the test performance as the models are being trained. We note that the Merck datasets have test set samples assayed at a different time than the samples in the training set, and thus the distribution of features is typically different between the training and test sets. Therefore the test performance can be a significantly noisy function of the train performance. However, the effect of the different graph estimation procedures is still clear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">ImageNet</head><p>In the experiments above our graph construction relied on estimation from the data. To measure the influence of the graph construction compared to the filter learning in the graph frequency domain, we performed the same experiments on the ImageNet dataset for which the graph is already known, namely it is the 2-D grid. The spectral network was thus a convolutional network whose weights were defined in the frequency domain using frequency smoothing rather than imposing compactly supported filters. Training was performed exactly as in Figure <ref type="figure" target="#fig_0">1</ref>, except that the linear transformation was a Fast Fourier Transform.</p><p>Our network consisted of 4 convolution/ReLU/max pooling layers with 48, 128, 256 and 256 feature maps, followed by 3 fully-connected layers each with 4096 hidden units regularized with dropout. We trained two versions of the network: one classical convolutional network and one as a spectral network where the weights were defined in the frequency domain only and were interpolated using a spline kernel. Both networks were trained for 40 epochs over the ImageNet dataset where input images were scaled down to 128 × 128 to accelerate training.   We see that both models yield nearly identical performance. Interstingly, the spectral network learns faster than the ConvNet during the first part of training, although both networks converge around the same time. This requires further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>ConvNet architectures base their appeal and success on their ability to produce highly informative local statistics using low learning complexity and avoiding expensive matrix multiplications. This motivated us to consider generalizations on high-dimensional, unstructured data.</p><p>When the statistical properties of the input satisfy both stationarity and composotionality, spectral networks have a learning complexity of the same order as Convnets. In the general setting where no prior knowledge of the input graph structure is known, our model requires estimating the similarities, a O(N 2 ) operation, but making the model deeper does not increase learning complexity as much as the general Fully Connected architectures. Moreover, in contexts where feature similarities can be estimated using unlabeled data (such as word representations), our model has less parameters to learn from labeled data. However, as our results demonstrate, their extension poses significant challenges:</p><p>• Although the learning complexity requires O(1) parameters per feature map, the evaluation, both forward and backward, requires a multiplication by the Graph Fourier Transform, which costs O(N 2 ) operations. This is a major difference with respect to traditional Con-vNets, which require only O(N ). Fourier implementations of Convnets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19]</ref> bring the complexity to O(N log N ) thanks again to the specific symmetries of the grid. An open question is whether one can find approximate eigenbasis of general Graph Laplacians using Givens' decompositions similar to those of the FFT.</p><p>• Our experiments show that when the input graph structure is not known a priori, graph estimation is the statistical bottleneck of the model, requiring O(N 2 ) for general graphs and O(M N ) for M -dimensional graphs. Supervised graph estimation performs significantly better than unsupervised graph estimation based on low-order moments. Furthermore, we have verified that the architecture is quite sensitive to graph estimation errors. In the supervised setting, this step can be viewed in terms of a Bootstrapping mechanism, where an initially unconstrained network is self-adjusted to become more localized and with weightsharing. • Finally, the statistical assumptions of stationarity and compositionality are not always verified. In those situations, the constraints imposed by the model risk to reduce its capacity for no reason. One possibility for addressing this issue is to insert Fully connected layers between the input and the spectral layers, such that data can be transformed into the appropriate statistical model. Another strategy, that is left for future work, is to relax the notion of weight sharing by introducing instead a commutation error W i L − LW i with the graph Laplacian, which puts a soft penalty on transformations that do not commute with the Laplacian, instead of imposing exact commutation as is the case in the spectral net.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Similarity graphs for the Reuters (top) and Merck DPP4 (bottom) datasets. Left plots correspond to global σ, right plots to local σ.</figDesc><graphic url="image-3.png" coords="6,208.86,236.85,97.57,73.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Evolution of Test accuracy. Left: Reuters dataset, Right: Merck dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: ConvNet vs. SpectralNet on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for Reuters dataset. Accuracy is shown at epochs 200 and 1500.</figDesc><table><row><cell>Graph</cell><cell>Architecture</cell><cell>P net</cell><cell cols="3">P graph Acc. (200) Acc. (1500)</cell></row><row><cell>-</cell><cell>FC2000-FC1000</cell><cell>6 • 10 6</cell><cell>0</cell><cell>70.18 2</cell><cell>70.18</cell></row><row><cell>Supervised</cell><cell>GC4-P4-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell>69.41</cell><cell>70.03</cell></row><row><cell>Supervised</cell><cell>GC8-P8-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell>69.15</cell><cell>-</cell></row><row><cell>Supervised low rank</cell><cell>GC4-P4-FC1000</cell><cell cols="2">2 • 10 6 5 • 10 5</cell><cell>69.25</cell><cell>-</cell></row><row><cell>Supervised low rank</cell><cell>GC8-P8-FC1000</cell><cell cols="2">2 • 10 6 5 • 10 5</cell><cell>68.35</cell><cell>-</cell></row><row><cell>Supervised</cell><cell cols="3">GC16-P4-GC16-P4-FC1000 2 • 10 6 2 • 10 6</cell><cell>69.04</cell><cell>-</cell></row><row><cell>Supervised</cell><cell cols="3">GC64-P8-GC64-P8-FC1000 2 • 10 6 2 • 10 6</cell><cell>69.09</cell><cell>-</cell></row><row><cell>RBF kernel</cell><cell>GC4-P4-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell>67.85</cell><cell>-</cell></row><row><cell>RBF kernel</cell><cell>GC8-P8-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell>66.95</cell><cell>-</cell></row><row><cell>RBF kernel</cell><cell cols="3">GC16-P4-GC16-P4-FC1000 2 • 10 6 2 • 10 6</cell><cell>67.16</cell><cell>-</cell></row><row><cell>RBF kernel</cell><cell cols="3">GC64-P8-GC64-P8-FC1000 2 • 10 6 2 • 10 6</cell><cell>67.42</cell><cell>-</cell></row><row><cell>RBF kernel (local)</cell><cell>GC4-P4-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell>68.56</cell><cell>-</cell></row><row><cell>RBF kernel (local)</cell><cell>GC8-P8-FC1000</cell><cell cols="2">2 • 10 6 2 • 10 6</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results for Merck DPP4 dataset.</figDesc><table><row><cell>Graph</cell><cell>Architecture</cell><cell>P net</cell><cell>P graph</cell><cell>R 2</cell></row><row><cell>-</cell><cell>FC4000-FC2000-FC1000-FC1000</cell><cell>22.1 • 10 6</cell><cell>0</cell><cell>0.2729</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ImageNet results</figDesc><table><row><cell>Graph</cell><cell>Architecture</cell><cell cols="2">Test Accuracy (Top 5) Test Accuracy (Top 1)</cell></row><row><cell cols="2">2-D Grid Convolutional Network</cell><cell>71.854</cell><cell>46.24</cell></row><row><cell>2-D Grid</cell><cell>Spectral Network</cell><cell>71.998</cell><cell>46.71</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">In our experiments we simply normalized each feature by its standard deviation, but one could also whiten completely the data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1">this is the maximum value before the fully connected starts overfitting</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps and spectral techniques for embedding and clustering</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
				<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="585" to="591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations</title>
				<meeting>the 2nd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised deep haar scattering on graphs</title>
		<author>
			<persName><forename type="first">Xiuyuan</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1709" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Selecting receptive fields in deep networks</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2528" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Emergence of complex-like cells in a temporal product network with local receptive fields</title>
		<author>
			<persName><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1006.0448</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel</forename><surname>Rahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05">05 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks as a method for quantitative structure-activity relationships</title>
		<author>
			<persName><forename type="first">Junshui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Svetnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A wavelet tour of signal processing</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName><surname>Shapenet</surname></persName>
		</author>
		<idno>CoRR, abs/1501.06297</idno>
		<title level="m">Convolutional neural networks on non-euclidean manifolds</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fast training of convolutional networks through ffts</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1279" to="1287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-dimensional ising model selection using 1 -regularized logistic regression</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1287" to="1319" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning the 2-d topology of images</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Nicolas L Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balázs</forename><surname>Joliveau</surname></persName>
		</author>
		<author>
			<persName><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
				<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="841" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fast convolutional nets with fbfft: A GPU performance evaluation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vasilache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno>CoRR, abs/1412.7580</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName><forename type="first">Lihi</forename><surname>Zelnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Manor</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
				<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
