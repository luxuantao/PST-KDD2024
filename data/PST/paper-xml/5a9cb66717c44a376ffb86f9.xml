<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DEEP LEARNING FOR JOINT SOURCE-CHANNEL CODING OF TEXT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nariman</forename><surname>Farsad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milind</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Goldsmith</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<settlement>Stanford</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DEEP LEARNING FOR JOINT SOURCE-CHANNEL CODING OF TEXT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C97A23C4EFE380E5DF98DA5DBC999AD2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>deep learning</term>
					<term>natural language processing</term>
					<term>Joint source-channel coding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of joint source and channel coding of structured data such as natural language over a noisy channel. The typical approach to this problem in both theory and practice involves performing source coding to first compress the text and then channel coding to add robustness for the transmission across the channel. This approach is optimal in terms of minimizing end-to-end distortion with arbitrarily large block lengths of both the source and channel codes when transmission is over discrete memoryless channels. However, the optimality of this approach is no longer ensured for documents of finite length and limitations on the length of the encoding. We will show in this scenario that we can achieve lower word error rates by developing a deep learning based encoder and decoder. While the approach of separate source and channel coding would minimize bit error rates, our approach preserves semantic information of sentences by first embedding sentences in a semantic space where sentences closer in meaning are located closer together, and then performing joint source and channel coding on these embeddings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In digital communications, data transmission typically entails source coding and channel coding. In source coding the data is mapped to a sequence of symbols where the sequence length is optimized. In channel coding redundant symbols are systematically added to this sequence to detect or correct at the receiver the errors that may be introduced during data transfer. One of the consequences of the source-channel coding theorem by Shannon <ref type="bibr" target="#b1">[1]</ref> is that source and channel codes can be designed separately, with no loss in optimality, for memoryless and ergodic channels when infinite block length codes are used. This is known as the separation theorem, and can be extended to a larger class of channels <ref type="bibr" target="#b2">[2]</ref>.</p><p>Optimality of separation in Shannon's theorem assumes no constraint on the complexity of the source and channel code design. However, in practice, having very large block lengths may not be possible due to complexity and delay constraints. Therefore, many communication systems may benefit from designing the source/channel codes jointly. Some examples demonstrating this benefit include: wireless channels <ref type="bibr" target="#b3">[3]</ref>, video transmission over noisy channels <ref type="bibr" target="#b4">[4]</ref>, and image transmission over noisy channels <ref type="bibr" target="#b5">[5,</ref><ref type="bibr">6]</ref>.</p><p>In this work, we consider design of joint source-channel coding for text data with constrained code lengths. Particularly, our ultimate goal is to design a messaging service where sentences are transmitted over an erasure channel. The erasure channel is used here since it can model a broad class of channels where errors are detected but not corrected. One example is timing channels, where information is encoded on the time of release of packets <ref type="bibr" target="#b7">[7]</ref>. Our proposed coding technique can be used in this channel to create a covert messaging service over packet-switched networks <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>. In our ⇤ The authors contributed equally. This work was funded by the TI Stanford Graduate Fellowship, NSF under CPS Synergy grant 1330081, and NSF Center for Science of Information grant NSF-CCF-0939370. messaging service, instead of recovering the exact sentence at the receiver, we are interested in recovering the semantic information such as facts or imperatives of the sentence. Therefore, any sentence that conveys the information in the originally transmitted sentence would be considered as an error free output by the decoder even if it differed from the exact sentence. For example, the phrase "the car stopped" and "the automobile stopped" convey the same information.</p><p>One of the first works that considered joint source-channel coding using neural networks is <ref type="bibr" target="#b12">[12]</ref>, where simple neural network architectures were used as encoder and decoder for Gauss-Markov sources over additive white Gaussian noise channel. There are also a number of works that use neural networks for compression without a noisy channel (i.e., only source coding). In particular, in <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b14">14]</ref> image compression algorithms are developed using RNNs, which outperformed other image compression techniques. Sentence and document encoding is proposed in <ref type="bibr" target="#b15">[15]</ref> using neural autoencoders.</p><p>Contributions: Inspired by the recent success of deep learning in natural language processing for tasks such as machine translation <ref type="bibr" target="#b16">[16]</ref>, we develop a neural network architecture for joint sourcechannel coding of text. Our model uses a recurrent neural network (RNN) encoder, a binarization layer, the channel layer, and a decoder based on RNNs. We demonstrate that using this architecture, it is possible to train a joint source-channel encoder and decoder, where the decoder may output a different sentence that preserves its semantic information content.</p><p>We compare the performance of our deep learning encoder and decoder with a separate source and channel coding design 1 . Since the channel considered here is the erasure channel, we use Reed-Solomon codes for channel coding. For source coding, we consider three different techniques: a universal source coding scheme, a Huffman coding, and 5-bit character encoding. We demonstrate that the proposed deep learning encoder and decoder outperform the traditional approach in term of word error rate (WER), when the bit budget per sentence encoding is low. Moreover, in many cases, although some words may be replaced, dropped, or added to the sentence by the deep learning decoder, the semantic information in the sentence is preserved in a qualitative sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM DESCRIPTION</head><p>In this section, we define our system model associated with transmitting sentences from a transmitter to a receiver using limited number of bits.</p><p>Let V be the set of all the words in the vocabulary and let s = [w1, w2, • • • , wm] be the sentence to be transmitted where wi 2 V is the i th word in the sentence. The transmitter converts the sentence into a sequence of bits prior to transmission using source and channel coding. Let b = ' `(s) be a binary vector of length-`, where ' ìs the function representing the combined effect of the source and channel encoder. Let o be the vector of observations at the receiver corresponding to each of the `-bits in the transmission. Note that o does not necessarily need to be a binary vector, and it could be a vector of real or natural numbers depending on the channel considered. Let the combined effect of the source and channel decoder function be given by ⌫</p><formula xml:id="formula_0">`(o). Then ŝ = [ ŵ1, ŵ2, • • • , ŵm 0 ] = ⌫ `(o),</formula><p>1 To the best of our knowledge there are no known joint source-channel coding schemes for text data over erasure channels. where ŝ is the recovered sentence. The traditional approach to designing the source and channel coding schemes is to minimize the word error rate while also minimizing the number of transmission bits. However, jointly optimizing the source coding and the channel coding schemes is a difficult problem and therefore, in practice, they are treated separately.</p><p>The problem considered in this work is designing a joint sourcechannel coding scheme that preserves the meaning between the transmitted sentence s and the recovered sentence ŝ, while the two sentences may have different words and different lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DEEP LEARNING ALGORITHM</head><p>Our work is motivated by the recent success of the sequence-tosequence learning framework <ref type="bibr" target="#b17">[17]</ref> in different tasks such as machine translation <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b18">18]</ref>. Our system, which is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, has three components: the encoder, the channel, and the decoder. The encoder takes as an input a sentence s, concatenated with the special end of sentence word &lt;eos&gt;, and outputs a bit vector b of length `. The channel takes an input bit vector b and produces an output vector o. The effects of this module is random. The channel output o is the input to the decoder, and the output of the decoder is the estimated sentence ŝ. We now describe each of these modules in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Encoder</head><p>The first step in the encoder uses an embedding vector to represent each word in the vocabulary. In this work, we initialize our embedding vectors using Glove <ref type="bibr" target="#b19">[19]</ref>. Let E = [e1, e2, • • • , em, eeos] be the m + 1 embeddings of words in the sentence. In the second step, the embedded words are the inputs to a stacked bidirectional long short term memory (BLSTM) network <ref type="bibr" target="#b20">[20]</ref>. The LSTM cell used in this work is similar to that used in <ref type="bibr" target="#b21">[21]</ref>. The j th BLSTM stack is represented by Cj, Hj = BLSTMj(Hj 1),</p><p>where Cj is the cell state matrix and Hj is output matrix. Each column of Cj and Hj represents the cell state vector at each time step, and H0 = E. Fig. <ref type="figure" target="#fig_0">1</ref> shows an encoder with two stacked BLSTM layers.</p><p>Let k be the total numbers of BLSTM stacks. We concatenate the outputs at the last step and similarly the cell states at the last step of each layer using</p><formula xml:id="formula_2">h = H1[m + 1] H2[m + 1] • • • H k [m + 1],<label>(2)</label></formula><formula xml:id="formula_3">c = C1[m + 1] C2[m + 1] • • • C k [m + 1],<label>(3)</label></formula><p>where is the concatenation operator, and Hj[m+1] and Cj[m+1] are the m + 1 column (i.e., the last step) of, respectively, the outputs and cell states of the j th stack.</p><p>To convert h and c into binary vectors of length `/2 we use the same technique as in <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b13">13]</ref>. The first step in this process uses two fully connected layers</p><formula xml:id="formula_4">h ⇤ = tanh(W h h + a h ), (4) c ⇤ = tanh(Wcc + ac),<label>(5)</label></formula><p>where W h and Wc are weight matrices each with `/2 rows, and a h and ac are the bias vectors. Note that although here we use one fully connected layer, it would be possible to use multiple layers where the size of h and c is increased or decreased to `/2 in multiple steps. However, the last layer's activation function must alway be a tanh, to keep the output value in the interval <ref type="bibr" target="#b1">[ 1,</ref><ref type="bibr" target="#b1">1]</ref>.</p><p>The second step maps the the values in h ⇤ and c ⇤ from the interval <ref type="bibr" target="#b1">[ 1,</ref><ref type="bibr" target="#b1">1]</ref> to binary values { 1, 1}. Define a stochastic binarization function as</p><formula xml:id="formula_5">(x) = x + Zx,<label>(6)</label></formula><p>where Zx 2 {1 x, x 1} is a random variable distributed according to</p><formula xml:id="formula_6">P (Zx = 1 x) = 1+x 2 and P (Zx = x 1) = 1 x 2 . Then final binarization step during training is b = (h ⇤ ) (c ⇤ )<label>(7)</label></formula><p>for the forward pass. During the back-propagation step of the training, the derivative with respect to the expectation E[ (x)] = x is used <ref type="bibr" target="#b24">[24]</ref>. Therefore, the gradients pass through the function unchanged.</p><p>After training the network using , during deployment or testing the stochastic function (x), is replaced with the deterministic function 2u(x) 1, where u(x) is the unit step function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Channel</head><p>To allow for end-to-end training of the encoder and the decoder, the channel must allow for back-propagation. Fortunately, some communication channels can be formulated using neural network layers. This includes the additive Gaussian noise channel, multiplicative Gaussian noise channel and the erasure channel. In this work, we consider the erasure channel as it could model packets of data being dropped in a packet switched networks, or wireless channels with deep fades or burst errors.</p><p>The erasure channel can be represented by a dropout layer <ref type="bibr" target="#b25">[25]</ref>,</p><formula xml:id="formula_7">o = dropout(b, p d ),<label>(8)</label></formula><p>where o is the vector of observations at the receiver, and p d is the probability that a bit is dropped. The elements of o are in { 1, 0, 1}, where 0 indicates erasure (i.e., a dropped bit). Every bit in b may be dropped independent of other bits with probability p d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Decoder</head><p>At the receiver we use a stack of LSTMs for decoding. The observation vector o is input to the decoder. Let (x, v) be the inverse of the concatenation operator, where the vector x is broken into v vectors of equal length. Then we have</p><formula xml:id="formula_8">h 0 , c 0 = (o, 2),<label>(9)</label></formula><p>which contribute to the initial h (j) 0</p><p>state and c (j) 0 state of the j th LSTM stack. Particularly, these initial states are given by h</p><formula xml:id="formula_9">(j) 0 = tanh ⇣ W (j) h h 0 + a (j) h ⌘ ,<label>(10)</label></formula><formula xml:id="formula_10">c (j) 0 = W (j) c c 0 + a (j) c ,<label>(11)</label></formula><p>where W The first input to the LSTM stack is the embedding vector for a special start of the sentence symbol &lt;sos&gt;. Note that after the first word ŵ1 is estimated, its embedding vector will be used as the input for the next time step. To speed up the training, during the first few epochs, with probability 1 we use the correct word wi as the input for the i + 1 time step at the decoder; we gradually anneal the probability with which we replace the correct word wi with the estimated word ŵi. During deployment and testing we always use the estimated words and the beam search algorithm to find the most likely sequences of words <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b16">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head><p>In this section, we compare the deep learning approach with traditional information theoretic baselines for bit erasure channels. The source code used to generate the results is available on github<ref type="foot" target="#foot_0">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Dataset</head><p>We work with the proceedings of the European Parliament <ref type="bibr" target="#b27">[27]</ref>. This is a large parallel corpus that is frequently used in statistical machine translation. The English version has around 2.2 million sentences and 53 million words.</p><p>We crawl through the corpus to extract the most common words which we call our vocabulary. We pre-process the dataset by selecting sentences of lengths 4-30 where less then 20% of the words in the sentences are unknown words (i.e., they are outside of the selected vocabulary). The corpus is split into a training and test data set, where the training set has more that 1.2 million sentences and the test data set has more than 200 thousand sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Deep Learning Approach</head><p>We initialize 200-dimensional word embeddings using the Glove pre-trained embeddings <ref type="bibr" target="#b19">[19]</ref> for words in our vocabulary as well as a few special words (unknowns, padding, start and end symbols). We batch the sentences from the corpus based on their sentence lengths to increase efficiency of computation -i.e. sentences of similar length are fed in batches of size 128.</p><p>Two layered BLSTM of dimension 256 with peepholes are used for the encoder followed by a dense layer thats brings the dimension of the resultant state to the required bit budget. The decoder has two layers of LSTM cells each with the dimensions 512 with peephole connections. Note that one disadvantage of the deep learning approach is the use of a fixed number of bits for encoding all sentences of different lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Separate Source-Channel Coding Baselines</head><p>We implement separate source and channel coding which we know is optimal in the asymptote of arbitrarily large block lengths and delays. The source coding is done using three approaches:</p><p>1. Universal compressors: We use gzip which combines a Lempel-Ziv universal compression <ref type="bibr" target="#b28">[28]</ref> scheme with Huffman coding. This method works universally with all kinds of data and theoretically reaches the entropy limit of compression in the asymptote. However, since this technique does not work well for single sentences, we improve its performance by jointly compressing sentences in batches of size 32 or more. Note that this will give this technique an unfair advantage since it will no longer perform source coding on single sentences. 2. Huffman coding: To allow for single sentence source coding, we use Huffman coding on characters in the sentence. Using the training corpus, we compute character frequencies, which are then used to generated the Huffman codebook. 3. Fixed length character encoding: In this approach, we use a fixed 5-bit encoding for characters (the corpus is converted to lower case) and some special symbols. Decoding gzip and Huffman codes when there are errors or corruptions in the output of the channel decoder is not trivial. However, this baseline with 5-bit encoding can be decoded. After source encoding using the above approaches, we use a Reed-Solomon code <ref type="bibr" target="#b29">[29]</ref> that can correct up to the expected number of erasures. In the comparison, we assume the channel code can exactly compensate for erasures that occur. This assumption favors separate source-channel coding baselines as we can expect the number of bit erasures to be larger than the expected number with high probability. If this occurs, the channel decoding process will have errors and this may result in irredeemable corruption for decoding the source codes (gzip or huffman).</p><p>Finally, we compare performance by using a fixed bit budget per sentence. However, these schemes inherently produce embeddings of different lengths. If the encoding of a sentence exceeds the bit budget, we re-encode the sentence without its last word (resulting in a word error). We repeat the procedure until the encoding is within the bit limit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance</head><p>There is no better metric than a human judge to establish the similarity between sentences. As a proxy, we measure performance of the deep learning approach as well as the baselines using the edit distance or the Levenshtein distance. This metric is commonly used to measure the dissimilarity of two strings. It is computed using a recursive procedure that establishes the minimum number of word insertion, deletion, or substitution operations that would transform one sentence to another. The edit distance normalized by the length of the sentence is what we refer to as the word error rate. Word error rate is commonly used to evaluate performance in speech recognition and machine translation <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b31">31]</ref>. A downside of the metric is that it cannot capture the effect of synonyms or other aspects of semantic similarity.</p><p>In Fig. <ref type="figure" target="#fig_2">2a</ref>, we study the impact of the bit budget or the number of bits per sentence on the word error rate when we have a bit erasure probability of 0.05. Among the traditional baselines, gzip outperforms Huffman codes, and Huffman codes outperform the fixed length encoding. All three approaches result in no error if the bit allocation exceeds the number of bits required. This is because we assume the Reed-Solomon code compensates for all channel erasures. We observe that the deep learning approach is most competitive with limited bit allocations. As we enter the regime of excessive redundancy, the word error rate continually falls.</p><p>In Fig. <ref type="figure" target="#fig_2">2b</ref>, we look at the impact of the channel on word error rates when we have a bit allocation of 400 bits per sentence. Between the traditional baselines, we observe again that gzip is optimal as it operates on large batches followed by Huffman codes. 400 bits is not enough to completely encode sentences even when the channel is lossless. We make the observation again that in stressed environments (low bit allocations for large bit erasure rates), the deep learning approach outperforms the baselines.</p><p>What Fig. <ref type="figure" target="#fig_2">2a</ref> and Fig. <ref type="figure" target="#fig_2">2b</ref> hide is the impact of varying sentence lengths. If we consider a batch of sentences in random order from the corpus, we will have both large and short sentences. Traditional baselines can allot large encodings to long sentences and short encodings to others leading to an averaged bit allocation that may be short with few errors. However, the deep learning approach has the same bit allocation for sentences regardless of their length. We can improve the performance of the deep learning approach here by varying the length of the embedding based on the sentence length.</p><p>Fig. <ref type="figure" target="#fig_2">2c</ref> illustrates this very clearly. In this case, instead of having batches with sentences of different lengths, we use homogeneous batches to show the impact of the sentence lengths on word error rates (bit allocation 400, bit erasure rate 0.05). For short sentences, we are in the excess bit allocation regime. As the sentence length increases beyond 20, the deep learning approach significantly outperforms baselines. Another aspect to consider is that word errors of the deep learning approach may not be word errors -that may include substitutions of words using synonyms or rephrasing which does not change the meaning of the word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Properties of the encoding</head><p>The deep learning approach results in a lossy compression of text. It is able to do this by encoding a semantic embedding of the sentence. We can watch this in action in Fig. <ref type="figure" target="#fig_2">2d</ref>. Here, we compute the embeddings of a few sentences, groups of which are thematically linked. One group of sentences is about a girl saying something to a man, another is about a car driving and the last is about politicians voting. We then find the Hamming distance between the embeddings and use this dissimilarity matrix and multidimensional scaling approaches <ref type="bibr" target="#b32">[32]</ref> to view it in two dimensions. Sentences that express the same idea have embeddings that are close together (a) Word error as bits per sentence changes for 0.05 bit erasure probability.</p><p>(b) Word error as erasure or bit-drop rate increases for 400 bit encoding.</p><p>(c) Effect of sentences of different sizes with 400 bit encoding, 0.05 drop rate.</p><p>(d) Sample embeddings mapped to two dimensions using manifold dimensions with hamming distances between codes.  TX: there is one salient fact running through these data : the citizens want more information and have chosen television as the best means to receive that information . RX: there is one glaring weaknesses , by the communication : the citizens want more information and hold ' television as the means to receive this information . Long sentence 2 TX: i hope we will be able to provide part -funding for a renovation programme for energy efficiency as a result of this decision of the eu . RX: i hope we will be able to provide for funding for the renovation programme for energy efficiency as a result of decision by the eu .</p><p>Table <ref type="table">1</ref>: Sample sentences which were transmitted and received using the deep learning approach.</p><p>in Hamming distance. We do not see such behavior in information theoretic baselines which do not consider the fact that it is text with semantic information that they are encoding.</p><p>A few representative errors are shown in Table <ref type="table">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We considered the problem of joint source-channel coding of text data using deep learning techniques from natural language processing. For example, in many applications, recovery of the exact transmitted sentence may not be important as long as the main information within the sentence is conveyed. We demonstrated that our proposed joint source-channel coding scheme outperforms separate source and channel coding, especially in scenarios with a small number of bits to describe each sentence.</p><p>One drawback of the current algorithm is that it uses a fixed bit length to encode sentences of different length. As part of future work, we investigate how to resolve this issue. With severe bit restrictions per sentence, we will also look at deep learning based summarization to represent information. Joint source-channel coding of other forms of structured data such as images, audio, and video would also be a relevant future direction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The encoder-decoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Performance plots.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://github.com/milindmrao/nlp_comm</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>References</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The mathematical theory of communication</title>
		<author>
			<persName><forename type="first">Claude</forename><forename type="middle">E</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><surname>Weaver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>University of Illinois press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The source-channel separation theorem revisited</title>
		<author>
			<persName><forename type="first">Sridhar</forename><surname>Vembu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Verdu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yossef</forename><surname>Steinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="54" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint source/channel coding for wireless channels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Goldsmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Vehicular Technology Conference. Countdown to the Wireless Twenty-First Century</title>
		<imprint>
			<date type="published" when="1995-07">Jul 1995</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="614" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint source-channel coding for video communications</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiftach</forename><surname>Eisenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Image and Video Processing</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint source and channel coding for image transmission over lossy packet networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Danskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. Wavelet Applications to Digital Image Processing</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="376" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint source-channel coding for deep-space image transmission using rateless codes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ozgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><surname>Bursalioglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dariush</forename><surname>Caire</surname></persName>
		</author>
		<author>
			<persName><surname>Divsalar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3448" to="3461" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bits through queues</title>
		<author>
			<persName><forename type="first">Venkat</forename><surname>Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Verdu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="18" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Secure bits through queues</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Brian P Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bloch</surname></persName>
		</author>
		<author>
			<persName><surname>Nicholas Laneman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Information Theory Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="37" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A timing channel spyware for the csma/ca protocol</title>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mavis</forename><surname>Todd P Coleman</surname></persName>
		</author>
		<author>
			<persName><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="477" to="487" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Covert bits through queues</title>
		<author>
			<persName><forename type="first">Pritam</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sennur</forename><surname>Ulukus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Communications and Network Security (CNS</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A survey of timing channels and countermeasures</title>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Kumar Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipak</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shishir</forename><surname>Nagaraja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint source/channel coding modulation based on bp neural networks</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Rongwei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Wu Lenan</surname></persName>
		</author>
		<author>
			<persName><surname>Dongliang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Networks and Signal Processing</title>
		<meeting>the International Conference on Neural Networks and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="156" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variable rate image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Full resolution image compression with recurrent neural networks</title>
		<author>
			<persName><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Minnen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Covell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A hierarchical neural autoencoder for paragraphs and documents</title>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01057</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Has ¸im Sak</surname></persName>
		</author>
		<author>
			<persName><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><surname>Franc</surname></persName>
		</author>
		<author>
			<persName><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Binaryconnect: Training deep neural networks with binary weights during propagations</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Pierre</forename><surname>David</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3123" to="3131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Techniques for learning binary stochastic feedforward neural networks</title>
		<author>
			<persName><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sequence transduction with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Europarl: A parallel corpus for statistical machine translation</title>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MT summit</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A universal algorithm for sequential data compression</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Ziv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abraham</forename><surname>Lempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="343" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Polynomial codes over certain finite fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustave</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="304" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monolingual machine translation for paraphrase generation</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 conference on empirical methods in natural language processing</title>
		<meeting>the 2004 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sentence simplification by monolingual machine translation</title>
		<author>
			<persName><forename type="first">Antal</forename><surname>Sander Wubben</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiel</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><surname>Krahmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1015" to="1024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName><forename type="first">Ingwer</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Jf Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
