<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bounded Activation Functions for Enhanced Training Stability of Deep Neural Networks on Visual Pattern Recognition Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-08-13">August 13, 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shan</forename><forename type="middle">Sung</forename><surname>Liew</surname></persName>
							<email>ssliew2@live.utm.my</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">VeCAD Research Laboratory</orgName>
								<orgName type="institution">Universiti Teknologi Malaysia</orgName>
								<address>
									<postCode>81310</postCode>
									<settlement>Skudai, Johor</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Khalil-Hani</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Electrical Engineering</orgName>
								<orgName type="laboratory">VeCAD Research Laboratory</orgName>
								<orgName type="institution">Universiti Teknologi Malaysia</orgName>
								<address>
									<postCode>81310</postCode>
									<settlement>Skudai, Johor</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rabia</forename><surname>Bakhteri</surname></persName>
							<email>rbakhteri@sightlineinnovation.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Machine Learning Developer Group</orgName>
								<orgName type="institution">Sightline Innovation</orgName>
								<address>
									<addrLine>#202, 435 Ellice Ave</addrLine>
									<postCode>R3B 1Y6</postCode>
									<settlement>Winnipeg</settlement>
									<region>MB</region>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bounded Activation Functions for Enhanced Training Stability of Deep Neural Networks on Visual Pattern Recognition Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-08-13">August 13, 2016</date>
						</imprint>
					</monogr>
					<idno type="MD5">968DB74DD094694C9E34486372B745E0</idno>
					<idno type="DOI">10.1016/j.neucom.2016.08.037</idno>
					<note type="submission">Preprint submitted to Neurocomputing</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-27T05:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Activation function</term>
					<term>output boundary</term>
					<term>generalization performance</term>
					<term>training stability</term>
					<term>deep neural network</term>
					<term>convolutional neural network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on the enhancement of the generalization ability and training stability of deep neural networks (DNNs). New activation functions that we call bounded rectified linear unit (ReLU), bounded leaky ReLU, and bounded bi-firing are proposed. These activation functions are defined based on the desired properties of the universal approximation theorem (UAT). An additional work on providing a new set of coefficient values for the scaled hyperbolic tangent function is also presented. These works result in improved classification performances and training stability in DNNs. Experimental works using the multilayer perceptron (MLP) and convolutional neural network (CNN) models have shown that the proposed activation functions outperforms their respective original forms in regards to the classification accuracies and numerical stability. Tests on MNIST, mnist-rot-bg-img handwritten digit, and AR Purdue face databases show that significant improvements of 17.31%, 9.19%, and 74.99% can be achieved in terms of the testing misclassification error rates (MCRs), applying both mean squared error (MSE) and cross-entropy (CE) loss functions This is done without sacrificing the computational efficiency. With the MNIST dataset, bounding the output of an activation function results in a 78.58% reduction in numerical instability, and with the mnist-rot-bg-img and AR Purdue databases the problem is completely eliminated. Thus, this work has demonstrated the significance of bounding an activation function in helping to alleviate the training instability problem when training a DNN model (particularly CNN).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the introduction of artificial neural networks (ANNs) in the last few decades, many research works have been conducted to explore the capability of ANNs in various reasoning and decision based applications. Inspired by how a biological brain works, an ANN possesses the ability to learn from experience, enabling it to solve sophisticated problems that are deemed too difficult for conventional methods.</p><p>The expressiveness and approximation properties of a neural network (NN) often rely heavily on its structure. It is believed that deeper models are more powerful in approximating functions, and can capture more information due to their large learning capacities. This is motivated from the biological point of view, that in general our brains and visual systems are composed of multiple stages of information processing. This leads to the development of the deep neural network (DNN) model, which can provide better and deeper abstraction.</p><p>In general, there are three main features that characterize a DNN: its model or architecture (connection pattern between neurons), the corresponding learning algorithm (method to search for the optimum weights) and its activation function (transfer function that provides the nonlinearity behavior <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>). Different connection patterns can create different types of neuron layers that exhibit various characteristics. Examples of neuron layers include convolutional and pooling layers. A suitable learning algorithm is usually decided upon the DNN model based on how it can lead to a better convergence within the shortest possible time period.</p><p>However, the impact of an activation function on the generalization performance and training stability of a DNN is often ignored. There is a lack of consensus on how to select a good activation function for an NN model, and a specific one may not be suitable for all applications. This is especially true for problem domains where the numerical boundaries of the inputs and outputs are the main considerations.</p><p>In addition, the training process is heavily dependent on the choice of the activation function. As most supervised learning algorithms are based on the backward propagation of the error gradients, the tendency at which an activation function saturates during the back-propagation is one of the main concerns. Output saturation can result in poorer convergence, which is undesirable. Also, since an activation function is applied to the outputs of all neurons in most cases, its computational complexity will contribute heavily to the overall execution time.</p><p>Training stability of an NN model defines the magnitude of diversity detected during the training process <ref type="bibr" target="#b2">[3]</ref>. A good train-ing stability refers to the condition where the learning process moves consistently (with small dispersion) towards the convergence state. This requires proper design of the NN model and the choice of learning algorithm. The numerical stability during a training process is also crucial to avoid numerical underflow or overflow problem that can render the whole training useless <ref type="bibr" target="#b3">[4]</ref>.</p><p>Many previous works have been reported that stressed on the importance of numerical stability during an NN training, and various approaches have been proposed to alleviate the problem. In general, most approaches ensure the numerical stability by bounding the variables within a certain range. Typical examples include input normalization that scales the input data to a range that is suitable to be processed by the NN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and output normalization to eliminate the overflow errors <ref type="bibr" target="#b6">[7]</ref>. As for the weights, appropriate initialization methods <ref type="bibr" target="#b7">[8]</ref> or weight normalization <ref type="bibr" target="#b8">[9]</ref> are performed to prevent any potential accumulation overflow. Weight regularization techniques are also incorporated into loss functions to control the maximum amplitudes that the weights can have through the training process <ref type="bibr" target="#b9">[10]</ref>.</p><p>A typical learning algorithm for the NN model involves weight updates through gradient computations, and requires a loss function to calculate the errors. Many works have been reported on modifying the loss functions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> (especially the ones that involve exponential operations) to alleviate the numerical overflow problem by limiting the magnitude of the errors produced, which are then used to calculate the gradients through back-propagations. Some works have proposed to clip the gradients with respect to a bounded range <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> (to avoid the 'exploding' gradients <ref type="bibr" target="#b17">[18]</ref>) or limit the maximum update sizes prior to the weight updates. Careful selection of hyperparameters that are involved in a training process can affect the overall numerical stability as well <ref type="bibr" target="#b18">[19]</ref>. For instance, improper learning rate values (that are too large) can cause the training to diverge and overflow <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Most research works on the activation functions are focused on the complexity of the nonlinearity that an activation function can provide <ref type="bibr" target="#b0">[1]</ref>, how well it can propagate errors <ref type="bibr" target="#b9">[10]</ref>, or how fast it can be executed <ref type="bibr" target="#b22">[23]</ref>, but often neglect its impact on the overall training stability. The numerical stability during a training process is largely dependent on the input and output boundaries of the activation function as well as the numerical representation of the physical computing machine <ref type="bibr" target="#b23">[24]</ref>. Larger boundary values allow for more efficient propagation of neurons' values, but with higher risk of getting into the numerical overflow problem, which causes unstable outputs in a trained NN model. This should be taken into considerations as well when designing a suitable activation function for an NN model.</p><p>In this paper, we propose new activation functions for DNNs that are applied in visual pattern recognition problems. Convolutional neural network (CNN) has been chosen as our baseline DNN model, and the proposed activation functions are evaluated on a number of case studies that include classification of handwritten digits and face recognition problem. The contributions of this paper are as follows:</p><p>• New bounded activation functions for DNNs are proposed.</p><p>The functions, which include bounded ReLU, bounded leaky ReLU, and bounded bi-firing, improve the training stability of the corresponding unbounded activation functions while achieving better generalization performances; and</p><p>• Derivation of a set of coefficient values for the scaled hyperbolic tangent activation function that leads to a more efficient network convergence and higher classification accuracy.</p><p>The paper is organized as follows. Section 2 discusses on common activation functions that are applied in DNN models. Section 3 proposes new activation functions based on the desired characteristics of a good activation function. Section 4 describes the experimental design and methodology of performing the analysis. Section 5 analyzes the experimental results, and Section 6 concludes the results and suggests future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Activation functions</head><p>An activation function is a transfer function that transforms the net input of a neuron into an output signal. It determines the total strength (amplitude) that a neuron will produce and receive, thus affecting the overall behavior of an NN. In addition, it is a vital component that usually provides nonlinearity <ref type="bibr" target="#b1">[2]</ref>, which is important for NNs to possess the ability of approximating functions as defined by the universal approximation theorem (UAT) <ref type="bibr" target="#b24">[25]</ref>.</p><p>In general, there are several universal approximation properties (UAPs) to be fulfilled by an activation function based on the UAT <ref type="bibr" target="#b0">[1]</ref>:</p><p>1. The output is non-constant for all ranges of inputs; 2. Bounded within a range of values, where there exists a real number P such that | f (x)| P; 3. Continuous over all values of point c of its domain, where lim x→c f (x) = f (c); 4. Monotonically increasing, where x y and f (x) f (y); and 5. Differentiable everywhere, i.e. the slope of the tangent line of points from the left is approaching the same value as another slope of the tangent of points from the right.</p><p>The fifth property is not necessary for the existence of UAP, but is valid for learning algorithms that involve the backpropagation (BP) algorithm, since the error gradients are computed through partial differentiations. However, an activation is not necessary to be differentiable everywhere, as long as it is partially differentiable to be able to propagate the error gradients (as in many recent activation functions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>).</p><p>Realizing the importance of the activation function in ANNs, many functions have been proposed for various specific applications. Most of them are designed based on the UAT, while others partially fulfill the properties, yet still achieve outstanding learning performance <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26]</ref>. Typically, activation functions can be categorized into two basic families based on their shape of curves, i.e. sigmoidal or non-sigmoidal <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sigmoidal functions</head><p>A sigmoidal function refers to any mathematical function having an S-shaped curve (sigmoid curve). It is also known as squashing function, since the permissible range of an output value is usually squashed into a finite boundary. Sigmoidal functions are usually continuous and differentiable for all realvalued inputs, which makes them ideal for NNs, especially when trained using the BP algorithm.</p><p>The most common form of a sigmoidal function is logistic (logsig). A logistic function typically has an output range of [0, 1], and can be defined as in Eq. (1):</p><formula xml:id="formula_0">f (x) = 1 1 + e -x (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where x is the input. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates the logistic function. A hyperbolic tangent (tanh) function is a ratio between hyperbolic sine and cosine functions with respect to the input x:</p><formula xml:id="formula_2">f (x) = tanh (x) = sinh (x) cosh (x) = e x -e -x e x + e -x = 1 -e -2x 1 + e -2x<label>(2)</label></formula><p>In comparison to the logistic function, hyperbolic tangent function has a wider output range of [-1, 1], with its curve midpoint at the origin. As a result, it has more gradual gradients than the logistic function, which is more preferable in NN learning. Also, it has odd symmetricity about the origin, which may yield faster convergence <ref type="bibr" target="#b28">[29]</ref>. However, hyperbolic tangent has large saturation regions on both sides of the output boundary, as the output can hardly approach to the boundary values even when the input is large due to its mathematical representation (see Fig. <ref type="figure" target="#fig_1">2</ref>). Therefore, scaled hyperbolic tangent (stanh) has been proposed to alleviate this problem by manipulating its amplitude and slope <ref type="bibr" target="#b28">[29]</ref>:</p><formula xml:id="formula_3">f (x) = A tanh (Bx) (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where A represents the amplitude, and B denotes the slope. The authors in <ref type="bibr" target="#b28">[29]</ref> stated that the function is more suitable to be used with ground truth labels of 1 and -1 during the training process by setting A = 1.7159 and B = 2 3 . Also, the maximum absolute values of its second order derivative are set at +1 and -1, which can improve the overall convergence rate <ref type="bibr" target="#b28">[29]</ref>. However, the authors did mention that the particular choice of parameters is merely a convenience that does not affect the result. This work aims to determine the exact parameter values (coefficients) and validate if the difference between these values brings any impact on the learning performance or not. Many variants of the sigmoidal activation functions have been derived from the logistic and hyperbolic tangent functions by adding more nonlinearity to them <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. However, this often comes with an additional computational cost. Near-sigmoidal functions <ref type="bibr" target="#b35">[36]</ref> are designed based on approximations to the sigmoid curve. Common approaches include piecewise linear (PWL) approximation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref> and piecewise quadratic (PWQ) interpolation <ref type="bibr" target="#b22">[23]</ref>. PWL and PWQ are among the favorite choices of approximation for hardware implementations of activation functions due to their efficient resource consumption.</p><p>Despite their popularity and wide acceptance in ANNs, sigmoidal functions can suffer badly from gradient diffusion problem. Gradient diffusion, also known as vanishing gradient, is the training condition when the proportion of error gradients being propagated backward decreases over a number of layers <ref type="bibr" target="#b9">[10]</ref>. This occurs mainly due to the saturation problem of an activation function, where changes on the neurons' outputs can be hardly observed for any inputs near to or within a saturation region <ref type="bibr" target="#b37">[38]</ref>. In the case of a sigmoidal function, the saturation region lies on both ends of the sigmoid curve. The nearer an input is to the saturation region, the higher the tendency of its output to stay within the corresponding region. This can be problematic, since the smaller gradient is produced, and this gradient is continuously reduced when being propagated from layer to layer. Small gradients can hurt overall training performance, as most learning algorithms rely on these gradients to perform optimization and parameter tuning. Careful initialization of weights, especially in hidden layers is essential in order to prevent significant saturation problem during the early training stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Non-sigmoidal functions</head><p>Non-sigmoidal functions refer to any mathematical functions that produce outputs other than the S-shaped curves. Their de-signs are mainly motivated by the drawbacks of a sigmoidal function, and are often influenced by their targeted applications. A typical example is a linear function which can be represented by straight lines. A neuron with a standard linear function is essentially a neuron without any activation functions. A ramp function is a non-negative real-valued function, with zero output value for any inputs less than zero. This function has not been widely used in ANNs in the early years, until researchers discovered its true potential in large ANNs <ref type="bibr" target="#b25">[26]</ref>. Rectified linear unit (ReLU) was proposed based on the ramp function, which is defined as:</p><formula xml:id="formula_5">f (x) = max (0, x)<label>(4)</label></formula><p>There are arguments than the ReLU (relu) is more biologically plausible than the standard sigmoidal functions, since it is often rare to have the cortical neurons in their maximum saturation regime <ref type="bibr" target="#b25">[26]</ref>. This is often proven by the state-of-the-art performances of neural networks (especially CNNs) using ReLU as their activation functions <ref type="bibr" target="#b38">[39]</ref>.</p><p>In general, the ReLU function is compute efficient, since it requires only a simple comparison between two values. ReLU has a sparse activation probability that creates sparse representation of data, which is beneficial for classification purposes. Gradient backward propagation is simple and ReLU does not suffer from the gradient diffusion problem as much as sigmoidal functions do. It is differentiable at any point in the function domain except at the origin (piecewise differentiable).</p><p>Some modifications have been proposed to improve the conditioning of the ReLU function. For instance, ReLU produces only zero gradient for any negative input values (see Fig. <ref type="figure" target="#fig_3">3</ref>). Therefore, a leaky version of ReLU (lrelu) function has been proposed to allow a small, non-zero gradient value whenever the neuron is inactive <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_6">f (x) = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ x x&gt; 0 0.01x otherwise (5)</formula><p>Overall, all variants of the ReLU function are susceptible to training instability due to their unbounded outputs, which will be discussed in detail later in this section.  A new type of non-sigmoidal activation function, i.e. the bi-firing (bifire) function was proposed in <ref type="bibr" target="#b9">[10]</ref> to alleviate the gradient diffusion problem. It is basically an even symmetric and piecewise function as given in Eq. ( <ref type="formula">6</ref>):</p><formula xml:id="formula_7">f (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ -x -A 2 x &lt; -A x -A 2 x &gt; A x 2 2A otherwise (6)</formula><p>where A is a smoothing parameter that determines the convexshaped curve's sharpness when the input is approaching to zero. Fig. <ref type="figure" target="#fig_5">4</ref> depicts the bi-firing function and its derivative.  Bi-firing activation function improves the condition for efficient error propagation by having a very small saturation region near the origin. Its computational cost is also cheaper than most sigmoidal functions that involve exponential operations. However, the authors stated that it can cause numerical problems due to its unbounded nature, thus it requires a weight penalty mechanism to control the maximum output that it produces <ref type="bibr" target="#b9">[10]</ref>. This induces more computational complexity to the training process, especially when the NN size is larger and more data samples are involved. In addition, it still requires more computations than a ReLU activation function.</p><p>Many recent state-of-the-art works focused on building larger and deeper NN models using ReLU as the main activation function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref>. These systems do not emphasize on the importance of the bounded outputs, since many NN libraries automatically detect and stabilize common numerically unstable expressions due to the unbounded values (e.g. Theano <ref type="bibr" target="#b41">[42]</ref>, Caffe <ref type="bibr" target="#b42">[43]</ref>, and cuDNN <ref type="bibr" target="#b43">[44]</ref>). They also support high floating-point precision number representation that alleviates the numerical stability issue, thus this issue is not of the concern in these cases.</p><p>However, the output boundary issue is very important, especially in embedded system applications or dedicated hardware design where the number representation is limited, since it can bring significant impact on the computational complexity and resource consumption of the system. This explains why many previous works on the hardware design of NN models still choose common sigmoidal functions as their activation functions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Also, there are some previous works reported on the numerical instability issue due to the unbounded nature of the ReLU function <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed activation functions</head><p>The generalization performance and training stability of an NN model are heavily dependent on the choice of activation function. This work aims to improve the existing activation functions for DNN models, particularly CNNs. Several existing non-sigmoidal activation functions (i.e. ReLU, leaky ReLU and bi-firing) are chosen based on their reported performances in the previous works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>Realizing the importance of bounding the output of activation function for better training stability (as discussed in Section 1), a new set of activation functions are defined from these functions, with UAT as the underlying theory (especially the second property) to support the idea of having a bounded output range. This is done by alleviating the numerical instability problem while still retaining their strengths. In addition, this work also proposes a new set of coefficients for the scaled hyperbolic tangent function that matches the desired characteristics as discussed in <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Bounded ReLU activation function</head><p>The conventional ReLU function has unbounded outputs for non-negative inputs. According to the UAT, a function should be bounded within a range of inputs. Therefore, an upper boundary condition is added to Eq. ( <ref type="formula" target="#formula_5">4</ref>) to produce a bounded version of the ReLU function:</p><formula xml:id="formula_8">f (x) = min (max (0, x) , A) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0 x 0 x 0 &lt; x A A x &gt; A (7)</formula><p>where A defines the maximum output value the function can produce. This only requires an additional minimum operator, and has an additional interesting effect of creating a sigmoidallike activation pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bounded leaky ReLU activation function</head><p>A leaky ReLU function has non-zero gradient values for all negative input values (as in Eq. ( <ref type="formula">5</ref>)). Similar to the bounded ReLU function, another limiting condition is introduced as its output approaches to a positive value A. Instead of hardlimiting any output values greater than A to be equal to A, a slightly oblique line with a constant small positive gradient is added to create the odd symmetricity about the origin, and its gradient value is set to be identical to the line region where x 0:</p><formula xml:id="formula_9">f (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0.01x x 0 x 0 &lt; x A 0.01x + c x &gt; A (8)</formula><p>In order to create a continuous function over all regions, c is set as 0.99A by solving the equation x = 0.01x + c when x = A, which produces the following equation:</p><formula xml:id="formula_10">f (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ 0.01x x 0 x 0 &lt; x A 0.01x + 0.99A x &gt; A (9)</formula><p>The bounded leaky ReLU function has a similar sigmoidal-like activation pattern as the bounded ReLU function, except that small gradients exist at both sides of the region where an input is not within the region of [0, A]. Fig. <ref type="figure" target="#fig_6">5</ref> illustrates the activation and gradient curves of the proposed bounded ReLU (brelu) and bounded leaky ReLU (blrelu) functions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bounded bi-firing activation function</head><p>As aforementioned previously, a bi-firing function has unbounded outputs, thus requires some weight penalty techniques during the training process to reduce the risk of training instability. This work attempts to avoid the necessity of using any weight penalty techniques when training an NN model with the bi-firing activation function. This is accomplished by restricting the maximum activation value that the function can produce by adding hard limits on both sides using an upper boundary value B to preserve the even symmetricity of the function:</p><formula xml:id="formula_11">f (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ B x&lt; D L -x -A 2 D L x &lt; -A x 2 2A -A x A x -A 2 A &lt; x D U B x&gt; D U (<label>10</label></formula><formula xml:id="formula_12">)</formula><p>According to the UAT, an activation function should be continuous. Therefore, by solving both equations -x -A 2 = B when x = D L and x -A 2 = B when x = D U , the final equation is obtained as:</p><formula xml:id="formula_13">f (x) = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ B x&lt; -B -A 2 -x -A 2 -B -A 2 x &lt; -A x 2 2A -A x A x -A 2 A &lt; x B + A 2 B x&gt; B + A 2 (<label>11</label></formula><formula xml:id="formula_14">)</formula><p>The new bounded bi-firing (bbifire) function has a near inverse bell-shaped activation curve which is even symmetrical about the origin (as illustrated in Fig. <ref type="figure" target="#fig_8">6</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Propositions based on the UAT</head><p>The UAT provides a mathematical justification for the approximation power of an arbitrary function to emulate the exact representation of a given problem. The following propositions are established for these three new activation functions to evaluate their characteristics: Proposition 1. The bounded ReLU and bounded bi-firing functions have non-constant outputs for a range of inputs, while bounded leaky ReLU has non-constant outputs for all input values.  Bounded ReLU function: The first derivative of the function is defined as:</p><formula xml:id="formula_15">d f (x) dx = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 0 &lt; x A 0 otherwise (12)</formula><p>Bounded leaky ReLU function: Its first derivative is defined as:</p><formula xml:id="formula_16">d f (x) dx = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 0&lt; x A 0.01 otherwise (<label>13</label></formula><formula xml:id="formula_17">)</formula><p>Bounded bi-firing function: The first derivative of the function is:</p><formula xml:id="formula_18">d f (x) dx = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ -1 -B -A 2 x &lt; -A x 2 2A -A x A 1 A &lt; x B + A 2 0 otherwise (14)</formula><p>From the Propositions 1 to 5, it is noted that both bounded ReLU and bounded leaky ReLU functions are non-constant, limited, continuous, monotonically increasing and piecewise differentiable for all values of input x. The bounded bi-firing function has all the above characteristics as well, with an exception of being monotonically increasing only when x 0. It is important to note that the UAT is primarily used in this work to support the importance of having a bounded output range for an activation function rather than designing a function to satisfy every property in the UAT.</p><p>As aforementioned previously, gradient diffusion problem occurs mainly due to the large saturation region of an activation function, where smaller gradients are produced which lead to inefficient error propagation. However, the proposed bounded activation functions are designed to alleviate the problem by having constant gradient values within the active regions of the functions (as shown in Eq. ( <ref type="formula">12</ref>), <ref type="bibr" target="#b12">(13)</ref>, and ( <ref type="formula">14</ref>)). This ensures that the gradient values are not suppressed when back propagating through these activation functions. Also, as the active region is dependent on the choice of the hyperparameter value for the upper output boundary, the idea is that as long as the output boundary is large enough to alleviate the vanishing gradient problem, but at the same time small enough to avoid the numerical instability problem, then the NN training will produce faster and more stable convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Better coefficient values for the scaled hyperbolic tangent function</head><p>The coefficients for the scaled hyperbolic tangent function as proposed in <ref type="bibr" target="#b28">[29]</ref> were decided merely due to convenience, which they claimed would not bring any impact on the learning performance of an NN model. However, this work argues otherwise that small changes in the characteristics of the function can still be affecting the NN's training convergence.</p><p>Based on the scaled hyperbolic tangent activation function proposed in <ref type="bibr" target="#b28">[29]</ref>, the desired characteristics of a good hyperbolic tangent function are:</p><p>1. f (-1) = -1 and f (1) = 1; and 2. Both f (-1) and f (1) have maximum absolute values.</p><p>The first order derivative of the activation function (Eq. ( <ref type="formula" target="#formula_3">3</ref>)) is defined as:</p><formula xml:id="formula_19">d f (x) dx = AB 1 -(tanh (Bx)) 2<label>(15)</label></formula><p>Then its second order derivative is:</p><formula xml:id="formula_20">d 2 f (x) dx 2 = -2AB 2 tanh (Bx) -(tanh (Bx)) 3<label>(16)</label></formula><p>For simplicity, let p = d 2 f (x) dx 2 , then p is differentiated with respect to x to find a maximum or minimum value:</p><formula xml:id="formula_21">dp dx = -2AB 3 3 (tanh (Bx)) 4 -4 (tanh (Bx)) 2 + 1 (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>Since dp dx represents the gradient curve of function p, when the gradient of a point is zero (i.e. dp dx = 0), the function p has a local maximum or minimum at that point. Therefore, dp dx = 0. Let C = (tanh (Bx)) 2 . By substituting C into Eq. ( <ref type="formula" target="#formula_21">17</ref>) and setting dp dx = 0 gives:</p><formula xml:id="formula_23">3C 4 -4C 2 + 1 = 0<label>(18)</label></formula><p>By solving Eq. ( <ref type="formula" target="#formula_23">18</ref>),</p><formula xml:id="formula_24">C = 1 3 or C = 1 (invalid), i.e. tanh (Bx) = 1 3 or tanh (Bx) = -1 3 .</formula><p>The positive value for tanh (Bx) is chosen to find the maximum absolute value. By substituting the value of tanh (Bx) into Eq. (3) while setting x = 1 and f (x) = 1 (based on the rationale in the previous work <ref type="bibr" target="#b28">[29]</ref> that the output of the function becomes 1 when its input is 1, and -1 when the input is -1 as well, hence the ground truth label can be set to either 1 or -1 for simplicity purposes) gives: A = 1.7321 and B = 0.6585 <ref type="bibr" target="#b18">(19)</ref> Finally, the definition of a better scaled hyperbolic tangent function becomes:</p><formula xml:id="formula_25">f (x) = 1.7321 tanh (0.6585x)<label>(20)</label></formula><p>Fig. <ref type="figure" target="#fig_12">7</ref> illustrates the activation and gradient curves for the hyperbolic tangent function with different sets of coefficients A and B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental design</head><p>The activation functions proposed in this work are evaluated on several case studies that include:</p><p>1. Classification of basic handwritten digits using the MNIST database; 2. Classification of complex handwritten digits using the mnist-rot-bg-img database; and 3. Face recognition using the AR Purdue database.</p><p>All experiments are carried out using the identical MLP and CNN models for fair comparisons and better understanding on how well the proposed activation functions perform.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset preparation and partitioning</head><p>The dataset in the first case study comes from the Mixed National Institute of Standards and Technology (MNIST) database which consists of 70000 28 × 28 pixel 8-bit grayscale images of handwritten digits. The default data partitioning scheme is applied, which results in 60000 training and 10000 testing samples respectively. This dataset can be considered as a moderately large one. The only preprocessing techniques to be applied on the images is z-score normalization, which requires the computation of the mean pixel value of an input image:</p><formula xml:id="formula_26">X = R C x rc R × C (21)</formula><p>where x rc is the value of pixel (r, c) in the image, R and C are the height and width of the image, respectively. Then the standard deviation is:</p><formula xml:id="formula_27">σ X = R C x rc -X 2 R × C -1 (<label>22</label></formula><formula xml:id="formula_28">)</formula><p>and finally the normalized image pixel value is computed as:</p><formula xml:id="formula_29">x rc norm = x rc -X σ X (<label>23</label></formula><formula xml:id="formula_30">)</formula><p>These samples are flattened to single-dimensional vectors only in experiments involving the MLP models. Fig. <ref type="figure" target="#fig_13">8</ref> illustrates some examples of the MNIST handwritten digit images. This MNIST dataset is chosen as the primary dataset to be evaluated in this work, since it is often the standard dataset used for the benchmarking of new algorithms (NN models and learning algorithms). In addition, it contains tens of thousands of samples that allow for realistic benchmarking of the proposed activation functions as compared to small and simple databases.</p><p>In the second case study, the mnist-rot-bg-img database is used. It also comes from MNIST. It is created by selecting the original handwritten digit images randomly to create complex perturbations <ref type="bibr" target="#b52">[53]</ref>. The images are rotated randomly between 0 and 2π radians, and random image patches with high pixel variance are added to the digits as the backgrounds. This results in a total of 12000 training and 50000 testing samples respectively. The complex perturbations on the handwritten digit images makes them hard to be trained and classified correctly. Also, the smaller ratio of the total training to testing samples serves as a difficult challenge for the NN models to learn and generalize well on the unseen samples. The same z-score normalization is applied to all the samples. Fig. <ref type="figure" target="#fig_14">9</ref> shows some examples of the mnist-rot-bg-img images. This dataset constitutes a complex, moderately large classification problem. In the face recognition case study, we use the AR Purdue face database which consists of over 4000 RGB face images of 126 subjects. This work uses the cropped version of the AR Purdue face database that contains a total of 100 subjects with 26 165 × 120 color images per subject <ref type="bibr" target="#b53">[54]</ref>. There are variations in these face images, which include different facial expressions (neutral, simile, anger, scream), partial occlusions (with sunglasses or scarf), and taken under different illumination conditions (left light on, right light on, both sides lights on). Fig. <ref type="figure" target="#fig_15">10</ref> depicts the possible variations of the facial images from a single subject. This creates a challenging dataset which is ideal to evaluate the performance of the proposed activation functions in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Neural network models used in the experiments</head><p>NN models applied in our experimental work include a multilayer perceptron (MLP) and two convolutional neural network (labeled CNN1 and CNN2) models. The MLP is used for benchmarking with previous works, while the CNNs are used to evaluate the learning performance of the NN models with the proposed activation functions for the rest of the experiments.</p><p>The MLP model is implemented based on the work in <ref type="bibr" target="#b9">[10]</ref>. Table <ref type="table" target="#tab_0">1</ref> shows the details of the seven-layered MLP model, which is used for the benchmarking of the activation functions on the mnist-rot-bg-img dataset. The CNN1 model consists of seven layers in total, with alternating layers of convolutions and max-pooling in the feature extraction stage, followed by a fully-connected layer and a softmax layer for the classification purpose unless otherwise stated. Table <ref type="table" target="#tab_1">2</ref> shows the details of CNN1 model, which is depicted as in Fig. <ref type="figure" target="#fig_17">11</ref>. This model is applied in the experiments using the MNIST and mnist-rot-bg-img datasets. The CNN2 model is derived from the work reported for face verification using the AR Purdue dataset <ref type="bibr" target="#b54">[55]</ref>. It has the same total number of feature maps per layer as the CNN1 model (except for the fully-connected and softmax layers). The architectural details of the CNN2 model are tabulated in Table <ref type="table" target="#tab_2">3</ref>.</p><p>All convolutional layers perform convolutions with single step-sized correlation filtering operation <ref type="bibr" target="#b55">[56]</ref>. Meanwhile, the max-pooling layers perform subsampling by selecting the maximum values within a window as the outputs. A fully-connected   layer works similar to a hidden layer of an MLP. The outputs of all layers are passed through an activation function, except for the max-pooling and softmax layers. In terms of the free parameters, all convolutional layers have shared weights and biases, which result in fewer parameters <ref type="bibr" target="#b56">[57]</ref>. No weights or biases are included in the max-pooling layers for simpler and faster computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training methodology</head><p>In this work, a training procedure consists of 3 training repetitions for each experiment. An NN model is trained for 50 epochs for each repetition unless otherwise stated, to follow the settings of the previous work <ref type="bibr" target="#b9">[10]</ref> for the benchmarking purposes (i.e. 100 epochs). At the beginning of every training epoch, the training samples are shuffled to ensure that the NN model can learn robustly and produces better learning performance.</p><p>Every new training repetition starts with the weight initialization process. Normalized weight initialization is performed throughout all experiments to generate the initial weights <ref type="bibr" target="#b57">[58]</ref>. For the normalized weight initialization, it takes into account both a neuron's fan-in F (l-1) and fan-out F (l+1) , which can be expressed as in Eq. ( <ref type="formula" target="#formula_31">24</ref>):</p><formula xml:id="formula_31">W (l) ji ∼ U ⎡ ⎢ ⎢ ⎢ ⎢ ⎣ - √ 6 √ F (l-1) + F (l+1) , √ 6 √ F (l-1) + F (l+1) ⎤ ⎥ ⎥ ⎥ ⎥ ⎦<label>(24)</label></formula><p>This initialization method does not require any additional hyperparameter and each weight is initialized individually based on the model structure, which tend to produce better learning performance.</p><p>As for the learning algorithm, the L-SDLM algorithm <ref type="bibr" target="#b58">[59]</ref> is applied in all training procedures unless otherwise stated. L-SDLM is chosen as the default learning algorithm since it requires tuning of a single hyperparameter only, and it can converge faster than the conventional SGD algorithm. All variants of the SDLM algorithms require a small subset of the training set to be used for the Hessian estimation. In this work, 1% of the training samples are randomly selected for the Hessian estimation for all experiments.</p><p>A typical NN learning process requires a loss function that evaluates the fitness of the training outcome. In this paper, we ran all experiments using two different sets of loss functions, i.e. MSE and CE. The objective of a MSE loss function is to minimize the distance between actual and desired outputs as small as possible to achieve discrimination among two or more classes in a classification problem:</p><formula xml:id="formula_32">(E MS E ) m = 1 2N (L) j∈N (L) Y (L) j m -d j m 2 (<label>25</label></formula><formula xml:id="formula_33">)</formula><p>where</p><formula xml:id="formula_34">Y (L) j m</formula><p>is the value of j th neuron at output layer L for m th sample, d j m is the desired (target) value of j th output neuron for m th sample, and N (L) denotes the total output neurons.</p><p>Since the calculation of the MSE loss function requires specific target values for each sample which directly correspond to an activation function's output boundary values, this can be problematic for the unbounded activation functions. Hence, the upper output boundary P U of these functions is set empirically to some finite value to avoid any numerical overflow condition due to a very large value of P U , i.e. P U ∈ {4, 64}. This is, however exceptional for the stanh function, as we fix the lower and upper boundary values to be -1 and 1 respectively for all sets of coefficient values as recommended in <ref type="bibr" target="#b28">[29]</ref>.</p><p>The cross-entropy (CE) loss function, on the other hand, is applied together with the softmax layer to evaluate the fitness of the NN model <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b60">61]</ref>. A softmax layer evaluates the probability of an input sample to be categorized into one of the predefined classes:</p><formula xml:id="formula_35">p (L) j m = exp Y (l-1) j m i∈N (l-1) exp Y (l-1)</formula><p>i m <ref type="bibr" target="#b25">(26)</ref> where</p><formula xml:id="formula_36">Y (l-1) j m</formula><p>is the output of j th neuron in the preceding fullyconnected layer (l -1) for m th sample, N (l-1) are total neuron outputs in layer (l -1), and p (L) j m denotes the probability of j th class in the softmax layer for m th sample. On the other hand, the CE error is computed as</p><formula xml:id="formula_37">e (L) j m = p (L) j m -d j m (<label>27</label></formula><formula xml:id="formula_38">)</formula><p>where e (L) j m</p><p>is the error for the m th sample, and d j m is the ground truth label for the m th sample:</p><formula xml:id="formula_39">d j m = ⎧ ⎪ ⎪ ⎨ ⎪ ⎪ ⎩ 1 correct class f or m th sample 0 otherwise (<label>28</label></formula><formula xml:id="formula_40">)</formula><p>The CE loss function for the m th sample is defined as</p><formula xml:id="formula_41">(E CE ) m = - J d j m ln p (L) j m (<label>29</label></formula><formula xml:id="formula_42">)</formula><p>Training is performed to minimize the average loss value over all training samples:</p><formula xml:id="formula_43">E = M E m M (<label>30</label></formula><formula xml:id="formula_44">)</formula><p>where M are total samples in the training set, and E m can be an MSE or CE loss function. In this work, the softmax function is not applied when evaluating the training process using the MSE loss function. This work utilizes the winner-takes-all (WTA) mechanism <ref type="bibr" target="#b61">[62]</ref> that compares among all output values to perform NN classification (regardless of which loss function to be applied during the training process).</p><p>In terms of the global learning rate, we run the experiments using the following values: 0.001, 0.0025, 0.005, 0.0075 and 0.01. Table <ref type="table" target="#tab_3">4</ref> summarizes the hyperparameter values for some activation functions that are evaluated in this work. By referring to this table, the values of hyperparameters A for both brelu and blrelu functions are set heuristically to the powers of 2. As for the bifire and bbifire functions, the values of A are adopted from the previous work in <ref type="bibr" target="#b9">[10]</ref>, while the values of B are set heuristically to the powers of 4. Finally, the values of A and B for the stanh function are derived from the previous work in <ref type="bibr" target="#b28">[29]</ref>. In theory, the upper boundary of the proposed bounded activation functions can be set according to the desired range of the number representation for the NN model, especially in the hardware design where there is limited numerical precision due to the resource constraint <ref type="bibr" target="#b40">[41]</ref>. No weight regularization techniques are applied in all the experiments to exclude the effect of the weight penalty on the training outcomes <ref type="bibr" target="#b56">[57]</ref>, hence evaluating the sole effect of bounding an activation function on the generalization performance and training stability of the NN models. The best results are presented in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Implementation Details</head><p>In this work, all training procedures, learning algorithms and NN models are developed from scratch. All codes are written in C/C++. Native compilation is performed on Ubuntu 14.04 64-bit LTS OS using G++ compiler with the highest code optimization level (O3). Real-valued data are represented by the single-precision floating data type throughout the experiments. All program codes and experiments are executed on a general purpose computing platform with an overclocked 4.5 GHz Intel Core i7 4790K CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and discussions</head><p>In this section, the experimental analysis of the proposed activation functions is discussed according to the following aspects: (a) benchmarking with previous works, (b) training efficiency, (c) classification performance, and (d) training stability. The results are reported based on the lowest MCR values within the 50 training epochs (or 100 in the case of benchmarking with the previous work <ref type="bibr" target="#b9">[10]</ref>). As for some activation functions with hyperparameters (see Table <ref type="table" target="#tab_3">4</ref>), the best results are reported from all the experimental settings using different hyperparameter values for these functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmarking with previous works</head><p>The proposed activation functions are first benchmarked using the baseline experimental setup in <ref type="bibr" target="#b9">[10]</ref> that uses the MLP model (as described in Table <ref type="table" target="#tab_0">1</ref>). Their performances are evaluated in terms of the testing MCRs on the mnist-rot-bg-img dataset.</p><p>Table <ref type="table" target="#tab_4">5</ref> summarizes the testing MCRs of the proposed activation functions together with other functions as evaluated in <ref type="bibr" target="#b9">[10]</ref>. Both MLPs with the proposed bounded ReLU (brelu) and bounded leaky ReLU (blrelu) functions outperform the ones with the ReLU (relu) and leaky ReLU (lrelu) functions (i.e. 50.09% versus 51.98%, and 49.88% versus 50.52%, respectively).</p><p>The MLP with the proposed bounded bi-firing (bbifire) function achieves a slightly higher testing MCR than the bi-firing (bifire) function (48.97% versus 48.75%), but with a very small difference of 0.22% only. Tuning the amplitude and slope of the hyperbolic tangent (tanh) function to create the scaled hyperbolic tangent (stanh) function <ref type="bibr" target="#b28">[29]</ref> is indeed advantageous, as the MLP with the stanh function scores a much lower testing MCR (50.12%) compared to the original tanh function (53.17%). Moreover, applying the proposed coefficient values to the stanh function manages to achieve an even lower MCR of 49.89% only.</p><p>There is an important point to note though: the results obtained in this work are based on an uneven experimental setup as compared to the work in <ref type="bibr" target="#b9">[10]</ref>. The results published in <ref type="bibr" target="#b9">[10]</ref> were the best results obtained from multiple training attempts with different MLPs and hyperparameter values. Their training procedure incorporates the weight regularization techniques, and most probably takes 200 training epochs.</p><p>Meanwhile, the MLP model used in this work is fixed, and is trained for only 100 epochs with a fixed global learning rate of 0.005. No weight regularization techniques are applied during the whole training procedure. Still, MLPs with the proposed activation functions are able to perform comparably well or even better than other activation functions, which clearly demonstrates their superior performances in achieving better generalization ability of a DNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training efficiency</head><p>The proposed activation functions are evaluated with respect to their original forms in terms of the training efficiencies using both the MSE and CE loss functions. This is done by observing both the training losses and testing MCRs on the MNIST dataset.</p><p>Fig. <ref type="figure" target="#fig_18">12</ref>(a) shows the training MSE losses for both the relu and brelu functions on the MNIST dataset. The CNN with the brelu function has a lower initial MSE loss (0.0068 versus 0.1634) that steadily decreases and achieves lower MSE loss (0.0019) than the relu function (0.0033). It is able to achieve a higher testing accuracy than the relu function (99.06% versus 99.03%), which is depicted in Fig. <ref type="figure" target="#fig_18">12(b)</ref>. This is due to the better control of the outputs' distribution through the additional boundary condition, which encourages the network convergence.</p><p>Training a CNN with the brelu function using the CE loss function results in a slower decrease of the training loss compared to the relu function (Fig. <ref type="figure" target="#fig_18">12(c</ref>)). This is most probably that the CE loss function is better suited for unbounded activation functions. However, this does not affect its generalization performance on the testing set. In fact, it is able to reach far lower testing MCR than the relu function, i.e. 0.92% versus 1.02% (as depicted in Fig. <ref type="figure" target="#fig_18">12(d)</ref>). Learning better on the training set does not necessarily mean a better accuracy on the unseen samples prior to the training.</p><p>The results have shown a very good training convergence of a CNN with the proposed blrelu function than the lrelu function when trained using the MSE loss function (0.0001 versus 0.1553). This is clearly indicated by the two training loss curves that are clearly separated by a distance (Fig. <ref type="figure" target="#fig_19">13(a)</ref>). The CNN model with the blrelu function also produces more stable MCRs over training epochs (Fig. <ref type="figure" target="#fig_19">13(b</ref>)), and eventually achieves a testing MCR comparable to the lrelu function. (0.96% versus 0.95%).</p><p>A similar pattern is observed for the training outcome using the CE loss function, where the training loss of the CNN with the blrelu function decreases faster and reaches less than 0.0007 within 18 epochs (Fig. <ref type="figure" target="#fig_19">13(c</ref>)). It also produces a much smoother MCR curve than the model with the lrelu function, and is able to achieve only 0.96% testing MCR (Fig. <ref type="figure" target="#fig_19">13(d)</ref>) as compared to the lrelu function (1.06%).</p><p>Experiments on the bifire and the proposed bbifire activation functions have shown the clear advantage of having a safe output boundary condition for better training stability. The differences between the training MSE losses of these two functions can be hardly observed (Fig. <ref type="figure" target="#fig_20">14(a)</ref>). However, by referring to Fig. <ref type="figure" target="#fig_20">14(b)</ref>, the trained CNN model with the bbifire function produces a more stable MCR curve and achieves the testing MCR comparable to the bifire function, i.e. 0.90% and 0.89%, respectively.</p><p>The positive effects of the bounded outputs are shown clearly in Fig. <ref type="figure" target="#fig_20">14(c)</ref>, where the training CE loss of the CNN with the bbifire function decreases steadily as the training proceeds. In contrast, it decreases initially for the CNN with the bifire function, but starts to increase after 30 training epochs. This clearly indicates the training instability due to the unbounded nature of the bifire function. Another evidence is clearly observed in Fig. <ref type="figure" target="#fig_20">14(d)</ref>, where the MCR curve of the CNN with the bifire function continues to fluctuate over time as more training epochs have passed. On the contrary, training of the CNN with the proposed bifire function makes smoother progress that leads to a very low testing MCR of 0.86% on the MNIST dataset.</p><p>As for the hyperbolic tangent (tanh) function, setting its output boundary values as the target values when training using the MSE loss function gives the best training loss as illustrated in Fig. <ref type="figure" target="#fig_21">15(a)</ref>. However, no direct relationship is observed between the training loss and its corresponding testing MCR (Fig. <ref type="figure" target="#fig_21">15(b)</ref>), as the CNN with the tanh function using the proposed coefficients (stanh) is able to achieve a slightly lower testing MCR (0.99%) than the conventional tanh function (1.01%).</p><p>The effect of having different amplitudes and slopes for the tanh function is clearly depicted in Fig. <ref type="figure" target="#fig_21">15(c)</ref>, where the training on the CNNs using the stanh functions results in significantly lower training CE losses than the one with the tanh function. More importantly, the stanh function with the proposed coefficients outperforms the other two functions again by achieving 99.01% testing accuracy (Fig. <ref type="figure" target="#fig_21">15(d)</ref>). The experimental results have proven the validity of the desired properties of a hyperbolic tangent activation function as proposed in <ref type="bibr" target="#b28">[29]</ref>. Table <ref type="table" target="#tab_5">6</ref> summarizes the classification performances of the proposed activation functions on the MNIST dataset.</p><p>Based on this table, it is observed that most of the proposed activation functions reduce the average execution time per epoch under the MSE training criterion. Having a bounded activation function allows for the target outputs to be set at its output boundary values when training using the MSE loss function. This encourages the weights to be tuned such that the out-  Training using CE loss function, however, requires that a softmax layer to be added to the output layer of the NN model which increases the overall computation time. The softmax layer squashes the output values of the activation function into the range of [0, 1] regardless of its original output range (as shown in Eq. ( <ref type="formula">26</ref>)). However, the larger the output range of an activation function, the more possibility that the softmax output probability will be suppressed to zero due to the exponential operations. As aforementioned previously, more sparse representation of the data reduces the execution time, which explains why NN models with unbounded activation functions perform faster than the proposed bounded functions under the CE training criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Classification performance</head><p>The classification performances of the CNN models using various activation functions are also reported in terms of the training and testing MCRs on the mnist-rot-bg-img and AR Purdue datasets. These results indicate how an activation function will affect the generalization performance of a CNN model.</p><p>Table <ref type="table" target="#tab_6">7</ref> shows the training and testing MCRs of the CNN models with various activation functions on the mnist-rot-bgimg dataset. Overall, the CNN models benefit more from training using the softmax function and CE loss function. Regardless of which loss function to be used, all the proposed activation functions outperform their original forms in terms of the testing MCRs. Improvements up to 6.42% (brelu), 9.19% (blrelu), 7.71% (bbifire), and 3.45% (stanh) are achieved respectively, while requiring comparable execution time as compared to their original forms (with the proposed brelu being the fastest, i.e. 18.59s). Lowest testing MCRs are obtained by training the CNN models with the proposed bbifire and brelu functions, i.e. 23.09% and 22.59% when trained using the MSE and CE loss functions, respectively.</p><p>The results in Table <ref type="table" target="#tab_7">8</ref> further demonstrate the superiority of the proposed activation functions by achieving significant improvements in terms of the testing MCRs, i.e. 16.12% (brelu), 74.99% (blrelu), 9.00% (bbifire), and 44.72% (stanh) respectively. These improvements are obtained without sacrificing the average execution time, with even the proposed brelu completing a training epoch within the shortest time (7.84s). Comparisons among various activation functions reveal that the lowest  By comparing the results in Table <ref type="table" target="#tab_5">6</ref>, 7, and 8, we note that the performance differences between different variants of the stanh function are relatively small as compared to the bounded activation functions (except on the AR Purdue dataset). This is due to the fact that the value differences between the original and proposed coefficients for the stanh function are very small (i.e. 1.7159 and 0.6667 versus 1.7321 and 0.6585). The aim here is to prove the importance of having the correct coefficients for the stanh function based on the properties as defined in <ref type="bibr" target="#b28">[29]</ref> instead of proposing a new sigmoidal function that outperforms it significantly.</p><p>There is an important thing to note though, that training the CNN model with the bifire function using the MSE loss function encounters the numerical instability problem consistently even with different sets of hyperparameter values. This signifies clearly the impact of having an unbounded activation function to the overall training stability, which serves as the main motivation of studying the effect of various activation functions on the training stability (in terms of the numerical stability), particularly on deep-layered NN models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Training stability</head><p>It is important to evaluate the training stability during a training process based on its numerical stability, since it dictates whether a CNN model will be successfully trained or not. A numerical instability problem occurs whenever a "not a number" (NaN) or infinity value is detected, or a sudden drastic increase in the instantaneous training loss (output of the MSE or CE loss function) to a very large value is observed.</p><p>The probability of a numerical instability problem is evaluated by calculating the proportion of the experimental settings for an activation function that result in the numerical instability during an NN training. A training using a specific experimental setting is perceived to encounter the instability problem whenever the numerical instability occurs during at least a single training repetition.</p><p>By referring to Table <ref type="table" target="#tab_8">9</ref>, training of the CNN models with the proposed bounded activation functions reduces the probability of numerical instability significantly as opposed to the ones with the unbounded functions. Analysis of the results reveals big improvements of 78.58% (both brelu and blrelu) and 71.93% (bbifire) on the MNIST dataset. This justifies the importance of manipulating the output boundary values for a more stable training process. Note that the stanh function with the proposed coefficients is not evaluated here, since its small bounded range guarantees the numerical stability in most cases. The proposed bounded activation functions are also evaluated on the mnist-rot-bg-img and AR Purdue datasets, and the results are tabulated as in Table <ref type="table" target="#tab_9">10</ref> and Table <ref type="table" target="#tab_10">11</ref>. Both results exhibit the similar pattern, where CNN models with either relu or brelu function does not suffer from the numerical instability problem during the training process. However, significant improvements in the numerical instability problem are observed from the experiments using the proposed blrelu function, where training of CNN models using the CE loss function does not encounter any numerical instability condition with the proposed blrelu function instead of lrelu.</p><p>Experiments on the bi-firing function even clearly indicate the necessity of bounding the output of an activation function, where replacing the original bifire function with the proposed bbifire function eliminates any numerical instability problem during the training process on both datasets, regardless of which type of the loss function to be applied. This proves the importance of having a bounded activation function as defined in the UAT.</p><p>There have been some recent works on improving the training stability of DNN models, notably batch normalization <ref type="bibr" target="#b62">[63]</ref>. This method focuses on the internal covariate shift problem (i.e. the change in the distribution of neuron inputs that leads to the difficulty of training the DNN effectively) by normalizing these neuron inputs based on the mini-batches in the training set. This results in an accelerated training process and higher training accuracy since larger learning rates can be used without deteriorating the learning outcome due to the training divergence. Also, the authors stated that the method can be applied on both sigmoidal and non-sigmoidal activation functions, hence the proposed bounded functions as well.</p><p>However, batch normalization does not address directly the impact of unbounded activation functions to the training stability of DNN models due to the numerical instability problem. It is suited for mini-batch learning algorithms only, requires more parameters to be trained per activation, and hence introduces far more computations to the training process. Nevertheless, we are positive that the combination of the proposed bounded activation functions and batch normalization can yield even better overall training outcome, since batch normalization adjusts the inputs' distributions for more effective training, while bounded activation functions alleviate the numerical instability problem by bounding the activation outputs. This corresponds to a relatively promising future work to improve the DNN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have presented three bounded activation functions for neural networks that deal with the generalization performance and training stability issues. Our proposed functions, named bounded ReLU (brelu), bounded leaky ReLU (blrelu), and bounded bi-firing (bbifire), are defined from the existing activation functions that excel in the DNN models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref> based on the desired properties of the UAT. We have also proposed a better set of coefficient values for the scaled hyperbolic tangent (stanh) function that is derived based on the rationales as discussed in the previous work <ref type="bibr" target="#b28">[29]</ref>.</p><p>To evaluate the performance of our activation functions, a series of experiments on MLP and CNN models are performed on three case studies that include classification of basic handwritten digits in MNIST database, complex handwritten digits from the mnist-rot-bg-img database, and face recognition using AR Purdue database. The effect of using different loss functions for the activation functions is analyzed by incorporating either MSE or CE loss function into the training process. Benchmarking with the previous work using MLP shows that the proposed activation functions perform comparably or better than their original forms, even with possibly smaller MLP model, less training epochs, and without any weight regularization techniques.</p><p>Analysis of the learning curves on the MNIST database reveals that CNN models with the proposed functions converge faster and more stable, signifying better training efficiency and with the proposed bbifire function achieving the lowest testing MCR (i.e. 0.86%) on the MNIST dataset. Experimental results on all the three datasets demonstrate the superiority of all the proposed activation functions, with significant improvements up to 17.31%, 9.19%, and 74.99% on MNIST, mnist-rot-bgimg, and AR Purdue databases respectively in terms of the testing MCRs. Training of the CNN models with the brelu function results in the fastest execution time per epoch on both the mnistrot-bg-img and AR Purdue databases.</p><p>In terms of the training stability, bounding the output of an activation function results in a significant reduction in the probability of numerical instability problem (i.e. 78.58%) on the MNIST dataset. Experiments on the mnist-rot-bg-img and AR Purdue databases reveals even bigger impact of having a bounded activation function in a DNN model, as using the bbifire function completely eliminates the numerical instability problem. This provides a solid evidence that the bounded output of an activation function is essential to alleviate the training instability problem when training a DNN model (particularly CNN) regardless of which loss function is to be applied.</p><p>Future work of this research involves extensive analysis of the proposed activation functions for different NN models as well as case studies in pattern recognition. This can be performed together with a wide range of sigmoidal and nonsigmoidal activation functions to evaluate the performance of NN models with various activation functions on different case studies. Batch normalization is also a promising method to be applied in conjunction with the proposed activation functions to achieve higher accuracies as discussed previously. Also, we do acknowledge the need of tuning the hyperparameters for the proposed activation functions manually despite of their superior performances over the unbounded ones. Therefore, another research direction is to incorporate the hyperparameter selection of the activation functions into the training process <ref type="bibr" target="#b63">[64]</ref>, where suitable hyperparameter values for the activation function of each neuron are tuned automatically to achieve better learning performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Activation and gradient curves of the logistic function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Activation and gradient curves of the hyperbolic tangent function.</figDesc><graphic coords="4,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Activation and gradient curves of the ReLU function.</figDesc><graphic coords="5,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Activation and gradient curves of the bi-firing function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Activation and gradient curves of proposed activation functions: (a) bounded ReLU and (b) leaky bounded ReLU.</figDesc><graphic coords="6,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Activation and gradient curves of the proposed bounded bi-firing activation function.</figDesc><graphic coords="7,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Proposition 2 .</head><label>2</label><figDesc>All three functions are bounded within a range of input values. Proposition 3. All three functions are continuous for all ranges of inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Proposition 4 .</head><label>4</label><figDesc>Both bounded ReLU and bounded leaky ReLU functions are monotonically increasing functions for any value of x, while the bounded bi-firing function is monotonically increasing only for x 0. Proposition 5. All three functions are piecewise differentiable over all input values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>7321, 0.6585) Df(1, 1) Df(1.7321, 0.6585)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Activation and gradient curves of the hyperbolic tangent function with different sets of coefficients.</figDesc><graphic coords="8,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Examples of the MNIST handwritten digit images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Examples of the handwritten digit images in the mnist-rot-bg-img database.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The face images of a single subject in the AR Purdue face database.</figDesc><graphic coords="9,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The baseline CNN1 model for handwritten digit classification using the MNIST and mnist-rot-bg-img datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Training efficiencies of the CNNs with the relu and brelu functions: (a) training losses and (b) testing MCRs using the MSE loss function; and (c) training losses and (d) testing MCRs using the CE loss function.</figDesc><graphic coords="12,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Training efficiencies of the CNNs with the lrelu and blrelu functions: (a) training losses and (b) testing MCRs using the MSE loss function; and (c) training losses and (d) testing MCRs using the CE loss function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Training efficiencies of the CNNs with the bifire and bbifire functions: (a) training losses and (b) testing MCRs using the MSE loss function; and (c) training losses and (d) testing MCRs using the CE loss function.</figDesc><graphic coords="13,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Training efficiencies of the CNNs with the tanh function with different sets of coefficient values: (a) training losses and (b) testing MCRs using the MSE loss function; and (c) training losses and (d) testing MCRs using the CE loss function.</figDesc><graphic coords="14,95.00,53.23,406.22,713.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The MLP model for the experiments using the mnist-rot-bg-img dataset.</figDesc><table><row><cell>Layer</cell><cell>Type</cell><cell>Total neurons</cell><cell>Activation function</cell><cell>Free parameters</cell></row><row><cell>N0</cell><cell>Input</cell><cell>784</cell><cell>No</cell><cell>0</cell></row><row><cell>F1</cell><cell>Fully-connected</cell><cell>500</cell><cell>Yes</cell><cell>392500</cell></row><row><cell>F2</cell><cell>Fully-connected</cell><cell>500</cell><cell>Yes</cell><cell>250500</cell></row><row><cell>F3</cell><cell>Fully-connected</cell><cell>500</cell><cell>Yes</cell><cell>250500</cell></row><row><cell>F4</cell><cell>Fully-connected</cell><cell>500</cell><cell>Yes</cell><cell>250500</cell></row><row><cell>F5</cell><cell>Fully-connected</cell><cell>10</cell><cell>No</cell><cell>5010</cell></row><row><cell>X6</cell><cell>Softmax</cell><cell>10</cell><cell>No</cell><cell>0</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>1149010</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The CNN1 model for the experiments using the MNIST and mnist-rotbg-img datasets.</figDesc><table><row><cell>Layer</cell><cell>Dimension</cell><cell>Operation</cell><cell>Activation function</cell><cell>Free parameters</cell></row><row><cell>N0</cell><cell>1@28 × 28</cell><cell>-</cell><cell>No</cell><cell>0</cell></row><row><cell>C1</cell><cell>4@24 × 24</cell><cell>5 × 5 convolution</cell><cell>Yes</cell><cell>104</cell></row><row><cell>S1</cell><cell>4@12 × 12</cell><cell>2 × 2 max-pooling</cell><cell>No</cell><cell>0</cell></row><row><cell>C2</cell><cell>16@8 × 8</cell><cell>5× 5 convolution</cell><cell>Yes</cell><cell>1616</cell></row><row><cell>S2</cell><cell>16@4 × 4</cell><cell>2× 2 max-pooling</cell><cell>No</cell><cell>0</cell></row><row><cell>C3</cell><cell>128@1 × 1</cell><cell>4× 4 convolution</cell><cell>Yes</cell><cell>32896</cell></row><row><cell>F4</cell><cell>10@1 × 1</cell><cell>Full connection</cell><cell>Yes</cell><cell>1290</cell></row><row><cell>X5</cell><cell>10@1 × 1</cell><cell>Softmax</cell><cell>No</cell><cell>0</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>35906</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The CNN2 model for the experiments using the AR Purdue dataset.</figDesc><table><row><cell>Layer</cell><cell>Dimension</cell><cell>Operation</cell><cell>Activation function</cell><cell>Free parameters</cell></row><row><cell>N0</cell><cell>1@56 × 46</cell><cell>-</cell><cell>No</cell><cell>0</cell></row><row><cell>C1</cell><cell>4@50 × 40</cell><cell>7 × 7 convolution</cell><cell>Yes</cell><cell>200</cell></row><row><cell>S1</cell><cell>4@25 × 20</cell><cell>2 × 2 max-pooling</cell><cell>No</cell><cell>0</cell></row><row><cell>C2</cell><cell>16@20 × 15</cell><cell>6 × 6 convolution</cell><cell>Yes</cell><cell>2320</cell></row><row><cell>S2</cell><cell>16@5 × 5</cell><cell>4× 3 max-pooling</cell><cell>No</cell><cell>0</cell></row><row><cell>C3</cell><cell>128@1 × 1</cell><cell>5× 5 convolution</cell><cell>Yes</cell><cell>51328</cell></row><row><cell>F4</cell><cell>100@1 × 1</cell><cell>Full connection</cell><cell>Yes</cell><cell>12900</cell></row><row><cell>X5</cell><cell>100@1 × 1</cell><cell>Softmax</cell><cell>No</cell><cell>0</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>66748</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for the activation functions and the corresponding search range during the training.</figDesc><table><row><cell>Activation</cell><cell>Boundary</cell><cell></cell><cell>Hyperparameters</cell><cell></cell></row><row><cell>function</cell><cell>Lower (P L )</cell><cell>Upper ( P U )</cell><cell>A</cell><cell>B</cell></row><row><cell>brelu</cell><cell>0</cell><cell>A</cell><cell>1, 2, 4, 8, 16, 32, 64</cell><cell>-</cell></row><row><cell>blrelu</cell><cell>0</cell><cell>A</cell><cell>1, 2, 4, 8, 16, 32, 64</cell><cell>-</cell></row><row><cell>bifire</cell><cell>0</cell><cell>P U</cell><cell>0.01, 0.1, 0.3, 0.5</cell><cell>-</cell></row></table><note><p>bbifire 0 B 0.01, 0.1, 0.3, 0.5 1, 4, 16 stanh -A A 1, 1.7159, 1.7321 0.6585, 0.6667, 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Testing MCRs of the MLPs with different activation functions on the mnist-rot-bg-img dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation function</cell><cell>Testing MCR (%)</cell></row><row><cell>logsig [10]</cell><cell>62.64</cell></row><row><cell>relu [10]</cell><cell>51.98</cell></row><row><cell>brelu</cell><cell>50.09</cell></row><row><cell>lrelu</cell><cell>50.52</cell></row><row><cell>blrelu</cell><cell>49.88</cell></row><row><cell>bifire [10]</cell><cell>48.75</cell></row><row><cell>bbifire</cell><cell>48.97</cell></row><row><cell>tanh [10]</cell><cell>53.17</cell></row><row><cell>stanh</cell><cell>50.12</cell></row><row><cell>stanh</cell><cell>49.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results of the CNN models using different activation functions on MNIST dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">WTA + MSE</cell><cell></cell><cell cols="2">Softmax + CE</cell><cell></cell></row><row><cell>function</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell></row><row><cell></cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell></row><row><cell>relu</cell><cell>0.14</cell><cell>0.97</cell><cell>49.97</cell><cell>0.00</cell><cell>1.02</cell><cell>24.58</cell></row><row><cell>brelu</cell><cell>0.18</cell><cell>0.94</cell><cell>32.60</cell><cell>0.10</cell><cell>0.92</cell><cell>40.61</cell></row><row><cell>lrelu</cell><cell>0.07</cell><cell>0.95</cell><cell>52.48</cell><cell>0.01</cell><cell>1.06</cell><cell>26.50</cell></row><row><cell>blrelu</cell><cell>0.11</cell><cell>0.96</cell><cell>44.08</cell><cell>0.01</cell><cell>0.96</cell><cell>48.48</cell></row><row><cell>bifire</cell><cell>0.71</cell><cell>0.89</cell><cell>58.10</cell><cell>0.79</cell><cell>1.04</cell><cell>33.14</cell></row><row><cell>bbifire</cell><cell>0.81</cell><cell>0.90</cell><cell>57.17</cell><cell>0.01</cell><cell>0.86</cell><cell>52.04</cell></row><row><cell>tanh</cell><cell>0.19</cell><cell>1.01</cell><cell>62.01</cell><cell>0.57</cell><cell>1.09</cell><cell>58.80</cell></row><row><cell>stanh</cell><cell>0.87</cell><cell>1.19</cell><cell>62.84</cell><cell>0.29</cell><cell>1.07</cell><cell>59.19</cell></row><row><cell>stanh</cell><cell>0.56</cell><cell>0.99</cell><cell>63.33</cell><cell>0.29</cell><cell>0.99</cell><cell>61.86</cell></row><row><cell cols="7">puts of the neurons are either zero (lower boundary) or a certain</cell></row><row><cell cols="7">positive value (upper boundary that depends on the value of hy-</cell></row><row><cell cols="7">perparameter A or B). More zero output values contribute to</cell></row><row><cell cols="7">more sparse representation of the data in the NN model, which</cell></row><row><cell cols="7">reduces the computational time due to more operations on the</cell></row><row><cell cols="2">zero-valued variables.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results of the CNN models using different activation functions on the mnist-rot-bg-img dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">WTA + MSE</cell><cell></cell><cell cols="2">Softmax + CE</cell><cell></cell></row><row><cell>function</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell></row><row><cell></cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell></row><row><cell>relu</cell><cell>3.53</cell><cell>25.22</cell><cell>21.94</cell><cell>5.72</cell><cell>22.97</cell><cell>19.91</cell></row><row><cell>brelu</cell><cell>4.65</cell><cell>23.60</cell><cell>18.59</cell><cell>2.72</cell><cell>22.59</cell><cell>21.12</cell></row><row><cell>lrelu</cell><cell>2.85</cell><cell>25.70</cell><cell>21.92</cell><cell>0.37</cell><cell>25.79</cell><cell>21.43</cell></row><row><cell>blrelu</cell><cell>9.37</cell><cell>24.44</cell><cell>22.10</cell><cell>0.10</cell><cell>23.42</cell><cell>20.50</cell></row><row><cell>bifire</cell><cell>15.58</cell><cell>25.02</cell><cell>21.67</cell><cell>13.82</cell><cell>25.61</cell><cell>22.10</cell></row><row><cell>bbifire</cell><cell>12.06</cell><cell>23.09</cell><cell>22.93</cell><cell>3.17</cell><cell>24.05</cell><cell>22.56</cell></row><row><cell>tanh</cell><cell>8.53</cell><cell>31.89</cell><cell>28.38</cell><cell>8.06</cell><cell>32.63</cell><cell>29.32</cell></row><row><cell>stanh</cell><cell>9.73</cell><cell>29.99</cell><cell>28.28</cell><cell>5.18</cell><cell>29.94</cell><cell>29.07</cell></row><row><cell>stanh</cell><cell>10.43</cell><cell>29.48</cell><cell>28.14</cell><cell>4.98</cell><cell>29.02</cell><cell>29.31</cell></row><row><cell cols="7">testing MCRs are achieved by training the CNN models with</cell></row><row><cell cols="7">the proposed stanh and bbifire functions using the MSE and CE</cell></row><row><cell cols="7">functions, respectively (i.e. 2.67% and 6.67%). These results</cell></row><row><cell cols="7">indicate that the MSE loss function is more suitable to be ap-</cell></row><row><cell cols="3">plied to this particular dataset.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Results of the CNN models using different activation functions on the AR Purdue dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">WTA + MSE</cell><cell></cell><cell cols="2">Softmax + CE</cell><cell></cell></row><row><cell>function</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell><cell>MCR (%)</cell><cell></cell><cell>Time</cell></row><row><cell></cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell><cell>Training</cell><cell>Testing</cell><cell>(s)</cell></row><row><cell>relu</cell><cell>16.50</cell><cell>20.50</cell><cell>8.93</cell><cell>11.85</cell><cell>19.67</cell><cell>8.47</cell></row><row><cell>brelu</cell><cell>12.90</cell><cell>17.33</cell><cell>7.84</cell><cell>8.95</cell><cell>16.50</cell><cell>8.19</cell></row><row><cell>lrelu</cell><cell>5.50</cell><cell>16.67</cell><cell>9.06</cell><cell>0.05</cell><cell>7.67</cell><cell>8.65</cell></row><row><cell>blrelu</cell><cell>0.90</cell><cell>4.17</cell><cell>8.90</cell><cell>0.15</cell><cell>7.00</cell><cell>8.40</cell></row><row><cell>bifire</cell><cell>OF</cell><cell></cell><cell>N/A</cell><cell>0 .00</cell><cell>7.33</cell><cell>8.85</cell></row><row><cell>bbifire</cell><cell>3.70</cell><cell>7.17</cell><cell>9.22</cell><cell>0.10</cell><cell>6.67</cell><cell>9.45</cell></row><row><cell>tanh</cell><cell>1.45</cell><cell>10.00</cell><cell>10.38</cell><cell>0.25</cell><cell>10.83</cell><cell>10.36</cell></row><row><cell>stanh</cell><cell>0.25</cell><cell>4.83</cell><cell>9.85</cell><cell>0.00</cell><cell>8.33</cell><cell>10.25</cell></row><row><cell>stanh</cell><cell>0.00</cell><cell>2.67</cell><cell>9.87</cell><cell>0.00</cell><cell>7.00</cell><cell>10.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Probability of numerical instability for the CNN models using different activation functions on the MNIST dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">Probability of numerical instability (%)</cell></row><row><cell>function</cell><cell>WTA + MSE</cell><cell>Softmax + CE</cell></row><row><cell>relu</cell><cell>0.00</cell><cell>40.00</cell></row><row><cell>brelu</cell><cell>0.00</cell><cell>8.57</cell></row><row><cell>lrelu</cell><cell>60.00</cell><cell>40.00</cell></row><row><cell>blrelu</cell><cell>51.43</cell><cell>8.57</cell></row><row><cell>bifire</cell><cell>95.00</cell><cell>93.33</cell></row><row><cell>bbifire</cell><cell>26.67</cell><cell>53.33</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Probability of numerical instability for the CNN models using different activation functions on the mnist-rot-bg-img dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">Probability of numerical instability (%)</cell></row><row><cell>function</cell><cell>WTA + MSE</cell><cell>Softmax + CE</cell></row><row><cell>relu</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>brelu</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>lrelu</cell><cell>60.00</cell><cell>60.00</cell></row><row><cell>blrelu</cell><cell>25.71</cell><cell>0.00</cell></row><row><cell>bifire</cell><cell>92.50</cell><cell>0.00</cell></row><row><cell>bbifire</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Probability of numerical instability for the CNN models using different activation functions on the AR Purdue dataset. The bolded functions denote the proposed functions.</figDesc><table><row><cell>Activation</cell><cell cols="2">Probability of numerical instability (%)</cell></row><row><cell>function</cell><cell>WTA + MSE</cell><cell>Softmax + CE</cell></row><row><cell>relu</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>brelu</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>lrelu</cell><cell>80.00</cell><cell>0.00</cell></row><row><cell>blrelu</cell><cell>31.43</cell><cell>0.00</cell></row><row><cell>bifire</cell><cell>100.00</cell><cell>45.00</cell></row><row><cell>bbifire</cell><cell>0.00</cell><cell>0.00</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by Universiti Teknologi Malaysia (UTM) and the Ministry of Science, Technology and Innovation of Malaysia (MOSTI) under the ScienceFund Grant No. 4S116.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bi-modal derivative activation function for sigmoidal feedforward networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Sodhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2014.06.007</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2014.06.007" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="182" to="196" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(91)90009-T</idno>
		<ptr target="http://dx.doi.org/10.1016/0893-6080(91)90009-T" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural networks and modelling in vacuum science</title>
		<author>
			<persName><forename type="first">I</forename><surname>Belič</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.vacuum.2006.02.017</idno>
		<ptr target="http://dx.doi.org/10.1016/j.vacuum.2006.02.017" />
	</analytic>
	<monogr>
		<title level="j">Vacuum</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1107" to="1122" />
			<date type="published" when="2006">2006</date>
			<pubPlace>the World Energy Crisis</pubPlace>
		</imprint>
	</monogr>
	<note>Some Vacuumbased Solutions</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Effective deep learning-based multi-modal retrieval</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-015-0391-4</idno>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling compressive strength of {EPS} lightweight concrete using regression, neural network and {ANFIS}</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadrmomtazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirgozar</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.conbuildmat.2013.01.016</idno>
		<ptr target="http://dx.doi.org/10.1016/j.conbuildmat.2013.01.016" />
	</analytic>
	<monogr>
		<title level="j">Construction and Building Materials</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="205" to="216" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ultrasound-assisted extraction of phenolics from longan (dimocarpus longan lour.) fruit seed with artificial neural network and their antioxidant activity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12161-012-9370-1</idno>
	</analytic>
	<monogr>
		<title level="j">Food Analytical Methods</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1244" to="1251" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evolving neural network weights for time-series prediction of general aviation flight data, in: Parallel Problem Solving from</title>
		<author>
			<persName><forename type="first">T</forename><surname>Desell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clachar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wild</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">8672</biblScope>
			<biblScope unit="page" from="771" to="781" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training cnns with low-rank filters for efficient image classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<idno>CoRR abs/1511.06744</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time video convolutional face finder on embedded platforms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<idno type="DOI">10.1155/2007/21724</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Embedded Syst</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bi-firing deep neural networks</title>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s13042-013-0198-9</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Machine Learning and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="83" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Agenet: Deeply learned regressor and classifier for robust apparent age estimation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">End-to-end photosketch generation via fully convolutional representation learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1501.07180</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised object segmentation with convolutional neural networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Idiap</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Idiap-RR Idiap-RR-13-2014</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Shet</surname></persName>
		</author>
		<idno>CoRR abs/1312.6082</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">EESEN: end-to-end speech recognition using deep RNN models and wfst-based decoding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<idno>abs/1507.08240</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Generating sequences with recurrent neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno>CoRR abs/1308.0850</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR abs/1511.06392</idno>
		<title level="m">Neural random-access machines</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Neural network for nonsmooth, nonconvex constrained minimization via smooth approximation, Neural Networks and Learning Systems</title>
		<author>
			<persName><forename type="first">W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2013.2278427</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="545" to="556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Low-order dominant harmonic estimation using adaptive wavelet neural network, Industrial Electronics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIE.2013.2242414</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="428" to="435" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melchior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wiskott</surname></persName>
		</author>
		<idno>abs/1401.5900</idno>
		<title level="m">Gaussian-binary restricted boltzmann machines on modeling natural image statistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast harmonic estimation of stationary and time-varying signals using ea-awnn, Instrumentation and Measurement</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIM.2012.2217637</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="343" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hardware implementation of evolvable block-based neural networks utilizing a cost efficient sigmoid-like activation function</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Nambiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Hani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sahnoun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Marsono</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2014.03.018</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="228" to="241" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On practical constraints of approximation using neural networks on current digital computers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Puheim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nyulaszi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Madarasz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gaspar</surname></persName>
		</author>
		<idno type="DOI">10.1109/INES.2014.6909379</idno>
	</analytic>
	<monogr>
		<title level="m">Intelligent Engineering Systems (INES), 2014 18th International Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="257" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/0893-6080(89)90020-8</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Netw</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A max-piecewise-linear neural network for function approximation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="843" to="852" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A non-sigmoidal activation function for feedforward artificial neural networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2015.7280440</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN), 2015 International Joint Conference on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<idno type="DOI">10.1109/5.726791</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Implementation of a new sigmoid function in backpropagation neural networks, Master&apos;s thesis</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bonnell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">8 2011</date>
		</imprint>
		<respStmt>
			<orgName>East Tennessee State University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparison of new activation functions in neural network for forecasting financial time series</title>
		<author>
			<persName><forename type="first">G</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ludermir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lima</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00521-010-0407-3</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="439" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A skewed derivative activation function for sffanns</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sodhi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRAIE.2014.6909324</idno>
	</analytic>
	<monogr>
		<title level="m">Recent Advances and Innovations in Engineering (ICRAIE)</title>
		<imprint>
			<date type="published" when="2014">2014, 2014</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A case for the self-adaptation of activation functions in {FFANNs}</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2003.08.005</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2003.08.005" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="447" to="454" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An activation function adapting training algorithm for sigmoidal feedforward networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singh</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2004.04.001</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2004.04.001" />
	</analytic>
	<monogr>
		<title level="m">hybrid Neurocomputing: Selected Papers from the 2nd International Conference on Hybrid Intelligent Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="429" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A class +1 sigmoidal activation functions for ffanns</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Economic Dynamics and Control</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="187" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scalable vlsi implementations for neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Den Bout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Franzon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Paulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF00929928</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of VLSI signal processing systems for signal, image and video technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="385" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multistability and instability of delayed competitive neural networks with nondecreasing piecewise linear activation functions</title>
		<author>
			<persName><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fei</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2013.03.030</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2013.03.030" />
	</analytic>
	<monogr>
		<title level="m">intelligent Processing Techniques for Semantic-based Image and Video Retrieval</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="281" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A weight initialization method for improving training speed in feedforward neural network</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Yam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="232" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep learning with limited numerical precision</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Narayanan</surname></persName>
		</author>
		<idno>CoRR abs/1502.02551</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>CoRR abs/1211.5590</idno>
		<title level="m">Theano: new features and speed improvements</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="DOI">10.1145/2647868.2654889</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the 22Nd ACM International Conference on Multimedia, MM &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Shelhamer, cudnn: Efficient primitives for deep learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename></persName>
		</author>
		<idno>CoRR abs/1410.0759</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The neuro vector engine: Flexibility to improve convolutional net efficiency for wearable vision</title>
		<author>
			<persName><forename type="first">M</forename><surname>Peemen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Juurlink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mesman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 Design, Automation Test in Europe Conference Exhibition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1604" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Shifting vision processing closer to the sensor</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fasthuber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ienne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<author>
			<persName><surname>Shidiannao</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750389</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42Nd Annual International Symposium on Computer Architecture, ISCA &apos;15</title>
		<meeting>the 42Nd Annual International Symposium on Computer Architecture, ISCA &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="92" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Pudiannao: A polyvalent machine learning accelerator</title>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Teman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/2694344.2694358</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15</title>
		<meeting>the Twentieth International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="369" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dadiannao: A machine-learning supercomputer</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1109/MICRO.2014.58</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-47</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO-47<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="609" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Diannao: A small-footprint high-throughput accelerator for ubiquitous machinelearning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Temam</surname></persName>
		</author>
		<idno type="DOI">10.1145/2541940.2541967</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;14</title>
		<meeting>the 19th International Conference on Architectural Support for Programming Languages and Operating Systems, ASPLOS &apos;14<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Approximate computing on programmable socs via neural acceleration</title>
		<author>
			<persName><forename type="first">T</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Esmaeilzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ceze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep neural nets as a method for quantitative structure-activity relationships</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Svetnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improving deep neural network acoustic models using generalized maxout networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2014.6853589</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="215" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="DOI">10.1145/1273496.1273556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning, ICML &apos;07</title>
		<meeting>the 24th International Conference on Machine Learning, ICML &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pca versus lda, Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kak</surname></persName>
		</author>
		<idno type="DOI">10.1109/34.908974</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="233" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2005.202</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
	<note>CVPR 2005</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gender classification: A convolutional neural network approach</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khalil-Hani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Syafeeza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bakhteri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Turk. J. Elec. Engin</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep learning for visual understanding: A review</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oerlemans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2015.09.116</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2015.09.116" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>doi:10.1.1.207.2059</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Milakov</surname></persName>
		</author>
		<title level="m">Convolutional Neural Networks in Galaxy Zoo Challenge</title>
		<imprint>
			<date type="published" when="2014-04">April. 2014</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic age estimation based on deep learning algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2015.09.115</idno>
		<ptr target="http://dx.doi.org/10.1016/j.neucom.2015.09.115" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.220</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Negnevitsky</surname></persName>
		</author>
		<title level="m">Artificial Intelligence: A Guide to Intelligent Systems, 1st Edition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
