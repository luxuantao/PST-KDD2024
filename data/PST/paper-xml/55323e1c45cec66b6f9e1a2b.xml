<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multiple Linear Mappings for Efficient Single Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kaibing</forename><surname>Zhang</surname></persName>
							<email>kbzhang@hbeu.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@uts.edu.au</email>
						</author>
						<author>
							<persName><roleName>Senior Member, IEEE</roleName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
							<email>xbgao@mail.xidian.edu.cn</email>
						</author>
						<author>
							<persName><roleName>Fellow, IEEE</roleName><forename type="first">Xuelong</forename><surname>Li</surname></persName>
							<email>xuelong_li@opt.ac.cn</email>
						</author>
						<author>
							<persName><forename type="first">Zenggang</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sina</forename><forename type="middle">K</forename><surname>Farsiu</surname></persName>
						</author>
						<author>
							<persName><surname>Zhang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Integrated Services Networks</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer and Information Science</orgName>
								<orgName type="institution">Hubei Engi-neering University</orgName>
								<address>
									<postCode>432000</postCode>
									<settlement>Xiaogan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Centre for Quantum Computation and Intelligent Systems</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering and Information Technology</orgName>
								<orgName type="institution">University of Technology</orgName>
								<address>
									<postCode>2007</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Electronic Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory of Integrated Services Networks</orgName>
								<orgName type="institution">Xidian University</orgName>
								<address>
									<postCode>710071</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Center for OPTical IMagery Analysis and Learning (OPTIMAL)</orgName>
								<orgName type="department" key="dep2">State Key Laboratory of Transient Optics and Photonics</orgName>
								<orgName type="department" key="dep3">Xi&apos;an Institute of Optics and Precision Mechanics</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>710119</postCode>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Computer and Information Science</orgName>
								<orgName type="institution">Hubei Engineering University</orgName>
								<address>
									<postCode>432000</postCode>
									<settlement>Xiaogan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multiple Linear Mappings for Efficient Single Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4585B4489459C0D06F7DA636075C8BE8</idno>
					<idno type="DOI">10.1109/TIP.2015.2389629</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T04:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fast non-local means</term>
					<term>feature subspace</term>
					<term>multiple linear mappings (MLMs)</term>
					<term>single image super-resolution (SR)</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Example learning-based superresolution (SR) algorithms show promise for restoring a high-resolution (HR) image from a single low-resolution (LR) input. The most popular approaches, however, are either time-or space-intensive, which limits their practical applications in many resource-limited settings. In this paper, we propose a novel computationally efficient single image SR method that learns multiple linear mappings (MLM) to directly transform LR feature subspaces into HR subspaces. In particular, we first partition the large nonlinear feature space of LR images into a cluster of linear subspaces. Multiple LR subdictionaries are then learned, followed by inferring the corresponding HR subdictionaries based on the assumption that the LR-HR features share the same representation coefficients. We establish MLM from the input LR features to the desired HR outputs in order to achieve fast yet stable SR recovery. Furthermore, in order to suppress displeasing artifacts generated by the MLM-based method, we apply a fast nonlocal means algorithm to construct a simple yet effective similaritybased regularization term for SR enhancement. Experimental results indicate that our approach is both quantitatively and qualitatively superior to other application-oriented SR methods, while maintaining relatively low time and space complexity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE objective of single image super-resolution (SR) is to restore a visually pleasing high-resolution (HR) image from a single low-resolution (LR) input. SR reconstruction is an effective signal recovery technique that produces high quality images from low-cost imaging systems (e.g., webcams or mobile phones) and limited environmental conditions (e.g., security surveillance or remote sensing imaging), and also offers an improved user experience in resource-limited sharing systems (e.g., social networking). For these reasons SR reconstruction has attracted extensive attention since the seminal publication by Tsai and Huang <ref type="bibr" target="#b0">[1]</ref>. Existing single image SR approaches can be broadly classified into three categories: interpolation-based methods, reconstruction-based methods, and example learning-based methods.</p><p>Interpolation-based methods are regarded as a more basic approach to SR and typically utilize either fixed-function kernels <ref type="bibr" target="#b1">[2]</ref> or adaptive-structure kernels <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> to estimate unknown pixels in the HR grid. Although these methods are efficient for real-time applications, in many cases the quality of the reconstructed images is unsatisfactory in practice.</p><p>Reconstruction-based methods usually assume that the observed LR image is the product of several degradation factors such as blurring, down-sampling, and noising with additive zero-mean white and Gaussian noise. Since many details are missing, one LR image may correspond to many HR images and, as a result, the SR problem is inherently ill-posed. In order to obtain a reliable solution certain a priori knowledge needs to be imposed on the result to be super-resolved, and various priors, such as edge-directed priors <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b8">[9]</ref> and similarity redundancy priors <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref> are widely used in reconstruction-based methods. While reconstruction-based methods produce sharp edges and suppress aliasing artifacts they do not add sufficient novel details to the HR output, especially at high magnification (e.g., greater than Ã—2).</p><p>Example learning-based SR approaches are superior to reconstruction-based methods since they are able to produce novel details that cannot be found in the LR input. Example learning-based methods generally use training datasets that contain millions of co-occurrence LR-HR image patches <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b17">[18]</ref>, a learned LR-HR overcomplete dictionary pair <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>, or a small number of representative prototypes <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> as priors to estimate the relationship between the LR and HR images. Depending on how the mapping relationships are formulated, example learningbased SR approaches can be further categorized into two major types: coding-based and regression-based methods. Coding-based mapping models include k-nearest neighbor (k-NN) learning <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, NE-based learning <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and sparse coding (SC) <ref type="bibr" target="#b17">[18]</ref>- <ref type="bibr" target="#b21">[22]</ref>. The k-NN and NE-based learning algorithms often need to search a vast reference dataset for similar patterns in order to optimally represent complicated structures in generic images, and therefore the SR lacks efficiency for practical applications. Recently, machine learning has been used to successfully obtain an overcomplete dictionary for sparse signal recovery, and dictionary learningbased SR algorithms <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> show great potential for practical applications. In particular, Yang et al. <ref type="bibr" target="#b18">[19]</ref> proposed jointly learning a compact LR-HR dictionary pair for sparse reconstruction. Compared to methods that directly sample a large dataset, this dictionary learning-based SR method produces superior results with remarkably low computational complexity. However, according to Fang's method <ref type="bibr" target="#b25">[26]</ref>, a jointly trained dictionary pair in the training phase cannot guarantee co-occurrence of sparse representations of LR and HR features in the reconstruction phase when the LR dictionary individually represents the input LR feature. Moreover, the optimization algorithm used in <ref type="bibr" target="#b18">[19]</ref> suffers from the highly intensive computation of the 1 -norm regularity for each image patch. Using a similar framework to <ref type="bibr" target="#b18">[19]</ref>, Zeyde et al. <ref type="bibr" target="#b19">[20]</ref> improved SR efficiency using principal component analysis (PCA)-based dimensionality reduction and orthogonal matching pursuit (OMP) <ref type="bibr" target="#b26">[27]</ref> for sparse representation, which outperformed <ref type="bibr" target="#b18">[19]</ref> both in terms of computational time and reconstruction quality. Although dictionary learning-based methods have notable advantages with respect to reconstruction quality and memory allocation, solving SC is still time consuming, especially with increasing size of dictionary or input image, and this hinders its practical use in many resource-limited systems. In contrast to codingbased methods, regression-based models <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> typically establish direct mapping relationships between the LR image and its corresponding HR image. The quality of prototypes is key to the reconstructed quality; large numbers of prototypes result in extensive computation, while fewer prototypes may inadequately approximate the image space. Unlike the aforementioned methods that rely on an external training dataset, several state-of-art approaches to SR have recently been developed that exploit the self-similarity of structures in the LR input <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. While empirically superior, they require a lot of extra memory and runtime to build counterparts across different scales in a recursive scheme, which is unsuitable for most resource-limited applications.</p><p>Computational time and space allocation need to be fully considered in real-world SR applications. Several recent efforts <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>- <ref type="bibr" target="#b32">[33]</ref> have been made to reduce processing time while maintaining SR reconstruction quality. For example, Yang et al. <ref type="bibr" target="#b20">[21]</ref> proposed a method in which two coupled dictionaries were learned across the LR and HR image spaces for sparse representation, with two acceleration schemes (selective patch processing and fast sparse inference using feed-forward neural networks) adopted to overcome the SC bottleneck. Timofte et al. <ref type="bibr" target="#b30">[31]</ref> constructed a set of mapping relationships between the LR and HR patches by grouping correlative neighbors using a learned LR-HR dictionary pair. In addition, Yang et al. <ref type="bibr" target="#b31">[32]</ref> advocated learning a set of simple mapping functions from numerous image subspaces using multivariate linear regression. Although these methods are computationally and qualitatively effective, there is still a need to develop efficient but high-quality SR algorithms for practical applications.</p><p>Inspired by the previous fast example learning-based SR methods <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, we adopt the similar idea from <ref type="bibr" target="#b31">[32]</ref> to infer the mapping relationship between two feature subspaces: LR feature subspace and HR feature subspace from a set of structural clusters. By contrast to <ref type="bibr" target="#b31">[32]</ref> that uses multivariate linear regression to directly build the mapping function within each LR-HR subspace, the proposed method first learns a compact LR-HR subdictionary pair regarding to each subspace, and then the shared representation coefficients are obtained to infer the mapping relationship, utilizing the global regression <ref type="bibr" target="#b30">[31]</ref>. Once Multiple Linear Mappings (MLM) are obtained, the missing details in a given LR input can be efficiently predicted by finding a matched model among those learned mapping functions. MLM differs from previous fast SR approaches as follows:</p><p>1) To approximate the complicated nonlinear structure of the feature space of large images, we first partition the feature space of training exemplars into a set of subspaces. A compact yet orthogonal LR subdictionary is individually learned in each subspace and used to infer the corresponding HR subdictionary, with the assumption that the LR-HR features share the same representation. Due to the relatively stable geometric structures in each feature subspace, these learned LR-HR subdictionaries help establish MLM for more faithful and efficient SR recovery. 2) To reduce artifacts in the images obtained by MLM, we present a fast SR enhancement algorithm that uses a fast non-local means (NLM) algorithm <ref type="bibr" target="#b33">[34]</ref>. This enhancement algorithm is time efficient and effectively suppresses edge artifacts. Specifically, our newly proposed SR algorithm consists of two stages, namely learning and reconstruction. The major steps in the learning stage are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>: we first prepare a training set consisting of a large number of co-occurrence LR-HR features to represent an enormous image feature space, and then use the standard k-means algorithm <ref type="bibr" target="#b34">[35]</ref> to divide the LR feature space into multiple feature subspaces. In order to improve learning efficiency, we use the PCA coefficient space of LR features, rather than its original high dimensional one, for subspace learning. A compact and orthogonal LR subdictionary is then trained in each LR coefficient subspace before inferring the corresponding HR subspace, presuming that the LR and HR features share the same representation coefficients over their own subdictionaries. MLMs are then built to transform the LR feature to the HR feature using the LR-HR subdictionary pairs. In the reconstruction stage, an extracted LR feature from the LR image is first transformed into the PCA coefficient space, before being mapped onto the HR feature space using the most matched mapping relationship. Finally, we employ a fast NLM-based enhancement algorithm to reduce artifacts in the resultant images obtained by MLM-based synthesis. Due to simple linear mapping and fast SR enhancement, our SR approach is highly computationally efficient and maintains quality when quantitatively and qualitatively compared to other state-of-art methods.</p><p>The remainder of the paper is organized as follows. Section II presents our MLM-based SR algorithm. A fast NLM-based enhancement algorithm is detailed in Section III. In Section IV we experimentally test the new algorithm and compare it to other state-of-the-art methods. Finally, we conclude our paper and discuss future work in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MULTIPLE LINEAR MAPPINGS FOR SR RECOVERY</head><p>In this section, we first describe how to train a group of LR-HR subdictionaries to best approximate a large image space. We then construct MLMs corresponding to different LR-HR feature subspaces. Finally, we outline the proposed MLM-based SR algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Learning Multiple Linear Subspaces</head><p>Suppose we have two feature spaces: the LR feature space X âŠ† R m and the HR feature space Y âŠ† R n . In order to adapt to the different contents of generic natural images, we first collect a large number of LR-HR image pairs to prepare a training dataset. Let X s = x i s N s i=1 be the LR training samples from X and Y s = y i s N s i=1 be the corresponding HR training samples from Y, where x i s is the feature vector representing the i th LR image patch, y i s is its counterpart representing the i th HR image patch, and N s is the number of LR-HR image pairs.</p><p>The key of example learning-based SR approaches is to infer the mapping functions between LR and HR feature spaces spanned by millions of co-occurrence LR-HR image patches. Although sparse representation is regarded as a promising technique for this task, solving the sparse model is computationally expensive. Moreover, natural images contain remarkably complicated structures, which means that the relationship between the LR and HR images is nonlinear, so it is not possible to learn a universal nonlinear model that describes such a complex relationship. Motivated by structural clustering philosophy in <ref type="bibr" target="#b25">[26]</ref> that learns one compact dictionary pair and mapping function in each cluster, we present a new computationally efficient single image SR method by learning multiple simple linear relationships from a cluster of LR/HR feature subspaces, to directly convert the LR feature subspaces into the HR feature subspaces. To this end, we employ the standard k-means clustering algorithm to divide the LR training set X s into K subsets by minimizing the intercluster variance, which leads to K anchor points {c k } K k=1 representing K LR feature subsets. Let X k s = x i s iâˆˆ k be the kth subset of X s , where k stands for the specified index set of X k s . Correspondingly, we divide the HR training set Y s into K subsets Y k s (k = 1, ..., K ) using the same indices as X k s . In this way, K coupled LR-HR feature subsets X k s , Y k s K k=1 are established. Once these coupled feature subsets are formed, multiple LR-HR subdictionary pairs are individually learned such that each pair can represent the underlying structure of the corresponding subspace; in this way both the LR and HR feature vectors can be approximately represented as linear combinations of dictionary atoms in their own subspaces. It is mathematically desirable to find an optimal LR-HR subdictionary pair, and the shared representation coefficients that minimize the data-cost function are</p><formula xml:id="formula_0">Bk l , Ã¢k i = arg min B k l , a k i iâˆˆ k x i s -B k l a k i 2 2 , Bk h , Ã¢k i = arg min B k h , a k i iâˆˆ k y i s -B k h a k i 2 2</formula><p>(1)</p><p>where B k l âˆˆ R mÃ—d is the kth LR subdictionary that best represents all the feature vectors in X k s , B k h âˆˆ R nÃ—d is the kth HR subdictionary that best represents all the feature vectors in Y k s , and a k i âˆˆ R d denotes their shared coefficient vector for linear combinations of LR dictionary atoms in B k l to represent x i s and HR dictionary atoms in B k h to represent y i s . Jointly solving problem (1) is clearly a sophisticated bi-level optimization, which can be achieved by an alternate minimization to approximate the desired solutions, where one of three variables is fixed and the data-function of ( <ref type="formula">1</ref>) is minimized by iteratively regarding the others. We divide the above problem into two sub-problems to simplify the learning structure by first learning the LR subdictionary and then directly inferring the HR subdictionary as follows:</p><formula xml:id="formula_1">Bk l , Ã¢k i = arg min B k l , a k i iâˆˆ k x i s -B k l a k i 2 2</formula><p>(2)</p><p>and</p><formula xml:id="formula_2">Bk h = arg min B k h iâˆˆ k y i s -B k h a k i 2 2 . (<label>3</label></formula><formula xml:id="formula_3">)</formula><p>Jointly solving problem (2) can also be achieved by an alternate minimization to approximate the desired solutions, where one of two variables is fixed and the data-function of ( <ref type="formula">2</ref>) is minimized by iteratively regarding the other. However, this is sensitive to the initial estimation of the solutions.</p><p>To circumvent this difficulty we employ the variable projection approach <ref type="bibr" target="#b35">[36]</ref>, i.e., we first apply an analytical expression to estimate a k i by assuming that B k l is known. With fixed B k l , we then estimate a k i that minimizes the cost function in (2) in a closed-form by</p><formula xml:id="formula_4">a k i = B kT l B k l -1 B kT l x i s . (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>Once a k i is solved we substitute it into (2) and reformulate another problem with regard to B k l alone, i.e., Bk l = arg min</p><formula xml:id="formula_6">B k l iâˆˆ k x i s -B k l a k i 2 2</formula><p>= arg min</p><formula xml:id="formula_7">B k l iâˆˆ k x i s -B k l B kT l B k l -1 B kT l x i s 2 2 . (<label>5</label></formula><formula xml:id="formula_8">)</formula><p>We favor subdictionary B k l to be orthonormal to make problem <ref type="bibr" target="#b4">(5)</ref> easily tractable. This leads to the following optimization problem:</p><formula xml:id="formula_9">Bk l = arg min B k l iâˆˆ k x i s -B k l B kT l x i s 2 2 = arg min B k l X k s -B k l B kT l X k s 2 F s.t. B k l B kT l = I,<label>(6)</label></formula><p>where X k s is the data matrix where each column is a vector from the subspace x i s iâˆˆ k , â€¢ F denotes the Frobenius norm for matrices, and I is an m-by-m identity matrix. The problem of dictionary learning in <ref type="bibr" target="#b5">(6)</ref> requires finding the principle components of data matrix X k s . This can be effectively solved using the PCA algorithm to seek a set of orthonormal basis vectors as the subdictionary atoms of B k l . All the energy is preserved for complete representation in order to minimize reconstruction error, i.e., all the basis vectors are used as the atoms of B k l . Each coefficient vector of a k i iâˆˆ k can be obtained by projecting the feature vector x i s âˆˆ X k s onto the basis vectors of B k l as</p><formula xml:id="formula_10">a k i = B kT l x i s , âˆ€i âˆˆ k . (<label>7</label></formula><formula xml:id="formula_11">)</formula><p>Recall that our intention is to recover the HR patch feature y i s âˆˆ Y k s by approximating it as y i s â‰ˆ B k h a k i , assuming that the LR-HR features share the same representation coefficients over their own subdictionaries. In practice, the obtained coefficient vector for the LR feature is multiplied by the HR subdictionary B k h to recover y i s âˆˆ Y k s , and as a consequence the subdictionary B k h is represented as the one that minimizes the least-squares error:</p><formula xml:id="formula_12">Bk h = arg min B k h iâˆˆ k y i s -B k h a k i 2 2</formula><p>= arg min</p><formula xml:id="formula_13">B k h Y k s -B k h A k F , (<label>8</label></formula><formula xml:id="formula_14">)</formula><p>where Y k s is the data matrix in which each column is a vector from all the feature vectors in the kth subspace, and</p><formula xml:id="formula_15">A k âˆˆ R dÃ—N k (N k = | k |) is the coefficient matrix that contains {a k i } iâˆˆ k as its columns.</formula><p>The optimization ( <ref type="formula" target="#formula_13">8</ref>) can be easily solved using least squares:</p><formula xml:id="formula_16">B k h = Y k s A kT A k A kT -1 . (<label>9</label></formula><formula xml:id="formula_17">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Constructing LR-HR Feature Mapping</head><p>In contrast to most example learning-based methods which compute the mapping relationship between the LR-HR features online, we are in favor of individually establishing multiple mappings within different feature subspaces in order to improve efficiency of SR in practical applications. Following the global regression approach proposed in <ref type="bibr" target="#b30">[31]</ref> for precomputed mapping matrix, we utilize Ridge Regression (also known as Collaborative Representation) with 2 -norm regularized least squares to obtain the mapping for each feature subspace <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_18">Ã¢ j t = arg min a j t x j t -B k l a j t 2 2 + Î» a j t 2 2 , (<label>10</label></formula><formula xml:id="formula_19">)</formula><p>where</p><formula xml:id="formula_20">x j t âˆˆ R m is a given input LR feature, B k l is the kth LR subdictionary whose anchor point c k is closest to x j t , a j t is the representation coefficient vector of x j t over the subdictionary B k</formula><p>l , and Î» is a regularization parameter acting as a tradeoff between the reconstruction error and regularization term. In <ref type="bibr" target="#b9">(10)</ref>, we employ the Ridge Regression to obtain a reliable estimation of a j t . The value of the parameter Î» affects the estimation accuracy of a j t . A larger Î» corresponds to a larger estimation error, because Î» â†’âˆž induces a j t â†’ 0. In our paper, we preset a small positive constant for a reliable solution to a j t . The closed-form solution to <ref type="bibr" target="#b9">(10)</ref> is represented as</p><formula xml:id="formula_21">a j t = B kT l B k l + Î»I -1 B kT l x j t . (<label>11</label></formula><formula xml:id="formula_22">)</formula><p>Thus, the desired HR feature vector y j t can be computed using the same coefficients a j t as</p><formula xml:id="formula_23">y j t = B k h B kT l B k l + Î»I -1 B kT l x j t . (<label>12</label></formula><formula xml:id="formula_24">)</formula><p>Algorithm 1 Proposed MLM-Based SR Algorithm From Eq. ( <ref type="formula" target="#formula_23">12</ref>), the mapping matrix corresponding to the kth coupled LR-HR subspace can be calculated offline as</p><formula xml:id="formula_25">F k = B k h B kT l B k l + Î»I -1 B kT l , (<label>13</label></formula><formula xml:id="formula_26">)</formula><p>where F k is an n-by-m matrix for converting a given LR feature vector into a desired HR one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Fast MLM-Based SR</head><p>In two subsections above we established a set of relationship mapping models {c k , F k } K k=1 with which we can efficiently perform patch-wise regression-based SR directly from the LR input to the desired HR output. In this stage the input LR image y is first magnified to the size of the desired HR image using Bi-cubic interpolation. The interpolated image is then partitioned into a set of overlapping 9 Ã— 9 patches. We adopt the same feature representation as Zeyde's method <ref type="bibr" target="#b19">[20]</ref> to represent all the LR image patches, i.e., we extract the first-and second-order gradient features in the horizontal and vertical directions and perform dimensionality reduction on those features using PCA (denoted as a transform matrix P). This modification is shown to be significantly more efficient for both learning and testing. We compute the Euclidean distances to K anchor points {c k } K k=1 for each transformed feature in order to select the best matched mapping function for the transformation from the LR to HR feature. The final HR image patch is obtained by adding each predicted HR feature (i.e., the high-frequency details) to the corresponding interpolated LR image patch. Averaging fusion is then applied to obtain the final pixels of the overlapping regions between adjacent image patches. The MLM-based SR algorithm is summarized in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FAST NLM-BASED SR ENHANCEMENT</head><p>It is widely accepted that example learning-based SR approaches are effective for generating plausible details but poor at suppressing artifacts. To overcome this negative effect, many example learning-based SR methods utilize the maximum a posteriori (MAP) framework, which combines the reconstruction constraint and some prior knowledge to further improve the quality of the obtained images. The reconstruction constraint requires that the resultant image should be consistent with the original LR input using the assumed degradation model, and the applied priors aim to reduce displeasing artifacts. Mathematically, the MAP probability for SR can be expressed as:</p><formula xml:id="formula_27">X = arg min X SHX -y 2 2 + Î³ â€¢ R(X), (<label>14</label></formula><formula xml:id="formula_28">)</formula><p>where S represents a downsampling operator with a factor of s, H denotes a blurring operator, and Î³ is a regularization coefficient that acts as a tradeoff between the reconstruction error and the regularization term R(X). Different priors are used to construct the regularization term, e.g., the consistency prior used in <ref type="bibr" target="#b17">[18]</ref> and the natural image prior of discontinuity used in <ref type="bibr" target="#b23">[24]</ref>. Recently, the NLM filter <ref type="bibr" target="#b37">[38]</ref>, as a popular technique that is used to obtain the similarity redundancy in natural images, is adopted in SR based on different ways. For example, Protter et al. <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b38">[39]</ref> applied the NLM filter to no explicit motion estimation towards sequences with general motion patterns. Zhang et al. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b39">[40]</ref> exploited the property of similar redundancy in single LR image as prior for reducing artifacts in example learning-based SR reconstruction. Despite their effectiveness, the calculation of similar weights with a sliding window scheme is computationally intensive and the overall processing time is unacceptable for fast SR applications. Motivated by several previous regularization based methods <ref type="bibr" target="#b40">[41]</ref>- <ref type="bibr" target="#b42">[43]</ref>, the purpose of the proposed SR enhancement is to employ a fast implementation of NLM algorithm <ref type="bibr" target="#b33">[34]</ref> to formulate the regularization term, for highly efficient enhancement. Mathematically, the NLM filter calculates the target pixel by taking a weighted average of a large neighborhood <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_29">X i = j âˆˆN (i) w i j X j , (<label>15</label></formula><formula xml:id="formula_30">)</formula><p>where X i is the ith pixel of HR image X, w i j represents the similarity weight for two pixels at the locations (x i , y i ) and x j , y j , and N (i ) is the index set of similar pixels. The similarity weight w i j is calculated by:</p><formula xml:id="formula_31">w i j = 1 C (i ) exp - S (i, j) 2h 2 , (<label>16</label></formula><formula xml:id="formula_32">)</formula><p>where C (i ) is a summed normalization constant such that for all the weights of similar neighbors we have C (i ) = j âˆˆN (i) w i j , and h is a global filter parameter to control the decay of the exponential expression in the weighting computation.</p><p>Let (2r + 1) Ã— (2r + 1) be the size of the local image patch to be compared. The Euclidean distance comparing two image patches centered at (x i , y i ) and x j , y j is expressed by: where Î´ x and Î´ y represent the neighborhood sites in the horizontal and vertical directions, respectively. Following <ref type="bibr" target="#b33">[34]</ref>, a new image SS D (d x,dy) is introduced to represent the sum of squared difference (SSD) of the image X and its translation by the vector (dx, dy) in the horizontal and vertical directions, i.e.,</p><formula xml:id="formula_33">S (i, j ) = r Î´ x =-r r Î´ y =-r X x i + Î´ x , y i + Î´ y -X x j + Î´ x , y j + Î´ y 2 (17)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS D</head><formula xml:id="formula_34">(d x,dy) ( p, q) = xâ‰¤ p yâ‰¤q [X (x, y) -X (x + dx, y + dy)] 2 . (<label>18</label></formula><formula xml:id="formula_35">)</formula><p>In order to calculate the similar weights for two pixels at locations (x i , y i ) and x j , y j , we reparameterize p = x i +Î´ x , q = y i + Î´ y , dx = x jx i , and dy = y jy i . With this modification, we change Eq. ( <ref type="formula">17</ref>) into</p><formula xml:id="formula_36">S (i, j) = x i +r p=x i -r y i +r q=y i -r X p, q -X p + dx, q + dy 2 . (<label>19</label></formula><formula xml:id="formula_37">)</formula><p>From <ref type="bibr" target="#b18">(19)</ref> we observe that the Euclidean distance to two compared image patches is equal for computing the SSD between the specified image patch and its translation by the vector (dx, dy). Note that SS D (d x,dy) ( p, q) records the SSD of pixels at the upper left of the location ( p, q). We can obtain the SSD of each pixel within any local region by splitting the task into four parts, as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>. To calculate the SSD in the region R 4 , three additional SSD calculations are required, which can be expressed as:</p><formula xml:id="formula_38">SS D (R 4 ) = SS D (R 1 âˆª R 2 âˆª R 3 âˆª R 4 ) + SS D (R 1 ) -SS D (R 1 âˆª R 2 ) -SS D (R 1 âˆª R 3 ) = SS D (x i + r, y i + r) + SS D (x i -r, y i -r ) -SS D (x i + r, y i -r ) -SS D (x i -r, y i + r). (<label>20</label></formula><formula xml:id="formula_39">)</formula><p>In contrast to the sliding window scheme applied in <ref type="bibr" target="#b28">[29]</ref>, the SSD-based method performs the calculation of similar weights between two pixels in a more efficient way. In brief, this method first shifts the whole image within a prefixed searching window along the horizontal and vertical directions. The original image and its shifted image are subtracted to obtain the SSD image. Next, the SSD values at the upper left, the lower left, the upper right, and the lower right of a fixedsize neighbor is employed to calculate the Euclidean distance between two pixels within neighbors.</p><p>Once all the Euclidean distances to similar neighbors are calculated, a set of similar weights can be obtained using Eq. ( <ref type="formula" target="#formula_31">16</ref>). Since Eq. ( <ref type="formula" target="#formula_29">15</ref>) is a pixel-wise regression it is not suitable for global optimization. In order to formulate a unified regularization term with respect to the whole image, Eq. ( <ref type="formula" target="#formula_29">15</ref>) is extended to the whole image domain:</p><formula xml:id="formula_40">X = arg min X iâˆˆ X i -w T i i 2 2 , (<label>21</label></formula><formula xml:id="formula_41">)</formula><p>where denotes the domain of X, w i is a column vector of regression weights obtained from Eq. ( <ref type="formula" target="#formula_31">16</ref>), and i represents a column vector obtained by lexicographically stacking all similar pixels specified in the index set N (i ). For a concise representation we follow the method in <ref type="bibr" target="#b9">[10]</ref> and transform <ref type="bibr" target="#b20">(21)</ref> into the following matrix form:</p><formula xml:id="formula_42">X = arg min X (I -W) X 2 2 , (<label>22</label></formula><formula xml:id="formula_43">)</formula><p>where I is the identity matrix and</p><formula xml:id="formula_44">W (i, j) = w i j j âˆˆ N (i ) 0 other wise. (<label>23</label></formula><formula xml:id="formula_45">)</formula><p>By transforming Eq. ( <ref type="formula" target="#formula_40">21</ref>) into Eq. ( <ref type="formula" target="#formula_42">22</ref>) we obtain the expected regularization term:</p><formula xml:id="formula_46">R( X) = (I -W) X 2 2 . (<label>24</label></formula><formula xml:id="formula_47">)</formula><p>Incorporating ( <ref type="formula" target="#formula_46">24</ref>) into ( <ref type="formula" target="#formula_27">14</ref>), a concrete data-cost function for SR enhancement can be defined as:</p><formula xml:id="formula_48">X = arg min X SHX -y 2 2 + Î³ â€¢ (I -W) X 2 2 . (<label>25</label></formula><formula xml:id="formula_49">)</formula><p>Problem ( <ref type="formula" target="#formula_48">25</ref>) is a simple quadratic convex function regarding X and it can easily be solved using a gradient descent algorithm (see Ref. <ref type="bibr" target="#b9">[10]</ref>). Fundamentally, the proposed regularization term (24) can be seen as a specific implementation of the Tikhonov regularization model, in which a high-pass operator is integrated to enforce the smoothness. In particular, if the matrix W is set to a zero matrix, the regularization model is reduced to a special case of the Tikhonov regularization model that makes the solution approaching to smaller norms. By contrast, if the matrix W is set to an identity matrix, then no any prior knowledge is enforced on the desired solution and Eq. ( <ref type="formula" target="#formula_48">25</ref>) is equal to a maximum likelihood estimation of the SR reconstruction.</p><p>The proposed fast NLM-based SR enhancement algorithm is summarized in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head><p>We next compare the proposed SR method with Bi-cubic interpolation and four state-of-art example learning-based SR algorithms, namely ANR <ref type="bibr" target="#b30">[31]</ref>, Yang's <ref type="bibr" target="#b31">[32]</ref>, SC-based <ref type="bibr" target="#b18">[19]</ref>, and Zeyde's methods <ref type="bibr" target="#b19">[20]</ref>. In order to mimic LR inputs, the original HR images are down-sampled by Bi-cubic interpolation Algorithm 2 Proposed Fast NLM-Based SR Enhancement Algorithm with a specified factor of 3. In the learning stage, 800,000 LR-HR image patch pairs are first randomly extracted from BSDS300 <ref type="bibr" target="#b43">[44]</ref> and similar features to Zeyde's methods in <ref type="bibr" target="#b19">[20]</ref> are used to represent both LR and HR image patches. The PCA algorithm is applied to reduce the dimensionality of LR feature vectors and the average energy of 99% is preserved to seek a subspace of the original LR feature for more efficient learning. The training set is then partitioned into 350 subsets by applying the standard k-means clustering algorithm to the reduced LR features. In the enhancement stage, the size of the image patch is set to 5Ã—5 in order to calculate the similarity weights, and the filter parameter h is set to 25. A search radius r is set to 3 for finding similar neighbors, and the maximum number of iterations is set to 200 for the gradient descent rule. When the iterative number reaches to the preset times or the mean value of the difference between two adjacent iterations is below a small threshold value (e.g., 5Ã—10 -5 ), the enhancement algorithm stops and outputs the desired HR image. Parameters Ï„ and Î³ in Algorithm 2 are set to 1.5 and 0.05 by experimental adjustment without any optimization. When solving problem <ref type="bibr" target="#b24">(25)</ref>, we simulate the combined degradation operation SH, i.e., blurring and down-sampling operators for the generation of LR image by down-sampling the original HR images using the Bi-cubic interpolation with a specified factor of 3. Accordingly, the compound operator (SH) T in Algorithm 2 is implemented by up-sampling using the Bi-cubic interpolation with a specified factor of 3. The experimental validation consists of five parts. Both quantitative and qualitative comparisons of the different methods on the simulated LR images are demonstrated in Section IV-A. A comparison of methods on two realworld LR images is shown in Section IV-B in order to validate the capability of SR for practical applications. The effects and advantages of the method on time and space (essential for practical applications) are shown in Section IV-C. Section IV-D presents SR results obtained using different numbers of mapping functions, which affects SR performance in our model. Finally, the effectiveness of our enhancement algorithm is validated in Section IV-E. The results of the MLM only are denoted as MLM-1, and those of enhanced MLM-based results as MLM-2, for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison Results on Simulated LR Inputs</head><p>We first compare the SR capability of our proposed method with Bi-cubic interpolation and the other four example learning-based methods described above in terms of Peak Signal to Noise Ratio (PSNR) and Structural Similarity (SSIM) indices (Table <ref type="table" target="#tab_0">I</ref>) <ref type="bibr" target="#b44">[45]</ref>. We evaluate the SR capability of the different algorithms using twenty benchmark test images used in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b29">[30]</ref>. Considering that the human visual system (HVS) is more sensitive to the luminance channel than the chrominance channels, we convert RGB color space into YCbCr color space and perform the SR process on the luminance channel in YCbCr. The chrominance channels (Cb and Cr) are directly magnified to the desired size using the Bi-cubic interpolation algorithm. As shown in Table <ref type="table" target="#tab_0">I</ref>, although the objective quality of a few test images obtained by the proposed method has no obvious advantage over the baselines, the average quantitative evaluations of MLM-1 are superior to those of the others. This fact tells us that the MLMs learned from different feature subspaces can produce a more reliable SR estimation. Moreover, the SR results by MLM-1 can be further boosted using a fast NLM-based enhancement. It is noticeable that the enhancement step is particularly effective for those images that contain repetitive patterns and dominant edges because example learning-based algorithms, including our own, unavoidably generate unwanted artifacts along dominant edges. By exploiting redundancies in the resultant image the negative effects caused by the MLM-based method are effectively alleviated, leading to more faithful SR recovery.</p><p>Besides these objective quality assessments, we visually evaluate SR of photo-realistic HR images in Figs. <ref type="figure" target="#fig_2">3</ref> and<ref type="figure" target="#fig_3">4</ref> respectively, where two test images with a Ã—3 magnification factor, Butterfly (255 Ã— 255 pixels) and Foreman (351 Ã— 288 pixels), are used for comparisons. Fig. <ref type="figure" target="#fig_2">3b</ref> shows the result of the Bi-cubic interpolation algorithm, which includes both visually displeasing blurred textural details and serious jaggy artifacts along edges. Fig. <ref type="figure" target="#fig_2">3c</ref> shows the result obtained by the ANR-based method <ref type="bibr" target="#b30">[31]</ref>. Although this method generates a high-quality HR image with many fine details, there are still some unpleasing artifacts along major edges. Fig. <ref type="figure" target="#fig_2">3d</ref> illustrates the result of Yang's method <ref type="bibr" target="#b31">[32]</ref>. This method can produce an HR image of high quality and with rich details. However, the generated result possesses both jaggy edge artifacts and preservation of annoying textural details, especially around the white eyespots on the Butterfly's body. The SC-based method <ref type="bibr" target="#b18">[19]</ref> can synthesize many novel details using jointly learned LR-HR dictionary pair as shown in Fig. <ref type="figure" target="#fig_2">3e</ref>. However, the resultant image possesses notable zigzagging effects along dominant edges. Zeyde's method <ref type="bibr" target="#b19">[20]</ref>  performs well in synthesizing many fine details but there are some noticeable blurring details along dominant edges shown in Fig. <ref type="figure" target="#fig_2">3f</ref>. Our proposed MLM-1 method (Fig. <ref type="figure" target="#fig_2">3g</ref>) can produce visually comparable result to Zeyde's method. Moreover, the proposed MLM-2 algorithm further improves the result obtained from MLM-1. As demonstrated in Fig. <ref type="figure" target="#fig_2">3h</ref>, the enhanced result is more faithful to the original HR image in terms of finer details and sharper edges by exploiting repetitive patterns to suppress the unexpected artifacts that most example learning-based approaches unavoidably produce. To further validate our proposed SR algorithm we present similar comparisons of the methods on the Foreman image in Fig. <ref type="figure" target="#fig_3">4</ref>. SR using MLM-1 and -2 (Fig. <ref type="figure" target="#fig_3">4g</ref> and Fig. <ref type="figure" target="#fig_3">4h</ref>) produces sharper edges and more faithful details with minimal artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison on Real-World LR Images</head><p>We next carry out comparative SR experiments on two representative real-world LR benchmarks, namely the Chip (with 244Ã—200 pixels) and Hatc (with 133Ã—174 pixels) images (shown in Figs. <ref type="figure" target="#fig_4">5</ref> and<ref type="figure" target="#fig_5">6</ref>, respectively). Chip contains many edges while Hatc possesses rich textural details. A comparison of the magnified letters in the red insets shows that both MLM-1 and MLM-2 produce images of better quality with minimal artifacts and, by comparison of the green inserts, sharper SR of dominant edges. Displeasing artifacts along edges are particularly reduced by the following enhancement.</p><p>A comparison of different SR approaches for restoring the textural details of Hatc is shown in Fig. <ref type="figure" target="#fig_5">6</ref>. Bi-cubic interpolation is most visually displeasing while the example learning-based approaches, including our own, show promise for reconstructing visual details. Nevertheless, both MLM-1 and -2 (Fig. <ref type="figure" target="#fig_5">6g</ref> and Fig. <ref type="figure" target="#fig_5">6h</ref>) are superior to the other methods (see Fig. <ref type="figure" target="#fig_5">6b-6f</ref>), because the multiple mapping functions statistically learned from different feature subspaces better represent the various structures in generic natural images. Our method also outperforms the others in terms of  computational complexity and reconstruction quality, crucial for resource-limited applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis of Time and Space Complexity</head><p>For a fair evaluation, these comparisons are conducted using ANR <ref type="bibr" target="#b30">[31]</ref>, Yang's <ref type="bibr" target="#b31">[32]</ref>, SC-based <ref type="bibr" target="#b18">[19]</ref>, Zeyde's method in <ref type="bibr" target="#b19">[20]</ref>, and our MLM-1 method ignorant of training cost. Without loss of generality, let M be the number of LR image patches to be super-resolved in the LR input, p be the dimensionality of the original representation of LR features, m be the reduced dimensionality of original LR features, n be the dimensionality of the HR feature vectors to be predicted, and K be the number of mapping functions. To perform  a concrete comparison of the time complexity, we consider the differences of feature representation used in these competitors.</p><p>For ANR <ref type="bibr" target="#b30">[31]</ref>, Zeyde's method <ref type="bibr" target="#b19">[20]</ref>, and our method, the same feature representation is adopted to represent each LR image patch (324D the first-and second-order gradients) and the same p Ã—m-dimensional PCA base matrix ( p = 324, m = 30) is trained for dimensionality reduction. In Yang's method, 7 Ã— 7 LR patch neglected of the four corners is extracted to represent a 45D LR feature vector ( p = 45) and 12 Ã— 12 central regions of the corresponding HR patch is set to 144D LR feature vector (n = 144). In the learning stage, the numbers of mapping functions established by ANR-based <ref type="bibr" target="#b30">[31]</ref>, Yang's <ref type="bibr" target="#b31">[32]</ref>, and the proposed methods are 2048, 4096, and 350, respectively. The major procedures involved in our method include: 1) selecting the most matched mapping function for each input LR feature using the nearest neighbor searching algorithm, which in total takes about O(Mpm 2 K )( p = 324, m = 30, K = 350); 2) converting all the LR features to the desired HR features, which takes about O(Mmn)(m = 30, n = 81); and 3) merging all the obtained HR image patches into the HR image, which takes about O(M). Both the ANR-based method and Yang's method take a similar order of time to MLM-1, except that both methods require many more mapping functions to achieve a competitive result. In terms of the aforementioned feature representation, two methods take about O(Mpm to transform all the LR features to the desired HR features and about O(M) to merge all the reconstructed HR features into the HR image, respectively. As such, these two methods not only take more time to match the best mapping function but also need a lot of extra memory to store the mapping models. In contrast, the MLM-based method achieves competitive results using fewer mapping functions (350 used in all experiments), which is particularly significant in practical applications. The SC-based method needs to solve a least squares optimization with 1 -norm regularity, and performing a quantitative analysis of the time complexity of an iterative convex optimization is difficult since it depends on a convergence criterion. Moreover, we observe that this method is consistently most time consuming. Zeyde's method improves computational complexity due to several modifications: dimensionality reduction is performed with PCA and the OMP algorithm <ref type="bibr" target="#b45">[46]</ref> is used for sparse coding, which take about O(Mpm) and O(2mq L M + 2q 2 m M + 2q(L + m)M + q 3 M) for all the image patches, respectively, where q is the sparsity constant, m is the dimensionality of the LR dictionary atoms, and L is the number of atoms in the LR dictionary.</p><p>Memory utilization is another important evaluation criterion for assessing the practical applicability of different SR methods. We consider all the variables recorded as single precision quantities. For MLM, the size of each LR image patch is 9Ã—9 and it is represented as a 324D feature vector. Correspondingly, each HR image patch is an 81D feature vector. We use a 324Ã—30D PCA transformation matrix for dimensionality reduction. Each mapping function is 30Ã—81 and the number of the learned mapping functions is 350. In total, our MLM-based algorithm requires additional memory since (324 Ã— 30 + 30 Ã— 81 Ã— 350 + 350) Ã— 4(bytes)/ (1024 Ã— 1024) = 3.28 (MB). The ANR-based method uses the same feature representation as Zeyde's method, but 2048 LR dictionary atoms and 2048 mapping functions are required for memory allocation. In total, ANR needs Algorithm 2 requires additional time and space for SR enhancement. Suppose the number of pixels in the desired HR image is N 2 , the size of searching window is bÃ—b (b = 2r +1), and the size of the image patch for calculating the similarity weights is c Ã— c. Although the NLM-based SR enhancement step is similar to our previous method <ref type="bibr" target="#b28">[29]</ref>, the newly proposed method advocates applying a fast NLM-based algorithm to compute similar weights, and thus the proposed NLM-based enhancement algorithm can significantly reduce the computational complexity. Compared to our method presented in <ref type="bibr" target="#b28">[29]</ref> that takes about O(b 2 c 2 N 2 ) to construct the regularization, Algorithm 2 has a much lower time complexity of O(4b 2 N 2 ) (about an order of O(c 2 ) improvement) but requires three additional images to store the partial immediate results.</p><p>We further compare the computational efficiency of the different methods. All the comparative experiments are performed using Matlab implementation on a 1.8GHz Intel Core i5 CPU with 4GB RAM. Fig. <ref type="figure" target="#fig_7">7</ref> shows the base 10 logarithmic CPU time taken to process the test images shown in Table <ref type="table" target="#tab_0">I</ref>. MLM-1 is consistently quicker than the other methods, despite efficient implementation of the compiled OMP MEX-function used in Zeyde's method. Furthermore, the running time of MLM-2 is still significantly lower than both the ANR-and SC-based methods.</p><p>To validate the efficiency of both MLM-1 and MLM-2 algorithms, Table <ref type="table" target="#tab_0">II</ref> evaluates the RMSEs and actual CPU time of three test images achieved on the same computational environment as the results in Fig. <ref type="figure" target="#fig_7">7</ref>, all with magnification factor  of 3. Since the radius of searching window affects the recovery quality and computational time when applying the MLM-2 algorithm for enhancement, we vary from 1 to 4 to test the performance. From Table <ref type="table" target="#tab_0">II</ref> we can see that the MLM-1 algorithm can super-resolve an image of medium-size (96 Ã— 117) to the factor of 3 in less 2 seconds. While MLM-2 algorithm also can further enhance the result of MLM-1 in less 10 seconds when a moderate size of searching window (r = 2) is applied to match similar local image patches. In particular, the computational time can be remarkably reduced by replacing Matlab simulation with an efficient C implementation. Our proposed method delivers superior SR recovery while maintaining lower time and space complexity and is therefore suitable for practical applications in resource-constrained systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Effects on Number of Mapping Functions</head><p>In the SR experiments performed on synthetic LR images we found that different numbers of mapping functions affect both reconstruction quality and computational cost. For efficient applications in resource-limited systems SR reconstruction needs to be fast but not compromise quality.</p><p>To achieve a reasonable selection on the mapping functions, fourteen SR models were built by changing the clustering number from 50 to 700 at intervals of 50 on the same training dataset. The averaged PSNRs and SSIMs of the twenty test images presented in Table I are quantitatively compared (Fig. <ref type="figure" target="#fig_8">8</ref>). More numbers of mapping functions are beneficial for SR quality, but when the number is greater than 350 the quality starts to slightly decrease. To benefit efficient SR applications we therefore suggest selecting medium-scale mapping functions to optimize the tradeoff between reconstruction quality and computational cost.</p><p>We further examine the effect of different numbers of mapping functions <ref type="bibr" target="#b49">(50,</ref><ref type="bibr">200,</ref><ref type="bibr">350</ref>, and 1000) on visual quality. SR (Ã—3) of an HR Monarch image downloaded from the internet is shown in Fig. <ref type="figure" target="#fig_9">9</ref>. Although there are no observable differences between the SR results, too few mapping functions (50 or 200) are not comparable to those using more mapping functions (350 or 1000), and simply increasing the number of mapping functions does not necessarily improve quality. The enhanced result (Fig. <ref type="figure" target="#fig_9">9f</ref>) produces better quality with fewer artifacts along major edges and in textural regions than those without any enhancement. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Influence on Fast NLM-Based Enhancement</head><p>In Section IV-A, both quantitative and qualitative results of MLM-2 show that our fast NLM-based enhancement algorithm can remarkably and further suppress unpleasing artifacts generated by our MLM-1 algorithm. The proposed SR enhancement algorithm can also be applied to improve To further demonstrate the capability of our enhancement algorithm, comparisons of Butterfly image are shown in Fig. <ref type="figure" target="#fig_10">10</ref>. We can see SR results without applying any enhancement algorithm appear to be serious jaggy artifacts along dominant edges and annoying details in textural regions (at the first row in Fig. <ref type="figure" target="#fig_10">10</ref>). Although the back-projection algorithm slightly reduces some aliased artifacts in resultant HR images, the noticeable artifacts remain along dominant edges and in textural regions (at the second row in Fig. <ref type="figure" target="#fig_10">10</ref>). By contrast, our enhancement method can significantly improve the outcomes of four methods because both sharper edges and finer details with minimal artifact can be reflected from the dominant edges in Butterfly image (at the last row in Fig. <ref type="figure" target="#fig_10">10</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper we present a novel efficient single image SR method for generic images based on learning a cluster of mapping relationships between the LR and HR feature subspaces. The learned mapping functions effectively and efficiently transform the input image into the expected HR image. Furthermore, we propose a fast yet effective NLM-based SR enhancement algorithm for reducing edge artifacts by exploiting similarity structures in the resultant image. Experimental results indicate that our approach is quantitatively and qualitatively superior to other applicationoriented SR methods, while maintaining relatively low time and space complexity.</p><p>Although our method shows potential for computationally efficient SR applications, several aspects need to be considered in our future research. First, a progressive SR scheme (similar to <ref type="bibr" target="#b27">[28]</ref>) can be adopted in our SR framework for more stable mapping between two cross-scale image spaces. Second, selective patch processing (similar to <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref>), where highaccuracy SR recovery is selectively applied to salient regions while simple interpolation is used in unimportant regions, can be used to further improve processing efficiency. Third, more sophisticated statistical learning algorithms [50]- <ref type="bibr" target="#b51">[52]</ref> could be used to simultaneously learn multiple subspaces and multiple mapping relationships in order to construct more effective LR-HR mapping models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Pipeline of our multiple linear mappings learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of computing the sum of squared difference (SSD) in the region R 4 .</figDesc><graphic coords="6,50.15,58.97,248.78,146.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Comparison of SR results (Ã—3) on Butterfly image. (a) Original HR image. (b) Bi-cubic. (c) ANR [31]. (d) Yang [32]. (e) SC-based [19]. (f) Zeyde [20]. (g) MLM-1. (h) MLM-2. Please refer to the electronical version and zoom in for obvious comparison.</figDesc><graphic coords="9,48.95,383.45,126.38,103.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of SR results (Ã—3) on Foreman image. (a) Original HR image. (b) Bi-cubic. (c) ANR [31]. (d) Yang [32]. (e) SC-based [19]. (f) Zeyde [20]. (g) MLM-1. (h) MLM-2. Please refer to the electronical version and zoom in for obvious comparison.</figDesc><graphic coords="9,48.95,503.21,126.38,103.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Comparison of SR results (Ã—3) on Chip image. (a) Original LR image. (b) Bi-cubic. (c) ANR [31]. (d) Yang [32]. (e) SC-based [19]. (f) Zeyde [20]. (g) MLM-1. (h) MLM-2. Please refer to the electronical version and zoom in for obvious comparison.</figDesc><graphic coords="10,186.35,327.17,118.94,155.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Comparison of SR results (Ã—3) on Hatc image. (a) Original LR image. (b) Bi-cubic. (c) ANR [31]. (d) Yang [32]. (e) SC-based (f) Zeyde [20]. (g) MLM-1. (h) MLM-2. Please refer to the electronical version and zoom in for obvious comparison.</figDesc><graphic coords="10,65.99,499.49,118.82,148.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2 K ) ( p = 324, m = 30, K = 2048) and O(Mp 2 K )( p = 45, K = 4096) to match the best mapping function, about O(Mmn)(m = 30, n = 81) and O(Mpn)( p = 45, n = 144)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Runtime comparisons of test images shown in Table I. (a) Runtime comparisons of the first ten images. (b) Runtime comparisons of the other ten images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Averaging performance comparisons on PSNR and SSIM scores versus different numbers of mapping functions. (a) Averaging PSNR versus the number of mapping functions. (b) Averaging SSIM versus the number of mapping functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Comparison of SR results (Ã—3) on Monarch image. (a) Original HR image. (b) Result of mapping function size 50. (c) Result of mapping function size 200. (d) Result of mapping function size 350. (e) Result of mapping function size 1000. (f) Enhanced result of mapping function size 350.</figDesc><graphic coords="13,221.39,191.69,169.10,112.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Comparisons of enhanced results (Ã—3) on Butterfly. (a) Bi-cubic interpolation. (b) SC-based method [19]. (c) Zeyde's method [20]. (d) MLM-based method. (e) Result (a) enhanced by back-projection. (f) Result (b) enhanced by back-projection. (g) Result (c) enhanced by back-projection. (h) Result (d) enhanced by back-projection. (i) Result (a) enhanced by proposed fast NLM-based method. (j) Result (b) enhanced by proposed fast NLM-based method. (k) Result (c) enhanced by proposed fast NLM-based method. (l) Result (d) enhanced by proposed fast NLM-based method.</figDesc><graphic coords="14,48.95,398.93,126.38,72.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PSNR</head><label>I</label><figDesc>(DECIBELS) AND SSIM RESULTS OF TWENTY COLOR TEST IMAGES FOR 3Ã— MAGNIFICATION FACTOR. FOR EACH IMAGE, WE HAVE TWO ROWS. THE FIRST ROW IS PSNR AND THE SECOND ONE IS SSIM</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1000 and 81 Ã— 1000 dictionaries for SR reconstruction. The total memory is therefore (324 Ã— 30 + 30 Ã— 1000 + 81 Ã— 1000) Ã— 4(bytes)/(1024 Ã— 1024) = 0.46 (MB). According to this analysis, although our method is third in terms of extra memory allocation, it remains more acceptable than the others when runtime and reconstruction quality are considered.</figDesc><table><row><cell>TABLE II</cell></row><row><cell>SR RECONSTRUCTION ACCURACY AND PROCESSING TIME</cell></row><row><cell>OF SR RECOVERY (Ã—3) TESTED ON THREE TEST</cell></row><row><cell>IMAGES WITH DIFFERENT SIZES</cell></row><row><cell>(324 Ã— 30 + 30 Ã— 81 Ã— 2048 + 30 Ã— 2048) Ã— 4(bytes)/</cell></row><row><cell>(1024 Ã— 1024) = 19.25 (MB). In Yang's method the size</cell></row><row><cell>of an LR image patch is a 45D feature vector, the number</cell></row><row><cell>of mapping functions is 4096, and each mapping function</cell></row><row><cell>is a 45Ã—121D transformation matrix; the total memory is</cell></row><row><cell>(45 Ã— 121 Ã— 4096 + 45 Ã— 4096) Ã— 4(bytes)/(1024 Ã— 1024) =</cell></row><row><cell>85.78 (MB). In the SC-based method, the total memory needed</cell></row><row><cell>for a 144Ã—1024 LR dictionary and an 81Ã—1024 HR dictionary</cell></row><row><cell>is (144 Ã— 1024 + 81 Ã— 1024) Ã— 4(bytes)/(1024 Ã— 1024) =</cell></row><row><cell>0.87 (MB). In Zeyde's method, there is a 324Ã—30D PCA</cell></row><row><cell>transformation matrix for dimensionality reduction and two</cell></row><row><cell>30 Ã—</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was supported in part by the National Basic Research Program (973 Program) of China under Grant 2012CB316400, in part by the National Natural Science Foundation of China under Grant 61471161, Grant 61125204, Grant 61432014, Grant 61172146, Grant 61125106, and Grant 61370092, in part by the Program for Changjiang Scholars and Innovative Research Team in University of China under Grant IRT13088, in part by the China Post-Doctoral Science Foundation under Grant 2013M540734 and Grant 2014T70905, in part by the Australian Research Council Projects under DP-140102164, FT-130101457, and LP-140100569, and in part by the Hubei Provincial Department of Education Outstanding Youth Scientific Innovation Team Support Foundation under Grant T201410. The associate editor coordinating the review of this manuscript and approving it for publication was Prof.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-frame image restoration and registration</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Y</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Comput. Vis. Image Process</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="317" to="339" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981-12">Dec. 1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">New edge-directed interpolation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Orchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1521" to="1527" />
			<date>Oct</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An edge-guided image interpolation algorithm via directional filtering and data fusion</title>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2226" to="2238" />
			<date type="published" when="2006-08">Aug. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kernel regression for image processing and reconstruction</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="349" to="366" />
			<date type="published" when="2007-02">Feb. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image upsampling via imposed edge statistics</title>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007-07">Jul. 2007</date>
			<pubPlace>Art</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-constrained hallucination for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2010-06">Jun. 2010</date>
			<biblScope unit="page" from="231" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image super-resolution with detail enhancement based on local fractal analysis of gradient</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1740" to="1754" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Edge-directed single-image super-resolution via adaptive gradient magnitude selfinterpolation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1289" to="1299" />
			<date type="published" when="2013-08">Aug. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Single image super-resolution with non-local means and steering kernel regression</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4544" to="4556" />
			<date type="published" when="2012-11">Nov. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution via nonlocal steering kernel regression regularization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 20th IEEE Conf. Image Process</title>
		<meeting>20th IEEE Conf. Image ess</meeting>
		<imprint>
			<date type="published" when="2013-09">Sep. 2013</date>
			<biblScope unit="page" from="943" to="946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generalizing the nonlocal-means to super-resolution reconstruction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="36" to="51" />
			<date type="published" when="2009-01">Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Example-based superresolution</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Pasztor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2002-04">Mar./Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image hallucination with primal sketch priors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N.-N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2003-06">Jun. 2003</date>
			<biblScope unit="page" from="729" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2004-07">Jun./Jul. 2004</date>
			<biblScope unit="page" from="275" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Partially supervised neighbor embedding for example-based image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Sel. Topics Signal Process</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="230" to="239" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image super-resolution with sparse neighbor embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3194" to="3205" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image super-resolution as sparse representation of raw image patches</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-06">Jun. 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010-11">Nov. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparse-representations</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 7th Int. Conf. Curves Surf</title>
		<meeting>7th Int. Conf. Curves Surf</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Coupled dictionary training for image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3467" to="3478" />
			<date type="published" when="2012-08">Aug. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image transformation based on learning dictionaries across image spaces</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="380" />
			<date type="published" when="2013-02">Feb. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image superresolution using support vector regression</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1596" to="1610" />
			<date type="published" when="2007-06">Jun. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single-image super-resolution using sparse regression and natural image prior</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kwon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1127" to="1133" />
			<date type="published" when="2010-06">Jun. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image hallucination using neighbor embedding over visual primitive manifolds</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007-06">Jun. 2007</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast acquisition and reconstruction of optical coherence tomography images via sparse representation</title>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2034" to="2049" />
			<date type="published" when="2013-11">Nov. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Signal recovery from random measurements via orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tropp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4655" to="4666" />
			<date type="published" when="2007-12">Dec. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Super-resolution from a single image</title>
		<author>
			<persName><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2009-10">Oct. 2009</date>
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Single image super-resolution with multiscale similarity learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1648" to="1659" />
			<date type="published" when="2013-10">Oct. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-scale dictionary for single image super-resolution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2012-06">Jun. 2012</date>
			<biblScope unit="page" from="1114" to="1121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anchored neighborhood regression for fast example-based super-resolution</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="1920" to="1927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast direct super-resolution by simple functions</title>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-12">Dec. 2013</date>
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast image super-resolution based on in-place example regression</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013-06">Jun. 2013</date>
			<biblScope unit="page" from="1059" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast nonlocal filtering applied to electron cryomicroscopy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Darbon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cunha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">F</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 5th IEEE Symp</title>
		<meeting>5th IEEE Symp</meeting>
		<imprint>
			<date type="published" when="2008-05">May 2008</date>
			<biblScope unit="page" from="1331" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982-03">Mar. 1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The differentiation of pseudo-inverses and nonlinear least squares problems whose variables separate</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pereyra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Numer. Anal</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="413" to="432" />
			<date type="published" when="1973-04">Apr. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sparse representation or collaborative representation: Which helps face recognition?</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2011-11">Nov. 2011</date>
			<biblScope unit="page" from="471" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2005-06">Jun. 2005</date>
			<biblScope unit="page" from="60" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Zernike-moment-based image super resolution</title>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2738" to="2747" />
			<date type="published" when="2011-10">Oct. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A unified learning framework for single image super-resolution</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="780" to="792" />
			<date type="published" when="2014-04">Apr. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local regularization of inverse problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bougleux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008-10">Oct. 2008</date>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast and robust multiframe super resolution</title>
		<author>
			<persName><forename type="first">S</forename><surname>Farsiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1327" to="1344" />
			<date type="published" when="2004-10">Oct. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image and video restorations via nonlocal kernel regression</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1035" to="1046" />
			<date type="published" when="2013-06">Jun. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2001-07">Jul. 2001</date>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004-04">Apr. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient implementation of the K-SVD algorithm using batch orthogonal matching pursuit</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zibulevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<idno>Rep. CS-2008-08</idno>
	</analytic>
	<monogr>
		<title level="j">Dept. Comput. Sci., Technion-Israel Inst. Technol</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<pubPlace>Haifa, Israel, Tech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Advances in superresolution using L-curve</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lertrattanapanich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits Syst</title>
		<meeting>IEEE Int. Symp. Circuits Syst</meeting>
		<imprint>
			<date type="published" when="2001-05">May 2001</date>
			<biblScope unit="page" from="433" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A computationally efficient superresolution image reconstruction algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="583" />
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adaptive multiple-frame image super-resolution based on U-curve</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3157" to="3169" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multivariate multilinear regression</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst., Man, Cybern. B, Cybern</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1560" to="1573" />
			<date type="published" when="2012-12">Dec. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Ensemble manifold regularization</title>
		<author>
			<persName><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1227" to="1233" />
			<date type="published" when="2012-06">Jun. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Online nonnegative matrix factorization with robust stochastic approximation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1087" to="1099" />
			<date type="published" when="2012-07">Jul. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
