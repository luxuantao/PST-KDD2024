<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event-Based Neuromorphic Vision for Autonomous Driving A paradigm shift for bio-inspired visual sensing and perception A</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hu</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jörg</forename><surname>Conradt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Huajin</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Florian</forename><surname>Röhrbein</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alois</forename><surname>Knoll</surname></persName>
						</author>
						<author>
							<persName><forename type="first">©istockphoto</forename><surname>Com</surname></persName>
						</author>
						<author>
							<persName><forename type="first">/</forename><surname>Oonal</surname></persName>
						</author>
						<title level="a" type="main">Event-Based Neuromorphic Vision for Autonomous Driving A paradigm shift for bio-inspired visual sensing and perception A</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">113905BC0D0B3F5B101A4C031992B6EC</idno>
					<idno type="DOI">10.1109/MSP.2020.2985815</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.3" ident="GROBID" when="2023-07-28T11:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>s a bio-inspired and emerging sensor, an event-based neuromorphic vision sensor has a different working principle compared to the standard frame-based cameras, which leads to promising properties of low energy consumption, low latency, high dynamic range (HDR), and high temporal resolution. It poses a paradigm shift to sense and perceive the environment by capturing local pixel-level light intensity changes and producing asynchronous event streams. Advanced technologies for the visual sensing system of autonomous vehicles from standard computer vision to event-based neuromorphic vision have been developed. In this tutorial-like article, a comprehensive review of the emerging technology is given. First, the course of the development of the neuromorphic vision sensor that is derived from the understanding of biological retina is introduced. The signal processing techniques for event noise processing and event data representation are then discussed. Next, the signal processing algorithms and applications for event-based neuromorphic vision in autonomous driving and various assistance systems are reviewed. Finally, challenges and future research directions are pointed out. It is expected that this article will serve as a starting point for new researchers and engineers in the autonomous driving field and provide a bird's-eye view to both neuromorphic vision and autonomous driving research communities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Over the past few decades, the rapid development of electronics, information technologies, and artificial intelligence have made great progress in artificial visual sensing and perception systems. For example, the vision system of an autonomous vehicle becomes more intelligent by using deep learning technology. However, it still has some shortcomings compared with biological counterparts, such as the human and animal visual systems. Even small insects, such as bees, outperform the most advanced artificial vision systems such as high-quality cameras in routine functions, including real-time sensing and processing, low-latency motion control, and so on. More importantly, such biological neural systems can well perform tasks with small energy consumption. In fact, biological neural systems usually consist of a large number of relatively simple elements. They operate in a massively parallel principle, which is different from the most common type of vision sensors such as CMOS cameras. Thus, some researchers and engineers have tried to mimic the working principles of the biological visual systems and come up with a new artificial visual system.</p><p>Recently, the developments of material technologies, lithographic processes, very large-scale integration (VLSI) design techniques, neuroscience, and neuromorphic technologies have enabled the novel conception and fabrication of bio-inspired visual sensors and processors. These new sensors and processors provide different methods to sense and perceive the world. The eventbased neuromorphic vision sensor is such a bio-inspired vision sensor mimicking biological retina from both the system level and element level; it poses a paradigm shift in the way of visual information acquisition, processing, and modeling. The dynamic vision sensor (DVS) proposed by the group of Tobi Delbruck <ref type="bibr" target="#b0">[1]</ref> is the first practicable event-based neuromorphic vision sensor based on the biological principle. DVS captures the per-pixel brightness changes (called events) asynchronously instead of measuring the absolute brightness of all pixels at constant rate, resulting in promising properties compared to standard frame-based cameras, such as low power consumption and low latency (in the order of microsecond), HDR (120 dB), and high temporal resolution <ref type="bibr" target="#b1">[2]</ref>. Thus, an alternative visual sensing and perception system for autonomous vehicles is provided in challenging scenarios that state-of-the-art standard frame-based cameras cannot well perform <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, such as high-speed scenes of the autonomous highway driving, low latency of motion control, and low power consumption of the vehicle onboard system.</p><p>It is well known in the research of autonomous driving that radar, lidar, ultrasound, and cameras form the backbone of sensor systems of the autonomous vehicle <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. These sensors acquire the visual data as a sequence of snapshots recorded at discrete time stamps; therefore, visual information is compressed and quantized at a predefined frame rate. Consequently, a problem that is often known from the signal processing domain (undersampling) arises due to the timescale of motions in the observed scenes and the frame-rate of the recording camera. Things occurring between the adjacent frames, along with the consequent information, would get lost. Generally, the advanced algorithms with multiple-sensor fusion are usually developed to compensate single-sensor shortcomings in demanding applications such as highly piloted driving systems with low-latency motion control and visual feedback loops. Rather than solving this problem from an algorithmic perspective, it is better to explore alternative methods from a novel sensing perspective, such as event-based neuromorphic vision sensors. This results in providing great value for promoting subsequent tasks to become more robust, accurate, and complementary together with advanced algorithm development.</p><p>As an emerging sensing technology, the algorithms and applications of event-based neuromorphic vision are in the preliminary stage. Some works have been summarized in <ref type="bibr" target="#b7">[8]</ref>. Unlike <ref type="bibr" target="#b7">[8]</ref>, this article aims to provide a thorough overview of the event-based neuromorphic vision for autonomous driving, from a signal processing perspective with a focus on visual perception algorithms and applications (see Figure <ref type="figure">1</ref>). Specifically, the introduction starts from the operation principle of this bio-inspired neuromorphic vision sensor; then, the unique advantages of the sensor and its connection with the perception system of autonomous vehicles are discussed. Taking these promising properties into consideration, the signal processing techniques about event noise processing, event data representation, and meaningful event-based neuromorphic vision algorithms of given autonomous driving tasks are illustrated. Afterward, the works of event-based neuromorphic vision that are dedicated to specific applications in autonomous driving are reviewed. Finally, we address the problems remaining to be tackled and the directions for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bio-inspired vision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A biological retina</head><p>The retina of vertebrates, such as humans, is a highly developed multilayer neural system consisting of light sensitive cells which contain millions of photoreceptors. It is the place where the acquisition and preprocessing of the visual information happen. As shown in Figure <ref type="figure" target="#fig_1">2</ref>(c), the retina has three primary layers including the photoreceptor layer, the outer plexiform layer, and the inner plexiform layer.</p><p>The photoreceptor layer consists of light-sensitive cells that convert incoming light into electrical signals and drive the horizontal cells and bipolar cells in the outer plexiform layer. There are two major types of bipolar cells: ON-and OFF-bipolar cells. The ON-and OFF-bipolar cells are responsible for coding the bright and dark spatial-temporal contrast changes, respectively. Particularly, the firing rate of the ON-bipolar cells will increase while the OFF-bipolar cells will no longer generate spikes if the illumination is increasing. This, in turn, increases the firing rate of OFF-bipolar cells in the case of illumination decreasing (such as getting darker). In the absence of a light stimulus, both cells generate few random spikes. This phenomenon is achieved by comparing the photoreceptor's signals with the spatial-temporal values, which are determined by the mean value of the horizontal cells, facilitating the connection between photoreceptors and bipolar cells laterally. In the outer plexiform layer, the ON-and OFF-bipolar cells synapse onto the amacrine cells and ON-and OFF-ganglion cells in the inner plexiform layer. The amacrine cells mediate signal transmission between bipolar cells and ganglion cells. The ganglion cells carry information along with different parallel pathways in the retina, which is conveyed to the visual cortex. Thus, the retina is responsible for converting spatial-temporal As an emerging sensing technology, the algorithms and applications of eventbased neuromorphic vision are in the preliminary stage. illumination changes into pulses, which is transmitted to the visual cortex via the optic nerve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Silicon retina</head><p>Silicon retinas are visuals that model the biological retina and follow neurobiological principles. Pioneers of silicon retinas are Mahowald and Mead, who introduced their silicon VLSI retina in 1991 <ref type="bibr" target="#b8">[9]</ref>. This kind of sensor is equipped with adaptable photoreceptors and a chip with a 2D hexagonal grid of pixels. It replicates parts of cell types of biological retinas, including the photoreceptors, bipolar cells, and horizontal cells. Therefore, this kind of sensor represents merely the photoreceptor layer and the outer plexiform layer. Later, Zaghloul and Boahen built the Parvo-Magno retina, which is superior to the silicon VLSI retina, by modeling five retina layers.</p><p>Despite the promising structure, many of the early silicon retinas originate from the biological sciences community and are mainly used to demonstrate neurobiological models and theories without considering real-world applications. Recently, an increasing amount of effort from Tobi Delbruck's team has been put into the development of practicable silicon retina DVS based on biological principles <ref type="bibr" target="#b0">[1]</ref>. In Figure <ref type="figure" target="#fig_1">2</ref> </p><formula xml:id="formula_0">(c) (d) (e) (f) (g) (h) (i) (j) (k) (l) FIGURE 1.</formula><p>An overview of event-based neuromorphic vision sensors for autonomous driving, with representative examples for emerging systems and applications: (a) tracking (adapted from <ref type="bibr" target="#b21">[22]</ref>), (b) optical flow (adapted from <ref type="bibr" target="#b28">[29]</ref>), (c) depth estimation (adapted from <ref type="bibr" target="#b28">[29]</ref>), (d) object detection (adapted from <ref type="bibr" target="#b51">[52]</ref>), (e) semantic segmentation (adapted from <ref type="bibr" target="#b34">[35]</ref>), (f) steering prediction (adapted from <ref type="bibr" target="#b2">[3]</ref>), (g) image reconstruction (IR) (adapted from <ref type="bibr" target="#b44">[45]</ref>), (h) panoramic stereo vision (adapted from <ref type="bibr" target="#b46">[47]</ref>), (i) DET data set (adapted from <ref type="bibr" target="#b15">[16]</ref>), (j) DDD17 data set <ref type="bibr" target="#b17">[18]</ref> (adapted from <ref type="bibr" target="#b2">[3]</ref>), (k) N-Cars data set (adapted from <ref type="bibr" target="#b16">[17]</ref>), and (l) MVSEC data set (adapted from <ref type="bibr" target="#b3">[4]</ref>).</p><p>the photocurrent through the photoreceptor layer circuit. The outer plexiform layer circuit responds with spike events ( ) vdiff of different polarities to positive and negative changes of the photocurrent. Spikes are transported to the next processing stage by the inner plexiform layer circuit. A large number of log-intensity changes are encoded in the events. Figure <ref type="figure" target="#fig_1">2(d)</ref> illustrates the accumulated events including ON event (illumination increased) and OFF event (illumination decreased) that are drawn as white and black dots.</p><p>Today's representatives of silicon retinas are mainly from pioneers Tobi Delbruck and Christoph Posch and represent a compromise between biological and technical aspects. In their development, one prominent challenge posed is usually regarded as a wiring problem, indicating that each pixel of the silicon retina needs its own cable, which is impossible for chip wiring. A key technique for the solution, named address event representation (AER) was originally from the Caltech group of Carver Mead; it is used as an event-controlled and asynchronous point-to-point communication protocol for prototypes of the silicon retina.</p><p>As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, the basic functionality of AER is implemented by an address encoder (AE), an address decoder (AD), and a digital bus. All neurons and pixels could transmit the time-coded information on the same line because the digital bus implements a multiplex strategy. The AE of the sending chip generates a unique binary address for each neuron or pixel in case of a change. The bus transmits the address at high speed to the receiver chip. Then, the AD determines the position and generates a spike on the receiver neuron. Event streams are employed in AER to communicate among chips. An event is a tuple ( , , , );</p><p>x y t p x and y are pixel addresses; t is the time stamp; and p represents the polarity. The polarity indicates the increase and decrease in the lighting intensity, corresponding to an ON event and OFF event, respectively.</p><p>This article focuses mainly on the first practically usable silicon retina, the DVS, which follows the natural, frame-free, and event-driven approach that triggers a plethora of research in event-based neuromorphic vision and autonomous driving.</p><p>[A recent approach by Tobi Delbruck is the so-called dynamic and active pixel vision sensor (DAVIS) that combines dynamic and static visual information into a single pixel.] The DVS pixel models a simplified three-layer biological retina by mimicking the information flow of the photoreceptor-bipolar-ganglion cells (see Figure <ref type="figure" target="#fig_1">2</ref>). Pixels operate independently and attach special importance to the temporal development of the local lighting intensity. The DVS pixel would automatically trigger an event (either ON event or OFF event) when the relative change in intensity exceeds the threshold. Therefore, the working principle of the DVS is fundamentally different from the  frame-based camera. There are three key properties of biological vision that are kept in this silicon retina: the relative illumination change, the sparse event data, and the separate output channels (ON/OFF). The major consequence of the DVS is that the acquisition of visual information is no longer controlled by any form of external timing signals such as frame clock or shutter, while the pixel itself controls its own visual information individually and autonomously.</p><formula xml:id="formula_1">I ph V log A b C1 C2 A V diff Reset Switch</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Advantages of bio-inspired vision sensors</head><p>Due to the fundamentally different working principle and the mimicking of the biological retina, the event-based neuromorphic vision sensors have several advantages over standard frame-based cameras.</p><p>■ Energy-friendly properties: Since event-based neuromorphic vision sensor transmits only events and autonomously filter redundant data, power is only used to process active pixels (e.g., the events triggered by illumination changes).</p><p>Particularly, an energy-friendly sensor is more important than advanced algorithms for the onboard computers and devices in autonomous vehicles. ■ Low latency: There is no need for the global exposure of the frame because each pixel works independently. Ideally, the minimal latency is 10 s.</p><p>n The low-latency control of the autonomous vehicle is highly dependent on the perception systems. A low-latency perception system such as an object-detection system based on an event-based neuromorphic vision sensor would save lots of time in avoiding obstacles for the control systems. ■ HDR: The event-based neuromorphic vision sensor such as DVS has an HDR (120 dB), which far exceeds that of the frame-based cameras (60 dB). Event-based neuromorphic vision sensors such as the DVS can simultaneously adapt to very dark and bright stimuli ensuring a highly robust perception system even in a light-changing scene such as an autonomous vehicle driving through a tunnel.</p><p>■ Microsecond resolution: The brightness changes can be captured quickly in analog circuity. With a 1-MHz clock, events can be detected and time-stamped with microsecond resolution. Considering the fast response requirement of the controller in autonomous vehicles in emergency driving scenes, this property is quite useful in autonomous driving. ■ No motion blur: In the high-speed driving scenario, the motion blur problem occurs when the motion of the moving objects is beyond the sampling frequency of the framebased camera; this may cause the failure of the perception system. An event-based neuromorphic vision sensor can capture dynamic motion precisely with no motion blur; it is of great value to autonomous driving community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event noise processing</head><p>The preprocessing of the raw data is essential for extracting meaningful information for sensor systems. An eventbased neuromorphic vision sensor not only captures the change in the light intensity caused by moving objects, it also generates some noise activities due to the movements of background objects and the sensor noise such as temporal noise and junction leakage currents <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, the event noise processing technique is responsible for excluding the event noises from the event stream. Two commonly used methods in the literature, namely the spatial-temporal correlation filter and the motion consistency filter, are illustrated as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-temporal correlation filter</head><p>For a newly incoming event ( , , , ), e x y t p i i i i i = the spatial-temporal filter searches the most recent neighborhood event around the current pixel location ( , )</p><p>x y i i within a distance D. The incoming event would be regarded as a nonnoise event if the time difference meets:</p><formula xml:id="formula_2">, t t d i n t 1 -<label>(1)</label></formula><p>Transmitter Receiver where ti is the time stamp of the event; tn is the time stamp of the most recent neighborhood event; and dt is the predefined threshold. The search for the most recent event checks eight neighborhood pixels around ( , ), x y i i as shown in Figure <ref type="figure" target="#fig_3">4</ref>. It lacks temporal correlation with events in their spatial neighborhood because the event noise occurs randomly. Hence, the spatial-temporal correlation filter can effectively filter out event noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Motion consistency filter</head><p>In Figure <ref type="figure" target="#fig_3">4</ref>, the principle of the motion consistency filter <ref type="bibr" target="#b12">[13]</ref> is depicted. The blue dot denotes an incoming event caused by the object motion and the black dot represents an event noise. In the spatial-temporal domain, a newly incoming event should be consistent with the previous events (represented by red dots) caused by the same moving object. In a local region, the incoming event can be modeled as a consistent "moving plane" M. In this way, the velocity ( , ), v v x y can be used to assess the motion consistency, and the event noise can be removed because the previous events (the red dots, signal) and the black dot are not on the same plane. Concretely, the motion consistency plane for each active event ei can be formulated as , ax by ct d 0</p><formula xml:id="formula_3">i i i + + + =<label>(2)</label></formula><p>where ( , , , )</p><formula xml:id="formula_4">a b c d R 4 ! defines the plane M; ( , ) x y i i is the coordinate of event ;</formula><p>ei and ti is the time stamp of event . ei The event noise processing is an essential step to extract useful information from unwanted noise data for bioinspired visual sensing and perception tasks of autonomous driving; it can promote the accuracy and speed of subsequent algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event data representation</head><p>As an emerging sensing modality, event-based neuromorphic vision sensors only transmit local pixel-level changes caused by movement or light intensity change in a scene. The output data are sparse and asynchronous event streams which cannot be directly processed by standard vision pipelines, such as convolutional neural network (CNN)-based architecture. Therefore, encoding methods are utilized to convert asynchronous events into synchronous image-or grid-like representations for subsequent tasks such as object detection and tracking. According to whether or not the methods contain temporal information in the converted representations, we introduce two state-of-the-art encoding methods: spatial encoding and spatial-temporal encoding methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial encoding</head><p>The spatial encoding methods convert event streams into event frames by storing event data at pixel location ( , )</p><p>x y i i with either fixed-time interval (e.g., 30 ms, constant time frame) or fixed number of events (e.g., 500 events, constant count frame). For an event frame, the value of the pixel is usually represented by the polarity of the last event (the positive event is 1 and the negative event is -1) or the statistical characteristics (such as the event count in a fixed-time interval, event count frame) of the events in the fixed interval. Assuming that ( , , , ) e x y t p [ , ]</p><formula xml:id="formula_5">i i i i i i N 1 !</formula><p>represents event stream, typical approaches based on spatial encoding can be defined as follows: 1) Constant time frames:</p><formula xml:id="formula_6">( ( ) ) , F e T j t T j 1 card j t i i $ $ ; # # = -<label>(3)</label></formula><p>where F j t represents the jth frame of time interval T; card() is the cardinality of a set; and ei is the ith event of the event stream. 2) Constant count frames:</p><formula xml:id="formula_7">( ( ) ) . F e E j i E j 1 card j e i $ $ ; # # = -<label>(4)</label></formula><p>The constant count frame is defined similarly to constant time frame. F j e is the jth frame that contains E events. 3) Event count frames:</p><formula xml:id="formula_8">. ( , ) ( , ) Hist x y x x y y , p t T i i 1 i i d = - - ! + =+ /<label>(5)</label></formula><p>Two separate histograms for positive and negative events are generated in a fixed-time interval T. ( , ) Hist x y + denotes the histogram for positive events, where d is the Kronecker delta function. The same goes for the negative-events histogram, which is represented by Hist -with . p 1 i = -The final representation of the events in the fixed-time interval T is an event frame, which consists of two histograms Hist + and , Hist -as shown in Figure <ref type="figure" target="#fig_4">5</ref>. Since the principle of the spatial encoding method is to project the events onto the spatial plane ( , x y plane)</p><p>it loses the temporal information of all of the events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial-temporal encoding</head><p>The microsecond temporal resolution of the event stream provides a highly precise recording and description of the scene dynamics, which is valuable in many perception tasks such as high-speed moving object detection (e.g., vehicles). Spatialtemporal encoding methods combine spatial and temporal information of the events and convert events into a compact representation. A comparison of spatial-temporal encoding methods is presented in Table <ref type="table" target="#tab_1">1</ref>. A detailed description of these methods is displayed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Surface of active events</head><p>The surface of active events (SAE) uses time-stamp values instead of intensity values to represent the pixel values. For each incoming event : ei</p><formula xml:id="formula_9">: ( , ), SAE t P x y i i i 7 (<label>6</label></formula><formula xml:id="formula_10">)</formula><p>where ti is the time stamp of the most recent event at each pixel, the pixel value P at ( , )</p><p>x y i i is directly determined by the occurrence time of the events. The disadvantage of the SAE method is that it completely ignores the information of previous events happening at <ref type="bibr">( , )</ref> x y i i and only uses the time stamp of the most recent event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leaky integrate-and-fire</head><p>Leaky integrate-and-fire (LIF) is an artificial neuron inspired by biological perception principles and computation primitives. A neuron receives input spikes (events) generated from a DVS, which modifies its membrane potential. If the membrane potential exceeds a predefined threshold, a spike stimulus will be sent to the output. The LIF neuron can be modeled as   </p><formula xml:id="formula_11">( ) ) ( ), dt dV V t V RI t reset x = - - +<label>(7)</label></formula><p>where, V(t) is the membrane potential, which is a function across time; I(t) is the total synaptic current; R is the membrane resistance; and x is the membrane time constant. The neuron fires (produces an output spike) when the membrane potential reaches the threshold voltage ( ) Vth and then resets to reset voltage (</p><p>). Vreset As shown in Figure <ref type="figure">6</ref>, the spatial-temporal events are encoded by an LIF neuron, in which each event updates membrane potential of the neuron and the final converted representation is composed of the output spikes. An LIF neuron can not only transform event data into representation, it also serves as the basic unit of a spiking neural network (SNN) (see the section "SNNs").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Voxel grid</head><p>Voxel grid is a novel event representation aiming to improve the resolution of event stream in the temporal domain. Given a set of N events ( , , , ) , x y t p</p><formula xml:id="formula_12">[ , ] i i i i i N 1 !</formula><p>B bins are used to split the time dimension; then, the time stamps of events are scaled to the range of <ref type="bibr">[ , ]</ref>.</p><formula xml:id="formula_13">B 0 1 -</formula><p>The event voxel grid is defined as</p><formula xml:id="formula_14">/ ( )( ) ( ), t B t t t t 1 i N 1 1 = - - - t<label>(8) ( , , ) ( ) ( ) ( )</label></formula><formula xml:id="formula_15">, V x y t p k x x k y y k t t i i N i i = - - -t / (9) ( ) ( , ), max k z z 0 1 ; ; = -<label>(10)</label></formula><p>where, k(z) the trilinear voting kernel, which is equivalent to the definition in <ref type="bibr" target="#b13">[14]</ref>. As shown in Figure <ref type="figure">7</ref>, events are converted into voxel grid representation with the fixed kernel. This representation retains the distribution of the events across the spatial-temporal dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event spike tensor</head><p>Event spike tensor (EST) is an end-to-end learned representation <ref type="bibr" target="#b14">[15]</ref>. In a given time interval  where, ( , , ) f x y t i i i ! is a measurement assigned to each event to represent the corresponding intensity value at the pixel location; kc is the kernel convolution function to derive meaningful signal from the event stream. Generally, both the measurement and kernel are handcrafted functions in previous works, as illustrated in Figure <ref type="figure">7</ref>. Particularly, the EST deploys a multilayer perception replacing the handcrafted kernel function in <ref type="bibr" target="#b10">(11)</ref> to fit the data with the purpose of finding the best function for event streams. Simultaneously, the measurement function is chosen from a set of fixed functions. Examples of such function are the event polarity ;</p><formula xml:id="formula_16">f 1 ! = ! the event count ; f 1 = ! the time stamp ; f t = ! and the normalized time stamp ( )/ . f t t T 0 = - ! Event-based neuromorphic</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vision algorithms and applications of autonomous driving</head><p>The fundamental algorithms are the basis of the perception system of autonomous driving. For emerging systems and applications of bio-inspired vision, event-based neuromorphic vision algorithms are designed to extract features from event streams to fulfill given tasks. These methods can run directly on the event stream or take event representations as input (see the section "Event Data Representation"). They have been applied successfully in many vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event-based data sets of autonomous driving</head><p>In recent years, researchers have started to investigate the usage of event-based neuromorphic vision sensor such as DVS and DAVIS in the visual sensing and perception system of the autonomous driving system. There are many data sets that are built to promote the research of event-based neuromorphic vision, neurorobotics, and autonomous vehicles. In this section, four public event-based data sets dedicated to autonomous driving are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DET data sets</head><p>The performance of conventional lane extraction algorithms is limited because a frame-based camera cannot work well when the light is extremely dark or changes rapidly. To tackle this problem, <ref type="bibr" target="#b15">[16]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N-CARS data sets</head><p>The N-CARS data set introduced by <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MVSEC data sets</head><p>In <ref type="bibr" target="#b3">[4]</ref>, the multivehicle stereo event camera data set (MVSEC) created for 3D perception with multiple sensors was presented.</p><p>The MVSEC is the first data set with a synchronized stereo event-based neuromorphic vision system. The ground-truth depth data are generated from a calibrated lidar system contributing to stereo depth estimation with the eventbased vision sensor. The MVSEC data set consists of long outdoor sequences in a variety of illuminations and driving speeds, which can be used for the evaluation of event-based visual odometry, localization, obstacle avoidance, and 3D reconstruction in challenging and real-word driving scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DDD17 data set</head><p>For self-driving applications, end-to-end learning of the control model is a fascinating direction. The DDD17 data set <ref type="bibr" target="#b17">[18]</ref> is the first large-scale public data set with a DAVIS sensor. The data are recorded in highway and city scenes driving from Switzerland to Germany. It has more than 12 h of data collected under different weather, road, and light conditions, covering the distance of more than 1,000 km. Furthermore, vehicle data, such as speed, GPS position, driver steering, throttle, and brake are also recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Handcrafted feature</head><p>The concept of time surface is proposed to track the activity of the object due to the lack of effective low-level feature representations and descriptors for an event-based vision mission. It represents temporal characteristics and describes the spatial-temporal context around an event. For an event ( , , , ), e x y t p</p><formula xml:id="formula_17">i i i i i = the time surface Si of dimension R R 2 2 # is defined as , , ; , S e p P 0 if otherwise ( , ) i t T C R P i i i = = x - - + ) where ( , ) C x y i i i =</formula><p>is the pixel coordinates of the incoming event ,</p><p>ei R is the radius of the spatial neighborhood around , ei ( , ) T C R P i + is the time stamp of the last event with polarity P received from pixel , C R i + and x is a constant decay factor. The exponential decay expands the activity of past events and records history information of the activity in the neighborhood. Time surface has been effectively used in various vision tasks, such as object recognition and feature tracking. Further, a hierarchy of time surface is introduced for object recognition <ref type="bibr" target="#b18">[19]</ref>. Relying on a time-oriented approach, this model is used to extract valuable spatial-temporal features from event There are many data sets that are built to promote the research of eventbased neuromorphic vision, neurorobotics, and autonomous vehicles.</p><p>streams. Based on the findings in <ref type="bibr" target="#b18">[19]</ref>, a sparse coding basis decomposition was used to reduce the number of prototypes in a hierarchy structure for lowering computational cost and memory need <ref type="bibr" target="#b19">[20]</ref>. However, they only achieved better recognition performance for simple shapes, such as numbers and letters, while they cannot well perform for complex objects, such as cars. Inspired by the histogram of orientation gradient feature widely used in frame-based vision, an effective event descriptor named histogram of averaged time surfaces (HATS) was constructed <ref type="bibr" target="#b16">[17]</ref>. Then, better classification performance and real-time computation were obtained. HATS convert event streams into local memory time surfaces and computes the histograms to formulate the final descriptor. After these features are extracted from event streams, a simple linear support vector machine classifier is used to recognize objects in the N-CARS data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clustering</head><p>A classical unsupervised learning approach is clustering. Given a set of data, the clustering algorithm can be used in this study to generate different groups. The data with different characteristics are grouped into different clusters. The clustering methods can be applied directly to generate object proposals because the event stream from the DVS can be treated as sparse point cloud data where each point is an event. For example, a clustering method named Gaussian mixture models (GMMs) is used to track the pedestrian <ref type="bibr" target="#b21">[22]</ref>. The method achieves accurate detection and tracking of pedestrian objects by extending GMMs with a stochastic prediction of objects' states. The goal of tracking is to estimate the state of one or multiple objects over time. In case of a possible collision with other traffic participants, the autonomous vehicle requires sufficient reaction time to ensure a safe brake distance. It is difficult to track a pedestrian because a pedestrian can suddenly change his or her moving direction. The results in <ref type="bibr" target="#b21">[22]</ref> indicated that applying clustering to spatial-temporal event data has a large potential for robust object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bio-inspired feature learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNNs</head><p>An SNN is a bio-inspired approach that can operate directly on spatial-temporal event data. The computational pattern of SNNs mimics the working principle of receptive fields in the primary visual cortex. As basic building blocks of SNNs, LIF and adaptive exponential are both inspired by the biological neurons found in the visual cortex of mammalians, which encode temporal information and make them naturally fit asynchronous event streams. The basic principle of SNN is that a neuron will not emit any spike if it has not received any input spike from the preceding SNN layer. Moreover, the corresponding neuron will generate spikes that are fed to the next layer only if the membrane voltage caused by received spikes exceeds a predefined threshold. The predefined network units, such as the difference of Gaussians or Gabor filters, are usually used in the first layer of SNN to extract features. Features are transmitted from the first layer of SNN to the deeper lay-ers in parallel <ref type="bibr" target="#b22">[23]</ref>- <ref type="bibr" target="#b24">[25]</ref>. The major disadvantage of conventional SNNs is not differentiable, causing the popular training methods to be inapplicable. In the context of autonomous driving, a SNN architecture consisting of refractory, convolution, and clustering layers was presented <ref type="bibr" target="#b25">[26]</ref>. It was designed with biorealistic LIF neurons and synapses. The LIF neurons are used as basic building blocks in the proposed algorithm, where the refractory layer filters off fraction of the input events to generate spike. Then, the spikes are convolved by convolution layer to produce region proposal boxes. Moreover, the clustering layer combines these boxes to cluster together to form the shapes of objects. This method is validated on object detection with real traffic scenes including humans, bikes, cars, vans, trucks, and buses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNN with backpropagation</head><p>SNN with handcrafted feature extractors (such as Gabor filters) cannot learn weights naturally from the data. To overcome this drawback, researchers established a novel architecture of SNN with LIF neuron and winner-takes-all (WTA) circuits <ref type="bibr" target="#b20">[21]</ref>. The LIF neuron uses dynamic weights rather than a simpler refractory mechanism to update its membrane potential. In a WTA circuit, it would inhibit other neurons from spiking once an output spike occurs in a neuron. Furthermore, the lateral inhibition is employed to put the dynamic weights of all inhibited neurons in the WTA circuit into the refractory state. The differentiable transfer functions are derived in the WTA configuration to make SNN trainable with backpropagation; moreover, the performance of SNN architecture is also improved. In Figure <ref type="figure" target="#fig_7">8</ref>, an SNN network with backpropagation is illustrated. However, trainable SNN is only tested on simple data sets (such as MNIST) and has not been applied in specific autonomous driving scenarios. As the output of event-based neuromorphic vision sensor is a spatial temporal event stream which is fundamentally different from frame-based camera, it requires the design of specifically tailored algorithms to accommodate the nature of events, and <ref type="bibr" target="#b20">[21]</ref> indicates the prospect of implementing deep SNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNN</head><p>CNN is a popular feature extraction architecture, which is composed of three types of layers, including a convolutional layer, a pooling layer, and a fully connected layer. It uses spatially localized convolutional filtering to capture local features of input image. Basic visual features, such as lines, edges, and corners, are learned in the first few layers, while more abstract features are learned in deeper layers. For an input image matrix I, the correspondence activation map M is computed in the nth neuron of the CNN as follows</p><formula xml:id="formula_18">[ , ] [ , ] [ , ] , M i j W x y I i x j y b y k k x k k 2 1 2 1 2 1 2 1 v = - -+ =-- + =-- + e o</formula><p>/ / <ref type="bibr" target="#b11">(12)</ref> where the image size is , k 2 1 + W is the nth convolutional filter, and v is the nonlinear activation function. Generally, a max pooling layer follows each convolutional layer, in which the local maximum is used to reduce the dimension of the matrix and prevent overfitting. Moreover, fully connected layers are usually added to learn the nonlinear combination of extracted features from previous layers. Over the decades, many variants of CNNs, such as fully CNNs and encoderdecoder networks, have emerged. These networks have different structures from traditional CNNs, such as removing the full connection layer. The performance of CNNs has surpassed traditional machine learning methods in many vision tasks, relying on successful training algorithms and large amounts of data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs for optical flow, depth, and egomotion</head><p>Known as a 2D motion estimation, the optical flow is defined as the distribution of apparent velocities of movement of brightness patterns between two images. It provides valuable information about the scene and serves as input for several tasks, such as tracking and visual odometry. In the neuromorphic vision research community, some works attempt to estimate optical flow by taking advantage of high temporal resolution of event-based sensors <ref type="bibr" target="#b26">[27]</ref>. EV-FlowNet, a self-supervised deep learning architecture for optical flow estimation for event-based sensors, is proposed in <ref type="bibr" target="#b27">[28]</ref>. In this method, a four-channel event representation consisting of the histogram ( <ref type="formula" target="#formula_8">5</ref>) and SAE ( <ref type="formula" target="#formula_9">6</ref>) of different polarity is used to pass through a pipeline that is composed of four stride convolutional layers, two residual blocks, and four up-sampling convolutional layers for obtaining flow estimation. By evaluating an MVSEC data set, the network is able to accurately predict optical flow from event streams. In <ref type="bibr" target="#b28">[29]</ref>, a novel neural network framework is proposed to acquire motion information including optical flow, depth, and egomotion from a set of inputs (a voxel grid) that is an event data representation mentioned in the "Voxel Grid" section. The network architecture consists of encoder-decoder networks and pose models; among them, the encoder-decoder section is responsible for predicting optical flow and depth, while the pose model is responsible for estimating egomotion.</p><p>Experimental results in the MVSEC data set indicate that the presented network can learn various motion information of events well. Recently, a lightweight evenly cascaded convolutional network (ECN) using monocular event-based sensor input for dense depth, optical flow, and egomotion estimation was introduced in <ref type="bibr" target="#b29">[30]</ref>. ECNs use an encoder network to predict pose; meanwhile, an encoder-decoder network is applied to obtain the scaled depth. The algorithm can operate at 250 frames/s (fps) on a single NVIDIA 1,080 titanium GPU. Compared with previous works, it makes significant improvements on the performance of the MVSEC data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs for object detection</head><p>Reliable object detection is essential to avoid accidents that might be life threatening because a self-driving car is sharing the road with many traffic participants, such as vehicles and pedestrians. For instance, a supervised learning method is applied on event data for object detection under egomotion <ref type="bibr" target="#b30">[31]</ref>. The data set used in this article is DDD17, which is a large event-based data set applying DAVIS to record various challenging scenarios under egomotion. The DAVIS is a sensor consisting of an event-based neuromorphic sensor and a synchronized gray-scale frame-based camera. In <ref type="bibr" target="#b30">[31]</ref>, gray-scale images are fed into a state-of-the-art frame-based CNN to generate outputs (pseudolabels), which are used as ground truths for subsequent training on event-based data. This method achieves high-speed detection (100 fps) in a real outdoor scenario within various backgrounds such as day and night. As pseudolabels are not explicit enough, the authors manually labeled the DDD17 data set to explore the potential of event-based neuromorphic sensor for vehicle detection in autonomous driving <ref type="bibr" target="#b31">[32]</ref>. A convolutional SNN is utilized to generate visual attention maps for synchronizing with the frame-based stream. Two separate event-based and framebased streams are incorporated into a CNN detector to obtain detection output. With a joint decision model to postprocess the output, the algorithm outperforms the state-of-the-art methods that only employ frame-based cameras. The detection for stationary and moving people around a self-driving car has attracted the attention of researchers. Specifically, a multicue event information fusion for pedestrian detection was proposed <ref type="bibr" target="#b32">[33]</ref>; it was evaluated on the data set recorded by a neuromorphic vision sensor. Based on the advantages of leveraging various properties of event streams, this article performed better on positioning and recognition of pedestrians. Recently, a cross-modal approach was presented in <ref type="bibr" target="#b33">[34]</ref>; wormhole learning was utilized to pair red, green, blue (RGB) camera and event-based neuromorphic vision sensors to improve the object detection performance under the scenario of urban driving. This method is different from transfer learning as it can be transferred back to the original domain to improve performance on the task. The experimental results of wormhole learning reveal that there are many innovative approaches to combine data from different heterogeneous sensors, such as RGB cameras, infrared cameras and neuromorphic vision sensors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs for semantic segmentation</head><p>In the sensing and perception system of autonomous driving, a comprehensive understanding of the surrounding environment is provided by semantic segmentation. The first CNN-based baseline for semantic segmentation with an event-based neuromorphic vision sensor is introduced in <ref type="bibr" target="#b34">[35]</ref>. In this article, the authors build an event-segmentation data set (Ev-Seg) that is an extended version of DDD17 for semantic segmentation. Inspired by the study in <ref type="bibr" target="#b30">[31]</ref>, the labels of Ev-Seg are generated by running a trained CNN on gray-scale images. Then, an Xception-based CNN architecture is trained to learn generalization ability from event streams. Finally, the complementarity between the frame-based camera and event-based neuromorphic vision sensor is presented through comparing the semantic segmentation results produced from event data and corresponding gray-scale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs for active perception</head><p>Controlling the autonomous vehicle in challenging scenes such as highway driving requires a low-latency perception system. Hence, researchers try to tackle this tough problem by unlocking the low-latency potential of event-based neuromorphic vision sensors. An end-to-end autonomous driving system, mapping from the event streams to the driving actions, is proposed in <ref type="bibr" target="#b2">[3]</ref>. This system converts events to event count frames (histogram of different polarities) mentioned in <ref type="bibr" target="#b4">(5)</ref>, which are fed into a residual neural network (ResNet)-inspired network to predict the steering angle of the vehicle. The proposed method can accurately predict the steering angle of vehicles and performs better on DDD17 data sets than the state-of-the-art systems using gray-scale images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CNNs-to-SNNs</head><p>CNNs have demonstrated their ability to deal with many difficult vision problems, such as object detection. SNNs have presented their potential for low-power event-driven neuromorphic hardware. However, the applications of SNNs are limited due to their shallow neural network architecture. Furthermore, the CNN-to-SNN model is developed to combine the benefits of deep architecture in CNNs with the bio-inspired mechanism of SNNs. References <ref type="bibr" target="#b35">[36]</ref>- <ref type="bibr" target="#b37">[38]</ref> illustrate that widely used CNNs, such as VGG, ResNet, and Inception-V3 can be converted into spiking networks. It is worth mentioning that the network can achieve a more robust performance via conversion from CNNs, although the conversion process would lose some precision and increase computation. Some works have been reviewed in <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transfer learning via pretrained network</head><p>Transfer learning is a very effective method to improve the training performance of the deep neural network. Knowledge learned from a different domain can be exploited to initialize the weights of a deep neural network. The availability of event-based data sets collected with a DVS sensor is limited compared with the data set recorded by frame-based cameras. Thus, by starting the supervised training process from a better set of initial weights, the requirement of the training data can be reduced, and the generalization ability of the network can be improved. Pretrained models, such as VGGnet and ResNet, can be applied to bio-inspired sensing and perception tasks of autonomous driving. Specifically, event streams can be transformed into a three-channel image-like representation to serve as input to pretrained CNNs. In <ref type="bibr" target="#b39">[40]</ref>, the authors combined an inceptive event time surface (IETS) with transfer learning to improve performance of object classification. IETSs are generated to utilized transfer learning from the GoogLeNet that is pretrained on ImageNet, including the millions of real-world images. Nearly 100% classification accuracy on the event-based N-CARS data set is achieved by the algorithm. In <ref type="bibr" target="#b40">[41]</ref>, a robust event stream object tracking method is presented. A VGG-16 model pretrained on Ima-geNet is used to extract features to represent the appearance of the object. Based on correlative filter mechanism, the correlation response map is computed on the extracted features. The proposed approach performs well in various challenging visual scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event-based assistance systems</head><p>After the basics of event-based perception system of autonomous driving are covered, the event-based assistance systems are discussed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image reconstruction</head><p>The event-based neuromorphic vision sensor generates HDR event data even in extreme illumination conditions and also avoids motion blur under rapid motion. Reconstructing HDR intensity images from event streams facilitates the adoption of mature computer vision techniques. Previous works focus on exploiting the low latency of neuromorphic vision sensor by directly processing event data (such as SNNs) or transferring events to image-like or grid-like representations as mentioned in the section "Event Data Representation." However, the deep neural network trained on real image data (such as ImageNet) cannot be effectively transferred to these representations, even though it achieves some performance improvements (see the section "Transfer Learning via Pretrained Network"). As an alternative method, image reconstruction (IR) from event streams is first proposed in <ref type="bibr" target="#b41">[42]</ref>. IR can achieve both high frame-rate images and high-quality images with no motion blur. In <ref type="bibr" target="#b42">[43]</ref>, the authors utilize the time stamp of new events to define a manifold for IR. With considering IR as an energy minimization problem, the proposed method is optimized and achieves real-time performance on a GPU. Furthermore, an asynchronous complementary filter is presented to reconstruct event streams for continuous-time intensity estimate <ref type="bibr" target="#b43">[44]</ref>. In this article, the gray-scale frames and events produced by DAVIS are fused into an image with high temporal resolution and HDR. In addition, a new framework for IR, named E2VID, is introduced in <ref type="bibr" target="#b44">[45]</ref>. E2VID converts event stream into 3D spatial-temporal voxel grid sequences (see the "Voxel Grid" section), which are taken as the input of the network. The algorithm is trained on a large synthetic event data simulated with ESIM <ref type="bibr" target="#b45">[46]</ref> to generate reconstructed image frames. The reconstructed image data from event streams can be used for various applications such as object recognition, SLAM, and optical flow estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Panoramic stereo vision</head><p>Panoramic vision in 3D offers a full 360° surrounding view which facilitates the navigation and localization tasks for autonomous driving. A novel multiperspective panoramic stereo event-based vision system is proposed in <ref type="bibr" target="#b46">[47]</ref>. It is composed of a pair of line event-based neuromorphic vision sensors. The authors present a novel event-driven stereo matching approach for 3D panoramic vision. The process steps of the event-driven stereo matchintg algorithm include event map generation, event distribution measure, cost calculation, disparity estimation and refinement. The experimental results indicate that the tailored event-driven stereo method achieves accurately 3D reconstruction in real time out of 360° panoramic views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual odometry</head><p>The goal of the visual odometry is to estimate the position and orientation of a vehicle with vision sensors. The visual odometry system of an autonomous vehicle with a traditional frame-based camera has been developed for many years, while the method based on an event-based neuromorphic vision sensor is still in the preliminary stage. For example, an event-based visual odometry system for intelligent vehicle applications is proposed in <ref type="bibr" target="#b47">[48]</ref>. The events generated from a DAVIS sensor are aggregated into constant time frame defined in (3) to serve as input to subsequent algorithms. The feature tracking is used by visual odometry system to develop parallel pose estimation and mapping. The feasibility of event-based neuromorphic vision sensors for bio-inspired visual odometry systems in real-word outdoor driving scenes is confirmed by the results of their experiment on the MVSEC data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drowsiness driving monitoring</head><p>Drowsiness driving monitoring is important to ensure that the autonomous driving vehicle is under the supervision of the drivers. In <ref type="bibr" target="#b48">[49]</ref>, an event-based drowsiness driving detection system is proposed. The event-based neuromorphic vision sensor is considered as an efficient and effective detector for the drowsiness driving-related motions due to the unique output. <ref type="bibr" target="#b48">[49]</ref> proposes to recognize and localize the driver's eyes and mouth motions from event streams, and extracts eventbased drowsiness-related features directly from the event streams caused by eye and mouth motions. Experiments in <ref type="bibr" target="#b48">[49]</ref> demonstrate the high efficiency and accuracy under different illumination conditions such as subjects wearing sunglasses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spike compression</head><p>The event data compression is particularly important for maintaining the real-time performance of the sensing system of autonomous vehicles because both the data storage and transmission bandwidth of on-board event-based neuromorphic visions sensors equipped on the autonomous vehicles are limited. To address this problem, a cube-based spike coding framework is proposed by <ref type="bibr" target="#b49">[50]</ref>. In the spatial-temporal dimension, an octree-based structure is put forward to adaptively cut the event (spike) stream into coding cubes, then address-prior mode and time-prior mode are designed to exploit the spatial and temporal characteristics of events for data compression. The proposed spike coding framework is evaluated on the DDD17 data set. Experimental results indicate that it can achieve a better compression ratio against the raw event data. Reference <ref type="bibr" target="#b50">[51]</ref> proposes to use mixture density autoencoder to learn a low-dimensional representation from an event stream, which preserves the nature of event-based data better while being easy to feed to a sequence classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Challenges and future directions in autonomous driving</head><p>Event-based neuromorphic vision is an emerging technique in the era of mature sensor hardware of autonomous driving. Comparing it with lidar, radar, and cameras is unfair because event-based sensors such as DVS are not at the same maturity level as others. Conversely, there is substantial room for the development and improvement in the cross-research of event-based neuromorphic vision and autonomous driving. Challenges and future directions closely related to autonomous driving are pointed out in numerous opportunities, as described later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sensor fusion in perception system of autonomous driving</head><p>To fuse the event-based neuromorphic vision sensor with others, there is an unavoidable problem that the sensor fusion brings back the disadvantages of providing a redundant, Event-based neuromorphic vision is an emerging technique in the era of mature sensor hardware of autonomous driving.</p><p>sampled intensity output with linear encoding of intensity. On the contrary, the advantages are also obvious; that is, different kinds of sensors are complementary. For example, DVS contains no color information, which is provided by frame-based cameras. The distance and speed information can be provided by lidar and radar. It remains to be seen whether the DVS output can be used to trigger frame captures of other sensors. If it is, the DVS and other sensors can operate together with mixed conventional machine vision, bio-inspired, and eventbased neuromorphic vision-based approaches. Therefore, some of the limitations of a traditional sensor-based perception system may be overcome; moreover, new scenarios that were previously inaccessible in the visual sensing and perception of autonomous vehicles might be reached.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Active vision system of autonomous driving</head><p>In robotics, the ability to directly fuse the perception with its motoric ability is often referred to as active perception. In autonomous driving, it is found that the perception and action are often kept in separated spaces; this is a consequence of state-of-the-art sensors equipped on the autonomous vehicle being frame-based. The sensing and perception only exist in a discrete moment while the motion is a continuous entity. It can be argued that the event-based neuromorphic vision sensor can see the motion, which has the potential to cross the bridge between perception and motor control. New methods of encoding perceptions and actions could be meaningful to the active perception system of autonomous driving. Moreover, this would create new opportunities for real-time navigation and obstacle avoidance for autonomous driving if the visual perception can be bound with the system dynamic to enable dynamic environment perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Large-scale autonomous driving benchmark based on an event-based neuromorphic vision sensor</head><p>It is well known that rapid development of autonomous driving is promoted by standardized benchmarks. For example, the growing popularity of deep neural networks in intelligent vehicles and large-scale benchmarks such as KITTI, Cityscale, and ImageNet, is interconnected and mutually reinforced. In the earlier days of event-based neuromorphic vison, most of the research work was done in an indoor environment due to the low resolution of sensors. Until recently, the event-based neuromorphic vision sensor has been expanded to outdoor scenarios, such as autonomous driving, by the teams of Tobi Debruck, Kostas Daniilidis, and David Scaramuzzsa. There is an emerging need for high-quality benchmarks in the fields of event-based neuromorphic vision and autonomous driving. A standard platform would bring the mainstream of computer vision-based intelligent vehicle research to pay attention to eventbased neuromorphic vision; furthermore, the unique strengths of bio-inspired vision would be leveraged to attract research interests in new sensing techniques for autonomous driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>From simulated event data to real-world autonomous driving</head><p>Labeling the asynchronous event data is always a challenging problem because almost all of the annotation tools are developed for frame-based cameras. Additionally, there is not a standard format for the annotations. From one perspective, developing an easy-to-use tool for recording and labeling event data would make a significant contribution to the community; from another perspective, the adoption of event-based neuromorphic vision technology would also be facilitated by developing simulators. Particularly, the corresponding event streams, intensity frames, and depth information could be generated by a simulator based on the working principle of the sensor. Simultaneously, the basic facts of all recording data including the trajectory of the sensor, the label of the object, and even the optical flow are also generated without the need for annotation. With photorealistic virtual driving scenes and realistic sensor models, the development of event-based visual sensing and perception system in autonomous vehicles will be accelerated by prototyping on simulated event data with transfer learning methods in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations that may exist as event-based neuromorphic vision sensors mature</head><p>There is no appearance feature such as color and texture because an event-based neuromorphic vision sensor only transmits local pixel-level changes, making it perform poorly in some applications with high requirements for appearance features. Although researchers have used the method of IR (mentioned in the section "Spatial Encoding") to reconstruct image frames from event streams, the quality of reconstructed image frames is still not comparable to the output data produced by RGB cameras. The application of an event-based neuromorphic vision sensor is limited in some scenarios where energy, latency, and dynamic range are not important, especially in high-resolution complex scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>Innovative solutions will emerge due to the challenges remaining on the road to fully autonomous driving. Concurrently, sophisticated signal processing techniques have been successfully applied to autonomous driving hardware such as cameras, lidars, and radars. Exploring alternative methods of visual sensing such as event-based neuromorphic vision is promising for promoting subsequent tasks to be more robust and complementary. It is reasonable to say that the research and development of an event-based neuromorphic vision for autonomous driving is still in its infancy. In this article, the advantages, signal processing techniques, emerging applications and systems, and future directions of an event-based neuromorphic vision for autonomous driving have been introduced and analyzed. This article helps researchers and engineers take the first step in developing innovative signal There is an emerging need for high-quality benchmarks in the fields of event-based neuromorphic vision and autonomous driving.</p><p>processing techniques toward bio-inspired visual sensing and perception of autonomous vehicles.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, the three-layer model of a human retina [Figure 2(c)], and corresponding DVS pixel circuitry [Figure 2(a)] are presented. Typical signals of the pixel circuits are displayed in Figure 2(b). The upper trace denotes a voltage waveform at the node ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. A practicable silicon retina DVS based on biological principles (adapted from [53]): (a) DVS pixel circuitry, (b) typical signals of the pixel circuits, (c) a three-layer model of a human retina, and (d) the accumulated events from a DVS. The accumulated event map has ON event (illumination increased) and OFF event (illumination decreased) drawn as white and black dots.</figDesc><graphic coords="4,56.33,542.07,281.78,114.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .</head><label>3</label><figDesc>FIGURE 3. The AER communication protocol: (a) three neurons on the sending chip generate spikes; (b) spikes are interpreted as binary events. A binary address is generated by the AE and transmitted to the receiver chip by the bus line; (c) the binary address is decoded to the binary event by the AD; and (d) spikes are emitted on the corresponding neurons of the receiver chip where the positions of the neurons are determined by the AD.</figDesc><graphic coords="5,52.83,527.40,141.86,144.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 4 .</head><label>4</label><figDesc>FIGURE 4. Event noise processing. The top branch is the spatial-temporal correlation filter; the bottom branch is the motion consistency filter.</figDesc><graphic coords="6,53.45,481.67,212.66,165.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>yFIGURE 5 .</head><label>5</label><figDesc>FIGURE 5. The process of converting asynchronous event data into an event frame. An event frame consists of two histograms from the positive events and negative events, respectively.</figDesc><graphic coords="7,50.93,78.57,245.06,144.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 7 .FIGURE 6 .</head><label>76</label><figDesc>FIGURE 7. The process of converting asynchronous event data into grid-based representation with a fixed kernel<ref type="bibr" target="#b13">[14]</ref> and a learnable kernel<ref type="bibr" target="#b14">[15]</ref>.</figDesc><graphic coords="8,55.55,565.67,141.86,131.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 8 .</head><label>8</label><figDesc>FIGURE 8. An example of how an SNN network works with backpropagation [21].</figDesc><graphic coords="11,53.69,530.20,141.74,131.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="1,-8.56,269.60,282.63,292.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 . The comparison of different event data representations of spatial-temporal encoding.</head><label>1</label><figDesc></figDesc><table><row><cell>Representation</cell><cell>Dimensions</cell><cell>Polarity Channel</cell><cell>Intensity</cell><cell>Weakness</cell></row><row><cell>SAE</cell><cell>H # W</cell><cell>2</cell><cell>Time stamp of the most recent event</cell><cell>Without temporal history</cell></row><row><cell>LIF</cell><cell>H # W</cell><cell>1</cell><cell>Event spikes</cell><cell>Without polarity information</cell></row><row><cell>Voxel grid</cell><cell>B # H # W</cell><cell>1</cell><cell>Sum event polarities</cell><cell>Without polarity information</cell></row><row><cell>EST</cell><cell>B # H # W</cell><cell>2</cell><cell>Sample event point-set into the grid</cell><cell>Without the least amount of information</cell></row></table><note><p>The polarity channel is 2 if the encoding method considers the polarities of events; otherwise, it is 1. H and W represent the image height and width dimensions, respectively; B denotes the number of temporal bins.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>uses event-based neuromorphic vision sensors to build a high-resolution data set, called the DET data sets, for lane extraction. The DET data set containing various traffic scenes is collected by driving on tunnels, bridges, overpasses,</figDesc><table><row><cell>and urban areas. The data set includes 5,424 event frames of</cell></row><row><cell>1,280 × 800 pixels with corresponding labels and consists of</cell></row><row><cell>a training set of 2,716 frames, a validation set of 873 frames,</cell></row><row><cell>and a test set of 1,835 frames. Two kinds of labels (per-pixel</cell></row><row><cell>label without distinguishing lanes and per-pixel label with dis-</cell></row><row><cell>tinguishing lanes) are provided. The DET data set is the first</cell></row><row><cell>bio-inspired vision data set for lane detection-a fundamental</cell></row><row><cell>problem in autonomous driving.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>,336 car samples and 11,693 noncar (background) samples. Specifically, 7,940 car samples and 7,842 background samples are training samples, and others are testing samples. Each example is labeled by semiautomatic protocol with manual correction of the wrong one.</figDesc><table /><note><p><p><ref type="bibr" target="#b16">17]</ref> </p>provides recording cars in urban environments with a DVS. The data set con-sists of 12</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Authorized licensed use limited to: University College London. Downloaded on July 03,2020 at 00:46:24 UTC from IEEE Xplore. Restrictions apply.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research has received funding from the Shanghai Automotive Industry Sci-Tech Development Program according to grant agreement 1838, from the Shanghai AI Innovation Development Program 2018, and from the European Union's Horizon 2020 Framework Program for Research and Innovation under the specific grant agreement 785907 (Human Brain Project SGA2).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors</head><p>Guang <ref type="bibr">Chen</ref>  Florian Röhrbein (florian@gmx.org) received his diploma and Ph.D. degree from the Technical University of Munich (TUM) and the venia legendi for computer science from the University of Bremen. He is responsible for the development and implementation of the artificial intelligence (AI) strategy for a world-leading company and is the chief editor of Frontiers in Neurorobotics. He was also the managing director in the Human Brain Project at TUM. He has international experience in various projects on AI, computational neuroscience and brain-inspired cognitive systems. Research stays include the MacKay Institute of Communication and Neuroscience (United Kingdom), the Honda Research Institute Europe, and the Albert Einstein College of Medicine (New York). He is a Senior Member of the IEEE.</p><p>Alois Knoll (knoll@in.tum.de) received his M.S. degree in electrical/communications engineering from the University of Stuttgart, Germany, in 1985 and his Ph. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A 128 × 128 120 db 15 μs latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSSC.2007.914337</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Event-driven sensing for efficient perception: Vision and audition algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rueckauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="DOI">10.1109/MSP.2019.2928127</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2019-11">Nov. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Event-based vision meets deep learning on steering prediction for self-driving cars</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Maqueda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00568</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5419" to="5427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The multivehicle stereo event camera dataset: An event camera dataset for 3d perception</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Özaslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfrommer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/LRA.2018.2800793</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2032" to="2039" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Autonomous driving in urban environments: Boss and the urban challenge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Urmson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anhalt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Duggins</surname></persName>
		</author>
		<idno type="DOI">10.1002/rob.20255</idno>
	</analytic>
	<monogr>
		<title level="j">J. Field Robot</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="425" to="466" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Self-driving cars: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Badue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guidolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">V</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">B</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Forechi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F R</forename><surname>Jesus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Berriel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04407</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How Google&apos;s self-driving car works</title>
		<author>
			<persName><forename type="first">E</forename><surname>Guizzo</surname></persName>
		</author>
		<ptr target="https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works" />
	</analytic>
	<monogr>
		<title level="j">IEEE Spectrum</title>
		<imprint>
			<date type="published" when="2011-10-18">Oct. 18, 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Event-based vision: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbrück</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08405</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The silicon retina</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Mahowald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mead</surname></persName>
		</author>
		<idno type="DOI">10.1038/scientificamerican0591-76</idno>
	</analytic>
	<monogr>
		<title level="j">Sci. Amer</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="76" to="82" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Design of a spatiotemporal correlation filter for event-based sensors</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Brandli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCAS.2015.7168735</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IEEE Int. Symp. Circuits and Systems (ISCAS)</title>
		<meeting>2015 IEEE Int. Symp. Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<biblScope unit="page" from="722" to="725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A noise filtering algorithm for event-based asynchronous change detection image sensors on TrueNorth and its implementation on TrueNorth</title>
		<author>
			<persName><forename type="first">V</forename><surname>Padala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2018.00118</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">O(n)-space spatiotemporal filter for reducing noise in neuromorphic vision sensors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Khodamoradi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kastner</surname></persName>
		</author>
		<idno type="DOI">10.1109/TETC.2017.2788865</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerg. Topics Comput., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">EV-gait: Event-based robust gait recognition using dynamic vision sensors</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00652</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="6351" to="6360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances Neural Information Processing Systems</title>
		<meeting>Advances Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">End-to-end learning of representations for asynchronous event-based data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Loquercio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00573</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Computer Vision (ICCV)</title>
		<meeting>IEEE Int. Conf. Computer Vision (ICCV)<address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5632" to="5642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DET: A high-resolution DVS dataset for lane extraction</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HATS: Histograms of averaged time surfaces for robust event-based object classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sironi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brambilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bourdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00186</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 IEEE/Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2018 IEEE/Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="page" from="1731" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end DAVIS driving dataset</title>
		<author>
			<persName><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbrück</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 34th Int. Conf. Machine Learning (ICML)</title>
		<meeting>34th Int. Conf. Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">HOTS: A hierarchy of event-based time-surfaces for pattern recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Galluppi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Benosman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2574707</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1346" to="1359" />
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sparse coding multi-scale precise-timing machine learning algorithm for neuromorphic event-based sensors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Haessig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2305933</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Microand Nanotechnology Sensors, Systems, and Applications X</title>
		<meeting>Microand Nanotechnology Sensors, Systems, and Applications X</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">10639</biblScope>
			<biblScope unit="page" from="289" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Training deep spiking neural networks using backpropagation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2016.00508</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">508</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiple persons tracking using dynamic vision sensor</title>
		<author>
			<persName><forename type="first">E</forename><surname>Piatkowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Belbachir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schraml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gelautz</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2012.6238892</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition Workshops</title>
		<meeting>2012 IEEE Computer Society Conf. Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HFirst: A temporal approach to object recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thakor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2015.2392947</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2028" to="2040" />
			<date type="published" when="2015-01">Jan. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards a cortical prosthesis: Implementing a spike-based HMAX model of visual object recognition in silico</title>
		<author>
			<persName><forename type="first">F</forename><surname>Folowosele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Etienne-Cummings</surname></persName>
		</author>
		<idno type="DOI">10.1109/JETCAS.2012.2183409</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Emerg. Select. Topics Circuits Syst</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="516" to="525" />
			<date type="published" when="2011-12">Dec. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An event-driven categorization model for AER image sensors using multispike encoding and learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2019.2945630</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spiking neural network based region proposal networks for neuromorphic vision sensors</title>
		<author>
			<persName><forename type="first">J</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Padala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Basu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISCAS.2019.8702651</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp. Circuits and Systems (ISCAS)</title>
		<meeting>IEEE Int. Symp. Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Event-based visual flow</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clercq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lagorce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2013.2273537</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Netw. Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="407" to="417" />
			<date type="published" when="2014-02">Feb. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">EV-flownet: Selfsupervised optical flow estimation for event-based cameras</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.15607/RSS.2018.XIV.062</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Robotics: Science and System</title>
		<meeting>Robotics: Science and System</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised event-based learning of optical flow, depth, and egomotion</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00108</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="989" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised learning of dense optical flow and depth from sparse event data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mitrokhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Parameshwara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fermüller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Yorke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
		<idno>arXiv:abs/1809.08625</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pseudo-labels for supervised learning on dynamic vision sensor data, applied to object detection under ego-motion</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">F Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2018.00107</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2018 IEEE/CVF Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>2018 IEEE/CVF Conf. Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<biblScope unit="page" from="757" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Event-based vision enhanced: A joint detection framework in autonomous driving</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICME.2019.00242</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2019 IEEE Int. Conf. Multimedia and Expo (ICME)</title>
		<meeting>2019 IEEE Int. Conf. Multimedia and Expo (ICME)</meeting>
		<imprint>
			<biblScope unit="page" from="1396" to="1401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-cue event information fusion for pedestrian detection with neuromorphic vision sensors</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbot.2019.00010</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurorobot</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-modal learning filters for RGB-neuromorphic wormhole learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zanardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aumiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Robotics: Science and System XV</title>
		<meeting>15th Robotics: Science and System XV</meeting>
		<imprint>
			<date type="published" when="2019-06-24">June 24, 2019</date>
			<biblScope unit="page">P45</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">EV-SegNet: Semantic segmentation for eventbased cameras</title>
		<author>
			<persName><forename type="first">I</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spiking deep convolutional neural networks for energy-efficient object recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Khosla</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-014-0788-3</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1573" to="1405" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conversion of continuous-valued deep networks to efficient event-driven networks for image classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Rueckauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-A</forename><surname>Lungu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2017.00682</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">682</biblScope>
			<date type="published" when="2017-12">Dec. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper in spiking neural networks: VGG and residual architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2019.00095</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="2019-03">Mar. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning with spiking neurons: Opportunities and challenges</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfeil</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2018.00774</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">774</biblScope>
			<date type="published" when="2018-10">Oct. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inceptive event time-surfaces for object classification using neuromorphic cameras</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Wes</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Almatrafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Asari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hirakawa</surname></persName>
		</author>
		<idno>doi: 07/978-3-030-27272-2_35</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Image Analysis and Recognition</title>
		<meeting>Int. Conf. Image Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="395" to="403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Robust event-based object tracking combining correlation filter and CNN representation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnbot.2019.00082</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurorobot</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">82</biblScope>
			<date type="published" when="2019-10">Oct. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interacting maps for fast visual interpretation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gugelmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Krautz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Steger</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2011.6033299</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2011 Int. Joint Conf. Neural Networks</title>
		<meeting>2011 Int. Joint Conf. Neural Networks</meeting>
		<imprint>
			<biblScope unit="page" from="770" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time intensity-image reconstruction for event cameras using manifold regularisation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Munda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-018-1106-2</idno>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1381" to="1393" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Continuous-time intensity estimation using event cameras</title>
		<author>
			<persName><forename type="first">C</forename><surname>Scheerlinck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2018</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="308" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Events-to-video: Bringing modern computer vision to event cameras</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">ESIM: An open event camera simulator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd Conf. Robot Learning</title>
		<meeting>2nd Conf. Robot Learning</meeting>
		<imprint>
			<date type="published" when="2018">Oct. 29-31, 2018</date>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="969" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Event-driven stereo matching for realtime 3D panoramic vision</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schraml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Belbachir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298644</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>2015 IEEE Conf. Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="466" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neuromorphic visual odometry system for intelligent vehicle application with bio-inspired vision sensor</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/ROBIO49542.2019.8961878</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Robotics and Biomimetics</title>
		<meeting>IEEE Int. Conf. Robotics and Biomimetics</meeting>
		<imprint>
			<date type="published" when="2019-09">Sept. 2019</date>
			<biblScope unit="page" from="2225" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">EDDD: Event-based drowsiness driving detection through facial motion analysis with neuromorphic vision sensor</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="DOI">10.1109/JSEN.2020.2973049</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Sensors J., to be published</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Spike coding for dynamic vision sensor in intelligent driving</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JIOT.2018.2872984</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Things J</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="71" />
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">FLGR: Fixed Length Gists Representation learning for RNN-HMM hybrid-based neuromorphic continuous gesture recognition</title>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lienen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Röhrbein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Knoll</surname></persName>
		</author>
		<idno type="DOI">10.3389/fnins.2019.00073</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Neurosci</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">73</biblScope>
			<date type="published" when="2019-02">Feb. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Wormhole learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zanardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aumiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Frazzoli</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICRA.2019.8794336</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. 2019 Int. Conf. Robotics and Automation (ICRA)</title>
		<meeting>2019 Int. Conf. Robotics and Automation (ICRA)<address><addrLine>Montreal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">May 20-24, 2019</date>
			<biblScope unit="page" from="7899" to="7905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Retinomorphic event-based vision sensors: Bioinspired cameras with spiking output</title>
		<author>
			<persName><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2014.2346153.SP</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE</title>
		<meeting>IEEE</meeting>
		<imprint>
			<date type="published" when="2014-10">Oct. 2014</date>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page" from="1470" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
