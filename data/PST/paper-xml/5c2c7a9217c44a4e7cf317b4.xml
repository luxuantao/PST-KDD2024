<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIMD-X: Programming and Processing of Graph Algorithms on GPUs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts Lowell</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">H</forename><forename type="middle">Howie</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The George Washington University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SIMD-X: Programming and Processing of Graph Algorithms on GPUs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2023-01-01T13:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With high computation power and memory bandwidth, graphics processing units (GPUs) lend themselves to accelerate data-intensive analytics, especially when such applications fit the single instruction multiple data (SIMD) model. However, graph algorithms such as breadth-first search and k-core, often fail to take full advantage of GPUs, due to irregularity in memory access and control flow. To address this challenge, we have developed SIMD-X, for programming and processing of single instruction multiple, complex, data on GPUs. Specifically, the new Active-Compute-Combine (ACC) model not only provides ease of programming to programmers, but more importantly creates opportunities for system-level optimizations. To this end, SIMD-X utilizes just-in-time task management which filters out inactive vertices at runtime and intelligently maps various tasks to different amount of GPU cores in pursuit of workload balancing. In addition, SIMD-X leverages push-pull based kernel fusion that, with the help of a new deadlock-free global barrier, reduces a large number of computation kernels to very few. Using SIMD-X, a user can program a graph algorithm in tens of lines of code, while achieving 3×, 6×, 24×, 3× speedup over Gunrock, Galois, CuSha, and Ligra, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of big data <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b81">83]</ref> exacerbates the need of extracting useful knowledge within an acceptable time envelope. For performance acceleration, many applications utilize graphics processing units (GPUs) whose huge success comes from exploiting the data-level parallelism in these applications.</p><p>Implicitly, the traditional single instruction multiple data (SIMD) model of GPUs assumes regular programming and processing, that is, not only the same instruction is executed but also the same amount of work is expected to perform on each piece of data. Unfortunately, neither assumption holds true for many emerging irregular applications, especially graph analytics which is the focus of this work. That is, such applications do not conform to the SIMD model, where different amount of work, or worse, completely different work, need to be performed on the data in parallel.</p><p>To enable graph computation on GPUs, this work advocates a new parallel framework, SIMD-X, for the programming and processing of single instruction multiple, complex, data on GPUs. At the heart of SIMD-X is the decoupling of programming and processing, that is, SIMD-X utilizes the data-parallel model for ease of expressing of graph applications, while enabling systemlevel optimizations at run time to deal with the taskparallel complexity on GPUs. With SIMD-X, a programmer simply needs to define what to do on which data, without worrying about the issues arisen from irregular memory access and control flow, both of which prevent GPUs from achieve massive parallelism.</p><p>SIMD-X consists of three major components: First, SIMD-X utilizes a new Active-Compute-Combine (ACC) programming model that asks a program to define three data-parallel functions: the condition for determining an active vertex, computation to be performed on an associated edge, and combining the updates from edge compute to vertex state. As we will show later, ACC is able to support a large variety of graph algorithms from breadth-first search, k-core, to belief propagation. While ACC adopts the Bulk Synchronous Parallel (BSP) model <ref type="bibr" target="#b48">[49]</ref>, it differs from traditional CPU-based graph abstractions such as edge-or vertex-centric models in that ACC avoids atomic operation, enables collaborative early termination (for BFS) and fine-grained task management on GPUs.</p><p>Second, SIMD-X relies on just-in-time (JIT) task management to balance parallel workloads across different GPU cores with minimal overhead. A good task list can increase not only parallelism, but also sequential memory access for the computation of next iteration, both of which are crucial for high-performance computing on GPUs. To this end, we have designed a set of new task management mechanisms, that is, online and ballot filters, each of which excels at the complementary scenarios, i.e., the former favors a small amount of tasks while the latter larger tasks. At runtime, SIMD-X judiciously selects the more suitable filter to assemble the active work list for the next iteration. Our JIT task management can largely reduce the memory consumption, thereby accommodate the graphs much larger than prior work <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b75">77]</ref>. Moreover, SIMD-X delivers 16×, on average, speedup across various algorithms and graphs. Third, SIMD-X designs a new technique of push-pull based kernel fusion which aims to further accelerate graph computing by reducing kernel invocation overhead and global memory traffic. SIMD-X addresses the deadlock issue which occurs in existing software global barrier <ref type="bibr" target="#b77">[79]</ref> that is adopted by Gunrock <ref type="bibr" target="#b75">[77]</ref>. Besides, instead of aggressively fusing the algorithm into one giant kernel, SIMD-X fuses the kernels around the pull and push stages within each computation to minimize both register consumption and kernel relaunching. The evaluation shows that the new fusion technique can reduce the register consumption by half and thus double the configurable thread count, leading to 42% and 25% performance improvement over non-fused and aggressive fusion, respectively. SIMD-X is different from prior work in several aspects. First, despite an array of graph frameworks has surged, majority of them are for CPU systems while SIMD-X is for GPU accelerators that come with mounting programming challenges. In order to use GPUs efficiently, a programmer needs to possess an in-depth knowledge of GPU architecture <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b0">1]</ref>, e.g., Gunrock requires explicit management of GPU threads and memory <ref type="bibr" target="#b74">[76]</ref>, and B40C <ref type="bibr" target="#b49">[50]</ref> and Enterprise <ref type="bibr" target="#b40">[41]</ref> need thousands of lines of CUDA code for BFS specific optimizations. One of the goals of this work is to provide a simple programming model and delegate the responsibility of task management to SIMD-X. Second, current systems either ignore workload imbalance as in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b89">91]</ref>, or resolve it reactively as in <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b70">72]</ref>, both of which result in undesired system performance. Lastly, because GPUs lack support for global synchronization, existing systems <ref type="bibr" target="#b71">[73,</ref><ref type="bibr" target="#b74">76,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b68">69]</ref> either rely on the multi-kernel design or runtime tunning, both of which come with considerable overhead, especially for graph algorithms with high iteration count. SIMD-X addresses these challenges with new filters, and a deadlock-free software barrier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">SIMD-X Challenges and Architecture 2.1 Graph Computing on GPUs</head><p>Generally speaking, regular applications present uniform workload distribution across the data set. As a result, such applications lend themselves to the data-parallel GPU architecture. For development and evaluation, this work mainly uses NVIDIA GPUs, which have tens of streaming processors and in total thousands of Compute Unified Device Architecture (CUDA) cores <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56]</ref>. Typically, a warp of 32 threads execute the same instruction in parallel on consecutive data.</p><p>On the other hand, task management for irregular applications is challenging on GPUs. In this work, we focus on a number of graph algorithms such as breadth-first search, k-core, and belief propagation. Here we use one algorithm -Single Source Shortest Path (SSSP) -to illustrates the challenges. Simply put, a graph algorithm computes on a graph G = (V , E, w), where V , E and w are the sets of vertices, edges, and edge weights. The computation updates the algorithmic metadata which are the states of vertices or edges in an iterative manner. A typical workflow of SSSP is shown in Figure <ref type="figure" target="#fig_0">1</ref>. Initially, SSSP assigns the infinite distance to each vertex in the distance array, which is represented as blank in the figure. Assuming the source vertex is a, the algorithm assigns 0 as its initial distance, and now vertex a becomes active. Next, SSSP computes on this vertex, that is, calculating the updates for all the neighbors of vertex a. In this case, vertices {b, d} have their distances updated to 5 and 1 in the distance array. At the next iteration, the vertices with newly updated distances become active and perform the same computation again. This process continues until no vertex gets updated. Different from breadth-first search, SSSP may update the distances of some vertices across multiple iterations, e.g., vertex b is updated in iteration 1 and 3. In this example, not every vertex is active at all time, and vertices with different degrees (number of edges) yield varying amounts of workloads. For instance, at iteration 3 of Figure <ref type="figure" target="#fig_0">1</ref>(d), one thread working on vertex c computes two neighbors, while another thread on vertex e four neighbors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Architecture</head><p>SIMD-X is motivated to achieve two goals simultaneously: providing ease of programming for a large variety of graph algorithms, whereas enabling fine-grained optimization of GPU resources at the runtime. Figure <ref type="figure" target="#fig_1">2</ref> presents an overview of SIMD-X architecture. To achieve the first goal, SIMD-X utilizes a simple yet powerful Active-Compute-Combine (ACC) model. This dataparallel API allows a programmer to implement graph algorithms with tens of lines of code (LOC). Prior work requires significant programming effort <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b74">76]</ref>, or runs the risk of poor performance <ref type="bibr" target="#b32">[33]</ref>.</p><p>In SIMD-X, high-performance graph processing on GPUs is achieved through the development of two components: (1) JIT task management, which is responsible for translating data-parallel code to parallel tasks on GPUs. Essentially, SIMD-X "filters" the inactive tasks and groups similar ones to run on the underlying SIMD architecture. In particular, SIMD-X develops online and ballot filters for handling different types of tasks, and dynamically selects the better filter during the execution of the algorithm. And (2) Pull-push based kernel fusion. Graph applications are iterative in nature and thus require synchronizations. Fusing kernels across iterations would yield indispensable benefits, because kernel launching at each iteration incurs non-trivial overhead. In SIMD-X, we observe that with aggressive kernel fusion, register consumption would increase dramatically, lowering the occupancy and thus performance. To this end, SIMD-X deploys kernel fusion around pull and push stages of each graph computation, seeking a sweet spot that not only maximizes the range of each kernel fusion but also minimizes the register consumption. It is worthy noting that we also address the deadlock issue faced by software global barrier in SIMD-X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ACC Programming Model</head><p>The novelty of SIMD-X lies at achieving both ease of programming and efficient workload scheduling, which is especially hard on GPUs. When it comes to graph computing, there are two main programming models: vertexcentric vs. edge-centric. Vertex-centric model, also referred to as "Think like a vertex" <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b88">90]</ref> focuses on active vertices in a graph, whereas the latter one <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b59">60]</ref> iterates on edges and simplifies programming.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation</head><p>Graph programming converges to either vertex-centric or edge-centric models. In particular, the vertex-centric model contains two functions: vertex scatter defines what operations should be done on this vertex, and vertex gather applies the updates on the vertex. This model has been adopted by a number of existing projects, e.g., Pregel <ref type="bibr" target="#b48">[49]</ref>, GraphLab <ref type="bibr" target="#b44">[45]</ref>, PowerGraph <ref type="bibr" target="#b17">[18]</ref>, GraphChi <ref type="bibr" target="#b38">[39]</ref>, FlashGraph <ref type="bibr" target="#b88">[90]</ref>, Mosaic <ref type="bibr" target="#b46">[47]</ref>, and Grid-Graph <ref type="bibr" target="#b90">[92]</ref>, as well as GPU-based implementation such as CuSha <ref type="bibr" target="#b32">[33]</ref> and Gunrock <ref type="bibr" target="#b74">[76]</ref>. On the other hand, the edge-centric model is initially introduced by the external-memory graph engine X-stream <ref type="bibr" target="#b60">[61]</ref> to improve IO performance. It requires a programmer to define two functions needed on each edge, edge scatter and edge gather. As such, this model schedules threads by the edge count. Particularly, one thread needs to send the information of the source vertex and the outbound edge to the destination vertex (edge scatter), which atomically applies the new updates in edge gather.</p><p>In this work, we believe the many-threaded nature of GPU architecture demands a new abstraction. We intend to exploit various thread scheduling options to better tackle workload imbalance <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b75">77]</ref>, while minimizing the overhead with regards to atomic operations on GPUs <ref type="bibr" target="#b45">[46]</ref>. Table <ref type="table" target="#tab_0">1</ref> summarizes the designs of recent GPU-based graph analytics systems. To avoid wasting the threads to compute on inactive vertices, task filtering is essential in generating a list of active vertices. Once task lists are ready, workload imbalance caused by skewed degree distribution in many graphs becomes the next concern. Since handling this issue in a vertex centric model involves nontrivial programming efforts <ref type="bibr" target="#b40">[41]</ref>, edge-based computing presents a desirable alternative. However, traditional edge-centric approach would result in atomic updates at the destination vertex, thus a proper schedule before applying the update is essential to avoid atomic operation. It is also important to note that compressed sparse row (CSR) is a preferable graph format which can save around 50% of the space over edge list format, as contemporary GPUs only feature tens of GB memory <ref type="bibr" target="#b0">[1]</ref>. The proposed ACC framework is designed to address these three challenges. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ACC Model</head><p>The new ACC model contains three functions: Active, Compute, and Combine. ACC supports a wide range of graph algorithms and requires much fewer lines of code compared to prior work. In this following, we will discuss the three functions.</p><p>Active allows a programmer to specify the condition whether a vertex is active. Formally it can be defined:</p><formula xml:id="formula_0">∃ v ← active(M v , v)</formula><p>where v is the vertex ID and M v represents its metadata. Depending on the algorithm, the Active function may vary. Belief propagation (BP) is simple which treats all vertices as active. In comparison, SSSP, as shown in Figure <ref type="figure" target="#fig_3">3</ref>(a), considers the vertices active when their current metadata differs from the prior iteration. Simply put, SIMD-X distinguishes active vertices from inactive ones, and focuses on the calculation needed for each vertex. This is different from the vertex-centric model which deals with not only the active vertex but also its neighbors. Because two vertices may have different numbers of neighbors, existing systems <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b17">18]</ref> likely suffer from workload imbalance. To this end, SIMD-X leverages a classification technique, similar to Enterprise <ref type="bibr" target="#b40">[41]</ref>, to group the active vertices depending on the expected workload. Compute defines the computation that happens on each edge. In particular, it specifies the operations on the metadata of edge (v, u) and two vertices v and u, which can be written as follows:</p><formula xml:id="formula_1">update v→u ← compute(M v , M (v,u) , M u )</formula><p>where the return value of update v→u will be used by the Combine function. For example, SSSP can be defined as shown in Figure <ref type="figure" target="#fig_3">3(a)</ref>. Combine merges all the updates, once the computations are completed. It can be represented:</p><formula xml:id="formula_2">update u ← ⊕ v∈Nbr[u]</formula><p>update v→u where ⊕ must be commutative and associative, e.g., sum and minimum, and is being applied to all the neighbors of vertex u. SIMD-X optimizes two types of combine operations, i.e., aggregation and voting. Particularly, aggregation cannot tolerate overwrites, that is, all updates are needed to arrive at the correct results. PageRank, SSSP and k-Core are representative examples of such operation. In contrast, voting relaxes this condition, that is, the algorithm is correct as long as one update is received because all updates are identical. For instance, BFS is valid once one parent vertex successfully visited the child vertex. Other algorithms, such as, weakly connected component and strongly connected component algorithms <ref type="bibr" target="#b66">[67]</ref> also fall into this category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Processing with ACC</head><p>This section uses SSSP an example to illustrate how the SIMD-X framework works. SSSP computes the shortest paths between the source vertex and the remaining vertices of the graph. Although similar to Breadth-First Search (BFS), SSSP is more challenging as only one vertex with the shortest distance should be computed at one time. To improve the parallelism, we adopt the deltastep <ref type="bibr" target="#b50">[51]</ref> algorithm which permits us to simultaneously compute a collection of the vertices whose distances are relatively shorter. We assume positive edge weights.</p><p>As shown in line 12 -21 of Figure <ref type="figure" target="#fig_3">3</ref>(b), SIMD-X structures graph computation as a loop. Similar to popular GPU-based frameworks <ref type="bibr" target="#b75">[77,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32]</ref>, ACC follows BSP model, that is, synchronization is required at the end of each iteration. As we will discuss in the next section, SIMD-X employ three kernels to balance the workload, Thread, Warp and CTA kernels working on small list, med list and large list, respectively. During computing, the online filter (Section 4) attempts to track the active vertices with the thread bins (i.e., small bin, med bin and large bin). Note that each active vertex is stored in one of these three bins based upon its degree. After a deadlock free software global barrier (Section 5), SIMD-X checks whether an overflow happens in any of the thread bins, which leads to either a ballot filter-based active lists generation or a simple prefix-scan based concatenation of all thread bins to produce the active lists (line 17-21).</p><p>In Figure <ref type="figure" target="#fig_3">3</ref>(b), Line 1 -8 exemplifies the interactions between ACC and SIMD-X. Firstly, SIMD-X will schedule a warp of threads to work on the neighbors of one active vertex from med list. Similarly, Thread and CTA will schedule a thread and CTA to work on each active vertex from small list and large list, respectively. During</p><formula xml:id="formula_3">Init (src){ • dist_curr [src] = 0; • large_list.insert (src); } Active (v){ • return dist_curr [v] != dist_prev [v]; } Compute (edge, weight){ • old_dist = dist_curr [edge.dest]; • new_dist = dist_curr [edge.src] + w; • return old_dist &gt; new_dist ? new_dist: old_dist; } Combine (dist[]){ • return min (dist[]); } Warp (med_list, Compute, Combine, Active, overflow)</formula><p>• for each active vertex v in med_list: //warp in parallel</p><p>• for each neighboring edge set edge <ref type="bibr" target="#b31">[32]</ref> to vertex v:</p><p>• res    <ref type="table" target="#tab_5">3</ref>.</p><formula xml:id="formula_4">[lane_id] = Compute ( edge[lane_id] ); • final = Combine (res[0 -31]); • if lane_id == 0: • metadata_curr[v] = final; • small_bin,</formula><p>Comparison Figure <ref type="figure">4</ref> studies the performance impact of ACC vs. Gunrock. The new ACC model follows a computation then combine approach which pays the extra overhead (i.e., assembling all updates residing in shared memory from participating threads) in order to achieve the benefits of atomic-free updates. Gunrock, in contrast, directly applies the update to vertex status with atomic operations, thereby avoids inter-thread communication but experiences heavier overhead from atomic operation. One can see that ACC is, on average, 12% and 9% faster on vote and aggregation operations, respectively. For vote, the speedup comes from that ACC can schedule all threads to collaboratively determine early termination, which is not possible in Gunrock. Aggregation gains the performance from the elimination of atomic updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Just-In-Time Task Management</head><p>Workload balancing is essential for graph applications.</p><p>The key is to ensure each GPU core, regardless of from which streaming processor, accounts for a similar amount of workload, which is often achieved with the following twin steps. Particularly, in step I: task management, the tasks are classified into various lists, namely small list, med list and large list. In step II: thread assignment, various granularity of GPU threads are scheduled to work on different worklists. That is, a single thread per small task, a warp per medium task and a CTA per large task. Note, Figure <ref type="figure" target="#fig_3">3(b)</ref> presents the pseudo code of step II and the bottom part of Figure <ref type="figure">6</ref> paints the corresponding workflow. We refer the readers to Enterprise <ref type="bibr" target="#b40">[41]</ref> for more details regarding the landscape of this attempt.</p><p>Unlike prior work <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b75">77,</ref><ref type="bibr" target="#b49">50]</ref> which places particular efforts at step II, SIMD-X focuses on step I as we find it to be the major culprit that offsets the benefits of workload balancing. In the following, we will first analyze the drawback of existing batch filter method, then describe two new filters, and JIT selection mechanism. Drawback of batch filter. This approach <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b10">11]</ref> first loads all the edges of the active vertices to construct an active edge list. Still using the example of SSSP in  Next, batch filter checks these edges and updates vertex metadata a2 , followed by recording the updated vertices in thread bin at step a3 . Eventually, batch filter will concatenate these thread bins to arrive at a potentially unsorted and redundant next active list -{b, f , h, f , g, i}.</p><p>Note, thread private local storage -thread bin -is used to avoid the expensive atomic operations, because multiple threads would need atomic operation to put active vertices directly into next active list. We observe several drawbacks when using the batch filter for various graph algorithms. First, the active list can consume up to 2•|E| memory space because majority of the vertices in a graph can become active at one iteration <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>, which is especially true for popular social and web graphs. Considering GPU has very limited onboard memory (e.g., 16 GB), this restriction makes largescale GPU-based graph computing intractable. Second, batch filter produces a worklist with unsorted and redundant active vertices, e.g., next active list -{b, f , h, f , g, i} of Figure <ref type="figure" target="#fig_5">5</ref>(a), which will lead to poor memory performance for next iteration computation. Ballot filter is designed to overcome all these shortcomings. It first loads the neighbors of active vertices and immediately updates vertex metadata. As shown at step b1 in Figure <ref type="figure" target="#fig_5">5</ref>(b), the neighbors of {e, c} get updated immediately. Afterwards, thread 0 and 1 (red and blue lines) will exploit ballot scan to inspect the updated metadata and record those updated vertices in local thread bin at step b3 . The eventual step is similar to batch filter -we concatenate these two thread bins to get the next active list, whereas, with sorted nonredundant active vertices.</p><p>Ballot scan is the key to comprehend why we arrive at a better next active list. In steps b2 and b3 of Figure <ref type="figure" target="#fig_5">5</ref>(b), threads 0 and 1 perform coalesced scan of vertex meta-data, and with the CUDA ballot() primitive, return a bit variable '01' to the first thread. Here 1 means active and 0 otherwise, in this case, vertex a is not active while b is. Through collaboratively working on the entire metadata array, the first thread eventually gets the bit value '0100' for the first four vertices, while the second thread '011110' for the remaining six vertices. Consequently, this approach produces a sorted active list, that is, {b, f , g, h, i} in b3 .</p><p>We intentionally schedule thread 0 and 1 to collaboratively scan the metadata in order to achieve coalesced memory access during scan, as well as, making thread 0 and 1 account for a continuous range of vertices, that is, vertices ad to thread 0 and ei to thread 1. This achieves the dual benefits: coalesced scan and sorted active vertices in next active list. Last but not the least, this scheduling lends ballot filter to be many-thread safe.</p><p>We also notice an unpublished parallel efforts from Khorasani's dissertation <ref type="bibr" target="#b30">[31]</ref> which is closely related to ballot filter. However, his design relies on atomic operation to compute the offsets of active vertices from each Warp in the next active list and subsequently assigns merely a single thread from the Warp to enqueue all these active vertices. This design implies twin disadvantages comparing to ours. First, atomic operation-based offset computation cannot yield sorted active lists. Second, single thread-based active vertices recording tends to be slower than Warp-based one which is our design.</p><p>Ballot filter is not without its own issue, especially when the amount of active vertices is low. In that case, scanning the metadata array would account for the majority of the runtime. For instance, in ER and RC graphs, 99.23% and 96.67% of the time is spent on scanning metadata in ballot filter alone solution, respectively. Online filter is designed to accommodate the issue faced by ballot filter. In the first step, this method loads the ac-  tive neighbors, updates the destination vertex, and simultaneously records the active vertices in the thread bin. In the last step, it assembles all thread bins together as the next active list. When the number of active vertices is small, this approach turns out to be extremely fast. Here we use the early stage of SSSP as an example to explain its working process. As shown in Figure <ref type="figure" target="#fig_5">5</ref>(c), {b, d} are active vertices, this approach loads their neighbors for computation ( c1 ), and immediately records the destination vertices. Eventually, it generates {e, c} as the active list for the next iteration as shown in c2 . It is also important to note that for online filter, the vertices in the active list may become redundant, and out of order.</p><p>In graph computing, it is possible that one GPU thread may encounter exceeding amount of active vertices, e.g., our tests on Twitter graph shows one GPU thread can reap more than 4,096 active vertices. Clearly, one cannot afford such a large thread bin for all threads, thus online filter will inevitably suffer from an overflow problem. Fortunately, ballot filter largely avoids this issue because it first updates the metadata of active vertices b2 , which, to some extent, averages out the active vertices across threads in step b3 .  Just-In-Time control adaptively exploits ballot and online filters to retain the best performance. As shown in Figure <ref type="figure">6</ref>, SIMD-X always activates the online filter first. Once a thread bin overflows, SIMD-X will switch on ballot filter to generate the correct task list for the next iteration. It is also worthy of mentioning that after JIT task management, we assign various granularity of threads to different lists in order to balance workload.</p><p>Interestingly, we find out that various algorithms and graph datasets present different selection patterns which tie closely to the amount of workload, that is, the higher volume of workload often results in the activation of ballot filter. As shown in Figure <ref type="figure" target="#fig_7">7</ref>, BFS and SSSP typically use the ballot filter in the middle of the computation and online filter at the beginning and end. For high-diameter graphs, BFS and SSSP avoid the use of ballot filter. For instance, ER and RC always use the online filter along 2,578, 555, 5,086 and 675 iterations. k-Core activates the ballot filter at the initial iterations, i.e., typically the first two iterations except RC which only experiences one iteration because all its vertices have &lt; 16 neighbors. At the extreme, BP and PageRank need the ballot filter at exactly the first iteration of computation.</p><p>Overflow thresholds for online filter. Clearly, this parameter directly determines when to switch on ballot filter, thereby affects the overall performance. Figure <ref type="figure" target="#fig_9">8(a)</ref> presents the normalized performance with respect to various thresholds. As expected, a too low or too high threshold limits the performance because in either case, SIMD-X is forced to switch to ballot filter either too early or too late, leading to performance penalty. As such, in this work we select 64 as the predefined overflow threshold for all algorithms.</p><p>Overhead of online filter. After switching to ballot filter, JIT task management also executes the online filter in case it needs to switch back. Figure <ref type="figure" target="#fig_9">8</ref>(b) studies the overhead of this design. On average, there is 0.02% slowdown, with the maximum of 2.1% observed for the OR graph. The reason for the small overhead is because online filter only tracks upto 64 (predefined threshold) active vertices for the next iteration and this operation is not on the critical path of the execution. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Push-Pull Based Kernel Fusion</head><p>Kernel fusion <ref type="bibr" target="#b71">[73]</ref>, a common optimization for a collection of iterative GPU applications, such as graph computing and deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>, reduces expensive overhead of kernel invocation, as well as minimizes the global memory traffic as the life time of registers and shared memory is limited in each kernel. However, traditional efforts, such as Gunrock <ref type="bibr" target="#b75">[77]</ref> and Xiao et al <ref type="bibr" target="#b77">[79]</ref>, fail to achieve cross the global barrier kernel fusion. This section starts with our observation and analysis of potential deadlock in the mainstream global barrier design <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b80">82]</ref> and subsequently introduces a lightweighted deadlock free solution which enables the global thread synchronization within the fused kernel. However, aggressive kernel fusion requires a large amount of the registers and thus supports fewer parallel warps which could hurt the overall performance. To this end, we introduce a push-pull based kernel fusion to minimize the kernel invocation times and register consumption.</p><p>Software global barrier is needed to enable the balanced kernel fusion. Generally speaking, this approach uses an array -lock -to synchronize all GPU threads upon arrival and departure. During the processing, it assumes the thread CTA as the monitor while the remaining threads as workers. At arrival, each worker CTA updates its own status in lock. Once all worker CTAs have arrived, the monitor changes the statuses of all CTAs to departure, allowing all threads to proceed forward. This approach, unfortunately, suffers from potential deadlock <ref type="bibr" target="#b77">[79]</ref>, as illustrated in Figure <ref type="figure" target="#fig_10">9</ref>. Specifically, the worker thread CTAs may hold all GPU hardware resources, such as streaming processors, registers and shared memory, while waiting for the monitor to update the lock array. In the meantime, the monitor cannot update the lock array, due to lack of hardware resources (e.g., thread over subscription). Compiler-based deadlock free barrier. SIMD-X utilizes the barrier in a way to ensure that every CTA, regardless of a work or the monitor, can obtain hardware resources when needed. This is achieved through comparing the resources needed by the kernels, against the total available resources. Based on the GPU architecture, we can obtain the total amount of regis-ters (#registerPerSMX) that can be provided by each streaming processor, e.g., 65,536 registers of NVIDIA K40 GPUs and 32,768 from K20 GPUs. On the other hand, we can collect the register consumption (#registerPerT hread) of each kernel at the compilation stage. Putting together, SIMD-X is able to calculate the appropriate thread configuration for kernels.</p><p>The number of CTA can be computed as follows:</p><formula xml:id="formula_5">#CTA = f loor( #registersPerSMX #registersPerT hread • #threadsPerCTA ) • #SMX (1)</formula><p>where Notably, portable Inter-Block Barrier <ref type="bibr" target="#b68">[69]</ref> is closely relevant to our effort. However, this method proposes extremely complicated thread block management mechanism that requires to distinguish whether one thread block will execute useful workloads or not during runtime. This requires nontrivial programmer efforts and scheduling overhead. In comparison, our method achieves this deadlock-free configuration before runtime and is completely transparent to the end users.</p><p>Push-Pull based kernel fusion. As shown in Table <ref type="table" target="#tab_4">2</ref>, the register consumption (using the compilation flag -Xptxas -v) increases from average 25 to 110, that is a 4.4× difference. Note, consuming many registers will curb the number of active threads (according to equation 1). Unfortunately, majority of the graph algorithms are data intensive, thus prefer a higher volume of active threads because more active threads can better hide the frequent memory access stalls caused by data intensive applications. Consequently, we need a balanced fusion strategy that keeps both register consumption and kernel invocation low.</p><p>To this end, SIMD-X leverages the push-pull model used in the graph algorithms. That is, such algorithms often use push or pull based computing in several consecutive iterations. Lines 12 -21 from Figure <ref type="figure" target="#fig_3">3(b)</ref>, for example, discuss the pull model and we can fuse these lines into a single GPU kernel. Similarly, push model can also be fused into a single kernel. Section 6 details how pull/push iterations occur in various graph algorithms.</p><p>SIMD-X adopts the pull-push model as in <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b40">41]</ref>, by controlling where (in/out edge) Compute happens and how to Combine the results and apply (in atomic or atomic free manner). Particularly, in the push model, SIMD-X conducts Compute on the out neighbors of each active vertex, and relies on atomic operations to apply the  updates to the destination vertices. In contrast, the pull model schedules Compute on the in neighbors of active vertices, and uses atomic-free strategy to Combine all updates and apply to the destination vertices. As different iterations favor one model over the other, we follow a similar rule as in Ligra <ref type="bibr" target="#b65">[66]</ref> to alternate between the push and pull models. That is, when the workload on the push model works on more than 30% of the edges, SIMD-X will switch to pull model. The idea of push-pull based kernel fusion is to fuse kernels around the pull and push computing. In other words, for the push-based iterations, SIMD-X fuses different compute kernels (for thread, warp, CTA), as well as task management kernel, into one push kernel. The kernel only terminates when the computation finishes or it needs to switch to pull computing according to the criterion discussed in Section 3.3. Similar optimizations are done for the pull-based iterations.</p><p>Using the new push-pull based fusion, the register consumption decreases to 48 and 55 thus increases the configurable thread count by 50%. Table <ref type="table" target="#tab_4">2</ref> presents the register consumption and kernel invocation of different kernel fusion techniques. By using the push-pull based kernel fusion, the kernel relaunch is merely three while its register consumption is cut by half.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Graph Algorithms and Datasets</head><p>In addition to SSSP that is discussed in Section 3.3, this section further presents a variety of algorithms which are implemented on SIMD-X to examine the expressiveness of ACC programming model, and performance impacts of task management and kernel fusion techniques. BFS <ref type="bibr" target="#b40">[41]</ref> traverses a graph level by level. At each level, it loads all neighbors that are connected to vertices visited in the preceding level, inspects their statuses (metadata), and subsequently marks those unvisited neighbors as active for the next iteration. Notably, BFS conducts synchronizations at the end of each level, relies on vote to combine the updates. During the entire process of traversal, BFS typically experiences light workload at the beginning and end of the computation while heavy workload in the middle.</p><p>Belief propagation (BP), also known as sum-product message passing algorithm, infers the posterior probability of each event based on the likelihoods and prior probabilities of all related events. Once modeled as a graph (Bayesian network or Markov random fields), each event becomes a vertex with all incoming vertices and edges as related events and corresponding likelihoods. In BP, vertex possibility is the metadata. k-Core (KC), which is widely used in graph visualization application <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b52">53]</ref>, iteratively deletes the vertices whose degree is less than k until all remaining vertices in this graph possess more than k neighbors. k-Core experiences large volume of workloads at initial iterations and follows with light workloads. This work uses a default value of k = 16. PageRank (PR) <ref type="bibr" target="#b56">[57]</ref> updates the rank value of one vertex based on the contribution of all in-neighbors iteratively till all vertices have stable rank values. Because the contributions of in neighbors are summarized to the destination vertex, we start PageRank with the pull model and agg sum as the merge operation. At the end of PageRank, we switch to the push model because the majority of the vertices are stable <ref type="bibr" target="#b85">[87]</ref>. The switch is decided by a decision tree.</p><p>Graph Benchmarks. We evaluate on a wide range of graphs as shown in Table <ref type="table" target="#tab_5">3</ref>, which falls into four types, i.e., social networks, road maps, hyperlink web and synthetic graphs. Particularly, Facebook <ref type="bibr" target="#b16">[17]</ref>, Live-Journal <ref type="bibr" target="#b67">[68]</ref>, Orkut <ref type="bibr" target="#b67">[68]</ref>, Pokec <ref type="bibr" target="#b67">[68]</ref>, and Twitter <ref type="bibr" target="#b37">[38]</ref> are common social networks. Europe-osm <ref type="bibr" target="#b11">[12]</ref> and RoadCA-net [70] are two large roadmap graphs, and UK-2002 [70] is a web graph. Furthermore, we use Graph500 generator to generate Kron24 <ref type="bibr" target="#b5">[6]</ref>, and GTgraph <ref type="bibr" target="#b18">[19]</ref> for R-MAT and random graphs. Europe-osm and RoadCAnet are high diameter graphs, with 2570 and 555 as their diameters, respectively. LiveJournal, Pokec, Twitter and UK-2002 are medium diameter graphs, i.e., 10 -30 as their diameters. The diameters of the remaining graphs are all smaller than 10. For graphs without edge weight, we use a random generator to generate one weight for each edge similar to Gunrock <ref type="bibr" target="#b74">[76]</ref>. These graphs are stored in compressed sparse row (CSR) format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>We implement SIMD-X<ref type="foot" target="#foot_0">1</ref> with 5,660 lines of CUDA and C++ code. All the algorithms presented in Section 6 are implemented with around 100 lines of C++ code. The source code is compiled by GCC 4.8.5 and NVIDIA nvcc 7.5 with the optimization flag as O3. In this work, we evaluate SIMD-X on a Linux workstation with two Intel Xeon E5-2683 CPUs (14 physical cores with 28 hyperthreads), and 512GB main memory. Throughout the evaluation, we use uint32 as the vertex ID and uint64 as index and evaluate our system on NVIDIA K40 GPUs unless otherwise is specified. We also test SIMD-X on earlier K20 and latest P100 GPUs. The timing is started once the graph data is loaded in GPU global memory. Each result is reported with an average of 64 runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Comparison with State-of-the-art</head><p>Table <ref type="table" target="#tab_6">4</ref> summarizes the runtime of SIMD-X against Galois and Gunrock which are state-of-the-art CPU and GPU graph processing systems, respectively, as well as CuSha (GPU) and Ligra (CPU), two popular graph frameworks. The take aways of this table are two folds. First, SIMD-X is both space efficient and robust. As one can see, since CuSha requires edge list as the input for computation, it cannot accommodate large graphs (e.g., FB and TW) across all algorithms. Besides, since Gunrock requires large amount of space for batch filter, it suffers out of memory (OOM) error for all larger graphs in SSSP. Even CPU systems (Galois and Ligra) enjoys affluent memory space (512 GB) from CPU, they cannot converge to a result for high diameter graphs. That is, Galois cannot converge for SSSP on ER while Ligra fails to obtain result for BFS on UK graph.</p><p>Second, SIMD-X outperforms all graph processing frameworks. In general, SIMD-X is 24×, 2.9×, 6.5× and 3.3× faster than CuSha, Gunrock, Galois and Ligra, respectively. In BFS, SIMD-X bests CuSha, Gunrock, Ga-lois and Ligra by 9.6×, 4.8×, 2.1× and 2.4×, respectively. We also notice that SIMD-X is slower than Galois on the RD graph because workload balancing brings negligible benefits to uniform-degree graph (RD). Also, SIMD-X is slightly worse than Ligra on RM graph since this graph only has a diameter of four thus both JIT task management and kernel fusion brings trivial benefits to GPU based graph systems, as evident by much lower performance on CuSha and Gunrock.</p><p>In PageRank, SIMD-X achieves 1.2×, 2.1×, 2.3× and 4× speedups over CuSha, Gunrock, Galois and Ligra, respectively. Note, even CuSha cannot support all large graphs due to large memory space consumption, it performs roughly similar to SIMD-X with even outperforming SIMD-X on LJ and OR. This is generally because PageRank tends to be more computation intensive than other graph algorithms and needs to compute all edges, curbing the benefits of task management and kernel fusion. However, edge list format (of CuSha) doubles memory consumption, facing OOM for large graphs.</p><p>For SSSP, SIMD-X wins 21×, on average, over all four projects. We project SIMD-X to better outperform all systems than observed for BFS algorithm because SSSP experiences more iterations with larger volume of active tasks, placing more favor towards ACC model, JIT task management and push-pull based kernel fusion. However, because Gunrock fails to accommodate all large graphs, our benefits cannot surface -ending with merely 1.8× speedup. Second, CuSha spends 519,674 ms on the high diameter ER graph which is 480× slower than SIMD-X because task management is absent from CuSha. We also notice Galois performs better than SIMD-X in RD, again, due to the uniform degree distribution.</p><p>For k-Core, where k = 32, SIMD-X wins Ligra by 20×. Such a striking advantage comes from three parts. First, as reflected by Figure <ref type="figure" target="#fig_12">11</ref>(b), k-Core generates extensive amount of workload variations thus benefits tremendously from JIT task management. Second, k-Core's iterative nature also enjoys the benefits from push-pull based kernel fusion, as shown in Figure <ref type="figure" target="#fig_13">12(c</ref>). Lastly, the flexibility of ACC allows innovative k-Core algorithm designs -we will stop further subtracting the degree of destination vertex once the destination vertex's degree goes below k -this reduces tremendous unnecessary updates. Note comparisons of Belief Propagation, as well as other systems for k-Core are not included because those systems fail to support such algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Benefits of Various Techniques</head><p>This section studies the performance impacts brought by JIT task management and push-pull based kernel fusion. As we have presented in Section 4, JIT task management only works for applications that experience work-  load variations, that is, BFS, k-Core and SSSP. On the other hand, push-pull based kernel fusion is applicable for all five algorithms On average, JIT task management presented in Figure <ref type="figure" target="#fig_12">11</ref>, is 16×, 26× and 4.5× faster than the ballot filter for BFS, k-Core and SSSP. As expected, online filter alone cannot work for many graphs, particularly large ones, e.g., Facebook, Twitter and UK2002 graphs in BFS and SSSP. Without considering overflow problem (ER and RC graphs), JIT task management adds a small 1-2% overhead on top of the online filter on BFS and SSSP.</p><p>On k-Core, JIT task management is, on average, 28.5× and 5% faster than ballot and online filter, respectively. We also observe that the ballot filter outperforms the online filter on ER and RC graphs by 3.4× and 1.7×, because k-Core removes a large volume of vertices which favors the former to produce a non-redundant and sorted work list.</p><p>Push-pull based kernel fusion brings, on average, 43% and 25% improvement over non-fusion and all-fusion across all algorithms and graphs. In particular, push-pull based kernel fusion tops non-fusion by 74%, 11%, 85%, 10% and 66% on BFS, BP, k-Core, PageRank and SSSP. BFS, k-Core and SSSP achieves more performance gains because they are not computation intensive and tend to run a higher number of iterations. For all fusion, our new kernel fusion is 55%, 6%, 62%, 25% and 11% faster on BFS, BP, k-Core, PageRank and SSSP. It is important to note that all fusion is not always beneficial, i.e., all fuse option of PageRank is average 13% slower than no fusion because all fusion limits the amount of configurable threads. However, for memory intensive applications, like BFS and SSSP on ER and RC, all fusion is on average 2× better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Performance on Different GPUs</head><p>We also evaluate SIMD-X, Gunrock and CuSha on various GPU models, such as K20 and P100 GPUs. It is not surprising to see tht SIMD-X presents the biggest performance gain on the latest GPUs. In detail, SIMD-X on K40 and P100 performs 1.7× and 5.1× better than K20 GPU. In contrast, Gunrock merely gets 1.1× and 1.7× performance improvement when moving from K20 to K40 and P100, respectively. Similarly for CuSha, its performance on K40 and P100 are 1.2× and 3.5× better than K20, respectively. The root cause of this disparity is that SIMD-X's kernel fusion technique can dynamically configure its GPU kernels to the fitting thread count on the corresponding hardware so as to achieve the peak performance. For instance, the thread count increases by 1.2× and 5.1× on K40 and P100 than on K20 for BFS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Work</head><p>Recent advance in graph computing falls in algorithm innovation <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b85">87,</ref><ref type="bibr" target="#b14">15]</ref>, framework developments <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b88">90,</ref><ref type="bibr" target="#b90">92,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b72">74,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b78">80,</ref><ref type="bibr" target="#b82">84,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b86">88,</ref><ref type="bibr" target="#b73">75,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b87">89,</ref><ref type="bibr" target="#b84">86,</ref><ref type="bibr" target="#b83">85,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b76">78,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b79">81]</ref> and accel- erator optimizations <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b12">13]</ref>. This section covers relevant work from three aspects: programming model, task management and kernel fusion.</p><p>Recently, we witness an array of graph analytical models. For instance, "think like a graph" <ref type="bibr" target="#b69">[71]</ref> requires each vertex to obtain the view of the entire partition on one machine in order to minimize the communication cost. Furthermore, domain specific programming language systems, such as Galois <ref type="bibr" target="#b53">[54]</ref>, Green-Marl <ref type="bibr" target="#b22">[23]</ref> and Trinity <ref type="bibr" target="#b62">[63]</ref>, allow programmers to write single-threaded source code while enjoying multi-threaded processing. In comparison, SIMD-X decouples the goal of programming simplicity and performance: with ACC, SIMD-X ultimately designs a data-parallel abstraction for deploying irregular graph applications on GPU. With JIT task management and push-pull based kernel fusion, SIMD-X is an order of magnitude faster than state-of-the-art CPU and GPU frameworks.</p><p>Task management is an important optimization for GPU-based graph computing. Besides batch filter <ref type="bibr" target="#b74">[76,</ref><ref type="bibr" target="#b49">50]</ref>, there also exist other task management approaches -strided filter <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43]</ref> and atomic filter <ref type="bibr" target="#b45">[46]</ref>. Particularly, strided filter resembles ballot filter but the former one experiences strided memory access when scanning the metadata thus performs up to 16× worse than ballot filter. Atomic filter relies is similar to online filter but it relies on atomic operation to put active vertices into global active list which suffers from orders of magnitude slow down than online filter. Besides ballot and online filter bests batch, stride and atomic filter, SIMD-X goes further via introducing a JIT controller to adaptively use online filter and ballot filter to further improve the performance. We also find that JIT task management can be exploited to help manage active lists for other applications such as warp segmentation <ref type="bibr" target="#b31">[32]</ref> and CSR5 <ref type="bibr" target="#b43">[44]</ref>.</p><p>Kernel fusion affects applications far beyond graph computations. SIMD-X is closely related to global software barrier <ref type="bibr" target="#b77">[79,</ref><ref type="bibr" target="#b80">82]</ref>. However, previous work fails to identify the deadlock issue in this global software barrier problem, thus no solution towards this issue. In con-trast, SIMD-X unveils, systematically analyzes, and resolves this problem. To avoid high register consumption, SIMD-X further selectively fuse kernels via exploiting the special kernel launching patterns of graph algorithms. It is also important to mention existing work <ref type="bibr" target="#b71">[73]</ref> that only fuse kernels to barrier boundary. In comparison, SIMD-X fuses kernels across barriers. Our design can also benefit the popular Persistent Kernel <ref type="bibr" target="#b19">[20]</ref> designs which have been found suffer from deadlock issues when the occupancy exceed an unknown bound <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b23">24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work, we propose SIMD-X, a parallel graph computing framework that supports programming and processing of single instruction multiple, complex, data on GPUs. Specifically, the Active-Compute-Combine (ACC) model provides ease of programming to programmers, while just-in-time task management and pushpull based kernel fusion leverage the opportunities for system-level optimization. Using SIMD-X, a user can program a graph algorithm in tens of lines of code, while achieving significant speedup over the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SSSP on a graph, with nine vertices {a, b, c, d, e, f, g, h, i} and ten undirected edges (with weights). SSSP iteratively computes on the graph and generates the distance array. Particularly, heavy and light shadows represent active and most recently updated vertices, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: SIMD-X architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3(a) presents the Combine examples of SSSP. Particularly, BP summarizes all updates, where SSSP combines all updates from compute by selecting the minimum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) SSSP in ACC model and (b) ACC abstraction and task management within SIMD-X framework. computation, each thread will conduct a local Compute and Combine at line 4. Once finished, a cross Warp Combine happens at line 5. Eventually, the first thread from the Warp applies the final updates (without atomic operation) and store this vertex (if active) into corresponding thread bins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 FBFigure 4 :</head><label>44</label><figDesc>Figure 4: Speedup of our ACC model over Gunrock. Note vote and aggregation operations are materialized by BFS and SSSP algorithms, respectively, and x-axis contains the graph datasets which are defined in Table3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Three task management methods. Particularly, batch filter and ballot filter work on Figure 1(d) to produce a task list for next iteration. Online filter does that for Figure 1(c). Note, we assume the arrow flows of red and blue indicate the execution paths of red and blue threads.</figDesc><graphic url="image-9.png" coords="7,138.76,198.82,68.51,68.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 (</head><label>1</label><figDesc>Figure 1(c), this step loads neighbors of vertex {e, c} and constructs the active edge list in a1 of Figure 5 (a).Next, batch filter checks these edges and updates vertex metadata a2 , followed by recording the updated vertices in thread bin at step a3 . Eventually, batch filter will concatenate these thread bins to arrive at a potentially unsorted and redundant next active list -{b, f , h, f , g, i}. Note, thread private local storage -thread bin -is used to avoid the expensive atomic operations, because multiple threads would need atomic operation to put active vertices directly into next active list.We observe several drawbacks when using the batch filter for various graph algorithms. First, the active list can consume up to 2•|E| memory space because majority of the vertices in a graph can become active at one iteration<ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>, which is especially true for popular social and web graphs. Considering GPU has very limited onboard memory (e.g., 16 GB), this restriction makes largescale GPU-based graph computing intractable. Second, batch filter produces a worklist with unsorted and redundant active vertices, e.g., next active list -{b, f , h, f , g, i} of Figure5(a), which will lead to poor memory performance for next iteration computation. Ballot filter is designed to overcome all these shortcomings. It first loads the neighbors of active vertices and immediately updates vertex metadata. As shown at step b1 in Figure5(b), the neighbors of {e, c} get updated immediately. Afterwards, thread 0 and 1 (red and blue lines) will exploit ballot scan to inspect the updated metadata and record those updated vertices in local thread bin at step b3 . The eventual step is similar to batch filter -we concatenate these two thread bins to get the next active list, whereas, with sorted nonredundant active vertices.Ballot scan is the key to comprehend why we arrive at a better next active list. In steps b2 and b3 of Figure5(b), threads 0 and 1 perform coalesced scan of vertex meta-</figDesc><graphic url="image-19.png" coords="7,176.58,219.03,68.51,68.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Ballot filter activation patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The (a) relative performance of JIT management with respective to various online filter overflow thresholds on BFS and (b) the overhead of JIT on SSSP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Deadlock in software global barrier, where 'C', '$', and 'R' represent core, L1 cache and register, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Consecutive iterations from graph algorithms often cluster to push and model computation separately: (a) all fusion, (b) selective fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Benefit of just-in-time task management, normalized to the performance of the ballot filter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Benefit of push-pull based kernel fusion, normalized to the performance of no fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-1.png" coords="1,-9.00,-10.14,630.00,272.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic url="image-2.png" coords="1,-9.00,533.00,630.00,269.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison between ACC and relevant GPU-based programming models.</figDesc><table><row><cell>denotes desirable feature.</cell></row><row><cell>Stages</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Workload balancing with the essential two steps: the novel JIT task management from SIMD-X and the thread assignment.</figDesc><table><row><cell></cell><cell>I: JIT task management</cell><cell cols="2">Online filter</cell></row><row><cell></cell><cell></cell><cell>Overflow ?</cell><cell>Yes</cell><cell>Ballot filter</cell></row><row><cell>Iteration ++</cell><cell></cell><cell>No</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Small_list</cell><cell cols="2">Med_list</cell><cell>Large_list</cell></row><row><cell></cell><cell>II: Thread</cell><cell>Thread</cell><cell cols="2">Warp</cell><cell>CTA</cell></row><row><cell></cell><cell>assignment</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1 thread</cell><cell cols="2">32 threads</cell><cell>256 threads</cell></row><row><cell>Figure 6: BFS</cell><cell>FB KR LJ OR PK ER</cell><cell></cell><cell></cell><cell>Iteration 2,578</cell></row><row><cell></cell><cell>RD RC</cell><cell></cell><cell></cell><cell>555</cell></row><row><cell></cell><cell>RM UK</cell><cell></cell><cell></cell><cell>29</cell></row><row><cell></cell><cell>TW</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FB ER</cell><cell></cell><cell></cell><cell>21</cell></row><row><cell>k-Core</cell><cell>KR LJ OR PK</cell><cell></cell><cell></cell><cell>38 25 20</cell></row><row><cell></cell><cell>RD</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RC</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RM UK</cell><cell></cell><cell></cell><cell>68</cell></row><row><cell></cell><cell>TW</cell><cell></cell><cell></cell></row><row><cell></cell><cell>FB</cell><cell></cell><cell></cell></row><row><cell></cell><cell>ER KR</cell><cell></cell><cell></cell><cell>5,086</cell></row><row><cell></cell><cell>LJ</cell><cell></cell><cell></cell><cell>21</cell></row><row><cell></cell><cell>OR</cell><cell></cell><cell></cell></row><row><cell>SSSP</cell><cell>PK</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RD</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RC UK RM</cell><cell></cell><cell></cell><cell>34 675</cell></row><row><cell></cell><cell>TW</cell><cell></cell><cell></cell></row><row><cell></cell><cell>: Online filter</cell><cell>: Ballot filter</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>#threadsPerCTA is configured by a user, i.e., 128 by default. For example, when deploying a kernel, each thread consumes 110 registers, and on K40 that contains 15 SMXs, each of which contains 65,536 registers. If #threadsPerCTA is set to 128, one gets #CTA = ceil( 65536 110×128 ) × 15 = 60. As a result, we can configure this kernel as CTA and thread count per CTA as 60 and 128, respectively.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Register consumption for various kernels.</figDesc><table><row><cell>Kernel</cell><cell>Thread</cell><cell cols="2">Push (no fusion) Warp CTA</cell><cell>Task mgt</cell><cell>Thread</cell><cell cols="2">Pull (no fusion) Warp CTA</cell><cell>Task mgt</cell><cell cols="2">Selective fusion push pull</cell><cell>All fusion</cell></row><row><cell>Register consumption</cell><cell>26</cell><cell>27</cell><cell>28</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>22</cell><cell>30</cell><cell>48</cell><cell>50</cell><cell>110</cell></row><row><cell>Kernel launching count</cell><cell></cell><cell></cell><cell></cell><cell cols="2">up to 40,688</cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell></cell><cell>1</cell></row><row><cell>Begin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Push model: JIT task management</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Graph Dataset.</figDesc><table><row><cell cols="3">Graph Name Abbrev. Vertex Count</cell><cell>Edge Count</cell></row><row><cell>Facebook</cell><cell>FB</cell><cell cols="2">16,777,215 775,824,943</cell></row><row><cell>Europe-osm</cell><cell>ER</cell><cell cols="2">50,912,018 108,109,319</cell></row><row><cell>Kron24</cell><cell>KR</cell><cell cols="2">16,777,216 536,870,911</cell></row><row><cell>LiveJournal</cell><cell>LJ</cell><cell cols="2">4,847,571 136,950,781</cell></row><row><cell>Orkut</cell><cell>OR</cell><cell cols="2">3,072,626 234,370,165</cell></row><row><cell>Pokec</cell><cell>PK</cell><cell>1,632,803</cell><cell>61,245,127</cell></row><row><cell>Random</cell><cell>RD</cell><cell cols="2">4,000,000 511,999,999</cell></row><row><cell>RoadCA-net</cell><cell>RC</cell><cell>1,971,281</cell><cell>5,533,213</cell></row><row><cell>R-MAT</cell><cell>RM</cell><cell cols="2">3,999,983 511,999,999</cell></row><row><cell>UK-2002</cell><cell>UK</cell><cell cols="2">18,520,343 596,227,523</cell></row><row><cell>Twitter</cell><cell>TW</cell><cell cols="2">25,165,811 787,169,139</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Runtime (ms) of SIMD-X and Gunrock, and Galois. A K40 GPU is used to test SIMD-X and Gunrock, and a CPU with 28 threads for Galois. The blank space indicates the test cannot complete for the given algorithm and graph.</figDesc><table><row><cell cols="2">Alg</cell><cell>System</cell><cell>FB</cell><cell></cell><cell>ER</cell><cell>KR</cell><cell>LJ</cell><cell>OR</cell><cell>PK</cell><cell>RD</cell><cell>RC</cell><cell cols="2">RM</cell><cell>UK</cell><cell>TW</cell><cell>Avg. speedup</cell></row><row><cell></cell><cell></cell><cell>SIMD-X</cell><cell>198</cell><cell></cell><cell>400</cell><cell>130</cell><cell>59</cell><cell>40</cell><cell>20</cell><cell>82</cell><cell>15</cell><cell>47</cell><cell></cell><cell>308</cell><cell>210</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>CuSha</cell><cell></cell><cell></cell><cell></cell><cell>988</cell><cell>224</cell><cell>341</cell><cell>72</cell><cell>435</cell><cell>297</cell><cell cols="2">674</cell><cell>4298</cell><cell>9.6</cell></row><row><cell cols="2">BFS</cell><cell cols="2">Gunrock 685</cell><cell></cell><cell>849</cell><cell>677</cell><cell>71</cell><cell>225</cell><cell>44</cell><cell>647</cell><cell>146</cell><cell cols="2">506</cell><cell>312</cell><cell>697</cell><cell>4.8</cell></row><row><cell></cell><cell></cell><cell>Galois</cell><cell>482</cell><cell></cell><cell>1068</cell><cell>140</cell><cell>139</cell><cell>42</cell><cell>34</cell><cell>48</cell><cell>53</cell><cell>65</cell><cell></cell><cell>229</cell><cell>322</cell><cell>2.1</cell></row><row><cell></cell><cell></cell><cell>Ligra</cell><cell cols="2">1086</cell><cell>1426</cell><cell>176</cell><cell>89</cell><cell>51</cell><cell>31</cell><cell>88</cell><cell>48</cell><cell>40</cell><cell></cell><cell>496</cell><cell>2.4</cell></row><row><cell></cell><cell></cell><cell>SIMD-X</cell><cell cols="2">1553</cell><cell>346</cell><cell cols="2">1141 236</cell><cell>435</cell><cell cols="3">118 1105 13</cell><cell cols="2">800</cell><cell>637</cell><cell>1525</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>CuSha</cell><cell></cell><cell></cell><cell></cell><cell>1704</cell><cell>182</cell><cell>323</cell><cell cols="3">180 1402 15</cell><cell cols="2">886</cell><cell>1.2</cell></row><row><cell cols="2">PR</cell><cell cols="3">Gunrock 3004</cell><cell>884</cell><cell cols="2">3129 275</cell><cell>927</cell><cell cols="3">166 2963 43</cell><cell cols="3">2208 784</cell><cell>3180</cell><cell>2.1</cell></row><row><cell></cell><cell></cell><cell>Galois</cell><cell cols="2">4552</cell><cell>603</cell><cell cols="2">3069 424</cell><cell cols="4">1061 218 3576 20</cell><cell cols="3">2067 842</cell><cell>4178</cell><cell>2.3</cell></row><row><cell></cell><cell></cell><cell>Ligra</cell><cell cols="2">16780</cell><cell>1368</cell><cell cols="5">2000 1324 1786 310 809</cell><cell>35</cell><cell cols="2">1703</cell><cell>9360</cell><cell>4</cell></row><row><cell></cell><cell></cell><cell>SIMD-X</cell><cell cols="2">1816</cell><cell>1080</cell><cell>998</cell><cell>284</cell><cell>604</cell><cell cols="3">143 1505 223</cell><cell cols="2">478</cell><cell>703</cell><cell>1344</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>CuSha</cell><cell></cell><cell></cell><cell cols="3">519674 1663 692</cell><cell cols="4">1120 260 1610 438</cell><cell cols="2">1236</cell><cell>62</cell></row><row><cell cols="2">SSSP</cell><cell>Gunrock</cell><cell></cell><cell></cell><cell>1206</cell><cell cols="2">1220 431</cell><cell cols="4">1259 336 5059 229</cell><cell></cell><cell></cell><cell>1.8</cell></row><row><cell></cell><cell></cell><cell>Galois</cell><cell cols="2">161596</cell><cell></cell><cell cols="4">8485 1785 1166 356</cell><cell>747</cell><cell cols="4">3440 5877 9081 1818</cell><cell>15</cell></row><row><cell></cell><cell></cell><cell>Ligra</cell><cell cols="2">14067</cell><cell>3043</cell><cell cols="6">2893 1627 1567 605 3353 301</cell><cell cols="3">2783 1300 5217</cell><cell>3.7</cell></row><row><cell cols="3">SIMD-X k-Core Ligra</cell><cell cols="2">366 6337</cell><cell>78 1167</cell><cell cols="5">131 2813 1707 1700 654 27 60 63 33 10</cell><cell>4 36</cell><cell cols="2">19 235</cell><cell>151 6627 5783 277</cell><cell>-20</cell></row><row><cell>0.5 1 1.5 2 Speedup</cell><cell>132 132</cell><cell>Ballot Online JIT</cell><cell>2.4</cell><cell>29 29</cell><cell>2.6</cell><cell>0.5 1 1.5 2 Speedup</cell><cell cols="2">2.8 2.3 2.7 Online Ballot JIT</cell><cell>8 8</cell><cell cols="2">14 14 4 2.1</cell><cell>0.5 1 1.5 2 Speedup</cell><cell></cell><cell>30 30</cell><cell>2.6 2.6</cell><cell>Ballot Online JIT</cell></row><row><cell>0</cell><cell cols="5">FB ER KR LJ OR PK RD RC RM UK TW</cell><cell>0</cell><cell cols="5">FB ER KR LJ OR PK RD RC RM UK TW</cell><cell>0</cell><cell cols="2">FB ER KR LJ OR PK RD RC RM UK TW</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0">SIMD-X source https://github.com/asherliu/simd-x.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>The authors would like to thank the anonymous reviewers and Shepherd Chris Rossbach for their feedback and suggestions. Hang Liu did part of this work at the George Washington University. This work was partially supported by National Science Foundation CAREER award 1350766 and grants 1618706 and 1717774 at George Washington University and CRII Award No. 1850274 at University of Massachusetts Lowell. We also would like to gracefully acknowledge the support from XSEDE supercomputers and Amazon AWS, as well as the NVIDIA Corporation for the donation of the Titan Xp and Quadro P6000 GPUs to the University of Massachusetts Lowell.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Nvidia cuda c programming guide</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>NVIDIA Corporation</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Squeezing out all the value of loaded data: An out-ofcore graph processing system with reduced disk i/o</title>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 USENIX Annual Technical Conference (USENIX ATC 17)</title>
				<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="125" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Directionoptimizing Breadth-First Search</title>
		<author>
			<persName><surname>Beamer</surname></persName>
		</author>
		<author>
			<persName><surname>Asanovic</surname></persName>
		</author>
		<author>
			<persName><surname>Patterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CECI: Compact Embedding Cluster Index for Scalable Subgraph Matching</title>
		<author>
			<persName><forename type="first">Bibek</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data, SIGMOD</title>
				<meeting>the 2019 International Conference on Management of Data, SIGMOD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-MAT: A Recursive Model for Graph Mining</title>
		<author>
			<persName><forename type="first">Deepayan</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
				<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Computation and communication efficient graph processing with distributed immutable view</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international symposium on Highperformance parallel and distributed computing</title>
				<meeting>the 23rd international symposium on Highperformance parallel and distributed computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="215" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kineograph: taking the pulse of a fast-changing and connected world</title>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuetian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM european conference on Computer Systems</title>
				<meeting>the 7th ACM european conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="85" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cliff</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Work-efficient parallel GPU methods for single-source shortest paths</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Baxter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th International Symposium on Parallel &amp; Distributed Processing (IPDPS)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="349" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="http://download.geofabrik.de/europe-latest.osm.bz2" />
		<title level="m">European Open Stream Map</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BFS: Data Centric Breadth-First Search on FPGAs</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Finnerty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Sherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><surname>Dr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Design Automation Conference</title>
				<meeting>the 56th Annual Design Automation Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page">208</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do Bitcoin Users Really Care About Anonymity? An Analysis of the Bitcoin Transaction Graph</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Gaihre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1198" to="1207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">XBFS: eXploring Runtime Optimizations for Breadth-First Search on GPUs</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Gaihre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenlin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on High-performance parallel and distributed computing (HPDC)</title>
				<meeting>the international symposium on High-performance parallel and distributed computing (HPDC)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can GPGPU Programming Be Liberated from the Data-Parallel Bottleneck?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Benedict</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename><surname>Gaster</surname></persName>
		</author>
		<author>
			<persName><surname>Howes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Practical Recommendations on Crawling Online Social Networks</title>
		<author>
			<persName><forename type="first">Minas</forename><surname>Gjoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Kurant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carter</forename><forename type="middle">T</forename><surname>Butts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athina</forename><surname>Markopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PowerGraph: Distributed Graph-Parallel Computation on Natural Graphs</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Joseph E Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haijie</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A suite of synthetic random graph generators</title>
		<author>
			<persName><surname>Gtgraph</surname></persName>
		</author>
		<ptr target="http://www.cse.psu.edu/~madduri/software/GTgraph/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A study of persistent threads style GPU programming for GPGPU workloads</title>
		<author>
			<persName><forename type="first">Kshitij</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Innovative Parallel Computing (InPar)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Chronos: a graph engine for temporal graph analysis</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth European Conference on Computer Systems</title>
				<meeting>the Ninth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TurboGraph: a fast parallel graph engine handling billion-scale graphs in a single PC</title>
		<author>
			<persName><forename type="first">Wook-Shin</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangyeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungyeol</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong-Hoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hwanjo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of international conference on Knowledge discovery and data mining (SIGKDD)</title>
				<meeting>international conference on Knowledge discovery and data mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Green-Marl: a DSL for easy and efficient graph analysis</title>
		<author>
			<persName><forename type="first">Sungpack</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Chafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edic</forename><surname>Sedlar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunle</forename><surname>Olukotun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</title>
				<meeting>the seventeenth international conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="349" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Heterogeneous-race-free memory models</title>
		<author>
			<persName><forename type="first">Blake</forename><forename type="middle">A</forename><surname>Derek R Hower</surname></persName>
		</author>
		<author>
			<persName><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedict</forename><forename type="middle">R</forename><surname>Bradford M Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Gaster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">K</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Reinhardt</surname></persName>
		</author>
		<author>
			<persName><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGARCH Computer Architecture News</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="427" to="440" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High-Performance Triangle Counting on GPUs</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE High Performance extreme Computing Conference (HPEC)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tricore: Parallel triangle counting on gpus</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC18: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Big data machine learning and graph analytics: Current state and future challenges</title>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="16" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">iSpan: Parallel Identification of Strongly Connected Components with Spanning Trees</title>
		<author>
			<persName><forename type="first">Yuede</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC18: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
				<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Distributed Multi-GPU System for Fast Graph Processing</title>
		<author>
			<persName><forename type="first">Zhihao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongkee</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galen</forename><surname>Shipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pat</forename><surname>Mccormick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mattan</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Aiken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="297" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">High Performance Vertex-Centric Graph Analytics on GPUs</title>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Riverside</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Dissertation</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable simd-efficient graph processing on gpus</title>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Laxmi N Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Architecture and Compilation (PACT), 2015 International Conference on</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">CuSha: vertex-centric graph processing on GPUs</title>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Khorasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keval</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><surname>Laxmi N Bhuyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international symposium on High-performance parallel and distributed computing</title>
				<meeting>the 23rd international symposium on High-performance parallel and distributed computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="239" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GTS: A fast and scalable graph processing method based on streaming topology to GPUs</title>
		<author>
			<persName><forename type="first">Min-Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyuhyeon</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunseok</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwook</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data</title>
				<meeting>the 2016 International Conference on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="447" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">G-store: high-performance graph store for trillion-edge processing</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Falcon: scaling IO performance in multi-SSD volumes</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 USENIX Conference on Usenix Annual Technical Conference</title>
				<meeting>the 2017 USENIX Conference on Usenix Annual Technical Conference</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="41" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GraphOne: A Data Store for Real-time Analytics on Evolving Graphs</title>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th USENIX Conference on File and Storage Technologies (FAST 19)</title>
				<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="249" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">Haewoon</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hosung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Moon</surname></persName>
		</author>
		<title level="m">WWW</title>
				<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>What is Twitter</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">GraphChi: large-scale graph computation on just a PC</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Blelloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation</title>
				<meeting>the 10th USENIX conference on Operating Systems Design and Implementation</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="31" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graphene: Fine-Grained IO Management for Graph Computing</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th USENIX Conference on File and Storage Technologies (FAST 17)</title>
				<imprint>
			<publisher>USENIX Association</publisher>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enterprise: Breadth-First Graph Traversal on GPU Servers</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for High Performance Computing, Networking, Storage and Analysis (SC)</title>
				<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graphene: Fine-Grained IO Management for Graph Computing</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Howie</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th USENIX Conference on File and Storage Technologies. USENIX Association</title>
				<meeting>the 15th USENIX Conference on File and Storage Technologies. USENIX Association</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">iBFS: Concurrent Breadth-First Search on GPUs</title>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 International Conference on Management of Data (SIGMOD)</title>
				<meeting>the 2016 International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CSR5: An efficient storage format for cross-platform sparse matrixvector multiplication</title>
		<author>
			<persName><forename type="first">Weifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Vinter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM on International Conference on Supercomputing</title>
				<meeting>the 29th ACM on International Conference on Supercomputing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="339" to="350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Graphlab: A new framework for parallel machine learning</title>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Bickson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An effective GPU implementation of breadth-first search</title>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 47th design automation conference</title>
				<meeting>the 47th design automation conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mosaic: Processing a trillion-edge graph on a single machine</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changwoo</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanidhya</forename><surname>Kashyap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Woonhak</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth European Conference on Computer Systems</title>
				<meeting>the Twelfth European Conference on Computer Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="527" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Higher-order and tuple-based massively-parallel prefix sums</title>
		<author>
			<persName><forename type="first">Sepideh</forename><surname>Maleki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burtscher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>ACM</publisher>
			<biblScope unit="volume">51</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pregel: a system for largescale graph processing</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Malewicz</surname></persName>
		</author>
		<author>
			<persName><surname>Matthew H Austern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Aart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Bik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilan</forename><surname>Dehnert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naty</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Leiser</surname></persName>
		</author>
		<author>
			<persName><surname>Czajkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 ACM SIGMOD International Conference on Management of data</title>
				<meeting>the 2010 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scalable GPU graph traversal</title>
		<author>
			<persName><forename type="first">Duane</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Grimshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
				<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
		<title level="m">∆-Stepping: A Parallel Single Source Shortest Path Algorithm. Algorithms-ESA&apos;98</title>
				<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Immortalgraph: A system for storage and analysis of temporal graphs</title>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayan</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Storage</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distributed k-Core Decomposition</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Montresor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Miorandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A lightweight infrastructure for graph analytics</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lenharth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Pingali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP)</title>
				<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="456" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tigr: Transforming Irregular Graphs for GPU-Friendly Graph Processing</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nodehi</forename><surname>Sabet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junqiao</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijia</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="622" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">NVIDIA Kepler GK110 Architecture Whitepaper</title>
		<author>
			<persName><surname>Nvidia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">The PageRank Citation Ranking: Bringing Order to the Web</title>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Stanford InfoLab</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Managing large graphs on multi-cores with graph awareness</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Vijayan Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuetian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><surname>Haridasan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of USENIX conference on Annual Technical Conference. USENIX Association</title>
				<meeting>USENIX conference on Annual Technical Conference. USENIX Association</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Chaos: Scaleout Graph Processing from Secondary Storage</title>
		<author>
			<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Bindschaedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmina</forename><surname>Malicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Symposium on Operating Systems Principles</title>
				<meeting>the 25th Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">X-stream: Edge-centric graph processing using streaming partitions</title>
		<author>
			<persName><forename type="first">Amitabha</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Mihailovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willy</forename><surname>Zwaenepoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles</title>
				<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">GraphReduce: processing large-scale graphs on accelerator-based systems</title>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaiwen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Leon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">High Performance Computing, Networking, Storage and Analysis, 2015 SC-International Conference for</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Trinity: A distributed graph engine on a memory cloud</title>
		<author>
			<persName><forename type="first">Bin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Management of Data (SIGMOD)</title>
				<meeting>International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="505" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Software Hardware Co-Optimized BFS on FPGAs</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Sherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Finnerty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
				<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="190" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fast and Concurrent RDF Queries with RDMA-Based Distributed Graph Exploration</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youyang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI) 16)</title>
				<imprint>
			<biblScope unit="page" from="317" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ligra: a lightweight graph processing framework for shared memory</title>
		<author>
			<persName><forename type="first">Julian</forename><surname>Shun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><forename type="middle">E</forename><surname>Blelloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">BFS and Coloring-based Parallel Algorithms for Strongly Connected Components and Related Problems</title>
		<author>
			<persName><forename type="first">Sivasankaran</forename><surname>George M Slota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamesh</forename><surname>Rajamanickam</surname></persName>
		</author>
		<author>
			<persName><surname>Madduri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Parallel and Distributed Processing Symposium (IPDPS)</title>
				<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<ptr target="http://snap.stanford.edu/data/" />
		<title level="m">SNAP: Stanford Large Network Dataset Collection</title>
				<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Portable inter-workgroup barrier synchronisation for GPUs</title>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alastair</forename><forename type="middle">F</forename><surname>Donaldson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Batty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Gopalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zvonimir</forename><surname>Rakamarić</surname></persName>
		</author>
		<ptr target="http://www.cise.ufl.edu/research/sparse/matrices/" />
	</analytic>
	<monogr>
		<title level="m">The University of Florida: Sparse Matrix Collection</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="39" to="58" />
		</imprint>
	</monogr>
	<note>ACM SIGPLAN Notices</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">From Think Like a Vertex to Think Like a Graph</title>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Balmin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Severin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirish</forename><surname>Corsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tatikonda</surname></persName>
		</author>
		<author>
			<persName><surname>Mcpherson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
				<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Task Management for Irregular-Parallel Workloads on the GPU</title>
		<author>
			<persName><forename type="first">Stanley</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anjul</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on High Performance Graphics. Eurographics Association</title>
				<meeting>the Conference on High Performance Graphics. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scalable Kernel Fusion for Memory-bound GPU applications</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Wahib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Maruyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis</title>
				<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">GraphQ: Graph Query Processing with Abstraction Refinement-Scalable and Programmable Analytics over Very Large Graphs on a Single PC</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Su</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fast and Concurrent RDF Queries using RDMA-assisted GPU Graph Exploration</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><forename type="middle">Lou</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Annual Technical Conference (USENIX ATC 18)</title>
				<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Gunrock: A high-performance graph processing library on the GPU</title>
		<author>
			<persName><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Riffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Owens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</title>
				<meeting>the 20th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="265" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">Yangzihao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuechao</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuduo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Osama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenshan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weitang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">T</forename><surname>Riffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01170</idno>
		<title level="m">GPU Graph Analytics</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">G ra M: scaling graph computation to the trillions</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilong</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wencong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youshan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafei</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lidong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth ACM Symposium on Cloud Computing</title>
				<meeting>the Sixth ACM Symposium on Cloud Computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="408" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Inter-block GPU communication via fast barrier synchronization</title>
		<author>
			<persName><forename type="first">Shucai</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu-Chun</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Parallel &amp; Distributed Processing (IPDPS)</title>
				<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Sync or async: Time to fuse for distributed graph-parallel computation</title>
		<author>
			<persName><forename type="first">Chenning</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibing</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyu</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices (PPoPP)</title>
				<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="194" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Parallel graph processing. Encyclopedia of Big Data Technologies</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">StreamScan: fast scan algorithms for GPUs without global barrier synchronization</title>
		<author>
			<persName><forename type="first">Shengen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoping</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunquan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPoPP</title>
				<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Aekyeung Moon, Hang Liu, and Seung Woo Son. Efficient Encoding and Reconstruction of HPC Datasets for Checkpoint/Restart</title>
		<author>
			<persName><forename type="first">Jialing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Mass Storage Systems and Technologies</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">NUMA-aware graph-structured analytics</title>
		<author>
			<persName><forename type="first">Kaiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices (PPoPP)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="183" to="193" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Exploring the Hidden Dimension in Graph Processing</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
				<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="285" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Wonderland: A Novel Abstraction-Based Out-Of-Core Graph Processing System</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengying</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</title>
				<meeting>the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="608" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Maiter: An Asynchronous Graph Processing Framework for Delta-based Accumulative Iterative Computation</title>
		<author>
			<persName><forename type="first">Yanfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cuirong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Submillisecond Stateful Stream Querying over Fastevolving Linked Data</title>
		<author>
			<persName><forename type="first">Yunhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Symposium on Operating Systems Principles (SOSP)</title>
				<meeting>the 26th Symposium on Operating Systems Principles (SOSP)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="614" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Making caches work for graph analytics</title>
		<author>
			<persName><forename type="first">Yunming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Kiriansky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charith</forename><surname>Mendis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Big Data (Big Data)</title>
				<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="293" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">FlashGraph: processing billion-node graphs on an array of commodity SSDs</title>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Disa</forename><surname>Mhembere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randal</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Vogelstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Szalay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th USENIX Conference on File and Storage Technologies</title>
				<meeting>the 13th USENIX Conference on File and Storage Technologies</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Medusa: Simplified graph processing on gpus. Parallel and Distributed Systems</title>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingsheng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1543" to="1552" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">GridGraph: Large-Scale Graph Processing on a Single Machine Using 2-Level Hierarchical Partitioning</title>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenguang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 USENIX Annual Technical Conference (USENIX ATC 15)</title>
				<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="375" to="386" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
