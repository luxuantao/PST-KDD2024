<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Visual Context by Comparison</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-15">15 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minchul</forename><surname>Kim</surname></persName>
							<email>minchul.kim@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jongchan</forename><surname>Park</surname></persName>
							<email>jcpark@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Seil</forename><surname>Na</surname></persName>
							<email>seil.na@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chang</forename><forename type="middle">Min</forename><surname>Park</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Seoul National University Hospital</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
							<email>dgyoo@lunit.io</email>
							<affiliation key="aff0">
								<orgName type="institution">Lunit Inc</orgName>
								<address>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Visual Context by Comparison</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-15">15 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.07506v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.7.2" ident="GROBID" when="2022-12-25T12:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Context Modeling</term>
					<term>Attention Mechanism</term>
					<term>Chest X-Ray</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finding diseases from an X-ray image is an important yet highly challenging task. Current methods for solving this task exploit various characteristics of the chest X-ray image, but one of the most important characteristics is still missing: the necessity of comparison between related regions in an image. In this paper, we present Attend-and-Compare Module (ACM) for capturing the difference between an object of interest and its corresponding context. We show that explicit difference modeling can be very helpful in tasks that require direct comparison between locations from afar. This module can be plugged into existing deep learning models. For evaluation, we apply our module to three chest Xray recognition tasks and COCO object detection &amp; segmentation tasks and observe consistent improvements across tasks. The code is available at https://github.com/mk-minchul/attend-and-compare.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among a variety of medical imaging modalities, chest X-ray is one of the most common and readily available examinations for diagnosing chest diseases. In the US, more than 35 million chest X-rays are taken every year <ref type="bibr" target="#b19">[20]</ref>. It is primarily used to screen diseases such as lung cancer, pneumonia, tuberculosis and pneumothorax to detect them at their earliest and most treatable stage. However, the problem lies in the heavy workload of reading chest X-rays. Radiologists usually read tens or hundreds of X-rays every day. Several studies regarding radiologic errors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9]</ref> have reported that 20-30% of exams are misdiagnosed. To compensate for this shortcoming, many hospitals equip radiologists with computer-aided diagnosis systems. The recent developments of medical image recognition models have shown potentials for growth in diagnostic accuracy <ref type="bibr" target="#b25">[26]</ref>.</p><p>With the recent presence of large-scale chest X-ray datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref>, there has been a long line of works that find thoracic diseases from chest X-rays using deep learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29]</ref>. Most of the works attempt to classify thoracic diseases, and some of the works further localize the lesions. To improve recognition performance, Yao et al. <ref type="bibr" target="#b40">[41]</ref> handles varying lesion sizes and Mao et al. <ref type="bibr" target="#b24">[25]</ref> No Finding Nodule Fig. <ref type="figure">1</ref>: An example of a comparison procedure for radiologists. Little differences indicate no disease (blue), the significant difference is likely to be a lesion (red).</p><p>takes the relation between X-rays of the same patient into consideration. Wang et al. <ref type="bibr" target="#b34">[35]</ref> introduces an attention mechanism to focus on regions of diseases.</p><p>While these approaches were motivated by the characteristics of chest Xrays, we paid attention to how radiology residents are trained, which led to the following question: why don't we model the way radiologists read X-rays? When radiologists read chest X-rays, they compare zones <ref type="bibr" target="#b0">[1]</ref>, paying close attention to any asymmetry between left and right lungs, or any changes between semantically related regions, that are likely to be due to diseases. This comparison process provides contextual clues for the presence of a disease that local texture information may fail to highlight. Fig. <ref type="figure">1</ref> illustrates an example of the process. Previous studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref> proposed various context models, but none addressed the need for the explicit procedure to compare regions in an image.</p><p>In this paper, we present a novel module, called Attend-and-Compare Module (ACM), that extracts features of an object of interest and a corresponding context to explicitly compare them by subtraction, mimicking the way radiologists read X-rays. Although motivated by radiologists' practices, we pose no explicit constraints for symmetry, and ACM learns to compare regions in a datadriven way. ACM is validated over three chest X-ray datasets <ref type="bibr" target="#b36">[37]</ref> and object detection &amp; segmentation in COCO dataset <ref type="bibr" target="#b23">[24]</ref> with various backbones such as ResNet <ref type="bibr" target="#b13">[14]</ref>, ResNeXt <ref type="bibr" target="#b39">[40]</ref> or DenseNet <ref type="bibr" target="#b15">[16]</ref>. Experimental results on chest X-ray datasets and natural image datasets demonstrate that the explicit comparison process by ACM indeed improves the recognition performance.</p><p>Contributions To sum up, our major contributions are as follows:</p><p>1. We propose a novel context module called ACM that explicitly compares different regions, following the way radiologists read chest X-rays. 2. The proposed ACM captures multiple comparative self-attentions whose difference is beneficial to recognition tasks. 3. We demonstrate the effectiveness of ACM on three chest X-ray datasets <ref type="bibr" target="#b36">[37]</ref> and COCO detection &amp; segmentation dataset <ref type="bibr" target="#b23">[24]</ref> with various architectures.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Context Modeling</head><p>Context modeling in deep learning is primarily conducted with the self-attention mechanism <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref>. Attention related works are broad and some works do not explicitly pose themselves in the frame of context modeling. However, we include them to highlight different methods that make use of global information, which can be viewed as context.</p><p>In the visual recognition domain, recent self-attention mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref> generate dynamic attention maps for recalibration (e.g., emphasize salient regions or channels). Squeeze-and-Excitation network (SE) <ref type="bibr" target="#b14">[15]</ref> learns to model channel-wise attention using the spatially averaged feature. A Style-based Recalibration Module (SRM) <ref type="bibr" target="#b21">[22]</ref> further explores the global feature modeling in terms of style recalibration. Convolutional block attention module (CBAM) <ref type="bibr" target="#b37">[38]</ref> extends SE module to the spatial dimension by sequentially attending the important location and channel given the feature. The attention values are computed with global or larger receptive fields, and thus, more contextual information can be embedded in the features. However, as the information is aggregated into a single feature by average or similar operations, spatial information from the relationship among multiple locations may be lost.</p><p>Works that explicitly tackle the problem of using context stem from using pixel-level pairwise relationships <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>. Such works focus on long-range dependencies and explicitly model the context aggregation from dynamically chosen locations. Non-local neural networks (NL) <ref type="bibr" target="#b35">[36]</ref> calculate pixel-level pairwise relationship weights and aggregate (weighted average) the features from all locations according to the weights. The pairwise modeling may represent a more diverse relationship, but it is computationally more expensive. As a result, Global-Context network (GC) <ref type="bibr" target="#b3">[4]</ref> challenges the necessity of using all pairwise relationships in NL and suggests to softly aggregate a single distinctive context feature for all locations. Criss-cross attention (CC) <ref type="bibr" target="#b16">[17]</ref> for semantic segmentation reduces the computation cost of NL by replacing the pairwise relationship attention maps with criss-cross attention block which considers only horizontal and vertical directions separately. NL and CC explicitly model the pairwise relationship between regions with affinity metrics, but the qualitative results in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17]</ref> demonstrate a tendency to aggregate features only among foreground objects or among pixels with similar semantics.</p><p>Sharing a similar philosophy, there have been works on contrastive attention <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref>. MGCAM <ref type="bibr" target="#b30">[31]</ref> uses the contrastive feature between persons and backgrounds, but it requires extra mask supervision for persons. C-MWP <ref type="bibr" target="#b41">[42]</ref> is a technique for generating more accurate localization maps in a contrastive manner, but it is not a learning-based method and uses pretrained models.</p><p>Inspired by how radiologists diagnose, our proposed module, namely, ACM explicitly models a comparing mechanism. The overview of the module can be found in Fig. <ref type="figure">2</ref>. Unlike the previous works proposed in the natural image domain, our work stems from the precise need for incorporating difference operation in </p><formula xml:id="formula_0">X − µ ∈ ℝ !×#×$ Q K K − Q ∈ ℝ !×#×# Y ∈ ℝ $×%×&amp; X ∈ ℝ $×%×&amp; µ ∈ ℝ !×#×# ℝ $ ' ×#×# P ∈ ℝ !×#×#</formula><p>Fig. <ref type="figure">2</ref>: Illustration of the ACM module. It takes in an input feature and uses the mean-subtracted feature to calculate two feature vectors (K, Q). Each feature vector (K or Q) contains multiple attention vectors from multiple locations calculated using grouped convolutions and normalizations. The difference of the vectors is added to the main feature to make the information more distinguishable. The resulting feature is modulated channel-wise, by the global information feature.</p><p>reading chest radiographs. Instead of finding an affinity map based attention as in NL <ref type="bibr" target="#b35">[36]</ref>, ACM explicitly uses direct comparison procedure for context modeling; instead of using extra supervision to localize regions to compare as in MGCAM <ref type="bibr" target="#b30">[31]</ref>, ACM automatically learns to focus on meaningful regions to compare. Importantly, the efficacy of our explicit and data-driven contrastive modeling is shown by the superior performance over other context modeling works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Chest X-ray as a Context-Dependent Task</head><p>Recent releases of the large-scale chest X-ray datasets <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> showed that commonly occurring diseases can be classified and located in a weakly-supervised multi-label classification framework. ResNet <ref type="bibr" target="#b13">[14]</ref> and DenseNet <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> have set a strong baseline for these tasks, and other studies have been conducted on top of them to cover various issues of recognition task in the chest X-ray modality.</p><p>To address the issue of localizing diseases using only class-level labels, Guendel et al. <ref type="bibr" target="#b11">[12]</ref> propose an auxiliary localization task where the ground truth of the location of the diseases is extracted from the text report. Other works use attention module to indirectly align the class-level prediction with the potentially abnormal location <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref> without the text reports on the location of the disease. Some works observe that although getting annotations for chest X-rays is costly, it is still helpful to leverage both a small number of location annotations and a large number of class-level labels to improve both localization and classification performances <ref type="bibr" target="#b22">[23]</ref>. Guan et al. <ref type="bibr" target="#b10">[11]</ref> also proposes a hierarchical hard-attention for cascaded inference.</p><p>In addition to such characteristics inherent in the chest X-ray image, we would like to point out that the difference between an object of interest and a corresponding context could be the crucial key for classifying or localizing several diseases as it is important to compare semantically meaningful locations. However, despite the importance of capturing the semantic difference between regions in chest X-ray recognition tasks, no work has dealt with it yet. Our work is, to the best of our knowledge, the first to utilize this characteristic in the Chest X-ray image recognition setting.</p><p>3 Attend-and-Compare Module</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>Attend-and-Compare Module (ACM) extracts an object of interest and the corresponding context to compare, and enhances the original image feature with the comparison result. Also, ACM is designed to be light-weight, self-contained, and compatible with popular backbone architectures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref>. We formulate ACM comprising three procedures as</p><formula xml:id="formula_1">Y = f ACM (X) = P (X + (K − Q)),<label>(1)</label></formula><p>where f ACM is a transformation mapping an input feature</p><formula xml:id="formula_2">X ∈ R C×H×W to an output feature Y ∈ R C×H×W in the same dimension. Between K ∈ R C×1×1 and Q ∈ R C×1×1</formula><p>, one is intended to be the object of interest and the other is the corresponding context. ACM compares the two by subtracting one from the other, and add the comparison result to the original feature X, followed by an additional channel re-calibration operation with P ∈ R C×1×1 . Fig. <ref type="figure">2</ref> illustrates Equation (1). These three features K, Q and P are conditioned on the input feature X and will be explained in details below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Components of ACM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object of Interest and Corresponding Context</head><p>To fully express the relationship between different spatial regions of an image, ACM generates two features (K, Q) that focus on two spatial regions of the input feature map X. At first, ACM normalizes the input feature map as X := X − µ where µ is a Cdimensional mean vector of X. We include this procedure to make training more stable as K and Q will be generated by learnable parameters (W K , W Q ) that are shared by all input features. Once X is normalized, ACM then calculates K with W K as</p><formula xml:id="formula_3">K = i,j∈H,W exp(W K X i,j ) H,W exp(W K X h,w ) X i,j ,<label>(2)</label></formula><p>where X i,j ∈ R C×1×1 is a vector at a spatial location (i, j) and W K ∈ R C×1×1 is a weight of a 1 × 1 convolution. The above operation could be viewed as applying 1 × 1 convolution on the feature map X to obtain a single-channel attention map in R 1×H×W , applying softmax to normalize the attention map, and finally weighted averaging the feature map X using the normalized map. Q is also modeled likewise, but with W Q . K and Q serve as features representing important regions in X. We add K − Q to the original feature so that the comparative information is more distinguishable in the feature.</p><p>Channel Re-calibration In light of the recent success in self-attention modules which use a globally pooled feature to re-calibrate channels <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>, we calculate the channel re-calibrating feature P as</p><formula xml:id="formula_4">P = σ • conv 1×1 2 • ReLU • conv 1×1 1 (µ),<label>(3)</label></formula><p>where σ and conv 1×1 denote a sigmoid function and a learnable 1×1 convolution function, respectively. The resulting feature vector P will be multiplied to X + (K − Q) to scale down certain channels. P can be viewed as marking which channels to attend with respect to the task at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Group Operation</head><p>To model a relation of multiple regions from a single module, we choose to incorporate group-wise operation. We replace all convolution operations with grouped convolutions <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b39">40]</ref>, where the input and the output are divided into G number of groups channel-wise, and convolution operations are performed for each group separately. In our work, we use the grouped convolution to deliberately represent multiple important locations from the input.</p><p>Here, we compute G different attention maps by applying grouped convolution to X, and then obtain the representation</p><formula xml:id="formula_5">K = [K 1 , • • • , K G ]</formula><p>by aggregating each group in X with each attention as follows:</p><formula xml:id="formula_6">K g = i,j∈H,W exp(W g K X g i,j ) H,W exp(W g K X g h,w ) X g i,j ,<label>(4)</label></formula><p>where g refers to g-th group.</p><p>Loss Function ACM learns to utilize comparing information within an image by modeling {K, Q} whose difference can be important for the given task. To further ensure diversity between them, we introduce an orthogonal loss. based on a dot product. It is defined as</p><formula xml:id="formula_7">orth (K, Q) = K • Q C ,<label>(5)</label></formula><p>where C refers to the number of channels. Minimizing this loss can be viewed as decreasing the similarity between K and Q. One trivial solution to minimizing the term would be making K or Q zeros, but they cannot be zeros as they come from the weighted averages of X. The final loss function for a target task can be written as</p><formula xml:id="formula_8">task + λ M m orth (K m , Q m ),<label>(6)</label></formula><p>where task refers to a loss for the target task, and M refers to the number of ACMs inserted into the network. λ is a constant for controlling the effect of the orthogonal constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Placement of ACMs</head><p>In order to model contextual information in various levels of feature representation, we insert multiple ACMs into the backbone network. In ResNet, following the placement rule of SE module <ref type="bibr" target="#b14">[15]</ref>, we insert the module at the end of every Bottleneck block. For example, a total of 16 ACMs are inserted in ResNet-50. Since DenseNet contains more number of DenseBlocks than ResNet's Bottleneck block, we inserted ACM in DenseNet every other three DenseBlocks. Note that we did not optimize the placement location or the number of placement for each task. While we use multiple ACMs, the use of grouped convolution significantly reduces the computation cost in each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate ACM in several datasets: internally-sourced Emergency-Pneumothorax (Em-Ptx) and Nodule (Ndl) datasets for lesion localization in chest X-rays, Chest X-ray14 <ref type="bibr" target="#b36">[37]</ref> dataset for multi-label classification, and COCO 2017 <ref type="bibr" target="#b23">[24]</ref> dataset for object detection and instance segmentation. The experimental results show that ACM outperforms other context-related modules, in both chest X-ray tasks and natural image tasks.</p><p>Experimental Setting Following the previous study <ref type="bibr" target="#b1">[2]</ref> on multi-label classification with chest X-Rays, we mainly adopt ResNet-50 as our backbone network.</p><p>To show generality, we sometimes adopt DenseNet <ref type="bibr" target="#b15">[16]</ref> and ResNeXt <ref type="bibr" target="#b39">[40]</ref> as backbone networks. In classification tasks, we report class-wise Area Under the Receiver Operating Characteristics (AUC-ROC) for classification performances.</p><p>For localization tasks, we report the jackknife free-response receiver operating characteristic (JAFROC) <ref type="bibr" target="#b4">[5]</ref> for localization performances. JAFROC is a metric widely used for tracking localization performance in radiology. All chest X-ray tasks are a weakly-supervised setting <ref type="bibr" target="#b26">[27]</ref> in which the model outputs a probability map for each disease, and final classification scores are computed by global maximum or average pooling. If any segmentation annotation is available, extra map losses are given on the class-wise confidence maps. For all experiments, we initialize the backbone weights with ImageNet-pretrained weights and randomly initialize context-related modules. Experiment details on each dataset are elaborated in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Localization on Em-Ptx Dataset</head><p>Task Overview The goal of this task is to localize emergency-pneumothorax (Em-Ptx) regions. Pneumothorax is a fatal thoracic disease that needs to be treated immediately. As a treatment, a medical tube is inserted into the pneumothorax affected lung. It is often the case that a treated patient repeatedly takes chest X-rays over a short period to see the progress of the treatment. Therefore, a chest X-ray with pneumothorax is categorized as an emergency, but a chest X-ray with both pneumothorax and a tube is not an emergency. The goal of the task is to correctly classify and localize emergency-pneumothorax. To accurately classify the emergency-pneumothorax, the model should exploit the relationship between pneumothorax and tube within an image, even when they are far apart. In this task, utilizing the context as the presence/absence of a tube is the key to accurate classification. We internally collected 8,223 chest X-rays, including 5,606 pneumothorax cases, of which 3,084 cases are emergency. The dataset is from a real-world cohort and contains cases with other abnormalities even if they do not have pneumothorax. We received annotations for 10 major x-ray findings (Nodule, Consolidation, etc) and the presence of medical devices (EKG, Endotracheal tube, Chemoport, etc). Their labels were not used for training. The task is a binary classification and localization of emergency-pneumothorax. All cases with pneumothorax and tube together and all cases without pneumothorax are considered a non-emergency. We separated 3,223 cases as test data, of which 1,574 cases are emergency-pneumothorax, 1,007 cases are non-emergency-pneumothorax, and 642 cases are pure normal. All of the 1,574 emergency cases in the test data were annotated with coarse segmentation maps by board-certified radiologists. Of the 1,510 emergency-pneumothorax cases in the training data, only 930 cases were annotated, and the rest were used with only the class label. An example of a pneumothorax case and an annotation map created by board-certified radiologists is provided in Fig. <ref type="figure" target="#fig_1">3</ref>.</p><p>Training Details We use Binary Cross-Entropy loss for both classification and localization, and SGD optimizer with momentum 0.9. The initial learning rate is set to 0.01. The model is trained for 35 epochs in total and the learning rate is dropped by the factor of 10 at epoch 30. For each experiment setting, we report the average AUC-ROC and JAFROC of 5 runs with different initialization.</p><p>Result The experimental result is summarized in Table <ref type="table" target="#tab_0">1</ref>. Compared to the baseline ResNet-50 and ResNet-101, all the context modules have shown performance improvements. ACM outperforms all other modules in terms of both AUC-ROC and JAFROC. The result supports our claim that the contextual information is critical to Em-Ptx task, and our module, with its explicit featurecomparing design, shows the biggest improvement in terms of classification and localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis on ACM with Em-Ptx dataset</head><p>In this section, we empirically validate the efficacy of each component in ACM and search for the optimal hyperparameters. For the analysis, we use the Em-Ptx dataset. The training setting is identical to the one used in Sec. 4.1. The average of 5-runs is reported.</p><p>Effect of Sub-modules As described in Sec. 3, our module consists of 2 submodules: difference modeling and channel modulation. We experiment with the two sub-modules both separately and together. The results are shown in Table <ref type="table" target="#tab_1">2</ref>. Each sub-module brings improvements over the baseline, indicating the context modeling in each sub-module is beneficial to the given task. Combining the submodules brings extra improvement, showing the complementary effect of the two sub-modules. The best performance is achieved when all sub-modules are used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Groups</head><p>By dividing the features into groups, the module can learn to focus on multiple regions, with only negligible computational or parametric costs. On the other hand, too many groups can result in too few channels per group, which prevents the module from exploiting correlation among many channels. We empirically find the best setting for the number of groups. The results are summarized in Table <ref type="table" target="#tab_1">2</ref>. Note that, training diverges when the number of groups is 1 or 4. The performance improves with the increasing number of groups and saturates after 32. We set the number of groups as 32 across all other experiments, except in DenseNet121 (Sec. 4.4) where we set the number of channel per group to 16 due to channel divisibility.</p><p>Orthogonal Loss Weight We introduce a new hyperparameter λ to balance between the target task loss and the orthogonal loss. Although the purpose of the orthogonal loss is to diversify the compared features, an excessive amount of λ can disrupt the trained representations. We empirically determine the magnitude of λ. Results are summarized in Table <ref type="table" target="#tab_1">2</ref>. From the results, we can empirically verify the advantageous effect of the orthogonal loss on the performance, and the optimal value of λ is 0.1. In the validation set, the average absolute similarities between K and Q with λ = 0, 0.1 are 0.1113 and 0.0394, respectively. It implies that K and Q are dissimilar to some extent, but the orthogonal loss further encourages it. We set λ as 0.1 across all other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Localization on Ndl Dataset</head><p>Task Overview The goal of this task is to localize lung nodules (Ndl) in chest X-ray images. Lung nodules are composed of fast-growing dense tissues and thus are displayed as tiny opaque regions. Due to inter-patient variability, view-point changes and differences in imaging devices, the model that learns to find nodular patterns with respect to the normal side of the lung from the same image (context) may generalize better. We collected 23,869 X-ray images, of which 3,052 cases are separated for testing purposes. Of the 20,817 training cases, 5,817 cases have nodule(s). Of the 3,052 test cases, 694 cases are with nodules. Images without nodules may or may not contain other lung diseases. All cases with nodule(s) are annotated with coarse segmentation maps by board-certified radiologists. We use the same training procedure for the nodule localization task as for the Em-Ptx localization. We train for 25 epochs with the learning rate dropping once at epoch 20 by the factor of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental result is summarized in Table <ref type="table" target="#tab_2">3</ref>. ACM outperforms all other context modeling methods in terms of both AUC-ROC and JAFROC. The results support our claim that the comparing operation, motivated by how radiologists read X-rays, provides a good contextual representation that helps with classifying and localizing lesions in X-rays. Note that the improvements in this dataset may seem smaller than in the Em-Ptx dataset. In the usage of context modules in general, the bigger increase in performance in Em-Ptx dataset is because emergency classification requires knowing both the presence of the tube and the presence of Ptx even if they are far apart. So the benefit of the contextual information is directly related to the performance. However, nodule classification can be done to a certain degree without contextual information.</p><p>Since not all cases need contextual information, the performance gain may be smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-label Classification on Chest X-ray14</head><p>Task Overview In this task, the objective is to identify the presence of 14 diseases in a given chest X-ray image. Chest X-ray14 <ref type="bibr" target="#b36">[37]</ref> dataset is the first largescale dataset on 14 common diseases in chest X-rays. It is used as a benchmark dataset in previous studies <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10]</ref>. The dataset contains 112,120 images from 30,805 unique patients. Image-level labels are mined from imageattached reports using natural language processing techniques (each image can have multi-labels). We split the dataset into training (70%), validation (10%), and test (20%) sets, following previous works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Training Details As shown in  DenseNet but does poorly in ResNet50. However, ACM shows consistency across all architectures. One of the possible reasons is that it provides a context based on a contrasting operation, thus unique and helpful across different architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Detection and Segmentation on COCO</head><p>Task Overview In this experiment, we validate the efficacy of ACM in the natural image domain. Following the previous studies <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>, we use COCO dataset <ref type="bibr" target="#b23">[24]</ref> for detection and segmentation tasks. Specifically, we use COCO Detection 2017 dataset, which contains more than 200,000 images and 80 object categories with instance-level segmentation masks. We train both tasks simultaneously using Mask-RCNN architecture <ref type="bibr" target="#b12">[13]</ref> in Detectron2 <ref type="bibr" target="#b38">[39]</ref>.</p><p>Training Details Basic training details are identical to the default settings in Detectron2 <ref type="bibr" target="#b38">[39]</ref>: learning rate of 0.02 with the batch size of 16. We train for 90,000 iterations, drop the learning rate by 0.1 at iterations 60,000 and 80,000. We use COCO2017-train for training and use COCO2017-val for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The experimental results are summarized in Table <ref type="table" target="#tab_6">6</ref>. Although originally developed for chest X-ray tasks, ACM significantly improves the detection  <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>. The result implies that the comparing operation is not only crucial for X-ray images but is also generally helpful for parsing scene information in the natural image domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Results</head><p>To analyze ACM, we visualize the attention maps for objects of interest and the corresponding context. The network learns to attend different regions, in such a way to maximize the performance of the given task. We visualize the attention maps to see if maximizing performance is aligned with producing attention maps that highlight interpretable locations. Ground-truth annotation contours are also visualized.</p><p>We use the Em-Ptx dataset and COCO dataset for the analysis. Since there are many attention maps to check, we sort the maps by the amount of overlap between each attention map and the ground truth location of lesions. We visualize the attention map with the most overlap. Qualitative results of other tasks are included in the supplementary material due to a space limit.</p><p>Pneumothorax is a collapsed lung, and on the X-ray image, it is often portrayed as a slightly darker region than the normal side of the lung. A simple way to detect pneumothorax is to find a region slightly darker than the normal side. ACM learns to utilize pneumothorax regions as objects of interest and normal lung regions as the corresponding context. The attention maps are visualized in Fig. <ref type="figure" target="#fig_2">4</ref>. It clearly demonstrates that the module attends to both pneumothorax regions and normal lung regions and compare the two sets of regions. The observation is coherent with our intuition that comparing can help recognize, and indicates that ACM automatically learns to do so. We also visualize the attended regions in the COCO dataset. Examples in Fig. <ref type="figure" target="#fig_2">4</ref> shows that ACM also learns to utilize the object of interest and the corresponding context in the natural image domain; for baseball glove, ACM combines the corresponding context information from the players' heads and feet; for bicycle, ACM combines information from roads; for frisbee, ACM combines information from dogs. We observe that the relationship between the object of interest and the corresponding context is mainly from co-occurring semantics, rather than simply visually similar regions. The learned relationship is aligned well with the design principle of ACM; selecting features whose semantics differ, yet whose relationship can serve as meaningful information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a novel self-contained module, named Attend-and-Compare Module (ACM), whose key idea is to extract an object of interest and a corresponding context and explicitly compare them to make the image representation more distinguishable. We have empirically validated that ACM indeed improves the performance of visual recognition tasks in chest X-ray and natural image domains. Specifically, a simple addition of ACM provides consistent improvements over baselines in COCO as well as Chest X-ray14 public dataset and internally collected Em-Ptx and Ndl dataset. The qualitative analysis shows that ACM automatically learns dynamic relationships. The objects of interest and corresponding contexts are different yet contain useful information for the given task.The qualitative analysis shows that ACM automatically learns dynamic relationships. The objects of interest and corresponding contexts are different yet contain useful information for the given task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Examples of pneumothorax cases and annotation maps in Em-Ptx dataset. Lesions are drawn in red. (a) shows a case with pneumothorax, and (b) shows a case which is already treated with a medical tube marked as blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Left: The visualized attention maps for the localization task on Em-Ptx dataset. The 11th group in the 16th module is chosen. Em-Ptx annotations are shown as red contours on the chest X-ray image. Right: The visualization on COCO dataset. Groundtruth segmentation annotations for each category are shown as red contours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on Em-Ptx dataset. Average of 5 random runs are reported for each setting with standard deviation. RN stands for ResNet<ref type="bibr" target="#b13">[14]</ref>.</figDesc><table><row><cell>Method</cell><cell>AUC-ROC</cell><cell>JAFROC Method</cell><cell>AUC-ROC JAFROC</cell></row><row><cell>RN-50</cell><cell cols="2">86.78±0.58 81.84±0.64 RN-101</cell><cell>89.75±0.49 85.36±0.44</cell></row><row><cell cols="4">RN-50 + SE [15] 93.05±3.63 89.19±4.38 RN-101 + SE [15] 90.36±0.83 85.54±0.85</cell></row><row><cell cols="4">RN-50 + NL [36] 94.63±0.39 91.93±0.68 RN-101 + NL [36] 94.24±0.34 91.70±0.83</cell></row><row><cell cols="4">RN-50 + CC [17] 87.73±8.66 83.32±10.36 RN-101 + CC [17] 92.57±0.89 89.75±0.89</cell></row><row><cell cols="4">RN-50 + ACM 95.35±0.12 94.16±0.21 RN-101 + ACM 95.43±0.14 94.47±0.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance with respect to varying module architectures and hyperparameters on Em-Ptx dataset. All the experiments are based on ResNet-50.</figDesc><table><row><cell>Module</cell><cell>AUC-ROC JAFROC</cell><cell>#groups AUC-ROC</cell><cell>JAFROC</cell></row><row><cell cols="2">None X + (K − Q) P X P (X + K) P (X + (K − Q)) 95.35±0.12 94.16±0.21 86.78±0.58 81.84±0.64 94.25±0.31 92.94±0.36 87.16±0.42 82.05±0.30 94.96±0.15 93.59±0.24 (a) Ablations on K, Q and P .</cell><cell cols="2">8 32 64 128 (b) Ablations on number of groups. 90.96±1.88 88.79±2.23 95.35±0.12 94.16±0.21 95.08±0.25 93.73±0.31 94.89±0.53 92.88±0.53</cell></row><row><cell>λ</cell><cell>AUC-ROC</cell><cell>JAFROC</cell><cell></cell></row><row><cell cols="2">0.00 95.11±0.20</cell><cell>93.87±0.20</cell><cell></cell></row><row><cell cols="2">0.01 95.29±0.34</cell><cell>94.09±0.41</cell><cell></cell></row><row><cell cols="2">0.10 95.35±0.12</cell><cell>94.16±0.21</cell><cell></cell></row><row><cell cols="2">1.00 95.30±0.17</cell><cell>94.04±0.11</cell><cell></cell></row><row><cell cols="3">(c) Ablations on orthogonal loss weight λ.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Results on Ndl dataset. Average of 5 random runs are reported for each setting with standard deviation.</figDesc><table><row><cell>Method</cell><cell>AUC-ROC</cell><cell>JAFROC</cell></row><row><cell>ResNet-50</cell><cell cols="2">87.34±0.34 77.35±0.50</cell></row><row><cell cols="3">ResNet-50 + SE [15] 87.66±0.40 77.57±0.44</cell></row><row><cell cols="3">ResNet-50 + NL [36] 88.35±0.35 80.51±0.56</cell></row><row><cell cols="3">ResNet-50 + CC [17] 87.72±0.18 78.63±0.40</cell></row><row><cell>ResNet-50 + ACM</cell><cell cols="2">88.60±0.23 83.03±0.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 ,</head><label>4</label><figDesc>previous works on CXR14 dataset vary in loss, input size, etc. We use CheXNet<ref type="bibr" target="#b28">[29]</ref> implementation 3 to conduct context module comparisons, and varied with the backbone architecture and the input size to find if context modules work in various settings. We use BCE loss with the SGD optimizer with momentum 0.9 and weight decay 0.0001. Although ChexNet uses the input size of 224, we use the input size of 448 as it shows a better result than 224 with DenseNet121. More training details can be found in the supplementary material.</figDesc><table><row><cell>Results Table 5 shows test set performance of ACM compared with other</cell></row><row><cell>context modules in multiple backbone architectures. ACM achieves the best</cell></row><row><cell>performance of 85.39 with ResNet-50 and 85.03 with DenseNet121. We also</cell></row><row><cell>observe that Non-local (NL) and Cross-Criss Attention (CC) does not perform</cell></row><row><cell>well in DenseNet architecture, but attains a relatively good performance of 85.08</cell></row><row><cell>and 85.11 in ResNet-50. On the other hand, a simpler SE module performs well in</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Reported performance of previous works on CXR14 dataset. Each work differs in augmentation schemes and some even in the usage of the dataset. We choose CheXNet<ref type="bibr" target="#b28">[29]</ref> as the baseline model for adding context modules.</figDesc><table><row><cell>Method</cell><cell cols="4">Backbone Arch Loss Family Input size Reported AUC (%)</cell></row><row><cell>Wang et al. [37]</cell><cell>ResNet-50</cell><cell>CE</cell><cell>1,024</cell><cell>74.5</cell></row><row><cell>Yao et al. [41]</cell><cell>ResNet+DenseNet</cell><cell>CE</cell><cell>512</cell><cell>76.1</cell></row><row><cell>Wang and Xia [35]</cell><cell>ResNet-151</cell><cell>CE</cell><cell>224</cell><cell>78.1</cell></row><row><cell>Li et al. [23]</cell><cell>ResNet-v2-50</cell><cell>BCE</cell><cell>299</cell><cell>80.6</cell></row><row><cell>Guendel et al. [12]</cell><cell>DenseNet121</cell><cell>BCE</cell><cell>1,024</cell><cell>80.7</cell></row><row><cell>Guan et al. [10]</cell><cell>DenseNet121</cell><cell>BCE</cell><cell>224</cell><cell>81.6</cell></row><row><cell>ImageGCN [25]</cell><cell>Graph Convnet</cell><cell>CE</cell><cell>224</cell><cell>82.7</cell></row><row><cell>CheXNet [29]</cell><cell>DenseNet121</cell><cell>BCE</cell><cell>224</cell><cell>84.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance in average AUC of various methods on CXR14 dataset. The numbers in the bracket after model names are the input sizes.</figDesc><table><row><cell>Modules</cell><cell cols="2">DenseNet121(448) ResNet-50(448)</cell></row><row><cell>None</cell><cell>(CheXNet [29]) 84.54</cell><cell>84.19</cell></row><row><cell>SE [15]</cell><cell>84.95</cell><cell>84.53</cell></row><row><cell>NL [36]</cell><cell>84.49</cell><cell>85.08</cell></row><row><cell>CC [17]</cell><cell>84.43</cell><cell>85.11</cell></row><row><cell>ACM</cell><cell>85.03</cell><cell>85.39</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Results on COCO dataset. All experiments are based on Mask-RCNN [13]. Method AP bbox AP bbox and segmentation performance in the natural image domain as well. In ResNet-50 and ResNeXt-101, ACM outperforms all other modules</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0">https://github.com/jrzech/reproduce-chexnet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Computerized detection of abnormal asymmetry in digital chest radiographs</title>
		<author>
			<persName><forename type="first">Iii</forename><surname>Armato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Giger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Macmahon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical physics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1761" to="1768" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Comparison of deep learning approaches for multi-label chest x-ray classification</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Baltruschat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saalbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6381</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bustos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pertusa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De La Iglesia-Vayá</surname></persName>
		</author>
		<idno>arXiv</idno>
		<title level="m">Padchest: A large chest x-ray image dataset with multi-label annotated reports</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gcnet: Non-local networks meet squeezeexcitation networks and beyond</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recent advances in observer performance methodology: jackknife free-response roc (jafroc)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiation protection dosimetry</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="26" to="31" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dualchexnet: dual asymmetric feature learning for thoracic disease classification in chest x-rays</title>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page">101554</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Radiologic errors in patients with lung cancer</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Forrest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Western Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">485</biblScope>
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-label chest x-ray image classification via category-wise residual attention learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thorax disease classification with attention guided convolutional neural network</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2019.11.040</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167865519303617" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="38" to="45" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to recognize abnormalities in chest x-rays with location-aware dense networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grbic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iberoamerican Congress on Pattern Recognition</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison</title>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ciurea-Ilcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chute</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haghgoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Berkowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Greenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scientific Data</title>
				<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Utilization trends in noncardiac thoracic imaging</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American College of Radiology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="342" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Srm: A style-based recalibration module for convolutional neural networks</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Thoracic disease identification and localization with limited supervision</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">coco: Common objects in context</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Imagegcn: Multi-relational image graph convolutional networks for disease identification with chest x-rays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Development and validation of deep learningbased automatic detection algorithm for malignant pulmonary nodules on chest radiographs</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1148/radiol.2018180237</idno>
		<idno>pMID: 30251934</idno>
		<ptr target="https://doi.org/10.1148/radiol.2018180237" />
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="218" to="228" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Is object localization for free?-weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Miss rate of lung cancer on the chest radiograph in clinical practice</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Quekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Kessels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Van Engelshoven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chest</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="720" to="724" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Irvin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bagul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Langlotz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shpanskaya</surname></persName>
		</author>
		<title level="m">Chexnet: Radiologist-level pneumonia detection on chest x-rays with deep learning</title>
				<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Concurrent spatial and channel squeeze &amp; excitationin fully convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wachinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
				<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attentionguided curriculum learning for weakly supervised classification and localization of thoracic diseases on chest radiographs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning in Medical Imaging</title>
				<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Chestnet: A deep neural network for classification of thoracic diseases on chest radiography</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Non-local neural networks</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019">Detectron2. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Weakly supervised medical diagnosis and localization from multiple resolutions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Prosky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Poblenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lyman</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1084" to="1102" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
