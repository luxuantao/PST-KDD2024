<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction</title>
				<funder ref="#_ZdB5VZm">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_hedfmD5 #_smhbg4K">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_9kFNhRE">
					<orgName type="full">China Scholarships Council</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">DeepGS: Deep Representation Learning of Graphs and Sequences for Drug-Target Binding Affinity Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurately predicting drug-target binding affinity (DTA) in silico is a key task in drug discovery. Most of the conventional DTA prediction methods are simulation-based, which rely heavily on domain knowledge or the assumption of having the 3D structure of the targets, which are often difficult to obtain. Meanwhile, traditional machine learning-based methods apply various features and descriptors, and simply depend on the similarities between drug-target pairs. Recently, with the increasing amount of affinity data available and the success of deep representation learning models on various domains, deep learning techniques have been applied to DTA prediction. However, these methods consider either label/one-hot encodings or the topological structure of molecules, without considering the local chemical context of amino acids and SMILES sequences. Motivated by this, we propose a novel end-to-end learning framework, called DeepGS, which uses deep neural networks to extract the local chemical context from amino acids and SMILES sequences, as well as the molecular structure from the drugs. To assist the operations on the symbolic data, we propose to use advanced embedding techniques (i.e., Smi2Vec and Prot2Vec) to encode the amino acids and SMILES sequences to a distributed representation. Meanwhile, we suggest a new molecular structure modeling approach that works well under our framework. We have conducted extensive experiments to compare our proposed method with state-of-the-art models including KronRLS, SimBoost, DeepDTA and DeepCPI. Extensive experimental results demonstrate the superiorities and competitiveness of DeepGS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Effectively predicting drug-target binding affinity (DTA) is one of the important problems in drug discovery. Drugs (or ligands) <ref type="bibr" target="#b16">[17]</ref> are chemical compounds, each of which can be represented by both a molecule graph with atoms as nodes and chemical bonds as edges, and a string obtained from the Simplified Molecular Input Line Entry System (SMILES) <ref type="bibr" target="#b35">[36]</ref>. Targets (or proteins) are sequences of amino acids. Binding affinity indicates the strength of the interactions of drug-target pairs. Through binding, drugs can have a positive or negative influence on functions carried out by proteins, affecting the disease conditions <ref type="bibr" target="#b38">[39]</ref>. By understanding drug-target binding affinity, it is possible to find out candidate drugs that are able to inhibit the target/protein and benefits many other bioinformatic applications <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>. As a result, DTA prediction has received much attention in recent years <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b37">38]</ref>.</p><p>Early approaches for DTA prediction can be roughly classified into two types: (i) simulation-based methods, and (ii) traditional machine learning-based methods. Simulation-based methods rely on do- 1 College of Computer Science and Technology, Hunan University, China main knowledge <ref type="bibr" target="#b20">[21]</ref> or the 3D structure of target/protein <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref>, which are often difficult to obtain. Meanwhile, traditional machine learning-based methods apply various features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b7">8]</ref> and descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b5">6]</ref>, and simply depend on the similarities between drugtarget pairs <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b18">19]</ref>. Recently, owing to the remarkable success in various machine learning tasks (e.g., image recognition and natural language processing), deep learning-based methods are also exploited for DTA prediction <ref type="bibr" target="#b22">[23]</ref>. These methods consider either label/one-hot encodings or the topological structure of molecules, they, however, do not consider the local chemical context of amino acids and SMILES sequences. It is easily understood that the topological structure information provides an overview of how the atoms are connected, while the local chemical context reveals the functionality of the atoms, like the semantic meaning of a word in a sentence. These two types of information complement each other and are both important for DTA prediction. It should be meaningful and interesting to take these two types of information consideration together. To this end, this paper proposes a novel end-to-end learning framework for DTA prediction, namely Deep representation learning framework for Graphs and Sequences (DeepGS).</p><p>In a nutshell, our framework consists of three major building blocks. One of the major blocks learns low-dimension vector representations for target/protein sequences, using a convolutional neural network (CNN). The other two blocks learn two representations for drugs, by using a graph attention network (GAT) and a bi-directional gate recurrent unit (BiGRU), respectively. Specifically, (i) the CNN and BiGRU blocks extract local chemical context information of amino acids in targets and atoms in drugs, respectively. Since the label/one-hot encodings of amino acids and atoms often neglect the context information, and motivated by the idea of Word2Vec <ref type="bibr" target="#b19">[20]</ref>, we leverage advanced techniques, Smi2Vec and Prot2Vec, to encode the amino acids and atoms to a distributed representation, before plugging them to CNN and BiGRU. (ii) The newly designed GAT-based molecular structure modeling approach extracts the topological features of drugs, by aggregating the representations of r-radius subgraphs. (iii) The learned representations for both drugs and targets are then passed to a neural network to predict the binding affinity.</p><p>Different from the existing simulation-based methods, our framework needs neither expert knowledge nor 3D structure of the targets, and so it could be more easy-to-use. Additionally, the proposed framework takes advantage of the local chemical context information of atoms/amino acids in drugs/proteins and uses a newly designed molecular structure modeling approach, which differ DeepGS from the existing deep learning models. To summarize, the main contributions of this paper are listed as follows:</p><p>? We propose a novel model DeepGS for DTA prediction. To the best of knowledge, this work is the first to consider both local arXiv:2003.13902v2 [cs.LG] 3 Apr 2020 chemical context and topological structure to learn the interaction between drugs and targets. ? We conduct extensive experiments to study the performance of our proposed method, based on both small and large benchmarking datasets. The experimental results demonstrate (i) the promising performance of our proposed model, (ii) considering jointly local chemical context and topological structure is effective, and (iii) the newly designed molecular structure modeling approach works well under our proposed framework. (The codes of our method are available at https://github.com/jacklin18/DeepGS.)</p><p>The rest of the paper is organized as follows. In Section 2, we introduce the proposed method for drug-target binding affinity prediction. In Section 3, we report and analyze the performance of our method. Section 4 reviews the related work. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The Proposed Method</head><p>In this section, we first provide an overview of the proposed DeepGS framework (Section 2.1). Then, we introduce the representation learning for drugs and targets, respectively (Sections 2.2?2.3). Finally, we discuss the binding affinity prediction with DeepGS (Section 2.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Overview of DeepGS</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the overview of DeepGS. It takes the symbolic sequences of target/protein and drug, as well as the molecular structure of the drug as the input. It outputs the binding affinity for the drugtarget pair. Remind that the central idea of DeepGS is to consider both local chemical context and the molecular structure, by using some embedding techniques (i.e., Smi2Vec and Prot2Vec) to encode the amino acids and atoms to a distributed representation. Therefore, we design DeepGS as a three-step framework for DTA prediction:</p><p>1. Encoding symbolic tokens in target/drug sequences; 2. Encoding the whole drug/target sequences and the molecular structure of the drug; 3. Predicting the binding affinity value based on the encodings of the drug and the target.</p><p>Specifically, motivated by Word2Vec <ref type="bibr" target="#b19">[20]</ref>, in the first step we encode the symbols in the sequence of the target/protein and the drug to a distributed representation, by using Prot2Vec and Smi2Vec, respectively. Then, the sequences can be transformed into matrices, where each row is the representation of a symbol in the sequences. In the second step we extract features, from the drug/target matrices and the molecule graph, to encode the whole sequences and graph. For the target/protein, we consider the local chemical context of the amino acids, by using a convolutional neural network (CNN). For the drug, we consider both the molecular structure and the local chemical context. Particularly, since the molecular structure can be represented by a graph, we suggest a graph attention network (GAT) based approach to extract the topological information of the drug. In the meantime, the local chemical context of atoms in the drug is captured, by using a bi-directional gated recurrent unit (BiGRU). As a result, we obtain a latent representation for the target and two latent presentations for the drug. To predict the binding affinity, in the third step DeepGS inputs the concatenation of the three latent representations to a stack of fully connected layers, and outputs a real value binding affinity. Next, we present the details of our method.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Local Chemical Context Modeling</head><p>Drugs are often presented in the format of SMILES (Simplified Molecular-Input Line-Entry System), a specification in the form of a line notation for describing the structure of chemical compound <ref type="bibr" target="#b35">[36]</ref>. For example, the SMILES string of the drug in Figure <ref type="figure" target="#fig_0">1</ref> is "CC1=C2C=C(C=CC...", which is a sequence of atoms and covalent bonds. For ease of representation, we consider both atoms and covalent bonds as symbolic tokens, and so the SMILES sequence is a sequence of symbols. To encode the SMILES sequence, existing deep learning approaches such as DeepDTA <ref type="bibr" target="#b30">[31]</ref> use label/one-hot encoding to represent each symbol in the SMILES sequence. However, label/one-hot encoding often neglects the context of the symbol, and thus cannot reveal the functionality of the symbol within the context. To remedy this, we propose to use Smi2Vec <ref type="bibr" target="#b25">[26]</ref>, a method similar to Word2Vec <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>, to represent the symbols in the SMILES sequence. Algorithm 1 shows the pseudo-codes of encoding SMILES symbols, based on the pre-trained Smi2Vec embeddings. In general, a SMILES string with fixed length, say m, is divided into a separate atom or symbol (Line 1). Then, it maps the atom by looking up each of the atom embeddings from the pre-trained dictionary, while it randomly generates values if it is not in the dictionary (Lines 2-6). Finally, it constructs an atom matrix A by aggregating embedding vectors (Lines 7-8), where each line represents the pre-trained vector of an atom.</p><p>Motivated by the gate function in GRU <ref type="bibr" target="#b4">[5]</ref>, we apply a 1-layer Bi-GRU on the resulting matrix to obtain a latent representation of the drug, which allows us to model the local chemical context. Note that BiGRU takes a fix-sized of matrix as the input, while the length of SMILES strings may vary. One simple solution is to fix the length of input sequence at approximately average length of the SMILES string in the dataset, and apply zero-paddings at the end of the input sequences. As we will show later in Section 3, an appropriate length (e.g., larger than the average length of sequences in the dataset) does not make the performance of our framework change a lot. Considering the training efficiency and the DTA performance, it is suggested that a small number is a good trade-off between efficiency and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Molecular Structure Modeling</head><p>In addition to the local chemical context, we exploit the molecular structure to uncover how the atoms connect in the drug. The molecular structure is an important cue for DTA prediction <ref type="bibr" target="#b30">[31]</ref>. To achieve this, we can first use the RDKit <ref type="bibr" target="#b10">[11]</ref> tool to transform SMILES string of a chemical compound into a molecule graph G = (V, E), in which the node vi ? V represents the i-th atom, and eij ? E represents the chemical bond between the i-th and the j-th atoms. Then, we can learn a graph attention network (GAT) <ref type="bibr" target="#b32">[33]</ref> from the molecule graphs G. To apply GAT on molecule graph, we can encode all atoms and chemical bonds to a d-dimensional vector, and aggregates the information from the r-radius subgraph for each atom in the molecular graph, where r is the number of hops from an atom. Algorithm 2 shows the pseudo-codes of applying GAT on molecule graphs. Specifically, it first computes an initial vector concatenating the fingerprint (i.e., the r-radius subgraph) and the adjacent information for each atom (Lines 1-4). Here it leverages Weisfeiler-Lehman algorithm to extract the fingerprint of the atoms. Then, it updates the atom vectors by propagating the information from its neighboring nodes (Lines 5-6). Finally, it aggregates the atom vectors to obtain the representation of the molecule (Line 7), each of which contains the r-radius subgraph information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: GAT on molecule graph</head><p>Input: Molecule graph G = (V, E), radius R Output: a vector y molecule for a molecule</p><formula xml:id="formula_0">1 for each node vi ? V do 2 adj(vi) ? extract adjacency(G) 3 f p(vi) ? extract f ingerprints(vi, G, R) 4 Vin ? [adj(vi); f p(vi)] 5 for each node vi ? V do 6 update Vin ? Vin + v j ?N eighbors(v in ) GATConv(Vj) 7 return y molecule? V v 1 V in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Representation Learning for Target/Protein</head><p>Targets/proteins are often represented as a sequence of amino acids (e.g., MKKFFDSRREQ... shown in Figure <ref type="figure" target="#fig_0">1</ref>). Similar to the SMILES string, we propose to first encode the amino acids into a ddimensional vector following Prot2Vec <ref type="bibr" target="#b1">[2]</ref>, which allows us to capture local chemical information in targets/proteins. As a single amino acid often makes no sense, we apply a fixed-length N -gram splitting approach to partition the sequence into meaningful "biological words". Note that, here the sequence refers to the fixed-length input protein sequence (instead of the full sequences), which is preprocessed as similar as we handle the SMILES strings (recall Section 2.2). Compared to the commonly used label encoding methods, the fixed-length N -gram divides the sequence into a sequence of N -grams. Each N -gram is considered as a "biological word". Intuitively, it can generates more "words context" than label encoded by one-hot encoding.</p><p>Considering that there are generally 20 kinds of amino acids, rendering that the maximum number of possible N-grams is 20 N . To make trade-off between the training feasibility and vocabulary size, in our paper we define N = 3. Specifically, given a protein or target sequence L = {xi|(i = 1, 2, ..., |l|)}, where xi represents the i-th amino acid and |l| represents the sequence length, the fixed-length 3-gram splitting method partitions the sequence into the following 3-grams, each of which is a biological word consisting of 3 amino acids:</p><formula xml:id="formula_1">[x1; x2; x3], [x4; x5; x6], ..., [x |l|-2 ; x |l|-1 ; x |l| ].</formula><p>For each biological word, we map it to an embedding vector by looking up a pretrained embedding dictionary for 9048 words <ref type="bibr" target="#b1">[2]</ref>, which is obtained from Swiss-Prot (https://www.uniprot.org/) with 560,118 manually annotated sequences. As a result, we transform each target sequence to a matrix, in which each row is the embedding of a biological word. The matrix is then fed into a CNN to extract the local chemical context of the target. It is worth noting that, different from the early ligand-based approach <ref type="bibr" target="#b12">[13]</ref> that neglects the local context information in targets/proteins, our solution above leverages the embedding technique to learn the representation from the protein sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Drug-target Binding Affinity Prediction</head><p>In this study, we look on drug-target prediction as a regression task by predicting the binding affinity values. With the representation learned from the previous sections, we can integrate all the information from drugs and targets to predict the binding affinity. In brief, we concatenate all the representations and feed them to three fullyconnected dense layers to output the affinity value. More precisely, for the GAT block, we use two graph attention layers to update the node vectors in a graph considering their neighbor nodes. For the CNN block, we use three consecutive 2D-convolutional layers. And for the BiGRU block, we use one BiGRU layer. Besides, we use Rectified Linear Unit (ReLU) <ref type="bibr" target="#b21">[22]</ref> as the activation function, which has been commonly adopted in deep learning research. Given a set of drug-target pairs and the ground-truth affinity values in the training dataset, we can use the mean square error (MSE) as the loss function: 2 , where ?i is the predicted value, yi is the ground-truth value, and N represents the number of drug-target pairs.</p><formula xml:id="formula_2">LMSE = 1 N N i=1 (?i -yi)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this part, we first describe the experimental settings (Section 3.1). Then, we compare our proposed method with state-of-the art models (Section 3.2). Besides, we conduct more experiments to analyze our model including the prediction performance and sensitiveness (Section 3.3). Finally, we conduct an ablation study to investigate the effectiveness of main strategies suggested in the paper (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Datasets</head><p>Following prior works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b11">12]</ref>, we employed widely-used datasets that are specialized for DTA prediction:</p><p>? The Davis dataset, which contains 68 drugs and 442 targets with 30,056 drug-target interactions.</p><p>? The KIBA dataset, which originally comes from a method named Kinase Inhibitor BioActivity (KIBA), and it introduces KIBA scores with integration of the statistic information of K d , Ki and IC50 into a single bioactivity score for drug-target interaction. The dataset contains 2,111 drugs and 229 targets with 118,254 interactions after processing <ref type="bibr" target="#b22">[23]</ref>.</p><p>We randomly split the datasets into 6 subsets with the equal size, and used five of them for training and the remaining one for testing. For Davis dataset, we use the K d values transformed into log space, pK d , as the binding affinity value. For KIBA dataset, it integrated from multiple sources (i.e., Ki, K d , and IC50) into a bioactivity matrix, we use the value (i.e., KIBA-values) in matrix as the binding affinity value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation Metrics</head><p>We used four metrics commonly used in regression task (recall Section 2.4) to evaluate the performance. They include: Mean Squared Error (MSE), Concordance Index (CI), r 2 m , and Area Under Precision Recall (AUPR) score.</p><p>MSE has been defined in the previous section as the objective of DeepGS. CI <ref type="bibr" target="#b22">[23]</ref> measures whether the predicted binding affinity values rank the corresponding drug-target interactions in the same order as the ground-truth does. It is computed as CI = The AUPR score is widely used for binary classification. A commonly used binding affinity value is defined based on the logarithm of K d as pK d = -log10( K d 1e9 ), where K d refers to the dissociation value <ref type="bibr" target="#b28">[29]</ref>. Here, we transformed the datasets into binary datasets with predefined thresholds. We followed the prior work <ref type="bibr" target="#b22">[23]</ref> to select pK d value of 7 and 12.1 as threshold for the Davis and KIBA dataset, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Baseline Methods</head><p>We compared DeepGS against the following state-of-the-art models:</p><p>? KronRLS <ref type="bibr" target="#b23">[24]</ref>: This approach is based on Kronecker Regularized Least Square (http://staff.cs.utu.fi/?aatapa/software/RLScore/). It aims to minimize the objective function,</p><formula xml:id="formula_3">J(f ) = m i=1 (yi - f (xi)) 2 + ? f 2</formula><p>k , where xi (i=1,...,m) is a set of training input features, f is a non-linear function, yi represents their corresponding real-valued labels, and ? &gt; 0 is a pre-defined regularization parameter, f 2 k is the norm of f with kernel k. ? SimBoost <ref type="bibr" target="#b11">[12]</ref>: This baseline constructs three kinds of features and trains a gradient boosting machine <ref type="bibr" target="#b3">[4]</ref> model to represent the nonlinear associations between the input features and the binding affinities.</p><p>? DeepCPI <ref type="bibr" target="#b30">[31]</ref>: This baseline is originally designed for CPI/DTI prediction, and cannot be used directly for DTA task. Here, we need to change it to a regression task. Specifically, we replaced the loss function of cross entropy with MSE, and set the dimension of output layer to 1. The rest is consistent with the original paper.</p><p>? DeepDTA <ref type="bibr" target="#b22">[23]</ref>: DeepDTA trains two 3-layer CNNs with label/one-hot encodings of compound and protein sequences to predict DTA task. Their CNN model consists of two separate CNN blocks to learn the features from SMILES strings of compounds and protein sequences, respectively. The representations of drugs and targets are concatenated and passed to a fully connected layer for DTA prediction.</p><p>As for KronRLS and SimBoost, they both use PubChem clustering server for drug similarity and Smith-Waterman for protein similarity computation; For DeepDTA, the input for Davis dataset consists of (85, 128) and (1200, 128) dimensional matrices for the compounds and proteins, respectively, and with a (100, 128) dimensional matrix for the compounds and a (1000, 128) dimensional matrix for the proteins for KIBA dataset. The other settings are kept as the same as the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Implementation Details</head><p>For Smi2Vec, we used an embedding layer with 100 dimensions to represent the symbols in SMILES sequences, and for Prot2Vect we used 100-dimensional pre-trained representations for the biological words. As a result, we constructed matrices with (100, 100) and (2000, 100) dimensions for drug and target, respectively. In our experiments, when the molecular graph was used, we employed the RDKit <ref type="bibr" target="#b10">[11]</ref> software to convert the textual representation in SMILES format to a graph representation. For the GAT block, we set the number of heads to 10, and it was implemented using pytorch geometric (https://github.com/rusty1s/pytorch geometric), and we set the same radius r = 2 as in <ref type="bibr" target="#b30">[31]</ref>. For the BiGRU block, we set the size of input and hidden layer to 100. For the CNN block, we set the size of kernel to 23. Note that, we performed grid search over a combination of the hyper-parameters to determine the settings. The detailed settings are summarized in Table <ref type="table" target="#tab_0">1</ref>. And we obtained a high performance of the proposed framework with a relatively small range on hyperparameter tuning. The proposed framework was implemented using PyTorch with Tensorflow <ref type="bibr" target="#b0">[1]</ref> backend and ADAM optimization. Our experiments were run on Linux 16.04.10 with Intel(R) Xeon(R) CPU E5-2678 v3@2.50GHz and GeForce GTX 1080Ti (11GB). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Comparison Results</head><p>To examine the competitiveness of the proposed model, we compared DeepGS with state-of-the-art models (including classic and This is because classic methods rely heavily on hand-crafted features and the similarity matrices of drugs and targets. In contrast, deep learning-based approaches capture more information via automatic feature engineering with CNN and GNN. In addition, we find that our method performs better than other two deep learning-based methods. The reason could be that (i) compared to DeepCPI, our method jointly considers topological structures and local chemical context, which is benefit to the performance; (ii) compared to Deep-DTA, we incorporate GAT model to obtain the topological information of drug and advanced embedding techniques which bring more contextual information than one-hot vectors for modeling both drugs and targets.</p><p>Overall, this set of experiments demonstrate that our proposed method DeepGS outperforms all these baselines in all metrics. This is a very encouraging result. It is worth noting that, although the improvements seem to be small at the first glance, it is essentially a non-trivial achievement in terms of DTA prediction. Besides the comparison on the Davis dataset, we also conduct the comparison on the KIBA dataset. Table <ref type="table" target="#tab_2">3</ref> shows the comparison results. It can be seen that, the overall performance tendency is similar to that on the Davis dataset. For example, the performance of Kron-RLS is inferior to that of deep learning-based approaches, the performance of DeepCPI is inferior to that of DeepDTA, and our method exhibits better performance on almost all these metrics. This further demonstrates the competitiveness of DeepGS. Note that, in terms of CI metric, our method still has the comparable performance to Deep-DTA, since the value of our method is only slightly smaller than that of DeepDTA. The possible reason is that, the KIBA dataset comes from multiple sources (e.g., Ki, K d and IC50, recall Section 3.1), the data heterogeneity in KIBA dataset may make a negative effect on the CI metric of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model Analysis</head><p>In this section, we conduct more experiments to analyze our model. In the first experiment, we further examine the prediction performance of our model based the predicted value (p) and measured value (m). In the second experiment, we examine the sensitiveness of our model by using various sequence lengths. Figure <ref type="figure" target="#fig_3">3</ref> plots the predicted value (p) and measured value (m) on these two datasets. Note that, a good model should hold that predicted value (p) is close to the measured value (m), and thus the samples should fall close to the dashed (p = m) line. One can see that, for the Davis dataset, the dense area of the pK d value is in the range of 5 to 6 in terms of x-axis. This is because the pK d value of 5 constitutes more than half of the dataset (i.e., 20,931 out of 30,056, as reported from <ref type="bibr" target="#b22">[23]</ref>). In addition, we observe that the dense area of the KIBA score is in the range of 10 to 14 in terms of x-axis. The reason is similar to that for the Davis dataset. Particularly, for both datasets, the samples are close to the dashed (p = m) line. This justifies, from another perspective, that the proposed solution has a good prediction performance.</p><p>To investigate the sensitiveness of our model, a simple way is to remove some information of the input sequences, and then to test the model's prediction performance. In this paper, we use the following scheme that not only can remove some information of input sequences but also can partially reflect the impact of sequence length. Specifically, we fix the length of the input sequences at some value, say l, when training BiGRU and CNN. To achieve this, we cut the input sequence if the length of the input sequence is longer than l, and otherwise we use the zero-padding at the end of the input sequence. As for the drug's input sequence (i.e., SMILES), we set its length to [50, 100, 500], as shown in Table <ref type="table" target="#tab_0">1</ref>. Note that, the average length of SMILES sequences in the Davis dataset is 64 and the bold refers to the default value. Correspondingly, we set the length of target/protein's input sequence (i.e., amino acids) to [500, 1000, 2000]. Here, the average length of protein sequences in the Davis dataset is 788. For clarity, we use L ds and Lps to denote the Length of drug sequence and that of protein sequence, respectively.</p><p>Figure <ref type="figure" target="#fig_4">2</ref> shows the results when we set L ds to 50, 100 and 500, respectively. Meanwhile, Figure <ref type="figure" target="#fig_5">4</ref> shows the results when we set Lps  to 500, 1000 and 2000, respectively. We observe that, (i) the performance gap between L ds = 100 and L ds = 500 is very tiny, and the performance gap between Lps = 1000 and Lps = 2000 is not so obvious; (ii) the performance gap between L ds = 50 and L ds = 100 can be easily perceived, and the performance gap between Lps = 500 and Lps = 1000 is obvious. This phenomenon is a little bit strange at the first glance. To dig out the reason behind it, we plot the distribution of sequence lengths, as shown in Figure <ref type="figure" target="#fig_7">6</ref>.</p><p>It can be seen that the lengths of most SMILES sequences are less than 100 and larger than 50. Thus, it is natural that the performance gap between L ds = 100 and L ds = 500 is very tiny, since almost all SMILES sequences do not need to be cut even if L ds = 100, i.e., few information is missing. However, when L ds = 50, many SMILES sequences may need to be cut, and so the performance degrades. With the similar argument, it is not hard to understand that the performance gap between Lps = 500 and Lps = 1000, since most protein sequences need to be cut when Lps = 500. The reason for the relatively small performance gap between Lps = 1000 and Lps = 2000 can be inferred with the similar argument. This result may imply that when the sequence length l is set to a value larger than the average length of sequences in the dataset, the performance degradation could be trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>As mentioned before, existing models for DTA prediction have leveraged the topological structure to learn the representation for drug and target/protein, while they often ignored the local chemical context. Thereby, this work considers both local chemical context and topological structure to learn the interaction between drugs and targets. More precisely, the core idea of DeepGS is to fully leverage local chemical context, by using advanced embedding techniques, to better learn the drug and target representations. To study the effectiveness of the central idea, we implemented a variant of our model, called DeepGS1. This variant model removes both drug's and protein's local chemical context information obtained by Smi2Vec and Prot2Vec from the framework. The detailed configuration of DeepGS1 is illustrated in Table <ref type="table" target="#tab_3">4</ref>. In the following experiments, we use the same experimental settings mentioned in Table <ref type="table" target="#tab_0">1</ref>.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> shows the comparison results. It can be seen that DeepGS1 is basically inferior to DeepGS in terms of CI, MSE, rm 2 and AUPR. These results demonstrate that combining the local chemical context information, which reflects the functionalities of the atoms, is benefit to learning a good representation for drugs and proteins, improving the prediction performance.</p><p>In addition to examining the effectiveness of the central idea, we also conduct another experiment, which is used to answer the following interesting question. Recall Section 2.2.2, we develop a GATbased molecular structure modeling approach. One could argue that, GNN (Graph Neural Network) can also map a graph to a vector that encodes the topological structure of the graph, since recent work <ref type="bibr" target="#b30">[31]</ref> have showed that GNN can effectively model drugs. To address it, we implemented another variant of our model, called DeepGS2. Compared to our model, the major difference is that it uses a GNN-based molecular structure modeling approach (cf., Table <ref type="table" target="#tab_3">4</ref>). The experimental results are also shown in Figure <ref type="figure" target="#fig_6">5</ref>. We can see that, the variant DeepGS2 is obviously poorer than our model. These results justify our choice in Section 2.2.2. The reasons could be twofold: (i) the molecular structure may contribute a lot to the prediction performance; and (ii) The changes to the molecular structure modeling approach are sensitive to the model, especially when local chemical context is also considered in the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Drug-target binding prediction has been an interesting topic in drug discovery. Most of previous works focused on simulationbased methods (i.e., molecule docking and descriptors) or machine  learning-based models. For example, Li et al. <ref type="bibr" target="#b15">[16]</ref> proposed a docking method based on random forrest (RF). The RF model was also adopted in KronRLS <ref type="bibr" target="#b23">[24]</ref> with a similarity score through the Kronecker product of similarity matrix to improve the predictive performance. To remedy the limitation of linear dependencies in Kro-nRLS, a gradient boosting method was proposed in SimBoost <ref type="bibr" target="#b11">[12]</ref> to construct the similarities among drugs and targets. Although classic methods show reasonable performance in drug-target prediction, they are often computational expensive, or require external expert knowledge or the 3D structure of target/protein, which are difficult to obtain. Different from the classic methods, the proposed framework is able to automatically extract features from the data, and requires neither expert knowledge nor 3D structure of the target/protein. These salient features make the proposed framework applicable to large scale affinity data which is becoming available.</p><p>Owing to the great success of deep learning, much attention has been devoted to applying deep learning techniques for drug-target prediction. Most of the existing methods are based on topological similarity. For example, in <ref type="bibr" target="#b36">[37]</ref> they developed a Deep Belief Network (DBN) model constructed by stacking Restricted Boltzmann Machines (RBMs). Instead of using DBN, a nonlinear end-to-end learning model named NeoDTI <ref type="bibr" target="#b33">[34]</ref> was proposed. NeoDTI integrates variety of information from heterogeneous network data and uses topology-preserving based representations of drugs and targets to facilitate drug-target prediction. With the increasing popularity of graph neural networks (GNNs), researchers are adopting GNNs model for drug prediction. For example, graph convolutional network was used to model molecule based on the extraction of their circular fingerprint <ref type="bibr" target="#b5">[6]</ref>. By learning from molecular structures and protein sequences, Gao et al. <ref type="bibr" target="#b9">[10]</ref> proposed a neural model for drug-target prediction and used a two-way attention mechanism to provide biological interpretation of the prediction. Ma et al. <ref type="bibr" target="#b18">[19]</ref> proposed to use multi-view graph auto-encoders to obtain better inter-pretability and they also added attentive mechanism to determine the weights for each view, according to the corresponding tasks and features. Moreover, Zitnik et al. <ref type="bibr" target="#b40">[41]</ref> presented a Decagon model used for modeling polypharmacy side effects, their model constructs a multimodal graph of various interactions (i.e., protein-protein interactions, drugtarget interactions) and the polypharmacy side effects.</p><p>Among the research on deep learning for drug discovery, Deep-DTA <ref type="bibr" target="#b22">[23]</ref> and DeepCPI <ref type="bibr" target="#b30">[31]</ref> are the most relevant to our work. Both of them addressed the problem of drug-target prediction. DeepDTA takes label/one-hot encodings of compound/protein sequences as input, and trains two CNNs for the drug and target, respectively, to predict the binding affinity value of drug-target pairs. DeepCPI is originally specialized for DTI prediction, it uses a traditional GNN based on representation of r-radius fingerprints to encode the molecular structure of drugs, and a CNN to encode protein sequences. Attention mechanism is adopted to concatenate drug and protein rep-resentations for prediction. Here, we take measures to revise it to be used for DTA prediction. Compared with DeepDTA and DeepCPI, the proposed framework considers both local chemical context and the topological information of drugs at the same time to improve the binding affinity prediction, by using Smi2Vec and Prot2Vec to encode the atoms in drugs and amino acids in targets, while the existing methods consider only one of these important factors. Moreover, our work also suggests a new molecular structure modeling approach that works well under our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Accurately predicting DTA is a vital and challenging task in drug discovery. In this paper, we have proposed an end-to-end deep learning framework named DeepGS for DTA prediction. It combines a GAT model to extract the topological information of molecular graph and a BiGRU model to obtain the local chemical context of drug. To assist the operations on the symbolic data, we used advanced embedding techniques (i.e., Smi2Vec and Prot2Vec) to encode the amino acids and SMILES sequences to a distributed representation. We have conducted extensive experiments to compare our proposed method with state-of-the-art models. The experimental results demonstrate that the promising performance of our proposed method. This study opens several future research directions: 1) investigating whether our method can be further improved by integrating other state-of-the-art techniques, for example, Generative Adversarial Networks; 2) extending our method to other types of problems in data mining and bioinformatics fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Overview of DeepGS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>Smi2Vec Input: a molecule s in the format of SMILES, dictionary D, atom vector's fixed length m, vector dimension d. Output: atom matrix A 1 atom set {xj|1 ? j &lt; |s|} ?split(s) 2 for j=1 to m do 3 if xj / ? dictionary then 4 embedding vector aj ?randomly generated value ? d -xj // by using D 7 atom matrix A ?-m j=1 aj 8 return A ? m?d 2.2 Representation Learning for Drug</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 Zy</head><label>1</label><figDesc>i &gt;y j ?(fi -fj) and ?(b) = {1, b &gt; 0; 0.5, b = 0; 0, b &lt; 0}, where Z is a normalization constant that equals the number of drugtarget pairs with different binding affinity values. More specifically, when yi &gt; yj, a positive score is given if and only if the predicted fi &gt; fj. Here, ?(b) is a step function.The metric r 2 m is used to evaluate the external prediction performance of QSAR (Quantitative Structure-Activity Relationship) models. A model is acceptable if and only if r 2 m ? 0.5. And r 2 m = r 2 * (1 -r 2 -r 2 0 ), where r 2 and r 2 0 represent the squared correlation coefficient values between the observed and predicted values with and without intercept, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Predictions from DeepGS model against measured binding affinity values. The left and right figures plot the results on the Davis and KIBA datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. CI, MSE, r 2 m , and AUPR vs. L ds (drug SMILES sequences in the Davis dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. CI, MSE, r 2 m , and AUPR vs. Lps (protein sequences in the Davis dataset).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Ablation study on all metrics for our proposed model and two variants on the Davis dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Sequence length distribution in Davis dataset. The left and right figures refer to the length distribution of SMILES and protein sequences, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The detailed training settings of DeepGS. The length of SMILES/protein sequence has three various settings, which are used to study the impact of SMILES/protein sequence length.</figDesc><table><row><cell>Parameter</cell><cell>Setting</cell><cell>Parameter</cell><cell>Setting</cell></row><row><cell>Radius r</cell><cell>2</cell><cell>Layer of CNN</cell><cell>3</cell></row><row><cell>N-gram</cell><cell>3</cell><cell>Layer of BiGRU</cell><cell>1</cell></row><row><cell>CNN kernel size</cell><cell>23</cell><cell>Learning rate (lr)</cell><cell>1e-4</cell></row><row><cell>Length of SMILES sequence</cell><cell>50, 100, 500</cell><cell>lr decay</cell><cell>0.9</cell></row><row><cell>Length of protein sequence</cell><cell>500, 1000, 2000</cell><cell>Decay interval</cell><cell>20</cell></row><row><cell>Vector dimension</cell><cell>32</cell><cell>Weight decay</cell><cell>1e-5</cell></row><row><cell>Window size</cell><cell>11</cell><cell>Epoch</cell><cell>100</cell></row><row><cell>Depth in GAT</cell><cell>2</cell><cell>Batchsize</cell><cell>1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The average CI, MSE, r 2 m and AUPR scores on the Davis dataset. The results of KronRLS, SimBoost and DeepDTA are reported from<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell>Drugs</cell><cell>Targets</cell><cell>CI</cell><cell>MSE</cell><cell>r 2 m</cell><cell>AUPR</cell></row><row><cell>KronRLS</cell><cell>Pubchem</cell><cell>S-W</cell><cell>0.871</cell><cell>0.379</cell><cell>0.407</cell><cell>0.661</cell></row><row><cell>SimBoost</cell><cell>Pubchem</cell><cell>S-W</cell><cell>0.872</cell><cell>0.282</cell><cell>0.644</cell><cell>0.709</cell></row><row><cell>DeepCPI</cell><cell>GNN</cell><cell>CNN (Embedding)</cell><cell>0.867</cell><cell>0.293</cell><cell>0.607</cell><cell>0.705</cell></row><row><cell>DeepDTA</cell><cell>CNN</cell><cell>CNN (One-hot)</cell><cell>0.878</cell><cell>0.261</cell><cell>0.630</cell><cell>0.714</cell></row><row><cell>DeepGS</cell><cell>GAT+Smi2Vec</cell><cell>CNN (Prot2Vec)</cell><cell>0.882</cell><cell>0.252</cell><cell>0.686</cell><cell>0.763</cell></row><row><cell cols="7">deep learning models) used for DTA prediction. Table 2 reports the</cell></row><row><cell cols="7">average CI, MSE, r 2 m and AUPR scores on the Davis dataset. From this table, we can see that, on the whole classic methods</cell></row><row><cell cols="7">such as KronRLS perform worse than deep learning-based methods.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The average CI, MSE, r 2 m and AUPR scores on the KIBA dataset. The results of KronRLS, SimBoost and DeepDTA are reported from<ref type="bibr" target="#b22">[23]</ref>.</figDesc><table><row><cell>Method</cell><cell>Drugs</cell><cell>Targets</cell><cell>CI</cell><cell>MSE</cell><cell>r 2 m</cell><cell>AUPR</cell></row><row><cell>KronRLS</cell><cell>Pubchem</cell><cell>S-W</cell><cell>0.782</cell><cell>0.411</cell><cell>0.342</cell><cell>0.635</cell></row><row><cell>SimBoost</cell><cell>Pubchem</cell><cell>S-W</cell><cell>0.836</cell><cell>0.222</cell><cell>0.629</cell><cell>0.760</cell></row><row><cell>DeepCPI</cell><cell>GNN</cell><cell>CNN (Embedding)</cell><cell>0.852</cell><cell>0.211</cell><cell>0.657</cell><cell>0.782</cell></row><row><cell>DeepDTA</cell><cell>CNN</cell><cell>CNN (One-hot)</cell><cell>0.863</cell><cell>0.194</cell><cell>0.673</cell><cell>0.788</cell></row><row><cell>DeepGS</cell><cell>GAT+Smi2Vec</cell><cell>CNN (Prot2Vec)</cell><cell>0.860</cell><cell>0.193</cell><cell>0.684</cell><cell>0.801</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>The detailed description of the variant of our model. DeepGS1 is mainly for investigating the local chemical context information, while DeepGS2 is used to justify the choice of GAT used in our molecular structure modeling component.</figDesc><table><row><cell>Model</cell><cell>Drug Representation</cell><cell>Target Representation</cell></row><row><cell>DeepGS</cell><cell>FP+GAT &amp; Smi2Vec+BiGRU</cell><cell>Prot2Vec+CNN</cell></row><row><cell>DeepGS1</cell><cell>FP+GAT &amp; one-hot/Label+BiGRU</cell><cell>one-hot/Label+CNN</cell></row><row><cell>DeepGS2</cell><cell>FP+GNN &amp; Smi2Vec+BiGRU</cell><cell>Prot2Vec+CNN</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the anonymous reviewers very much for their effort in evaluating our paper. This work was supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2018YFB0204302</rs>), in part by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">61972425</rs>, <rs type="grantNumber">U1811264</rs>), and the <rs type="funder">China Scholarships Council</rs> (No. <rs type="grantNumber">201906130128</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ZdB5VZm">
					<idno type="grant-number">2018YFB0204302</idno>
				</org>
				<org type="funding" xml:id="_hedfmD5">
					<idno type="grant-number">61972425</idno>
				</org>
				<org type="funding" xml:id="_smhbg4K">
					<idno type="grant-number">U1811264</idno>
				</org>
				<org type="funding" xml:id="_9kFNhRE">
					<idno type="grant-number">201906130128</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Continuous distributed representation of biological sequences for deep proteomics and genomics</title>
		<author>
			<persName><forename type="first">Ehsaneddin</forename><surname>Asgari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Rk</forename><surname>Mofrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">141287</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Drug target identification using side-effect similarity</title>
		<author>
			<persName><forename type="first">Monica</forename><surname>Campillos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Claude</forename><surname>Gavin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><forename type="middle">Juhl</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Bork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">321</biblScope>
			<biblScope unit="issue">5886</biblScope>
			<biblScope unit="page" from="263" to="266" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Higgs boson discovery with boosted trees</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gated feedback recurrent neural networks</title>
		<author>
			<persName><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2067" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Genome scale enzyme-metabolite and drug-target interaction predictions using the signature molecular descriptor</title>
		<author>
			<persName><forename type="first">Jean-Loup</forename><surname>Faulon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milind</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Sale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Sapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="233" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Utilizing text mining on online medical forums to predict label change due to adverse drug reactions</title>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oded</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aviv</forename><surname>Peretz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binyamin</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1779" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Protein interface prediction using graph convolutional networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basir</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asa</forename><surname>Ben-Hur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6530" to="6539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretable drug target prediction using deep neural representation</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Yingkai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Iyengar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3371" to="3377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics</title>
		<author>
			<persName><forename type="first">L</forename><surname>Greg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simboost: a read-across approach for predicting drugtarget binding affinities using gradient boosting machines</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marten</forename><surname>Heidemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuqiang</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artem</forename><surname>Cherkasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Protein-ligand interaction prediction: an improved chemogenomics approach</title>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Philippe</forename><surname>Vert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">19</biblScope>
			<biblScope unit="page" from="2149" to="2156" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting new molecular targets for known drugs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Keiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">J</forename><surname>Setola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Irwin</surname></persName>
		</author>
		<author>
			<persName><surname>Laggner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Atheir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandra</forename><forename type="middle">J</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><forename type="middle">H</forename><surname>Hufeisen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">B</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><forename type="middle">C</forename><surname>Kuijer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thuy</forename><forename type="middle">B</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">462</biblScope>
			<biblScope unit="issue">7270</biblScope>
			<biblScope unit="page">175</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acvtree: A new method for sentence similarity modeling</title>
		<author>
			<persName><forename type="first">Yuquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4137" to="4143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-quality structural and interaction data improves binding affinity prediction via random forest</title>
		<author>
			<persName><forename type="first">Hongjian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwong-Sak</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man-Hon</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Molecules</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="10947" to="10962" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adaptive graph convolutional neural networks</title>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiyun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A novel molecular representation with BiGRU neural networks for learning atom</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in Bioinformatics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Drug similarity integration through attentive multi-view graph auto-encoders</title>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3477" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">People on drugs: credibility of user statements in health communities</title>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning</title>
		<meeting>the 27th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deepdta: deep drug-target binding affinity prediction</title>
		<author>
			<persName><forename type="first">Hakime</forename><surname>?zt?rk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arzucan</forename><surname>?zg?r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elif</forename><surname>Ozkirimli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="821" to="829" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward more realistic drug-target interaction predictions</title>
		<author>
			<persName><forename type="first">Tapio</forename><surname>Pahikkala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Airola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Pietil?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushil</forename><surname>Shakyawar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="325" to="337" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Agnieszka Szwajda, Jing Tang, and Tero Aittokallio</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graphcpi: Graph neural representation learning for compound-protein interaction</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangxiang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="717" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A system for learning atoms based on long short-term memory recurrent neural networks</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenli</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="728" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An efficient framework for sentence similarity modeling</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="853" to="865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Using recursive neural networks to detect and classify drug-drug interactions from biomedical texts</title>
		<author>
			<persName><forename type="first">V?ctor</forename><surname>Su?rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Paniagua</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Segura-Bedmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-second European Conference on Artificial Intelligence</title>
		<meeting>the Twenty-second European Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1666" to="1667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Making sense of large-scale kinase inhibitor bioactivity data sets: A comparative and integrative analysis</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agnieszka</forename><surname>Szwajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sushil</forename><surname>Shakyawar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petteri</forename><surname>Hintsanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krister</forename><surname>Wennerberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tero</forename><surname>Aittokallio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Modeling</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="735" to="743" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading</title>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Trott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">J</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational chemistry</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="455" to="461" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Tsubaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Tomii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gaussian interaction profile kernels for predicting drug-target interaction</title>
		<author>
			<persName><forename type="first">Sander</forename><forename type="middle">B</forename><surname>Twan Van Laarhoven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Nabuurs</surname></persName>
		</author>
		<author>
			<persName><surname>Marchiori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="3036" to="3043" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Neodti: neural integration of neighbor information from a heterogeneous network for discovering new drug-target interactions</title>
		<author>
			<persName><forename type="first">Fangping</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lixiang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="111" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prediction of drug-target interactions for drug repositioning only based on genomic expression similarity</title>
		<author>
			<persName><forename type="first">Kejian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shufeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunling</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">1003315</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</title>
		<author>
			<persName><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and computer sciences</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep-learning-based drug-target interaction prediction</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhi</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghuan</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of proteome research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1401" to="1409" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adverse drug reaction prediction with symbolic latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chaovalitwongse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianying</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1590" to="1596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Graph convolutional policy network for goal-directed molecular graph generation</title>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6410" to="6421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Collaborative matrix factorization with multiple similarities for predicting drug-target interactions</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Mamitsuka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanfeng</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1025" to="1033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
