<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Predictive Model for Dynamic Microarchitectural Adaptivity Control</title>
				<funder>
					<orgName type="full">Royal Academy of Engineering</orgName>
				</funder>
				<funder>
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder ref="#_3FdDSN7">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Australian Government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Christophe</forename><surname>Dubach</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Members of HiPEAC</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Members of HiPEAC</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edwin</forename><forename type="middle">V</forename><surname>Bonilla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NICTA &amp; Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><forename type="middle">F P</forename><surname>O'boyle</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Member of HiPEAC</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Predictive Model for Dynamic Microarchitectural Adaptivity Control</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-01-03T09:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adaptive microarchitectures are a promising solution for designing high-performance, power-efficient microprocessors. They offer the ability to tailor computational resources to the specific requirements of different programs or program phases. They have the potential to adapt the hardware costeffectively at runtime to any application's needs. However, one of the key challenges is how to dynamically determine the best architecture configuration at any given time, for any new workload.</p><p>This paper proposes a novel control mechanism based on a predictive model for microarchitectural adaptivity control. This model is able to efficiently control adaptivity by monitoring the behaviour of an application's different phases at runtime. We show that using this model on SPEC 2000, we double the energy/performance efficiency of the processor when compared to the best static configuration tuned for the whole benchmark suite. This represents 74% of the improvement available if we knew the best microarchitecture for each program phase ahead of time. In addition, we show that the overheads associated with the implementation of our scheme have a negligible impact on performance and power.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Adaptive superscalar microarchitectures are a promising solution to the challenge of designing high-performance, power-efficient microprocessors. They offer the ability to tailor computational resources to the specific requirements of an application, providing performance when the application needs it. At other times, hardware structures can be reorganised or scaled down for a significantly reduced energy cost. These architectures have the potential to costeffectively adapt the hardware at runtime to any application's needs.</p><p>The amount of adaptation available directly determines the level of performance and power-savings achievable. With high adaptivity the processor is able to vary many different microarchitectural parameters. This maximises the degree of flexibility available to the hardware, allowing adaptation of the computational resources to best fit the varying structure of the running program. Although previous work has quantified the theoretical benefits of high adaptivity <ref type="bibr" target="#b0">[1]</ref>, predicting and delivering this adaptation is still an open and challenging problem. The key question is how to dynamically determine the right hardware configuration at any time, for any unseen program.</p><p>In order to achieve the potential efficiencies of high adaptivity we require an effective control mechanism that predicts the right hardware configuration in time. Simple feedback mechanisms that predict the future occupancy requirements of a resource based on the recent past <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> will not scale to a large number of configurations. Other prior works have used statistical machine learning to construct models which estimate the performance and/or power as a function of the microarchitectural configuration <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. However, these approaches are not practical in a dynamic setting. We wish to predict the best microarchitectural parameter values rather than the performance of any given configuration. Prior work would require online searching and evaluation of the microarchitectural configuration space which is not realistic for anything other than trivial design spaces. What we require are light-weight, runtime control mechanisms. This paper develops a runtime resource management scheme that predicts the best hardware configuration for any phase of a program to maximise energy efficiency. We use a soft-max machine learning model based on runtime hardware counters to predict the best level of resource adaptation. Our model is constructed empirically by identifying optimal designs on training data. Optima from offline training quickly guide the model to runtime optima for each adaptive interval. We show that determining the right hardware counters is critical in accurately predicting the right hardware configuration. We also show that predicting the right configuration is an unusually difficult learning problem which explains the lack of progress in this area.</p><p>Whenever the program enters a new phase of execution, our technique profiles the application to gather a new type of temporal histogram hardware counter. These are fed into our model which dynamically predicts the best hardware configuration to use for that phase and enables us to double the average energy/performance efficiency over the best possible static design. This represents 74% of the improvement available from knowing the best microarchitecture for each program phase from our sample space ahead of time.</p><p>The rest of this paper is structured as follows. Section II motivates the use of machine learning for adaptivity. Section III then describes our approach to dynamic adaptation using a model explained in section IV. Section V presents the experimental setup and section VI evaluates our approach. Section VII investigates model accuracy and section VIII describes implementation details. Section IX describes related work and finally section X concludes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. THE NEED FOR ML-BASED CONTROL</head><p>This paper proposes a novel technique for dynamic microprocessor adaptation that differs substantially from prior work. Existing schemes, described in section IX, have either focused on adapting only a few microarchitectural parameters at a time, or proposed techniques for efficient searching of the design space at runtime. However, these schemes are not suited for adapting an entire processor's resources due to the complex interactions that exist between hardware structures. Furthermore, runtime searching is undesirable since it would inevitably visit poorly-performing configurations, reducing overall efficiency. We require a control mechanism that can quickly identify the optimal global hardware configuration to minimise power consumption whilst maintaining high performance.</p><p>To illustrate this point, consider figure <ref type="figure" target="#fig_0">1</ref> where we show the changing requirements of two hardware structures for three applications over time, in order to maximise efficiency. The first line in each graph shows the size required for best efficiency when the pipeline width is 8 instructions. The second line shows the desired size when this is reduced to 4 instructions.</p><p>It is clear from this figure that the sizes of the issue queue and register file leading to the best efficiency vary over time. Furthermore, they are different when the width is fixed to 4 compared to a width of 8. For example, in gap the optimal register file size is initially 113 in both cases, but quickly needs to be adjusted to 67 when the width is 4. Conversely, for applu the desired size does not depend on the width. Furthermore, looking at the required issue queue size for each application is not enough to find the desired register file size. In other words, the structures' optimal sizes change over time and these changes are not necessarily correlated with one another.</p><p>This motivates the need for machine learning based control mechanisms to learn how to adapt each structure and determine the optimal configuration for the entire processor. The next section discusses our approach, then section IV gives a formal description of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MACHINE LEARNING FOR ADAPTIVITY CONTROL</head><p>Our approach to microarchitectural adaptivity control uses a machine learning model to automatically determine the best hardware configuration for each phase of a program. Our model predicts the best parameters for the entire processor design space with only one attempt. To do this we gather hardware counters that can be used to characterise the phase and then provide them as an input to our model to guide its predictions. We first give an overview of how our scheme works, then describe the counters that we gather through dynamic profiling of each program phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>Figure <ref type="figure">2</ref> shows an overview of how our technique works. In stage 1 the application is monitored so that we can detect when the program enters a new phase of execution. We then profile the application on a pre-defined profiling configuration in stage 2 to gather characteristics of the new phase. These are fed as an input into our machine learning model which gives us a prediction of the best configuration to use (stage 3). After the processor has been reconfigured we continue running the application until the next phase change is detected.</p><p>Figure <ref type="figure">2</ref>. Overview of our technique. The hardware detects phase changes, then profiles the application on a pre-defined configuration to extract hardware counters. These are used as an input to our model that predicts the optimal microarchitectural parameters for the phase. The hardware is then reconfigured and execution continues.</p><p>Table <ref type="table">I</ref> shows the configurable microarchitectural parameters that we have considered. It represents the design space of a high-performance out-of-order superscalar processor and is similar to spaces that other researchers have considered <ref type="bibr" target="#b0">[1]</ref>. We vary fourteen different microarchitectural parameters across a range of values, giving a total design space of 627 billion points. The prior analysis column cites papers that have developed techniques to resize each of the structures we consider. We discuss this further in section VIII.</p><p>The main contribution of this work is a machine learning model that can accurately predict the best microarchitectural configuration to use for each program phase. We therefore focus solely on stages 2 and 3 from figure 2 in this paper. Section V describes the experimental methodology and execution environment in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dynamic Profiling</head><p>To characterise each application phase we extract hardware counters from the running program. These are used as an input to our machine learning model to allow it to predict the best hardware configuration for the phase.</p><p>1) Profiling Configuration: One of the main problems with extracting hardware counters at runtime is the risk of the internal processor resources saturating: the resources can become full, causing bottlenecks in the processor. This, in turn, can hide the real resource requirements making it difficult to extract accurate information about the program's runtime behaviour. To overcome this problem we need to extract counters on a configuration that makes saturation unlikely. We therefore briefly use the microarchitectural configuration with the largest structures and the highest level of branch speculation (named the profiling configuration).  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> Total 627bn</p><p>For each program phase we gather hardware counters on the profiling configuration. We then reconfigure to the configuration predicted by our model and run the application for that phase. Section VIII demonstrates that the cost of gathering these counters is negligible. The next section now describes the counters gathered during this profiling phase.</p><p>2) Hardware Counters: Table <ref type="table">II</ref> gives a summary of the counters that we gather for each processor structure. They monitor the usage of each structure and the events that occur during the profile gathering phase and would therefore be simple to extract in a real implementation. We discuss their implementation in section VIII, showing that they can be gathered with low overhead.</p><p>One key aspect of our counters is the notion of a temporal histogram. This shows the distribution of events over time and is vital to capture the exact requirements of each structure. Each bin of the histogram stores the number of cycles that the structure has a particular usage (e.g., 100 cycles with 16 entries used, 200 cycles with 32 entries used, etc.).</p><p>Width: For the pipeline width we build a temporal histogram that keeps track of the usage frequency of each functional unit type. The histogram bins correspond directly to the number of units in use.</p><p>Queues: We use temporal histograms to collect the number of entries used in the queue on each cycle. In addition to this we add information about the average number of speculative instructions present in the queue and the number that were mis-speculated. Since our profiling configuration performs a high level of speculation, it is important to know how many of the instructions are really useful.</p><p>Register File: We use temporal histograms to summarise the number of the integer and floating point registers used. In addition, temporal histograms are used to store the usage of the read and write ports. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pipeline depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cycles per instruction</head><p>Caches: We use temporal histograms representing stack distance <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and reuse distance. Each bin corresponds to a specific distance. Intuitively the stack distance is important since it characterises the capacity usage of the cache. We also estimate the potential conflicts that could arise if the cache size were smaller in the Reduced set reuse distance histogram. To do this we map the sets to those of the smallest cache size (as though "emulating" the smallest cache size available).</p><p>Branch Predictor: We use the access reuse distance within the BTB, which is similar to the block reuse distance in the caches. The second counter corresponds to the branch mis-prediction rate which is useful to control the degree of speculation within the processor.</p><p>Pipeline Depth: We only need the average number of instructions executed per cycle over the entire phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Example</head><p>This section gives an example of how the hardware counters are used to determine the size of the load/store queue that will lead to the best energy efficiency value. Figure <ref type="figure" target="#fig_1">3</ref> shows the efficiency values and counters extracted from phases within four different programs. For each figure, the top graph shows the relative efficiency of the processor when the load/store queue size is varied. By choosing the best configuration for this phase from our training data (described in section V-C), we can determine the optimal values for all other parameters. To obtain maximum efficiency, the size of the load/store queue for mgrid should be 32, swim 72, parser 16 and vortex 16. Underneath are the counters gathered. The queue usage histogram on the left has bins corresponding to queue sizes. On the right is the average number of speculative instructions in the queue and the fraction that were mis-speculated.</p><p>For mgrid and swim we see the best queue size directly corresponds to the observed usage during the profiling phase. For these applications there are few mis-speculated instructions (mis-spec) present in the queue during the phase. Now consider parser and vortex which both have a significant number of mis-speculated instructions. This time the largest bin in the queue usage histogram counter is 8 which does not directly correspond to the size of the queue that maximises efficiency. Instead, the best size of the queue is 16 entries in both cases. Since these programs have similar counters and the same desired queue size, our model can "learn" this information. So, after training on parser, it can make the correct prediction when it sees the same counters again in vortex.</p><p>The next section shows how these counters can be used to build a model that makes a single prediction of the best hardware configuration to use for this phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. MODELLING GOOD MICROARCHITECTURAL CONFIGURATIONS ACROSS PROGRAM PHASES</head><p>In order to build a model that predicts good microarchitectural configurations across program phases we require examples of various microarchitectural configurations on different program phases and their corresponding performance metrics (e.g., their energy-efficiency values). Additionally, we require a program phase to be characterised by a set of hardware counters described in the previous section.</p><p>Let {X (j) } M j=1 be the set of training program phases and {x (j) } M j=1 be their corresponding D-dimensional vector of counters. For each of these program phases we record the performance on a set of N distinct microarchitectural configurations {y (i) } N i=1 . Each component of a microarchitectural configuration y is a single microarchitectural parameter y a with a = 1, . . . , A, with A representing the number of architectural parameters (14 in this paper). Given a new program phase X * described by a set of counters x * , we aim to predict a set of (good) microarchitectural parameters y * that are expected to lead to the highest energy-efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Model</head><p>Our goal is to build a model that correctly captures the relationship between program phases' hardware counters and good microarchitectural configurations. In other words, we aim to learn a mapping f : X ? Y from the space of program phase counters X to the space of good microarchitectural configurations Y .</p><p>In order to achieve this we model the conditional distribution P (?|x) of good microarchitectural configurations ? . Load/store queue counters for four phases from different programs. We also show the relative efficiency achieved when varying the load/store queue parameters on the best configuration found (higher is better).</p><p>given a set program phase's counters x. In our approach we consider each microarchitectural parameter to be conditionally independent given the counters:</p><formula xml:id="formula_0">P (?|x) = A a=1 P (? a |x).<label>(1)</label></formula><p>It is important to note that there are dependencies between microarchitectural parameters. However, our model assumes that good parameters are conditionally independent given the program phase's counters, rather than assuming marginal independence between parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Predictions</head><p>Given the learnt model, we can predict a set of expected good microarchitectural configurations y on a new program phase x * by determining the most likely configuration under the learnt distribution:</p><formula xml:id="formula_1">y * = argmax ? P (?|x * ),<label>(2)</label></formula><p>where we note that, due to conditional independence, this reduces to computing the value of each ?a that maximises each single distribution P (? a |x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Model Parametrisation</head><p>In our model the conditional distribution of each microarchitecture parameter ? (where we omit the subindex a for clarity) given a set of counters x is described by a soft-max function:</p><formula xml:id="formula_2">P (? = s k |x) = ? k (x, W) = exp(w T k x) K j=1 exp (w T j x) ,<label>(3)</label></formula><p>where P (? = s k |x) denotes the probability of microarchitectural parameter ? having the value s k (out of K possible values) given the program phase's counters x; and the D?K matrix of weights W are the model parameters where each column {w k } K k=1 corresponds to a set of weights one for each value ? can take on<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Model Learning</head><p>In order to learn the parameters of the model our approach is based upon likelihood maximisation. For clarity, we focus on a single microarchitectural parameter y which can take one out of K possible values as we can learn the model parameters for each architectural parameter independently.</p><p>The data likelihood is given by: <ref type="bibr" target="#b3">(4)</ref> where x (n) is the vector of counters corresponding to architecture configuration ?(n) and ?(y (n) = s k ) is an indicator function that is 1 only when the particular architecture parameter on data-point n (y (n) ) takes on the value s k and zero otherwise. Additionally, we have introduced a new symbol ? denoting the number of good architecture configurations. In our experiments we have selected the set of good configurations to be those that are within 5% of the best empirical performance.</p><formula xml:id="formula_3">L(W) = ? n=1 K k=1 P (? (n) = s k |x (n) ) ?(y (n) =s k ) ,</formula><p>By taking the logarithm of equation ( <ref type="formula">4</ref>) and using equation ( <ref type="formula" target="#formula_2">3</ref>) the expression for the data log-likelihood that we aim to maximise is:</p><formula xml:id="formula_4">L = ? n=1 K k=1 ?(? (n) = s k ) log ? k (x (n) , W).<label>(5)</label></formula><p>We note that a na?ve maximum likelihood approach can lead to severe over-fitting. Hence we have considered a regularised version of the data log-likelihood by adding a term to penalise large weights, preventing over-fitting:</p><formula xml:id="formula_5">L POST = L + ? tr (W T W),<label>(6)</label></formula><p>where tr (.) denotes the trace operator and ? is the regularisation parameter. Thus, the optimal solution to the weight parameters is obtained with:</p><formula xml:id="formula_6">W Reg = argmax W (L POST )<label>(7)</label></formula><p>Training our model means finding the solution for W Reg . This can be done by using conjugate gradient optimisation with a deterministic initialisation of all the weights to 1 and with ? = 0.5. See <ref type="bibr" target="#b20">[21]</ref> for more information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Prediction</head><p>To make predictions, only equations ( <ref type="formula" target="#formula_1">2</ref>) and (3) need to be considered because the training is performed off-line. Let us assume that we are concerned with making predictions on single architecture parameter and that this parameter may take on one out of K possible values. Additionally, lets say that the corresponding model parameters are denoted by the D ? K matrix W. Hence, the computations involved for a new program phase characterised by the D ? 1 vector of counters x * are:</p><formula xml:id="formula_7">b = W T x * (8) y * = argmax k (b 1 , . . . , b K ),<label>(9)</label></formula><p>where we have avoided the exponentiation in equation ( <ref type="formula" target="#formula_2">3</ref>) by realising that, at prediction time, we can make a hard decision without computing the probabilities explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL METHODOLOGY</head><p>This section presents the simulator and benchmarks used. We also describe how we gathered our training data and the methodology used to evaluate our technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Simulator and Benchmarks</head><p>Our cycle-accurate simulator is based on Wattch <ref type="bibr" target="#b21">[22]</ref>, an extension to SimpleScalar <ref type="bibr" target="#b22">[23]</ref>. We altered Wattch's underlying Cacti <ref type="bibr" target="#b23">[24]</ref> models to updated circuit parameters. We also removed the SimpleScalar RUU and added a reorder buffer, issue queue and register files. To make our simulations as realistic as possible we used Cacti to accurately model the latencies of the microarchitectural components as they varied in size. To avoid errors resulting from cold structures, we warmed the caches and branch predictor for 10 million instructions before performing each detailed simulation.</p><p>To evaluate our technique, we used all 26 SPEC CPU 2000 benchmarks <ref type="bibr" target="#b24">[25]</ref> compiled with the highest optimisation level. We ran each benchmark using the reference input set. We extracted 10 phases per program using SimPoint with an interval size of 10 million instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Performance Metric</head><p>We have evaluated the results of our predictor using energy efficiency as a metric, measured as [ips 3 /Watt] where [ips] is the number of instructions executed per second and [Watt] is the power consumption in Watts. This metric represents the trade-offs between power and performance, or the efficiency of each design point. It is widely used within the architecture community <ref type="bibr" target="#b25">[26]</ref> to indicate how efficient a configuration is at converting energy into processing speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gathering the Training Data</head><p>As seen in section IV, we need to gather data to train our model and find good solutions within our design space. To achieve this we first searched the design space by uniformly sampling 1000 random configurations. We found the best configuration for each phase, then randomly chose 200 local neighbour configurations. Finally, we repeated this by choosing the best out of the 1,200 for each phase and altered each parameter one at a time to each of its possible values. This totals 1,298 simulations per phase, or more than 300,000 in total. In addition, the results of the search were also used to approximate the best possible performance achievable per phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation Methodology</head><p>With this data we built our model and evaluated it using leave-one-out cross-validation. This is standard machine learning methodology that ensures that when we present results for a specific program, our model has never been trained with it.</p><p>To evaluate our technique, we proceed in three stages. We first characterise the current program phase by running part of it on the profiling configuration in order to gather the hardware counters. We use our model to make a prediction and then continue execution of the current phase with the configuration supplied by our model. We repeat this process for all the program's phases extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. RESULTS</head><p>This section presents the results of our technique, compared against a baseline static processor configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Configuration</head><p>In order to determine a suitable baseline, we examined all the architecture configurations in our sample space and selected the static configuration that led to the best energyefficiency on average across the benchmarks. This represents the best achievable with a single fixed static hardware configuration and is an aggressive baseline. Table <ref type="table" target="#tab_4">III</ref> shows its configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results with two Hardware Counter Sets</head><p>In this section we evaluate the gains achievable with our technique across the benchmark suite for two sets of hardware counters. The first is composed of standard performance counters available in current processors. This includes average queue occupancy, number of ALU operations, average register file usage, cache access and miss rates, branch predictor access and miss rates, and average number of instructions per cycle. The second set of counters corresponds to the more advanced features presented in section III-B2 that includes temporal histograms.</p><p>Figure <ref type="figure">4</ref> shows the energy-efficiency improvement achieved by our approach relative to the baseline configuration for the two counter sets. When compared to the best static hardware we achieve on average a factor 2x improvement in energy-efficiency with the advanced counter set. In some cases we achieve over 4x the performance of the best static hardware for vortex, art, equake and up to 6.5x for mcf . Only in two cases is the best static configuration slightly better than our approach: eon and lucas. With the basic counter set, our model only achieves 1.3x average improvement over the best overall static configuration. For several benchmarks, the performance is significantly below that of the advanced counters. This shows that the more advanced set of counters is necessary in order to achieve good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Breakdown in Performance and Energy</head><p>Having seen the results for the combined efficiency metric, we now look at the breakdown in terms of performance [ips] and energy [Joules]. Figure <ref type="figure">5</ref> shows these two metrics individually compared to the best overall static configuration. On average we observe a 15% increase in performance and a 21% decrease in energy. For some benchmarks such as crafty, the model achieves a remarkable 48% cut in energy while maintainaing the same performance as the baseline configuration. The model detects that the L2 cache and the register file are not being fully utilised and reduces their correspoding size to 256K and 64 respectively. In other cases, such as art, the model decreases the energy consumption by 15% while at the same time increasing performance by a factor 2. In this case, the model increases the issue width and the number of read/write ports to the register files and at the same time decreases the size of the instrution cache to achieve lower energy consumption. This clearly shows that our approach of driving adaptivity with a predictive model can offer large benefits to these applications. They would otherwise exhibit poor energyefficiency had we use a fixed static configuration tuned for the average case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. ANALYSIS OF THE ACCURACY OF THE MODEL</head><p>In this section we evaluate the accuracy of our approach in predicting the best configuration for each phase of the applications. We also present an analysis of the model performance at a phase level and show how architectural configurations vary with program phases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison Against Specialised Static Configurations</head><p>Although our approach clearly outperforms any fixed static configuration, having different specialised static configurations for each program may be considered an attractive alternative. This approach is used for domain specific processors such as DSPs and GPUs. Figure <ref type="figure">6</ref> shows the performance of our technique relative to the best specialised static configuration found in our sample space for that program. Clearly such an approach cannot be applied to "unseen" programs and is not viable for general-purpose computing. Nonetheless, it gives an important limit evaluation of our approach.</p><p>On average, a specialised static configuration gives a factor 1.5x improvement compared to the factor 2x of our approach. It is guaranteed never to perform worse than the best average static configuration so does not suffer performance loss in lucas and eon. Conversely, it is unable to exploit those cases where there is significant improvement available, e.g., mcf and equake, due to the large intraprogram dynamic phase variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison Against Ideal Dynamic Configurations</head><p>We now wish to determine how far our model is from the upper bound on efficiency. For this purpose we consider a scheme that has the ability to adapt the microarchitecture on a per-phase basis with full knowledge about how the application and architecture will perform. Therefore we selected, offline, the best configuration from the sample space for each phase of each program and then ran each phase with its corresponding ideal configuration (best dynamic) leading to maximum energy-efficiency.</p><p>As can be seen in figure <ref type="figure">6</ref>, on average this ideal setup gives an improvement of 2.7x over the best fixed static configuration. In some cases, like mcf , this improvement is more than 7x. Even in the worst case, eon, there is an improvement of 1.5x over the static baseline. As seen our technique gives an average improvement of 2x, thus achieving 74% of the available improvement. Generally the performance of our approach tracks the maximum available. In the case of galgel, however, there is a 4x improvement available, yet we achieve only a factor 2x, showing there is still room for improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Accuracy of Our Approach on a Phase Basis</head><p>This section evaluates the accuracy of the predictive model on a per phase basis. Figure <ref type="figure" target="#fig_2">7(a)</ref> shows two graphs overlaid. The first is a histogram representing the distribution of the efficiency values for the 260 phases. The x-axis shows the improvement achieved for a particular phase relative to the baseline. The y-axis represents the percentage of phases with a specific efficiency value. So, for example. the largest bin has an efficiency between 1x and 1.5x of the baseline and corresponds to approximately 30% of the phases. As in the previous section, the efficiency values are normalised according to the baseline (i.e., the best overall static configuration).</p><p>To determine how often we are better (or worse) than the baseline and by how much, we can look at the continuous line on the graph which is the ECDF (Estimated Cumulative Distribution Function). It shows how often our approach achieves at least a certain efficiency improvement. For example we see that our model predicts a configuration better than the baseline for 80% of the phases. We also notice that for approximately 33% of the phases the predicted configuration has an efficiency of at least two times that of the baseline. There are even a small number of phases that achieve improvement of 32 times the baseline.</p><p>Although it is important to evaluate our approach relative to the best static configuration, it is equally important to compare its accuracy against the best dynamic configurations found in the sample space for each phase as shown in figure <ref type="figure" target="#fig_2">7(b</ref>). The best configuration has a value of 1. If the performance of the predicted configuration is lower than 1, it means that it is less efficient. A value greater than 1, although surprising at first, indicates that the prediction is actually better than the best found in the sample space. This can occur because the best was not established by using an exhaustive search of the entire space. We notice that 50% of the phases achieved at least 74% of the efficiency of the best configuration. In other words, on average, we expect our model to achieve 74% of the maximum available (confirming earlier results). Interestingly, for about 9% of the phases, the predicted configuration actually performs better than the best found using a thousand samples. This provides evidence that our model can actually predict very efficient parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Architecture Configuration Variation</head><p>We now want to show how architectural configurations affect the efficiency of the overall processor design. Due to space considerations we only present results for three out of the fourteen microarchitectural parameters.</p><p>Figure <ref type="figure">8</ref> shows the distribution of efficiency values for our 260 phases as violin diagrams for the width, instruction queue, and instruction cache. These graphs show what happens when the considered parameter is fixed to a specific value and all others are allowed to vary in order to find the highest-efficiency configuration for each phase. This best efficiency value is recorded on the graph for each phase and the distribution of these values represented by the violin (the thicker the violin, the more phases are concentrated around By observing these graphs it is clear that there is no single parameter value that is good for all phases. Considering the issue queue for instance (figure <ref type="figure">8(b</ref>)), we see that a size of 72 is only optimal for 34% of the phases. However, for 25% of the phases, those below the quantile black line, this value would mean that the best achievable would be 0.6 that of the optimal (i.e., 40% less efficient). In addition we see that the efficiency of some phases can drop to 0.3, the extreme lower point of the violin's distribution.</p><p>Looking at the instruction cache in figure <ref type="figure">8</ref>(c) we see that a small size (64 sets) is optimal for 28% of the phases. It is also the value that gives the highest median (white dot) at about 0.9 from the optimal. So if a designer was to choose a static architecture, this could be a good candidate. However, the smallest size is also the one that corresponds to the lowest efficiency for some phases. We conclude that there isn't a one-fits-all approach and shows the challenges in building predictors for microarchitectural adaptivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. IMPLEMENTATION ANALYSIS</head><p>This section describes how our technique could be implemented in an actual processor design. We have evaluated the costs of gathering our hardware counters and performing reconfiguration to demonstrate that our approach can be implemented at low cost and with few overheads.</p><p>Gathering Hardware Counters: The construction of our temporal histograms is the main overhead when gathering our hardware counters. However, an efficient implementation is feasible. Since the caches contain the most complex histograms and consume the largest fraction of total processor power, they represent an upper bound on the overheads necessary to characterise program behaviour. The block and set reuse histograms are the most costly to gather. For each block the former requires two timestamps (to record the time the block was brought into the cache and the last hit), and a hit counter. The latter requires a hit counter per set.</p><p>We have used dynamic set sampling <ref type="bibr" target="#b26">[27]</ref> to reduce the number of sets and blocks that need monitoring in order to build these histograms. We ran the profiling configuration on all program phases and determined the optimum number of sets that need to be sampled to maintain high prediction accuracy. The results are shown in table <ref type="table" target="#tab_5">IV</ref> to gather the data cache's set reuse distance histogram we only need to sample four sets. Figure <ref type="figure">9</ref> shows how this translates into energy overheads. The maximum dynamic energy overhead is 1.6% when extracting the block reuse histogram from the data cache, which also incurs a leakage energy overhead of 1.4%. However, these overheads are only required when running the profiling configuration. We have verified experimentally that reconfiguration occurs once every 10 intervals, on average. Therefore, the overall overheads of gathering these counters become almost insignificant. These results show that gathering our hardware counters is cost-effective considering the efficiency savings that our model achieves.</p><p>Resource Reconfiguration: Adaptation can be achieved through the use of simple bitline segmentation of processor structures <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. This allows partitions to be turned off in isolation. We have modelled this within our simulator, allowing a 200ns delay to power up 1.2 million transistors <ref type="bibr" target="#b27">[28]</ref>. In addition, we have accurately modelled the delays required to flush caches and stall the pipeline when resources need reconfiguration. Table <ref type="table" target="#tab_7">V</ref> shows the results.</p><p>We see that the branch predictor is the quickest to reconfigure at 154 cycles whereas the L2 cache takes the longest at almost 20,000 cycles. However, the majority of this time is hidden as transistors can be powered up and down whilst the resource is still being used. Our results shows that the overall performance penalty when reconfiguration occurs is just 3% for one interval and the energy overheads are also 3%. However, since reconfiguration only occurs once every 10 intervals, the overheads for the whole phase are significantly reduced. This shows that reconfiguring processor resources can be achieved with very few overheads that are amortized over the execution of the whole phase. Model: Work by Jim?nez and Lin <ref type="bibr" target="#b28">[29]</ref> has shown how to build a perceptron-based neural branch predictor. At prediction time, our technique can be seen as a multiclass generalisation of the perceptron. We can therefore use a low-overhead version of their proposed circuit-level implementation, since our approach does not need to be trained online. This can be achieved, for example, by using 8bit signed integers for the weights (W). Since we have approximately 2000 of these, this would require 2KB of storage. Given that the model is only employed once every 10 intervals, on average, we estimate the runtime overheads to be insignificant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. PRIOR WORK ON MICROARCHITECTURAL ADAPTIVITY</head><p>Recently, Lee and Brooks <ref type="bibr" target="#b0">[1]</ref> showed that it is possible to significantly increase processor energy efficiency by adapting it as a program is running. Our work takes this a step further and shows that it is possible to build a model that can automatically drive the adaptation process.</p><p>Adaptive Processor Structures: Many researchers have examined how processor structures can be made adaptive.</p><p>The last column of table I summarises this information. In particular the issue queue <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b29">[30]</ref>, reorder buffer <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, register files <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, pipeline <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b7">[8]</ref> and caches <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref> have been studied.</p><p>Dhodapkar and Smith <ref type="bibr" target="#b30">[31]</ref> focused on control mechanisms by assessing the use of working set signatures to detect changes in behaviour of the program. <ref type="bibr">Liang, et. al. and Tiwari, et. al.</ref> separately proposed variable latency architectures where additional stages can be added to the pipeline to combat process variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>However, these studies considered only a limited adaptivity scope and looked at each of the components of the processor in isolation using control mechanisms based on simple heuristics. More recently a table-driven technique <ref type="bibr" target="#b31">[32]</ref> was proposed to reduce peak power in an adaptive processor. In comparison, our work considers varying all these parameters together and uses a machine learning model to control the adaptation process.</p><p>Multicore Adaptivity: For multicore processors, Mai et. al. illustrated an adaptive memory substrate and its flexibility when implementing very different architectures named "Smart Memories" <ref type="bibr" target="#b14">[15]</ref>. Later Sankaralingam et. al. proposed the TRIPS architecture <ref type="bibr" target="#b32">[33]</ref>, Ipek et. al. "Core Fusion" <ref type="bibr" target="#b8">[9]</ref> and Tarjan et al. "Core Federation" <ref type="bibr" target="#b9">[10]</ref>. These last two approaches merge simple cores together in order to create a wide superscalar processor.</p><p>Software-Controlled Adaptivity: Several researchers have looked at adaptivity control from the software side. Hughes et. al. <ref type="bibr" target="#b7">[8]</ref> looked at multimedia applications characterised by repeated frame processing. Hsu and Kremer <ref type="bibr" target="#b33">[34]</ref> implemented a compiler algorithm that adapts the voltage and frequency based on the characteristics of the code. Later Wu et. al. <ref type="bibr" target="#b34">[35]</ref> looked at adapting the voltage within the context of a dynamic compilation framework which can monitor and transform the program as it is running. Huang et. al. <ref type="bibr" target="#b35">[36]</ref> proposed using subroutines as a natural way to decide when to reconfigure the processor. Finally Isci et. al. <ref type="bibr" target="#b36">[37]</ref> developed a real system framework that predicts program phases on the fly to guide dynamic voltage and frequency scaling.</p><p>Heuristic-Driven Schemes: Prior work <ref type="bibr" target="#b2">[3]</ref> has also considered controlling adaptivity by looking at hardware counters extracted at runtime. However, they make use of a heuristic for search at runtime whereas we directly predict the best configuration using a machine learning model. Furthermore, they only focus on three processor queues whereas we consider 14 parameters at once.</p><p>Runtime Exploration: Other researchers looked at learning the space at runtime <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. In our context it is undesirable to perform any sort of runtime exploration since this would inevitably result in visiting poorly-performing configurations and reduce the overall efficiency.</p><p>Predictive Models: Recently, Ipek et. al. <ref type="bibr" target="#b4">[5]</ref>, Lee and Brooks <ref type="bibr" target="#b6">[7]</ref> and Joseph et al. <ref type="bibr" target="#b5">[6]</ref> proposed predictive modelling (i.e., machine learning) for architectural design space exploration. These models predict the design space of a whole program for various architecture configurations, thus enabling the efficient exploration of large design spaces. However, these are limited to whole program modelling and must first be trained for each application needing prediction. Furthermore, they are not directly usable within the context of dynamic adaptation since they would require a search of the design space at runtime.</p><p>Phase Detection: Phase detection techniques are at the core of any dynamic adaptive system and have been extensively studied previously. The work from Dhodapkar and Smith <ref type="bibr" target="#b39">[40]</ref> offers a good comparison between many proposed techniques. There are a number of examples of online phase detection techniques in the literature that rely on basic block vectors <ref type="bibr" target="#b40">[41]</ref>, instruction working sets <ref type="bibr" target="#b30">[31]</ref> or conditional branch counts <ref type="bibr" target="#b13">[14]</ref>, for example. Wavelet analysis has also gained some attention <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>X. CONCLUSION AND FUTURE DIRECTIONS</head><p>This paper has proposed a novel technique for dynamic microprocessor adaptation that differs substantially from prior work. We built a machine-learning model to predict the best configuration that uses hardware counters collected at runtime. We have introduced the notion of a temporal histogram and shown that our model is able to perform much better using these than conventional performance counters. By using our model to drive adaptivity we were able to double the energy-efficiency over the best overall static configuration. This represents 74% of the best that achievable within our sampled space.</p><p>In this work we have assumed a fixed profiling period and that all resources are adapted at the same time. Given a hardware substrate capable of reconfiguring itself at different frequencies for each resource, the challenge will be to find the degree of adaptation suitable for each hardware structure.</p><p>Finally, this paper has targeted a uniprocessor design. However, the technique presented can be directly applicable in the context of a multicore processor. If each of the cores could implement our scheme and dynamic adapt to their own workloads, this would lead to true heterogeneity; the key to high energy-efficiency. In this scenario a possible extension to this work could be to look at the implications of resource sharing when driving adaptivity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. How the optimal size of two processor structures varies with time for pipeline widths 8 and 4 for three applications.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3</head><label>3</label><figDesc>Figure 3. Load/store queue counters for four phases from different programs. We also show the relative efficiency achieved when varying the load/store queue parameters on the best configuration found (higher is better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Histograms showing the distribution of energy-efficiency values for the 260 different phases extracted from SPEC 2000 when compared to the baseline (a) and the best (b). In addition the ECDF (estimated cumulative distribution function) is represented by the solid line. The values are accumulated from the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table III THE</head><label>III</label><figDesc>CONFIGURATION OF OUR BASELINE ARCHITECTURE. Figure6. Energy-efficiency achieved by our model for all of SPEC CPU 2000 compared to the best static configuration tailored for each program and compared with the best dynamic configuration tailored for each program's phase. All the values are normalised by the best overall static configuration (higher is better).</figDesc><table><row><cell cols="2">Width</cell><cell cols="3">ROB</cell><cell></cell><cell></cell><cell cols="2">IQ</cell><cell></cell><cell></cell><cell cols="3">LSQ</cell><cell></cell><cell></cell><cell cols="2">RF</cell><cell></cell><cell></cell><cell cols="4">RF rd</cell><cell></cell><cell></cell><cell cols="2">RF wr</cell><cell>Gshare</cell><cell>BTB</cell><cell>Branches</cell><cell>Icache</cell><cell>Dcache</cell><cell>Ucache</cell><cell>Depth</cell></row><row><cell>4</cell><cell></cell><cell cols="3">144</cell><cell></cell><cell></cell><cell cols="2">48</cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell></cell><cell></cell><cell cols="3">160</cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>16K</cell><cell>1K</cell><cell>24</cell><cell>64K</cell><cell>32K</cell><cell>1M</cell><cell>12</cell></row><row><cell></cell><cell>7 7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Best static per program</cell></row><row><cell></cell><cell>6 6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Best dynamic Prediction</cell></row><row><cell>Relative efficiency Relative efficiency</cell><cell>2 3 4 5 2 3 4 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>bzip2_source bzip2_source</cell><cell>crafty crafty</cell><cell>eon_rushmeier eon_rushmeier</cell><cell>gap gap</cell><cell>gcc_integrate gcc_integrate</cell><cell>gzip_graphic gzip_graphic</cell><cell>mcf mcf</cell><cell>parser parser</cell><cell>perlbmk_704 perlbmk_704</cell><cell>twolf twolf</cell><cell>vortex_lendian1 vortex_lendian1</cell><cell>vpr_route vpr_route</cell><cell>ammp ammp</cell><cell>applu applu</cell><cell>apsi apsi</cell><cell>art_1 art_1</cell><cell>equake equake</cell><cell>facerec facerec</cell><cell>fma3d fma3d</cell><cell>galgel galgel</cell><cell>lucas lucas</cell><cell>mesa mesa</cell><cell>mgrid mgrid</cell><cell>sixtrack sixtrack</cell><cell>swim swim</cell><cell>wupwise wupwise</cell><cell>MEAN MEAN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table IV NUMBER</head><label>IV</label><figDesc>OF SETS SAMPLED FOR EACH CACHE PER FEATURE TYPE.</figDesc><table><row><cell>Feature Type</cell><cell>Insn. cache</cell><cell>Data cache</cell><cell>L2 cache</cell></row><row><cell>Set reuse distance</cell><cell>256</cell><cell>4</cell><cell>16</cell></row><row><cell>Blk reuse distance</cell><cell>16</cell><cell>128</cell><cell>32</cell></row><row><cell cols="4">that value). The % value on top, shows the percentage of</cell></row><row><cell cols="4">phases for which that fixed hardware parameter is best. For</cell></row><row><cell cols="4">instance, in the case of processor width (figure 8(a)), a width</cell></row><row><cell cols="4">of 2 is best in 22% of cases, while a width of 4 is best in</cell></row><row><cell>32% of cases.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. For example, Distribution of the highest energy-efficiency achievable for the 260 phases when the value of one parameter is fixed and the rest of the parameters are allowed to vary. For each parameter's value the white central dot represent the median efficiency value achievable in the phases and the black rectangle shows the two quartiles, where 50% of the data lies. The % value on top, shows the percentage of phases for which that fixed hardware parameter is best. Energy overheads of extracting the set and block reuse distance for each cache. The maximum overhead for the dynamic energy is 1.55% in the case of the data cache when extracting the block reuse distance. For the static energy, a maximum overhead of 1.4% is reached.</figDesc><table><row><cell></cell><cell>22%</cell><cell>32%</cell><cell>28%</cell><cell>18%</cell><cell></cell><cell cols="2">1% 5% 2% 12% 5% 18% 10% 10% 34% 3%</cell><cell>28%</cell><cell>7%</cell><cell>27%</cell><cell>26%</cell><cell>12%</cell></row><row><cell>Efficiency (relative to best found)</cell><cell>0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell></cell><cell>Efficiency (relative to best found)</cell><cell>0.0 0.2 0.4 0.6 0.8 1.0</cell><cell></cell><cell>Efficiency (relative to best found)</cell><cell>0.4 0.6 0.8 1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2</cell><cell>4</cell><cell>6</cell><cell>8</cell><cell></cell><cell>8</cell><cell>16 24 32 40 48 56 64 72 80</cell><cell>64</cell><cell>128</cell><cell>256</cell><cell>512</cell><cell>1024</cell></row><row><cell></cell><cell></cell><cell cols="2">Parameter value (fixed)</cell><cell></cell><cell></cell><cell></cell><cell>Parameter value (fixed)</cell><cell></cell><cell cols="3">Parameter value (fixed)</cell></row><row><cell></cell><cell></cell><cell>(a) Width</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Issue Queue</cell><cell cols="4">(c) Instruction Cache</cell></row><row><cell cols="3">overhead (%) overhead (%) Figure 8. Dynamic 0.5 1.0 1.5 2.0 Dynamic 0.5 1.0 1.5 2.0 0.0 0.0 ICache DDache</cell><cell>UCache</cell><cell cols="2">Static Static Block reuse distance Set reuse distance ICache DCache</cell><cell cols="2">UCache</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Figure 9.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table V OVERHEADS</head><label>V</label><figDesc>OF RECONFIGURING EACH STRUCTURE IN CYCLES.</figDesc><table><row><cell>Processor structure</cell><cell>Cycle overhead</cell></row><row><cell>Width</cell><cell>443</cell></row><row><cell>RF</cell><cell>487</cell></row><row><cell>Bpred</cell><cell>154</cell></row><row><cell>ROB</cell><cell>255</cell></row><row><cell>IQ / LSQ</cell><cell>234 / 275</cell></row><row><cell>ICache / DCache</cell><cell>478 / 620</cell></row><row><cell>UCache</cell><cell>18322</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Other approaches were tried and we found that a soft-max model led to the best results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was supported by the <rs type="funder">Royal Academy of Engineering</rs> and <rs type="funder">EPSRC</rs>. It has made use of the resources provided by the <rs type="institution">Edinburgh Compute and Data Facility (ECDF)</rs>. (http://www.ecdf.ed.ac.uk/). The ECDF is partially supported by the eDIKT initiative (http://www.edikt.org.uk). NICTA is funded by the <rs type="funder">Australian Government</rs> as represented by the <rs type="institution">Department of Broadband, Communications</rs> and the <rs type="institution">Digital Economy</rs> and the <rs type="funder">Australian Research Council</rs> through the <rs type="programName">ICT Centre of Excellence program</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3FdDSN7">
					<orgName type="program" subtype="full">ICT Centre of Excellence program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficiency trends and limits from comprehensive microarchitectural adaptivity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Energy-effective issue logic</title>
		<author>
			<persName><forename type="first">D</forename><surname>Folegnani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Reducing power requirements of instruction scheduling through dynamic allocation of multiple datapath resources</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ponomarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kucuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghose</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Microarchitectural design space exploration using an architecture-centric approach</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dubach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'boyle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Efficiently exploring architectural design spaces via predictive modeling</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Mckee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Supinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A predictive performance model for superscalar processors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Thazhuthaveetil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Accurate and efficient regression modeling for microarchitectural performance and power prediction</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ASPLOS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Saving energy with architectural and frequency adaptations for multimedia applications</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Core fusion: Accommodating software diversity in chip multiprocessors</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kirman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Federation: Repurposing scalar cores for out-of-order instruction issue</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Boyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Skadron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DAC</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">On reducing register pressure and energy in multiple-banked register files</title>
		<author>
			<persName><forename type="first">J</forename><surname>Abella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Integrating adaptive on-chip storage structures for reduced dynamic power</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dropsho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Semerano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Magklis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of Rochester, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A circuit level implementation of an adaptive issue queue for power-aware microprocessors</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note>in GLSVLSI</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Memory hierarchy reconfiguration for energy and performance in general-purpose processor architectures</title>
		<author>
			<persName><forename type="first">R</forename><surname>Balasubramonian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Buyuktosunoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dwarkadas</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Smart memories: A modular reconfigurable architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Paaske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jayasena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adaptive pipeline structures for speculation control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Efthymiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Garside</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003-05">May 2003</date>
			<publisher>ASYNC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revival: Variation tolerant architecture using voltage interpolation and variable latency</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recycle: Pipeline adaptation to tolerate process variation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reuse distance as a metric for cache behavior</title>
		<author>
			<persName><forename type="first">K</forename><surname>Beyls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hollander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PDCS</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting whole-program locality through reuse distance analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PLDI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics)</title>
		<imprint>
			<publisher>Springer-Verlag New York, Inc</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wattch: A framework for architectural-level power analysis and optimizations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The simplescalar tool set, version 2.0</title>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Austin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
		<respStmt>
			<orgName>University of Wisconsin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep. TR-1342</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><surname>Tarjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thoziyoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jouppi</surname></persName>
		</author>
		<idno>HPL-2006-86</idno>
	</analytic>
	<monogr>
		<title level="j">HP Laboratories Palo Alto</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spec cpu2000: Measuring cpu performance in the new millenium</title>
		<author>
			<persName><forename type="first">J</forename><surname>Henning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Computer</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Optimum power/performance pipeline depth</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hartstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Puzak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A case for mlp-aware cache replacement</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Patt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">90nm low leakage soc design techniques for wireless applications</title>
		<author>
			<persName><forename type="first">P</forename><surname>Royannez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bouetel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blasquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Clasen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Semino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pitts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raibaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISSCC</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Neural methods for dynamic branch prediction</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Jim?nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Computer Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic ipc/clock rate optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Albonesi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Managing multi-configuration hardware via dynamic working set analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reducing peak power with a table-driven adaptive processor core</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kontorinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Tullsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<pubPlace>Micro</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exploiting ilp, tlp, and dlp with the polymorphous trips architecture</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaralingam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The design, implementation, and evaluation of a compiler algorithm for cpu energy reduction</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Kremer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLDI</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A dynamic compilation framework for controlling microprocessor energy and performance</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Connors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Positional adaptation of processors: Application to energy reduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Renau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Torrellas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Live, runtime phase monitoring and prediction on real systems with application to dynamic power management</title>
		<author>
			<persName><forename type="first">C</forename><surname>Isci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Contreras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martonosi</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bitirgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ipek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Martinez</surname></persName>
		</author>
		<editor>MICRO</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning-based smt processor resource distribution via hill-climbing</title>
		<author>
			<persName><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISCA</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Comparing program phase detection techniques</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Dhodapkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Phase tracking and prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sherwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Calder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISCA</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Informed microarchitecture design space exploration using workload dynamics</title>
		<author>
			<persName><forename type="first">C.-B</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>MICRO</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Locality phase prediction</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASPLOS</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
